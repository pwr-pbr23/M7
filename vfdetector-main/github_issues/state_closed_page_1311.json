[{"number": 13777, "title": "s390x support for google/nsync", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: building from source\r\n- **TensorFlow version (use command below)**: master \r\n- **Python version**:  Python 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.6.1\r\n- **Exact command to reproduce**:  bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nTensorflow master build is failing on s390x platform with an error below: \r\n```\r\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n.................................................\r\nERROR: /home/test/.cache/bazel/_bazel_test/dece280ae0e9dc772a9ff752f1374540/external/nsync/BUILD:401:13: Configurable attribute \"copts\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n@nsync//:android_arm\r\n@nsync//:android_arm64\r\n@nsync//:android_armeabi\r\n@nsync//:android_x86_32\r\n@nsync//:android_x86_64\r\n@nsync//:clang_macos_x86_64\r\n@nsync//:gcc_linux_aarch64\r\n@nsync//:gcc_linux_ppc64\r\n@nsync//:gcc_linux_x86_64_1\r\n@nsync//:gcc_linux_x86_64_2\r\n@nsync//:ios_x86_64\r\n@nsync//:msvc_windows_x86_64.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\n```\r\n\r\nRecently,  [google/nsync](https://github.com/google/nsync) is added to Tensorflow master as an [external dependency](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl).  \r\nSo, s390x support needs to be added to nsync module which includes code changes in BUILD file as well as adding source code in nsync package for s390x.   \r\n\r\nWe could create platform specific subdirectory inside [nysnc/builds](https://github.com/google/nsync) for s390x by executing  tools/mkmakefile.sh script. Also, could build nsync separately on s390x by executing command make depend test.\r\nHowever, Tensorflow also uses code from platform-specific sub directories available inside  [nsync/platform/ ](https://github.com/google/nsync/tree/master/platform)  which has source code containing assembly instructions. \r\n\r\nNeed information on how to generate similar code structure for s390x (we could see ppc, windows , arm etc platforms are supported).  \r\n@gunan As we are not able to create an issue on nsync repository, could you please let me know whom to contact for this? \r\n\r\nNote: In Tensorflow, nsync dependency is added through [this ](https://github.com/tensorflow/tensorflow/commit/b48cfaea2aea3707a33e60c10385a87e37101b95)commit. \r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Hello @Nayana-ibm \r\nIt looks like we just need to add a config option here for s390x:\r\nhttps://github.com/google/nsync/blob/master/BUILD#L29\r\nAnd add the necessary copts options for the architecture.\r\nI confirmed with @m3bm3b that nsync, unlike boringssl is meant to be compatible and should work on s390x.\r\n\r\nOne detail is, We only accept changes to nsync internally.\r\nIf you can create a patch to support s390x, we can apply it internally and push the changes outside.", "\r\n\r\nThanks @gunan for information. \r\nI have added support for s390x in nsync code : BUILD as well as bazel/BUILD.bazel files. PFA code patch. \r\n[nsync_s390x_patch.txt](https://github.com/tensorflow/tensorflow/files/1407028/nsync_s390x_patch.txt)\r\n\r\nApart from above changes, I guess we also need to create platform specific sub-directory inside `nysnc/builds` for s390x by executing `tools/mkmakefile.sh` script. \r\n\r\nWith above changes, I could resolve an error :  `Configurable attribute \"copts\" doesn't match this configuration`  However Tensorflow build again failed at:\r\n\r\n```\r\nERROR: missing input file '@nsync//:platform/s390x/cputype.h'.\r\nERROR: /home/test/tensorflow/tensorflow/tools/pip_package/BUILD:139:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@nsync//:platform/s390x/cputype.h'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/test/tensorflow/tensorflow/tools/pip_package/BUILD:139:1 1 input file(s) do not exist.\r\n```\r\nTensorflow uses nsync's platform-specific sub directories available inside `nsync/platform/` which has source code containing assembly instructions ( [cputype.h](https://github.com/google/nsync/blob/master/platform/x86_64/cputype.h) and [nsync_atm_<platform>.s](https://github.com/google/nsync/blob/master/platform/x86_64/src/nsync_atm_x86_64.s) ) \r\n\r\nThe file `cputype.h` declares a flag` ATM_LD_IS_ACQ_ST_IS_REL_` . I'm not sure what should be the value (0 or 1) of this flag for s390x.  For now, I have set this flag value to 1 and with this change Tensorflow build succeeds. However, need to confirm on this. \r\n\r\nAlso, need information on what assembly instructions are required for s390x from file `nsync_atm_<platform>.s` \r\n@gunan @m3bm3b Can you please check and let me know. \r\n\r\n", "My plan is to introduce a change to the bazel BUILD file that should make\nnsync build with bazel\non linux s390x.\n\nI believe ATM_LD_IS_ACQ_ST_IS_REL_ should be 1 for s390x.\nWhen 1, this setting means\n  \"on this machine, all loads act as acquire loads, and all stores act as\nrelease stores\".\nI believe that s390x is a sequentially consistent machine, and thus the\nvalue should be 1.\nIf you believe that's not right please let me know.   (Incidentally,\nit is _always_ safe to use the value 0 for this parameter on any machine.\nTo use 0 when the value could be 1 will merely cause extra memory\nbarriers to be inserted that will cause no harm, except to be slightly\nslower\nthan necessary.   If you don't create this file, in a posix environment\nnsync will automatically pick up platform/posix/cputype.h, which has the\nvalue 0.)\nI plan to create this file for s390x with the value 1 in an upcoming change.\n\n\nIt is usually not essential to create nsync_atm_<platform>.s for each new\nmachine type.\nThe library needs to get atomic operations from somewhere, but there are\nseveral possibilities.\nAs well as assembly language, nsync may get atomic operations\nfrom language-dependent libraries (it can from C++11 or C11),\ncompiler intrinsics (it can from gcc and clang),  or from\noperating system dependent libraries (such as on MacOS, NetBSD, Win32).\nWhen compiling TensorFlow, it's normal to use the C++11 atomic ops because\nit's all in C++11.\nThus, assuming you have a complete C++11 implementation,\nthe compiler and its runtime will already provide the operations in a\nTensorFlow build,\nand handwritten assembly language is not needed in that case.\nThe assembly language is\nneeded typically when compiling with unusual or proprietary compilers in\nC90,\noften on uncommon operating systems (embedded systems, for example).\n\nMike\n\nOn 23 October 2017 at 05:22, Nayana Thorat <notifications@github.com> wrote:\n\n> Thanks @gunan <https://github.com/gunan> for information.\n> I have added support for s390x in nsync code : BUILD as well as\n> bazel/BUILD.bazel files. PFA code patch.\n>\n> Apart from above changes, I guess we also need to create platform specific\n> sub-directory inside nysnc/builds for s390x by executing\n> tools/mkmakefile.sh script.\n>\n> With above changes, I could resolve an error : Configurable attribute\n> \"copts\" doesn't match this configuration However Tensorflow build again\n> failed at:\n>\n> ERROR: missing input file '@nsync//:platform/s390x/cputype.h'.\n> ERROR: /home/test/tensorflow/tensorflow/tools/pip_package/BUILD:139:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@nsync//:platform/s390x/cputype.h'.\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\n> Use --verbose_failures to see the command lines of failed build steps.\n> ERROR: /home/test/tensorflow/tensorflow/tools/pip_package/BUILD:139:1 1 input file(s) do not exist.\n>\n> Tensorflow uses nsync's platform-specific sub directories available inside\n> nsync/platform/ which has source code containing assembly instructions (\n> cputype.h\n> <https://github.com/google/nsync/blob/master/platform/x86_64/cputype.h>\n> and nsync_atm_.s\n> <https://github.com/google/nsync/blob/master/platform/x86_64/src/nsync_atm_x86_64.s>\n> )\n>\n> The file cputype.h declares a flagATM_LD_IS_ACQ_ST_IS_REL_ . I'm not sure\n> what should be the value (0 or 1) of this flag for s390x. For now, I have\n> set this flag value to 1 and with this change Tensorflow build succeeds.\n> However, need to confirm on this.\n>\n> Also, need information on what assembly instructions are required for\n> s390x from file nsync_atm_<platform>.s\n> @gunan <https://github.com/gunan> @m3bm3b <https://github.com/m3bm3b> Can\n> you please check and let me know.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13777#issuecomment-338640123>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsp2SZlizVVf-rnQx3TJc_BP9Q_ZMhFks5svITogaJpZM4P70PT>\n> .\n>\n", "Thanks @m3bm3b for detailed explanation. \r\nSo, now to add support for s390x in nsync, the provident code patch and cputype.h file with the value 1 would be sufficient?", "I've just uploaded the change to github as branch v1.11,\nchange e14acb6027eca0d7eb5044d70bc9879acbedb727.\nI'd be grateful if you could try it.   It updates the BUILD file (assuming\nyou're building with bazel),\nand adds that header file.    I haven't added a makefile directory for it\nbecause I was unable to test. (I tried repeatedly to get Debian up using\nthe Hercules emulator,\nbut every time I installed, the resulting system failed to boot, even\nthough the Debian install\nhad claimed the installation had succeeded.)\n\nBeware that TensorFlow has not been updated to fetch the new version\nof nsync automatically---it will still fetch the old one.  I will do that\nupdate\nto TensorFlow assuming all is well with the change to nsync.\n\nOn 23 October 2017 at 10:45, Nayana Thorat <notifications@github.com> wrote:\n\n> Thanks @m3bm3b <https://github.com/m3bm3b> for detailed explanation.\n> So, now to add support for s390x in nsync, the provident code patch and\n> cputype.h file with the value 1 would be sufficient?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13777#issuecomment-338739686>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsp2cRnzRn_zD3YY54bUvk4HS9xhlArks5svNCjgaJpZM4P70PT>\n> .\n>\n", "Yes...Will verify the nsync code on s390x with the commit id you have mentioned.\r\nI guess I need to create a makefile directory also.   Earlier, I had used command make depend test. Is that sufficient?\r\nIs there any specific command  to build nsync using basel build?\r\nI'll also try building Tensorflow using above commit id (just by making change in workspace.bzl file of Tensorflow) on s390x using basel build command.", "Assuming you wish to build nsync as C++11\n(which is what TensorFlow requires), at the root of the nsync tree do:\n  cp -r builds/x86_64.linux.c++11 builds/s390x.linux.c++11\n  cd builds/s390x.linux.c++11\n  make depend test\nThis doesn't actually use the new header file because it's using C++11\natomics.\n\nTo try the new header file, do a C build, like this:\n    mkdir builds/s390x.linux.old_gcc\n    sed 's,x86_64,s390x,g' builds/x86_64.linux.old_gcc/Makefile >\nbuilds/s390x.linux.old_gcc/Makefile\n    touch builds/s390x.linux.old_gcc/dependfile\n    cd builds/s390x.linux.old_gcc\n    make depend test\nThis will build nsync as C (not C++11)\nusing \"old-style\" gcc atomics; that will use the header file.\n(Old-style gcc atomics have compare and swap, but\ndon't include loads and stores, so those are\nimplemented with ordinary loads and stores, with extra memory barriers\nas indicated by the flag in the new header.   The assumption here is\nthat the platform will perform loads and stores atomically if\nthey are 32-bits and naturally aligned.\nNew-style gcc atomics do include loads and stores, and so again wouldn't\nreally need\nthe header file.)\n\nAs you can tell, there are a distressingly large number of\noptions for getting atomic operations, because some platforms\nimplement some techniques, while others implement different ones.\n\nOn 23 October 2017 at 11:33, Nayana Thorat <notifications@github.com> wrote:\n\n> Yes...Will verify the nsync code on s390x with the commit id you have\n> mentioned.\n> I guess I need to create a makefile directory also. Earlier, I had used\n> command make depend test. Is that sufficient?\n> Is there any specific command to build nsync using basel build?\n> I'll also try building Tensorflow using above commit id (just by making\n> change in workspace.bzl file of Tensorflow) on s390x using basel build\n> command.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13777#issuecomment-338753351>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsp2Y_8wV4xPN7Ep_0BZ0adkQdXOGbYks5svNvZgaJpZM4P70PT>\n> .\n>\n", "I forgot to say, there's nothing special needed to build nsync with bazel.\n   bazel test :all\nshould work to build and test both with a C and a C++11 build.\n\n\nOn 23 October 2017 at 11:57, Mike Burrows <m3b@google.com> wrote:\n\n> Assuming you wish to build nsync as C++11\n> (which is what TensorFlow requires), at the root of the nsync tree do:\n>   cp -r builds/x86_64.linux.c++11 builds/s390x.linux.c++11\n>   cd builds/s390x.linux.c++11\n>   make depend test\n> This doesn't actually use the new header file because it's using C++11\n> atomics.\n>\n> To try the new header file, do a C build, like this:\n>     mkdir builds/s390x.linux.old_gcc\n>     sed 's,x86_64,s390x,g' builds/x86_64.linux.old_gcc/Makefile >\n> builds/s390x.linux.old_gcc/Makefile\n>     touch builds/s390x.linux.old_gcc/dependfile\n>     cd builds/s390x.linux.old_gcc\n>     make depend test\n> This will build nsync as C (not C++11)\n> using \"old-style\" gcc atomics; that will use the header file.\n> (Old-style gcc atomics have compare and swap, but\n> don't include loads and stores, so those are\n> implemented with ordinary loads and stores, with extra memory barriers\n> as indicated by the flag in the new header.   The assumption here is\n> that the platform will perform loads and stores atomically if\n> they are 32-bits and naturally aligned.\n> New-style gcc atomics do include loads and stores, and so again wouldn't\n> really need\n> the header file.)\n>\n> As you can tell, there are a distressingly large number of\n> options for getting atomic operations, because some platforms\n> implement some techniques, while others implement different ones.\n>\n> On 23 October 2017 at 11:33, Nayana Thorat <notifications@github.com>\n> wrote:\n>\n>> Yes...Will verify the nsync code on s390x with the commit id you have\n>> mentioned.\n>> I guess I need to create a makefile directory also. Earlier, I had used\n>> command make depend test. Is that sufficient?\n>> Is there any specific command to build nsync using basel build?\n>> I'll also try building Tensorflow using above commit id (just by making\n>> change in workspace.bzl file of Tensorflow) on s390x using basel build\n>> command.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/13777#issuecomment-338753351>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AJsp2Y_8wV4xPN7Ep_0BZ0adkQdXOGbYks5svNvZgaJpZM4P70PT>\n>> .\n>>\n>\n>\n", "@m3bm3b I could build and verify nsync code on s390x platform as C++11 as well as C. \r\n(Using steps that you have mentioned in your previous comment)\r\n\r\nAs C++11:\r\n```\r\ncp -r builds/x86_64.linux.c++11 builds/s390x.linux.c++11 \r\ncd builds/s390x.linux.c++11\r\nmake depend test \r\n```\r\nAs C:\r\n```\r\nmkdir builds/s390x.linux.old_gcc \r\nsed 's,x86_64,s390x,g' builds/x86_64.linux.old_gcc/Makefile > builds/s390x.linux.old_gcc/Makefile \r\ntouch builds/s390x.linux.old_gcc/dependfile \r\ncd builds/s390x.linux.old_gcc \r\nmake depend test \r\n```\r\n\r\nAlso, could verify using `bazel test :all` as well.\r\n\r\n@gunan I could build Tensorflow master on s390x by updating Tensorflow workspace.bzl with nsync commit id as : e14acb6027eca0d7eb5044d70bc9879acbedb727. Should I go ahead and raise a PR for the same?\r\n\r\n ", "I've made the equivalent change to TensorFlow's workspace.bzl\nso it should now pick up the version of nsync that will work for you.\n(The sha256 is different (839fcc53ff9be58218ed55397deb3f8376a1444e),\nbecause I made another change to\nnsync, unrelated to s390x support, but it includes the change you need.)\n\n\nOn 24 October 2017 at 08:08, Nayana Thorat <notifications@github.com> wrote:\n\n> @m3bm3b <https://github.com/m3bm3b> I could build and verify nsync code\n> on s390x platform as C++11 as well as C.\n> (Using steps that you have mentioned in your previous comment)\n>\n> As C++11:\n>\n> cp -r builds/x86_64.linux.c++11 builds/s390x.linux.c++11\n> cd builds/s390x.linux.c++11\n> make depend test\n>\n> As C:\n>\n> mkdir builds/s390x.linux.old_gcc\n> sed 's,x86_64,s390x,g' builds/x86_64.linux.old_gcc/Makefile > builds/s390x.linux.old_gcc/Makefile\n> touch builds/s390x.linux.old_gcc/dependfile\n> cd builds/s390x.linux.old_gcc\n> make depend test\n>\n> Also, could verify using bazel test :all as well.\n>\n> @gunan <https://github.com/gunan> I could build Tensorflow master on\n> s390x by updating Tensorflow workspace.bzl with nsync commit id as :\n> e14acb6027eca0d7eb5044d70bc9879acbedb727. Should I go ahead and raise a\n> PR for the same?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13777#issuecomment-339021966>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsp2Znoy6goii3Ye1EBFTb9oQO8_XWTks5svf1egaJpZM4P70PT>\n> .\n>\n", "@m3bm3b I couldn't see the changes done in TensorFlow's workspace.bzl file in TensorFlow master. \r\nThe build is failing with same error on s390x. Am I missing anything here?", "Sorry.  I made the changes to an internal version that gets pushed to\ngithub periodically.\nI had forgotten that that process takes a while.\n\nThe diff is below.  It should appear on github within a day or so.\nSorry for the confusion.\n\n==== //depot/google3/third_party/tensorflow/workspace.bzl#252 -\n/google/src/files/173208878/depot/google3/third_party/tensorflow/workspace.bzl\n====\n432,433c432,433\n<           \"https://mirror.bazel.build/github.com/google/nsync/archive/ad722c76c6e6653f66be2e1f69521b7f7517da55.tar.gz\",\n<           # \"https://github.com/google/nsync/archive/ad722c76c6e6653f66be2e1f69521b7f7517da55.tar.gz\",\n---\n>           \"https://mirror.bazel.build/github.com/google/nsync/archive/839fcc53ff9be58218ed55397deb3f8376a1444e.tar.gz\",\n>           # \"https://github.com/google/nsync/archive/839fcc53ff9be58218ed55397deb3f8376a1444e.tar.gz\",\n435,436c435,436\n<       sha256 =\n\"7dd8ca49319f77e8226cd020a9210a525f88ac26e7041c59c95418223a1cdf55\",\n<       strip_prefix = \"nsync-ad722c76c6e6653f66be2e1f69521b7f7517da55\",\n---\n>       sha256 = \"124d105edb0313ef2d7f5bb86ec94d9f8de95479e55641c4254ffa8f795e9b37\",\n>       strip_prefix = \"nsync-839fcc53ff9be58218ed55397deb3f8376a1444e\",\n\n\n\nOn 24 October 2017 at 23:32, Nayana Thorat <notifications@github.com> wrote:\n\n> @m3bm3b <https://github.com/m3bm3b> I couldn't see the changes done in\n> TensorFlow's workspace.bzl file in TensorFlow master.\n> The build is failing with same error on s390x. Am I missing anything here?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13777#issuecomment-339228417>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsp2cy1IBAkXbD_hh5Q2tceX7gV1zRsks5svtX4gaJpZM4P70PT>\n> .\n>\n", "Thanks @m3bm3b ", "I believe the change has now reached github.  You might check again.\n\nOn 25 October 2017 at 10:22, Nayana Thorat <notifications@github.com> wrote:\n\n> Thanks @m3bm3b <https://github.com/m3bm3b>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13777#issuecomment-339404743>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsp2ZoJ4fprZZhBzapXuVDWH5rXA3rlks5sv25ugaJpZM4P70PT>\n> .\n>\n", "@m3bm3b Yes. Changes are now available in Tensorflow master.  I could build Tensorflow master successfully. Thank you for your help and fast response. "]}, {"number": 13776, "title": "Revert \"Add GPU and CPU implementation of `tf.histogram_fixed_width`.\u2026", "body": "\u2026 (#13731)\"\r\n\r\nThis reverts commit 528457ea3cbe4edfbd3eb90c303b2a1408fe8d65.\r\n\r\nThe failing test is `//tensorflow/contrib/distributions:poisson_lognormal_test`\r\n\r\nhttps://source.cloud.google.com/results/invocation/28566b59-d961-458c-8978-b16dc248782f/%2F%2Ftensorflow%2Fcontrib%2Fdistributions:poisson_lognormal_test?page=log\r\n\r\nSee comment: https://github.com/tensorflow/tensorflow/pull/13731#issuecomment-337127918\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "/cc @gunan ", "Also running tests here, in case the other fix cannot go in.\r\n\r\nJenkins, test this please.", "Looks like the other fix worked, so closing."]}, {"number": 13775, "title": "Installed Tensorflow-gpu but i cant import it it gives me this error", "body": "Traceback (most recent call last):\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Mohammad Reza\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Could you try to run [this script](https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c) and see what it points you to?", "/CC @mrry ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 13774, "title": "Cannot install Tensorflow with specific GCC version", "body": "I'm trying to install Tensorflow 1.3 without success \r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Gentoo\r\n- **TensorFlow installed from (source or binary)**: Trying from source\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.4.5\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**: 8.0 / 5.1.5\r\n- **GPU model and memory**: GTX 960\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\n\r\nI'm trying to install Tensorflow 1.3 (branch r1.3) from sources on my Gentoo machine. \r\n\r\nThe problem with the default compiler is that it it's a version 6.3 that is not supported by Tensorflow: I've tried and got an error indicating that GCC later than 5 are not supported. \r\nTherefore, I need to specify a specific GCC version. I'm setting it via $CC/$CXX and in the installation for CUDA:\r\n\r\n`build --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc-5.4.0\"`\r\n\r\nBut when I try to run bazel, I got the following error: \r\n\r\n> gcc-5.4.0: error trying to exec 'cc1': execvp: No such file or directory\r\n\r\nWhat can I do to install Tensorflow using /usr/bin/gcc-5.4.0 and /usr/bin/g++-5.4.0 rather than /usr/bin/gcc ? \r\n\r\nI've seen this f0faf5139819e2c6d7b0437d9e03ffce71c7d6e5 that seems releated, but in my case, this is already done. And it seems to be used for clang compilation anyway. \r\n\r\n### Source code / logs\r\n\r\n> ____Loading complete.  Analyzing...\r\n> WARNING: /home/wichtounet/dev/tensorflow13/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately..\r\n> WARNING: /home/wichtounet/dev/tensorflow13/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately..\r\n> ____Found 1 target...\r\n> ____Building...\r\n> ____[3 / 255] Writing file external/snappy/libsnappy.a-2.params [for host]\r\n> ERROR: /home/wichtounet/.cache/bazel/_bazel_wichtounet/bb5d0ce0cffd837b9339296f34d5478c/external/nasm/BUILD.bazel:8:1: C++ compilation of rule '@nasm//:nasm' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 34 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n> gcc-5.4.0: error trying to exec 'cc1': execvp: No such file or directory\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> ____Elapsed time: 0.285s, Critical Path: 0.08s", "comments": ["@jart Can you comment on how to override to a supported version of GCC here?\r\nYou might also try stackoverflow for some help...", "I ended up changing the configuration of my computer in order to have GCC 4.9 as the default compiler to compile Tensorflow. "]}, {"number": 13773, "title": "ERROR:tensorflow:Exception in QueueRunner: truncated record at 935047", "body": "```\r\n###########################small data train, train_step code:\r\ndef run_train():\r\n    with tf.Graph().as_default():\r\n        global_step = tf.Variable(0, trainable=False)\r\n        images,labels=read_and_decode('./dog_train')\r\n        images_batch,labels_batch = add_batch(images,labels,5,5)\r\n        softmax_linear = cnn_model(images_batch)\r\n        loss = loss(softmax_linear, labels_batch)\r\n        train_op = train(loss, global_step)\r\n        saver = tf.train.Saver(tf.global_variables())\r\n        summary_op = tf.summary.merge_all()\r\n        init = tf.global_variables_initializer()\r\n        sess = tf.Session(config=tf.ConfigProto(log_device_placement=FLAGS.log_device_placement))\r\n        sess.run(init)\r\n        tf.train.start_queue_runners(sess=sess)\r\n        summary_writer = tf.summary.FileWriter(FLAGS.train_dir,graph=sess.graph)\r\n        for step in xrange(FLAGS.max_steps):\r\n            start_time = time.time()\r\n            _, loss_value = sess.run([train_op, loss])\r\n            duration = time.time() - start_time\r\n            assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\r\n            if step % 10 == 0:\r\n                num_examples_per_step = FLAGS.batch_size\r\n                examples_per_sec = num_examples_per_step / duration\r\n                sec_per_batch = float(duration)\r\n\r\n                format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f ''sec/batch)')\r\n                print(format_str % (datetime.now(), step, loss_value,examples_per_sec, sec_per_batch))\r\n            if step % 100 == 0:\r\n                summary_str = sess.run(summary_op)\r\n                summary_writer.add_summary(summary_str, step)\r\n            if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\r\n                checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\r\n                saver.save(sess,checkpoint_path, global_step=step)\r\n#######################################and ERROR:\r\n2017-10-17 15:08:31.510128: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x46f9230\r\nERROR:tensorflow:Exception in QueueRunner: truncated record at 935047\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input_producer)]]\r\n\t [[Node: DecodeRaw/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_17_DecodeRaw\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\nException in thread Thread-3:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 237, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1060, in _single_operation_run\r\n    target_list_as_strings, status, None)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\nDataLossError: truncated record at 935047\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input_producer)]]\r\n\t [[Node: DecodeRaw/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_17_DecodeRaw\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nERROR:tensorflow:Exception in QueueRunner: truncated record at 935047\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input_producer)]]\r\nException in thread Thread-2:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 237, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1060, in _single_operation_run\r\n    target_list_as_strings, status, None)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\nDataLossError: truncated record at 935047\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input_producer)]]\r\n\r\n2017-10-17 15:08:49.270117: step 0, loss = 21.79 (0.6 examples/sec; 8.205 sec/batch)\r\n ############ here stop \r\n```\r\nso,why this errors ?\r\n\r\n\r\n", "comments": ["It looks like there is some error in the tf record. The queues are a little harder to debug.\r\n\r\nWould you consider `tf.contrib.data.DataSet`? This is a much easier API.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "sorry \uff0cSo late to respond\uff0cit seems my samples is small\uff0cbecause when i Increase the number of samples \uff0cit  works\uff0c", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It sounds like it works for you, closing this issue."]}, {"number": 13772, "title": "No OpKernel was registered to support Op 'StridedSlice'", "body": "## Wrong usage. Fixed.\r\n\r\nI trained a model via python and load the freeze model for prediction using c++. When running prediction binary, some errors occur.\r\n\r\nOS: macOS 10.12.6\r\nTensorflow: master. built from source via `makefile/build_all_ios.sh`\r\n\r\nDeadly error:\r\n```\r\nInvalid argument: No OpKernel was registered to support Op 'StridedSlice' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BOOL]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n\r\n\t [[Node: emb_179/embedding_lookup_sparse/strided_slice = StridedSlice[Index=DT_INT32, T=DT_INT64, begin_mask=1, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=2](input/linear_179/index/Placeholder, emb_179/embedding_lookup_sparse/strided_slice/stack, emb_179/embedding_lookup_sparse/strided_slice/stack_1, emb_179/embedding_lookup_sparse/strided_slice/stack_2)]]\r\n```\r\n\r\nFull error message:\r\n```\r\nYour CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nSession created successfully\r\nLoad graph protobuf successfully\r\n2017-10-17 12:51:52.708004: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"PopulationCount\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: PopulationCount\r\n2017-10-17 12:51:52.708091: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseAnd\r\n2017-10-17 12:51:52.708111: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseXor\r\n2017-10-17 12:51:52.708138: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"EncodeWav\" device_type: \"CPU\"') for unknown op: EncodeWav\r\n2017-10-17 12:51:52.708147: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"DecodeWav\" device_type: \"CPU\"') for unknown op: DecodeWav\r\n2017-10-17 12:51:52.708188: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"Mfcc\" device_type: \"CPU\"') for unknown op: Mfcc\r\n2017-10-17 12:51:52.708205: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: Invert\r\n2017-10-17 12:51:52.708277: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseOr\r\n2017-10-17 12:51:52.708346: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"AudioSpectrogram\" device_type: \"CPU\"') for unknown op: AudioSpectrogram\r\nInvalid argument: No OpKernel was registered to support Op 'StridedSlice' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BOOL]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n\r\n\t [[Node: emb_179/embedding_lookup_sparse/strided_slice = StridedSlice[Index=DT_INT32, T=DT_INT64, begin_mask=1, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=2](input/linear_179/index/Placeholder, emb_179/embedding_lookup_sparse/strided_slice/stack, emb_179/embedding_lookup_sparse/strided_slice/stack_1, emb_179/embedding_lookup_sparse/strided_slice/stack_2)]]\r\n```", "comments": ["please include a reproducible test case. One way this error can occur is if the op kernel is not registered for the specific data type you are using: float, double, etc...", "@andydavis1 \r\nBecause the code is too much, I just put the core part here.\r\n\r\nDuring training, the inputs to `embedding_lookup_sparse` are `tf.int64`. It works fine. After training, the node info of `emb_179/embedding_lookup_sparse/strided_slice` is \r\n```\r\nnode {\r\n  name: \"emb_179/embedding_lookup_sparse/strided_slice\"\r\n  op: \"StridedSlice\"\r\n  input: \"input/ParseExample/ParseExample:1\"\r\n  input: \"emb_179/embedding_lookup_sparse/strided_slice/stack\"\r\n  input: \"emb_179/embedding_lookup_sparse/strided_slice/stack_1\"\r\n  input: \"emb_179/embedding_lookup_sparse/strided_slice/stack_2\"\r\n  attr {\r\n    key: \"Index\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_INT64\r\n    }\r\n  }\r\n  attr {\r\n    key: \"begin_mask\"\r\n    value {\r\n      i: 1\r\n    }\r\n  }\r\n  attr {\r\n    key: \"ellipsis_mask\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"end_mask\"\r\n    value {\r\n      i: 1\r\n    }\r\n  }\r\n  attr {\r\n    key: \"new_axis_mask\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"shrink_axis_mask\"\r\n    value {\r\n      i: 2\r\n    }\r\n  }\r\n}\r\n```\r\n## python part for training\r\nIt works fine. Just show the core part here.\r\n```\r\nwith tf.variable_scope('input/sparse_field'):\r\n  with tf.variable_scope('index'):\r\n    sparse_index = tf.placeholder(tf.int64)\r\n  with tf.variable_scope('id'):\r\n   sparse_id = tf.placeholder(tf.int64)\r\n   with tf.variable_scope('value'):\r\n    sparse_val = tf.placeholder(tf.float32)\r\n  with tf.variable_scope('shape'):\r\n    sparse_shape = tf.placeholder(tf.int64)\r\nwith tf.variable_scope('label'):\r\n  label = tf.placeholder(tf.float32)\r\nsparse_ids = tf.SparseTensor(sparse_index, sparse_id, sparse_shape)\r\nsparse_vals = tf.SparseTensor(sparse_index, sparse_val, sparse_shape)\r\n\r\ninput_size = 100\r\nembedding_size = 50\r\nwith tf.variable_scope(\"emb_179\"):\r\n  embedding_variable = tf.Variable(tf.truncated_normal([input_size, embedding_size], stddev=0.05), name='emb' + str(field_id))\r\n  embedding = tf.nn.embedding_lookup_sparse(embedding_variable, sparse_ids, sparse_vals, \"mod\", combiner=\"sum\")\r\n...\r\n```\r\n\r\n## c++ prediction demo\r\nIt load the graph and checkpoint successfully, but give an error in line `session.Run(...)` on node `emb_179/embedding_lookup_sparse/strided_slice`. The inputs shows here.\r\n```\r\nauto id_indice_tensor =\r\ntest::AsTensor<int64>(indice, {static_cast<int64>(indice.size()/2), 2});\r\ninputs.push_back(std::pair<std::string, Tensor>(\"input/sparse_field/index/Placeholder\", id_indice_tensor));\r\nauto id_list_tensor = test::AsTensor<int64>(fid_list);\r\ninputs.push_back(std::pair<std::string, Tensor>(\"input/sparse_field/id/Placeholder\", id_list_tensor));\r\nauto val_list_tensor = test::AsTensor<float>(fval_list);\r\ninputs.push_back(std::pair<std::string, Tensor>(\"input/sparse_field/value/Placeholder\", val_list_tensor));\r\n\r\nstd::vector<tensorflow::Tensor> outputs;\r\nStatus status = session->Run(inputs, {\"predict/add\"}, {}, &outputs);\r\n```\r\n\r\nMaybe should register a type of `DT_INT64`.", "Yes, it looks like there is no kernel registered for this (perhaps there was issue int64 on GPU). But registering an int64 strided slice for CPU shouldn't be an issue. Marking as contributions welcome.", "@formath could you provide environment infomation? Say, tensorflow version etc.\r\n\r\n```python\r\nimport tensorflow as tf\r\na = tf.constant([[[1, 1, 1], [2, 2, 2]],\r\n                         [[3, 3, 3], [4, 4, 4]],\r\n                         [[5, 5, 5], [6, 6, 6]]], dtype=tf.int64)\r\n\r\nb = tf.strided_slice(a, [1, 0, 0], [2, 1, 3], [1, 1, 1])\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(b))        # [[[3 3 3]]]\r\n```\r\n\r\nI test the script above on tf 1.3, and it works well.", "It seems that `StridedSlice` has registered all types for cpu, see:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/513f7df42e4eadfcd241a3be695af6fd426b734e/tensorflow/core/kernels/strided_slice_op.cc#L388", "@formath I see, the exception is raised when predicting. If I understand correctly, `TF_CALL_ALL_TYPES` will register all numerical types, including `TF_CALL_int64`, so I believe it is correct here. Perhaps there exists some limitation for prediction, say, int64 may be not supported now, I don't know.", "fixed", "Wonderful. Just a little curious, could you explain how to fix it?", "`build_all_ios.sh` is for mobile platform which has no support for int64. osx should use `build_all_linux.sh`.", "@formath im getting the same error running the code from Android Studio. im not sure how to use build_all_linux.sh into my project in android studio. can u help me with this? \r\n\r\n> `build_all_ios.sh` is for mobile platform which has no support for int64. osx should use `build_all_linux.sh`.\r\n\r\n"]}, {"number": 13771, "title": "[Error] unknown op in C++", "body": "I load a simple graph `c=a*b` in C++ and do prediction for `c`. It gives right result but some errors `unknown op` occur. What's the problem?\r\n\r\nOS: macOS 10.12.6\r\nTensorflow: master. build from source using `makefile/build_all_ios.sh`\r\n\r\nprediction cpp code:\r\n```\r\nGraphDef graph_def;\r\nstd::string graph_path = argv[1];\r\nstatus = ReadBinaryProto(Env::Default(), graph_path, &graph_def);\r\nif (!status.ok()) {\r\n    std::cout << status.ToString() << std::endl;\r\n    return 1;\r\n} else {\r\n    std::cout << \"Load graph protobuf successfully\" << std::endl;\r\n}\r\nstatus = session->Create(graph_def);\r\nif (!status.ok()) {\r\n    std::cout << status.ToString() << std::endl;\r\n    return 1;\r\n} else {\r\n    std::cout << \"Add graph to session successfully\" << std::endl;\r\n}\r\n\r\n// Setup inputs and outputs:\r\nTensor a(DT_FLOAT, TensorShape());\r\na.scalar<float>()() = 3.0;\r\nTensor b(DT_FLOAT, TensorShape());\r\nb.scalar<float>()() = 2.0;\r\nstd::vector<std::pair<string, tensorflow::Tensor>> inputs = {\r\n    { \"a\", a },\r\n    { \"b\", b },\r\n};\r\n\r\nstd::vector<tensorflow::Tensor> outputs;\r\n\r\n// Run the session, evaluating our \"c\" operation from the graph\r\nstatus = session->Run(inputs, {\"c\"}, {}, &outputs);\r\nif (!status.ok()) {\r\n    std::cout << status.ToString() << std::endl;\r\n    return 1;\r\n} else {\r\n    std::cout << \"Run session successfully\" << std::endl;\r\n}\r\n\r\nauto output_c = outputs[0].scalar<float>();\r\nstd::cout << outputs[0].DebugString() << std::endl; \r\nstd::cout << \"output value: \" << output_c() << std::endl; \r\n\r\nsession->Close();\r\n```\r\n\r\nFull error message:\r\n```\r\n2017-10-17 12:32:11.796090: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nSession created successfully\r\nLoad graph protobuf successfully\r\n2017-10-17 12:32:11.808591: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"PopulationCount\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: PopulationCount\r\n2017-10-17 12:32:11.808658: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseAnd\r\n2017-10-17 12:32:11.808669: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseXor\r\n2017-10-17 12:32:11.808691: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"EncodeWav\" device_type: \"CPU\"') for unknown op: EncodeWav\r\n2017-10-17 12:32:11.808699: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"DecodeWav\" device_type: \"CPU\"') for unknown op: DecodeWav\r\n2017-10-17 12:32:11.808732: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"Mfcc\" device_type: \"CPU\"') for unknown op: Mfcc\r\n2017-10-17 12:32:11.808747: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: Invert\r\n2017-10-17 12:32:11.808806: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseOr\r\n2017-10-17 12:32:11.808857: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"AudioSpectrogram\" device_type: \"CPU\"') for unknown op: AudioSpectrogram\r\nAdd graph to session successfully\r\nRun session successfully\r\nTensor<type: float shape: [] values: 6>\r\noutput value: 6\r\n```", "comments": ["It looks like it's doing `3.0 * 2.0`, but other than that, it works fine?", "@drpngx Yes. `c=a*b`. It gives right result. But I'm not sure whether it's normal for those messages\r\n```\r\n2017-10-17 12:32:11.808591: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"PopulationCount\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: PopulationCount\r\n2017-10-17 12:32:11.808658: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseAnd\r\n2017-10-17 12:32:11.808669: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseXor\r\n2017-10-17 12:32:11.808691: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"EncodeWav\" device_type: \"CPU\"') for unknown op: EncodeWav\r\n2017-10-17 12:32:11.808699: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"DecodeWav\" device_type: \"CPU\"') for unknown op: DecodeWav\r\n2017-10-17 12:32:11.808732: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"Mfcc\" device_type: \"CPU\"') for unknown op: Mfcc\r\n2017-10-17 12:32:11.808747: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: Invert\r\n2017-10-17 12:32:11.808806: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseOr\r\n2017-10-17 12:32:11.808857: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"AudioSpectrogram\" device_type: \"CPU\"') for unknown op: AudioSpectrogram\r\n```", "Basically, it's a poor interface, but yes, these errors should be ignored.", "@drpngx `export TF_CPP_MIN_LOG_LEVEL=3` works for me but it always block all errors. Maybe those error messages should be print in level 2.", "There is no way to block these messages other than modifying the code."]}, {"number": 13770, "title": "the comment is confusing", "body": "I find the comment say that embedding shape is [num_encoder_symbols x input_size], I was so confused and checked the source code, I believe it will be better to change the input_size to embedding_size", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 13769, "title": "How periodicaly evaluate the Performance of Models in TF-Slim?", "body": "I am trying to use [DensNet][1] for regression problem with TF-Slim. My data contains 60000 jpeg images with 37 float labels for each image. I divided my data into three different tfrecords files of a train set (60%), a validation set (20%) and a test set (20%). \r\n\r\nI need to evaluate validation set during training loop and make a plot like [image][2]. \r\nIn TF-Slim documentation they just explain train loop and evaluation loop separately. I can just evaluate validation or test set after training loop finished. While as I said I need to evaluate during training.\r\n\r\nI tried to use slim.evaluation.evaluation_loop function instead of slim.evaluation.evaluate_once. But it doesn't help.\r\n\r\n    slim.evaluation.evaluation_loop(\r\n        master=FLAGS.master,\r\n        checkpoint_dir=checkpoint_path,\r\n        logdir=FLAGS.eval_dir,\r\n        num_evals=num_batches,\r\n        eval_op=list(names_to_updates.values()) + print_ops,\r\n        variables_to_restore=variables_to_restore,\r\n        summary_op = tf.summary.merge(summary_ops),\r\n        eval_interval_secs = eval_interval_secs )\r\n\r\nI tried evaluation.evaluate_repeatedly as well.\r\n\r\n    from tensorflow.contrib.training.python.training import evaluation\r\n\r\n    evaluation.evaluate_repeatedly(\r\n        master=FLAGS.master,\r\n        checkpoint_dir=checkpoint_path,\r\n        eval_ops=list(names_to_updates.values()) + print_ops,\r\n        eval_interval_secs = eval_interval_secs )\r\n\r\nIn both of these functions, they just read the latest available checkpoint from checkpoint_dir and apparently waiting for the next one, however when the new checkpoints are generated, they don't perform at all.\r\n\r\nI use Python 2.7.13 and Tensorflow 1.3.0 on CPU.\r\n\r\nAny help will be highly appreciated.\r\n\r\n  [1]: https://github.com/pudae/tensorflow-densenet\r\n  [2]: https://i.stack.imgur.com/HzLPq.jpg", "comments": ["@sguada could you take a look?", "I have similar problem. Same as asked here (https://stackoverflow.com/questions/46781847/how-periodicaly-evaluate-the-performance-of-models-in-tf-slim)\r\n", "I have just encountered the same problem. To me the error was caused by the following: \r\nif tf.gfile.IsDirectory(FLAGS.checkpoint_path):\r\n                checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\r\n            else:\r\n                checkpoint_path = FLAGS.checkpoint_path\r\n\r\nI solved it by simply setting \r\ncheckpoint_path = FLAGS.checkpoint_path\r\n\r\nIt seems like slim.evaluation.evaluation_loop needs the actual directory of the checkpoint files where as evaluate_once needs the latest checkpoint file. Which makes sense. \r\nI hope it helps. ", "@rasorensen90 \r\nYes, you are right. In this code when the folder is empty in FLAGS.checkpoint_path and waiting for new checkpoints, the checkpoint_path returns NONE. \r\n`  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\r\n         checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path) \r\n    else:\r\n         checkpoint_path = FLAGS.checkpoint_path\r\n`\r\n\r\nI added this line and it works fine.\r\n\r\n`       \r\n       \r\n           if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\r\n\r\n                if tf.train.latest_checkpoint(FLAGS.checkpoint_path):\r\n       \r\n                         checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\r\n        \r\n                else:\r\n        \r\n                         checkpoint_path = FLAGS.checkpoint_path\r\n`", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Looks like @Ellie68 posted a resolution."]}, {"number": 13768, "title": "Packages missing in current channels: - tensorflow", "body": "I typed the following in the Anaconda Prompt\r\n\"\r\nconda create --name=IntroToTensorFlow python=3 anaconda\r\nsource activate IntroToTensorFlow\r\nconda install -c conda-forge tensorflow\"\r\n\r\nThen it shows\r\n\r\nPackages missing in current channels: \r\n      -  tensorflow\r\n- http://conda.anaconda.org/conda-forge/win-32\r\n- http://conda.anaconda.org/conda-forge/noarch\r\n- http://repo.continuum.io/pkgs/main/win-32\r\n- http://repo.continuum.io/pkgs/main/noarch\r\n- http://repo.continuum.io/pkgs/free/win-32\r\n- http://repo.continuum.io/pkgs/free/noarch\r\n- http://repo.continuum.io/pkgs/r/win-32\r\n- http://repo.continuum.io/pkgs/r/noarch\r\n- http://repo.continuum.io/pkgs/pro/win-32\r\n- http://repo.continuum.io/pkgs/pro/noarch\r\n- http://repo.continuum.io/pkgs/msys2/win-32\r\n- http://repo.continuum.io/pkgs/msys2/noarch\r\n", "comments": ["btw, conda-forge is community maintained, the maintainers may be able to answer https://github.com/conda-forge/tensorflow-feedstock/", "Thanks @yaroslavvb. Please pose this question to the conda-forge community."]}, {"number": 13767, "title": "Branch 172408922", "body": "", "comments": ["@tensorflow-jenkins test this please", "Failures seem unrelated, so I'm merging."]}, {"number": 13766, "title": "Add sparse tensor support to `tf.data.Dataset`", "body": "The `tf.data.Dataset` class does not currently recognize a `tf.SparseTensor` object as a component of a dataset element. This makes it difficult to use the full capabilities of `SparseTensor`-producing ops such as `tf.parse_single_example()` in `Dataset.map()` transformations, and then manipulating those elements using operations like `Dataset.batch()`. The existing `tf.train.batch()` and related functions for queue-based pipelines support `tf.SparseTensor`, and we should add support to `Dataset` for parity.\r\n\r\nThis [Stack Overflow answer](https://stackoverflow.com/a/46732695/3574081) suggests some possible workarounds in the meantime.", "comments": ["@ebrevdo Can you comment on this sparsity question?", "PS. Eugene: There's no obligation on you to answer or take this issue :). It's a placeholder while we do the necessary work to make `SparseTensor` work in `tf.data`. However, any thoughts you have would be welcome!", "It should be relatively simple to make this work.  Most of the hard work has been done by Derek already (iirc, there's already code to split and concat STs in dataset).  It feels like just a matter of writing python glue, right?", "Indeed I'd hope it's possible to do in pure-Python with existing ops and transformations... though as you well know, the glue for handling `SparseTensor` can get a bit sticky :).\r\n\r\n/cc @jsimsa who is taking a look.", "is there any progress here?", "Lots of progress, in fact! If you upgrade to the nightly build, `tf.SparseTensor` is now usable wherever `tf.Tensor` is in all core `Dataset` operations including `Dataset.from_tensors()`, `Dataset.from_tensor_slices()`, `Dataset.map()`, and `Dataset.batch()`. The `tf.parse_single_example()` and `tf.parse_example()` operations work as normal with sparse and variable-length features.\r\n\r\nThanks for reminding us about this issue... I'm going to mark it as closed, but feel free to reply if there are any remaining difficulties.\r\n\r\n(All credit due to @jsimsa.)", "great news, and what for `Dataset.padded_batch`, is it in plans?", "@svetlov, could you please describe your use case to make sure that when the support is implemented, it addresses your needs? Thank you.", "@jsimsa, I learn model to estimate video - text relevance, so from one side i have sequence of frames(var length), and by other side i have bag of words/ngrams. For this it's convinient to use old interface `tf.batch(..., dynamic_pad=True)`, cause i want padded video frames for lstm and sparse representation of bag of words/ngrams cause of `tf.contrib.layers.safe_embedding_lookup_sparse`. ", "I'd also like to see `Dataset.padded_batch` support `tf.SparseTensor`.  Right now I'm using Spark to create word vectors and I'd prefer to keep all data in TFRecord files and implement zero-padding at the last moment in TensorFlow.\r\n\r\nBasically would like something like the below to work where `max_tokens` could be selected during model generation.\r\n\r\n```\r\ndef parse_tokens(example):\r\n    parsed = tf.parse_single_example(example, features={\r\n        'tokens': tf.VarLenFeature(tf.int64)\r\n    })\r\n\r\ndataset = dataset.map(parse_tokens)\r\ndataset = dataset.padded_batch(args.batch_size, [args.max_tokens])\r\n```", "@jsimsa, any progress here?", "up, it blocks migration to dataset api from queues for me :-(", "FWIW this turned out to be an easy fix for me.\r\n\r\n```\r\ndataset = dataset.map(parse_training_example)\r\ndataset = dataset.map(lambda tokens: tf.sparse_tensor_to_dense(tokens, default_value=''))\r\ndataset = dataset.padded_batch(batch_size, tf.TensorShape([None]), EOS)\r\n```", "In which release are sparse tensors supported in `tf.data.Dataset`? I have 1.4.0 and I'm getting the following error:\r\n\r\n```\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"ParseSingleExample/Slice_Indices_label:0\", shape=(?, 1), dtype=int64), values=Tensor(\"Cast_1:0\", shape=(?,), dtype=int32), dense_shape=Tensor(\"ParseSingleExample/Squeeze_Shape_label:0\", shape=(1,), dtype=int64)). Consider casting elements to a supported type.\r\n```", "They're supported in TF 1.5.0.", "Ty!", "Unfortunately, no support for batching padded sparse tensors. Currently getting this  error:\r\n```\r\nTypeError: Batching of padded sparse tensors is not currently supported\r\n```\r\n\r\nAny thoughts on when this will get implemented? I'm currently using `1.5.0-rc1`\r\n\r\nCheers,\r\nTom", "I should've found this sooner. My bad :( But for others encountering similar issues. Here are a couple of good work arounds suggested by @mrry \r\n\r\nhttps://stackoverflow.com/questions/46651137/tf-contrib-data-dataset-seems-does-not-support-sparsetensor?noredirect=1&lq=1", "@themodernlife Your solution worked for me as well. Thanks! ", "@mrry Any update on this? I have upgraded to version 1.7.0 and still encountering the same issue.\r\n\r\n```python\r\nfrom argparse import ArgumentParser\r\nimport pickle\r\n\r\nimport tensorflow as tf\r\n\r\ndef main(args):\r\n\r\n    # Generate the complete source and meta paths.\r\n    source = args.source\r\n    if not source.endswith('.tfrecords'):\r\n        source = '{}.tfrecords'.format(source)\r\n\r\n    meta = args.source\r\n    if not source.endswith('.tfrecords'):\r\n        meta = '{}.meta.pkl'.format(meta)\r\n    else:\r\n        meta = '{}.meta.pkl'.format(meta.rsplit('.', 1)[0])\r\n    \r\n    # Train the model.\r\n    train(source, meta, args.destination)\r\n\r\ndef parse(example_proto):\r\n\r\n    features = {\r\n        'bucket': tf.FixedLenFeature([], tf.int64),\r\n        'coefficients': tf.FixedLenFeature([], tf.string),\r\n        'coefficients_length': tf.FixedLenFeature([], tf.int64),\r\n        'label': tf.VarLenFeature(tf.int64)\r\n    }\r\n    parsed_features = tf.parse_single_example(example_proto, features)\r\n\r\n    bucket = tf.cast(parsed_features['bucket'], tf.int32)\r\n    coefficients = tf.decode_raw(parsed_features['coefficients'], tf.float32)\r\n    coefficients_length = tf.cast(parsed_features['coefficients_length'], tf.int32)\r\n    label = tf.cast(parsed_features['label'], tf.int32)\r\n\r\n    return bucket, coefficients, coefficients_length, label\r\n\r\ndef train(source, meta, destination, batch_size=64, epochs=1):\r\n\r\n    # Load the training meta data.\r\n    file = open(meta, 'rb')\r\n    meta = pickle.load(file)\r\n    file.close()\r\n\r\n    # Create a tf.data input pipe line.\r\n    dataset = tf.data.TFRecordDataset([source])\r\n    dataset = dataset.map(parse)\r\n    dataset = dataset.padded_batch(batch_size, padded_shapes=(0, None, 0, None))\r\n    dataset = dataset.repeat(epochs) \r\n    iterator = dataset.make_initializable_iterator()\r\n\r\n    bucket, coefficients, coefficients_length, label = iterator.get_next()\r\n\r\n    with tf.Session() as session:\r\n        \r\n        session.run(iterator.initializer)\r\n        \r\n        print(session.run([bucket, coefficients, coefficients_length, label]))\r\n\r\nif __name__ == '__main__':\r\n    \r\n    PARSER = ArgumentParser(description='Trains a model.')\r\n    PARSER.add_argument('--destination', required=True, type=str,\r\n    \t\t\t\t\thelp='The path where the trained model should be stored.')\r\n    PARSER.add_argument('--source', required=True, type=str,\r\n    \t\t\t\t\thelp='The path to the training data.')\r\n    ARGS = PARSER.parse_args()\r\n\r\n    main(ARGS)\r\n```\r\n\r\nOutput:\r\n```\r\nTraceback (most recent call last):\r\n  File \"trainer.py\", line 70, in <module>\r\n    main(ARGS)\r\n  File \"trainer.py\", line 20, in main\r\n    train(source, meta, args.destination)\r\n  File \"trainer.py\", line 49, in train\r\n    dataset = dataset.padded_batch(batch_size, padded_shapes=(0, None, 0, None))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 821, in padded_batch\r\n    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1713, in __init__\r\n    \"Batching of padded sparse tensors is not currently supported\")\r\nTypeError: Batching of padded sparse tensors is not currently supported\r\n```", "@thomasquintana Can you please open a new issue for this? /cc @jsimsa ", "Same issue as @thomasquintana here. Any updates?\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-24-90b0df972296> in <module>()\r\n      1 X, y = iter(dataset).next()\r\n----> 2 y, predict(X)\r\n\r\n<ipython-input-23-f1d970eaa549> in predict(X)\r\n      5 \r\n      6 def predict(X):\r\n----> 7     return W @ X + B\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in _run_op(a, *args)\r\n    795       # pylint: disable=protected-access\r\n    796       value = a._AsTensor()\r\n--> 797       return getattr(ops.Tensor, operator)(value, *args)\r\n    798 \r\n    799     # Propagate __doc__ to wrapper\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\r\n    977           else:\r\n    978             raise\r\n--> 979       return func(x, y, name=name)\r\n    980 \r\n    981   def binary_op_wrapper_sparse(sp_x, y):\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\r\n   2062         a = ops.convert_to_tensor(a, name=\"a\")\r\n   2063       if not isinstance(b, (ops.EagerTensor, _resource_variable_type)):\r\n-> 2064         b = ops.convert_to_tensor(b, name=\"b\")\r\n   2065     else:\r\n   2066       a = ops.convert_to_tensor(a, name=\"a\")\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n   1012       name=name,\r\n   1013       preferred_dtype=preferred_dtype,\r\n-> 1014       as_ref=False)\r\n   1015 \r\n   1016 \r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1102 \r\n   1103     if ret is None:\r\n-> 1104       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1105 \r\n   1106     if ret is NotImplemented:\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    233                                          as_ref=False):\r\n    234   _ = as_ref\r\n--> 235   return constant(v, dtype=dtype, name=name)\r\n    236 \r\n    237 \r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    183   ctx = context.context()\r\n    184   if ctx.executing_eagerly():\r\n--> 185     t = convert_to_eager_tensor(value, ctx, dtype)\r\n    186     if shape is None:\r\n    187       return t\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n    129     return t\r\n    130   else:\r\n--> 131     return ops.EagerTensor(value, context=handle, device=device, dtype=dtype)\r\n    132 \r\n    133 \r\n\r\nValueError: Attempt to convert a value (<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x119ec7780>) with an unsupported type (<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>) to a Tensor.\r\n```", "@byronyi That looks like a different issue: the exception and stack trace are almost completely different. Please open a new issue with details for reproducing the problem.", "1. In parse_record_fn, use:\r\na = tf.sparse_tensor_to_dense(features['a'])\r\ninstead of:\r\na = features['a'].\r\n2. dataset = dataset.map(parse_record_fn).padded_batch(batch_size, padded_shapes=padded_shapes)\r\n\r\nThese help me"]}, {"number": 13765, "title": "Import \"google/protobuf/any.proto\" was not found or had errors.", "body": " make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=\"-Os\" CXX=g++-4.8\r\nPROTOC = \"protoc\"\r\nCC_PREFIX = \"\"\r\nprotoc  tensorflow/contrib/boosted_trees/proto/learner.proto --cpp_out /home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/\r\nprotoc  tensorflow/contrib/boosted_trees/proto/quantiles.proto --cpp_out /home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/\r\nprotoc  tensorflow/contrib/boosted_trees/proto/split_info.proto --cpp_out /home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/\r\nprotoc  tensorflow/contrib/boosted_trees/proto/tree_config.proto --cpp_out /home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/\r\nprotoc  tensorflow/core/util/test_log.proto --cpp_out /home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/\r\ngoogle/protobuf/any.proto: File not found.\r\ntensorflow/core/util/test_log.proto: Import \"google/protobuf/any.proto\" was not found or had errors.\r\ntensorflow/core/util/test_log.proto:132:12: \"google.protobuf.Any\" is not defined.\r\nmake: *** [/home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/core/util/test_log.pb.cc] Error 1", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "OS -> Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-59-generic x86_64)\r\nuname -r -> 4.4.0-59-generic\r\nPython 2.7.6\r\npip --version ->  pip 1.5.4 from /usr/lib/python2.7/dist-packages (python 2.7)\r\n\r\ngcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.8/lto-wrapper\r\nTarget: x86_64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.3)\r\n\r\nBuild from tensorflow master branch by following these steps from this link ->\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#raspberry-pi\r\n\r\ntensorflow/contrib/makefile/download_dependencies.sh\r\nsudo apt-get install -y autoconf automake libtool gcc-4.8 g++-4.8\r\ncd tensorflow/contrib/makefile/downloads/protobuf/\r\n./autogen.sh\r\n./configure\r\nmake\r\nsudo make install\r\nsudo ldconfig  # refresh shared library cache\r\ncd ../../../../..\r\nexport HOST_NSYNC_LIB=`tensorflow/contrib/makefile/compile_nsync.sh`\r\nexport TARGET_NSYNC_LIB=\"$HOST_NSYNC_LIB\"\r\n\r\nBut none of the following command work and have the error that I report.\r\n\r\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=\"-Os\" CXX=g++-4.8\r\n\r\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI \\\r\n OPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=g++-4.8\r\n\r\nCannot import tensorflow too in python\r\n\r\npython\r\nPython 2.7.6 (default, Oct 26 2016, 20:30:19)\r\n[GCC 4.8.4] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n\r\n", "bazel version\r\n/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.UTF-8)\r\nWARNING: ignoring http_proxy in environment.\r\nBuild label: 0.6.1\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Oct 5 21:54:59 2017 (1507240499)\r\nBuild timestamp: 1507240499\r\nBuild timestamp as int: 1507240499\r\n\r\nI try this command but it does not work.\r\n\r\nsudo -E bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n\r\nWARNING: Config values are not defined in any .rc file: opt\r\nERROR: /home/a_name/tensorflow.build/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n        File \"/home/a_name/tensorflow.build/third_party/py/python_configure.bzl\", line 310\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"/home/a_name/tensorflow.build/third_party/py/python_configure.bzl\", line 268, in _create_local_python_repository\r\n                _get_python_bin(repository_ctx)\r\n        File \"/home/a_name/tensorflow.build/third_party/py/python_configure.bzl\", line 166, in _get_python_bin\r\n                _get_env_var(repository_ctx, _PYTHON_BIN_PATH, No..., ...)\r\n        File \"/home/a_name/tensorflow.build/third_party/py/python_configure.bzl\", line 49, in _get_env_var\r\n                _python_configure_fail((\"'%s' environment variable is n...))\r\n        File \"/home/a_name/tensorflow.build/third_party/py/python_configure.bzl\", line 37, in _python_configure_fail\r\n                fail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: 'PYTHON_BIN_PATH' environment variable is not set\r\n and referenced by '//third_party/py/numpy:headers'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\nINFO: Elapsed time: 4.234s\r\nFAILED: Build did NOT complete successfully (25 packages loaded)\r\n    currently loading: tensorflow/core\r\n", "No, you need to run the pip install as root, not build the pip package. Please follow the instructions.", "I have the same issue using TF 1.4.0 with python 2.7 (via venv). I use Centos 7, CUDA 9, CNDNN 7 and GCC 4.8.5", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Stumbled on this issue for a similar problem and realized I wasn't including the base includes from the protoc zip.\r\n\r\nprotoc-3.6.1-linux-x86_64.zip unzips with\r\n\r\n````\r\n~/bin/protoc$ ls -lh\r\ntotal 1.4M\r\ndrwxr-xr-x 2 mmacdermaid mmacdermaid 4.0K Jul 30 15:49 bin\r\ndrwxr-xr-x 3 mmacdermaid mmacdermaid 4.0K Jul 30 15:49 include\r\n-rw-rw-r-- 1 mmacdermaid mmacdermaid 1.4M Nov  5 16:04 protoc-3.6.1-linux-x86_64.zip\r\n-rw-r--r-- 1 mmacdermaid mmacdermaid  715 Jul 30 15:49 readme.txt\r\n````\r\n\r\nWhich the readme does say\r\n````\r\nIf you intend to use the included well known types then don't forget to\r\ncopy the contents of the 'include' directory somewhere as well, for example\r\ninto '/usr/local/include/'.\r\n````\r\n\r\nBut who reads those?\r\n\r\nIf you don't add them into /usr/local/include you have to explicitly include them via protoc options\r\n````\r\nprotoc -I ~/bin/protoc/include\r\n````\r\n\r\nHope this helps someone.", "Note that we have our own protobuf text serialization within tensorflow and that it doesn't support any. The closest is to use a map of Attr.", "This was fixed when we upgraded our protoc version. Thanks\r\nprotoc --version\r\nlibprotoc 3.6.0"]}, {"number": 13764, "title": "Failure in TestNewTensor when running go test", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source (branch 1.4)\r\n- **TensorFlow version (use command below)**: 1.4.0-dev\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: nVidia 1080Ti 11G\r\n- **Exact command to reproduce**: go test -v github.com/tensorflow/tensorflow/tensorflow/go\r\n\r\n### Describe the problem\r\n\r\nI'm trying to use the go bindings to the tensorflow c library. When I run the tests, I get a nil pointer dereference and a segfault. The details are below. Note that I've built the c library from source using the following options:\r\n\r\n`bazel build -c opt --config=cuda --config=mkl -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 -c opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow:libtensorflow.so`\r\n\r\n### Source code / logs\r\nWhen I run go test -v github.com/tensorflow/tensorflow/tensorflow/go I get the following error:\r\n```\r\n2017-10-16 17:12:30.568054: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\n2017-10-16 17:12:30.568065: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\r\n--- FAIL: TestNewTensor (0.00s)\r\npanic: runtime error: invalid memory address or nil pointer dereference [recovered]\r\n        panic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x536098]\r\n\r\ngoroutine 168 [running]:\r\ntesting.tRunner.func1(0xc42059c4e0)\r\n        /usr/lib/go-1.8/src/testing/testing.go:622 +0x29d\r\npanic(0x6a0b80, 0xa18e80)\r\n        /usr/lib/go-1.8/src/runtime/panic.go:489 +0x2cf\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.tensorData(0x7fa8f40195b0, 0xc420595900, 0x688a80, 0x6ffb90)\r\n        /home/vishvananda/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:209 +0x48\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.NewTensor(0x683d20, 0xc4205945e0, 0xc42004d9a0, 0x2, 0x2)\r\n        /home/vishvananda/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:92 +0x221\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.TestNewTensor(0xc42059c4e0)\r\n        /home/vishvananda/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor_test.go:92 +0x2526\r\ntesting.tRunner(0xc42059c4e0, 0x6ffbd0)\r\n        /usr/lib/go-1.8/src/testing/testing.go:657 +0x96\r\ncreated by testing.(*T).Run\r\n        /usr/lib/go-1.8/src/testing/testing.go:697 +0x2ca\r\nexit status 2\r\nFAIL    github.com/tensorflow/tensorflow/tensorflow/go  0.443s\r\n```\r\nAdding some debugging, it turns out that the TestNewTensor test fails when attempting to create the following tensor `{[]int64{2, 0}, [][]int64{{}, {}}}`. If I comment out that line, the tests pass.", "comments": ["It looks like this is due to the fact that TF_TensorData returns nil if no data is allocated. Assuming this is correct behavior and nil needs to be checked for on the go side then the following patch fixes the problem:\r\n```\r\ndiff --git a/tensorflow/go/tensor.go b/tensorflow/go/tensor.go\r\nindex e8fa21a..6cbf759 100644\r\n--- a/tensorflow/go/tensor.go\r\n+++ b/tensorflow/go/tensor.go\r\n@@ -205,6 +205,9 @@ func (t *Tensor) WriteContentsTo(w io.Writer) (int64, error) {\r\n func tensorData(c *C.TF_Tensor) []byte {\r\n        // See: https://github.com/golang/go/wiki/cgo#turning-c-arrays-into-go-slices\r\n        cbytes := C.TF_TensorData(c)\r\n+       if cbytes == nil {\r\n+               return nil\r\n+       }\r\n        length := int(C.TF_TensorByteSize(c))\r\n        slice := (*[1 << 30]byte)(unsafe.Pointer(cbytes))[:length:length]\r\n        return slice\r\n```", "Thanks for the report @vishvananda. I'm unable to reproduce the problem using the [1.3.0 release binary](\"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.3.0.tar.gz\"), or [1.4.0-rc0 release binary](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.3.0.tar.gz) (will try rebuilding from source using the exact flags you're using). Do you see the same problem when using the release binaries of the C API?\r\n\r\nWhich version of `go` are you using? Also, is it possible that `LD_LIBRARY_PATH` is somehow bringing in an older version of the C API libraries that your go program ends up using?\r\n\r\nIt should be okay for `TF_TensorData` to return `nil`.\r\n\r\nAny additional information in reproducing the environment will be helpful. (I'll try to dig into this a bit more by rebuilding from source using the command you provided above)", "Fascinating, both the 1.3.0 and the 1.4.0-rc0 release binary return zero length from TF_TensorData but they return a pointer to an actual buffer instead of nil. I'm attempting my flags on the 1.3 branch to see if it is the flags that are causing it to return nil. Next, I'll try removing the extra flags one at a time to see if I can narrow it down. I suspect AllocateTensor ends up with a nil buffer in certain cases. In the successful versions I don't see this error message:\r\n```\r\n2017-10-16 21:31:57.797656: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\n2017-10-16 21:31:57.797690: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\r\n```\r\nIn any case the nil check is probably good to have anyway.", "Ok, I think I've tracked down the issue to building with MKL. If I build without `--config=mkl` the tests pass fine. The issue is that building with MKL uses the bfc_allocator to allocate memory. That allocator explicitly returns nil when an allocation of zero bytes is performed (As a side note, this probably should not be an error or warning if we expect it to happen when we request a zero length tensor). In the case of running without MKL, the allocation eventually calls malloc (or jemalloc or alloc_aligned). The man tells me that malloc(0) is implementation defined and our version returns a non-nil pointer so the code does not throw a null-pointer exception. I suggest using something like the patch I included above for the go side, and maybe downgrading the error and warning messages in bfc_allocator.cc and allocator_retry.cc to something a bit less scary (maybe Info?).", "Thanks for the detailed trackdown @vishvananda, much appreciated.\r\n\r\nYes, what you said makes sense. For the Go side, would you like to contribute a pull request to make the fix? If not, let me know and I'm happy to make the change as well.\r\n\r\nThanks!"]}, {"number": 13763, "title": "Modify the TensorFlow Scheduler and Runtime to Change the Operations Priority", "body": "Hi,\r\n\r\nI am trying to modify the TensorFlow scheduler and runtime to change the operation priorities. As my understanding, TensorFlow has inter-operation and intra-operation thread pool with a scheduler scheduling operations for different threads and there is also a FIFO queue of operations for operations waiting. The workflow between them is that operations are sent to the inter-op thread pool from the executor, and then that work is running through XLA compiler to eventually be executed on the intra-op thread pool. Schedule() is in inside a specific ThreadPool and is to schedule function for the threads in the pool. The thread scheduler selects a subset of threads to run at any given moment. When the tasks are passed to the ThreadPool, they are added to one of the scheduler thread\u2019s FIFO queues and then the scheduler will pick up the tasks distributed to the available worker threads. I have all the control and data dependencies got from the Graph in TensorFlow by using TensorBoard. Now I am not sure whether my understanding for the TensorFlow scheduler and runtime is correct or not.\r\n\r\nThe problem is that I still have no clue how and where to modify the threadpool/scheduler/ready_queue or others to change the operations priority/sequence for different threads at the TensorFlow runtime by modifying the source code. Does anyone have any ideas?", "comments": ["So overall I think your understanding is correct. The Executor basically runs the graph. You would have to plumb execution policies all the way.\r\n\r\nIs there something specific you'd like to do? Can you describe the API at a high level?", "Thank you for your reply. \r\n\r\nActually, I did some research in the executor.cc and directSession.cc, and can print out the operations sequence and threads id in the runtime. I am still stuck in \r\n\r\n1. how to grab an operation before it's put into the ready_queue to be executed? \r\n\r\n2. How to schedule more intra/inter threads for a specific operation at the executor? Since the TensorFlow uses the Eigen threadpool which is not able to be changed, we can not do that at the TF runtime specifically in executor.cc. \r\n\r\nThe purpose to do that is to improve the performance in finer-grained operation level with parallelism.\r\n\r\nI am looking forward to any advise/suggestion.", "@mrry or @vrv or @benoitsteiner if you have time to process this, that would be great, o/w I'll take a look.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@Jiawen1991 are you still interested? Did you make progress on your own?", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Hi, I am working on something very similar as well. Is there a way to control scheduling of operations in TensorFlow to improve the performance with operation level parallelism?"]}, {"number": 13762, "title": "How are threads bound to physical cores in TensorFlow runtime and inter/intra threadpools?", "body": "Hi all,\r\n\r\nIf one CPU has 8 cores and each core has two threads, does TF runtime use the OS threads scheduler to match the logical cores and threads to the physical cores and threads? Or does it has its own policy to match the threads and cores? Does TF only recognize cores and but not the whole device(CPU) like the system does? (The OS only knows the number/id of cores and sockets and doesn't know how many CPU devices) Does the \"device\" in TF correspond to the physical core, e.g., TF recognizes 8 devices in the above CPU?\r\n\r\nI found out that there are inter/intra global and local threadpools in /tensorflow/core/common_runtime/local_device.cc and /tensorflow/core/common_runtime/dirrect_session.cc. Does that mean each device (or core?) have its own local inter/intra threadpool and there is only one inter/intra global threadpool in the whole TF runtime?\r\n\r\nThank you for your time.", "comments": ["TF does not pin threads to cores (though if you build with MKL, I believe that Intel's internal thread pool implementation does (and probably makes use of hyperthreads if it makes sense.\r\n\r\nThanks for your question, but please ask these types of questions on Stackoverflow..."]}, {"number": 13761, "title": "tf.reduce_sum gives value error when given int64 as input.", "body": "Passing a tensor of dtype=int64 into tf.reduce sum I receive the following error:\r\n\r\nTensor(\"loss/diff:0\", shape=(50,), dtype=int64)\r\nValueError: Invalid type tf.int64 for loss/Sum:0, expected: [tf.float32, tf.float64, tf.float16].\r\n\r\nAccording to the documents from https://www.tensorflow.org/api_docs/python/tf/reduce_sum:\r\n\r\ninput_tensor: The tensor to reduce. Should have numeric type.\r\n\r\nAs int64 is a numeric type I am not sure what's wrong. \r\n\r\nTo create the diff tensor I do:\r\n\r\n```\r\n        self.predictions = tf.argmax(self.logits, 1, name='predictions')\r\n\r\n         # Loss\r\n        with tf.name_scope(\"loss\"):\r\n            self.diff = tf.subtract(self.predictions, self.targets)\r\n            self.diff = tf.multiply(self.diff,self.diff, name='diff')\r\n            print self.diff\r\n            self.diff = tf.reduce_sum(self.diff)\r\n```\r\n\r\nWhere predictions is of type int64 and my targets placeholder is also of type int64.\r\n\r\nIs this a tensorflow error or an error on my end?\r\n", "comments": ["Could you list all the environment information required by issue template? ", "Sure:\r\n\r\nSystem information\r\n\r\nLinux Ubuntu 16.04\r\nTensorFlow installed via pip \r\nTensorflow Version: v1.3.0-rc2-20-g0787eee\r\nPython 2.7.12\r\nCUDA Version 8.0.61\r\ncuDNN V5 (I believe)\r\nGeForce GTX 1070 and GeForce GTX 780 Ti", "OK, looking at the comment in the code, it looks like we have `int32` as a special case, but not `int64`. There appears to be some subtlety around the implementation.\r\n\r\nMaybe @mrry can clarify.\r\n", "Now it's coming back to me. IIRC, because meta-data information is stored on the host, running operations on GPU was slower because of the overhead. So we have traditionally stayed away from integer operations on GPU. Not sure if that changed.\r\n\r\nIn your case, you can still cast to int32 where the op is implemented.", "I tried making all tensors involved into int32's:\r\n\r\n```\r\nself.targets = tf.placeholder(tf.int32, [batchSize])\r\nself.predictions = tf.argmax(self.logits, 1, output_type=tf.int32, name='predictions')\r\n```\r\n\r\nAlthough I still receive the same error.\r\n\r\nValueError: Invalid type tf.int32 for loss/Sum:0, expected: [tf.float32, tf.float64, tf.float16].\r\n\r\nI don't believe I can cast in this instance as casting is an operation that doesn't create a gradient. I initially tried casting both tensors to tf.float32 and using tf.losses.mean_squared_error but I received a 'ValueError: No gradients provided for any variable'. This is an attempt at a possible work around.\r\n", "Oh, when does it crash? If it's integer, I am not sure how the gradient would work. Instead of `argmax`, you'll want to train against the likelihood loss.", "I am attempting to implement the one hot cnn as described here : https://arxiv.org/pdf/1412.1058.pdf\r\n\r\nIn the paper Rie Johnson mentions they minimize square loss. I also read https://arxiv.org/abs/1702.05659 which states that loss functions other than the most popular log loss sometimes produce better results.\r\n\r\nI had the CNN working with cross entropy:\r\n\r\n```\r\n losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.targets, logits=self.logits,\r\n                name=\"xentropy\")\r\n            self.loss = tf.reduce_mean(losses)\r\n```\r\n\r\nbut wish to try some other loss functions to see if I can get an accuracy boost. \r\n\r\nIf I change my loss to:\r\n\r\n```\r\nself.loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=self.targets, predictions=self.predictions))\r\n```\r\n\r\nThis is the error message I receive:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"cnn_Sparse_TwoFilt.py\", line 261, in <module>\r\n    train_op = optimizer.minimize(cnn.loss, global_step=global_step)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 322, in minimize\r\n    ([str(v) for _, v in grads_and_vars], loss))\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'conv-maxpool-1/W:0' shape=(28395, 50) dtype=float32_ref>\", \"<tf.Variable 'conv-maxpool-1/b:0' shape=(50,) dtype=float32_ref>\", \"<tf.Variable 'conv-maxpool-2/W:0' shape=(56790, 50) dtype=float32_ref>\", \"<tf.Variable 'conv-maxpool-2/b:0' shape=(50,) dtype=float32_ref>\", \"<tf.Variable 'output/W:0' shape=(100, 20) dtype=float32_ref>\", \"<tf.Variable 'output/b:0' shape=(20,) dtype=float32_ref>\"] and loss Tensor(\"loss/Mean:0\", shape=(), dtype=float32).\r\n```\r\n\r\nThis seems like a similar issue to: https://stackoverflow.com/questions/40294271/tensorflow-minimize-l2-loss-on-int64-data-without-casting-to-float32-because-ca which there is no answer to. The author of that post also posted here: https://github.com/tensorflow/tensorflow/issues/1511 but she received instructions to go to stackoverflow, which I assume resulted in the first link. \r\n", "Can you describe a bit more what `targets` are? It sounds strange. I think you're meaning to use\r\n`[p(y|x)-I(x,y)]^2`.", "The ground truth labels, they are integer's denoting the location of the proper class index.\r\n\r\nI believe did [ preds - labels ] ^ 2 (this is backwards, but shouldn't matter due to the square) with the integers here: \r\n\r\n```\r\n self.diff = tf.subtract(self.predictions, self.targets)\r\n self.diff = tf.multiply(self.diff,self.diff, name='diff')\r\n```\r\n", "Right. So, is there a special order in the labels that would make you want to prefer that loss? In that case, if you misclassify `1` for `10`, it's not the same as if you misclassify `1` for `2`.", "That is true, the further apart the classification indexes the larger the weight of that error would be. In sentiment classification lets say 1 represents \"Hate it!\" and 5 represents \"Love it!\". In this case the squared error might be useful.\r\n\r\nI recently saw https://arxiv.org/abs/1702.05659 which states that loss functions other than the most popular log loss sometimes produce better results for classification asks. It would be nice to have the option to try.", "In the sentiment classification case, it's better to use something like `p(like|x)`, then map from `0-1` to `1-5`. The point is that you want the soft estimate, which is differentiable. It can be made arbitrarily sharp by scaling the logits.\r\n\r\nI will close this since this is not strictly a bug. Feel free to start a discussion on stackoverflow.", "Hey i am facing same type issue \r\nI want to feed calculated loss ( which is self.calc_loss and its value is 1.0288568 )\r\nto \r\n`self.sess.run(tf.compat.v1.train.AdamOptimizer().minimize(d_reward_sum))`\r\nthen it shows error \r\n\r\n> AttributeError: 'numpy.dtype' object has no attribute 'base_dtype'\r\n\r\nIf i use placeholder \r\n\r\n```\r\nself.calc_loss = tf.placeholder(tf.float32) \r\nself.train_opt = tf.compat.v1.train.AdamOptimizer().minimize(self.calc_loss)\r\n...   \r\ntrain_modal = self.sess.run([self.train_opt],feed_dict = {\r\n            self.calc_loss :d_reward_sum ,\r\n        })\r\n```\r\n\r\nthen it give error \r\n\r\n> ([str(v) for _, v in grads_and_vars], loss))\r\n> ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(4, 24) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(24,) dtype=float32_ref>\", \"<tf.Variable 'Variable_2:0' shape=(24, 12) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(12,) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(12, 4) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(4,) dtype=float32_ref>\", \"<tf.Variable 'Variable_6:0' shape=(4, 2) dtype=float32_ref>\"] and loss Tensor(\"Placeholder:0\", dtype=float32).", "I also have stackoverflow question \r\nUnable to get help \r\nWill glad if any one can help \r\nhttps://stackoverflow.com/questions/61174513/tensorflow-custom-loss-feed", "You need to pass tensors. In the first example, it looks like you put a\nnumpy object. This needs to be a tensorflow object, otherwise we won't know\nhow to compute the gradient, because we need to keep track of the\ncomputation that led to it. Same for the second example.\n\nYou need something like this:\n```\nv = tf.Variable([init_value])\n# Example:\nl = tf.reduce_sum((v - 1.) * (v - 1.))\nopt = tf.AdamOptimizer().minimize(l)\n```\n\n\nOn Sun, Apr 12, 2020 at 9:19 AM rajanlagah <notifications@github.com> wrote:\n\n> I also have stackoverflow question\n> Unable to get help\n> Will glad if any one can help\n> https://stackoverflow.com/questions/61174513/tensorflow-custom-loss-feed\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13761#issuecomment-612640412>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AE75E3OM2THDQI3O4XZXGZTRMHSYNANCNFSM4D7NNMNA>\n> .\n>\n"]}, {"number": 13760, "title": "Revert \"Fix broken link in debugger doc\"", "body": "Reverts tensorflow/tensorflow#13757", "comments": []}, {"number": 13759, "title": "Branch 172380659", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 13758, "title": "Fix ambiguous type comparison in s3_crypto.cc", "body": "We were seeing the following compilation error on Windows builds.\r\n\r\ntensorflow/contrib/s3/s3_crypto.cc(74): error C2666:\r\n'std::fpos<_Mbstatet>::operator ==': 3 overloads have similar conversions\r\ncould be 'bool std::fpos<_Mbstatet>::operator ==(std::streamoff) const'\r\nor 'bool std::fpos<_Mbstatet>::operator ==(const std::fpos<_Mbstatet> &)\r\n", "comments": ["Only failure appears to be the batch_matmul_test on windows, which is not a build failure, so merging."]}, {"number": 13757, "title": "Fix broken link in debugger doc", "body": "See https://www.tensorflow.org/programmers_guide/debugger\r\n\r\nI'm not sure if this is the correct syntax to make the link render. But it's my best guess based on examining other files.", "comments": ["Silly me, I didn't know we had build docs tests.  I'll have to revert this CL.\r\n\r\nSkipping tf.contrib.learn.DynamicRnnEstimator.__repr__, defined in <class 'tensorflow.contrib.ERROR:\r\n    output file name: /private/var/tmp/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin_x86_64-opt/bin/tensorflow/tools/docs/build_docs_test.runfiles/org_tensorflow/tensorflow/docs_src/programmers_guide/debugger.md\r\n    Unknown Document \"tflearn\"\r\n\r\n\r\nF\r\n======================================================================\r\nFAIL: testBuildDocs (__main__.BuildDocsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin_x86_64-opt/bin/tensorflow/tools/docs/build_docs_test.runfiles/org_tensorflow/tensorflow/tools/docs/build_docs_test.py\", line 70, in testBuildDocs\r\n    self.fail('Found %s Errors!' % status)\r\nAssertionError: Found 1 Errors!\r\n\r\n\r\n"]}, {"number": 13756, "title": "Does this version support the CUDA 9.0?", "body": "Does this version support the CUDA 9.0?\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Starting with 1.5 TF  will be based on CUDA 9 and cuDNN 7."]}, {"number": 13755, "title": "Allow num parameter in tf.linspace to be int64.", "body": "According to the API, tf.linspace is defined for num int32 or int64.\r\nHowever the C++ kernel only allows int32, even though the op in core/ops/math_ops permits int64 too.\r\nSo I slightly changed the kernel to allow int64 too. I also added tests for RangeOp and LinSpaceOp.\r\n\r\nThe following code shows the issue:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    step = tf.constant(1, tf.int64)\r\n    x = tf.linspace(10.0, 12.0, step)\r\n\r\nwith tf.Session() as sess:\r\n   x_np = sess.run(x)\r\n   print(x_np)\r\n```", "comments": ["Can one of the admins verify this patch?", "Overall looks great, just a minor variable naming nit, and then I can kick off tests :)", "I changed the variable names, I hope it's fine now. Please let me know if there is anything else I should do.", "Great!\r\n\r\n@tensorflow-jenkins test this please"]}, {"number": 13754, "title": "graph_editor uses deprecated API", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: b'v1.3.0-21-gc701d19b2' 1.3.0\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhen I use graph_editor, my console fills up with dozens of repetitions of the following error message:\r\n\r\n```\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n```\r\nThe number of repetitions of the message scales with the number of tensors being copied and can get very large.  To reproduce the problem, run the script below.\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable(1.0)\r\ny = tf.Variable(2.0)\r\na = x+1\r\nreplace = {tf.convert_to_tensor(x):tf.convert_to_tensor(y)}\r\nb = tf.contrib.graph_editor.graph_replace(a, replace)\r\n```\r\n", "comments": ["Work-around\r\n\r\n`setattr(tf.GraphKeys, \"VARIABLES\", \"variables\")`\r\n\r\nThe reason this warning happens is because graph_editor programmatically scans all values in GraphKeys and each time you access tf.GraphKeys.VARIABLES, it issues a warning.\r\n\r\nI think this is more a problem of how deprecation warnings are generated, rather than an issue with graph editor relying on deprecated API", "Thanks again @yaroslavvb!"]}, {"number": 13753, "title": "Fix dev remnants 1.4", "body": "", "comments": ["These all look good, no tests needed since it's just metadata files."]}, {"number": 13752, "title": "Creating a patch for the wrong links that still point to dev.", "body": "", "comments": []}, {"number": 13751, "title": "Parsing TFRecords bug in TensorFlow v1.2", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nI believe there's a bug in TensorFlow v1.2. The code below runs fine in v1.4, while erroring out in v1.2. Here's the code:\r\n```python\r\nfilenames = [\"gs://bucket/file.tfrecords\"]\r\ndataset = tf.contrib.data.TFRecordDataset(filenames)\r\nparse_fn = lambda r: tf.parse_single_example(r, {\"f1\": tf.VarLenFeature(tf.int64)})\r\ndataset.map(parse_fn)\r\n```\r\n\r\nHere's the stacktrace that I get:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-42-0473d32d7931> in <module>()\r\n----> 1 dataset.map(parser)\r\n\r\n/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.pyc in map(self, map_func, num_threads, output_buffer_size)\r\n    811       A `Dataset`.\r\n    812     \"\"\"\r\n--> 813     return MapDataset(self, map_func, num_threads, output_buffer_size)\r\n    814\r\n    815   def flat_map(self, map_func):\r\n\r\n/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.pyc in __init__(self, input_dataset, map_func, num_threads, output_buffer_size)\r\n   1434\r\n   1435     self._map_func = tf_map_func\r\n-> 1436     self._map_func.add_to_graph(ops.get_default_graph())\r\n   1437     if num_threads is not None:\r\n   1438       self._num_threads = ops.convert_to_tensor(\r\n\r\n/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/function.pyc in add_to_graph(self, g)\r\n    617   def add_to_graph(self, g):\r\n    618     \"\"\"Adds this function into the graph g.\"\"\"\r\n--> 619     self._create_definition_if_needed()\r\n    620\r\n    621     # pylint: disable=protected-access\r\n\r\n/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/contrib/data/python/framework/function.pyc in _create_definition_if_needed(self)\r\n    165       # Call func and gather the output tensors.\r\n    166       with vs.variable_scope(\"\", custom_getter=temp_graph.getvar):\r\n--> 167         outputs = self._func(*inputs)\r\n    168       # If func only returned one value, make it a tuple.\r\n    169       if not isinstance(outputs, (list, tuple)):\r\n\r\n/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.pyc in tf_map_func(*args)\r\n   1425\r\n   1426       # Extract shape information from the returned values.\r\n-> 1427       flattened_ret = [ops.convert_to_tensor(t) for t in nest.flatten(ret)]\r\n   1428       self._output_shapes = nest.pack_sequence_as(\r\n   1429           ret, [t.get_shape() for t in flattened_ret])\r\n\r\n/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n    674       name=name,\r\n    675       preferred_dtype=preferred_dtype,\r\n--> 676       as_ref=False)\r\n    677\r\n    678\r\n\r\n/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)\r\n    739\r\n    740         if ret is None:\r\n--> 741           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    742\r\n    743         if ret is NotImplemented:\r\n\r\n/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    111                                          as_ref=False):\r\n    112   _ = as_ref\r\n--> 113   return constant(v, dtype=dtype, name=name)\r\n    114\r\n    115\r\n\r\n/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name, verify_shape)\r\n    100   tensor_value = attr_value_pb2.AttrValue()\r\n    101   tensor_value.tensor.CopyFrom(\r\n--> 102       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    103   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    104   const_tensor = g.create_op(\r\n\r\n/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    460       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\r\n    461                       \"Contents: %s. Consider casting elements to a \"\r\n--> 462                       \"supported type.\" % (type(values), values))\r\n    463     tensor_proto.string_val.extend(str_values)\r\n    464     return tensor_proto\r\n\r\nTypeError: Failed to convert object of type <type 'dict'> to Tensor. Contents: {'f1': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x117918410>}. Consider casting elements to a supported type.\r\n```\r\n\r\nAnd again, this works fine in TensorFlow v1.4.0rc0 that I tested.\r\n\r\nLet me know if I should provide any more info!", "comments": ["The `tf.contrib.data` API in TF 1.2 doesn't support dictionaries as `Dataset` nested structures. This feature was added in TF 1.3. To work around it, you will need to convert the dictionary to a `tuple` or `namedtuple`.", "thanks @mrry! Are you referring to #10151 with your answer? It seems to me as it's not about what the parsing function returns but rather what we pack into `tf.parse_single_example()` e.g.\r\n```python\r\ndataset = tf.contrib.data.TFRecordDataset(filenames)\r\nparse_fn = lambda r: [tf.parse_single_example(r, {\"f1\": tf.VarLenFeature(tf.int64)})]\r\ndataset.map(parse_fn)\r\n```\r\nThis wouldn't work as well.", "Is that not-working in 1.2 or 1.4?", "Fails in v1.2, works in v1.4 @mrry ", "Perhaps it's the list return value that's the problem. (I fixed an issue related to that in d001b58de9d0d99ef34637b9a9bed5763875f655, which is only in the TF 1.4 release.) Can you try changing `parse_fn` to the following?\r\n\r\n```python\r\nparse_fn = lambda r: tuple([tf.parse_single_example(r, {\"f1\": tf.VarLenFeature(tf.int64)})])\r\n```", "also breaks @mrry, the same error. It seems to not matter whether I return a list, tuple or a dict. Can you reproduce it?\r\n\r\nI can also try in TF v1.3 if that would be helpful", "The code doesn't work in `tf-nightly` for me. What code did you run that works in TF 1.4?\r\n\r\nFWIW, the failure in `tf-nightly` is due to the current lack of `tf.SparseTensor` support. See this Stack Overflow answer for some workarounds:\r\n\r\nhttps://stackoverflow.com/a/46732695/3574081", "![](https://preview.ibb.co/iAbRi6/Screen_Shot_2017_10_16_at_23_31_18.png)\r\n\r\n@mrry ", "Can you point me to some guide how to load TFRecords in TF1.2? I imagine it's quite a standard procedure and I don't want to do anything fancy. The ml-engine clusters run only v1.2, so I need to work with this version.", "The 1.4 code is different: it uses a `tf.FixedLenFeature` named `\"mau_d180\"`, whereas the code you're trying in 1.2 uses a `tf.VarLenFeature` named `\"f1\"`. \r\n\r\nThe `FixedLenFeature` code should work in 1.2 if you convert the return type to a tuple. The `VarLenFeature` code won't work in either version until we add `tf.SparseTensor` support.\r\n\r\nFor many common cases, the [`tf.contrib.data.read_batch_features()`](https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/data/read_batch_features) function (available in TF 1.2 and 1.4) should work, even with `tf.SparseTensor`-valued features.", "![](https://preview.ibb.co/ixncqm/Screen_Shot_2017_10_16_at_23_57_14.png)\r\n\r\nIt doesn't seem to work in v1.2. You're right that the code is slightly different - I simplified it for the sake of the example (named the variable `f1` and used `tf.VarLenFeature`), but this shouldn't change the behaviour.\r\n\r\n@mrry - I can check TF v1.3 tomorrow morning, I'm in the European timezone.", "Can you share the whole 1.2 traceback?\r\n\r\nI think you'll need to rewrite `parser()` as follows to work on TF 1.2:\r\n\r\n```python\r\ndef parser(record):\r\n  return tf.parse_single_example(record, {\"f1\": tf.FixedLenFeature([], tf.int64)})[\"f1\"]\r\n```\r\n\r\nIf you have more than one feature, you'll need to extract the values from the dictionary in some consistent order, and create a tuple by hand.\r\n\r\nWhile switching between `tf.FixedLenFeature` and `tf.VarLenFeature` might not seem like a significant difference, it changes the return type from something that `Dataset` currently supports (`tf.Tensor`) to something that it currently doesn't (`tf.SparseTensor`). It'd be great to separate these two issues!", "Make sense! The code that you gave works - I will probably have tens of features, so it might get quite awkward to build all of that by hand.\r\n\r\nFinal question - what's the recommended and suggested way moving forward? Should I not use TFRecords and maybe move to CSV? Or should I use the `read_batch_features()` function that you suggested? My usecase is not special in any way and I also control the serialisation code.\r\n\r\nThanks for the help annyway @mrry! I'll get you a beer if you're around Stockholm at any point", "I'd suggest trying `read_batch_features()`... it'll work in the older version of TF, and make it easier to deal with multiple (potentially sparse) features. It's also got a pretty simple implementation on top of standard `Dataset` transformations, so it's pretty hackable if you end up needing something different (just copy the code to your own project and customize it as necessary).", "Seems to work well, the `read_batch_features()` is what I was looking for. It's nowhere in the docs, if you didn't tell me, I wouldn't find it. Maybe it would not be a bad idea to include it somewhere in the docs/tutorials, so that other people don't go my path.\r\n\r\nCheers!"]}, {"number": 13750, "title": "Improve shape inference with `DecodeAndCropJpeg`", "body": "While working on improving shape inference for several other ops in #13561 and #13193, I noticed that `DecodeAndCropJpeg` does not inference shape even though crop size might have already be provided. In that case the shape will be `[h, w, channel]` and `h`, `w` is part of the `crop_window` in input.\r\n\r\nThis fix updates the shape function in `DecodeAndCropJpeg` for improving shape inference.\r\n\r\nThe test has also been updated to cover the changes.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Hm, looks like some failures:\r\n\r\nhttps://source.cloud.google.com/results/invocation/446601e1-e3c1-4c09-bf89-903051546c3c/%2F%2Ftensorflow%2Fcore:ops_image_ops_test\r\n\r\n(not sure you can see that, but I hope you can).  Can you take a look?  Thanks!\r\n\r\n", "Thanks @vrv for the review. The PR has been updated with tests fixed. Please take a look.", "Cool, lgtm!  cc'ing @mingxingtan just as an FYI.", "LG. Thanks for the fix!", "@tensorflow-jenkins test this please"]}, {"number": 13749, "title": "Add `fast_tensor_util.cpp` to .gitignore", "body": "While working on building TensorFlow I noticed that a file `fast_tensor_util.cpp` is generated:\r\n```sh\r\nubuntu@ubuntu:~/tensorflow$ git status\r\nOn branch master\r\nYour branch is up-to-date with 'origin/master'.\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n\r\n        tensorflow/python/framework/fast_tensor_util.cpp\r\n\r\nnothing added to commit but untracked files present (use \"git add\" to track)\r\nubuntu@ubuntu:~/tensorflow$\r\n```\r\n\r\nThis fix adds `fast_tensor_util.cpp` to .gitignore so that it will not be added inadvertently when adding commit with `git add -A`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13748, "title": "Utility class for outputing/formatting generated source code", "body": "The following class is used in an upcoming PR for generating Java operation wrappers but is generic and useful enough to take place in the core library. This way, any source code generated from C++ can make use of it. ", "comments": ["Can one of the admins verify this patch?", "For now, let's not include this in core but instead include it only with the Java code generator.\r\nIf and when we decide that the C++/Python code generator should use this too, we can move it then.\r\n\r\nSound fair?", "It is ok with me but just to clarify why I thought it has its place in the core:\r\n\r\nI'm already planning to have a JavaWriter which decorates this generic interface with Java specific features. I'd like to keep them in two separated classes, it is just a bit strange having both in the same package (but no big deal here). Also, I wanted to make sure its existence would be noticed by developers of the other clients better than if it's \"hidden\" in the java package.\r\n\r\nThat being said, if we decide to move this in the java package, I guess it would be easier to close this PR and push it with the upcoming Java stuff?", "Thanks for your understanding. Yeah, I think for now let's close this PR and push it with the Java stuff.\r\n\r\n(I feel that any generalization in location or on structure of the code should occur only when needed - or at least when there is someone who is actively interested in integration. Since we don't have that yet for the C++/Python code generators, let's leave it in the Java dir and not worry about more generalization than required. That said, I'm okay with your proposed class structure)\r\n\r\nThanks!", "Understood and agreed, thanks!"]}]