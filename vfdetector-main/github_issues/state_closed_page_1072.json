[{"number": 21115, "title": "#Distributed Tensorflow", "body": "* Overview: I'm struggling  with \"Distributed Tensorflow\" and the error is \"Could not satisfy explicit device specification '/job:worker/task:0' because the node was colocated with a group of nodes that required incompatible device '/job:ps/task:0'\". I also try to research on StackOverflow,.. but it doesn't help much. \r\n\r\nThe code I used to work with \"Distributed Tensorflow\":  https://imgur.com/Wna7Yhz\r\n\r\nThank you so much \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "**System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No \r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\nMobile device (e.g iphone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No \r\nTensorFlow installed from (source or binary): Source\r\nTensorFlow version (use command below):1.8\r\nPython version:3.6.5\r\nBazel version (if compiling from source):No \r\nGCC/Compiler version (if compiling from source): No \r\nCUDA/cuDNN version: No \r\nGPU model and memory: \r\nExact command to reproduce: No ", "same issue here", "@guptapriya could you take a look.", "It seems like you're using the approach described here: https://www.tensorflow.org/deploy/distributed. \r\nUnfortunately, this approach doesn't work with keras fit, as far as I know. cc @anj-s - who can perhaps add more. \r\n\r\nIf you'd like to run your training in keras and distribute on multiple GPUs, you can try using DistributionStrategy instead, which is now supported in keras fit. See example here for now: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/keras_test.py#L244\r\nWe have more documentation coming soon. ", "chinhncse62121@ Thank you filing the issue! As guptapriya@ mentioned we don't have support for distributed training over multiple nodes in Keras using native APIs such as `fit`.  However you can use the `model_to_estimator` function to convert your Keras model to an Estimator instance and then use [`train_and_evaluate`](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate). Let me know if you run into any issues.\r\n\r\nAs mentioned above, if you are looking to train your model on a single machine with multiple GPUs you can use DistributionStrategy with Keras native APIs such as `fit`, `evaluate` and `predict`.\r\n", "Nagging Assignee @guptapriya: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as there is nothing to be done from our side currently (apart from supporting multi node with keras directly which we are actively working on)"]}, {"number": 21114, "title": "Improve shape function for CudnnRNNParamsSize", "body": "In cudnn_rnn_ops.cc, the CudnnRNNParamsSize does not have restrictions on num_layers, num_units, and input_size, though they all should be scalars.\r\n\r\nThis fix adds the shape check of num_layers, num_units, and input_size\r\nfor CudnnRNNParamsSize.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang can you check output of \"Experimental clang-format Check\" and update the formatting according to the errors reported?", "Thanks @annarev for the review. The PR has been updated with clang-format issue fixed. Please take a look.", "Nagging Assignee @caisq: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 35 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21113, "title": "Two improvements to tf.split's shape function", "body": "1. Don't break if the size corresponding to a -1 is 0.\r\n2. Infer the size corresponding to a -1 if possible.\r\n\r\nFixes #19360.", "comments": ["@caisq or @jhseu: Can I get a review?  The failures look unrelated.", "@girving I can take a look at this later today.", "Actually @girving could you push again?", "@drpngx Not before I finish running the tests.", "Pushed.  Apologies for delay.  (Cc @drpngx)", "One more commit to (hopefully) fix clang format errors.", "Thanks @girving! Looks like clang-format passed, but the build status reporting is broken. Let me fix that on the infra side."]}, {"number": 21112, "title": "failed to build unit test for arm64", "body": "I would like to run unit test on arm64, with command,\r\n\r\n\tbazel build --cxxopt=--std=c++11 //tensorflow/core/kernels:quantized_matmul_op_test --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a --verbose_failures\r\n\r\nBut got errors like\r\n\r\n\tERROR: /home/local/SPREADTRUM/ben.shi/git-repo/tensorflow/tensorflow/core/kernels/BUILD:2879:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 1): aarch64-linux-android-gcc failed: error executing command \r\n\t  (cd /home/local/SPREADTRUM/ben.shi/.cache/bazel/_bazel_spreadtrum.com/41de783a40029e841491a00f78b435ef/execroot/org_tensorflow && \\\r\n\t  exec env - \\\r\n\t    PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin:/usr/lib/jvm/java-8-openjdk-amd64/jre/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n\t    PWD=/proc/self/cwd \\\r\n\t    PYTHON_BIN_PATH=/usr/bin/python \\\r\n\t    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n\t    TF_DOWNLOAD_CLANG=0 \\\r\n\t    TF_NEED_CUDA=0 \\\r\n\t    TF_NEED_OPENCL_SYCL=0 \\\r\n\t  external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64/bin/aarch64-linux-android-gcc -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -fno-canonical-system-headers -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_bessel.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_bessel.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' '-DS_IEXEC=S_IXUSR' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/arm64-v8a-opt/genfiles -iquote external/nsync -iquote bazel-out/arm64-v8a-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/arm64-v8a-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm64-v8a-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/arm64-v8a-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm64-v8a-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/arm64-v8a-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/arm64-v8a-opt/genfiles/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/zlib_archive -iquote external/double_conversion -iquote bazel-out/arm64-v8a-opt/genfiles/external/double_conversion -isystem external/nsync/public -isystem bazel-out/arm64-v8a-opt/genfiles/external/nsync/public -isystem bazel-out/arm64-v8a-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/arm64-v8a-opt/genfiles/external/eigen_archive -isystem bazel-out/arm64-v8a-opt/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/arm64-v8a-opt/genfiles/external/gif_archive/lib -isystem bazel-out/arm64-v8a-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/arm64-v8a-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/arm64-v8a-opt/genfiles/external/zlib_archive -isystem bazel-out/arm64-v8a-opt/bin/external/zlib_archive -isystem external/double_conversion -isystem bazel-out/arm64-v8a-opt/genfiles/external/double_conversion -isystem bazel-out/arm64-v8a-opt/bin/external/double_conversion '--std=c++11' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-std=c++11' -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-24/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/arm64-v8a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/core/kernels/cwise_op_bessel.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_bessel.o)\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:387:0,\r\n\t                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n\t                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n\t                 from ./tensorflow/core/kernels/cwise_ops.h:23,\r\n\t                 from ./tensorflow/core/kernels/cwise_ops_common.h:29,\r\n\t                 from tensorflow/core/kernels/cwise_op_bessel.cc:16:\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of 'static Scalar Eigen::internal::i1e_impl<Scalar>::run(Scalar) [with Scalar = Eigen::half]':\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2132:49:   required from 'typename Eigen::internal::i1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::i1e(const Scalar&) [with Scalar = Eigen::half; typename Eigen::internal::i1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = Eigen::half]'\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:323:17:   required from 'const Scalar Eigen::internal::scalar_i1e_op<Scalar>::operator()(const Scalar&) const [with Scalar = Eigen::half]'\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:341:44:   required from 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::CoeffReturnType Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::coeff(Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::Index) const [with UnaryOp = Eigen::internal::scalar_i1e_op<Eigen::half>; ArgType = const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::CoeffReturnType = Eigen::half; Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::Index = long int]'\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:137:28:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalScalar(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::Index) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>; RightArgType = const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_i1e_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer> >; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::Index = long int]'\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:93:7:   required from 'static void Eigen::internal::EvalRange<Evaluator, Index, Vectorizable>::run(Evaluator*, Index, Index) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_i1e_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::ThreadPoolDevice>; Index = long int; bool Vectorizable = false]'\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:157:98:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:154:7:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_i1e_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer> > >; bool Vectorizable = false]'\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:79:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_i1e_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer> >; ExpressionType = Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>; DeviceType = Eigen::ThreadPoolDevice]'\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:277:17:   required from 'void tensorflow::functor::Assign(const D&, Out, Rhs) [with D = Eigen::ThreadPoolDevice; Out = Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>; Rhs = Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_i1e_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer> >]'\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:525:58:   required from 'void tensorflow::functor::UnaryFunctor<Eigen::ThreadPoolDevice, Functor>::operator()(const CPUDevice&, typename Functor::tout_type, typename Functor::tin_type) [with Functor = tensorflow::functor::bessel_i1e<Eigen::half>; tensorflow::functor::CPUDevice = Eigen::ThreadPoolDevice; typename Functor::tout_type = Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>; typename Functor::tin_type = Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>]'\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:250:72:   required from 'void tensorflow::UnaryOp<Device, Functor>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; Functor = tensorflow::functor::bessel_i1e<Eigen::half>]'\r\n\ttensorflow/core/kernels/cwise_op_bessel.cc:29:1:   required from here\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1896:5: error: static assertion failed: THIS_TYPE_IS_NOT_SUPPORTED\r\n\t     EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n\t     ^\r\n\tTarget //tensorflow/core/kernels:quantized_matmul_op_test failed to build\r\n\tINFO: Elapsed time: 860.780s, Critical Path: 51.03s\r\n\tINFO: 483 processes: 483 local.\r\n\tFAILED: Build did NOT complete successfully\r\n\t\r\n\r\n", "comments": ["My platform\r\n\r\nubuntu 16.04,\r\nandroid ndk r12b\r\nandroid sdk r27\r\n\r\n", "I use the newest tensorflow by \"git clone https://github.com/tensorflow/tensorflow\" and the newest bazel 0.15.2", "still failed.\r\n\r\n\tERROR: /home/local/SPREADTRUM/ben.shi/work/tensorflow/tensorflow/core/kernels/BUILD:3010:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 1)\r\n\tIn file included from tensorflow/core/kernels/cwise_op_igammas.cc:16:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops.h:23:\r\n\tIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:31:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:820:5: error: static_assert failed \"THIS_TYPE_IS_NOT_SUPPORTED\"\r\n\t    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n\t    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'\r\n\t    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);\r\n\t                                       ^             ~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2096:47: note: in instantiation of member function 'Eigen::internal::igamma_generic_impl<float, Eigen::internal::IgammaComputationMode::VALUE>::run' requested here\r\n\t  return EIGEN_MATHFUNC_IMPL(igamma, Scalar)::run(a, x);\r\n\t                                              ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:28:34: note: in instantiation of function template specialization 'Eigen::numext::igamma<float>' requested here\r\n\t    using numext::igamma; return igamma(a, x);\r\n\t                                 ^\r\n\t./tensorflow/core/kernels/cwise_ops.h:227:20: note: in instantiation of member function 'Eigen::internal::scalar_igamma_op<float>::operator()' requested here\r\n\t    return Binary::operator()(left, *right);\r\n\t                   ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:341:12: note: in instantiation of member function 'Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igamma_op<float> >::operator()' requested here\r\n\t    return m_functor(m_argImpl.coeff(index));\r\n\t           ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:137:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igamma_op<float> >, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here\r\n\t    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);\r\n\t                                         ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:93:17: note: (skipping 3 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)\r\n\t      evaluator.evalScalar(i);\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:277:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igamma_op<float> >, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t  out.device(d) = rhs;\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:307:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igamma_op<float> >, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t    Assign(d, out, in.unaryExpr(Unary(scalar.data())));\r\n\t    ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:110:54: note: in instantiation of member function 'tensorflow::functor::BinaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::igamma<float>, 1, false>::Right' requested here\r\n\t        functor::BinaryFunctor<Device, Functor, 1>().Right(\r\n\t                                                     ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:87:12: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igamma<float> >::Compute' requested here\r\n\t  explicit BinaryOp(OpKernelConstruction* ctx)\r\n\t           ^\r\n\ttensorflow/core/kernels/cwise_op_igammas.cc:20:11: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igamma<float> >::BinaryOp' requested here\r\n\tREGISTER2(BinaryOp, CPU, \"Igamma\", functor::igamma, float, double);\r\n\t          ^\r\n\tIn file included from tensorflow/core/kernels/cwise_op_igammas.cc:16:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops.h:23:\r\n\tIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:31:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:820:5: error: static_assert failed \"THIS_TYPE_IS_NOT_SUPPORTED\"\r\n\t    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n\t    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'\r\n\t    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);\r\n\t                                       ^             ~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2096:47: note: in instantiation of member function 'Eigen::internal::igamma_generic_impl<double, Eigen::internal::IgammaComputationMode::VALUE>::run' requested here\r\n\t  return EIGEN_MATHFUNC_IMPL(igamma, Scalar)::run(a, x);\r\n\t                                              ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:28:34: note: in instantiation of function template specialization 'Eigen::numext::igamma<double>' requested here\r\n\t    using numext::igamma; return igamma(a, x);\r\n\t                                 ^\r\n\t./tensorflow/core/kernels/cwise_ops.h:227:20: note: in instantiation of member function 'Eigen::internal::scalar_igamma_op<double>::operator()' requested here\r\n\t    return Binary::operator()(left, *right);\r\n\t                   ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:341:12: note: in instantiation of member function 'Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igamma_op<double> >::operator()' requested here\r\n\t    return m_functor(m_argImpl.coeff(index));\r\n\t           ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:137:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igamma_op<double> >, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here\r\n\t    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);\r\n\t                                         ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:93:17: note: (skipping 3 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)\r\n\t      evaluator.evalScalar(i);\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:277:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igamma_op<double> >, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t  out.device(d) = rhs;\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:307:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igamma_op<double> >, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t    Assign(d, out, in.unaryExpr(Unary(scalar.data())));\r\n\t    ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:110:54: note: in instantiation of member function 'tensorflow::functor::BinaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::igamma<double>, 1, false>::Right' requested here\r\n\t        functor::BinaryFunctor<Device, Functor, 1>().Right(\r\n\t                                                     ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:87:12: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igamma<double> >::Compute' requested here\r\n\t  explicit BinaryOp(OpKernelConstruction* ctx)\r\n\t           ^\r\n\ttensorflow/core/kernels/cwise_op_igammas.cc:20:11: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igamma<double> >::BinaryOp' requested here\r\n\tREGISTER2(BinaryOp, CPU, \"Igamma\", functor::igamma, float, double);\r\n\t          ^\r\n\tIn file included from tensorflow/core/kernels/cwise_op_igammas.cc:16:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops.h:23:\r\n\tIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:31:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:820:5: error: static_assert failed \"THIS_TYPE_IS_NOT_SUPPORTED\"\r\n\t    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n\t    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'\r\n\t    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);\r\n\t                                       ^             ~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2102:53: note: in instantiation of member function 'Eigen::internal::igamma_generic_impl<float, Eigen::internal::IgammaComputationMode::DERIVATIVE>::run' requested here\r\n\t  return EIGEN_MATHFUNC_IMPL(igamma_der_a, Scalar)::run(a, x);\r\n\t                                                    ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:55:12: note: in instantiation of function template specialization 'Eigen::numext::igamma_der_a<float>' requested here\r\n\t    return igamma_der_a(a, x);\r\n\t           ^\r\n\t./tensorflow/core/kernels/cwise_ops.h:227:20: note: in instantiation of member function 'Eigen::internal::scalar_igamma_der_a_op<float>::operator()' requested here\r\n\t    return Binary::operator()(left, *right);\r\n\t                   ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:341:12: note: in instantiation of member function 'Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igamma_der_a_op<float> >::operator()' requested here\r\n\t    return m_functor(m_argImpl.coeff(index));\r\n\t           ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:137:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igamma_der_a_op<float> >, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here\r\n\t    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);\r\n\t                                         ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:93:17: note: (skipping 3 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)\r\n\t      evaluator.evalScalar(i);\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:277:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igamma_der_a_op<float> >, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t  out.device(d) = rhs;\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:307:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igamma_der_a_op<float> >, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t    Assign(d, out, in.unaryExpr(Unary(scalar.data())));\r\n\t    ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:110:54: note: in instantiation of member function 'tensorflow::functor::BinaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::igamma_grad_a<float>, 1, false>::Right' requested here\r\n\t        functor::BinaryFunctor<Device, Functor, 1>().Right(\r\n\t                                                     ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:87:12: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igamma_grad_a<float> >::Compute' requested here\r\n\t  explicit BinaryOp(OpKernelConstruction* ctx)\r\n\t           ^\r\n\ttensorflow/core/kernels/cwise_op_igammas.cc:21:11: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igamma_grad_a<float> >::BinaryOp' requested here\r\n\tREGISTER2(BinaryOp, CPU, \"IgammaGradA\", functor::igamma_grad_a, float, double);\r\n\t          ^\r\n\tIn file included from tensorflow/core/kernels/cwise_op_igammas.cc:16:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops.h:23:\r\n\tIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:31:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:820:5: error: static_assert failed \"THIS_TYPE_IS_NOT_SUPPORTED\"\r\n\t    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n\t    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'\r\n\t    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);\r\n\t                                       ^             ~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2102:53: note: in instantiation of member function 'Eigen::internal::igamma_generic_impl<double, Eigen::internal::IgammaComputationMode::DERIVATIVE>::run' requested here\r\n\t  return EIGEN_MATHFUNC_IMPL(igamma_der_a, Scalar)::run(a, x);\r\n\t                                                    ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:55:12: note: in instantiation of function template specialization 'Eigen::numext::igamma_der_a<double>' requested here\r\n\t    return igamma_der_a(a, x);\r\n\t           ^\r\n\t./tensorflow/core/kernels/cwise_ops.h:227:20: note: in instantiation of member function 'Eigen::internal::scalar_igamma_der_a_op<double>::operator()' requested here\r\n\t    return Binary::operator()(left, *right);\r\n\t                   ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:341:12: note: in instantiation of member function 'Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igamma_der_a_op<double> >::operator()' requested here\r\n\t    return m_functor(m_argImpl.coeff(index));\r\n\t           ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:137:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igamma_der_a_op<double> >, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here\r\n\t    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);\r\n\t                                         ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:93:17: note: (skipping 3 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)\r\n\t      evaluator.evalScalar(i);\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:277:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igamma_der_a_op<double> >, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t  out.device(d) = rhs;\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:307:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igamma_der_a_op<double> >, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t    Assign(d, out, in.unaryExpr(Unary(scalar.data())));\r\n\t    ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:110:54: note: in instantiation of member function 'tensorflow::functor::BinaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::igamma_grad_a<double>, 1, false>::Right' requested here\r\n\t        functor::BinaryFunctor<Device, Functor, 1>().Right(\r\n\t                                                     ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:87:12: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igamma_grad_a<double> >::Compute' requested here\r\n\t  explicit BinaryOp(OpKernelConstruction* ctx)\r\n\t           ^\r\n\ttensorflow/core/kernels/cwise_op_igammas.cc:21:11: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igamma_grad_a<double> >::BinaryOp' requested here\r\n\tREGISTER2(BinaryOp, CPU, \"IgammaGradA\", functor::igamma_grad_a, float, double);\r\n\t          ^\r\n\tIn file included from tensorflow/core/kernels/cwise_op_igammas.cc:16:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops.h:23:\r\n\tIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:31:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:721:5: error: static_assert failed \"THIS_TYPE_IS_NOT_SUPPORTED\"\r\n\t    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n\t    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'\r\n\t    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);\r\n\t                                       ^             ~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2114:48: note: in instantiation of member function 'Eigen::internal::igammac_impl<float>::run' requested here\r\n\t  return EIGEN_MATHFUNC_IMPL(igammac, Scalar)::run(a, x);\r\n\t                                               ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:108:35: note: in instantiation of function template specialization 'Eigen::numext::igammac<float>' requested here\r\n\t    using numext::igammac; return igammac(a, x);\r\n\t                                  ^\r\n\t./tensorflow/core/kernels/cwise_ops.h:227:20: note: in instantiation of member function 'Eigen::internal::scalar_igammac_op<float>::operator()' requested here\r\n\t    return Binary::operator()(left, *right);\r\n\t                   ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:341:12: note: in instantiation of member function 'Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igammac_op<float> >::operator()' requested here\r\n\t    return m_functor(m_argImpl.coeff(index));\r\n\t           ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:137:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igammac_op<float> >, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here\r\n\t    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);\r\n\t                                         ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:93:17: note: (skipping 3 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)\r\n\t      evaluator.evalScalar(i);\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:277:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igammac_op<float> >, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t  out.device(d) = rhs;\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:307:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_igammac_op<float> >, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t    Assign(d, out, in.unaryExpr(Unary(scalar.data())));\r\n\t    ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:110:54: note: in instantiation of member function 'tensorflow::functor::BinaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::igammac<float>, 1, false>::Right' requested here\r\n\t        functor::BinaryFunctor<Device, Functor, 1>().Right(\r\n\t                                                     ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:87:12: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igammac<float> >::Compute' requested here\r\n\t  explicit BinaryOp(OpKernelConstruction* ctx)\r\n\t           ^\r\n\ttensorflow/core/kernels/cwise_op_igammas.cc:22:11: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igammac<float> >::BinaryOp' requested here\r\n\tREGISTER2(BinaryOp, CPU, \"Igammac\", functor::igammac, float, double);\r\n\t          ^\r\n\tIn file included from tensorflow/core/kernels/cwise_op_igammas.cc:16:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:\r\n\tIn file included from ./tensorflow/core/kernels/cwise_ops.h:23:\r\n\tIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:31:\r\n\tIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:721:5: error: static_assert failed \"THIS_TYPE_IS_NOT_SUPPORTED\"\r\n\t    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n\t    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'\r\n\t    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);\r\n\t                                       ^             ~\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2114:48: note: in instantiation of member function 'Eigen::internal::igammac_impl<double>::run' requested here\r\n\t  return EIGEN_MATHFUNC_IMPL(igammac, Scalar)::run(a, x);\r\n\t                                               ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:108:35: note: in instantiation of function template specialization 'Eigen::numext::igammac<double>' requested here\r\n\t    using numext::igammac; return igammac(a, x);\r\n\t                                  ^\r\n\t./tensorflow/core/kernels/cwise_ops.h:227:20: note: in instantiation of member function 'Eigen::internal::scalar_igammac_op<double>::operator()' requested here\r\n\t    return Binary::operator()(left, *right);\r\n\t                   ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:341:12: note: in instantiation of member function 'Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igammac_op<double> >::operator()' requested here\r\n\t    return m_functor(m_argImpl.coeff(index));\r\n\t           ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:137:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igammac_op<double> >, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here\r\n\t    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);\r\n\t                                         ^\r\n\texternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:93:17: note: (skipping 3 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)\r\n\t      evaluator.evalScalar(i);\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:277:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igammac_op<double> >, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t  out.device(d) = rhs;\r\n\t                ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:307:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<double, double, Eigen::internal::scalar_igammac_op<double> >, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >' requested here\r\n\t    Assign(d, out, in.unaryExpr(Unary(scalar.data())));\r\n\t    ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:110:54: note: in instantiation of member function 'tensorflow::functor::BinaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::igammac<double>, 1, false>::Right' requested here\r\n\t        functor::BinaryFunctor<Device, Functor, 1>().Right(\r\n\t                                                     ^\r\n\t./tensorflow/core/kernels/cwise_ops_common.h:87:12: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igammac<double> >::Compute' requested here\r\n\t  explicit BinaryOp(OpKernelConstruction* ctx)\r\n\t           ^\r\n\ttensorflow/core/kernels/cwise_op_igammas.cc:22:11: note: in instantiation of member function 'tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::igammac<double> >::BinaryOp' requested here\r\n\tREGISTER2(BinaryOp, CPU, \"Igammac\", functor::igammac, float, double);\r\n\t          ^\r\n\t6 errors generated.\r\n\tTarget //tensorflow/core/kernels:quantized_matmul_op_test failed to build\r\n\tUse --verbose_failures to see the command lines of failed build steps.\r\n\tINFO: Elapsed time: 203.866s, Critical Path: 29.09s\r\n\tINFO: 509 processes: 509 local.\r\n\tFAILED: Build did NOT complete successfully\r\n", "tested with\r\n\r\n\tcommit 6f90abf4a274e700769c927ea8727f59e39cc919\r\n\tMerge: 479fdf5 76f35cb\r\n\tAuthor: TensorFlower Gardener <gardener@tensorflow.org>\r\n\tDate:   Mon Oct 15 19:58:08 2018 -0700\r\n\t\r\n\t    Merge pull request #21456 from Intel-tensorflow:int8-part2\r\n\r\n\t    PiperOrigin-RevId: 217252902\r\n\t", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this still an issue with the latest tensorflow version ? If so, please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21111, "title": "Tensorflow-GPU on Docker freezes after calling tf.Session()", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution** : Centos 7 and Ubuntu 16.04 running Nvidia-docker\r\n- **TensorFlow installed from (source or binary)**: `pip install tensorflow-gpu`\r\n- **TensorFlow version**: all versions from 1.6 to 1.9\r\n- **Python version**: both 2.7 and 3.4\r\n- **CUDA/cuDNN version**: `nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04` (On Centos 9.1/7.1.3, On Ubuntu driver only)\r\n- **Exact command to reproduce**:\r\n`>>import tensorflow as tf`\r\n`>>sess = tf.Session()`\r\n- **Run on native pip**:\r\nI have successfully built tensorflow outside docker and it run properly\r\n- **Bazel version**: N/A\r\n- **GPU model and memory**: GTX 745/ 4GB\r\n- **Mobile device**: N/A\r\n\r\n\r\n### Describe the problem\r\nHi, i am struggling with the problem of using tensorflow inside nvidia-docker. I have successfully installed nvidia-docker and tested with nvidia-smi.\r\nI used the docker image from nvidia, it works on Ubuntu but in Centos, whenever it runs `tf.Session` for both files or intepreter (type in `python` in terminal) it stucks even if I interupted by Control+C, Control+Z. The only way to get out is close the terminal window.\r\nIt seems that there are some problems with stdout.\r\n\r\nSame problems:\r\nhttps://stackoverflow.com/questions/45068455/tensorflow-app-freezes-in-docker-container\r\nhttps://github.com/tensorflow/tensorflow/issues/1947\r\n\r\nThanks in advance!\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nGPU model and memory\nMobile device", "@tensorflowbutler updated already", "IPlease keep in mind that the TensorFlow GitHub Issues page is for tracking bugs and feature requests. It is not a way to escalate a Stack Overflow support question that is not getting enough attention. Thanks!\r\n\r\nIt seems the issue is resolved at Stack Overflow. Can you please close this bug?", "I have the same problem. \r\nI have 2 1070 ti gpu.\r\nubuntu 18\r\nwhen I use tensorflow to check gpus it freeze, no matter I am in docker or I directly run on system.", "I updated the GPU driver and the problem vanished!", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21110, "title": "run with big data breakdown sometimes", "body": "I use .pb file to prediction . if I use sample image is good .  if the I use big data\uff0csometime while breakdown.   I use  tensorflow1.3 1.4 1.5 is OK,  but the version is higher than 1.5 have the problem.The pb is producted with tensorflow 1.4. \r\n\r\nCUB segmented reduce errorinvalid configuration argument\r\n   [[Node: inception_v3/logits/predictions = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](inception_v3/logits/logits/xw_plus_b)]]", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@mengfanyu1991 could you please fill in the form, and give us enough data to be able to reproduce your problem? ", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21109, "title": "make build_pip_package work again on rpi3", "body": "make something like\r\n```\r\nbazel build --config opt --local_resources 1024.0,0.5,0.5 --cxxopt=-mfpu=neon-vfpv4 --cxxopt=-ftree-vectorize --cxxopt=-funsafe-math-optimizations --cxxopt=-ftree-loop-vectorize --cxxopt=-fomit-frame-pointer //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nwork.\r\n\r\nWhen building tensorflow pip package natively on RPI 3 (yes, I know it's slow, not recommended), I saw error message like\r\n```\r\nERROR: /home/pi/work/tensorflow/tensorflow/BUILD:589:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE\r\n\r\n```\r\n\r\nDemangling with `c++filt`\r\n```\r\n$ c++filt _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE \r\nvoid tensorflow::ConcatCPU<tensorflow::bfloat16>(tensorflow::DeviceBase*, std::vector<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> >, std::allocator<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> > > > const&, tensorflow::TTypes<tensorflow::bfloat16, 2, int>::Matrix*)\r\n```\r\nwe can see that we need instantiation for `ConcatCPU<bfloat16>(.....)`.\r\n\r\nTested on Raspbian Jun-2018.", "comments": ["Nagging Assignee @caisq: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looks like there are some presubmit errors, @freedomtan can you take a look?", "@annarev I'll check. Didn't see why there are problems for such one-line change. It seems it's CI's problem.", "@freedomtan, this is the error I see:\r\n```\r\ntensorflow/core/kernels/concat_lib_cpu.cc:69:44: error: duplicate explicit instantiation of 'void tensorflow::ConcatCPU(tensorflow::DeviceBase*, const std::vector<std::unique_ptr<typename tensorflow::TTypes<T, 2>::ConstMatrix> >&, typename tensorflow::TTypes<T, 2>::Matrix*) [with T = tensorflow::bfloat16; typename tensorflow::TTypes<T, 2>::ConstMatrix = Eigen::TensorMap<Eigen::Tensor<const tensorflow::bfloat16, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T, 2>::Matrix = Eigen::TensorMap<Eigen::Tensor<tensorflow::bfloat16, 2, 1, long int>, 16, Eigen::MakePointer>]' [-fpermissive]\r\n       typename TTypes<T, 2>::Matrix* output);\r\n                                            ^\r\n```\r\nThis is certainly caused by this PR.\r\nPlease take a look.", "@gunan Thanks. I'll take a look and build on other platforms. It seems this instantiation problem is RPI 3 specific. If that's the case, I'll add `#ifdef`", "@annarev and @gargn It seems I was trying to solve the wrong problem. Yes, `#ifdef something` will help build on both RPI 3 and other platforms. But adding something like `--copt=-DRASPBERRY_PI` will be able to build pip package w/o problem. Will verify this on RPI 3, if that's true I'll close this PR later. Building TensorFlow natively on RPI 3 is painfully slow.", "@petewarden can correct me if I am wrong, but we are already cross-compiling our releases and deploying them on piwheels.", "@gunan Yes, there are pip packages for python 2.7 and 3.4. However, the last time I tried to use TF Lite Python binding, I ran into problem. That's why I want to try build it natively.\r\n\r\n```\r\npython3 ./label_image.py \r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 56, got 52\r\n  return f(*args, **kwds)\r\n/tmp/mobilenet_v1_1.0_224_quant.tflite\r\nTraceback (most recent call last):\r\n  File \"/tmp/label_image.py\", line 64, in <module>\r\n    interpreter = interpreter_wrapper.Interpreter(model_path=model_file)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter.py\", line 50, in __init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.5/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 673, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 693, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKfiiS2_iPfi\r\n```\r\n\r\n```\r\nc++filt _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKfiiS2_iPfi\r\ntflite::tensor_utils::NeonMatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int)\r\n```\r\n\r\nSomething NEON related probably was wrong when cross-building.", "bad news and good news.\r\n\r\n- bad news: adding `--copt=-DRASPBERRY_PI` is not enough. Still saw the same error message.\r\n- good news: `--copt=-DRASPBERRY_PI --host_copt=-DRASPBERRY_PI` works.\r\n\r\nWith \r\n```\r\nbazel build --config opt --local_resources 1024.0,0.5,0.5 \\\r\n--copt=-mfpu=neon-vfpv4 \\\r\n--copt=-ftree-vectorize \\\r\n--copt=-funsafe-math-optimizations \\\r\n--copt=-ftree-loop-vectorize \\\r\n--copt=-fomit-frame-pointer \\\r\n--copt=-DRASPBERRY_PI \\\r\n--host_copt=-DRASPBERRY_PI \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n```\r\nI got a good pip package. I'll close this.\r\n"]}, {"number": 21108, "title": "Higher iterations_per_loop values in TPU training lead to NaN gradients!", "body": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud Platform (Linux Debian)\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):Binary\r\nTensorFlow version (use command below):1.9\r\nPython version: 2.7\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: NA (TPU)\r\nGPU model and memory: NA (TPU)\r\nExact command to reproduce:\r\n\r\n### Describe the problem\r\nI am using TPU and my model trains properly with `tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=20)` but when I increase `iterations_per_loop` to say 50 or 100, the gradients becomes NaN during the training (see the error below). I consistently see this behavior with higher values of `iterations_per_loop` while lower values (e.g <20) never lead to NaN gradients. \r\n\r\nI tried to set `num_shards=1` but the problem is the same.\r\n\r\n \r\n### Source code / logs\r\n```\r\nInvalidArgumentError (see above for traceback): Gradient for ic_enc/ic_enc_2_ics/logvar/b:0 is NaN : Tensor had NaN values\r\n\t [[Node: CheckNumerics_11 = CheckNumerics[T=DT_FLOAT, message=\"Gradient for ic_enc/ic_enc_2_ics/logvar/b:0 is NaN\", _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](Read_13/ReadVariableOp)]]\r\n```\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new), including code that can replicate the bug. Thanks!\r\n"]}, {"number": 21107, "title": "Updating to bazel 0.15.0.", "body": "", "comments": []}, {"number": 21106, "title": "1.10.0-rc1 cherry-pick request: Update tensorboard dependency to 1.10.x", "body": "TensorBoard 1.10.0 has been released to PyPI: https://pypi.org/project/tensorboard/1.10.0/\r\n\r\nPiperOrigin-RevId: 205895470", "comments": []}, {"number": 21105, "title": "Remove functions from TFLite public Python API.", "body": "", "comments": []}, {"number": 21104, "title": "Feature Request: 5D rot90 (for voxel grid rotations)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Amazon Deep Learning AMI w/ TF 1.9\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Pip\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9\r\n- **GPU model and memory**: K80\r\n- **Exact command to reproduce**: N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIt would be great to have a 5D [batch, height, width, depth, channels] version of tf.image.rot90. This is useful for data augmentation when using voxel grids. I suspect that this would be an extension of the existing rot90.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I have started working on this", "I have implemented this feature in pull request [#21928](https://github.com/tensorflow/tensorflow/pull/21928). \r\n\r\nThis is my first contribution to the tensorflow project, so any feedback would be appreciated!", "I understand that this was resolved by #21928. If you think I am mistaken, please feel free to open again. Thanks!"]}, {"number": 21103, "title": "tf.gradients should return an error if the xs list is empty", "body": "You can currently call tf.gradients(ys, xs) where xs is an empty list. The returned result is None. I feel like it should return an error, though, to keep in line with the definition of the gradient -- dx/dy is undefined if x is undefined. \r\n\r\nExample code:\r\n\r\n    a = tf.Variable([100])\r\n    b = tf.reduce_sum(a)\r\n    grads = tf.gradients(b, [])\r\n    with tf.Session() as sess:\r\n        print(sess.run(grads))\r\n\r\n`None`\r\n\r\nThis is somewhat related to issue #783 (Gradient computation erroneously returns None), at least in the sense that the tf.gradients implementation deviates from the mathematical definition. I also understand that there may be downstream complications that make this unreasonable to implement. I looked around for an issue regarding this but couldn't find one... sorry if I missed anything!\r\n\r\nEDIT: filling out template\r\nPlatform - Ubuntu 16.04\r\nTensorflow installed with pip3 in venv\r\nTensorflow version 1.8.0\r\nN/A for the rest (they don't really apply to the question)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I've updated my question regarding the information. See edits at the bottom of my original post.", "Nagging Assignee @robieta: It has been 110 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21102, "title": "[INTEL MKL] Using MKL DNN sgemm", "body": "Added code to call MKL DNN sgemm when building from MKL DNN opensources only.  This PR depends on https://github.com/tensorflow/tensorflow/pull/21011, so it should be merged only after 21011 is merged", "comments": ["@rmlarsen It looks like there are some unit tests that specifically require Eigen. I think a change was made by someone at Google to add the Eigen registrations. https://github.com/tensorflow/tensorflow/commit/d8697935d334bb0f2e1c9bccfe9a2a7bee9785cc . For MKL accelerated matmuls we define Eigen version as well default cpu which uses MKL.", "@agramesh1 Sounds good. I'll try to remember why that is on my own time ;-)"]}, {"number": 21101, "title": "1.10.0-rc1 cherry-pick request: Expose proto serialization publicly", "body": "This change is required to ensure TF Serving 1.10.0-rcX builds clean.\r\nTF Serving 1.10.0 is based of TF 1.10.0 release.\r\n\r\nMore details from the original change is as follows:\r\n===\r\nExpose proto serialization publicly, to avoid code duplication in tensorflow_serving.\r\n\r\nPiperOrigin-RevId: 205788702", "comments": []}, {"number": 21100, "title": "Let TrtEngineOp fallback to tf function call on failures", "body": "Let TrtEngineOp fallback to native tf function call for most of the failure cases, and fix a bug where it sets the status of the context but still execute the op.", "comments": []}, {"number": 21099, "title": "Tensorflow Optimizer cannot optimize gradients when they are the output of tf.cond", "body": "I have the following code in tensorflow where I cam trying to compute the gradients in 2 ways:\r\n\r\n    def optimize(loss):\r\n        \r\n        with tf.name_scope('Optimizer'):\r\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        \r\n            with tf.control_dependencies(update_ops):\r\n                optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n\t\t\t\r\n                def prev_grads_are_not_None():\r\n                \r\n                    ....... (Not important for now)....\r\n                    return grads_and_vars, grads_wrt_initial_state\r\n            \r\n                # This is executed when feeding the first batch only. => there is no previously collected gradients.\r\n                def prev_grads_are_None():\r\n                \r\n                    # This will compute the local gradient given the loss function (taking into account all trainable variables)\r\n                    local_grads = tf.gradients(loss, tf.trainable_variables())\r\n                    grads_wrt_initial_state = tf.gradients(loss, initial_state, grad_ys=None)\r\n                \r\n                    grads_and_vars = list(zip(local_grads, tf.trainable_variables()))\r\n                    return grads_and_vars, grads_wrt_initial_state\r\n            \r\n                grads_and_vars, grads_wrt_initial_state = tf.cond(this_is_last_batch, lambda: prev_grads_are_None(), lambda: prev_grads_are_None())\r\n            \r\n                train_step = optimizer.apply_gradients(grads_and_vars)            \r\n                return train_step, grads_wrt_initial_state\r\nPlease note that I have used the same methods 2 times on purpose just for testing tf.cond.\r\nand this will throw the following error:\r\n\r\n    ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in update_op(self, optimizer, g)\r\n        199 \r\n        200   def update_op(self, optimizer, g):\r\n    --> 201     raise NotImplementedError(\"Trying to update a Tensor \", self._v)\r\n        202 \r\n        203 \r\n\r\n    NotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'Optimizer/cond/Merge_1:0' shape=(264, 128) dtype=float32>)\r\n\r\nso what is the reason for this bug?\r\nPlease note that I am trying to train a 2 layer GRU.\r\n\r\nI have tried the following code as well:\r\n\r\ndef optimize(loss):\r\n        \r\n    with tf.name_scope('Optimizer'):\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        \r\n        with tf.control_dependencies(update_ops):\r\n            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n            \r\n            # When training we start from the end and proceed back ward. while we are in the middle of the training\r\n            # then we will have previously collected gradients to be used. \r\n            def prev_grads_are_not_None():\r\n                \r\n                # ..... Not needed for now .....\r\n                return grads, grads_wrt_initial_state\r\n            \r\n            # This is executed when feeding the first batch only. => there is no previously collected gradients.\r\n            def prev_grads_are_None():\r\n                \r\n                # This will compute the local gradient given the loss function (taking into account all trainable variables)\r\n                local_grads_ = tf.gradients(loss, tf.trainable_variables())\r\n                grads_wrt_initial_state_ = tf.gradients(loss, initial_state)\r\n                \r\n                local_grads = [g for g in local_grads_]\r\n                grads_wrt_initial_state = [g for g in grads_wrt_initial_state_]\r\n                \r\n                print(\"---<<<\")\r\n                for h in local_grads:\r\n                    print(h)\r\n                    \r\n                return local_grads, grads_wrt_initial_state\r\n            \r\n            grads_r, grads_wrt_initial_state = tf.cond(this_is_last_batch, prev_grads_are_None, prev_grads_are_None)\r\n            \r\n            print(\"---->>\")\r\n            for r in grads_r:\r\n                print(r)\r\n            \r\n            grads_and_vars = list(zip(grads_r, tf.trainable_variables()))\r\n\r\n            train_step = optimizer.apply_gradients(grads_and_vars)            \r\n            return train_step, grads_wrt_initial_state\r\n\r\nBut that will run and then stall, but showing kernel is busy in jupyter notebook. \r\n\r\nThe printed tensors shows:\r\n\r\n\t---<<<\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/MatMul/Enter_grad/b_acc_3:0\", shape=(264, 128), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/BiasAdd/Enter_grad/b_acc_3:0\", shape=(128,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/MatMul_1/Enter_grad/b_acc_3:0\", shape=(264, 64), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0\", shape=(64,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/MatMul/Enter_grad/b_acc_3:0\", shape=(96, 64), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/BiasAdd/Enter_grad/b_acc_3:0\", shape=(64,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/MatMul_1/Enter_grad/b_acc_3:0\", shape=(96, 32), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0\", shape=(32,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/output_layer/MatMul_grad/MatMul_1:0\", shape=(32, 3), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/output_layer/add_grad/Reshape_1:0\", shape=(3,), dtype=float32)\r\n\t---->>\r\n\tTensor(\"Optimizer/case/cond/Merge:0\", shape=(264, 128), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_1:0\", shape=(128,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_2:0\", shape=(264, 64), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_3:0\", shape=(64,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_4:0\", shape=(96, 64), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_5:0\", shape=(64,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_6:0\", shape=(96, 32), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_7:0\", shape=(32,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_8:0\", shape=(32, 3), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_9:0\", shape=(3,), dtype=float32)\r\n\r\nTherefore, is the difference in the type of the tensor considered the reason for this problem?\r\nAny help is much appreciated!! \r\n\r\n**System Information**\r\n\r\n- Python version: 3.6\r\n- My tensorflow version is 1.8\r\n- CUDA: 7/ cuDNN: 9\r\n- GPU Model: GeForce 1080 Ti/ Memory: 12 GB\r\n- tensorflow installed from binaries \r\n- OS: windows 10.\r\n- Have I written custom code: N/A\r\n- Bazel Version: N/A\r\n- Mobile Device N/A\r\n- Exact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "The problem here is that the vars are also coming from the cond, not just the grads. Make the cond return the grads and do the zip with variables outside the cond and you should be fine.\r\n\r\nAlternatively, if the actual set of variables to be optimized is conditional, you can call apply_gradients inside the cond and return the train op from both branches.", "Hello alextp; \r\n\r\nI guess `tf.gradients` will return only the gradients of y w.r.t x; so what vars do you mean in your answer? `optimizer.compute_gradients()` will return the gradients and the variables; but not `tf.gradients` Please let me know what you think. \r\n\r\nThank you", "Right; so don't use optimizer.compute_gradients in a cond, use tf.gradients directly instead."]}, {"number": 21098, "title": "Updating the version string to rc1.", "body": "", "comments": []}, {"number": 21097, "title": "Update re2 library to 2018-07-01", "body": "This fix updates re2 library from 2018-04-01\r\nto the latest 2018-07-01.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Could you take a look at the android build failure? Thanks!", "Thanks @yifeif. The PR has been updated. The issue was that new version of re2 has not been propagated to `mirror.bazel.build` yet. I temporarily points to GitHub to fix the build failure. Once `mirror.bazel.build` is propagated I will create a follow up PR to update.", "@yongtang do you mind pull rebase and push again?", "Thanks @drpngx. The PR has been rebased with merge conflict resolved. Please take a look and let me know if there are any other issues."]}, {"number": 21096, "title": "documentation request: Prebuilt binary build configuration", "body": "There are many flags/options for tensorflow build, some of them are performance related.\r\nUser may want to build a whl from source (with some modifications) , using **exactly the same** build configuration that prebuilt binary used\r\n\r\nWith previous Jenkins CI server, we can still figure out the build configuration from log of a release-xxxx build job.\r\nBut now with the new internal build system, it is not quite clear what build configuration is used for prebuilt binary anymore.\r\ne.g. \r\ncuda compute capability: default value in configure.py (3.5,7.0) seems not complete.\r\ncpu simd option: one may have to search release note to figure out whether it is still AVX.\r\n\r\nIt will be great to document the build setting somewhere, or introduce some script to produce prebuilt binary for a release.\r\nThanks.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@gunan can you shed any light here, please?\r\n@drpngx was also curious. ", "@av8ramit Do we have these options documented?\r\nI think at the moment, in configure you need to just pick the deafult options.\r\nThe only non-defaults we may have are CPU optimization levels, which is fixed to avx, and CUDA, which is selected and unselected depending on CPU/GPU builds.", "No we don't but that is a good point. I can add this information to the build from sources page if you'd like.", "I'm working on an update to this page which has a section for [configuration options](https://github.com/lamberta/docs/blob/install/site/en/install/source.md#configuration-options).\r\n\r\nWe can add some more after this lands. If this list becomes unwieldy for the doc page, maybe we can just put it in a file in the tensorflow repo and link to it.", "The updated *build from source* page is up, here's a link to the section: https://www.tensorflow.org/install/source#configure_the_build\r\nTo update, it's here: https://github.com/tensorflow/docs/blob/master/site/en/install/source.md", "Thank you @lamberta ! To be precise, I think we're looking for the exact scripts that are used to generate the images, docker etc.", "Thanks for the update, @lamberta \r\n\r\nAgree with @drpngx, it would be great to share the exact script which is used to produce the pre-built binary.\r\n\r\nE.g. from https://github.com/tensorflow/docs/blob/master/site/en/install/source.md, I still have no idea what CUDA compute capability is used for pre-built binary.\r\n", "Except for windows, we bake all supported CUDA Compute Capabilities into our binary.\r\nThe reason for windows not having this is a bug in eigen we are waiting a fix on.\r\n\r\nSince our strategy is to include all compute capabilities, they have not been documented.", "Sharing the script sounds like a fine idea, but then this PR is no longer a documentation request :)\r\n", "Status update: Still working on this. ", "Hey @angersson I'm assigning this over to you. It's possible you may make our build scripts opensource in your work this quarter. Unfortunately, I never got around to this.\r\n\r\nWe did make the [pip_new.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/builds/pip_new.sh) the standard for building packages so maybe these are some temporary answers.", "So all our build scripts are now externally available. Closing this now.\r\n[Presubmit example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/presubmit)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21096\">No</a>\n"]}, {"number": 21095, "title": "TFRecord documentation is still lacking", "body": "1. It must be a bug, a feature request, or **a significant problem with documentation** (for small docs fixes please send a PR instead).\r\n------------------------\r\n\r\n### System information (N/A)\r\n\r\n### Describe the problem\r\n#1749 was closed with the note that \"We revamped our docs. Feel free to open a new issue if this is still missing from our new docs.\" \r\n\r\nDespite being the \"recommended format\" for input data, I still can't find any official documentation on creating TFRecords, and what the best practices are surrounding this task.\r\n\r\nIt seems strange that blogs and the source code are the only sources of information on this. I think it would be a good idea to have an official guide showing how to go from a directory of images to some TFRecords. Maybe using [this script from tensorflow/models](https://github.com/tensorflow/models/blob/master/research/inception/inception/data/build_imagenet_data.py) as an example.\r\n\r\n### Source code / logs (N/A)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "There's a preliminary tutorial here:\r\n\r\nhttps://github.com/tensorflow/docs/pull/188", "I understand that this was resolved with the recent tutorials. I am closing this issue. Please open a new ticket if you see similar issue. Thanks!\r\n", "This is finally published:\r\n\r\nhttps://www.tensorflow.org/tutorials/load_data/tf_records\r\n"]}, {"number": 21094, "title": "libnccl-dev is not installed in Dockerfile.gpu", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master branch\r\n- **Python version**: not related\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: not related\r\n- **CUDA/cuDNN version**: CUDA9, cuDNN7\r\n- **GPU model and memory**: not related\r\n- **Exact command to reproduce**:\r\ntensorflow/tools/ci_build/ci_build.sh GPU bazel test //tensorflow/...\r\n\r\n\r\n### Describe the problem\r\nwhen build from source inside docker container, configure.py will fail with error message below due to nccl.h is missing.\r\n\r\n```\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nTraceback (most recent call last):\r\n  File \"configure.py\", line 1586, in <module>\r\n    main()\r\n  File \"configure.py\", line 1525, in main\r\n    set_tf_nccl_install_path(environ_cp)\r\n  File \"configure.py\", line 1156, in set_tf_nccl_install_path\r\n    _DEFAULT_PROMPT_ASK_ATTEMPTS)\r\n__main__.UserInputError: Invalid TF_NCCL setting was provided 10 times in a row. Assuming to be a scripting mistake.\r\n```\r\n\r\n\r\n### Source code / logs\r\nhttps://github.com/tensorflow/tensorflow/blob/226831aab92a395a26824a08caa9d43f0c3d604e/tensorflow/tools/ci_build/Dockerfile.gpu#L18-L20\r\nlibnccl-dev is not installed, so nccl.h is missing.", "comments": ["it is strange that recent GPU related CI checks on master branch can still pass without libnccl-dev installed. \r\ne.g. https://source.cloud.google.com/results/invocations/ae1f1de9-f91b-4138-b89d-d56b3dfebb80\r\n\r\n@gunan, does current CI check still use ci_build.sh to build inside a docker container? \r\n", "No, current CI runs outside docker, because we still try to support ubuntu 14, and there is no nvidia docker containers for ubuntu 14.\r\nWe will also soon remove all docker containers under `tools/ci_build`\r\nCould you try using the devel-gpu container here?\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker", "@gunan, thanks for the response.\r\n \r\nTo confirm, `We will also soon remove all docker containers under tools/ci_build`, do you mean stop official support for build inside docker container? \r\nwhen will this happen?\r\n\r\nI believe this is a useful feature to build inside docker. \r\nFor example, our CI build machines are shared with other team, so we prefer to build inside docker for isolation. \r\nMaybe we should still keep it, and leave the maintenance work to the community?\r\n\r\n\r\n\r\n`tools/docker` is not quite friendly for ci build and release scenario. \r\nAlso, it lacks of support for multiple python versions, e.g. build whl for python 3.5, python 3.6.\r\n", "Multiple python versions was something we have not considered, I will take that into account when moving forward. In our dockerfiles, we certainly want to default to latest python versions moving forward.\r\nHowever, I think we can parameterize python versions on the dockerfiles so it is possible to run our CI in them.\r\nHaving the community maintain these dockerfiles is also another viable option.", "Just submitted a PR https://github.com/tensorflow/tensorflow/pull/21127\r\n@gunan could you or someone help to take a look please?\r\nThanks", "I saw @tfboyd already submitted a fix https://github.com/tensorflow/tensorflow/commit/402bb7353289013520fa99a38559319bc7b3d845, so I closed my previous PR.\r\n\r\nThen when I pull latest cuda image, I realized ci_build.sh needs an option to allow something like `docker build --pull`, so I opened another PR https://github.com/tensorflow/tensorflow/pull/21147\r\nThanks.\r\n"]}, {"number": 21093, "title": "tf.metrics.*_at_thresholds inconsistent with other metrics which return a single value", "body": "when I try to solve a binary classification problem with a custom tf.Estimator,\r\nI made a model_fn refer to https://www.tensorflow.org/guide/custom_estimators,\r\nbut I want add 2 eval metrics like this:\r\n...\r\ntn = tf.metrics.true_positives(labels, predictions=predicted_classes)\r\ntp_5 = tf.metrics.true_positives_at_thresholds(labels, predicted_classes, [0.2, 0.4, 0.6, 0.8, 1.0])\r\nmetrics = {'tn': tn, 'tp_5': tp_5}\r\nif mode == tf.estimator.ModeKeys.EVAL:\r\n  return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\r\n...\r\n\r\nIt will report TypeError: eval_metric_ops[tp_5] must be Operation or Tensor, given: <tf.Variable 'true_positives/true_positives:0' shape=(5,) dtype=float32_ref>\r\n\r\na workaround \r\nmetrics = {'tn': tn, 'tp_5': tuple([tf.convert_to_tensor(tp_5[0]), tp_5[1]])}\r\nwill as expected.\r\n\r\nIs this API design intentional?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21092, "title": "[Java] Improve creation of constant tensors", "body": "Allow simple creation of scalar values using the `Constant` operator. ex: `ops.constant(0.0f);` \r\n\r\nAlso, added a new operator `Zeros` that allows the creation of constant tensors initialized to zero w.r.t. its type and shape. ex: `ops.zeros(Float.class, Shape.make(10, 10));`", "comments": ["@asimshankar : updated", "@asimshankar : updated, sorry, completely forgot about the notices"]}, {"number": 21091, "title": "iOS session create crash ", "body": "Hi there,\r\nI follow the iOS example \"simple\" to build an iOS cocoa touch static library.\r\nUsing camera to detect something and square it to image, then output feature data array. \r\nIt seems crash when ''session creating'' sometimes and I don't know how to fix it.\r\nhere is the link about the static library:([click me](https://github.com/AlysonQ/GetFaceFeatureData))\r\nIs that possible crash because of sending images too fast?\r\n<img width=\"1680\" alt=\"2018-07-24 5 48 47\" src=\"https://user-images.githubusercontent.com/9098498/43138691-4571cabe-8f82-11e8-9331-4792f47ef8ce.png\">\r\n\r\nI tried #7108 and #2927 before, don't work for me.\r\n\r\nSorry for missing some information as below.\r\nHave I written custom code: yes\r\nOS Platform and Distribution: xcode 9.4.1 \r\nTensorFlow installed from : TensorFlow static library build from source.\r\nTensorFlow version: 1.08\r\nBazel version: 0.14.1-homebrew\r\nCUDA/cuDNN version : none\r\nGPU model and memory : none\r\nExact command to reproduce : ([click me](https://github.com/AlysonQ/GetFaceFeatureData))\r\nMobile device: iPhone8", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Sorry for missing some information as below.\r\nHave I written custom code: yes\r\nOS Platform and Distribution: xcode 9.4.1\r\nTensorFlow installed from : TensorFlow static library build from source.\r\nTensorFlow version: 1.08\r\nBazel version: 0.14.1-homebrew\r\nCUDA/cuDNN version : none\r\nGPU model and memory : none\r\nExact command to reproduce : (click me)\r\nMobile device: iPhone8", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Nagging Assignee @poxvoculi: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21090, "title": "Is there a benchmark that can be used on GPU?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 21089, "title": "[Building Error] No rule to make target", "body": "+ set -e\r\n+++ dirname tensorflow/contrib/lite/build_ios_universal_lib.sh\r\n++ cd tensorflow/contrib/lite\r\n++ pwd\r\n+ SCRIPT_DIR=/Users/ctr-daben/desktop/tensorflow-master/tensorflow/contrib/lite\r\n+ cd /Users/ctr-daben/desktop/tensorflow-master/tensorflow/contrib/lite/../../..\r\n+ make_library libtensorflow-lite.a\r\n+ for arch in x86_64 armv7 armv7s arm64\r\n+ make -f tensorflow/contrib/lite/Makefile TARGET=IOS IOS_ARCH=x86_64 -j 8 /Users/ctr-daben/desktop/tensorflow-master/tensorflow/contrib/lite/gen/lib/ios_x86_64/libtensorflow-lite.a\r\n/Users/ctr-daben/Desktop/tensorflow-master/tensorflow/contrib/lite/gen/bin/ios_x86_64/benchmark_model\r\nmake: *** No rule to make target `/Users/ctr-daben/desktop/tensorflow-master/tensorflow/contrib/lite/gen/lib/ios_x86_64/libtensorflow-lite.a'.  Stop.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code    N\r\nOS Platform and Distribution   macOS High Sierra 10.13.4 (17E202)\r\nTensorFlow installed from   github\r\nTensorFlow version   tensorflow-master\r\nBazel version   0.15.2\r\nCUDA/cuDNN version    N\r\nGPU model and memory N\r\nExact command to reproduce  tensorflow/contrib/lite/build_ios_universal_lib.sh\r\nMobile device  iOS", "I tried this in master branch and it worked with no problem. (commit 81d927bfc)\r\nCould you provide more information? (The full console log, git commit hash)\r\nThanks!", "meet the same error."]}, {"number": 21088, "title": "3*cycle_length readers are created when using tf.data.Dataset? ", "body": "Hello, I am creating a tensorflow dataset using the following code.\r\n```\r\nfiles = 'hdfs://default/testset/*'\r\nds = tf.data.Dataset.list_files(files)\r\nds = ds.apply(interleave_ops.parallel_interleave(tf.data.TextLineDataset, cycle_length=1))\r\nds = ds.shuffle(buffer_size=10000)\r\nds = ds.map(parse_line)\r\nds = ds.repeat()\r\nds = ds.batch(128)\r\nds = ds.prefetch(1)\r\niterator = ds.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n```\r\nSince the first line of each file is an empty line, I can notice it read 3*cycle_length(here cycle_length=1) files at the beginning from the the following output. After that, it still read files in order(not recurrently).\r\n```\r\n18/07/24 11:07:44 WARN hdfs.DFSClient: zero\r\n18/07/24 11:07:44 WARN hdfs.DFSClient: zero\r\n18/07/24 11:07:44 WARN hdfs.DFSClient: zero\r\n```\r\nI could not find a argument whose default value is 3. Does anyone know why it is 3? Thank you.", "comments": ["The default value of `parallel_interleave()`'s `prefetch_input_elements` argument is 2. Thus when the `cycle_length` is 1, there will be 1 + 2 = 3 files opened at the start of execution. I suspect it is not 3 * `cycle_length` for other values of `cycle_length`.\r\n\r\nIf you want to avoid this behavior, set `prefetch_input_elements` to 0.", "Hi, @mrry, thank you for your response. \r\nYou are right that `prefetch_input_elements` is the key. But for the default setting, it did read  3*`cycle_length` even for other values of `cycle_length`, and if I give a value to `prefetch_input_elements`, it reads `prefetch_input_elements+cycle_length` files.  "]}, {"number": 21087, "title": "Replace `NWHC` with `NHWC`", "body": "This PR replaces `NWHC` with `NHWC`. The `array_ops.transpose(x, [0, 3, 1, 2])` performs `NHWC` to `NCHW`.", "comments": ["cc @fchollet "]}, {"number": 21086, "title": "Fix typos", "body": "This PR fixes some typos: `not not`, `fuction`, `constrast`, `be be`, `is is`, `by by`, `Arithmatic`, `of of`, `Parition`, `Instatiate`, `orignal`, `to to`, and `mutliple`.", "comments": []}]