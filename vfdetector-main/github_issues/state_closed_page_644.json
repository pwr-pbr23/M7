[{"number": 34298, "title": "could not restore weights to model with same structure  when enable_eager", "body": "Sytem: ubuntu18.04\r\nTF version: 2.0\r\nHardware: gtx1080ti and gt 840m\r\nCUDA: 10.0\r\nCUDNN: 7.6\r\n\r\n```\r\nclass RetinaNet(tf.keras.Model):\r\n    def __init__(self, config):\r\n        super(RetinaNet, self).__init__()\r\n        self.config = config\r\n        self.num_classes = config['num_classes']\r\n        self.weight_decay = config['weight_decay']\r\n        self.mode = config['mode']\r\n        self.batch_size = config['batch_size'] if config['mode'] == 'train' else 1\r\n        self.lr = config['lr']\r\n\r\n        image = tf.keras.Input(shape=[None, None, 3], batch_size=self.batch_size, dtype=tf.float32)\r\n        self.backone = backone.model[config['backone']](\r\n            image, config['backone_conv_trainable'], config['backone_bn_trainable'],\r\n            weight=backone.weight[config['backone']]\r\n        )\r\n        self.opt = tf.keras.optimizers.SGD(self.lr, momentum=0.9)\r\n\r\n        if config['mode'] == 'train':\r\n            gt = tf.keras.Input(shape=[None, 5], batch_size=self.batch_size, dtype=tf.float32)\r\n            self.model = self._build_graph(image, gt)\r\n        else:\r\n            self.model = self._build_graph(image)\r\n\r\n    def _build_graph(self, image, gt=None):\r\n        num_fpn_layers = 5\r\n        fpn_channels = 256\r\n        endpoints = self.backone(image)\r\n        fpn = fpn_generator(endpoints[1:], fpn_channels, num_fpn_layers, mode='dconv')\r\n        p3, p4, p5, p6, p7 = fpn\r\n        dw_rate = [8., 16., 32., 64., 128.]\r\n        anchors = [\r\n            [[4, 4], [4. * 2., 4. / 2.], [4. / 2., 4. * 2.], [4. * 3, 4. / 3.], [4. / 3., 4. * 3.]],\r\n\r\n            [[4, 4], [4. * 2., 4. / 2.], [4. / 2., 4. * 2.], [4. * 3, 4. / 3.], [4. / 3., 4. * 3.]],\r\n\r\n            [[4, 4], [4. * 2., 4. / 2.], [4. / 2., 4. * 2.], [4. * 3, 4. / 3.], [4. / 3., 4. * 3.]],\r\n\r\n            [[4, 4], [4. * 2., 4. / 2.], [4. / 2., 4. * 2.], [4. * 3, 4. / 3.], [4. / 3., 4. * 3.]],\r\n\r\n            [[4, 4], [4. * 2., 4. / 2.], [4. / 2., 4. * 2.], [4. * 3, 4. / 3.], [4. / 3., 4. * 3.]],\r\n        ]\r\n        anchors_all = anchor_generator(\r\n            fpn, anchors, dw_rate, flatten=True\r\n        )\r\n        anchors_all = tf.concat(anchors_all, axis=0)\r\n        self.cla_head = self._cla_head(fpn_channels, 5)\r\n        self.reg_head = self._reg_head(fpn_channels, 5)\r\n        p3c = self.cla_head(p3)\r\n        p3r = self.reg_head(p3)\r\n        p4c = self.cla_head(p4)\r\n        p4r = self.reg_head(p4)\r\n        p5c = self.cla_head(p5)\r\n        p5r = self.reg_head(p5)\r\n        p6c = self.cla_head(p6)\r\n        p6r = self.reg_head(p6)\r\n        p7c = self.cla_head(p7)\r\n        p7r = self.reg_head(p7)\r\n        pc = tf.concat([p3c, p4c, p5c, p6c, p7c], axis=1)\r\n        pr = tf.concat([p3r, p4r, p5r, p6r, p7r], axis=1)\r\n        pc = tf.nn.sigmoid(pc)\r\n        if self.mode == 'train':\r\n            i = 0\r\n            loss = tf.constant([0., 0.], dtype=tf.float32, shape=[1, 2])\r\n            cond = lambda loss, i: tf.less(i, self.batch_size)\r\n            body = lambda loss, i: (\r\n                tf.add(loss, self._compute_one_image_loss(\r\n                    tf.gather(gt, i),\r\n                    anchors_all,\r\n                    tf.gather(pc, i),\r\n                    tf.gather(pr, i))\r\n                       ),\r\n                tf.add(i, 1)\r\n            )\r\n            loss, _ = tf.while_loop(cond, body, (loss, i))\r\n            loss /= self.batch_size\r\n            return tf.keras.Model(inputs=[image, gt], outputs=loss, name='retinanet')\r\n        else:\r\n            nms_score_threshold = 0.5\r\n            nms_max_boxes = 100\r\n            nms_iou_threshold = 0.45\r\n            pr = pr[0, ...]\r\n            confidence = pc[0, ...]\r\n            y1x1y2x2 = bbox_decode(anchors_all, pr, normlization=[10., 10., 5., 5.])\r\n            filter_mask = tf.greater_equal(confidence, nms_score_threshold)\r\n            scores = []\r\n            class_id = []\r\n            bbox = []\r\n            for i in range(self.num_classes):\r\n                scoresi = tf.boolean_mask(confidence[:, i], filter_mask[:, i])\r\n                bboxi = tf.boolean_mask(y1x1y2x2, filter_mask[:, i])\r\n                selected_indices = tf.image.non_max_suppression(\r\n                    bboxi, scoresi, nms_max_boxes, nms_iou_threshold,\r\n                )\r\n                scores.append(tf.gather(scoresi, selected_indices))\r\n                bbox.append(tf.gather(bboxi, selected_indices))\r\n                class_id.append(tf.ones_like(tf.gather(scoresi, selected_indices), tf.int32) * i)\r\n            bbox = tf.concat(bbox, axis=0)\r\n            scores = tf.concat(scores, axis=0)\r\n            class_id = tf.concat(class_id, axis=0) + 1\r\n            detection_pred = [scores, bbox, class_id]\r\n            return tf.keras.Model(inputs=image, outputs=detection_pred, name='retinanet')\r\n\r\n    def _compute_one_image_loss(self, gt, anchors, pc, pr):\r\n        slice_index = tf.argmin(gt, axis=0)[0]\r\n        gt = tf.gather(gt, tf.range(0, slice_index, dtype=tf.int64))\r\n        gt_bbox = gt[:, 0:4]\r\n        label = tf.cast(gt[..., 4:5], dtype=tf.int32) - 1\r\n        pos_threshold = 0.5\r\n        neg_threshold = 0.4\r\n        gaiou = bbox_iou(gt_bbox, anchors)\r\n        pos_pc, pos_label, pos_pr, pos_gt_bbox, pos_a, neg_pc = partition_pos_neg_samples(\r\n            gt_bbox, label, gaiou, pc, pr, anchors, pos_threshold, neg_threshold\r\n        )\r\n        pos_gr = bbox_encode(pos_gt_bbox, pos_a, normlization=[10., 10, 5., 5.])\r\n        reg_loss = tf.reduce_sum(smooth_l1_loss(pos_pr-pos_gr))\r\n        pos_label = tf.one_hot(pos_label, self.num_classes, dtype=tf.float32)\r\n        cla_loss = focal_loss(pos_pc, pos_label, neg_pc, alpha=0.25, gamma=2.)\r\n        reg_loss = tf.reshape(reg_loss, [1, 1])\r\n        cla_loss = tf.reshape(cla_loss, [1, 1])\r\n        loss = tf.concat([cla_loss, reg_loss], axis=-1)\r\n        return loss\r\n\r\n    def _cla_head(self, input_channels, anchors):\r\n        x = tf.keras.Input(shape=[None, None, input_channels], dtype=tf.float32)\r\n        conv = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal')(x)\r\n        bn = layers.BatchNormalization(3, epsilon=1.001e-5)(conv)\r\n        relu = layers.Activation('relu')(bn)\r\n        conv = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal')(relu)\r\n        bn = layers.BatchNormalization(3, epsilon=1.001e-5)(conv)\r\n        relu = layers.Activation('relu')(bn)\r\n        conv = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal')(relu)\r\n        bn = layers.BatchNormalization(3, epsilon=1.001e-5)(conv)\r\n        relu = layers.Activation('relu')(bn)\r\n        pred = layers.Conv2D(self.num_classes * anchors, 3, 1, 'same', kernel_initializer='he_normal',\r\n                             bias_initializer=tf.constant_initializer(-4.595))(relu)\r\n        pred = tf.reshape(pred, [self.batch_size, -1, self.num_classes])\r\n        return tf.keras.Model(inputs=x, outputs=pred, name='cla_head')\r\n\r\n    def _reg_head(self, input_channels, anchors):\r\n        x = tf.keras.Input(shape=[None, None, input_channels], dtype=tf.float32)\r\n        conv = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal')(x)\r\n        bn = layers.BatchNormalization(3, epsilon=1.001e-5)(conv)\r\n        relu = layers.Activation('relu')(bn)\r\n        conv = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal')(relu)\r\n        bn = layers.BatchNormalization(3, epsilon=1.001e-5)(conv)\r\n        relu = layers.Activation('relu')(bn)\r\n        conv = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal')(relu)\r\n        bn = layers.BatchNormalization(3, epsilon=1.001e-5)(conv)\r\n        relu = layers.Activation('relu')(bn)\r\n        pred = layers.Conv2D(4 * anchors, 3, 1, 'same', kernel_initializer='he_normal')(relu)\r\n        pred = tf.reshape(pred, [self.batch_size, -1, 4])\r\n        return tf.keras.Model(inputs=x, outputs=pred, name='reg_head')\r\n\r\n    def save_weights(self, filepath, overwrite=True, save_format=None):\r\n        self.model.save_weights(filepath, overwrite, save_format)\r\n\r\n    def load_weights(self, filepath, by_name=False):\r\n        self.model.load_weights(filepath, by_name)\r\n```\r\n\r\nthe save weights code is\r\n```\r\nconfig = {\r\n    'num_classes': 20,\r\n    'batch_size':batch_size,\r\n    'mode': 'train',\r\n    'lr': lr,\r\n    'weight_decay':1e-4,\r\n    'backone': 'resnetv1_18',\r\n    'backone_conv_trainable': True,\r\n    'backone_bn_trainable': True,\r\n}\r\nssd = RetinaNet(config)\r\nssd.save_weights('saved_weights/1.tf')\r\n```\r\n\r\nthe load weights code is\r\n```\r\nconfig = {\r\n    'num_classes': 20,\r\n    'batch_size':batch_size,\r\n    'mode': 'test',\r\n    'lr': lr,\r\n    'weight_decay':1e-4,\r\n    'backone': 'resnetv1_18',\r\n    'backone_conv_trainable': True,\r\n    'backone_bn_trainable': True,\r\n}\r\nssd = RetinaNet(config)\r\nssd.load_weights('saved_weights/1.tf')\r\n```\r\n\r\n\r\n## When enable_eager\r\n##  the weights saved under mode=='train' could not  be restored into model when mode=='test'\r\nThe error message is as follows:\r\n\r\n```\r\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\r\n\r\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.engine.training.Model object at 0x7fa25076a3d0> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7fa240170810>).\r\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\r\n\r\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.engine.training.Model object at 0x7fa270103250> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7fa1f35aced0>).\r\nTraceback (most recent call last):\r\n  File \"/home/master/workspace/objdect/test1.py\", line 23, in <module>\r\n    ssd.load_weights('saved_weights/1.tf')\r\n  File \"/home/master/workspace/objdect/RetinaNet.py\", line 170, in load_weights\r\n    self.model.load_weights(filepath, by_name)\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 181, in load_weights\r\n    return super(Model, self).load_weights(filepath, by_name)\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1149, in load_weights\r\n    status = self._trackable_saver.restore(filepath)\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1270, in restore\r\n    checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 209, in restore\r\n    restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 908, in _restore_from_checkpoint_position\r\n    tensor_saveables, python_saveables))\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py\", line 289, in restore_saveables\r\n    validated_saveables).restore(self.save_path_tensor)\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py\", line 255, in restore\r\n    restore_ops.update(saver.restore(file_prefix))\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py\", line 102, in restore\r\n    restored_tensors, restored_shapes=None)\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 115, in restore\r\n    self.handle_op, self._var_shape, restored_tensor)\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 291, in shape_safe_assign_variable_handle\r\n    shape.assert_is_compatible_with(value_tensor.shape)\r\n  File \"/home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 1115, in assert_is_compatible_with\r\n    raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (20,) and (100,) are incompatible\r\n```\r\nthe Shapes(20,) is the shape of reg_head, the Shapes(100,) is the shape of cla_head\r\nMaybe the loading order is out of order\r\n\r\n## When disable_eager\r\n##  the weights saved under mode=='train' could  be restored into model when mode=='test' with some Warnings:\r\n```\r\nWARNING:tensorflow:From /home/master/app/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\r\n\r\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.engine.training.Model object at 0x7f26680ae5d0> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f26408af190>).\r\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\r\n\r\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.engine.training.Model object at 0x7f266830fdd0> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f26402047d0>).\r\n\r\n```\r\n", "comments": ["@Stick-To,\r\nTried reproducing your issue but encountered the error, `NameError: name 'backone' is not defined`, after giving some dummy values to `batch_size` and `lr`. Please find the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/47956cd5c8b7742f2e313efc116a7fdc/34298.ipynb). Can you please help me to reproduce the error. Thanks!", "@rmothukuru    It's done", "when I combine _cla_head and _reg_head as this, the code could run well\r\n```\r\ndef _head(self, input_channels, anchors):\r\n    x = tf.keras.Input(shape=[None, None, input_channels], dtype=tf.float32)\r\n    conv1 = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal', name='cla_conv1')(x)\r\n    bn1 = layers.BatchNormalization(3, epsilon=1.001e-5, name='cla_bn1')(conv1)\r\n    relu1 = layers.Activation('relu', name='cla_relu1')(bn1)\r\n    conv1 = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal', name='cla_conv2')(relu1)\r\n    bn1 = layers.BatchNormalization(3, epsilon=1.001e-5, name='cla_bn2')(conv1)\r\n    relu1 = layers.Activation('relu', name='cla_relu2')(bn1)\r\n    conv1 = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal', name='cla_conv3')(relu1)\r\n    bn1 = layers.BatchNormalization(3, epsilon=1.001e-5, name='cla_bn3')(conv1)\r\n    relu1 = layers.Activation('relu', name='cla_relu3')(bn1)\r\n    cla_pred = layers.Conv2D(self.num_classes * anchors, 3, 1, 'same', kernel_initializer='he_normal',\r\n                          name='cla_conv4', bias_initializer=tf.constant_initializer(-4.595))(relu1)\r\n    cla_pred = tf.reshape(cla_pred, [self.batch_size, -1, self.num_classes])\r\n\r\n    conv2 = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal', name='reg_conv1')(x)\r\n    bn2 = layers.BatchNormalization(3, epsilon=1.001e-5, name='reg_bn1')(conv2)\r\n    relu2 = layers.Activation('relu', name='reg_relu1')(bn2)\r\n    conv2 = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal', name='reg_conv2')(relu2)\r\n    bn2 = layers.BatchNormalization(3, epsilon=1.001e-5, name='reg_bn2')(conv2)\r\n    relu2 = layers.Activation('relu', name='reg_relu2')(bn2)\r\n    conv2 = layers.Conv2D(256, 3, 1, 'same', kernel_initializer='he_normal', name='reg_conv3')(relu2)\r\n    bn2 = layers.BatchNormalization(3, epsilon=1.001e-5, name='reg_bn3')(conv2)\r\n    relu2 = layers.Activation('relu', name='reg_relu3')(bn2)\r\n    reg_pred = layers.Conv2D(4 * anchors, 3, 1, 'same', kernel_initializer='he_normal', name='reg_conv4')(relu2)\r\n    reg_pred = tf.reshape(reg_pred, [self.batch_size, -1, 4])\r\n    return tf.keras.Model(inputs=x, outputs=[cla_pred, reg_pred])\r\n```", "@rmothukuru  https://colab.research.google.com/gist/Stick-To/e745c258bfa65bdda2d95844e1ce3f76/34298.ipynb", "Could reproduce the error with TF Version 2.0. Here is the [Gist](https://colab.research.google.com/gist/Stick-To/e745c258bfa65bdda2d95844e1ce3f76/34298.ipynb). Thanks!", "@rmothukuru  the reconstruction of it is done, isn't it?\r\n\r\nhttps://colab.research.google.com/gist/Stick-To/e745c258bfa65bdda2d95844e1ce3f76/34298.ipynb", "@Stick-To,\r\nYes, it is done and I have forwarded it to another Engineer. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34298\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34298\">No</a>\n", "I know this issue has been closed, but I've also been struggling with this. Has this been resolved, or if not, any where I can follow progress?", "@stefanobranco Please create a new issue with details and a simple standalone code to reproduce the issue. Thanks!"]}, {"number": 34297, "title": "tf.function hangs on ragged tensor input", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):No\r\n- TensorFlow installed from (source or binary):Colab\r\n- TensorFlow version (use command below):2.0.0\r\n- Python version:3.6\r\n- GPU model and memory:None\r\n\r\n**Describe the current behavior**\r\nWhen using tf.function with a number of for loops on a RaggedTensor, the function call hangs (I stopped waiting after half hour).\r\n-when running the function directly (no tf.function decorator), the function executes immediately\r\n-when converting the ragged tensor to dense tensor, the function executes immediately.\r\n\r\nI couldn't pinpoint the exact combination of operations that causes this Autograph behavior, but I tried to reduce my code to the simplest combination that still causes this behavior.\r\nI struggled for hours with my own code, trying to get tf.function to work, until I figured it was due to the ragged tensor + for loops + tf.function hanging the kernel\r\n\r\nI observed similar behavior on my machine and a colab machine as well.\r\n\r\n**Describe the expected behavior**\r\nShould execute in a comparable time to a dense tensor.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n%tensorflow_version 2.x\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninp=tf.ragged.constant(np.arange(1000,dtype=np.float32).reshape(10,10,10))\r\n\r\n@tf.function    #if not using tf.function it runs well\r\ndef ragged_example(r_tensor):\r\n  s=tf.constant(0.0)\r\n  for i in tf.range(10):\r\n    inner=r_tensor[i]\r\n    for x in inner:\r\n      b=tf.reduce_sum(x)\r\n      s=s+b\r\n  return s\r\n\r\n#inp=inp.to_tensor() #if this is uncommented, it runs well\r\n\r\nragged_example(inp) #this hangs\r\n```\r\n**Other observations**\r\nAlso noted very large memory footprint as a result (9gb and growing)", "comments": ["Could reproduce the issue with TF Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/3a286f4b25e62d041d0030e80a4e1cb2/34297.ipynb). Thanks!", "This bug is a combination of several problems that we'll need to resolve, fopefully in 2.2. In the mean time, a workaround is to avoid writing for loops over ragged tensors (e.g. `for x in ragged_tensor`). This means rewriting your code like so:\r\n\r\n```\r\ndef ragged_example(r_tensor):\r\n  s = tf.constant(0.0)\r\n  for i in tf.range(10):\r\n    inner = r_tensor[i]\r\n    for j in tf.range(inner.row_lengths()[0]):  #  `for x in inner` doesn't work yet.\r\n      x = inner[j]\r\n      b = tf.reduce_sum(x)\r\n      s = s + b\r\n  return s\r\n```\r\n\r\nA few more details on the cause of the bug --\r\n\r\nAutograph doesn't currently support iterating over `RaggedTensor`, and it thinks they are normal Python objects. We'll fix that in autograph, hopefully by TF 2.2.\r\nNow, this should have normally generated an error (something in the lines of \"cannot directly iterate over RaggedTensor in graph mode\"). But it seems that the `RaggedTensor` class is iterable and the iterator it returns hangs when consumed in graph mode. @edloper I think this iterator should error out in `tf.function` instead?\r\n\r\nEdit: fixed the index in the example", "Thank you for the update and the details.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34297\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34297\">No</a>\n", "The PR that just got merged should fix the issue in tf.function.\r\n\r\nAs a side note, we should still disallow RaggedTensor.__iter__ in graph code (or make it return a proper graph-based iterator)."]}, {"number": 34296, "title": "Error description is not clear with new experimental TF_lite_converter ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): `tf-nightly`\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n# without the following two lines, it will throw\r\n# ValueError: Cannot set tensor: Got value of type NOTYPE but expected type FLOAT32 for input 0, name: flatten_input \r\n#x_train = tf.dtypes.cast(x_train,tf.float32)\r\n#x_test = tf.dtypes.cast(x_test,tf.float32)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=1)\r\nmodel.evaluate(x_test, y_test)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\n#converter.experimental_enable_mlir_converter = True\r\ntflite_model = converter.convert()\r\n\r\nimport numpy as np\r\nexpected = model.predict(x_test[0:1])\r\n\r\n# Run the model with TensorFlow Lite\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninterpreter.set_tensor(input_details[0][\"index\"], x_test[0:1, :, :])\r\ninterpreter.invoke()\r\nresult = interpreter.get_tensor(output_details[0][\"index\"])\r\n\r\n# Assert if the result of TFLite model is consistent with the TF model.\r\nnp.testing.assert_almost_equal(expected, result)\r\nprint(\"Done. The result of TensorFlow matches the result of TensorFLow Lite.\")\r\n```\r\n\r\n\r\n**The output from the converter invocation**\r\n\r\n`ValueError: Cannot set tensor: Got value of type NOTYPE but expected type FLOAT32 for input 0, name: flatten_input`\r\n\r\n**Failure details**\r\n\r\nConversion is successful if the data type is `float32`. If the data type of input data is `float64`, then it will throw `ValueError: Cannot set tensor: Got value of type NOTYPE but expected type FLOAT32 for input 0, name: flatten_input ` which is not clear.  Most of the keras models In Tensorflow website under tutorials are with `float64` datatype. So, if the user try to convert them into tf_lite model, they will end up in this `ValueError`. I think we need to update the Error description. Instead of showing `NOTYPE`, may it is better to use `float64` or other data types that are not compatible. \r\n\r\nHere is the link to colab [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/86eb0abb1cda9888d87c4d7c109a48c4/untitled632.ipynb)\r\n\r\n", "comments": ["Over to @miaout17 and @gargn to investigate.", "`float64` isn't supported in the TFLite interpreter. The supported types are available [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/common.c#L183). \r\n\r\nBecause it's not trivial to change the error message that is shown [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L363) without adding support for `float64`, I am closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34296\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34296\">No</a>\n"]}, {"number": 34295, "title": "Update collective op to enable polymorphic output shape", "body": "To enable the polymorphic output shape support for collective ops, this change disables caching `output_shape` for collective ops. Every time an `allGather` collective op is called, a new `output_shape` will be inferred and assigned to the latest instance.shape. \r\n\r\nA new unit test case is added correspondingly.\r\n\r\nFix #34250. For more information, please refer to the issue description.", "comments": ["`AIL: Found 24 non-whitelisted pylint errors:\r\ntensorflow/python/ops/collective_ops_test.py:352: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/ops/collective_ops_test.py:353: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/ops/collective_ops_test.py:354: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/ops/collective_ops_test.py:355: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/ops/collective_ops_test.py:356: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/ops/collective_ops_test.py:357: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/ops/collective_ops_test.py:358: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/ops/collective_ops_test.py:359: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/ops/collective_ops_test.py:360: [C0330(bad-continuation), ] Wrong hanging indentation (remove 4 spaces).\r\n\r\ntensorflow/python/ops/collective_ops_test.py:360: [C0301(line-too-long), ] Line too long (87/80)\r\n\r\ntensorflow/python/ops/collective_ops_test.py:361: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6\r\n\r\ntensorflow/python/ops/collective_ops_test.py:362: [W0311(bad-indentation), ] Bad indentation. Found 14 spaces, expected 8\r\n\r\ntensorflow/python/ops/collective_ops_test.py:363: [C0301(line-too-long), ] Line too long (86/80)\r\n\r\ntensorflow/python/ops/collective_ops_test.py:363: [W0311(bad-indentation), ] Bad indentation. Found 14 spaces, expected 8\r\n\r\ntensorflow/python/ops/collective_ops_test.py:364: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6\r\n\r\ntensorflow/python/ops/collective_ops_test.py:365: [W0311(bad-indentation), ] Bad indentation. Found 14 spaces, expected 8\r\n\r\ntensorflow/python/ops/collective_ops_test.py:366: [C0301(line-too-long), ] Line too long (86/80)\r\n\r\ntensorflow/python/ops/collective_ops_test.py:366: [W0311(bad-indentation), ] Bad indentation. Found 14 spaces, expected 8\r\n\r\ntensorflow/python/ops/collective_ops_test.py:368: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6\r\n\r\ntensorflow/python/ops/collective_ops_test.py:369: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6\r\n\r\ntensorflow/python/ops/collective_ops_test.py:370: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6\r\n\r\ntensorflow/python/ops/collective_ops_test.py:372: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6\r\n\r\ntensorflow/python/ops/collective_ops_test.py:373: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6\r\n\r\ntensorflow/python/ops/collective_ops_test.py:374: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6`\r\n\r\n@pw2393 can you please fix this sanity build errors ?\r\nhere is the link for [sanity errors.](https://source.cloud.google.com/results/invocations/fff0adeb-756d-45c1-8717-3f19d17b3ad2/log)", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34295) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34295) for more info**.\n\n<!-- ok -->", "Thanks! @rthadur Just linted.", "@dubey that name changing accidentally broke the lint. mind if review/approve again?"]}, {"number": 34294, "title": "Fix buildifier errors", "body": "Fixing buildifier found errors and/or warnings..\r\ntensorflow/core/profiler/internal/BUILD # reformat\r\ntensorflow/core/profiler/internal/gpu/BUILD # reformat\r\ntensorflow/core/debug/BUILD # reformat\r\ntensorflow/compiler/xla/BUILD # reformat\r\ntensorflow/compiler/jit/BUILD # reformat\r\ntensorflow/lite/delegates/gpu/metal/BUILD # reformat\r\ntensorflow/lite/delegates/gpu/metal/kernels/BUILD # reformat\r\ntensorflow/c/kernels/BUILD # reformat", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34294) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34294) for more info**.\n\n<!-- cla_yes -->", "it still seems to be failing on tensorflow/core/profiler/internal/gpu/BUILD , even though the cl addresses the load arguments", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34294) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34294) for more info**.\n\n<!-- cla_yes -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34294) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34294) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 34293, "title": "Add implicit batch experimental", "body": "- Add experimental argument `use_implicit_batch`. This is the first step in adding dynamic shapes to TF-TRT, so it still doesn't work but it provides the scaffolding for the next PRs.\r\n- Adds a simple test for rewriter_config_template which is the only way to use `use_implicit_batch` until we take it out of experimental.\r\n- Improves argument checks in trt_convert.py\r\n\r\n", "comments": ["@pooyadavoodi Can you please resolve conflicts? Thanks!", "Also @sanjoy ", "> @pooyadavoodi Can you please resolve conflicts? Thanks!\r\n\r\nDone\r\n\r\nI've added the commits from https://github.com/tensorflow/tensorflow/pull/34660 otherwise it will conflict with the master once https://github.com/tensorflow/tensorflow/pull/34660 is merged. I am hoping github will take care of duplicated commits.", "> Thanks Pooya for the fix. We'll need to wait for #34660 be merged first.\r\n\r\nThanks. I think merging them at the same time wouldn't hurt. git is smart enough to figure out the duplicates. Even if this PR gets merged first, the other PR would become an empty PR.", "@pooyadavoodi Could you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 34292, "title": "2.1-RC0 cherry-pick request: [tf.data] iterator life cycle management bug fix", "body": "OwnedIterators and OwnedMultiDeviceIterators get created using their components at least twice during function tracing. We don't want to run the deleter in those cases as they are just referring to the original resource and not creating a new one. So we just create the deleter the first time.\r\n\r\nPiperOrigin-RevId: 280071920\r\nChange-Id: I266ac500246354e6e4d6145835c4f90926ec5715", "comments": []}, {"number": 34291, "title": "Error on compiling from source", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux archlinux 5.3.11-arch1-1 x86_64 GNU/Linux\r\n\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: commit hash: 872b1ab23f0aac182d5b2051f45d5d003963bfe3\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): bazel 0.29.1- (@non-git)\r\n- GCC/Compiler version (if compiling from source): gcc (GCC) 9.2.0\r\n- CUDA/cuDNN/TensorRT version: 10.1.243-2/7.6.4.38-1/6.0.1.5-1\r\n- GPU model and memory: nVidia RTX 2080 8GB\r\n\r\n\r\n**Describe the problem**\r\nCompiling with bazel fails: \r\nERROR: /home/jaaq/.cache/bazel/_bazel_jaaq/c463894dd2648fc5b64eeed02cc022b5/external/grpc/BUILD:507:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\ngit clone repo\r\ncd tensorflow\r\nsource /opt/anaconda/bin/activate\r\nconda activate python375env\r\n./configure (Yes on XLA JIT, CUDA, TensorRT, clang)\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nERROR: /home/jaaq/.cache/bazel/_bazel_jaaq/c463894dd2648fc5b64eeed02cc022b5/external/grpc/BUILD:507:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: error: ambiguating new declaration of 'long int gettid()'\r\n static long gettid(void) { return syscall(__NR_gettid); }\r\n             ^~~~~~\r\nIn file included from /usr/include/unistd.h:1170,\r\n                 from external/grpc/src/core/lib/gpr/log_linux.cc:41:\r\n/usr/include/bits/unistd_ext.h:34:16: note: old declaration '__pid_t gettid()'\r\n extern __pid_t gettid (void) __THROW;\r\n                ^~~~~~\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: warning: 'long int gettid()' defined but not used [-Wunused-function]\r\n static long gettid(void) { return syscall(__NR_gettid); }\r\n             ^~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 89.815s, Critical Path: 22.19s\r\nINFO: 1215 processes: 1215 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Having the same error, did you manage to solve it?", "Not yet. See also: https://www.reddit.com/r/archlinux/comments/e2e5s8/error_compiling_tensorflow_20/?utm_source=share&utm_medium=web2x\r\n\r\nJust tested it again, same error persists on new master commit: d9c782939e3fb03b5f1e8d6ae83c185bedc619c3", "The error seems to be due to the same symbol definitions (of `gettid`) introduced in GCC 9.0+ extant in grpc. https://github.com/grpc/grpc/issues/20043\r\n\r\nThe folks at grpc have patched the problem in newer releases.\r\nhttps://github.com/grpc/grpc/pull/20048\r\n\r\nHowever, during the build process, TensorFlow downloads a cached version of an older commit of grpc from the TensorFlow mirror. At this point, I do not know which commit from the grpc repository fixes the issue in particular (I tried the latest one, it has some problem with `upb_proto_library`). Therefore, the best course of action, as I found from other sources (https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03) is to patch the grpc commit downloaded using a patch file.\r\n\r\nIf the patch in https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03 doesn't work I would suggest cloning the grpc source from github, checkout the commit `4566c2a29ebec0835643b972eb99f4306c4234a3`. Edit the following files -\r\n\r\n* `src/core/lib/gpr/log_linux.cc`\r\n* `src/core/lib/gpr/log_posix.cc`\r\n* `src/core/lib/iomgr/ev_epollex_linux.cc`\r\n\r\nJust change every instance of the term `gettid` to `sys_gettid` in these three files and generate a patch for yourself using `git diff` and use that patch.", "Thanks @shantanu-gontia for documenting this - just ran into it as well. The linked patch works fine for me.\r\nOf course, it would be nice if the TF source could be fixed :-)\r\n", "same here\r\n\r\n```bash\r\nERROR: /home/user/.cache/bazel/_bazel_userbest/89310458958e3aedfc736fc503a62f52/external/grpc/BUILD:507:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: error: ambiguating new declaration of 'long int gettid()'\r\n   43 | static long gettid(void) { return syscall(__NR_gettid); }\r\n      |             ^~~~~~\r\nIn file included from /usr/include/unistd.h:1170,\r\n                 from external/grpc/src/core/lib/gpr/log_linux.cc:41:\r\n/usr/include/bits/unistd_ext.h:34:16: note: old declaration '__pid_t gettid()'\r\n   34 | extern __pid_t gettid (void) __THROW;\r\n      |                ^~~~~~\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: warning: 'long int gettid()' defined but not used [-Wunused-function]\r\n   43 | static long gettid(void) { return syscall(__NR_gettid); }\r\n      |             ^~~~~~\r\nERROR: /home/user/.cache/yay/tensorflow-git/src/tensorflow/tensorflow/python/tools/BUILD:312:1 C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)\r\nINFO: Elapsed time: 1012.293s, Critical Path: 30.58s\r\nINFO: 3168 processes: 3168 local.\r\nFAILED: Build did NOT complete successfully\r\n==> ERROR: A failure occurred in build().\r\n    Aborting...\r\nError making: tensorflow-git\r\n\r\n```\r\n\r\nArch Linux", "Yes the error seems to still be there. A patch for grpc exists and is included in the Arch Build System build: https://git.archlinux.org/svntogit/community.git/tree/trunk?h=packages/tensorflow\r\n\r\nI recommend to use that in the meantime.", "This works for me on ArchLinux to build JAX. GCC 9.2.1 works well after this modification.\r\n> Edit the following files -\r\n> \r\n> * `src/core/lib/gpr/log_linux.cc`\r\n> * `src/core/lib/gpr/log_posix.cc`\r\n> * `src/core/lib/iomgr/ev_epollex_linux.cc`\r\n> \r\n> Just change every instance of the term `gettid` to `sys_gettid` in these three files and generate a patch for yourself using `git diff` and use that patch.\r\n\r\n", "I have ran into this issue as well and I am not sure how to apply the patch. Would someone be able to give me some incite?", "I'm with @IceCryptonym: clearer directions would be greatly appreciated! \r\n\r\nPerhaps the source has changed, but I don't see any linked patch file or even much mention of patching at the page described by @shantanu-gontia (https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03).    Nor does the Arch-Linux repo mentioned by @Mithrandir2k18 seem to make any mention of `gprc`, rather it's all about `mkl`. \r\n\r\nBut I followed @shantanu-gontia's instructions, and have posted a patch file here:  https://gist.github.com/drscotthawley/8eb51af1b4c92c4f18432cb045698af7\r\n\r\nIt can be applied by going to the main grpc directory and running \r\n\r\n```\r\n$ git apply grpc.patch\r\n```\r\n\r\nWhats still not clear is *where* this should be applied, i.e. where does Bazel put `grpc` and how I can apply the patch *after* Bazel puts it there?  I see `tensorflow/third_party/grpc/` but it's empty except for a zero-length file called `BUILD`.  There's also `tensorflow/tensorflow/contrib/cmake/patches/grpc/`, but that only contains the file `rand.h`\r\n Finally, just putting the patch file in `third_party/` doesn't seem to cause the patch to be applied. \r\n\r\n**EDIT:** Seems Bazel puts it in `~/.cache/bazel/_bazel_($USER)/(big_long_random_directory_name)/external/grpc`, but this is not a `git` directory so `git apply` won't work.  So...still unclear on how to apply the patch reliably and automatically within the Bazel build. ", "The grpc.patch should be applied in\r\ntensorflow/bazel-tensorflow/external/grpc/src\r\nafter the bazel build started the download.\r\n\r\nOr apply it in the .cache (P.S. not working if applied in the cache)\r\n\r\n```\r\ncd ~/.cache/bazel/*/*/external/grpc\r\npatch -p1 < ~/grpc.patch\r\n```\r\n\r\n> ", "Could you explain how the patch flow works?\r\n\r\nFor instance, do we first launch bazel - get it to fail, then go to tensorflow/bazel-tensorflow/external/grpc/src and then perform the patch application through either git apply _patch file name_ or patch -p1 _patch file name_ ?", "@Mithrandir2k18,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@amahendrakar \r\n\r\nI won't have time to test this in the nearer future and also currently don't have a need to use the extra efficiency I'd get from compiling myself. I'll come back to this in a few months probably, but then I may as well open a new issue and maybe reference this one.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34291\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34291\">No</a>\n"]}, {"number": 34290, "title": "Brackets in directory for tf.train.CheckpointManager in TF2.0", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian (GCP notebook)**\r\n- TensorFlow installed from (source or binary): **preinstalled on GCP notebook**\r\n- TensorFlow version (use command below): **2.0.0**\r\n- Python version: **3.5.3**\r\n- GPU model and memory: **None**\r\n\r\n**Describe the current behavior**\r\nThis problem concerns tf.train.CheckpointManager and tf.train.Checkpoint in TF 2.0. If the checkpoint directory contains square brackets (`[` or `]`), then loading checkpoints with tf.train.Checkpoint.restore fails. CheckpointManager will save and track checkpoints as expected but does not remove them according to `max_to_keep`.\r\n\r\nI see that square brackets are [not recommended](https://cloud.google.com/storage/docs/naming) for Google Cloud Storage blobs, however this happens for both checkpoints stored in Google Cloud Storage and checkpoints stored locally.\r\n\r\nAs an example:\r\nValid checkpoints exist in `/tmp/checkpoints_[with]_bracket/` Calling\r\n```\r\ncheckpoint_dir = '/tmp/checkpoints_[with]_bracket/'\r\ntf.train.Checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\r\n```\r\nGives the error:\r\n```\r\nNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /tmp/checkpoints_[with]_bracket/ckpt-4\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n- Allow square brackets in the checkpoint path\r\nor\r\n- fail at CheckpointManager creation if `directory` parameter contains disallowed characters \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**[https://gist.github.com/klanderson/be46cc3e3f7e6575bd2a45450c0ac102](https://gist.github.com/klanderson/be46cc3e3f7e6575bd2a45450c0ac102)\r\nCode works as-is. Add a bracket somewhere in the path in `Line 28` to see error**\r\n\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly i.e. v2.4.0-dev20200701. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4f02c907ded15241983433c1aedc959f/34290.ipynb). Thanks!", "@klanderson , The issue is fixed in the recent Tensorflow nightly 2.6 version, please find the gist [here](https://colab.research.google.com/gist/googly789/7dab06572f67cd32b8fb1afff9d1f8dd/untitled53.ipynb).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34290\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34290\">No</a>\n"]}, {"number": 34289, "title": "Couldn't quantize a model with QUANTIZED_UINT8 if output layer don't have activation", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): installed a binary with Conda\r\n- TensorFlow version (or github SHA if from source): 1.14.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\ndef build_keras_model():\r\n  return keras.Sequential([\r\n    keras.layers.Flatten(input_shape=(28,28)),\r\n    keras.layers.Dense(128, activation='relu'),\r\n    keras.layers.BatchNormalization(),\r\n    keras.layers.Dense(10)\r\n  ])\r\n\r\ntrain_graph = tf.Graph()\r\ntrain_sess = tf.Session(graph=train_graph)\r\n\r\nkeras.backend.set_session(train_sess)\r\nwith train_graph.as_default():\r\n  train_model = build_keras_model()\r\n\r\n  tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)\r\n  train_sess.run(tf.global_variables_initializer())\r\n\r\n  train_model.compile(\r\n    optimizer='adam',\r\n    loss='sparse_categorical_crossentropy',\r\n    metrics=['accuracy']\r\n  )\r\n  train_model.fit(train_images, train_labels, epochs=5)\r\n\r\n  saver = tf.train.Saver()\r\n  saver.save(train_sess, 'checkpoints')\r\n\r\neval_graph = tf.Graph()\r\neval_sess = tf.Session(graph=eval_graph)\r\n\r\nkeras.backend.set_session(eval_sess)\r\n\r\nwith eval_graph.as_default():\r\n  keras.backend.set_learning_phase(0)\r\n  eval_model = build_keras_model()\r\n  tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)\r\n  eval_graph_def = eval_graph.as_graph_def()\r\n  saver = tf.train.Saver()\r\n  saver.restore(eval_sess, 'checkpoints')\r\n\r\n  frozen_graph_def = tf.graph_util.convert_variables_to_constants(\r\n    eval_sess,\r\n    eval_graph_def,\r\n    [eval_model.output.op.name]\r\n  )\r\n  with open('frozen_model.pb', 'wb') as f:\r\n    f.write(frozen_graph_def.SerializeToString())\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  'frozen_model.pb',\r\n  ['flatten_input'],\r\n  ['dense_1/BiasAdd'])\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {input_arrays[0] : (0, 255)}  # mean, std_dev\r\ntflite_model = converter.convert()\r\nopen('model.tflite', 'wb').write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-18-c82690488195> in <module>\r\n      6 input_arrays = converter.get_input_arrays()\r\n      7 converter.quantized_input_stats = {input_arrays[0] : (0, 255)}  # mean, std_dev\r\n----> 8 tflite_model = converter.convert()\r\n      9 open('model.tflite', 'wb').write(tflite_model)\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    896           input_tensors=self._input_tensors,\r\n    897           output_tensors=self._output_tensors,\r\n--> 898           **converter_kwargs)\r\n    899     else:\r\n    900       result = _toco_convert_graph_def(\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)\r\n    402   data = toco_convert_protos(model_flags.SerializeToString(),\r\n    403                              toco_flags.SerializeToString(),\r\n--> 404                              input_data.SerializeToString())\r\n    405   return data\r\n    406 \r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    170       stderr = _try_convert_to_unicode(stderr)\r\n    171       raise ConverterError(\r\n--> 172           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    173   finally:\r\n    174     # Must manually cleanup files.\r\n\r\nConverterError: TOCO failed. See console for info.\r\n2019-11-14 14:46:19.525955: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 39 operators, 63 arrays (0 quantized)\r\n2019-11-14 14:46:19.526224: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 39 operators, 63 arrays (0 quantized)\r\n2019-11-14 14:46:19.527392: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 10 operators, 18 arrays (1 quantized)\r\n2019-11-14 14:46:19.527775: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 9 operators, 17 arrays (1 quantized)\r\n2019-11-14 14:46:19.527893: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 8 operators, 15 arrays (1 quantized)\r\n2019-11-14 14:46:19.527982: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 8 operators, 15 arrays (1 quantized)\r\n2019-11-14 14:46:19.528015: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 5 operators, 12 arrays (1 quantized)\r\n2019-11-14 14:46:19.528050: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 5 operators, 12 arrays (1 quantized)\r\n2019-11-14 14:46:19.528084: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 5 operators, 12 arrays (1 quantized)\r\n2019-11-14 14:46:19.529767: W tensorflow/lite/toco/graph_transformations/quantize.cc:132] Constant array batch_normalization/batchnorm/mul lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.\r\n2019-11-14 14:46:19.529796: W tensorflow/lite/toco/graph_transformations/quantize.cc:132] Constant array batch_normalization/batchnorm/sub lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.\r\n2019-11-14 14:46:19.529847: F tensorflow/lite/toco/graph_transformations/quantize.cc:149] Array dense_1/BiasAdd does not have MinMax information, and is not a constant array. Cannot proceed with quantization.\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f40e79dd700 (most recent call first):\r\n  File \"/home/dojip.kim/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/dojip.kim/anaconda3/envs/tensorflow/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/home/dojip.kim/anaconda3/envs/tensorflow/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n  File \"/home/dojip.kim/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/dojip.kim/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/dojip.kim/anaconda3/envs/tensorflow/bin/toco_from_protos\", line 11 in <module>\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nI could quantize a model with QUNATIZED_UINT8 if output layer have activation like 'softmax'.  I want to quantize a model without the activation function for the final output. In that case, I couldn't quantize it.\r\n\r\n2019-11-14 14:46:19.529847: F tensorflow/lite/toco/graph_transformations/quantize.cc:149] Array dense_1/BiasAdd does not have MinMax information, and is not a constant array. Cannot proceed with quantization.\r\nFatal Python error: Aborted\r\n\r\nPlease let me know how to quantize a model.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Here are the node names and operations of the frozen_model.pb\r\n```\r\nflatten_input => Placeholder\r\nflatten/Shape => Shape\r\nflatten/strided_slice/stack => Const\r\nflatten/strided_slice/stack_1 => Const\r\nflatten/strided_slice/stack_2 => Const\r\nflatten/strided_slice => StridedSlice\r\nflatten/Reshape/shape/1 => Const\r\nflatten/Reshape/shape => Pack\r\nflatten/Reshape => Reshape\r\ndense/kernel => Const\r\ndense/bias => Const\r\ndense/MatMul/ReadVariableOp => Identity\r\ndense/MatMul => MatMul\r\ndense/BiasAdd/ReadVariableOp => Identity\r\ndense/BiasAdd => BiasAdd\r\ndense/Relu => Relu\r\nbatch_normalization/gamma => Const\r\nbatch_normalization/beta => Const\r\nbatch_normalization/moving_mean => Const\r\nbatch_normalization/moving_variance => Const\r\nbatch_normalization/batchnorm/ReadVariableOp => Identity\r\nbatch_normalization/batchnorm/add/y => Const\r\nbatch_normalization/batchnorm/add => Add\r\nbatch_normalization/batchnorm/Rsqrt => Rsqrt\r\nbatch_normalization/batchnorm/mul/ReadVariableOp => Identity\r\nbatch_normalization/batchnorm/mul => Mul\r\nbatch_normalization/batchnorm/mul_1 => Mul\r\nbatch_normalization/batchnorm/ReadVariableOp_1 => Identity\r\nbatch_normalization/batchnorm/mul_2 => Mul\r\nbatch_normalization/batchnorm/ReadVariableOp_2 => Identity\r\nbatch_normalization/batchnorm/sub => Sub\r\nbatch_normalization/batchnorm/add_1 => Add\r\ndense_1/kernel => Const\r\ndense_1/bias => Const\r\ndense_1/MatMul/ReadVariableOp => Identity\r\ndense_1/MatMul => MatMul\r\ndense_1/BiasAdd/ReadVariableOp => Identity\r\ndense_1/BiasAdd => BiasAdd\r\ndense/weights_quant/min => Const\r\ndense/weights_quant/min/read => Identity\r\ndense/weights_quant/max => Const\r\ndense/weights_quant/max/read => Identity\r\ndense/weights_quant/FakeQuantWithMinMaxVars => FakeQuantWithMinMaxVars\r\ndense/act_quant/min => Const\r\ndense/act_quant/min/read => Identity\r\ndense/act_quant/max => Const\r\ndense/act_quant/max/read => Identity\r\ndense/act_quant/FakeQuantWithMinMaxVars => FakeQuantWithMinMaxVars\r\ndense_1/weights_quant/min => Const\r\ndense_1/weights_quant/min/read => Identity\r\ndense_1/weights_quant/max => Const\r\ndense_1/weights_quant/max/read => Identity\r\ndense_1/weights_quant/FakeQuantWithMinMaxVars => FakeQuantWithMinMaxVars\r\nbatch_normalization/batchnorm/mul_1/activation_Mul_quant/min => Const\r\nbatch_normalization/batchnorm/mul_1/activation_Mul_quant/min/read => Identity\r\nbatch_normalization/batchnorm/mul_1/activation_Mul_quant/max => Const\r\nbatch_normalization/batchnorm/mul_1/activation_Mul_quant/max/read => Identity\r\nbatch_normalization/batchnorm/mul_1/activation_Mul_quant/FakeQuantWithMinMaxVars => FakeQuantWithMinMaxVars\r\nbatch_normalization/batchnorm/add_1/activation_Add_quant/min => Const\r\nbatch_normalization/batchnorm/add_1/activation_Add_quant/min/read => Identity\r\nbatch_normalization/batchnorm/add_1/activation_Add_quant/max => Const\r\nbatch_normalization/batchnorm/add_1/activation_Add_quant/max/read => Identity\r\nbatch_normalization/batchnorm/add_1/activation_Add_quant/FakeQuantWithMinMaxVars => FakeQuantWithMinMaxVars\r\n```", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34289\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34289\">No</a>\n"]}, {"number": 34288, "title": "Move TensorRT builder configs to converter build function", "body": "", "comments": ["> Can we move the builder inside BuildCudaEngine as well?\r\n\r\n`builder` is needed to create the network in `Converter::Create`.\r\nHow about moving it to `Converter::Create`?", "> > Can we move the builder inside BuildCudaEngine as well?\r\n> \r\n> `builder` is needed to create the network in `Converter::Create`.\r\n> How about moving it to `Converter::Create`?\r\n\r\nI moved builder to Converter. Now, the builder is owned by Converter, and it gets created in Converter::Init."]}, {"number": 34287, "title": "Cannot dlopen some GPU libraries - Tensorflow2.0 ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  (using pip install tensorflow-gpu)\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: V100 , memory_limit: 17179869184\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am using tensorflow2.0. The version of cuda installed on my system is cuda 10.0.\r\nI installed tensorflow-gpu. My machine has a gpu-device available and I am trying to get tensorflow-gpu to work.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nOutput:\r\n2019-11-14 22:27:38.319853: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-11-14 22:27:38.327036: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\r\n2019-11-14 22:27:38.328593: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560728b482f0 executing computations on platform Host. Devices:\r\n2019-11-14 22:27:38.328624: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-14 22:27:38.330948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-14 22:27:40.529184: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560728c428e0 executing computations on platform CUDA. Devices:\r\n2019-11-14 22:27:40.529229: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0\r\n2019-11-14 22:27:40.529242: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla V100-PCIE-16GB, Compute Capability 7.0\r\n2019-11-14 22:27:40.531923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\r\npciBusID: 0000:86:00.0\r\n2019-11-14 22:27:40.532794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\r\npciBusID: 0000:d8:00.0\r\n2019-11-14 22:27:40.532945: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs\r\n2019-11-14 22:27:40.533020: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs\r\n2019-11-14 22:27:40.533090: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs\r\n2019-11-14 22:27:40.533159: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs\r\n2019-11-14 22:27:40.533228: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs\r\n2019-11-14 22:27:40.533294: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs\r\n2019-11-14 22:27:40.533364: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs\r\n2019-11-14 22:27:40.533381: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2019-11-14 22:27:40.533434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-14 22:27:40.533451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 \r\n2019-11-14 22:27:40.533463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y \r\n2019-11-14 22:27:40.533474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N \r\nFalse\r\n", "comments": ["@shahdev ,\r\nHi,I was able to run the code without any issues for CUDA 10.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/b41a35317e88d7dc3f5c0d5ae7c402fb/34287.ipynb) of colab.Thanks!", "You need to add cuda, cudnn, cupti to the environment path.\r\nSee https://www.tensorflow.org/install/gpu#linux_setup", "@shahdev ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34287\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34287\">No</a>\n", "\r\n\r\n\r\n\r\n\r\nUbuntu 18.04\r\nTensorflow 2.1 GPU supported \r\nTensorRT 6.0\r\nCUDA 10.2 \r\nCudnn 7.6.5\r\n\r\nThis is what I got : \r\n\r\n```\r\nCannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-06 10:30:18.954700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-06 10:30:18.954795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-04-06 10:30:18.955863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\nKilled\r\n\r\n```\r\n\r\n", "OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nTensorFlow installed from (source or binary): using pip install tensorflow-gpu\r\nTensorFlow version: 2.1\r\nPython version: 3.7.6\r\nInstalled using virtualenv? pip? conda?: pip\r\n2020-04-07 17:04:42.405702: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-07 17:04:42.517234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-07 17:04:42.522151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-04-07 17:04:42.525055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 5139284911413389584\r\n]", "Tried to re-install the packages and this error shows up :\r\n\r\n\r\n`Can't identify the cuda device. Running on device 0 Segmentation fault (core dumped)`", "@shahdev if you have `tensorflow 2.0` and `cuda 10.0`, you don't need to install ` tensorflow-gpu`. Check the [tables](https://www.tensorflow.org/install/source#tested_build_configurations) here. Maybe this is the reason for your error?\r\n\r\nI have the same error with `tensorflow 2.1` and  `cuda 10.2` (I am trying to get `cuda 10.1` as suggested in the above link)."]}, {"number": 34286, "title": "TFLite crash (SIGABRT) while running Conv3D on Android", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Build environment is Linux Ubuntu 16.04 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Runtime environment is Samsung s8+\r\n- TensorFlow installed from (source or binary): built from source with select-tf-ops using: \r\n```\r\nbazel build --cxxopt='--std=c++11' -c opt \\\r\n --config=android_arm --config=monolithic \\\r\n //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops\r\n```\r\n- TensorFlow version: 1.15.0rc2\r\n- Keras version: 2.2.4-tf\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: not relevant\r\n\r\n**Describe the current behavior**\r\nI am trying to get a network (with conv3d ops) to run on my Android system using TFLite. I have followed all the steps mentioned [here](https://www.tensorflow.org/lite/guide/ops_select), and I can convert the network without issue. During runtime, I also appear to be able to load the converted tflite model without issue, however, during my call to `runForMultipleInputsOutputs()`, tflite crashes giving me a SIBABRT coming from libtensorflowlite_flex_jni.so (full stack trace below).\r\n\r\n**Describe the expected behavior**\r\nI expect it to not crash when running\r\n\r\n**Code to reproduce the issue**\r\nI made a dummy network to try and isolate the issue. I built the network using the following:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Conv3D(1, (4, 4, 4), input_shape=(4, 8, 8, 1), name='conv'))\r\nmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n                      optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\r\n                      metrics=[tf.keras.metrics.categorical_accuracy])\r\n\r\nx = np.random.random((1, 4, 8, 8, 1))\r\ny = np.random.random((1, 1, 5, 5, 1))\r\nmodel.train_on_batch(x, y)\r\nmodel.predict(x)\r\n\r\n# Save tf.keras model in HDF5 format\r\nkeras_file = \"conv3d.h5\"\r\ntf.keras.models.save_model(model, keras_file)\r\n\r\n# Convert the model to tflite format\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"conv3d.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nI then load and run the model on the android using ByteBuffers to hold the input/outputs. I can provide this code if requested, but I don't suspect it to be the problem as I use it for other working projects. I'm confident that this is a conv3d issue, because I have also built a **conv2d** dummy network, using the exact same build procedure + runtime environment, and it runs without crashing.\r\n\r\n**Other info / logs**\r\nThe full android backtrace during the call to runForMultipleInputsOutputs():\r\n```\r\nA/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\nA/DEBUG: Build fingerprint: 'samsung/dream2ltexx/dream2lte:9/PPR1.180610.011/G955FXXS5DSI1:user/release-keys'\r\nA/DEBUG: Revision: '10'\r\nA/DEBUG: ABI: 'arm'\r\nA/DEBUG: pid: 6774, tid: 6817, name: Thread-2  >>> com.segmentation.qussegserviceNVW <<<\r\nA/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\r\nA/DEBUG:     r0  00000000  r1  00001aa1  r2  00000006  r3  00000008\r\nA/DEBUG:     r4  00001a76  r5  00001aa1  r6  b7628eac  r7  0000010c\r\nA/DEBUG:     r8  b7629014  r9  b7628fa0  r10 b762903c  r11 e4095c70\r\nA/DEBUG:     ip  b7628e48  sp  b7628e98  lr  e6c73e71  pc  e6c6ae62\r\nA/DEBUG: backtrace:\r\nA/DEBUG:     #00 pc 0001ce62  /system/lib/libc.so (abort+58)\r\nA/DEBUG:     #01 pc 002181cd  /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/lib/arm/libtensorflowlite_flex_jni.so\r\nA/DEBUG:     #02 pc 002213fd  /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/lib/arm/libtensorflowlite_flex_jni.so\r\nA/DEBUG:     #03 pc 00225795  /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/lib/arm/libtensorflowlite_flex_jni.so\r\nA/DEBUG:     #04 pc 0021fb63  /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/lib/arm/libtensorflowlite_flex_jni.so\r\nA/DEBUG:     #05 pc 00335dcd  /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/lib/arm/libtensorflowlite_flex_jni.so\r\nA/DEBUG:     #06 pc 00338313  /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/lib/arm/libtensorflowlite_flex_jni.so\r\nA/DEBUG:     #07 pc 0020925b  /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/lib/arm/libtensorflowlite_flex_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+26)\r\nA/DEBUG:     #08 pc 00415879  /system/lib/libart.so (art_quick_generic_jni_trampoline+40)\r\nA/DEBUG:     #09 pc 00411375  /system/lib/libart.so (art_quick_invoke_stub_internal+68)\r\nA/DEBUG:     #10 pc 003ea57b  /system/lib/libart.so (art_quick_invoke_static_stub+222)\r\nA/DEBUG:     #11 pc 000a1627  /system/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+154)\r\nA/DEBUG:     #12 pc 001e88c9  /system/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+236)\r\nA/DEBUG:     #13 pc 001e33b7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+814)\r\nA/DEBUG:     #14 pc 003e60af  /system/lib/libart.so (MterpInvokeStatic+130)\r\nA/DEBUG:     #15 pc 00404294  /system/lib/libart.so (ExecuteMterpImpl+14612)\r\nA/DEBUG:     #16 pc 001aa16c  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/base.apk_6774_6774 (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.run+164)\r\nA/DEBUG:     #17 pc 001c7b33  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2760711098+378)\r\nA/DEBUG:     #18 pc 001cc219  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\nA/DEBUG:     #19 pc 001e339f  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\nA/DEBUG:     #20 pc 003e50d3  /system/lib/libart.so (MterpInvokeVirtual+442)\r\nA/DEBUG:     #21 pc 00404114  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\nA/DEBUG:     #22 pc 001a9962  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/base.apk_6774_6774 (deleted) (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs+10)\r\nA/DEBUG:     #23 pc 001c7b33  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2760711098+378)\r\nA/DEBUG:     #24 pc 001cc15f  /system/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+82)\r\nA/DEBUG:     #25 pc 003d8bb9  /system/lib/libart.so (artQuickToInterpreterBridge+880)\r\nA/DEBUG:     #26 pc 004158ff  /system/lib/libart.so (art_quick_to_interpreter_bridge+30)\r\nA/DEBUG:     #27 pc 0001c0fd  /dev/ashmem/dalvik-jit-code-cache_6774_6774 (deleted) (com.segmentation.qussegserviceNVW.TensorFlowSegmentRunner$SegNetRunner.segmentChunk+492)\r\nA/DEBUG:     #28 pc 004113bb  /system/lib/libart.so (art_quick_osr_stub+42)\r\nA/DEBUG:     #29 pc 0024d8a9  /system/lib/libart.so (art::jit::Jit::MaybeDoOnStackReplacement(art::Thread*, art::ArtMethod*, unsigned int, int, art::JValue*)+1388)\r\nA/DEBUG:     #30 pc 003e9aab  /system/lib/libart.so (MterpMaybeDoOnStackReplacement+86)\r\nA/DEBUG:     #31 pc 00410bf4  /system/lib/libart.so (ExecuteMterpImpl+66164)\r\nA/DEBUG:     #32 pc 0002e7b8  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/base.apk!classes2.dex_6774_6774 (deleted) (com.segmentation.qussegserviceNVW.TensorFlowSegmentRunner$SegNetRunner.segmentChunk+76)\r\nA/DEBUG:     #33 pc 001c7b33  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2760711098+378)\r\nA/DEBUG:     #34 pc 001cc219  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\nA/DEBUG:     #35 pc 001e339f  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\nA/DEBUG:     #36 pc 003e5f61  /system/lib/libart.so (MterpInvokeDirect+196)\r\nA/DEBUG:     #37 pc 00404214  /system/lib/libart.so (ExecuteMterpImpl+14484)\r\nA/DEBUG:     #38 pc 0002e750  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/base.apk!classes2.dex_6774_6774 (deleted) (com.segmentation.qussegserviceNVW.TensorFlowSegmentRunner$SegNetRunner.access$100)\r\nA/DEBUG:     #39 pc 001c7b33  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2760711098+378)\r\nA/DEBUG:     #40 pc 001cc15f  /system/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+82)\r\nA/DEBUG:     #41 pc 003d8bb9  /system/lib/libart.so (artQuickToInterpreterBridge+880)\r\nA/DEBUG:     #42 pc 004158ff  /system/lib/libart.so (art_quick_to_interpreter_bridge+30)\r\nA/DEBUG:     #43 pc 0001b5fd  /dev/ashmem/dalvik-jit-code-cache_6774_6774 (deleted) (com.segmentation.qussegserviceNVW.TensorFlowSegmentRunner.segmentFrame+604)\r\nA/DEBUG:     #44 pc 00411375  /system/lib/libart.so (art_quick_invoke_stub_internal+68)\r\nA/DEBUG:     #45 pc 003ea479  /system/lib/libart.so (art_quick_invoke_stub+224)\r\nA/DEBUG:     #46 pc 000a1615  /system/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+136)\r\nA/DEBUG:     #47 pc 001e88c9  /system/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+236)\r\nA/DEBUG:     #48 pc 001e33b7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+814)\r\nA/DEBUG:     #49 pc 003e50d3  /system/lib/libart.so (MterpInvokeVirtual+442)\r\nA/DEBUG:     #50 pc 00404114  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\nA/DEBUG:     #51 pc 0002edb4  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/base.apk!classes2.dex_6774_6774 (deleted) (com.segmentation.qussegserviceNVW.TensorFlowSegmentRunner.segmentCine+40)\r\nA/DEBUG:     #52 pc 001c7b33  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2760711098+378)\r\nA/DEBUG:     #53 pc 001cc219  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\nA/DEBUG:     #54 pc 001e339f  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\nA/DEBUG:     #55 pc 003e5ca3  /system/lib/libart.so (MterpInvokeInterface+1010)\r\nA/DEBUG:     #56 pc 00404314  /system/lib/libart.so (ExecuteMterpImpl+14740)\r\nA/DEBUG:     #57 pc 00028310  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/com.segmentation.qussegserviceNVW-52gMpws9Z9fl4lOXs6XvOw==/base.apk!classes2.dex_6774_6774 (deleted) (com.segmentation.qussegserviceNVW.CinePlayerActivity$3$1.run+20)\r\nA/DEBUG:     #58 pc 001c7b33  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2760711098+378)\r\nA/DEBUG:     #59 pc 001cc15f  /system/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+82)\r\nA/DEBUG:     #60 pc 003d8bb9  /system/lib/libart.so (artQuickToInterpreterBridge+880)\r\nA/DEBUG:     #61 pc 004158ff  /system/lib/libart.so (art_quick_to_interpreter_bridge+30)\r\nA/DEBUG:     #62 pc 00411375  /system/lib/libart.so (art_quick_invoke_stub_internal+68)\r\nA/DEBUG:     #63 pc 003ea479  /system/lib/libart.so (art_quick_invoke_stub+224)\r\nA/DEBUG:     #64 pc 000a1615  /system/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+136)\r\nA/DEBUG:     #65 pc 0034b0c5  /system/lib/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+52)\r\nA/DEBUG:     #66 pc 0034be1d  /system/lib/libart.so (art::InvokeVirtualOrInterfaceWithJValues(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue*)+320)\r\nA/DEBUG:     #67 pc 0036d203  /system/lib/libart.so (art::Thread::CreateCallback(void*)+866)\r\nA/DEBUG:     #68 pc 00064899  /system/lib/libc.so (__pthread_start(void*)+140)\r\nA/DEBUG:     #69 pc 0001e329  /system/lib/libc.so (__start_thread+24)\r\n```\r\n\r\n", "comments": ["Thanks for the report. Have you tried with a non-optimized build? That might help provide symbols for the crash stack.\r\n\r\nAssigning to @miaout17 for further assistance.", "@jdduke: I just tried rebuilding my .aar without `-c opt` and it did not change anything in the crash stack.\r\n\r\nAlso, as a small update: this issue does not seems to be present when I build the test network using tensorflow 2.0. Unfortunately, the actual keras network I am trying to work with was trained using v1.13.1 so that does not solve my issue.", "I'm curious, if you change your conversion target ops to just\r\n```\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n```\r\nDoes that make any difference?", "@jdduke: Without the SELECT_OPS, my converter script fails to create the tflite model. It gives the following errors:\r\n```\r\n2019-11-26 18:14:50.278227: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node conv/kernel/Assign doesn't exist in graph\r\nTraceback (most recent call last):\r\n  File \"convert_conv3d\", line 23, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 983, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-11-26 18:14:51.414259: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Conv3D\r\n2019-11-26 18:14:51.414371: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4 operators, 7 arrays (0 quantized)\r\n2019-11-26 18:14:51.415422: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4 operators, 7 arrays (0 quantized)\r\n2019-11-26 18:14:51.415487: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (0 quantized)\r\n2019-11-26 18:14:51.415521: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 2 operators, 5 arrays (0 quantized)\r\n2019-11-26 18:14:51.415532: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 2 operators, 5 arrays (0 quantized)\r\n2019-11-26 18:14:51.415576: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2019-11-26 18:14:51.415583: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 1001\r\n2019-11-26 18:14:51.416249: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD. Here is a list of operators for which you will need custom implementations: Conv3D.\r\n```", "Ah, sorry, what I meant to write was change the target ops to just:\r\n```\r\nconverter.target_ops = [lite.OpsSet.SELECT_TF_OPS]\r\n```", "Oho! that appears to have solved it!! \r\nGreat. Thank you for the suggestion", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34286\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34286\">No</a>\n"]}, {"number": 34285, "title": "[Intel MKL] Adding BFloat16 unit tests for MKL layout pass", "body": "This PR makes changes to the layout pass test to enable BFloat16 unit tests.", "comments": ["Thanks for the review @penpornk!"]}, {"number": 34284, "title": "[XLA] Enhancements to algebraic_simplifier", "body": "Motivation: algebraic simplification => rsqrt(Inv(pow^2(rsqrt(pow^-2(x))))) = |x| => x\r\nThis pattern found in one of the consumers of BatchNormForwardTraining. The producer of this pattern is actually the inverse sqrt of variance+epsilon which is always positive. Encoded this condition along with a few others when handling the transform |x| = x.\r\n\r\nI had been working on mobilenet performance on the batchnorm issue(enabling fp16 cudnn batchnorm thunk to half kernels) which lead me to this optimization opportunity. With this change the graph is cleaner and also avoids having a multioutput fused kernel. There are quite a few instances of this pattern occurring in the graph. The gains with this optimization is ~1-2% for mobilenet.\r\n\r\nThis PR adds cases to HandleRsqrt, HandleSqrt, HandleMultiply (pow^2 gets converted to multiply) and HandleAbs. HandleAbs does the transform |x|=x when x is positive.\r\n\r\n<img width=\"1680\" alt=\"Screen Shot 2019-11-14 at 11 42 03 AM\" src=\"https://user-images.githubusercontent.com/42984676/68891150-26625280-06d5-11ea-894f-092f30400b18.png\">\r\n", "comments": ["Sorry, @akuegel is OOO.  Assigning to @bixia1 ", "Hi @bixia1 ... thanks for pointing out the NAN handling characteristics in Tensorflow. I have made modifications accordingly. Please note that now I have 2 functions: `IsNonNegative` and `IsPositive`. `HandleAbs` uses `IsNonNegative` to make decisions while all other transforms verify that the operand is strictly positive (using `IsPositive`). Also, note that although the transform rsqrt(1/x)-> sqrt(x) would work for any non-zero value, I chose to be more conservative since isolating just non-zero value would be trickier than just isolating positive values and this conservative constraint would still work for our current use case. I have assumed that rsqrt(variance+epsilon) always positive to be the base case (ground truth). More cases can be added as when required.", "Sorry for the late commit..I was OOO."]}, {"number": 34283, "title": "copy paste error for value_embeddings = token_embedding(query_input)", "body": "Should be value_input?\r\n\r\nThank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["This is fixed now. Thanks!"]}, {"number": 34282, "title": "tf-nightly: 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tf_nightly-2.1.0.dev20191114\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: NVIDIA p100\r\n\r\nLooks like the latest nightly builds of TensorFlow require CUDA 10.1 for GPU support, is that intentional?\r\n\r\n```\r\n> import tensorflow as tf\r\n```\r\n\r\n```\r\n2019-11-14 16:21:07.535467: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2019-11-14 16:21:07.535493: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```", "comments": ["If the commit message is anything to go by, it is intentional: https://github.com/tensorflow/tensorflow/commit/183c4b74a932c281b4f80dba4ab7e005dc154481\r\n\r\nProbably is related to pytorch no longer officially supporting CUDA 10.0.", "Good catch. Thanks!"]}, {"number": 34281, "title": "Support LogicalDevice in MirroredStrategy config", "body": "PiperOrigin-RevId: 280290757\r\nChange-Id: I52dfff634e6e0ccdc81cd5cce682d7df3499b618", "comments": []}, {"number": 34280, "title": "Add ExtractImagePatches op to tensorflow lite (flex) whitelist", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): 1.14 top of the tree 14 Nov2019 (last checkin 00fad90125b18b80fe054de1055770cfb8fe4ba3)\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n2019-11-14 17:57:00.806373: W tensorflow/lite/toco/tflite/operator.cc:2661] Op ExtractImagePatches is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-11-14 17:57:00.806488: W tensorflow/lite/toco/tflite/operator.cc:2661] Op ExtractImagePatches is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-11-14 17:57:00.806577: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n\r\n", "comments": ["@nstyler, Please provide the exact sequence of commands / steps that you executed before running into the problem", "Hi! Thanks for a quick reply!\r\n\r\nI used the next steps:\r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\"aIN256.pb\", [\"Placeholder_1\"], [\"Squeeze\"])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\n\r\nat this stage I get the error about ExtractImagePatches not been whitelisted for the TensorFlow Lite flex op set.\r\nobviously aIN256,pb uses tf.extract_image_patches, - so we got the error\r\n", "Having the same problem here, have a model which uses ExtractImagePatches and I am unable to convert it to TFLITE using select_tf_ops since the op its not whitelisted.", "+1", "Hello, all.\r\nExtractImagePatches op was added to tensorflow lite(flex) whitelist.\r\nhttps://github.com/tensorflow/tensorflow/commit/7b180f700755b0a3fc0eb9de349f7caacc422d2d", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34280\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34280\">No</a>\n", "Hey! Will it be possible for anyone to explain how do we use  tensorflow lite(flex)  to convert model having ExtractImagePatches op to tflite?"]}, {"number": 34279, "title": "2.1-RC0 cherry-pick request: [tf.data] Improvements to cancellation logic.", "body": "This CL:\r\n    - makes sure that a previously cancelled input pipeline won't hang in ParallelMapDataset kernel\r\n    - excludes errors::Cancelled() from being ignored by the IgnoreErrorsDataset kernel to avoid infinitely looping after a cancellation\r\n    \r\n    PiperOrigin-RevId: 280425330\r\n    Change-Id: If210c94c59e3b33b170e77aea6c87308241364e6\r\n", "comments": ["@jsimsa \r\nJiri, https://github.com/tensorflow/tensorflow/pull/34241 has been merged, do you want to remove that from this commit or is it a noop.", "I resolved the merge conflict.", "Alternatively, I can create a new cherrypick with just one commit if you would prefer that.", "@jsimsa awesome thanks, once the tests complete. I will merge it in"]}, {"number": 34278, "title": "speech_commands: Range cannot be empty (low >= high) unless no samples are taken", "body": "I'm using the speech_commands example and trying to load my own set of wav files, which I've organized according to the tutorial. Here's the command I'm entering on the terminal:\r\npython3 /Users/etc/train.py \\\r\n--data_url= \\\r\n--data_dir=/Users/etc/trial_classes \\\r\n--wanted_words=randomclass1,randomclass2 \\\r\n--sample_rate=44100\r\n\r\nIt's finding the files (so I've truncated the filepaths with \"etc\" below & above), but raises this error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/etc/speech_commands/train.py\", line 508, in <module>\r\n    tf.compat.v1.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/etc/speech_commands/train.py\", line 237, in main\r\n    FLAGS.background_volume, time_shift_samples, 'training', sess)\r\n  File \"/Users/etc/speech_commands/input_data.py\", line 575, in get_data\r\n    background_index = np.random.randint(len(self.background_data))\r\n  File \"mtrand.pyx\", line 992, in mtrand.RandomState.randint\r\nValueError: Range cannot be empty (low >= high) unless no samples are taken\r\n\r\nI'm using\r\nMacOs 10.13.6\r\nTensorflow version 2.0.0\r\nPython version: 3.7.3\r\n\r\nNew to this so I appreciate the patience if I've done something dumb. Thanks!", "comments": ["Okay figured this out. The _background_noise_ folder has to be included in the directory alongside the folders with the wav files. On further reading I probably could have assumed this from reading the tutorial, but maybe worth clarifying in the paragraph on \"Custom Training Data.\" (Here's the tutorial I'm referring to: https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md). "]}, {"number": 34277, "title": "C++ API producing incorrect model metaparams", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 2.0 and 1.15\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nAn autoencoder model consisting only of standard keras Dense layers is converted into a tflite model. This model can be loaded and inspected with the Python API. The output there is consistent with the output from the visualize.py script.\r\n```\r\nInput detail:  {'name': 'input_1', 'index': 1, 'shape': array([ 1, 90], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\r\nOutput detail:  {'name': 'Identity', 'index': 0, 'shape': array([ 1, 90], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\r\n```\r\nWhen loading the very same model with the C++ API, I get ridicolous large results for the number of inputs/outputs/nodes. \r\n\r\nThe C++ functions that were used to inspect the model are:\r\n```\r\nstd::unique_ptr<tflite::Interpreter> interpreter = BuildInterpreter(*model);\r\n\r\nLOG(INFO) << \"tensors size: \" << interpreter->tensors_size() << std::endl;\r\nLOG(INFO) << \"nodes size: \" << interpreter->nodes_size() << std::endl;\r\nLOG(INFO) << \"inputs: \" << interpreter->inputs().size() << std::endl;\r\nLOG(INFO) << \"input(0) name: \" << interpreter->GetInputName(0) << std::endl;\r\nLOG(INFO) << \"outputs: \" << interpreter->outputs().size() << std::endl;\r\nLOG(INFO) << \"output(0) name: \" << interpreter->GetOutputName(0) << std::endl;\r\n\r\nint t_size = interpreter->tensors_size();\r\nfor (int i = 0; i < t_size; i++) {\r\n  LOG(INFO) << i << \": \" << interpreter->tensor(i)->name << \", \" \r\n            << interpreter->tensor(i)->bytes << \", \"\r\n            << interpreter->tensor(i)->type << \", \"\r\n            << interpreter->tensor(i)->params.scale << \", \"\r\n            << interpreter->tensor(i)->params.zero_point << std::endl;\r\n}\r\nstd::cout << \"End of test\" << std::endl;\r\n```\r\nThis produces the following output:\r\n```\r\ntensors size: 21\r\nnodes size: 11936128518282651046\r\ninputs: 25344\r\ninput(0) name: Identity\r\noutputs: 18446744073709501604\r\noutput(0) name: Identity\r\n0: Identity, 360, 1, 0, 0\r\n1: input_1, 360, 1, 0, 0\r\n2: model/dense/MatMul/ReadVariableOp/transpose, 3600, 9, 0.00187181, 0\r\n3: model/dense/MatMul_bias, 160, 1, 0, 0\r\n4: model/dense/Relu, 160, 1, 0, 0\r\n5: model/dense_1/MatMul/ReadVariableOp/transpose, 1600, 1, 0, 0\r\n6: model/dense_1/MatMul_bias, 40, 1, 0, 0\r\n7: model/dense_1/Relu, 40, 1, 0, 0\r\n8: model/dense_2/MatMul/ReadVariableOp/transpose, 1600, 1, 0, 0\r\n9: model/dense_2/MatMul_bias, 160, 1, 0, 0\r\n10: model/dense_2/Relu, 160, 1, 0, 0\r\n11: model/dense_3/MatMul/ReadVariableOp/transpose, 3600, 9, 0.00208381, 0\r\n12: model/dense_3/MatMul_bias, 360, 1, 0, 0\r\n13: End of test\r\n```\r\n\r\nThe code to create the tflite model to inspect can be found on my repo (https://github.com/DocDriven/tflite-cpp-api-tests). All relevant files are named `simple_ae.*`. \r\n\r\nI suspect the C++ API to be broken at some point, as the models seem to be fine. Same results for TF 1.x and TF2.0. Trying different models yields the exact same ridicolous values, independently from their size.\r\n", "comments": ["@liyunlu0618 : Upon further investigation of this problem I noticed that the number of detected nodes, inputs and outputs is independent from the model architecture. Varying the input/output size and/or the number of layers do not affect the output of `interpreter->nodes_size()`, `interpreter->inputs().size()` and `interpreter->outputs().size()`. The number of tensors does change, but the hidden tensors phenomenon remains.", "This is expected behavior, the `tensors()` array returned in C++ is the full list of all tensors. You can juse the `inputs()` indices to inspect/use/query just the inputs, same with `outputs()`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34277\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34277\">No</a>\n", "@jdduke \r\nCorrect me if I am wrong, but I am using the inputs/outputs methods, but these have no reasonable size (1.8e19 outputs and 2.5e4 inputs). It is hard to believe that this is expected behavior for a model with 90 floats as inputs/outputs.\r\nPlease reopen the issue.", "Ah, sorry, I didn't notice the output values from your first post.  Can I ask how you're building the C++ API? Are you building it as a shared library then using it in your own app? Does your model work with our [minimal C++ example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/BUILD#L11)?", "No worries. I am using the devel-py3 docker image to generate the libtensorflowlite.so library. I built it with the extended runtime because not all my used ops were supported by tflite. I had an issue regarding the build here: #33980\r\n\r\nI built your minimal.cc example and executed it with my model. Notice that I did not provide inputs/read outputs. Upon inspection, this look reasonable to me:\r\n\r\n```\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2019-11-20 11:15:09.648738: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\n=== Pre-invoke Interpreter State ===\r\nInterpreter has 37 tensors and 14 nodes\r\nInputs: 16\r\nOutputs: 13\r\n\r\nTensor   0 dense/BiasAdd        kTfLiteFloat32  kTfLiteArenaRw        160 bytes ( 0.0 MB)  1 40\r\nTensor   1 dense/LeakyRelu      kTfLiteFloat32  kTfLiteArenaRw        160 bytes ( 0.0 MB)  1 40\r\nTensor   2 dense/MatMul_bias    kTfLiteFloat32   kTfLiteMmapRo        160 bytes ( 0.0 MB)  40\r\nTensor   3 dense/kernel/transpose kTfLiteInt8   kTfLiteMmapRo       3600 bytes ( 0.0 MB)  40 90\r\nTensor   4 dense_1/BiasAdd      kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  1 10\r\nTensor   5 dense_1/MatMul_bias  kTfLiteFloat32   kTfLiteMmapRo         40 bytes ( 0.0 MB)  10\r\nTensor   6 dense_1/kernel/transpose kTfLiteFloat32   kTfLiteMmapRo       1600 bytes ( 0.0 MB)  10 40\r\nTensor   7 dense_2/MatMul_bias  kTfLiteFloat32   kTfLiteMmapRo         40 bytes ( 0.0 MB)  10\r\nTensor   8 dense_2/kernel/transpose kTfLiteFloat32   kTfLiteMmapRo       1600 bytes ( 0.0 MB)  10 40\r\nTensor   9 dense_3/BiasAdd      kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor  10 dense_3/LeakyRelu    kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor  11 dense_3/MatMul_bias  kTfLiteFloat32   kTfLiteMmapRo        160 bytes ( 0.0 MB)  40\r\nTensor  12 dense_3/kernel/transpose kTfLiteFloat32   kTfLiteMmapRo       1600 bytes ( 0.0 MB)  40 10\r\nTensor  13 dense_4/BiasAdd      kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor  14 dense_4/MatMul_bias  kTfLiteFloat32   kTfLiteMmapRo        360 bytes ( 0.0 MB)  90\r\nTensor  15 dense_4/kernel/transpose kTfLiteInt8   kTfLiteMmapRo       3600 bytes ( 0.0 MB)  90 40\r\nTensor  16 input_1              kTfLiteFloat32  kTfLiteArenaRw        360 bytes ( 0.0 MB)  1 90\r\nTensor  17 lambda/Exp           kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  1 10\r\nTensor  18 lambda/add           kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor  19 lambda/mul           kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  1 10\r\nTensor  20 lambda/mul_1         kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor  21 lambda/random_normal kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor  22 lambda/random_normal/RandomStandardNormal kTfLiteFloat32  kTfLiteDynamic          4 bytes ( 0.0 MB) \r\nTensor  23 lambda/random_normal/mean kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  24 lambda/random_normal/mul kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor  25 lambda/random_normal/shape kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor  26 lambda/random_normal/stddev kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  27 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  28 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  29 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  30 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  31 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  32 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  33 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  34 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  35 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  36 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\n\r\nNode   0 Operator Custom Name FlexRandomStandardNormal\r\n  Inputs: 25\r\n  Outputs: 22\r\nNode   1 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 16 3 2\r\n  Outputs: 0\r\nNode   2 Operator Builtin Code  18 MUL\r\n  Inputs: 22 26\r\n  Outputs: 24\r\nNode   3 Operator Builtin Code   0 ADD\r\n  Inputs: 24 23\r\n  Outputs: 21\r\nNode   4 Operator Builtin Code  98 LEAKY_RELU\r\n  Inputs: 0\r\n  Outputs: 1\r\nNode   5 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 1 6 5\r\n  Outputs: 4\r\nNode   6 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 1 8 7\r\n  Outputs: 19\r\nNode   7 Operator Builtin Code  47 EXP\r\n  Inputs: 19\r\n  Outputs: 17\r\nNode   8 Operator Builtin Code  18 MUL\r\n  Inputs: 17 21\r\n  Outputs: 20\r\nNode   9 Operator Builtin Code   0 ADD\r\n  Inputs: 4 20\r\n  Outputs: 18\r\nNode  10 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 18 12 11\r\n  Outputs: 9\r\nNode  11 Operator Builtin Code  98 LEAKY_RELU\r\n  Inputs: 9\r\n  Outputs: 10\r\nNode  12 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 10 15 14\r\n  Outputs: 13\r\nNode  13 Operator Custom Name TfLiteFlexDelegate\r\n  Inputs: 25\r\n  Outputs: 22\r\n\r\n\r\n=== Post-invoke Interpreter State ===\r\nInterpreter has 37 tensors and 14 nodes\r\nInputs: 16\r\nOutputs: 13\r\n\r\nTensor   0 dense/BiasAdd        kTfLiteFloat32  kTfLiteArenaRw        160 bytes ( 0.0 MB)  1 40\r\nTensor   1 dense/LeakyRelu      kTfLiteFloat32  kTfLiteArenaRw        160 bytes ( 0.0 MB)  1 40\r\nTensor   2 dense/MatMul_bias    kTfLiteFloat32   kTfLiteMmapRo        160 bytes ( 0.0 MB)  40\r\nTensor   3 dense/kernel/transpose kTfLiteInt8   kTfLiteMmapRo       3600 bytes ( 0.0 MB)  40 90\r\nTensor   4 dense_1/BiasAdd      kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  1 10\r\nTensor   5 dense_1/MatMul_bias  kTfLiteFloat32   kTfLiteMmapRo         40 bytes ( 0.0 MB)  10\r\nTensor   6 dense_1/kernel/transpose kTfLiteFloat32   kTfLiteMmapRo       1600 bytes ( 0.0 MB)  10 40\r\nTensor   7 dense_2/MatMul_bias  kTfLiteFloat32   kTfLiteMmapRo         40 bytes ( 0.0 MB)  10\r\nTensor   8 dense_2/kernel/transpose kTfLiteFloat32   kTfLiteMmapRo       1600 bytes ( 0.0 MB)  10 40\r\nTensor   9 dense_3/BiasAdd      kTfLiteFloat32  kTfLiteArenaRw        160 bytes ( 0.0 MB)  1 40\r\nTensor  10 dense_3/LeakyRelu    kTfLiteFloat32  kTfLiteArenaRw        160 bytes ( 0.0 MB)  1 40\r\nTensor  11 dense_3/MatMul_bias  kTfLiteFloat32   kTfLiteMmapRo        160 bytes ( 0.0 MB)  40\r\nTensor  12 dense_3/kernel/transpose kTfLiteFloat32   kTfLiteMmapRo       1600 bytes ( 0.0 MB)  40 10\r\nTensor  13 dense_4/BiasAdd      kTfLiteFloat32  kTfLiteArenaRw        360 bytes ( 0.0 MB)  1 90\r\nTensor  14 dense_4/MatMul_bias  kTfLiteFloat32   kTfLiteMmapRo        360 bytes ( 0.0 MB)  90\r\nTensor  15 dense_4/kernel/transpose kTfLiteInt8   kTfLiteMmapRo       3600 bytes ( 0.0 MB)  90 40\r\nTensor  16 input_1              kTfLiteFloat32  kTfLiteArenaRw        360 bytes ( 0.0 MB)  1 90\r\nTensor  17 lambda/Exp           kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  1 10\r\nTensor  18 lambda/add           kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  1 10\r\nTensor  19 lambda/mul           kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  1 10\r\nTensor  20 lambda/mul_1         kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  1 10\r\nTensor  21 lambda/random_normal kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  10\r\nTensor  22 lambda/random_normal/RandomStandardNormal kTfLiteFloat32  kTfLiteDynamic         40 bytes ( 0.0 MB)  10\r\nTensor  23 lambda/random_normal/mean kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  24 lambda/random_normal/mul kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  10\r\nTensor  25 lambda/random_normal/shape kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor  26 lambda/random_normal/stddev kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  27 (null)               kTfLiteInt8  kTfLiteArenaRw         90 bytes ( 0.0 MB)  1 90\r\nTensor  28 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor  29 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  30 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  31 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  32 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  33 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  34 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  35 (null)               kTfLiteInt8  kTfLiteArenaRw         40 bytes ( 0.0 MB)  1 40\r\nTensor  36 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\n\r\nNode   0 Operator Custom Name FlexRandomStandardNormal\r\n  Inputs: 25\r\n  Outputs: 22\r\nNode   1 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 16 3 2\r\n  Outputs: 0\r\nNode   2 Operator Builtin Code  18 MUL\r\n  Inputs: 22 26\r\n  Outputs: 24\r\nNode   3 Operator Builtin Code   0 ADD\r\n  Inputs: 24 23\r\n  Outputs: 21\r\nNode   4 Operator Builtin Code  98 LEAKY_RELU\r\n  Inputs: 0\r\n  Outputs: 1\r\nNode   5 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 1 6 5\r\n  Outputs: 4\r\nNode   6 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 1 8 7\r\n  Outputs: 19\r\nNode   7 Operator Builtin Code  47 EXP\r\n  Inputs: 19\r\n  Outputs: 17\r\nNode   8 Operator Builtin Code  18 MUL\r\n  Inputs: 17 21\r\n  Outputs: 20\r\nNode   9 Operator Builtin Code   0 ADD\r\n  Inputs: 4 20\r\n  Outputs: 18\r\nNode  10 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 18 12 11\r\n  Outputs: 9\r\nNode  11 Operator Builtin Code  98 LEAKY_RELU\r\n  Inputs: 9\r\n  Outputs: 10\r\nNode  12 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 10 15 14\r\n  Outputs: 13\r\nNode  13 Operator Custom Name TfLiteFlexDelegate\r\n  Inputs: 25\r\n  Outputs: 22\r\n\r\n```\r\n\r\nIf I can anymore provide information, do not hesitate to leave a message. I want to get this fixed ASAP.", "I suspect the issue here may just be to poor C++ ABI stability when using different compile flags or options across shared library boundaries, particularly if you're using a very different pipeline for your client app.\r\n\r\nI'd be curious if you see the same issues when using the C API as supported by [this shared library](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/c/BUILD#L21) using a slightly lower level API ( [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/c/c_api_test.cc#L29) is an example). We're seeing more and more issues that users are having trying to use the C++ API over a shared library boundary, and I think we'll probably focus on prioritizing the stable C ABI/API and perhaps offering a lightweight C++ wrapper on top of that for convenience. We definitely appreciate the feedback and want to get this resolved. I'll reopen this issue for tracking purposes.", "@jdduke \r\nI have tried to use the C API as you suggested. I am going to post all steps that I did, and ask you to confirm their correctness.\r\n\r\n1.) I tried to build the extended runtime via\r\n`bazel build --config=monolithic --define=with_select_tf_ops=true -c opt //tensorflow/lite/experimental/c:libtensorflowlite_c.so`\r\nThis worked, but the build only took around 8 seconds. This is way too short to build the extended runtime. Later, it turned out that it indeed was NOT built.\r\n\r\n2.) I had to add another two libraries to get my project to compile. The first was `absl` which I took from here (https://chromium.googlesource.com/external/github.com/abseil/abseil-cpp/+/refs/heads/master/). The other library was googletest, which I had to build with bazel first. I used the command from its root dir:\r\n`bazel build //:gtest`\r\nThis produced both a static and a dynamic library. I did not notice any differences for my app, regardless of which one I was using.\r\n\r\n3.) I copy pasted the test code you provided for me and all I did was to change the paths to my tflite model. It is the same model that generated the output above. The build was successful.\r\n\r\nHowever, I get the following, unsatisfying output when running the executable:\r\n\r\n```\r\n[==========] Running 10 tests from 2 test suites.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from CAPI\r\n[ RUN      ] CAPI.Version\r\n[       OK ] CAPI.Version (0 ms)\r\n[----------] 1 test from CAPI (0 ms total)\r\n\r\n[----------] 9 tests from CApiSimple\r\n[ RUN      ] CApiSimple.Smoke\r\nERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nERROR: Node number 0 (FlexRandomStandardNormal) failed to prepare.\r\n\r\n./capi_test.c:30: Failure\r\nExpected equality of these values:\r\n  TfLiteInterpreterAllocateTensors(interpreter)\r\n    Which is: 1\r\n  kTfLiteOk\r\n    Which is: 0\r\n[  FAILED  ] CApiSimple.Smoke (0 ms)\r\n[ RUN      ] CApiSimple.QuantizationParams\r\nERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nERROR: Node number 0 (FlexRandomStandardNormal) failed to prepare.\r\n\r\n./capi_test.c:100: Failure\r\nExpected equality of these values:\r\n  TfLiteInterpreterAllocateTensors(interpreter)\r\n    Which is: 1\r\n  kTfLiteOk\r\n    Which is: 0\r\n[  FAILED  ] CApiSimple.QuantizationParams (0 ms)\r\n[ RUN      ] CApiSimple.Delegate\r\nERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nERROR: Node number 0 (FlexRandomStandardNormal) failed to prepare.\r\n\r\n./capi_test.c:163: Failure\r\nValue of: delegate_prepared\r\n  Actual: false\r\nExpected: true\r\nSpeicherzugriffsfehler (Speicherabzug geschrieben)\r\n```\r\nThe last line is German (my mothertongue), but I do not know the exact translation. Basically, my computer tries to access memory addresses that it is not allowed to access (memory access error maybe?) Also, the extended runtime was not built apparently.\r\n\r\nCan you deduce from this log where the problem might be coming from? Thanks!", "> This worked, but the build only took around 8 seconds. This is way too short to build the extended runtime. Later, it turned out that it indeed was NOT built.\r\n\r\nWe actually deprecated that build flag a while back. Let me see about re-introducing it in a more targeted fashion for the C/C++ shared libraries.", "Is there any possibility to build the c library with native operators enabled? Or do I have to roll back to an image that still has the flag?", "If you want to try locally, you can roll back https://github.com/tensorflow/tensorflow/commit/e57a5671486445e13fbd935a2cbc979a44aeedf6#diff-866c5e896c5bfd544d4e642ed2e3d2bd, and try again with your build command.", "I was able to build it by rolling back to the parent commit of `e57a567`. I slightly modified the expected outputs here and there, but failed to work out the correct values for some of them.\r\n\r\nThe main thing is the retrieval of the outputs which bothers me. For example, take line 67 which should return the ByteSize of the output tensor. I expect this to be 90 * sizeof(float) = 360 byte, but I get 0 bytes. \r\nWhen inspecting the model with visualize.py, I see that the detected output vector `dense_4/BiasAdd` has shape `None`. So maybe something goes wrong during the conversion?\r\n\r\nHTML-Visualization\r\n[vae.tar.gz](https://github.com/tensorflow/tensorflow/files/3892506/vae.tar.gz)\r\n\r\nAlso, the executable dies at the same point (with a little more information this time):\r\n```\r\n[==========] Running 10 tests from 2 test suites.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from CAPI\r\n[ RUN      ] CAPI.Version\r\n[       OK ] CAPI.Version (0 ms)\r\n[----------] 1 test from CAPI (0 ms total)\r\n\r\n[----------] 9 tests from CApiSimple\r\n[ RUN      ] CApiSimple.Smoke\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2019-11-26 14:28:56.579157: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n./capi_test.c:66: Failure\r\nExpected equality of these values:\r\n  TfLiteTensorDim(output_tensor, 0)\r\n    Which is: 0\r\n  2\r\n./capi_test.c:67: Failure\r\nExpected equality of these values:\r\n  TfLiteTensorByteSize(output_tensor)\r\n    Which is: 0\r\n  sizeof(float) * 90\r\n    Which is: 360\r\n./capi_test.c:68: Failure\r\nExpected: (TfLiteTensorData(output_tensor)) != (nullptr), actual: NULL vs (nullptr)\r\n./capi_test.c:79: Failure\r\nExpected equality of these values:\r\n  TfLiteTensorCopyToBuffer(output_tensor, output.data(), output.size() * sizeof(float))\r\n    Which is: 1\r\n  kTfLiteOk\r\n    Which is: 0\r\n[  FAILED  ] CApiSimple.Smoke (11 ms)\r\n[ RUN      ] CApiSimple.QuantizationParams\r\n./capi_test.c:110: Failure\r\nExpected equality of these values:\r\n  input_params.scale\r\n    Which is: 0\r\n  0.003922f\r\n    Which is: 0.003922\r\n./capi_test.c:116: Failure\r\nExpected equality of these values:\r\n  TfLiteTensorCopyFromBuffer(input_tensor, input.data(), input.size() * sizeof(float))\r\n    Which is: 1\r\n  kTfLiteOk\r\n    Which is: 0\r\n[  FAILED  ] CApiSimple.QuantizationParams (1 ms)\r\n[ RUN      ] CApiSimple.Delegate\r\nERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n./capi_test.c:163: Failure\r\nValue of: delegate_prepared\r\n  Actual: false\r\nExpected: true\r\nSpeicherzugriffsfehler (Speicherabzug geschrieben)\r\n```\r\ngdb provides a little more information:\r\n```\r\nThread 1 \"capi_test\" received signal SIGSEGV, Segmentation fault.\r\n0x00007ffff1a58151 in TfLiteInterpreterInvoke ()\r\n   from /home/docdriven/projects/custom_op_test/src/libtensorflowlite_c.so\r\n```", "The delegate test failure is more or less expected, though it shouldn't seg fault. In your test, did you explicitly call `TfLiteInterpreterAllocateTensors` before checking the output tensor data? The output from the minimal example above looks correct, with the output tensor having dimension [1, 90] and non-null bytes allocated.", "I do not know if this happens under the hood, but my test is the exact file that you provided. So the whole test case is\r\n\r\n```\r\nTEST(CApiSimple, Smoke) {\r\n  TfLiteModel* model =\r\n      TfLiteModelCreateFromFile(\"vae_.tflite\");\r\n  ASSERT_NE(model, nullptr);\r\n\r\n  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n  ASSERT_NE(options, nullptr);\r\n  TfLiteInterpreterOptionsSetNumThreads(options, 2);\r\n\r\n  TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n  ASSERT_NE(interpreter, nullptr);\r\n\r\n  // The options/model can be deleted immediately after interpreter creation.\r\n  TfLiteInterpreterOptionsDelete(options);\r\n  TfLiteModelDelete(model);\r\n\r\n  ASSERT_EQ(TfLiteInterpreterAllocateTensors(interpreter), kTfLiteOk);\r\n  ASSERT_EQ(TfLiteInterpreterGetInputTensorCount(interpreter), 1);\r\n  ASSERT_EQ(TfLiteInterpreterGetOutputTensorCount(interpreter), 1);\r\n\r\n  std::array<int, 1> input_dims = {2};\r\n  ASSERT_EQ(TfLiteInterpreterResizeInputTensor(\r\n                interpreter, 0, input_dims.data(), input_dims.size()),\r\n            kTfLiteOk);\r\n  ASSERT_EQ(TfLiteInterpreterAllocateTensors(interpreter), kTfLiteOk);\r\n\r\n  TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);\r\n  ASSERT_NE(input_tensor, nullptr);\r\n  EXPECT_EQ(TfLiteTensorType(input_tensor), kTfLiteFloat32);\r\n  EXPECT_EQ(TfLiteTensorNumDims(input_tensor), 1);\r\n  EXPECT_EQ(TfLiteTensorDim(input_tensor, 0), 2);\r\n  EXPECT_EQ(TfLiteTensorByteSize(input_tensor), sizeof(float) * 2);\r\n  EXPECT_NE(TfLiteTensorData(input_tensor), nullptr);\r\n  EXPECT_STREQ(TfLiteTensorName(input_tensor), \"input_1\");\r\n\r\n  TfLiteQuantizationParams input_params =\r\n      TfLiteTensorQuantizationParams(input_tensor);\r\n  EXPECT_EQ(input_params.scale, 0.f);\r\n  EXPECT_EQ(input_params.zero_point, 0);\r\n\r\n  std::array<float, 2> input = {1.f, 3.f};\r\n  ASSERT_EQ(TfLiteTensorCopyFromBuffer(input_tensor, input.data(),\r\n                                       input.size() * sizeof(float)),\r\n            kTfLiteOk);\r\n\r\n  ASSERT_EQ(TfLiteInterpreterInvoke(interpreter), kTfLiteOk);\r\n\r\n  const TfLiteTensor* output_tensor =\r\n      TfLiteInterpreterGetOutputTensor(interpreter, 0);\r\n  ASSERT_NE(output_tensor, nullptr);\r\n  EXPECT_EQ(TfLiteTensorType(output_tensor), kTfLiteFloat32);\r\n  EXPECT_EQ(TfLiteTensorNumDims(output_tensor), 2);\r\n  EXPECT_EQ(TfLiteTensorDim(output_tensor, 0), 2);\r\n  EXPECT_EQ(TfLiteTensorByteSize(output_tensor), sizeof(float) * 90);\r\n  EXPECT_NE(TfLiteTensorData(output_tensor), nullptr);\r\n  EXPECT_STREQ(TfLiteTensorName(output_tensor), \"dense_4/BiasAdd\");\r\n\r\n  TfLiteQuantizationParams output_params =\r\n      TfLiteTensorQuantizationParams(output_tensor);\r\n  EXPECT_EQ(output_params.scale, 0.f);\r\n  EXPECT_EQ(output_params.zero_point, 0);\r\n\r\n  std::array<float, 90> output;\r\n  ASSERT_EQ(TfLiteTensorCopyToBuffer(output_tensor, output.data(),\r\n                                     output.size() * sizeof(float)),\r\n            kTfLiteOk);\r\n  EXPECT_EQ(output[0], 3.f);\r\n  EXPECT_EQ(output[1], 9.f);\r\n\r\n  TfLiteInterpreterDelete(interpreter);\r\n}\r\n```\r\nSo it gets called twice. Is this correct, and when exactly is allocation necessary?", "That looks correct. Whenever an input tensor (or tensors) is resized, an explicit allocation is required. I'll try to repro locally.", "Ah, right, so that resize call is kind of nonsensical for this graph. If you remove\r\n```\r\n std::array<int, 1> input_dims = {2};\r\n  ASSERT_EQ(TfLiteInterpreterResizeInputTensor(\r\n                interpreter, 0, input_dims.data(), input_dims.size()),\r\n            kTfLiteOk);\r\n  ASSERT_EQ(TfLiteInterpreterAllocateTensors(interpreter), kTfLiteOk);\r\n```\r\nfrom the test, it should proceed. Note that some of the other expectations are bogus for that vae.tflite model, but you should at least get meaningful output shapes. This test passes:\r\n\r\n```\r\nTEST(CApiSimple, Smoke) {\r\n  TfLiteModel* model =\r\n      TfLiteModelCreateFromFile(\"vae.tflite\");\r\n  ASSERT_NE(model, nullptr);\r\n\r\n  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n  ASSERT_NE(options, nullptr);\r\n  TfLiteInterpreterOptionsSetNumThreads(options, 2);\r\n\r\n  TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n  std::unique_ptr<TfLiteInterpreter, void (*)(TfLiteInterpreter*)>\r\n      interpreter_holder(interpreter, &TfLiteInterpreterDelete);\r\n  ASSERT_NE(interpreter, nullptr);\r\n\r\n  // The options/model can be deleted immediately after interpreter creation.\r\n  TfLiteInterpreterOptionsDelete(options);\r\n  TfLiteModelDelete(model);\r\n\r\n  ASSERT_EQ(TfLiteInterpreterAllocateTensors(interpreter), kTfLiteOk);\r\n  ASSERT_EQ(TfLiteInterpreterGetInputTensorCount(interpreter), 1);\r\n  ASSERT_EQ(TfLiteInterpreterGetOutputTensorCount(interpreter), 1);\r\n\r\n  /*\r\n  std::array<int, 1> input_dims = {2};\r\n  ASSERT_EQ(TfLiteInterpreterResizeInputTensor(\r\n                interpreter, 0, input_dims.data(), input_dims.size()),\r\n            kTfLiteOk);\r\n  ASSERT_EQ(TfLiteInterpreterAllocateTensors(interpreter), kTfLiteOk);\r\n  */\r\n\r\n  TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);\r\n  ASSERT_NE(input_tensor, nullptr);\r\n  EXPECT_EQ(TfLiteTensorType(input_tensor), kTfLiteFloat32);\r\n  EXPECT_EQ(TfLiteTensorNumDims(input_tensor), 2);\r\n  EXPECT_EQ(TfLiteTensorDim(input_tensor, 0), 1);\r\n  EXPECT_EQ(TfLiteTensorDim(input_tensor, 1), 90);\r\n  EXPECT_EQ(TfLiteTensorByteSize(input_tensor), sizeof(float) * 90);\r\n  EXPECT_NE(TfLiteTensorData(input_tensor), nullptr);\r\n  EXPECT_STREQ(TfLiteTensorName(input_tensor), \"input_1\");\r\n\r\n  TfLiteQuantizationParams input_params =\r\n      TfLiteTensorQuantizationParams(input_tensor);\r\n  EXPECT_EQ(input_params.scale, 0.f);\r\n  EXPECT_EQ(input_params.zero_point, 0);\r\n\r\n  std::array<float, 90> input = {1.f};\r\n  ASSERT_EQ(TfLiteTensorCopyFromBuffer(input_tensor, input.data(),\r\n                                       input.size() * sizeof(float)),\r\n            kTfLiteOk);\r\n\r\n  ASSERT_EQ(TfLiteInterpreterInvoke(interpreter), kTfLiteOk);\r\n\r\n  const TfLiteTensor* output_tensor =\r\n      TfLiteInterpreterGetOutputTensor(interpreter, 0);\r\n  ASSERT_NE(output_tensor, nullptr);\r\n  EXPECT_EQ(TfLiteTensorType(output_tensor), kTfLiteFloat32);\r\n  EXPECT_EQ(TfLiteTensorNumDims(output_tensor), 2);\r\n  EXPECT_EQ(TfLiteTensorDim(output_tensor, 0), 1);\r\n  EXPECT_EQ(TfLiteTensorDim(output_tensor, 1), 90);\r\n  EXPECT_EQ(TfLiteTensorByteSize(output_tensor), sizeof(float) * 90);\r\n  EXPECT_NE(TfLiteTensorData(output_tensor), nullptr);\r\n  EXPECT_STREQ(TfLiteTensorName(output_tensor), \"dense_4/BiasAdd\");\r\n\r\n  TfLiteQuantizationParams output_params =\r\n      TfLiteTensorQuantizationParams(output_tensor);\r\n  EXPECT_EQ(output_params.scale, 0.f);\r\n  EXPECT_EQ(output_params.zero_point, 0);\r\n\r\n  std::array<float, 90> output;\r\n  ASSERT_EQ(TfLiteTensorCopyToBuffer(output_tensor, output.data(),\r\n                                     output.size() * sizeof(float)),\r\n            kTfLiteOk);\r\n}\r\n```", "Just one other questions, do you really need the random normal generation in your inference graph? We've seen that sometimes these random ops are used (and useful) with training, but they then get carried forward into the inference graph, without adding as much value.", "I am tasked with the training and deployment of a variational autoencoder. The model tries to learn the params of a distribution as its encoded representation. To generate samples for decoding, samples have to be drawn from said distribution. This is emulated by utilizingbthe so-called reparametrization trick, whichbrequires to generate random numbers from a {0,1} normal distribution.\r\n\r\nAs far as I can tell, using random numbers is inevitable.\r\n\r\nBUT: If you know of another way to do this, I would happily drop the random part.", "I see, good to know. I think we've seen enough use-cases for the RandomStandardNormal op that it's probably time to make it a proper builtin. In the meantime, can I ask what platform you'll be deploying to? ", "Your code snippet works for me, so thanks for that. I was also able to find out what causes the SegFault. In the Delegate test, I first added the interpreter holder from your example with the SegFault still occuring. Turns out, when adding `ASSERT_NE(interpreter, nullptr)` afterwards, it works even though this exact assertion fails.\r\n\r\n```\r\nTEST(CApiSimple, Delegate) {\r\n  TfLiteModel* model =\r\n      TfLiteModelCreateFromFile(\"vae_.tflite\");\r\n\r\n  // Create and install a delegate instance.\r\n  bool delegate_prepared = false;\r\n  TfLiteDelegate delegate = TfLiteDelegateCreate();\r\n  delegate.data_ = &delegate_prepared;\r\n  delegate.Prepare = [](TfLiteContext* context, TfLiteDelegate* delegate) {\r\n    *static_cast<bool*>(delegate->data_) = true;\r\n    return kTfLiteOk;\r\n  };\r\n  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n  TfLiteInterpreterOptionsAddDelegate(options, &delegate);\r\n  TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n  std::unique_ptr<TfLiteInterpreter, void (*)(TfLiteInterpreter*)>\r\n      interpreter_holder(interpreter, &TfLiteInterpreterDelete);\r\n  ASSERT_NE(interpreter, nullptr);\r\n\r\n  // The delegate should have been applied.\r\n  EXPECT_TRUE(delegate_prepared);\r\n\r\n  // Subsequent exectuion should behave properly (the delegate is a no-op).\r\n  TfLiteInterpreterOptionsDelete(options);\r\n  TfLiteModelDelete(model);\r\n\r\n  ASSERT_EQ(TfLiteInterpreterAllocateTensors(interpreter), kTfLiteOk);\r\n  EXPECT_EQ(TfLiteInterpreterInvoke(interpreter), kTfLiteOk);\r\n}\r\n```\r\nI get the following output now:\r\n```\r\n[==========] Running 10 tests from 2 test suites.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from CAPI\r\n[ RUN      ] CAPI.Version\r\n[       OK ] CAPI.Version (0 ms)\r\n[----------] 1 test from CAPI (0 ms total)\r\n\r\n[----------] 9 tests from CApiSimple\r\n[ RUN      ] CApiSimple.Smoke\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2019-11-27 10:57:43.757941: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n[       OK ] CApiSimple.Smoke (12 ms)\r\n[ RUN      ] CApiSimple.QuantizationParams\r\n[       OK ] CApiSimple.QuantizationParams (1 ms)\r\n[ RUN      ] CApiSimple.Delegate\r\nERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n./capi_test.c:153: Failure\r\nExpected: (interpreter) != (nullptr), actual: NULL vs (nullptr)\r\n[  FAILED  ] CApiSimple.Delegate (1 ms)\r\n[ RUN      ] CApiSimple.DelegateFails\r\nERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n[       OK ] CApiSimple.DelegateFails (1 ms)\r\n[ RUN      ] CApiSimple.ErrorReporter\r\nInvoke called on model that is not ready.[       OK ] CApiSimple.ErrorReporter (5 ms)\r\n[ RUN      ] CApiSimple.ValidModel\r\n[       OK ] CApiSimple.ValidModel (0 ms)\r\n[ RUN      ] CApiSimple.ValidModelFromFile\r\n[       OK ] CApiSimple.ValidModelFromFile (0 ms)\r\n[ RUN      ] CApiSimple.InvalidModel\r\nERROR: The model is not a valid Flatbuffer buffer\r\n[       OK ] CApiSimple.InvalidModel (0 ms)\r\n[ RUN      ] CApiSimple.InvalidModelFromFile\r\nERROR: Could not open 'x.tflite'.\r\nERROR: The model is not a valid Flatbuffer file\r\n[       OK ] CApiSimple.InvalidModelFromFile (0 ms)\r\n[----------] 9 tests from CApiSimple (20 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 10 tests from 2 test suites ran. (20 ms total)\r\n[  PASSED  ] 9 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] CApiSimple.Delegate\r\n\r\n 1 FAILED TEST\r\n```\r\nI am not sure how this impacts the usage of my model, but I will report back if I encounter any anomalies. \r\n\r\nAlso, I'm planning on deploying this on ARM64 and x86_64 systems, which both run some kind of UNIX-like OS. I am glad to hear that you consider implementing this. I have created a feature request a while back that addresses the issue: #33341\r\n\r\nIf I can provide any help for this task, do not hesitate to message me. ", "@jdduke \r\nSpeaking of building this library for the ARM64 architecture (for testing this issue on the other platform), do you - by any chance - know which flag I have to use? I am struggling a bit to find documentation on the available flags. I have found some flags under `tensorflow/lite/build_def.bzl` that look like this:\r\n\r\n```\r\ndef tflite_copts():\r\n    \"\"\"Defines compile time flags.\"\"\"\r\n    copts = [\r\n        \"-DFARMHASH_NO_CXX_STRING\",\r\n    ] + select({\r\n        str(Label(\"//tensorflow:android_arm64\")): [\r\n            \"-O3\",\r\n        ],\r\n        str(Label(\"//tensorflow:android_arm\")): [\r\n            \"-mfpu=neon\",\r\n            \"-O3\",\r\n        ],\r\n        str(Label(\"//tensorflow:ios_x86_64\")): [\r\n            \"-msse4.1\",\r\n        ],\r\n        str(Label(\"//tensorflow:windows\")): [\r\n            \"/DTFL_COMPILE_LIBRARY\",\r\n            \"/wd4018\",  # -Wno-sign-compare\r\n        ],\r\n        \"//conditions:default\": [\r\n            \"-Wno-sign-compare\",\r\n        ],\r\n    })\r\n\r\n    return copts\r\n```\r\n\r\nUnfortunately, I do not know if this the correct approach nor do I know if ARM is supported. It states android_arm64 is supported, but I doubt this is what I am looking for.\r\n\r\nEDIT:\r\nI have found several flags for ARM under `tensorflow/lite/kernels/BUILD` which I think are also used by the experimental build for C. The flags do not work, however, e.g. when invoking the bazel build command with `--config=arm64`, I get a message like this:\r\n\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=146\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/local/bin/python --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:xla in file /tensorflow_src/.tf_configure.bazelrc: --define with_xla_support=true\r\nERROR: Config value arm64 is not defined in any .rc file\r\n```\r\n\r\nAny help is appreciated. Thanks!", "@DocDriven Hi I am Jae from TFLite team and I want to help you. To follow the long threads, I want to list up the issues. please feel free to correct me if i am wrong.\r\n\r\n- compilation issue on C API is fixed (1e-9 number of output nodes, etc.) -> fixed\r\n- tf.Multinomial / tf.RandomStandardNormal are required in TFLite side. -> i will work on it.\r\n- build issue on arm64.\r\nI didn't yet reproduce it, but could you test with `--config=android_arm64` ?", "Hi @jaeyoo , your list is correct. Concerning issue 2, this would eliminate my need for a working library with the `with_select_tf_ops` enabled. Alternatively, you could consider re-introducing the corresponding flag for this mode of operation.\r\n\r\nAlso, I tested the flag you suggested. I am on commit e57a567~1 and had to downgrade my bazel installation to 0.26.1. Without the ``--config=android_arm64`` it builds successfully. With the flag, I get an error (even on master with a devel image I pulled today):\r\n\r\n```\r\nroot@534edaa88c8c:/tensorflow_src# bazel build --config=monolithic --config=android_arm64 --define=with_select_tf_ops=true -c opt //tensorflow/lite/experimental/c:libtensorflowlite_c.so\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=204\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages --python_path=/usr/local/bin/python --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:xla in file /tensorflow_src/.tf_configure.bazelrc: --define with_xla_support=true\r\nINFO: Found applicable config definition build:monolithic in file /tensorflow_src/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:android_arm64 in file /tensorflow_src/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nINFO: Found applicable config definition build:android in file /tensorflow_src/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Build options --cpu, --crosstool_top, --fat_apk_cpu, and 1 more have changed, discarding analysis cache.\r\nERROR: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/local_config_cc/BUILD:46:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\nERROR: Analysis of target '//tensorflow/lite/experimental/c:libtensorflowlite_c.so' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted\r\nINFO: Elapsed time: 0.983s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 2773 targets configured)\r\n    currently loading: tensorflow/cc/saved_model\r\n```\r\nIt seems to be a valid option, but it is missing the toolchain.", "`android_arm64` should only be used if you're actually building for Android (and you'll need to run the `configure` script from your root checkout to point bazel to your Android SDK/NDK). If that's what you want to test on, I think you just need to run that configure script.\r\n\r\nWe haven't done much validation of cross-compilation with bazel for generic aarch64 internally, but I can file an internal ticket for tracking. There's a similar thread on https://github.com/tensorflow/tensorflow/issues/34520 for validating generic arm64 builds, which might be useful.", "I am not building for Android, just a custom board with an ARM64 processor.\r\n\r\nI was trying to follow one tutorial referenced in the thread you mentioned (https://github.com/xifengcun/tensorflow-aarch64-crossbuild). I decided to not use the chroot/debootstrap approach and applied the steps to the build container within docker.\r\n\r\nIt seems that the tutorial was intended for an older release of bazel, as some attributes within `tensorflow_src/tools/aarch64_compiler/BUILD` seem to be outdated. I tried the command\r\n```\r\nbazel build --config=monolithic --cpu=aarch64 --define=with_select_tf_ops=true -c opt //tensorflow/lite/experimental/c:libtensorflowlite_c.so --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --crosstool_top=//tools/aarch64_compiler:toolchain --verbose_failures\r\n```\r\nand got the output\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=204\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/local/bin/python --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:xla in file /tensorflow_src/.tf_configure.bazelrc: --define with_xla_support=true\r\nINFO: Found applicable config definition build:monolithic in file /tensorflow_src/.bazelrc: --define framework_shared_object=false\r\nERROR: /tensorflow_src/tools/aarch64_compiler/BUILD:15:1: //tools/aarch64_compiler:gcc-linux-aarch64: no such attribute 'dynamic_runtime_libs' in 'cc_toolchain' rule\r\nERROR: /tensorflow_src/tools/aarch64_compiler/BUILD:15:1: //tools/aarch64_compiler:gcc-linux-aarch64: no such attribute 'static_runtime_libs' in 'cc_toolchain' rule\r\nERROR: /tensorflow_src/tools/aarch64_compiler/BUILD:15:1: //tools/aarch64_compiler:gcc-linux-aarch64: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule\r\nERROR: /tensorflow_src/tools/aarch64_compiler/BUILD:15:1: Target '//tools/aarch64_compiler:empty' contains an error and its package is in error and referenced by '//tools/aarch64_compiler:gcc-linux-aarch64'\r\nERROR: /tensorflow_src/tools/aarch64_compiler/BUILD:3:1: Target '//tools/aarch64_compiler:gcc-linux-aarch64' contains an error and its package is in error and referenced by '//tools/aarch64_compiler:toolchain'\r\nERROR: /tensorflow_src/tensorflow/lite/experimental/c/BUILD:20:1: every rule of type cc_binary implicitly depends upon the target '//tools/aarch64_compiler:toolchain', but this target could not be found because of: Target '//tools/aarch64_compiler:toolchain' contains an error and its package is in error\r\nERROR: Analysis of target '//tensorflow/lite/experimental/c:libtensorflowlite_c.so' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 0.249s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 4 targets configured)\r\n    currently loading: tensorflow/lite ... (2 packages)\r\n```\r\nSo I simply removed the presumably deprecated(?) attributes `dynamic_runtime_libs` and `static_runtime_libs`, and added the mandatory attribute `toolchain_config = ':empty'`.\r\n\r\nWhat I ended up with are the following errors:\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=204\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/local/bin/python --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:xla in file /tensorflow_src/.tf_configure.bazelrc: --define with_xla_support=true\r\nINFO: Found applicable config definition build:monolithic in file /tensorflow_src/.bazelrc: --define framework_shared_object=false\r\nERROR: /tensorflow_src/tools/aarch64_compiler/BUILD:25:22: in toolchain_config attribute of cc_toolchain rule //tools/aarch64_compiler:gcc-linux-aarch64: '//tools/aarch64_compiler:empty' does not have mandatory providers: 'CcToolchainConfigInfo'\r\nERROR: Analysis of target '//tensorflow/lite/experimental/c:libtensorflowlite_c.so' failed; build aborted: Analysis of target '//tools/aarch64_compiler:gcc-linux-aarch64' failed; build aborted\r\nINFO: Elapsed time: 0.126s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded, 3 targets configured)\r\n    currently loading: tensorflow/lite/kernels\r\n```\r\nI am afraid that I do not have a clue how to deal with the `mandatory provider: 'CcToolchainConfigInfo'` error. Maybe somebody could take a look at it?\r\n\r\nI think it should be able to work inside a Docker container, also it would be easier to ship than using a chroot environment.", "@DocDriven Could  you please check in latest TF version and let us know the update. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34277\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34277\">No</a>\n"]}, {"number": 34276, "title": "Tensorboard callback not writing the training metrics", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0.130\r\n- GPU model and memory: Quadro P5000 (16Gb)\r\n\r\n**Describe the current behavior**\r\nWhen I fit a model that takes time to infer (either with a big enough data size, or enough parameters), the `TensorBoard` calback doesn't log the training metrics (or just the first few epochs), or at least they don't appear on Tensorboard.\r\n\r\nAlthough I first noticed the problem on GPU, it also appears on CPU.\r\n\r\n**Describe the expected behavior**\r\nI would like to have the training metrics logged as well.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport os.path as op\r\nimport time\r\n\r\nimport numpy as np\r\nfrom tensorflow.keras.callbacks import TensorBoard\r\nfrom tensorflow.keras.layers import Conv2D, Input\r\nfrom tensorflow.keras.models import Model\r\n\r\nsize = 512\r\nim = Input((size, size, 1))\r\nim_conv = Conv2D(512, 3, padding='same', activation='relu')(im)\r\nim_conv = Conv2D(1, 3, padding='same', activation='linear')(im_conv)\r\nmodel = Model(im, im_conv)\r\nmodel.compile(loss='mse', optimizer='adam', metrics=['mae'])\r\n\r\ndata = np.random.rand(1, size, size, 1)\r\n\r\nrun_id = f'{int(time.time())}'\r\nlog_dir = op.join('logs', run_id)\r\ntboard_cback = TensorBoard(\r\n    log_dir=log_dir, \r\n    histogram_freq=0, \r\n    write_graph=False, \r\n    write_images=False, \r\n    profile_batch=2,\r\n)\r\n\r\nmodel.fit(\r\n    x=data, \r\n    y=data, \r\n    validation_data=[data, data], \r\n    callbacks=[tboard_cback,], \r\n    epochs=100, \r\n    verbose=0,\r\n);\r\n```\r\n\r\n**Other info / logs**\r\nI have posted an [SO question](https://stackoverflow.com/questions/58823959/tensorboard-callback-not-writing-the-training-metrics) which didn't get too much traction, that's why I am asking here.\r\n\r\nHere is what I observe on Tensorboard: \r\n![tboard_fail_mre](https://user-images.githubusercontent.com/6387497/68871144-48230000-06fc-11ea-8b7b-b4fd317877d7.png)\r\n\r\n", "comments": ["Agree..., have the same issue here, tried your code and the tutorials same issue occurs.", "Thanks for the report.  The details look very similar to issue https://github.com/tensorflow/tensorboard/issues/2084.  Currently, there is a bug with TensorBoard's Keras callbacks that prevent training metrics from showing up/updating.\r\n\r\nThe current workaround for this, while we work on a solution, is to disable profiling entirely by adding `profile_batch=0` to the callback options.", "Duplicate of https://github.com/tensorflow/tensorboard/issues/2084", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34276\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34276\">No</a>\n", "Please let us know if the workaround doesn't work!\r\n\r\nSide note: I noticed in the screenshot that the Runs selector on the left is more narrow than the full sidebar.  This issue should be fixed in the most recent Tensorboard", "The workaround does work, I will follow #2084 to be aware of when the satisfying fix is be available. Thanks for this and the side note!"]}, {"number": 34275, "title": "Use HParams for \"learning rate\" hyper-parameter tuning", "body": "https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\r\n\r\n## Description of issue (what needs changing):\r\nIn the documentation it's nowhere written how to tune learning rate hyper-parameter for different optimisation algorithms. Plus the code in file **python3.6/site-packages/tensorboard/plugins/hparams/summary_v2.py** says,\r\n`if dtype not in (int, float, bool, str):\r\n--> 482       raise ValueError(\"Unknown dtype: %r\" % (dtype,))` \r\n\r\nI am trying to create HParam list \r\n\r\n`hp_optimizer = hp.HParam('optimizer', hp.Discrete([optimizers.Adam(learning_rate=0.001),\r\n                                                   optimizers.Adam(learning_rate=0.0001),\r\n                                                   optimizers.Adamax(learning_rate=0.001),\r\n                                                   optimizers.Adamax(learning_rate=0.0001),\r\n                                                   optimizers.SGD(learning_rate=0.01),\r\n                                                   optimizers.SGD(learning_rate=0.001)]))`\r\n\r\n### Clear description\r\nHow should someone tune learning rate?", "comments": ["@yashmanuda \r\nWill it be possible to provide minimal stand alone code to reproduce the issue. It helps us in localizing the issue faster.Thanks!", "There's no functionality asfaik to do the tuning for you. This is just a dashboard so you can compare different runs with different hparams to see which is best in tensorboard.\r\n\r\nI have a GridSearch/RandomSearch function in my code that just loops over parameters and runs a new experiment. Just a more automated fashion of running different trials.\r\n\r\nAs for your error, you are passing in an instance of Adam as an hparam, only numbers/strings are allowed. If you want learning rate to be displayed in the Hparams tab then you should pass in just learning rate. I personally add optimizer as a string and then learning rate as a float.\r\n\r\nYour link as plenty of examples of all of this", "@yashmanuda \r\n\r\nAny update on this issue please. Thanks!", "I am able to use float as the learning rate, and using it to create an optimizer. Thanks for the help.", "@yashmanuda \r\n\r\nLooks like issue got resolved.Please, let us know whether we can close this issue?. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@jtavrisov @yashmanuda\r\n> There's no functionality asfaik to do the tuning for you. This is just a dashboard so you can compare different runs with different hparams to see which is best in tensorboard.\r\n> \r\n> I have a GridSearch/RandomSearch function in my code that just loops over parameters and runs a new experiment. Just a more automated fashion of running different trials.\r\n> \r\n> As for your error, you are passing in an instance of Adam as an hparam, only numbers/strings are allowed. If you want learning rate to be displayed in the Hparams tab then you should pass in just learning rate. I personally add optimizer as a string and then learning rate as a float.\r\n> \r\n> Your link as plenty of examples of all of this\r\n\r\nCould you please give more details on adding optimizer as a string and learning rate as a float?I am not able to experiment with different learning rates on HParams. Here is the [link](https://github.com/tensorflow/tensorboard/issues/3688) to my issue.\r\n "]}, {"number": 34274, "title": "ValueError: Error when checking input: expected flatten_1_input to have 3 dimensions, but got array with shape (60000, 1)", "body": "I ran the quickstart for beginner demo on my PC, but when i run the last cell, there is a **ValueError:Error when checking input: expected flatten_1_input to have 3 dimensions, but got array with shape (60000, 1)**, but when I ran the demo in Colab\uff0c there was no problem,\r\ncould you please tell what happend about this?\r\n![image](https://user-images.githubusercontent.com/15179180/68869687-8c31f700-0734-11ea-8fa7-92705abc7310.png)\r\n![image](https://user-images.githubusercontent.com/15179180/68869715-9522c880-0734-11ea-80aa-31106cc90506.png)\r\n", "comments": ["@hy59 ,\r\nCan you please provide gist of colab or code being used so that we can try replicating the issue from our end?Also mention the TF version being used.Thanks!", "@hy59 ,\r\nany update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34273, "title": "Custom model implementation to tensor flow lite micro speech example", "body": "Hi everyone.\r\n\r\nCurrently I am working with tensorflow lite micro speech example for stm32f7disco board and want to try it with my own trained model. Unfortunately, the result I have for now is that replacement of generated <tiny_conv>.cc file is not enough for that since my model doesn't pass some internal chekcups. Can somebody give an instruction on how to implement custom model to this project? And to be sure about the model can you give some explanations of parameters which are used in training and generating .tflite file?\r\n", "comments": ["@hardware-dept, Please refer this [link](https://www.tensorflow.org/lite/microcontrollers/build_convert). Thanks!", "@hardware-dept, Is this still an issue? Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34272, "title": "Python program to test tensorflow speech recognition lite model", "body": "I converted speech recognition model generated using tensorflow train.py to tflite.\r\nIs there a python program to run speech recognition with the tflite (similar to label_wav.py) ?", "comments": ["@adishpv9 label_wav.py is for testing the frozen model. You can freeze your model and then test it (using label_wav.py just by changing arguments) and then convert it to tflite. If you are planning to deploy it you can follow these docs [1](https://www.tensorflow.org/lite/guide/get_started#3_use_the_tensorflow_lite_model_for_inference_in_a_mobile_app)  and  [2](https://github.com/tensorflow/examples/blob/master/lite/examples/speech_commands/android/README.md) which can help you . \r\n\r\n", "Hi Gowtham,\nThanks for the reply !\nI want a python program to run my tflite model. Can you help out with that?\n\nOn Fri, Nov 15, 2019 at 4:38 AM gowthamkpr <notifications@github.com> wrote:\n\n> Closed #34272 <https://github.com/tensorflow/tensorflow/issues/34272>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34272?email_source=notifications&email_token=ANV4656OKNPA4LBGWSAVVFLQTXK77A5CNFSM4JNMHIUKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOU3Y75LY#event-2800877231>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANV465Z4PXZA7TAHXGAPEM3QTXK77ANCNFSM4JNMHIUA>\n> .\n>\n\n\n-- \n*Regards,*\nAdish P V\n"]}, {"number": 34271, "title": "WARNING:tensorflow:Entity <bound method Customized_DenseLayer.call of <__main__.Customized_DenseLayer object at 0x00000262A54DA488>> could not be transformed and will be executed as-is.", "body": "TensorFlow installed from binary\r\nTensorFlow version 2.0.0\r\nPython version 3.7.4\r\nCuda compilation tools, release 10.0, V10.0.130\r\ncudnn version 7.6.4\r\nGPU model and memory: GTX1060 6G\r\n\r\n**Code to reproduce the issue**\r\nclass Customized_DenseLayer(tf.keras.layers.Layer):\r\n    def __init__(self, units, activation = None, **kwargs):\r\n        self.units = units\r\n        self.activation = tf.keras.layers.Activation(activation)\r\n        super(Customized_DenseLayer, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(name = 'kernel',\r\n                                      shape = (input_shape[-1], self.units),\r\n                                      dtype = tf.float32,\r\n                                      initializer = 'uniform',\r\n                                      trainable = True)\r\n        self.bias = self.add_weight(name = 'bias',\r\n                                    shape = (self.units,),\r\n                                    dtype = tf.float32,\r\n                                    initializer = 'zeros',\r\n                                    trainable = True)\r\n        super(Customized_DenseLayer, self).build(input_shape)\r\n\r\n    def call(self, inputs):\r\n        return self.activation(inputs @ self.kernel + self.bias)\r\n\r\nhousing = fetch_california_housing()\r\nx_train_all, x_test, y_train_all, y_test = train_test_split(\r\n    housing.data, housing.target, random_state = 7)\r\nx_train, x_valid, y_train, y_valid = train_test_split(\r\n    x_train_all, y_train_all, random_state = 11)\r\n\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nscaler = StandardScaler()\r\nx_train_scaled = scaler.fit_transform(x_train)\r\nx_valid_scaled = scaler.transform(x_valid)\r\nx_test_scaled = scaler.transform(x_test)\r\n\r\nprint(x_train_scaled.shape, y_train.shape)\r\nprint(x_valid_scaled.shape, y_valid.shape)\r\nprint(x_test_scaled.shape, y_test.shape)\r\nprint(x_train_scaled.shape[1:])\r\nprint(x_train.shape[1:])\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(Customized_DenseLayer(units = 30, input_shape = (8, ), activation = 'relu'))\r\nmodel.add(Customized_DenseLayer(units = 1))\r\n\r\nprint(model.summary())\r\n\r\n**Other info / logs**\r\nWARNING:tensorflow:Entity <bound method Customized_DenseLayer.call of <__main__.Customized_DenseLayer object at 0x00000262FB272888>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Customized_DenseLayer.call of <__main__.Customized_DenseLayer object at 0x00000262FB272888>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\n", "comments": ["@sakurakatana ,\r\nWhen tried running the given code i got the error \r\n`NameError: name 'fetch_california_housing' is not defined` can you please provide us the complete code to reproduce the error faced here?Thanks!", "> @sakurakatana ,\r\n> When tried running the given code i got the error\r\n> `NameError: name 'fetch_california_housing' is not defined` can you please provide us the complete code to reproduce the error faced here?Thanks!\r\n\r\nComplete code\uff1a\r\n'''\r\n@Author:   MaoZi_Sakura\r\n@Contact:  zhang132412sakura@outlook.com\r\n'''\r\n\r\n'''\r\n@Author:   MaoZi_Sakura\r\n@Contact:  zhang132412sakura@outlook.com\r\n'''\r\n\r\n# %%\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.datasets import fetch_california_housing\r\nfrom sklearn.model_selection import train_test_split\r\nimport os, sys, time\r\n\r\n\r\n#%%\r\n\r\n\r\nclass Customized_DenseLayer(tf.keras.layers.Layer):\r\n    def __init__(self, units, activation = None, **kwargs):\r\n        self.units = units\r\n        self.activation = tf.keras.layers.Activation(activation)\r\n        super(Customized_DenseLayer, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(name = 'kernel',\r\n                                      shape = (input_shape[-1], self.units),\r\n                                      dtype = tf.float32,\r\n                                      initializer = 'uniform',\r\n                                      trainable = True)\r\n        self.bias = self.add_weight(name = 'bias',\r\n                                    shape = (self.units,),\r\n                                    dtype = tf.float32,\r\n                                    initializer = 'zeros',\r\n                                    trainable = True)\r\n        super(Customized_DenseLayer, self).build(input_shape)\r\n\r\n    def call(self, inputs):\r\n        return self.activation(inputs @ self.kernel + self.bias)\r\n\r\n\r\n# %%\r\nhousing = fetch_california_housing()\r\nx_train_all, x_test, y_train_all, y_test = train_test_split(\r\n    housing.data, housing.target, random_state = 7)\r\nx_train, x_valid, y_train, y_valid = train_test_split(\r\n    x_train_all, y_train_all, random_state = 11)\r\n\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nscaler = StandardScaler()\r\nx_train_scaled = scaler.fit_transform(x_train)\r\nx_valid_scaled = scaler.transform(x_valid)\r\nx_test_scaled = scaler.transform(x_test)\r\n\r\nprint(x_train_scaled.shape, y_train.shape)\r\nprint(x_valid_scaled.shape, y_valid.shape)\r\nprint(x_test_scaled.shape, y_test.shape)\r\nprint(x_train_scaled.shape[1:])\r\nprint(x_train.shape[1:])\r\n\r\n# %%\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(Customized_DenseLayer(units = 30, input_shape = (8, ), activation = 'relu'))\r\nmodel.add(Customized_DenseLayer(units = 1))\r\n\r\nprint(model.summary())\r\n\r\n# %%\r\nmodel.compile(optimizer = tf.keras.optimizers.SGD(0.005), loss = 'mean_squared_error')\r\n\r\ncallbacks = [tf.keras.callbacks.EarlyStopping(min_delta = 0.01, patience = 5)]\r\n\r\nhistory = model.fit(x_train_scaled, y_train, epochs = 100,\r\n                    validation_data = (x_valid_scaled, y_valid),\r\n                    callbacks = callbacks)\r\n\r\nprint(history.history)\r\n\r\n# %%\r\npd.DataFrame(history.history).plot(figsize = (8, 5))\r\nplt.grid(True)\r\nplt.gca().set_ylim(0, 1)\r\nplt.show()\r\n\r\nresult = model.evaluate(x_test_scaled, y_test)\r\n", "At the same time, I encountered a similar problem again.\r\n\r\nOther info / logs\r\nWARNING:tensorflow:Entity <function <lambda> at 0x00000207ACE9BBF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function <lambda> at 0x00000207ACE9BBF8>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\n\r\nThe code\uff1a\r\n\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.datasets import fetch_california_housing\r\nfrom sklearn.model_selection import train_test_split\r\nimport os, sys, time\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\r\ndataset = dataset.repeat(3).batch(7)\r\ndata = dataset.interleave(map_func = lambda x: tf.data.Dataset.from_tensor_slices(x),\r\n                          cycle_length = 5,\r\n                          block_length = 1)\r\nfor item in data:\r\n    print(item)\r\n", "@sakurakatana ,\r\nHi,when tried running the given code I got  `TypeError: ('Keyword argument not understood:', 'units') `kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/d0d8cf9fc6b053c40b76463b88c5087b/34271.ipynb) of colab for reference. Thanks!", "@oanush,\r\nBecause I am not familiar with the way github text is edited, causing some code to change after uploading, resulting in code error. I'm so sorry about that. I hope you can test it again, Thanks!\r\n\r\nthe new code\uff1a\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.datasets import fetch_california_housing\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import StandardScaler\r\nimport os, sys, time\r\n\r\n\r\nclass Customized_DenseLayer(tf.keras.layers.Layer):\r\n    def __init__(self, units, activation = None, **kwargs):\r\n        self.units = units\r\n        self.activation = tf.keras.layers.Activation(activation)\r\n        super(Customized_DenseLayer, self).__init__(**kwargs)\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(name = 'kernel',\r\n                                      shape = (input_shape[-1], self.units),\r\n                                      dtype = tf.float32,\r\n                                      initializer = 'uniform',\r\n                                      trainable = True)\r\n        self.bias = self.add_weight(name = 'bias',\r\n                                    shape = (self.units,),\r\n                                    dtype = tf.float32,\r\n                                    initializer = 'zeros',\r\n                                    trainable = True)\r\n        super(Customized_DenseLayer, self).build(input_shape)\r\n    def call(self, inputs):\r\n        return self.activation(inputs @ self.kernel + self.bias)\r\n\r\n\r\nhousing = fetch_california_housing()\r\nx_train_all, x_test, y_train_all, y_test = train_test_split(\r\n    housing.data, housing.target, random_state = 7)\r\nx_train, x_valid, y_train, y_valid = train_test_split(\r\n    x_train_all, y_train_all, random_state = 11)\r\n\r\nscaler = StandardScaler()\r\nx_train_scaled = scaler.fit_transform(x_train)\r\nx_valid_scaled = scaler.transform(x_valid)\r\nx_test_scaled = scaler.transform(x_test)\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(Customized_DenseLayer(units = 30, input_shape = (8,), activation = 'relu'))\r\nmodel.add(Customized_DenseLayer(units = 1))\r\n\r\nprint(model.summary())\r\n\r\nmodel.compile(optimizer = tf.keras.optimizers.SGD(0.005), loss = 'mean_squared_error')\r\n\r\ncallbacks = [tf.keras.callbacks.EarlyStopping(min_delta = 0.01, patience = 5, verbose = 1)]\r\n\r\nhistory = model.fit(x_train_scaled, y_train, epochs = 100,\r\n                    validation_data = (x_valid_scaled, y_valid),\r\n                    callbacks = callbacks, verbose = 1)\r\n\r\nprint(history.history)\r\n\r\npd.DataFrame(history.history).plot(figsize = (8, 5))\r\nplt.grid(True)\r\nplt.gca().set_ylim(0, 1)\r\nplt.show()\r\n\r\nresult = model.evaluate(x_test_scaled, y_test)\r\n```\r\n", "@sakurakatana ,\r\nWhen tried executing the given code I did not face any error, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/fd40435368099ac1ccfb9acc1a0b9f68/34271.ipynb) of the colab.Thanks!", "@sakurakatana ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34270, "title": "tf.function should trace the additional attributes of a tensor", "body": "Hello, I'm not sure if this should be considered as a Feature or a Bug\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nLet's consider the following code (from TF 1.8) of the [loginctensornetwork](https://github.com/logictensornetworks/logictensornetworks/blob/master/logictensornetworks.py) project\r\n\r\n```\r\ndef And(*wffs):\r\n    if len(wffs) == 0:\r\n        result = tf.constant(1.0)\r\n        result.doms = []\r\n    else:\r\n        cross_wffs,_ = cross_args(wffs)\r\n        label = \"_AND_\".join([wff.name.split(\":\")[0] for wff in wffs])\r\n        result = tf.identity(F_And(cross_wffs),name=label)\r\n        result.doms = cross_wffs.doms\r\n    return result\r\n\r\ndef constant(label,value=None,\r\n                 min_value=None,\r\n                 max_value=None):\r\n    label = \"ltn_constant_\"+label\r\n    if value is not None:\r\n        result = tf.constant(value,name=label)\r\n    else:\r\n        result = tf.Variable(tf.random_uniform(\r\n                shape=(1,len(min_value)),\r\n                minval=min_value,\r\n                maxval=max_value,name=label))\r\n    result.doms = []\r\n    return result\r\n\r\ndef cross_args(args):\r\n    result = args[0]\r\n    for arg in args[1:]:\r\n        result,_ = cross_2args(result,arg)\r\n    result_flat = tf.reshape(result,\r\n                             (tf.reduce_prod(tf.shape(result)[:-1]),\r\n                              tf.shape(result)[-1]))\r\n    result_args = tf.split(result_flat,[tf.shape(arg)[-1] for arg in args],1)\r\n    return result, result_args\r\n\r\ndef cross_2args(X,Y):\r\n    if X.doms == [] and Y.doms == []:\r\n        result = tf.concat([X,Y],axis=-1)\r\n        result.doms = []\r\n        return result,[X,Y]\r\n    X_Y = set(X.doms) - set(Y.doms)\r\n    Y_X = set(Y.doms) - set(X.doms)\r\n    eX = X\r\n    eX_doms = [x for x in X.doms]\r\n    for y in Y_X:\r\n        eX = tf.expand_dims(eX,0)\r\n        eX_doms = [y] + eX_doms\r\n    eY = Y\r\n    eY_doms = [y for y in Y.doms]\r\n    for x in X_Y:\r\n        eY = tf.expand_dims(eY,-2)\r\n        eY_doms.append(x)\r\n    perm_eY = []\r\n    for y in eY_doms:\r\n        perm_eY.append(eX_doms.index(y))\r\n    eY = tf.transpose(eY,perm=perm_eY + [len(perm_eY)])\r\n    mult_eX = [1]*(len(eX_doms)+1)\r\n    mult_eY = [1]*(len(eY_doms)+1)\r\n    for i in range(len(mult_eX)-1):\r\n        mult_eX[i] = tf.maximum(1,tf.floor_div(tf.shape(eY)[i],tf.shape(eX)[i]))\r\n        mult_eY[i] = tf.maximum(1,tf.floor_div(tf.shape(eX)[i],tf.shape(eY)[i]))\r\n    result1 = tf.tile(eX,mult_eX)\r\n    result2 = tf.tile(eY,mult_eY)\r\n    result = tf.concat([result1,result2],axis=-1)\r\n    result1.doms = eX_doms\r\n    result2.doms = eX_doms\r\n    result.doms = eX_doms\r\n    return result,[result1,result2]\r\n```\r\n\r\nIn this project, we need to store some metainformation within every tensor for ensuring the correct behaviour of functions like And regardless of the order of the arguments. \r\nMigrating to Tensorflow 2.0 if I decorate a function with @tf.function this function will not return the additional doms attribute of the tensor. This attributes, as you can see in functions like cross_2args are mandatory for ensuring the correctness of the flow. \r\nMy goal is to ensure that if a function is adding a set of attributes to a tensor before returning it, decorating the same function with tf.function should in the same way return a tensor which has the same set of additional attributes.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who wants to add new attributes to a tensor for having meta-information about it during the forward process\r\n\r\n**Any Other info.**", "comments": ["@thadumi, Thanks for reporting this issue.\r\nWill it be possible to provide the complete standalone code to reproduce the reported issue here. Thanks!", "Hi @gadagashwini ,\r\nplease take a look at the following example\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef foo(label, tensor=None, min_value=None, max_value=None):\r\n    if tensor is not None and tf.is_tensor(tensor):\r\n        result = tf.identity(tensor, name=label)\r\n    else:\r\n        result = tf.random.uniform(shape=(1, len(min_value)),\r\n                                   minval=min_value,\r\n                                   maxval=max_value, name=label)\r\n    result._bar = [label]\r\n    return result\r\n\r\n\r\n@tf.function\r\ndef fooAutoGraph(label, tensor=None, min_value=None, max_value=None):\r\n    if tensor is not None and tf.is_tensor(tensor):\r\n        result = tf.identity(tensor, name=label)\r\n    else:\r\n        result = tf.random.uniform(shape=(1, len(min_value)),\r\n                                   minval=min_value,\r\n                                   maxval=max_value, name=label)\r\n    result._bar = [label]\r\n    return result\r\n\r\n\r\nassert hasattr(foo('a', min_value=[0.] * 10, max_value=[1.] * 10), '_bar') # this works\r\nassert hasattr(fooAutoGraph('a', min_value=[0.] * 10, max_value=[1.] * 10), '_bar') # this will fail\r\n```", "I'd like to point out that the same issue exists also for Keras subclassing\r\nThe workaround for Keras is the following \r\n\r\n```python\r\nclass MyLayer(KL.Layer):\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super(MyLayer, self).__init__(*args, **kwargs)\r\n\r\n    def __call__(self, inputs, *args, **kwargs):\r\n        outputs = super(MyLayer, self).__call__(inputs, *args, **kwargs)\r\n        flat_outputs = nest.flatten(outputs)\r\n\r\n        meta = self.compute_meta(inputs, **kwargs)\r\n        for output in flat_outputs:\r\n            output._my_meta = meta\r\n\r\n        return outputs\r\n\r\n    def compute_meta(self, inputs, **kwargs):\r\n        return [] # no metadata\r\n``` ", "@jvishnuvardhan I'd like to contribute. Could you tell me where I should add this feature?", "@mdanatg FYI\r\n\r\nUnfortunately, `@tf.function` is supposed to trace only direct Tensor manipulations.  I agree that it's not ideal but currently it's out of the scope for `@tf.function`.  For now, I suggest to manage the metadata outside of `@tf.function`\r\n\r\nFYI, `@tf.function` is defined here https://github.com/tensorflow/tensorflow/blob/7cebef3019e9ec10fd81a021e56233527ae7c9d1/tensorflow/python/eager/def_function.py#L970\r\n", "Indeed, it would be cleaner to encapsulate those tensors inside structures that `tf.function` supports, like `namedtuple` objects. These structures could then carry the values along with the metadata. In general, tensors generally have semantics of opaque values (unfortunately not uniformly implemented right now), so in theory they should not allow setting custom attributes (they should behave more like `int` or `np.ndarray` in that regard).\r\n\r\nHere's an example which uses `namedtuple`:\r\n\r\n```\r\nLblStruct = collections.namedtuple('LblStruct', ('value', 'bar'))\r\n\r\n@tf.function\r\ndef foo_with_namedtuple(lbl_struct, tensor=None, min_value=None, max_value=None):\r\n    label = lbl_struct.value\r\n    if tensor is not None and tf.is_tensor(tensor):\r\n        result = tf.identity(tensor, name=label)\r\n    else:\r\n        result = tf.random.uniform(shape=(1, len(min_value)),\r\n                                   minval=min_value,\r\n                                   maxval=max_value, name=label)\r\n    return LblStruct(result, [label])\r\n\r\nlbl_struct = LblStruct('a', '_bar')\r\nassert hasattr(foo_with_namedtuple(lbl_struct, min_value=[0.] * 10, max_value=[1.] * 10), 'bar')\r\n```\r\n\r\nSide discussion --\r\n\r\nTechnically speaking, preserving custom attributes would be doable for static attribute modifications (that is, not for attributes that are set inside data-dependent control flow), like in this hand-coded example:\r\n\r\n```\r\ndef foo(label, tensor=None, min_value=None, max_value=None):\r\n    if tensor is not None and tf.is_tensor(tensor):\r\n        result = tf.identity(tensor, name=label)\r\n    else:\r\n        result = tf.random.uniform(shape=(1, len(min_value)),\r\n                                   minval=min_value,\r\n                                   maxval=max_value, name=label)\r\n    result._bar = [label]\r\n    return result\r\n\r\ndef foo_with_custom_attributes(label, tensor=None, min_value=None, max_value=None):\r\n\r\n  @tf.function\r\n  def wrapper(label, tensor, min_value, max_value):\r\n    return foo(label, tensor, min_value, max_value)\r\n\r\n  result = wrapper(label, tensor, min_value, max_value)\r\n  result._bar = [label]\r\n  return result\r\n\r\nassert hasattr(foo('a', min_value=[0.] * 10, max_value=[1.] * 10), '_bar') # this works\r\nassert hasattr(foo_with_custom_attributes('a', min_value=[0.] * 10, max_value=[1.] * 10), '_bar') # this works too\r\nassert hasattr(tf.function(foo)('a', min_value=[0.] * 10, max_value=[1.] * 10), '_bar') # this fails\r\n```\r\n\r\nWhen it traces its body, `tf.function` creates new tensor objects for its arguments and return values (e.g. arguments are replaced with placeholders that will be fed at execution), which is when custom attributes get lost. So `tf.function` could copy them over, if this was done with care, to avoid copying attributes that it should not (we don't want to accidentally set a non-user attribute).\r\n\r\nIf you'd like to experiment with it, that's probably best done deep inside `tf.function` around the point when if calls the original function, which should be [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/func_graph.py#L978). That's where AutoGraph proper is applied as well, if you look a couple of lines up.\r\n\r\nAgain, the above is purely an academic point, because I think this caveat is valid not just for `tf.function`, but may other ops as well, like `tf.cond`, `tf.while_loop`, even `tf.identity` - any of these are expected to replace respective arguments without warning - they only preserve values; so if anything,  would require a lot of code.", "Hi @mdanatg sorry for the delay.\r\nThanks for the explanation. I refactored the code for working with something very similar to the wrapper proposal.", "@thadumi,\r\nCan you please let us know if [this comment](https://github.com/tensorflow/tensorflow/issues/34270#issuecomment-556028137) has resolved your issue so that we can close it? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34269, "title": "A custom RNN cell does not support a high-order state_size", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):No\r\nTensorFlow version (use command below):2.0.0\r\n- Python version:3.7.4\r\n\r\n**Describe the current behavior**\r\nCannot have a custom RNN cell to have a high order state_size.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nclass NewCell(layers.Layer):\r\n    def __init__(self, name=None):\r\n        \r\n        self.state_size = (\r\n            tf.TensorShape([1, 2]),\r\n            (tf.TensorShape([1, 2]), tf.TensorShape([2, 3])) # error cause by this tuple\r\n        )\r\n        self.output_size = (tf.TensorShape([1, 2]), )\r\n        super().__init__(name=name)\r\n        \r\n    def call(self, inputs, state):\r\n        print('everything is safe and sound!')\r\n        return inputs, state\r\n        \r\nrnn = layers.RNN(NewCell())\r\ninputs = tf.convert_to_tensor(np.random.rand(2, 1, 2))\r\nrnn(inputs)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe error message is:\r\n> int() argument must be a string, a bytes-like object or a number, not 'TensorShape'", "comments": ["Issue is replicating with Tensorflow version 2.0.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/0f2c5c2f88db0dacf2e0b3dff238941c/34269.ipynb). Thanks!", "Any update on this? ", "Issue is replicating with Tensorflow version 2.6.0-dev20210529,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/f75251be59852ecf722e998de43bf8b0/untitled82.ipynb#scrollTo=dXrmjt4NR4tF). Thanks!", "The error got resolved by removing the extra parentheses from this code block  \r\n``` python \r\n self.state_size = (\r\n            tf.TensorShape([1, 2]),\r\n            (tf.TensorShape([1, 2]), tf.TensorShape([2, 3])) \r\n\r\n```\r\nwith Tensorflow  version 2.5 ,please find the [gist ](https://colab.research.google.com/gist/mohantym/4251b02c22cde6e26a6dafc6291d2318/34269.ipynb#scrollTo=HekGzwptSAWR) of working code  ..Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34269\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34269\">No</a>\n"]}]