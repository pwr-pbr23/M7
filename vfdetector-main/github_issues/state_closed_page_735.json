[{"number": 31522, "title": "1.13.2 build failed with \"Not Found\" error for file that exists", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04 (Windows 10 subsystem)\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.13.2\r\n- Python version: 3.6.8\r\n- Bazel version: 0.21.0\r\n- GCC version: 7.4.0\r\n\r\n**Describe the problem**\r\nJust trying to get a CPU hardware-specific install; selected \"No\" to all except XLA JIT support during configuration. I don't need CUDA/GPU support.\r\n\r\n```\r\nERROR: /mnt/c/Users/peter/Desktop/projects/tensorflow/tensorflow/python/BUILD:5873:1: Executing genrule //tensorflow/python:nccl_ops_pygenrule failed (Aborted): bash failed: error executing command /bin/bash bazel-out/k8-opt/genfiles/tensorflow/python/nccl_ops_pygenrule.genrule_script.sh\r\n2019-08-11 14:35:54.567507: F tensorflow/python/framework/python_op_gen_main.cc:123] Non-OK-status: api_def_map.LoadFileList(env, api_files) status: Not found: tensorflow/core/api_def/base_api/api_def_Imag.pbtxt; No such file or directory\r\n...\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 11735.779s, Critical Path: 3155.14s\r\nINFO: 7082 processes: 7082 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nbazel clean --expunge\r\n./configure\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nI verified that `tensorflow/core/api_def/base_api/api_def_Imag.pbtxt` does indeed exist. \r\n", "comments": ["Just to verify did you get chance to follow instructions from TensorFlow [website](https://www.tensorflow.org/install/source) .Please, let us know. Thanks!", "Yes. specifically:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ngit checkout v1.13.2\r\ncd tensorflow\r\n```\r\nfollowed by the build commands from my original post.", "@peterse What version of nccl and visual studio you are using for the build? Are you installing on Ubuntu or Windows? Thanks!", "@jvishnuvardhan I am not using nccl (I do not want GPU support) or visual studio; I am installing on an Ubuntu 18.04 Windows Subsystem for Linux (like described at this link: [https://docs.microsoft.com/en-us/windows/wsl/install-win10](https://docs.microsoft.com/en-us/windows/wsl/install-win10)).\r\n\r\nThis subsystem has been compatibile with every other linux application I've tried to install, so I don't think the subsystem is the problem.", "Can you try with master instead of 1.13?", "@peterse \r\nDid you get a chance to check with master as suggested by mihaimaruseac. Thanks!", "This did not resolve the issue. I now get the error:\r\n\r\n```\r\nERROR: /mnt/c/Users/peter/Desktop/projects/tensorflow/tensorflow/python/BUILD:2088:1: Executing genrule //tensorflow/python:batch_ops_pygenrule failed (Aborted): bash failed: error executing command /bin/bash bazel-out/k8-py2-opt/bin/tensorflow/python/batch_ops_pygenrule.genrule_script.sh\r\n2019-08-19 14:49:50.188080: F tensorflow/python/framework/python_op_gen_main.cc:123] Non-OK-status: api_def_map.LoadFileList(env, api_files) status: Not found: tensorflow/core/api_def/base_api/api_def_RandomShuffleQueue.pbtxt; No such file or directory\r\nbazel-out/k8-py2-opt/bin/tensorflow/python/batch_ops_pygenrule.genrule_script.sh: line 2: 19688 Aborted               \r\n```\r\n\r\nAnd again, I verified that this file does indeed exist:\r\n![image](https://user-images.githubusercontent.com/14928221/63291701-11bb1d80-c292-11e9-82b0-79b0010f37d1.png)\r\n\r\nAttached: \"ERROR\" component of failure (there were quite a few Bazel warnings that I wasn't able to capture)\r\n[tf_core_dump.txt](https://github.com/tensorflow/tensorflow/files/3517270/tf_core_dump.txt)\r\n", "Also the specs in the original post are now outdated; the relevant changes to versions are:\r\n - TensorFlow version: 1.14.0 (master)\r\n - Bazel version: 0.25.2\r\n", "I see that the error now points to a different missing file.\r\n\r\nCan you confirm that you are running from a fresh clone?\r\n\r\nI'm going to try to reproduce", "I was not able to reproduce at all on the latest master, fresh clone:\r\n\r\n```shell\r\n/tmp $ git clone https://github.com/tensorflow/tensorflow.git tf\r\n/tmp $ cd tf/\r\n/tmp/tf $ bazel clean --expunge\r\n/tmp/tf $ yes \"\" | ./configure \r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\n...\r\n/tmp/tf $ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n...\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 2093.317s, Critical Path: 223.10s\r\nINFO: 19513 processes: 19513 local.\r\nINFO: Build completed successfully, 20530 total actions\r\n```\r\n\r\nI have bazel 0.26 instead, but doubt this makes a difference. `bazel clean --expunge` at the beginning also should have no effect.\r\n\r\nCan you try running the following long command, exactly as written?\r\n\r\n```\r\nmkdir tf-new; cd tf-new; git clone https://github.com/tensorflow/tensorflow.git; cd tensorflow; yes \"\" | ./configure 2>&1 | tee -a github.log; bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package 2>&1 | tee -a github.log\r\n```\r\n\r\nThen, if that still fails, can you please attach `github.log` so that we can see if there is another error ahead that causes the behavior you're seeing?", "I have run the command exactly as you wrote it, and got yet another error claiming that a different file doesn't exist:\r\n```\r\nERROR: /mnt/c/Users/peter/Desktop/projects/tf-new/tensorflow/tensorflow/python/BUILD:2284:1: Executing genrule //tensorflow/python:ragged_array_ops_pygenrule failed (Aborted): bash failed: error executing command /bin/bash bazel-out/host/bin/tensorflow/python/ragged_array_ops_pygenrule.genrule_script.sh\r\n2019-08-23 14:35:54.149273: F tensorflow/python/framework/python_op_gen_main.cc:123] Non-OK-status: api_def_map.LoadFileList(env, api_files) status: Not found: tensorflow/core/api_def/base_api/api_def_Prelinearize.pbtxt; No such file or directory\r\n```\r\n\r\nThough this file does indeed exist:\r\n![image](https://user-images.githubusercontent.com/14928221/63616656-0def0a00-c5b6-11e9-8ec1-b6363e83661f.png)\r\n\r\nThe log is attached:\r\n[github.log](https://github.com/tensorflow/tensorflow/files/3535816/github.log)\r\n", "I'll also mention that the command you provided defaulted to a python2 installation, while I had previously been running `./configure` and pointing to my python3 executable.", "Start of the log, there are a bunch of errors:\r\n\r\n```\r\nImportError: No module named builtins\r\n```\r\n\r\n```\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\n```\r\n\r\nSeems to be an environment issue, not TensorFlow.", "Apologies, those errors were from setting up the environment (I had two failed runs due to `./configure` options using Python 2. Since I am not a Python 2 user, `future` and `numpy` were not installed and since I couldn't locate any explicit Python 2 requirements or a `requirements.txt` I just attempted installation until the build no longer threw import errors. Those errors ended up in the log because of the append mode `tee`).\r\n\r\nPlease review the revised log where I've removed those irrelevant runs, here:\r\n[github.log](https://github.com/tensorflow/tensorflow/files/3536095/github.log)\r\n", "Oh, makes sense.\r\n\r\nSeeing warning like the following in the log makes me wonder what compiler are you using?\r\n\r\n```\r\nINFO: From Compiling tensorflow/core/protobuf/autotuning.pb.cc [for host]:\r\nIn file included from bazel-out/host/bin/tensorflow/core/protobuf/autotuning.pb.cc:4:0:\r\nbazel-out/host/bin/tensorflow/core/protobuf/autotuning.pb.h:234:13: warning: In the GNU C Library, \"major\" is defined\r\n by <sys/sysmacros.h>. For historical compatibility, it is\r\n currently defined by <sys/types.h> as well, but we plan to\r\n remove this soon. To use \"major\", include <sys/sysmacros.h>\r\n directly. If you did not intend to use a system-defined macro\r\n \"major\", you should undefine it after including <sys/types.h>.\r\n   ::PROTOBUF_NAMESPACE_ID::int32 major() const;\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from bazel-out/host/bin/tensorflow/core/protobuf/autotuning.pb.cc:4:0:\r\nbazel-out/host/bin/tensorflow/core/protobuf/autotuning.pb.h:240:13: warning: In the GNU C Library, \"minor\" is defined\r\n by <sys/sysmacros.h>. For historical compatibility, it is\r\n currently defined by <sys/types.h> as well, but we plan to\r\n remove this soon. To use \"minor\", include <sys/sysmacros.h>\r\n directly. If you did not intend to use a system-defined macro\r\n \"minor\", you should undefine it after including <sys/types.h>.\r\n   ::PROTOBUF_NAMESPACE_ID::int32 minor() const;\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nbazel-out/host/bin/tensorflow/core/protobuf/autotuning.pb.h:374:13: warning: In the GNU C Library, \"major\" is defined\r\n by <sys/sysmacros.h>. For historical compatibility, it is\r\n currently defined by <sys/types.h> as well, but we plan to\r\n remove this soon. To use \"major\", include <sys/sysmacros.h>\r\n directly. If you did not intend to use a system-defined macro\r\n \"major\", you should undefine it after including <sys/types.h>.\r\n   ::PROTOBUF_NAMESPACE_ID::int32 major() const;\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nbazel-out/host/bin/tensorflow/core/protobuf/autotuning.pb.h:380:13: warning: In the GNU C Library, \"minor\" is defined\r\n by <sys/sysmacros.h>. For historical compatibility, it is\r\n currently defined by <sys/types.h> as well, but we plan to\r\n remove this soon. To use \"minor\", include <sys/sysmacros.h>\r\n directly. If you did not intend to use a system-defined macro\r\n \"minor\", you should undefine it after including <sys/types.h>.\r\n   ::PROTOBUF_NAMESPACE_ID::int32 minor() const;\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n\r\nAlso, could it be possible you're running out of space? Timeout? Corrupted disk? Can you try compiling on a Linux only box or directly on Windows instead of WSL? Can you reinstall the WSL?\r\n\r\nWhen the compilation ends in error, do you have the bazel-* symlinks?", "Found out the problem: it was a multithreading bug from the _host_ Windows OS that causes these kinds of 'file not found' errors in WSL if using Windows 10 OS versions before 17655.\r\n\r\nFor future reference, the problem can be resolved by updating Windows 10 to some version after 17655; thread detailing this issue [here](https://stackoverflow.com/questions/50206890/intermittent-random-file-not-found-errors-under-windows-subsystem-for-linux). After this I was able to build and install successfully.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31522\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31522\">No</a>\n"]}, {"number": 31521, "title": "Error in Android Example", "body": "Hello,\r\n\r\nI want to tell that I am using source code which I got from tensorflow.com/lite website. Now I am frustrated to solve this error. Please see screenshot and let me know. how to fix it and use this. When I start the app then it takes seconds to take to crash my app.\r\n\r\n![Screenshot (10)](https://user-images.githubusercontent.com/39325207/62836209-78cb4780-bc7e-11e9-9a8d-2bd625c9dd41.png)\r\n![Screenshot (11)](https://user-images.githubusercontent.com/39325207/62836210-7963de00-bc7e-11e9-94d6-8fa373f1fcb0.png)\r\n![Screenshot (12)](https://user-images.githubusercontent.com/39325207/62836211-7963de00-bc7e-11e9-9502-74f18d87622c.png)\r\n\r\n\r\n", "comments": ["@KanwarpartapSinghBimrah ,\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here in accessible format. Thanks\r\n", "@oanush Now it's working fine and I fix all the errors. if in future I need help I let you know.", "Thank you for the feedback.Closing since the issue is resolved."]}, {"number": 31520, "title": "yet another windows 10 build fail ( 2.0 rc.0 )", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0-rc.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.26.0 x64\r\n- GCC/Compiler version (if compiling from source):  Visual Studio 2019\r\n- CUDA/cuDNN version: 10.1 / 7.6.2\r\n- GPU model and memory: RTX 2080Ti GDDR6 11GB\r\n\r\n\r\n**Describe the problem**\r\nbuild failed \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build --config=opt --config=v2 --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: D:/repo/tensorflow/tensorflow/core/grappler/optimizers/data/BUILD:712:1: C++ compilation of rule '//tensorflow/core/grappler/optimizers/data:rebatch' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/alan-workstation/_bazel_alan-workstation/ibqopsat/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\ALAN-W~1\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\ALAN-W~1\\AppData\\Local\\Temp\r\n  C:/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-py2-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-py2-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-py2-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-py2-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-py2-opt/bin/external/jpeg /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-py2-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-py2-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-py2-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-py2-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-py2-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-py2-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-py2-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-py2-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_tensorrt /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Iexternal/eigen_archive /Ibazel-out/x64_windows-py2-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-py2-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-py2-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-py2-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-py2-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-py2-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/cublas/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX /Fobazel-out/x64_windows-py2-opt/bin/tensorflow/core/grappler/optimizers/data/_objs/rebatch/rebatch.o /c tensorflow/core/grappler/optimizers/data/rebatch.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ntensorflow/core/grappler/optimizers/data/rebatch.cc(66): fatal error C1001: An internal error has occurred in the compiler.\r\n(compiler file 'msc1.cpp', line 1468)\r\n To work around this problem, try simplifying or changing the program near the locations listed above.\r\nPlease choose the Technical Support command on the Visual C++\r\n Help menu, or open the Technical Support help file for more information\r\nInternal Compiler Error in C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\cl.exe.  You will be prompted to send an error report to Microsoft later.\r\nINTERNAL COMPILER ERROR in 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\cl.exe'\r\n    Please choose the Technical Support command on the Visual C++\r\n    Help menu, or open the Technical Support help file for more information\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1520.158s, Critical Path: 200.17s\r\nINFO: 3706 processes: 3706 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Just to verify did you get chance to follow instructions from TensorFlow [website](https://www.tensorflow.org/install/source_windows) .Please, let us know. \r\nPlease, go through the below link and see if it helps you.Thanks!\r\nhttps://github.com/tensorflow/tensorflow/issues/31085\r\n", "@ravikyram \r\n\r\nI see, so vs2019 is not supported and seems to be never?", "build success with VS2017, closing", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31520)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31520)\r\n", "Hello,\r\n\r\nI tried to compile with **VS2017**, **CUDA 10** and the bazel version listed in [tutorial](https://www.tensorflow.org/install/source_windows) but **rebatch.cc** couldn't get compiled. So I changed the file like this and the build had success.\r\n\r\nPrevious:\r\n```\r\nconstexpr std::array<const char*, 6> kBatchDatasetOps = {\r\n    kBatchOp,       kBatchV2Op,      kMapAndBatchOp, kExperimentalMapAndBatchOp,\r\n    kPaddedBatchOp, kPaddedBatchV2Op};\r\n```\r\n\r\nChanged:\r\n```\r\nconstexpr std::array<const char*, 6> kBatchDatasetOps = {\r\n    \"BatchDataset\", \"BatchDatasetV2\", \"MapAndBatchDataset\", \"ExperimentalMapAndBatchDataset\",\r\n    \"PaddedBatchDataset\", \"PaddedBatchDatasetV2\"\r\n};\r\n```\r\n\r\nGreetings!"]}, {"number": 31519, "title": "TF 2.0  ft.GradientTape() gradient()  second gradient None", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Anaconda \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): tensorflow 2.0b1\r\n- Python version:  3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nsecond gradient with input  is None\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom matplotlib import pyplot as plt\r\nimport time\r\nimport math\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow as keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.utils import plot_model\r\nfrom scipy . stats import multivariate_normal as normal\r\n\r\nd = 2\r\nbatch_size = 3\r\n\r\n\r\ndef func(x):\r\n    x = x*x\r\n    x = keras.layers.Dense(d)(x)\r\n    return x\r\n\r\nInput  = keras.Input(shape = (d,), dtype = tf.float64, name = 'X' )\r\nOutput = func(Input)\r\nmodel = keras.Model(inputs=Input, outputs=Output)\r\n\r\n\r\n\r\nx_train = tf.random.uniform( (batch_size, d), minval=0, maxval=1, dtype=tf.float64, seed=1000)\r\n\r\nfor epoch in range (1):\r\n    with tf.GradientTape() as t1:\r\n        t1.watch(x_train)\r\n        with tf.GradientTape() as t2:\r\n            t2.watch(x_train)\r\n            predictions = model( x_train )\r\n\r\n        dy_dx = t2.gradient(predictions, x_train)\r\n        print(\"dy_dx = \", dy_dx)\r\n    dyy_dx = t1.gradient(dy_dx, x_train)\r\n\r\n    print(\"dyy_dx = \", dyy_dx)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nW0811 23:13:04.473541 11804 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8>>: AssertionError: \r\ndy_dx =  tf.Tensor(\r\n[[0.03395048 0.86635576]\r\n [0.52642456 0.80100264]\r\n [1.05554004 0.05975098]], shape=(3, 2), dtype=float64)\r\ndyy_dx =  None", "comments": ["@zwenju ,\r\nHi,When tried executing the given code error `NameError: name 'neutron_network' is not defined` was faced.Thanks!", "Please , try the new code,  I have change neutron_network to func.  thanks", "@zwenju,\r\nThank you for the code,when tried executing the given code i got the output as per the screenshot\r\n![output](https://user-images.githubusercontent.com/52397990/63010108-667e2300-bea3-11e9-958e-b8a9a8e843f5.png)\r\n", "Thank you,      **dyy_dx = None**  is  actually not the expected result.\r\nCould you fix this bug ?\r\nThanks.\r\n", "This is fixed with latest version TF 2.0 nightly\r\nOutput in TF 2.0 nightly version 2.0.0-dev20190821\r\n```python\r\ndy_dx =  tf.Tensor(\r\n[[ 2.84032801 -0.34441594]\r\n [ 0.12755782 -0.69508436]\r\n [ 1.86527439 -0.02203789]], shape=(3, 2), dtype=float64)\r\ndyy_dx =  tf.Tensor(\r\n[[ 3.79678631 -0.80638027]\r\n [ 3.79678631 -0.80638027]\r\n [ 3.79678631 -0.80638027]], shape=(3, 2), dtype=float64)\r\n```", "Great Thanks. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31519\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31519\">No</a>\n"]}, {"number": 31518, "title": "saved_model_cli convert hub module to tensorRT model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Colab Notebook\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary): Provided by Colab\r\n- TensorFlow version (use command below): Provided by Colab\r\n- Python version: Provided by Colab\r\n- Bazel version (if compiling from source): Provided by Colab\r\n- GCC/Compiler version (if compiling from source): Provided by Colab\r\n- CUDA/cuDNN version: Provided by Colab\r\n- GPU model and memory: Provided by Colab\r\n\r\n**Describe the current behavior**\r\nI am saving the BigGan-Deep-512 module from the TF hub into a savedModel.\r\nAnd when I try to convert it to a tensorRT module it fails\r\n\r\n**Describe the expected behavior**\r\nI expect that the **convert** should be applicable directly to the hub module.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nCode to save the hub module to a savedModel:\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nfrom datetime import datetime\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nfrom tensorflow.python.summary import summary\r\n\r\nprint(tf.__version__)\r\n\r\nwith tf.Graph().as_default():\r\n    module = hub.Module(\"https://tfhub.dev/deepmind/biggan-deep-512/1\")\r\n    initial_inputs = {k: tf.compat.v1.placeholder(v.dtype, v.get_shape().as_list(), k)\r\n                      for k, v in module.get_input_info_dict().items()}\r\n    output = module(initial_inputs)\r\n    print(initial_inputs)\r\n    print(output)\r\n    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\r\n    with tf.Session() as session:\r\n        session.run(init_op)\r\n        tf.saved_model.simple_save(\r\n            session,\r\n            \"bg/serving_saved_model\",\r\n            inputs={'z': initial_inputs['z'], 'y': initial_inputs['y'], 'truncation': initial_inputs['truncation']},\r\n            outputs={\"output\": output},\r\n            legacy_init_op=tf.tables_initializer()\r\n        )\r\n```\r\n\r\nBash command to convert to tensorRT model:\r\n\r\n```\r\n%%bash\r\n\r\nsaved_model_dir=bg/serving_saved_model\r\nopt_model_dir=bg/opt_saved_model\r\n\r\nsaved_model_cli convert --dir=${saved_model_dir} --output_dir=${opt_model_dir} --tag_set serve tensorrt --precision_mode FP32 --max_batch_size 1 --is_dynamic_op True\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nHere are the logs: \r\n\r\n```\r\n2019-08-11 11:52:49.646188: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-08-11 11:52:49.646774: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5590ff396140 executing computations on platform Host. Devices:\r\n2019-08-11 11:52:49.646832: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-11 11:52:49.651856: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-08-11 11:52:49.799218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:52:49.799757: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5590ff3979c0 executing computations on platform CUDA. Devices:\r\n2019-08-11 11:52:49.799788: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\r\n2019-08-11 11:52:49.800013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:52:49.800421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:00:04.0\r\n2019-08-11 11:52:49.800717: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-11 11:52:49.801856: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-11 11:52:49.803002: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-11 11:52:49.803339: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-11 11:52:49.804771: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-11 11:52:49.805786: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-11 11:52:49.808901: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-11 11:52:49.809023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:52:49.809465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:52:49.809790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-08-11 11:52:49.814498: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-11 11:52:49.815576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-11 11:52:49.815605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-08-11 11:52:49.815613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-08-11 11:52:49.822976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:52:49.823434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:52:49.823790: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2019-08-11 11:52:49.823830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13500 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0811 11:52:49.824687 139988375598976 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py:245: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nW0811 11:53:27.486973 139988375598976 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nW0811 11:53:48.502595 139988375598976 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py:268: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0811 11:53:48.502814 139988375598976 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\n2019-08-11 11:53:54.431183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:53:54.431681: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-08-11 11:53:54.431812: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-08-11 11:53:54.432418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:53:54.432767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:00:04.0\r\n2019-08-11 11:53:54.432835: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-11 11:53:54.432858: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-11 11:53:54.432869: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-11 11:53:54.432885: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-11 11:53:54.432900: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-11 11:53:54.432910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-11 11:53:54.432920: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-11 11:53:54.432990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:53:54.433378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:53:54.433695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-08-11 11:53:54.433737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-11 11:53:54.433754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-08-11 11:53:54.433762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-08-11 11:53:54.434036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:53:54.434450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-11 11:53:54.434793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13500 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\r\n2019-08-11 11:54:02.561839: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: tf_graph\r\n2019-08-11 11:54:02.561909: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 7430 nodes (-10408), 12752 edges (-11753), time = 4901.51221ms.\r\n2019-08-11 11:54:02.561918: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 7580 nodes (150), 13019 edges (267), time = 683.694ms.\r\n2019-08-11 11:54:02.561924: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 7549 nodes (-31), 12986 edges (-33), time = 1578.22205ms.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/saved_model_cli\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 909, in main\r\n    args.func(args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 680, in convert_with_tensorrt\r\n    output_saved_model_dir=args.output_dir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 51, in create_inference_graph\r\n    session_config=session_config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1148, in create_inference_graph\r\n    trt_converter.save(output_saved_model_dir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 825, in save\r\n    super(TrtGraphConverter, self).save(output_saved_model_dir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 432, in save\r\n    importer.import_graph_def(self._converted_graph_def, name=\"\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 431, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: Input 0 of node module_apply_default/cond/AssignVariableOp/Switch was passed float from module/prev_truncation:0 incompatible with expected resource.\r\n\r\n```\r\n", "comments": ["I was able to reproduce the reported behavior with tf-nightly (1.15.0-dev20190812) as well.", "@aaroey Any update on the issue?", "@vio-codes sorry for the late response. I think the root cause is not in TF-TRT but in the tool that freezes the graph. I can reproduce the problem with the following script:\r\n```\r\nismdir = \"the directory containing the saved hub model\"\r\ng = tf.Graph()\r\nsess = tf.Session()\r\nmeta_graph_def = loader.load(sess, [tag_constants.SERVING], ismdir)\r\noutput_node_names = set()\r\n\r\ndef _gather_names(tensor_info):\r\n  return set([tensor_info[key].name.split(\":\")[0] for key in tensor_info])\r\n\r\nfor key in meta_graph_def.signature_def:\r\n  signature_def = meta_graph_def.signature_def[key]\r\n  output_node_names.update(_gather_names(signature_def.inputs))\r\n  output_node_names.update(_gather_names(signature_def.outputs))\r\nfrozen_graph_def = graph_util.convert_variables_to_constants(\r\n    sess, sess.graph.as_graph_def(add_shapes=True), list(output_node_names))\r\ng = tf.Graph()\r\nwith g.as_default():\r\n  importer.import_graph_def(frozen_graph_def, name=\"\")\r\n```\r\nWill need more time to investigate.", "The reason is, both `tf.compat.v1.graph_util.convert_variables_to_constants` and `convert_variables_to_constants_v2` doesn't update the datatype of the Switch nodes if their input was DT_RESOURCE before the variable->constant conversion. \r\n\r\n@gargn could you help to take a look at the v2 case? Thanks a lot.", "Hello @gargn ,\r\nAny update related to this issue?", "Hi There,\r\n\r\nWe are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31518\">No</a>\n"]}, {"number": 31517, "title": "How to reduce the memory use of image operations", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 1809 x64\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7.3 x64\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda_10.0.130_411.31_win10 & cudnn-10.0-windows10-x64-v7.5.1.10\r\n- GPU model and memory: GTX 1070 Max-Q 6376 MB\r\n\r\n**Describe the current behavior**\r\n\r\nI want to do some operations like:\r\n1. Split the image into 12*12 squares.\r\n2. For each sub square, split it into 6*(2*12) horizontal & 6*(12*2) vertical rectangles.\r\n3. For each rectangle, calculate it's pixels horizontal & vertical gradient's mean and standard deviation.\r\nSo, for each sub square, i want 48 values, which is \r\n6*(hori_gx_mean,hori_gy_mean,hori_gx_std,hori_gy_std)----24\r\nand\r\n6*(vert_gx_mean,vert_gy_mean,vert_gx_std,vert_gy_std)----24\r\n\r\nNow I have a 1920 * 1280 image with 3 chanels.\r\nI read image by \r\n`tf.Session().run(tf.image.decode_image(tf.read_file(imgDir)))/255.0`\r\n\r\nand calculate image by \r\n\r\n`\r\n    for y in range(int(srcImg.shape[0]/12)-1):\r\n        for x in range(int(srcImg.shape[1]/12)-1):\r\n            sub12_12 = tf.image.crop_to_bounding_box(_src, y*12, x*12, 12, 12)\r\n            # computes every 2*12 and 12*2 subSet in sub12_12\r\n            for div in range(6):\r\n                # height=2 width=12\r\n                sub12_2 = tf.image.crop_to_bounding_box(\r\n                    sub12_12, div * 2, 0, 2, 12)\r\n                # height=12 width=2\r\n                sub2_12 = tf.image.crop_to_bounding_box(\r\n                    sub12_12, 0, div* 2, 12, 2)\r\n                    \r\n                # computes gradient, return (dy, dx)\r\n                _dy_12_2, _dx_12_2 = tf.image.image_gradients(\r\n                    tf.image.rgb_to_grayscale(sub12_2))\r\n                _dy_2_12, _dx_2_12 = tf.image.image_gradients(sub2_12)\r\n\r\n                # computes standard deviation\r\n                _stDy12_2 = tf.math.reduce_std(_dy_12_2)\r\n                _stDx12_2 = tf.math.reduce_std(_dx_12_2)\r\n\r\n                _stDy2_12 = tf.math.reduce_std(_dy_2_12)\r\n                _stDx2_12 = tf.math.reduce_std(_dx_2_12)\r\n                # compute gradient's mean\r\n                _uDy12_2 = tf.math.reduce_mean(_dy_12_2)\r\n                _uDx12_2 = tf.math.reduce_mean(_dx_12_2)\r\n\r\n                _uDy2_12 = tf.math.reduce_mean(_dy_2_12)\r\n                _uDx2_12 = tf.math.reduce_mean(_dx_2_12)\r\n\r\n                # 6 set of horizontal and vertical sub-rectangle feature\r\n                feaBuf.append([_uDy12_2, _uDx12_2, _stDy12_2, _stDx12_2,\r\n                                _uDy2_12, _uDx2_12, _stDy2_12, _stDx2_12])\r\n            _retExp.append(feaBuf)\r\n            feaBuf=[]\r\n`\r\n\r\nand cost about 0.5g of memory per line of image, and it is\u2018t affordable.\r\n\r\n**Describe the expected behavior**\r\n\r\nReduce memory cost to about 500kB per line.\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 31516, "title": "tensorflow 2.0 tensorflow.python.eager.function.TfMethodTarget object at could not be transformed and will be executed as-is.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA485A58>> could not be transformed and will be executed as-is. \r\nPlease report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA485A58>>: AssertionError: W0811 12:49:24.207197  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA4A82B0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA4A82B0>>: AssertionError:\r\n\r\n**System information**\r\nWindows 10 , Anaconda, Python 3.7  Tensorflow 2.0b1\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport time\r\nimport math\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow as keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.utils import plot_model\r\nfrom scipy . stats import multivariate_normal as normal\r\n\r\nd = 10\r\nT = 0.1\r\nn_time = 5\r\nn_sample = 100\r\nbatch_size = 100\r\nn_maxstep = 400\r\nh = (T + 0.0) / n_time\r\nt_stamp = np.arange (0, n_time) * h\r\n\r\ndef f_tf (t, X, Y, Z):\r\n    V =  Y - tf.math.sin (Y)\r\n    return V\r\n\r\ndef g_tf (t, X):\r\n    V =  tf.math.reduce_sum (X**3, 1,keepdims=True)\r\n    return V\r\n\r\ndef k_tf ( n_sample ):\r\n    W = np.zeros ([ n_sample, d, n_time  ], dtype = np.float64)\r\n    X_sample  = np.zeros ([ n_sample, d, n_time+1], dtype = np.float64)\r\n    for i in range (n_time):\r\n        W [:, :, i  ] = np.reshape ( normal.rvs ( mean = np.zeros(d,dtype = np.float64),\\\r\n                                      cov =1, size = n_sample ), ( n_sample, d))\r\n        X_sample  [:, :, i+1] =  W [:, :, i]\r\n    return W, X_sample\r\n\r\ndef nn_tf(x):\r\n    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)\r\n    x = keras.layers.Dense(d, batch_size = n_sample)(x)\r\n    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)\r\n    return x\r\n\r\ndW = keras.Input(shape = (d, n_time  ), batch_size=n_sample, dtype = tf.float64, name = 'dW')\r\nXX = keras.Input(shape = (d, n_time+1), batch_size=n_sample, dtype = tf.float64, name = 'X' )\r\nX = XX\r\nY = tf.zeros([n_sample, 1], dtype = tf.float64)\r\nZ = tf.zeros([n_sample, d], dtype = tf.float64)\r\n\r\nfor it in range(n_time-1):\r\n    with tf.name_scope(str(it+1)):\r\n        Y = Y +  tf.math.reduce_sum( Z * dW[:,:,it],  1, keepdims=True)\r\n        subX = tf.reshape(X[:,:,it], shape = [n_sample, d])\r\n        Z = nn_tf(subX) / d\r\n\r\nY = Y + tf.math.reduce_sum (Z * dW [:, :, n_time-1], 1, keepdims=True)\r\nmodel = keras.Model(inputs=[XX,dW], outputs=[Y])\r\n\r\noptimizer = keras.optimizers.Adam(learning_rate=1e-3)\r\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\ndW_train, X_train = k_tf ( n_sample )\r\n\r\nfor epoch in range (10):\r\n    with tf.GradientTape() as tape:\r\n        predictions = model( [X_train, dW_train] )\r\n        label = g_tf (T, X_train[:, :, n_time])\r\n        loss_value = tf.reduce_sum( tf.keras.losses.MSE (label, predictions ) )\r\n    grads = tape.gradient(loss_value,  model.trainable_variables)\r\n    optimizer.apply_gradients( zip(grads, model.trainable_variables) )\r\n    accuracy = train_accuracy(label, predictions)\r\n    print(\"step \", epoch, \"loss = \", loss_value.numpy(), \"accuracy = \", accuracy.numpy())\r\n\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["@zwenju ,\r\nI was unable execute the give code, can you please provide the code with proper indentation?Thanks", "Thank you,  the new code is provided", "@zwenju ,\r\nThank you for the updated code,when tried executing the same I got the output as per the screenshot\r\n![output](https://user-images.githubusercontent.com/52397990/63011334-bcec6100-bea5-11e9-967e-2f2f8ef832dd.png)\r\n ", "Please run this code twice continuously,   I find the first run no errors,    the second run will show the errors.", "I tried executing the given code in both colab and Jupyter notebook,couldn't replicate the issue.", "Thank you\uff0cmay be it a bug of Spyder?  \r\nI am not sure why I got this error.\r\nthanks again.", "@zwenju I think there are issues with `dtype` in your code specifically related to `keras.layers.BatchNormalization`. In `keras.layers.BatchNormalization`, if you don't set `dtype`, it will use default dtype which is `tf.float32`. Most of your code user `tf.float64` except in the `keras.layers.BatchNormalization` layer. \r\n\r\nYou can remove the errors in two ways\r\n1. use `dtype=tf.float64` in the `keras.layers.BatchNormalization` layer.  \r\n2. If you think you want set default `dtype` as `tf.float64` then add `tf.keras.backend.set_floatx('float64')` at the top of the program as shown in the code below. \r\n\r\n```\r\nimport time\r\nimport math\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow as keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.utils import plot_model\r\nfrom scipy . stats import multivariate_normal as normal\r\n# tf.keras.backend.set_floatx('float64') # sets dtype as tf.float64\r\n\r\nd = 10\r\nT = 0.1\r\nn_time = 5\r\nn_sample = 100\r\nbatch_size = 100\r\nn_maxstep = 400\r\nh = (T + 0.0) / n_time\r\nt_stamp = np.arange (0, n_time) * h\r\n\r\ndef f_tf (t, X, Y, Z):\r\n    V =  Y - tf.math.sin (Y)\r\n    return V\r\n\r\ndef g_tf (t, X):\r\n    V =  tf.math.reduce_sum (X**3, 1,keepdims=True)\r\n    return V\r\n\r\ndef k_tf ( n_sample ):\r\n    W = np.zeros ([ n_sample, d, n_time  ], dtype = np.float64)\r\n    X_sample  = np.zeros ([ n_sample, d, n_time+1], dtype = np.float64)\r\n    for i in range (n_time):\r\n        W [:, :, i  ] = np.reshape ( normal.rvs ( mean = np.zeros(d,dtype = np.float64),\\\r\n                                      cov =1, size = n_sample ), ( n_sample, d))\r\n        X_sample  [:, :, i+1] =  W [:, :, i]\r\n    return W, X_sample\r\n\r\ndef nn_tf(x):\r\n    x = keras.layers.BatchNormalization(batch_size = n_sample,dtype=tf.float64)(x)\r\n    x = keras.layers.Dense(d, batch_size = n_sample)(x)\r\n    x = keras.layers.BatchNormalization(batch_size = n_sample,dtype=tf.float64)(x)\r\n    return x\r\n\r\ndW = keras.Input(shape = (d, n_time  ), batch_size=n_sample, dtype = tf.float64, name = 'dW')\r\nXX = keras.Input(shape = (d, n_time+1), batch_size=n_sample, dtype = tf.float64, name = 'X' )\r\nX = XX\r\nY = tf.zeros([n_sample, 1], dtype = tf.float64)\r\nZ = tf.zeros([n_sample, d], dtype = tf.float64)\r\n\r\nfor it in range(n_time-1):\r\n    with tf.name_scope(str(it+1)):\r\n        Y = Y +  tf.math.reduce_sum( Z * dW[:,:,it],  1, keepdims=True)\r\n        subX = tf.reshape(X[:,:,it], shape = [n_sample, d])\r\n        Z = nn_tf(subX) / d\r\n\r\nY = Y + tf.math.reduce_sum (Z * dW [:, :, n_time-1], 1, keepdims=True)\r\nmodel = keras.Model(inputs=[XX,dW], outputs=[Y])\r\n\r\noptimizer = keras.optimizers.Adam(learning_rate=1e-3)\r\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\ndW_train, X_train = k_tf ( n_sample )\r\n\r\nfor epoch in range (10):\r\n    with tf.GradientTape() as tape:\r\n        predictions = model( [X_train, dW_train] )\r\n        label = g_tf (T, X_train[:, :, n_time])\r\n        loss_value = tf.reduce_sum( tf.keras.losses.MSE (label, predictions ) )\r\n    grads = tape.gradient(loss_value,  model.trainable_variables)\r\n    optimizer.apply_gradients( zip(grads, model.trainable_variables) )\r\n    accuracy = train_accuracy(label, predictions)\r\n    print(\"step \", epoch, \"loss = \", loss_value.numpy(), \"accuracy = \", accuracy.numpy())\r\n```\r\n\r\nI am closing this issue as it was resolved. Feel free to reopen if the issue persists again. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/31720e5d1d44dcb9ffeaa84c98301307/tf_31516_batchnormalization.ipynb) is the gist for your reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31516\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31516\">No</a>\n", "Thanks."]}, {"number": 31515, "title": "tensorflow 2.0   tensorflow.python.eager.function.TfMethodTarget object could not be transformed and will be executed as-is", "body": "WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361ABA58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361ABA58>>: AssertionError: \r\nW0811 11:46:20.769339  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361AB470>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361AB470>>: AssertionError: \r\nW0811 11:46:20.802253  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1748>>: AssertionError: \r\nW0811 11:46:20.815217  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1860>>: AssertionError: \r\nW0811 11:46:20.827151  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361FB1D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361FB1D0>>: AssertionError: \r\nW0811 11:46:20.838119  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5360F8DD8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5360F8DD8>>: AssertionError: \r\nW0811 11:46:20.852117  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362195F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362195F8>>: AssertionError: \r\nW0811 11:46:20.863052  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536110F98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536110F98>>: AssertionError: \r\nW0811 11:46:20.879015  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362362B0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362362B0>>: AssertionError: \r\nW0811 11:46:20.888983  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536236320>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536236320>>: AssertionError: \r\nW0811 11:46:20.905940  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536247B38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536247B38>>: AssertionError: \r\nW0811 11:46:20.917908  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53626E0B8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53626E0B8>>: AssertionError: \r\nW0811 11:46:20.938849  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53627C550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53627C550>>: AssertionError: \r\nW0811 11:46:20.948858  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC668>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC668>>: AssertionError: \r\nW0811 11:46:20.961826  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362BC588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362BC588>>: AssertionError: \r\nW0811 11:46:20.970813  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362D5048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362D5048>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361AB470>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361AB470>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1748>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361F1860>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361FB1D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5361FB1D0>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5360F8DD8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5360F8DD8>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362195F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362195F8>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536110F98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536110F98>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362362B0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362362B0>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536236320>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536236320>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536247B38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536247B38>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53626E0B8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53626E0B8>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53627C550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53627C550>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC668>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC668>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362BC588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362BC588>>: AssertionError: \r\nW0811 11:46:21.001721  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362F85F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362F85F8>>: AssertionError: \r\nW0811 11:46:21.017639  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC208>>: AssertionError: \r\nW0811 11:46:21.033596  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536340080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536340080>>: AssertionError: \r\nW0811 11:46:21.070497  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536350390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536350390>>: AssertionError: \r\nW0811 11:46:21.083488  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536395C88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536395C88>>: AssertionError: \r\nW0811 11:46:21.097465  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5363BF6A0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5363BF6A0>>: AssertionError: \r\nW0811 11:46:21.119367  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5363D2E48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5363D2E48>>: AssertionError: \r\nW0811 11:46:21.144337  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5364100F0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5364100F0>>: AssertionError: \r\nW0811 11:46:21.158263  2284 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53642CE10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D53642CE10>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362D5048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362D5048>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362F85F8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362F85F8>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5362AC208>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536340080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536340080>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536350390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536350390>>: AssertionError: \r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536395C88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000001D536395C88>>: AssertionError: ", "comments": []}, {"number": 31514, "title": "Bug in keras.layers.DepthwiseConv2D when using stride and dilation", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (attached)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: Nvidia Quadro P2000 with 4GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen executing the following lines of code, I get an assertion, because the result is not as expected, but actually the values [-29, -19, 1]. I would see how the -19 would be correct if I had a stride of 1 along the y-axis, however, then it would not match up again with the third number being 1 instead of -4. So it is not only that either dilation or stride is ignored, but actually providing for both non-trivial values results in some strange behavior, which I think is actually a bug.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nkernel = np.array([5, 5], dtype=np.float32).reshape([1, 2, 1, 1])\r\nbias = np.array([-4], dtype=np.float32)\r\nimg = np.array([-2, -1, -3, -2, 3, -1, -2, -1, 2], dtype=np.float32).reshape([1, 1, 9, 1])\r\n\r\nstrides = (2, 2)\r\ndilation = (1, 2)\r\n\r\ndepthconv2d_layer = tf.keras.layers.DepthwiseConv2D(\r\n    depth_multiplier=1,\r\n    kernel_size=(1, 2),\r\n    strides=strides,\r\n    dilation_rate=dilation,\r\n    padding='valid',\r\n    depthwise_initializer=lambda *args, **kwargs: tf.constant(kernel),\r\n    bias_initializer=lambda *args, **kwargs: tf.constant(bias),\r\n)\r\ndepthconv2d_output = depthconv2d_layer(tf.constant(img))\r\n\r\nwith tf.Session() as s:\r\n    s.run(tf.global_variables_initializer())\r\n    result = s.run(depthconv2d_output)\r\n    print(result)\r\n    assert np.all(result == np.array([-29, -4, 1, -4]).reshape(1, 1, 4, 1))\r\n```\r\n**Describe the expected behavior**\r\nNo assertion, as I computed the values by hand and expect them to be right.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nNothing else.\r\n", "comments": ["Seems that this is not a valid argument combination, but TF does not try to raise the assertion or print some warning message.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py#L1700-L1705\r\n>> strides: An integer or tuple/list of 2 integers,\r\n      specifying the strides of the convolution along the height and width.\r\n      Can be a single integer to specify the same value for\r\n      all spatial dimensions.\r\n      Specifying any stride value != 1 is incompatible with specifying\r\n      any `dilation_rate` value != 1.", "I was aware of this documentation, however this is only noted on the regular convolutions. For the depthwise convolution this documentation (and the raising of an exception) is either missing, or the case is not correctly implemented! I assumed the second case is true, because depthwise convolutions are probably handled differently concerning GPU optimization which might not yield the restriction of being able to specify only one of stride or dilation. Also I find it too coincidental that the docs and the exception is missing.", "I was able to reproduce the issue with 1.13.1 and even with `tf-nightly`. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/4057ece04f24fa13db3c57c3bb36bad0/tf_31514.ipynb) is the gist. Thanks!", "@DavidS3141 @WindQAQ   Sorry for the delay! The link is outdated: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py#L1700-L1705\r\n\r\nDo you have a new link to what it means by \"Seems that this is not a valid argument combination, but TF does not try to raise the assertion or print some warning message\"?", "It is said here:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\r\nunder the argument description for dilation rate!", "> It is said here:\r\n> https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\r\n> under the argument description for dilation rate!\r\n\r\nThe `dilation_rate` is not an argument for DepthwiseConv2D, right?\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D\r\n\r\n(We should probably just error out if user passes dilation_rate into DepthwiseConv2D thought, instead of processing it silently, or implement it)", "Yes that is true, quite a lot changed of course since TF 1.13.1 which I used in August when I opened this issue. Then the argument was still documented for that version and I opened this issue. TF 1.15 API docs has also no mention of this argument. Not sure what happens with these newer versions when passing the `dilation_rate` as argument to the Depthwise Conv2D", "\r\n\r\n\r\n\r\n> Yes that is true, quite a lot changed of course since TF 1.13.1 which I used in August when I opened this issue. Then the argument was still documented for that version and I opened this issue. TF 1.15 API docs has also no mention of this argument. Not sure what happens with these newer versions when passing the `dilation_rate` as argument to the Depthwise Conv2D\r\n\r\nYeah it was never part of the API, but it works as intended. I think we should add it to _init_.\r\nOn the other hand, I tried to do this through:\r\n```python\r\ntf.nn.depthwise_conv2d(tf.constant(img), tf.reshape(tf.constant([5., 5.]), (1, 2, 1, 1)), strides=(1, 2, 2, 1), padding='VALID', data_format='NHWC', dilations=(1, 2))\r\n```\r\nIt looks like the result is the same as the keras layer. I think this is probably a low level bug instead of a tf.keras bug then?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31514\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31514\">No</a>\n"]}, {"number": 31513, "title": "tf.keras.metrics.TruePositives() returning wrong value in model.fit() when passed as metric to model.compile()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.7.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen I pass one-hot encoded labels as train and validation data into tensorflow keras' model.fit() function, the metric `tf.keras.metrics.TruePositives()` (and TN, FN, FP) return wrong values.\r\nIf\r\n`train_labels` is this: `array([[1, 0], [1, 0], [0, 1]])`\r\n\r\nand the resulting `y_pred`'s are `array([[1, 0], [1, 0], [0, 1]])`\r\n\r\nthen the sum of\r\n```\r\ntf.keras.metrics.TruePositives()\r\ntf.keras.metrics.TrueNegatives() \r\ntf.keras.metrics.FalsePositives() \r\ntf.keras.metrics.FalseNegatives()\r\n```\r\nis 6.\r\n\r\n**Describe the expected behavior**\r\nThe sum should be 3.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import models, layers\r\nimport numpy as np\r\n\r\n\r\n# Generate Data\r\ntrain_data = np.random.randint(0, 100, size=(10, 215, 215, 1))\r\ntrain_labels = np.eye(2)[np.random.randint(0, 2, size=(10, 1)).reshape(-1)]\r\n\r\n\r\n# Set up metrics\r\nmetrics = ['accuracy',\r\n           tf.keras.metrics.TruePositives(),\r\n           tf.keras.metrics.TrueNegatives(),\r\n           tf.keras.metrics.FalseNegatives(),\r\n           tf.keras.metrics.FalsePositives()]\r\n\r\n# Set up model type\r\nmodel = models.Sequential(name='CNN')\r\n\r\n# Add layers\r\nmodel.add(layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), input_shape=train_data[0].shape))\r\nmodel.add(layers.Conv2D(filters=64, kernel_size=(5, 5), strides=(2, 2)))\r\nmodel.add(layers.Conv2D(filters=128, kernel_size=(5, 5), strides=(2, 2)))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(units=128))\r\nmodel.add(layers.BatchNormalization())\r\nmodel.add(layers.ReLU())\r\nmodel.add(layers.Dense(units=2, activation='sigmoid'))\r\nmodel.compile('sgd', 'binary_crossentropy', metrics)\r\n\r\nhistory = model.fit(train_data, train_labels, batch_size=1, epochs=30)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThis also impacts recall, precision, etc. which are not correctly calculated as a result.", "comments": ["Ok I did some more experimenting and it is fixed when the input is not 1-hot encoded and there is only 1 output neuron. So at this point I am not sure if this is intended behaviour, but all metrics run correctly if we change the following 2 lines:\r\n\r\nThis: `train_labels = np.eye(2)[np.random.randint(0, 2, size=(10, 1)).reshape(-1)]`\r\nTo: `train_labels = np.random.randint(0, 2, size=(10, 1))`\r\nand\r\nThis: `model.add(layers.Dense(units=2, activation='sigmoid'))`\r\nTo: `model.add(layers.Dense(units=1, activation='sigmoid'))`", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}, {"number": 31512, "title": "Remove executable mode from `tensorflow/c/eager/c_api.h`.", "body": "This fixes a codesigning issue for [Swift for TensorFlow](https://github.com/apple/swift/tree/tensorflow) on macOS:\r\n```console\r\n# After this patch: https://github.com/apple/swift/pull/26595.\r\n$ ./swift/utils/build-toolchain-tensorflow --pkg\r\n...\r\nerror: cannot parse the debug map for './/Library/Developer/Toolchains/swift-tensorflow-LOCAL-2019-08-09-a.xctoolchain/usr/Library/Frameworks/LLDB.framework/Versions/A/Resources/Swift/macosx/x86_64/modulemaps/CTensorFlow/c_api_eager.h-e': The file was not recognized as a valid object file\r\nerror: cannot parse the debug map for './/Library/Developer/Toolchains/swift-tensorflow-LOCAL-2019-08-09-a.xctoolchain/usr/Library/Frameworks/LLDB.framework/Versions/A/Resources/Swift/macosx/x86_64/modulemaps/CTensorFlow/c_api_eager.h': The file was not recognized as a valid object file\r\n```", "comments": ["I'm confused, github says \"no changes\"?", "Oh I see nvm", "> I'm confused, github says \"no changes\"?\r\n\r\nThere are no content changes - the change is actually `chmod -x tensorflow/c/eager/c_api.h`.\r\n\r\nPrior to this patch, `tensorflow/c/eager/c_api.h` had executable mode for some reason. Other headers in the same directory do not:\r\n\r\n```\r\n$ ls -alh tensorflow/c/eager\r\ntotal 600\r\ndrwxr-xr-x  15 danielzheng  primarygroup   480B Jul 31 14:26 .\r\ndrwxr-xr-x  51 danielzheng  primarygroup   1.6K Jul 31 14:26 ..\r\n-rw-r--r--   1 danielzheng  primarygroup   9.9K Jul 31 14:26 BUILD\r\n-rw-r--r--   1 danielzheng  primarygroup    40K Jul 31 14:26 c_api.cc\r\n-rwxr-xr-x   1 danielzheng  primarygroup    23K Jul 31 14:26 c_api.h\r\n-rw-r--r--   1 danielzheng  primarygroup   5.6K Jul 31 14:26 c_api_debug.cc\r\n-rw-r--r--   1 danielzheng  primarygroup   1.8K Jul 31 14:26 c_api_debug_test.cc\r\n-rw-r--r--   1 danielzheng  primarygroup    21K Jul 31 14:26 c_api_experimental.cc\r\n-rw-r--r--   1 danielzheng  primarygroup    18K Jul 31 14:26 c_api_experimental.h\r\n-rw-r--r--   1 danielzheng  primarygroup    20K Jul 31 14:26 c_api_experimental_test.cc\r\n-rw-r--r--   1 danielzheng  primarygroup    10K Jul 31 14:26 c_api_internal.h\r\n-rw-r--r--   1 danielzheng  primarygroup    69K Jul 31 14:26 c_api_test.cc\r\n-rw-r--r--   1 danielzheng  primarygroup   8.1K Jul 31 14:26 c_api_test_util.cc\r\n-rw-r--r--   1 danielzheng  primarygroup   2.5K Jul 31 14:26 c_api_test_util.h\r\n-rw-r--r--   1 danielzheng  primarygroup    43K Jul 31 14:26 tape.h\r\n```\r\n\r\nThe fact that `c_api.h` has executable mode causes a codesigning issue for Swift for TensorFlow, as mentioned in the PR description.", "@yifeif @mihaimaruseac any idea how we merge this ?", "Oh, it needs to be fixed from the inside. Let me give it a try", "Actually, seems copybara migration worked this time. It will get merged, thanks for the PR", "@mihaimaruseac thank you "]}, {"number": 31511, "title": "Cherrypicks to fix the disconnected graph issue, and missing CUDA compute capabilities.", "body": "Cherrypicks to fix the disconnected graph issue, and missing CUDA compute capabilities.", "comments": []}, {"number": 31510, "title": "TF 1.14 on AI Platform: MirroredStrategy fails on 2 GPUs with RuntimeError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): AI Platform 1.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.14 \r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: AI Platform\r\n- GPU model and memory: V100 16GB\r\n\r\n**Describe the current behavior**\r\n\r\nI have written a custom estimator and wanted to train it on 2 GPUs using `tf.distribute.MirroredStrategy`, submitting [this job](https://github.com/sdll/psenet/blob/cc3041da4565da03f5ebe206962ee424c1f36bbb/train.sh#L4) to the AI Platform. Unfortunately, after ~900 steps, the training fails with `RuntimeError: Variable creator scope nesting error: move call to tf.distribute.set_strategy() out of `with` scope.`\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should train on two GPUs asynchronously.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe estimator definition starts [here](https://github.com/sdll/psenet/blob/cc3041da4565da03f5ebe206962ee424c1f36bbb/psenet/train.py#L193).\r\n\r\n**Other info / logs**\r\n\r\n- [the entire AI platform log](https://github.com/tensorflow/tensorflow/files/3489041/log_rc47.txt)\r\n- another curious detail is that the estimator seems to be executed only on a single GPU.\r\n\r\nTurning on the placement logging with a similar set-up confirmed that training just used the first GPU before failing.\r\n\r\nHere is the usage graph, where you can see a short spike before the failure:\r\n\r\n<img width=\"975\" alt=\"Screen Shot 2019-08-10 at 6 42 35 PM\" src=\"https://user-images.githubusercontent.com/17913919/62823852-b0fb5900-bb9e-11e9-8cd2-d4eacd73cfeb.png\">\r\n", "comments": ["You can use `MirroredStrategy` with one GPU; perhaps try that to suss out other errors?\r\n\r\nYou haven't by chance tried this on a local or AWS instance, have you?  I have recently seen Google Cloud instances give me multiple GPUs but one or more of them showed 100% utilization in `nvidia-smi` even though I had no processes running in my VM at the time.  It's possible that bugs in Google Cloud are confounding the problem.", "@pwais, thanks for your reply! I have switched to using 4 K80 Teslas instead of 2 V100 GPUs and also disabled evaluation by replacing `train_and_evaluate` with `train`, and the training started working fine. One difference between training and evaluation is that during evaluation I skip cropping the image for data augmentation, which results in a larger input. Wonder whether this is a latent OOM error.\r\n\r\nI do not have a local GPU and have not tried training on AWS.", "I think that variable scope error isn't the root cause, rather this is:\r\n```\r\nInternalError: File contents are inconsistent for file: gs://gsoc-tfjs/weights/psenet/custom/psenet_rc47/checkpoint @ 0.\r\ngs://gsoc-tfjs/weights/psenet/custom/psenet_rc47/checkpoint: Checkpoint ignored\r\nEstimator is not trained yet. Will start an evaluation when a checkpoint is ready.\r\n```\r\n\r\nIs this failure deterministic?   Does it happen every time you run it with 2 GPUs?", "@isaprykin, yes, this seems to happen only during the evaluation phase. I have managed to revert to `train_and_evaluate` for a multi-worker multi-gpu setup and avoid this problem by setting a humongous `eval-throttle-secs`.", "Does it happen every time during the evaluation phase when using `train_and_evaluate`?", "It failed two times in a row at about the time evaluation must have started, after which I have switched to training without evaluation, so I assume it is reproducible. ", "Is it possible that multiple jobs are using the same directory for their checkpoint in parallel?", "I make sure that each job creates its own folder with weights in the bucket. \r\n\r\nWonder whether setting MirroredStrategy is a bad idea as eval_strategy, or maybe keep_checkpoint_every_n_hours does not play well with save_checkpoint_secs.", "Do you mind running the code with some extra logging turned on?\r\n\r\nos.environ[\"TF_CPP_VMODULE\"] = \"gcs_file_system=2\"\r\nYou might need to remove this line too:\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\n", "I tried running the code again, reducing the time until the first evaluation, and the job failed with a simple OOM. Closing the issue, since it is not reproducible.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31510\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31510\">No</a>\n"]}, {"number": 31509, "title": "BaseCollectiveExecutor::StartAbort Out of range: warnings when fit model in graph mode (TF 2.0 Nightly)", "body": "**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from : Binary\r\n- TensorFlow version : TF 2.0 Nightly GPU Preview\r\n- Python version:3.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 960M\r\n\r\nI have a very simple model that I have made by inheriting from tf.Keras.model, which I feed with a dataset i,e\r\n\r\nmodel = MyModel(...)\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, amsgrad=True),\r\n                       loss=loss_fn,\r\n                       run_eagerly=False)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((x, y))\r\ndataset = dataset.shuffle(buffer_size=10000)\r\ndataset = dataset.batch(batch_size=1000)\r\n\r\nmodel.fit(dataset,\r\n          epochs=100,\r\n          verbose=0,\r\n          callbacks=[LossAndErrorPrintingCallback()])\r\n\r\nIf I run this using TF 2.0 (beta), works perfectly fine i.e with run_eagerly=False. If I run it using TF nightly preview with run_eagerly=True, again fine. However if I try with run_eagerly=False using nightly preview I get a stream of the following warnings,\r\n\r\n 2019-08-10 16:35:40.168418: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}}]]", "comments": ["@oracle3001 ,\r\nIn order to expedite the trouble-shooting process, can you please provide complete code snippet to reproduce the issue reported here.Thanks!\r\n", "Here is a minimal model that produces the same issue when using TF 2.0 nightly (but not TF 2.0 beta)...\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_graphics as tfg\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import math_ops\r\nimport numpy as np\r\n\r\n\r\nclass MinimalModelTest(tf.keras.Model):\r\n\r\n    def __init__(self,\r\n                 control_vert_pos,\r\n                 dtype=tf.float64):\r\n        super(MinimalModelTest, self).__init__(dtype=dtype)\r\n        self.t_y_spherical = tfg.math.math_helpers.cartesian_to_spherical_coordinates(control_vert_pos)\r\n        self.n_control_verts = control_vert_pos.shape[0]\r\n        self.t_y_radius = self.t_y_spherical[:, 0]\r\n        self.av_radius = np.mean(self.t_y_radius.numpy())\r\n\r\n        self.t_av_radius = tf.Variable(self.av_radius,\r\n                                       name='t_av_radius',\r\n                                       dtype=self.dtype,\r\n                                       trainable=False)\r\n\r\n        self.t_y_polar = tf.Variable(self.t_y_spherical[:, 1:3],\r\n                                     name='t_y_polar',\r\n                                     dtype=self.dtype,\r\n                                     trainable=True)\r\n\r\n    def build(self, input_shape):\r\n        super(MinimalModelTest, self).build(input_shape)\r\n\r\n    def spherical_to_cartesian_coordinates(self, point_spherical, name=None):\r\n        r, theta, phi = tf.unstack(point_spherical, axis=-1)\r\n        # r = asserts.assert_all_above(r, 0)\r\n        tmp = r * tf.sin(theta)\r\n        x = tmp * tf.cos(phi)\r\n        y = tmp * tf.sin(phi)\r\n        z = r * tf.cos(theta)\r\n        return tf.stack((x, y, z), axis=-1)\r\n\r\n    def loss_fn(self, y_true, y_pred):\r\n        y_pred = ops.convert_to_tensor(y_pred)\r\n        y_true = math_ops.cast(y_true, y_pred.dtype)\r\n        return tf.reduce_mean(tf.square(y_true - y_pred))\r\n\r\n    def call(self, inputs):\r\n        t_radius = self.t_av_radius * tf.ones([self.n_control_verts, 1], dtype=self.dtype)\r\n        t_cv_sphere_spherical = tf.concat([t_radius, self.t_y_polar], axis=-1)\r\n        return self.spherical_to_cartesian_coordinates(t_cv_sphere_spherical)\r\n\r\n\r\n\r\nn_points = 10\r\n# generate_prototypes produce n 3d points that lie on a unit sphere\r\nrand_sphere_points_start = np.asarray(generate_prototypes(n_points))\r\nrand_sphere_points_target = np.asarray(generate_prototypes(n_points))\r\n\r\njoint_sample_model = MinimalModelTest(\r\n        control_vert_pos=rand_sphere_points_start)\r\n\r\noptimizer = tf.optimizers.Adam(learning_rate=0.005, amsgrad=True)\r\njoint_sample_model.compile(optimizer=optimizer,\r\n              loss=[joint_sample_model.loss_fn],\r\n              run_eagerly=False)\r\n\r\nbatch_size = n_points\r\ndataset = tf.data.Dataset.from_tensor_slices((rand_sphere_points_start, rand_sphere_points_target))\r\ndataset = dataset.batch(batch_size=batch_size)\r\n\r\njoint_sample_model.fit(dataset,\r\n          epochs=100,\r\n          verbose=1)\r\n```", "@oracle3001 ,\r\nWhen tried executing the given code, error `NameError: name 'generate_prototypes' is not defined` was faced.Thanks!", "Sorry, yes it is just a function to create random points. Here is the missing functions.\r\n\r\n```\r\ndef polar2cartesian(theta, phi):\r\n    return (\r\n        np.cos(theta) * np.sin(phi),\r\n        np.sin(theta) * np.sin(phi),\r\n        np.cos(phi)\r\n    )\r\n\r\ndef generate_prototypes(n):\r\n    \"\"\"\r\n        ref: \"How to generate equidistributed points on the surface of a sphere\" by Markus Deserno\r\n        note that in the paper, {phi, theta} correspond to {theta, phi} in the rest of this code\r\n        n is an approximate number. the number of returned points are smaller\r\n    \"\"\"\r\n    a = 2.0*3.1415926/n\r\n    d = np.sqrt(a)\r\n    mtheta = int(round(0.5*3.1415926/d))\r\n    dtheta = 0.5*3.1415926/mtheta\r\n    dphi = a/dtheta\r\n\r\n    lst = []\r\n    for m in range(mtheta - 1):\r\n        theta = 0.5*3.1415926 * (m + 0.5)/mtheta\r\n        mphi = int(round(2.0*3.1415926*np.sin(theta)/dphi))\r\n        for n in range(mphi):\r\n            phi = 2.0*3.1415926*n/mphi\r\n            lst.append(polar2cartesian(phi, theta))\r\n    return lst\r\n```", "This is fixed with latest version of tf nightly 2.0 build '2.0.0-dev20190906' . Thanks!\r\nReplace  ```tf.optmizer``` with ```tf.keras.optimizer``` on line 64\r\n```python\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.005, amsgrad=True)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31509\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31509\">No</a>\n", "This bug seems still present as of 2.0.0 official release (as well as RC1)\r\nI tried just now with the code snippet above. I have seen a similar problem in our code.\r\n\r\nAny hint?\r\n\r\nThanks\r\nGraffa", " @ymodak  ^^", "bump", "any update?", "Please look into this issue. The console output becomes a complete mess", "Hi, I am facing this issue while training tiny yolo on VOC2012 dataset using model.fit() method. Note that I am using repo https://github.com/zzh8829/yolov3-tf2 and using TF2.0 official release. Any suggestion to fix the error?", "Hi, I also have got this problem. Model is learning normally, however console output is unreadable. After every epoch there is an error with message \"BaseCollectiveExecutor::StartAbort Out of range: End of sequence\".", "Same issue on fresh install, using [code found on the official documentation](https://www.tensorflow.org/tutorials/text/text_generation)\r\n\r\n**Operating System**: Windows 10\r\n**Python**: 3.7-64\r\n**Tensorflow Package**: tensorflow-gpu@2.0.0\r\n**GPU Hardware**: 1080Ti\r\n**CUDA Lib**: 10.0\r\n**cuDNN**: 7.6.5.32\r\n\r\n# Example Code:\r\n\r\n<details><summary>Click to expand</summary>\r\n\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\n\r\nimport numpy as np\r\nimport os\r\nimport time\r\n\r\npath_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\r\n\r\n\r\n# Read, then decode for py2 compat.\r\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n# length of text is the number of characters in it\r\nprint ('Length of text: {} characters'.format(len(text)))\r\n\r\n# Take a look at the first 250 characters in text\r\nprint(text[:250])\r\n\r\n# The unique characters in the file\r\nvocab = sorted(set(text))\r\nprint ('{} unique characters'.format(len(vocab)))\r\n\r\n# Creating a mapping from unique characters to indices\r\nchar2idx = {u:i for i, u in enumerate(vocab)}\r\nidx2char = np.array(vocab)\r\n\r\ntext_as_int = np.array([char2idx[c] for c in text])\r\n\r\nprint('{')\r\nfor char,_ in zip(char2idx, range(20)):\r\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\r\nprint('  ...\\n}')\r\n\r\n# Show how the first 13 characters from the text are mapped to integers\r\nprint ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\r\n\r\n# The maximum length sentence we want for a single input in characters\r\nseq_length = 100\r\nexamples_per_epoch = len(text)//(seq_length+1)\r\n\r\n# Create training examples / targets\r\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\r\n\r\nfor i in char_dataset.take(5):\r\n  print(idx2char[i.numpy()])\r\n\r\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)\r\n\r\nfor item in sequences.take(5):\r\n  print(repr(''.join(idx2char[item.numpy()])))\r\n\r\ndef split_input_target(chunk):\r\n    input_text = chunk[:-1]\r\n    target_text = chunk[1:]\r\n    return input_text, target_text\r\n\r\ndataset = sequences.map(split_input_target)\r\n\r\nfor input_example, target_example in  dataset.take(1):\r\n  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\r\n  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\r\n\r\nfor i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\r\n    print(\"Step {:4d}\".format(i))\r\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\r\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\r\n\r\n# Batch size\r\nBATCH_SIZE = 64\r\n\r\n# Buffer size to shuffle the dataset\r\n# (TF data is designed to work with possibly infinite sequences,\r\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\r\n# it maintains a buffer in which it shuffles elements).\r\nBUFFER_SIZE = 10000\r\n\r\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n\r\n# Length of the vocabulary in chars\r\nvocab_size = len(vocab)\r\n\r\n# The embedding dimension\r\nembedding_dim = 256\r\n\r\n# Number of RNN units\r\nrnn_units = 1024\r\n\r\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n  model = tf.keras.Sequential([\r\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\r\n                              batch_input_shape=[batch_size, None]),\r\n    tf.keras.layers.GRU(rnn_units,\r\n                        return_sequences=True,\r\n                        stateful=True,\r\n                        recurrent_initializer='glorot_uniform'),\r\n    tf.keras.layers.Dense(vocab_size)\r\n  ])\r\n  return model\r\n\r\nmodel = build_model(\r\n  vocab_size = len(vocab),\r\n  embedding_dim=embedding_dim,\r\n  rnn_units=rnn_units,\r\n  batch_size=BATCH_SIZE)\r\n\r\nfor input_example_batch, target_example_batch in dataset.take(1):\r\n  example_batch_predictions = model(input_example_batch)\r\n  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\r\n\r\nsampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\r\n\r\nprint(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\r\nprint()\r\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\r\n\r\ndef loss(labels, logits):\r\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\r\n\r\nexample_batch_loss  = loss(target_example_batch, example_batch_predictions)\r\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())\r\n\r\nmodel.compile(optimizer='adam', loss=loss)\r\n\r\n# Directory where the checkpoints will be saved\r\ncheckpoint_dir = './resources/training_checkpoints'\r\n# Name of the checkpoint files\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n\r\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_prefix,\r\n    save_weights_only=True)\r\n\r\nEPOCHS=10\r\n\r\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\r\n\r\ntf.train.latest_checkpoint(checkpoint_dir)\r\n\r\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\r\n\r\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n\r\nmodel.build(tf.TensorShape([1, None]))\r\n\r\nmodel.summary()\r\n\r\ndef generate_text(model, start_string):\r\n  # Evaluation step (generating text using the learned model)\r\n\r\n  # Number of characters to generate\r\n  num_generate = 1000\r\n\r\n  # Converting our start string to numbers (vectorizing)\r\n  input_eval = [char2idx[s] for s in start_string]\r\n  input_eval = tf.expand_dims(input_eval, 0)\r\n\r\n  # Empty string to store our results\r\n  text_generated = []\r\n\r\n  # Low temperatures results in more predictable text.\r\n  # Higher temperatures results in more surprising text.\r\n  # Experiment to find the best setting.\r\n  temperature = 1.0\r\n\r\n  # Here batch size == 1\r\n  model.reset_states()\r\n  for i in range(num_generate):\r\n      predictions = model(input_eval)\r\n      # remove the batch dimension\r\n      predictions = tf.squeeze(predictions, 0)\r\n\r\n      # using a categorical distribution to predict the word returned by the model\r\n      predictions = predictions / temperature\r\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n\r\n      # We pass the predicted word as the next input to the model\r\n      # along with the previous hidden state\r\n      input_eval = tf.expand_dims([predicted_id], 0)\r\n\r\n      text_generated.append(idx2char[predicted_id])\r\n\r\n  return (start_string + ''.join(text_generated))\r\n\r\nprint(generate_text(model, start_string=u\"ROMEO: \"))\r\n```\r\n\r\n</details>\r\n\r\n# Output\r\n\r\n<details><summary>Click to expand</summary>\r\n\r\n```\r\n2019-11-27 19:22:25.907687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\nLength of text: 1115394 characters\r\nFirst Citizen:\r\nBefore we proceed any further, hear me speak.\r\n\r\nAll:\r\nSpeak, speak.\r\n\r\nFirst Citizen:\r\nYou are all resolved rather to die than to famish?\r\n\r\nAll:\r\nResolved. resolved.\r\n\r\nFirst Citizen:\r\nFirst, you know Caius Marcius is chief enemy to the people.\r\n\r\n65 unique characters\r\n{\r\n  '\\n':   0,\r\n  ' ' :   1,\r\n  '!' :   2,\r\n  '$' :   3,\r\n  '&' :   4,\r\n  \"'\" :   5,\r\n  ',' :   6,\r\n  '-' :   7,\r\n  '.' :   8,\r\n  '3' :   9,\r\n  ':' :  10,\r\n  ';' :  11,\r\n  '?' :  12,\r\n  'A' :  13,\r\n  'B' :  14,\r\n  'C' :  15,\r\n  'D' :  16,\r\n  'E' :  17,\r\n  'F' :  18,\r\n  'G' :  19,\r\n  ...\r\n}\r\n'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\r\n2019-11-27 19:22:27.947945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-11-27 19:22:27.981240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\n2019-11-27 19:22:27.987413: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-27 19:22:27.992523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-27 19:22:27.995950: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-11-27 19:22:28.007916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\n2019-11-27 19:22:28.014056: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-27 19:22:28.023461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-27 19:22:28.605781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-27 19:22:28.610061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-11-27 19:22:28.612627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-27 19:22:28.616313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8784 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nF\r\ni\r\nr\r\ns\r\nt\r\n'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\r\n'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\r\n\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\r\n\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\r\n'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\r\nInput data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\r\nTarget data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\r\nStep    0\r\n  input: 18 ('F')\r\n  expected output: 47 ('i')\r\nStep    1\r\n  input: 47 ('i')\r\n  expected output: 56 ('r')\r\nStep    2\r\n  input: 56 ('r')\r\n  expected output: 57 ('s')\r\nStep    3\r\n  input: 57 ('s')\r\n  expected output: 58 ('t')\r\nStep    4\r\n  input: 58 ('t')\r\n  expected output: 1 (' ')\r\n2019-11-27 19:22:30.659472: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-11-27 19:22:31.438471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n(64, 100, 65) # (batch_size, sequence_length, vocab_size)\r\nInput: \r\n 'ct,\\nSuch as move men; beside, she hath prosperous art\\nWhen she will play with reason and discourse,\\n'\r\n\r\nNext Char Predictions:\r\n \"dBBEE'E;Qc\\nlUTzu,nVzYVyaQZt?kU,q 3;PE:EuGignL,$oV;LM!NS:&er$RvG'g,Kkzt-knZOrg\\n-vmGpSpuiE hzz&ZSHIre\\n\"\r\nPrediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\r\nscalar_loss:       4.1739583\r\nEpoch 1/10\r\n2019-11-27 19:22:32.762911: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_gru_with_fallback_2273_2411_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_3042' and '__inference___backward_standard_gru_2524_2900' both implement 'gru_ea298c2e-507f-4d8b-9d0f-d620fe9617aa' but their signatures do not match.\r\n    172/Unknown - 8s 45ms/step - loss: 2.67862019-11-27 19:22:39.347075: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_29]]\r\n2019-11-27 19:22:39.354004: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n172/172 [==============================] - 8s 46ms/step - loss: 2.6786\r\nEpoch 2/10\r\n171/172 [============================>.] - ETA: 0s - loss: 1.97222019-11-27 19:22:45.913512: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_29]]\r\n2019-11-27 19:22:45.921313: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n172/172 [==============================] - 7s 38ms/step - loss: 1.9722\r\nEpoch 3/10\r\n171/172 [============================>.] - ETA: 0s - loss: 1.70472019-11-27 19:22:52.567125: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_29]]\r\n2019-11-27 19:22:52.573933: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n172/172 [==============================] - 7s 39ms/step - loss: 1.7047\r\nEpoch 4/10\r\n171/172 [============================>.] - ETA: 0s - loss: 1.55052019-11-27 19:22:59.280955: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_29]]\r\n2019-11-27 19:22:59.288517: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n172/172 [==============================] - 7s 39ms/step - loss: 1.5505\r\nEpoch 5/10\r\n171/172 [============================>.] - ETA: 0s - loss: 1.46012019-11-27 19:23:05.895398: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_29]]\r\n2019-11-27 19:23:05.911084: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n172/172 [==============================] - 7s 39ms/step - loss: 1.4601\r\nEpoch 6/10\r\n171/172 [============================>.] - ETA: 0s - loss: 1.39762019-11-27 19:23:12.458498: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_29]]\r\n2019-11-27 19:23:12.465456: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n172/172 [==============================] - 7s 38ms/step - loss: 1.3976\r\nEpoch 7/10\r\n171/172 [============================>.] - ETA: 0s - loss: 1.35182019-11-27 19:23:19.054523: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_29]]\r\n2019-11-27 19:23:19.061413: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n172/172 [==============================] - 7s 38ms/step - loss: 1.3518\r\nEpoch 8/10\r\n171/172 [============================>.] - ETA: 0s - loss: 1.31302019-11-27 19:23:25.635973: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_29]]\r\n2019-11-27 19:23:25.642910: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n172/172 [==============================] - 7s 38ms/step - loss: 1.3130\r\nEpoch 9/10\r\n171/172 [============================>.] - ETA: 0s - loss: 1.27652019-11-27 19:23:32.265397: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_29]]\r\n2019-11-27 19:23:32.273219: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n172/172 [==============================] - 7s 38ms/step - loss: 1.2765\r\nEpoch 10/10\r\n171/172 [============================>.] - ETA: 0s - loss: 1.24442019-11-27 19:23:38.871165: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_29]]\r\n2019-11-27 19:23:38.878105: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n172/172 [==============================] - 7s 38ms/step - loss: 1.2444\r\nModel: \"sequential_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nembedding_1 (Embedding)      (1, None, 256)            16640\r\n_________________________________________________________________\r\ngru_1 (GRU)                  (1, None, 1024)           3938304\r\n_________________________________________________________________\r\ndense_1 (Dense)              (1, None, 65)             66625     \r\n=================================================================\r\nTotal params: 4,021,569\r\nTrainable params: 4,021,569\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nROMEO: forget, you seems together:\r\nAnd not of an artiff; stop, and make these wopes\r\nin there.\r\n\r\nYORREL:\r\nNo. Which if thou forgot thee in the presence;\r\nFor by the senate company, for truth her brother:\r\nMy gracious son, the copple of love go\r\nTo-morrow, and here must attend my own,\r\nWhich, laduces and the rockle stands;\r\nAnd not proof an adult coal.\r\nNow, fie, fines with his grace, got her.\r\n\r\nMERCUTIO:\r\nWas ever anothir tidings: if I flatter me\r\nThurt can cry him to answer to the people.\r\n\r\nCLEOREO:\r\nThese earthly face lunes;\r\nRows, away by lime there, to bring forth;\r\nTo rich shall not find a supposh-back crume\r\nUnless the grace as wretching desting.\r\nThere is thy face; it is not to?\r\n\r\nPOLIXENES:\r\nIt for a divided care\r\nWill break against his fiery drops!\r\nWhat! would not know him? I'll sit him achore in the\r\nsubjects grow, that loves my ear.\r\n\r\nCOMINIUS:\r\nYou scat of a year of remedy; I would put on this blood.\r\nBe gole and love in blows, there is not intend\r\nAnd princely fisk and done from sweeted knight.\r\n\r\nROMEO:\r\nAy\r\n```\r\n\r\n</details>", "I'm having the same issue, I though I was doing something wrong at first loading numpy data to tf.data, but then it gave me same errors following this example \r\n\r\nhttps://www.tensorflow.org/tutorials/load_data/numpy", "This bug still seems to be in the Tensorflow 2.0.0 release. I'm getting exactly that error messages multiple times when running the following tutorial project: https://www.tensorflow.org/tutorials/text/text_classification_rnn\r\n\r\nThe model seems to train and accuracy is increasing, so the warning doesn't seem to cause any harm besides messing up the shell.\r\n\r\n**Operating System:** Ubuntu 18.04.3 LTS\r\n**Python:** 3.7.5\r\n**Tensorflow Package:** tensorflow-gpu@2.0.0\r\n**CUDA Lib:** 10.0.130\r\n**GPU Model and Memory:** GTX 1050 Ti, 4GB", "Getting same issue, would be nice to know the reason for it and how to fix it if possible.", "It seems fixed in tensorflow-gpu@2.1.0rc1 (https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0-rc1)\r\n\r\n**Operating System:** Ubuntu 18.04\r\n**Python:** 3.6.9\r\n**Tensorflow:** tensorflow-gpu@2.1.0rc1\r\n**CUDA:** 10.1.243\r\n**CuDNN:** 7.6\r\n**GPU:** Quadro RTX 5000\r\n**NVidia driver:** 430.26", "Also facing this problem:\r\n>Hi, I am facing this issue while training tiny yolo on VOC2012 dataset using model.fit() method. Note that I am using repo https://github.com/zzh8829/yolov3-tf2 and using TF2.0 official release. Any suggestion to fix the error?\r\n\r\nUnfortunately \r\n> pip3 install --user tensorflow-gpu==2.1.0rc1\r\n\r\ndid not fix the error", "The bug still seems to be in Tensorflow 2.1.0.\r\n\r\n(I am now working on a different machine than the one I mentioned above. So it doesn't even seem to be specific to some setup. Different GPU, CPU, OS-Version, etc.)\r\n\r\n**Operating System:** Ubuntu 19.10\r\n**Python:** 3.7.5\r\n**Tensorflow Package:** tensorflow@2.1.0\r\n**CUDA:** 10.1.243\r\n**CuDNN:** 7.6.5\r\n**GPU Model and Memory:** RTX 2070 Super, 8GB", "I'm facing the same issue in Tensorflow 2.1.0.\r\n\r\nAfter following all the steps in official tutorial https://tensorflow.google.cn/tutorials/load_data/csv,  I get the trained model. But the issue comes out each time when I tried to evaluate the model on test_data:\r\n  ` test_loss, test_accuracy = model.evaluate(test_data)`\r\n\r\n**OS**:  Windows 10\r\n**Python** :  3.7.6\r\n**Tensorflow**:  2.1.0", "Based on https://github.com/keras-team/autokeras/issues/839#issuecomment-590097076:\r\n\r\nConcerning a `model.fit`, setting `validation_steps` in case you are using `validation_data` or `steps_per_epoch` if your input `x` is a `tf.data dataset` solves this issue.\r\n\r\nI have not tested a `model.evaluate` but following the same logic, I think you should set the `steps` parameter.", "> Based on [keras-team/autokeras#839 (comment)](https://github.com/keras-team/autokeras/issues/839#issuecomment-590097076):\r\n> \r\n> Concerning a `model.fit`, setting `validation_steps` in case you are using `validation_data` or `steps_per_epoch` if your input `x` is a `tf.data dataset` solves this issue.\r\n> \r\n> I have not tested a `model.evaluate` but following the same logic, I think you should set the `steps` parameter.\r\n\r\nThanks for your reply, that's the solution.", "This bug still occurs when calling `model.predict()` on a `tf.data.Dataset.from_generator(gen)` dataset in TF 2.1.0.", "I'm getting it during the validation portion of `model.fit()`", "Still present during validation indeed (or just before).\r\n\r\n`W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n2020-05-20 10:49:02.923506: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_6]]\r\n2020-05-20 10:49:17.997026: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n2020-05-20 10:49:17.997362: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_4]]`", "Same here!\r\n```\r\nW tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}}]]\r\n```", "In my case with TF-2.1, I load both train and validation dataset from a TFRecord file. E.g. tf.data.TFRecordDataset(filePath). This means, the tf.fit function has no chance to know the number of samples in the file. As @rmarru pointed out, if the params in the tf.fit function (_steps_per_epoch_, _validation_steps_) are set, then the function knows the number of elements and no warning should be seen.\r\n\r\nIf I dont set _steps_per_epoch_ and _validation_steps_, after the first epoch tf.fit will remember _steps_per_epoch_, but not  _validation_steps_. That means, from the second epoch I will receive less warnings, only for the validation."]}, {"number": 31508, "title": "Update import_pb_to_tensorboard to support meta graph and saved model", "body": "When I try to use import_pb_to_tensorboard to import an **saved model pb file**, following error happens.\r\n\r\n```\r\n2019-08-10 22:19:09.280602: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-08-10 22:19:09.316408: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\r\n2019-08-10 22:19:09.338194: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x43fff70 executing computations on platform Host. Devices:\r\n2019-08-10 22:19:09.338316: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"import_pb_to_tensorboard.py\", line 84, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/.../lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"import_pb_to_tensorboard.py\", line 66, in main\r\n    import_to_tensorboard(FLAGS.model_dir, FLAGS.log_dir)\r\n  File \"import_pb_to_tensorboard.py\", line 56, in import_to_tensorboard\r\n    graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\n\r\n\r\nThe original `import_pb_to_tensorboard.py`  only supports `GraphDef` pb file and the flag `--model_dir` is quite confuse, because it looks like need pass directory.\r\n\r\nMost of all, I think **saved model pb** format is more common than  `GraphDef` pb file, so I submit this PR.\r\n\r\nCould you help have a look? Thanks a lot!", "comments": ["Hi @nfelt, could you please help to take a look at this? Thanks.", "@zhjunqin Thanks for sending this PR!\r\n\r\nThe TensorBoard team hasn't really maintained this script (it's the first I heard of it), and in general I think the approach we'd prefer is to add any new functionality like this within the `tensorboard` command itself, rather than directing users to a separate converter script.  In particular, this script looks like it was added in #8867 as a temporary measure, and it doesn't appear to have been updated for TF 2.0, so I wouldn't expect it to stick around long-term.\r\n\r\nWe have the following GitHub issue open in the TensorBoard repo to improve visualization for SavedModels: https://github.com/tensorflow/tensorboard/issues/1962\r\n\r\nIf you're interested in contributing there, we can figure out what the best design would be for having TensorBoard visualize a SavedModel directly.\r\n", "> @zhjunqin Thanks for sending this PR!\r\n> \r\n> The TensorBoard team hasn't really maintained this script (it's the first I heard of it), and in general I think the approach we'd prefer is to add any new functionality like this within the `tensorboard` command itself, rather than directing users to a separate converter script. In particular, this script looks like it was added in #8867 as a temporary measure, and it doesn't appear to have been updated for TF 2.0, so I wouldn't expect it to stick around long-term.\r\n> \r\n> We have the following GitHub issue open in the TensorBoard repo to improve visualization for SavedModels: [tensorflow/tensorboard#1962](https://github.com/tensorflow/tensorboard/issues/1962)\r\n> \r\n> If you're interested in contributing there, we can figure out what the best design would be for having TensorBoard visualize a SavedModel directly.\r\n\r\nThanks for reply. I also think it is a good idea to make tensorboard can visualize a Savedmodel directly.\r\nI can look into the issue you mentioned to see what I can do for it."]}, {"number": 31507, "title": "Update TensorFlow For Poets", "body": "It would be awesome if someone could update TensorFlow For Poets, with the latest stable TensorFlow and Inception v4.\r\n\r\nCurrently, there are lots of deprecation warnings logged when TensorFlow For Poets is run on TensorFlow 1.7.\r\n\r\nIn addition, I am unable to get TensorFlow For Poets to run using Inception v4.  The code currently downloads ``inception-2015-12-05.tgz``, which contains the training file ``classify_image_graph_def.pb``.  But when I tried to substitute ``inception_v4_2016_09_09.tar.gz``, I found that it contained a ``inception_v4.ckpt`` file instead.  I understand that ``.ckpt`` implies a checkpoint file, but I do not know how to convert a ``.ckpt`` file to the ``.pb`` file that TensorFlow For Poets is trying to load.\r\n\r\nI am sure all of this is documented somewhere, e.g. how to swap out the code to initialize from a .ckpt instead, but given that the example is supposed to be an introduction, doing so is a bit above my head at present.  Likely so for other newcomers.\r\n\r\nThanks for the awesome work.  I have gotten TforP running with the old inception model and my own image classifications, but my results, while good, I think could be improved somewhat by the newer Inception v4 pre-trained model.\r\n", "comments": ["Highly recommend trying [TensorFlow For Poets TF 2.0 version](https://colab.sandbox.google.com/github/tensorflow/examples/blob/master/community/en/flowers_tf_lite.ipynb) if haven't already.", "Closing out this feature request, as TF for Poets has been updated to support TensorFlow 2.0 (see link above). Thank you for the suggestion, and thanks to the TF Lite team for creating! \ud83d\ude04 "]}, {"number": 31506, "title": "Fix wrong includes for tensorflow-lite for ios", "body": "I was trying to build the ios examples for tensorflow lite, however several include statements seemed to be wrong. This PR is fixing this. \r\nFeel free to suggest any improvements.", "comments": ["Also I found this issue #26924. This should fix it as well.", "I'm not sure of the status of these examples. Maybe they should move into the `/lite` section of the [tensorflow/examples](https://github.com/tensorflow/examples) repo? @MarkDaoust @wolffg ", "Thanks for fixing these. It seems accidentally broken a while ago. \r\n\r\n@lamberta Yes we plan to move most of the examples into tensorflow/examples, but it's not done yet. Meanwhile these changes look right. I'm going to approve this. ", "@dansitu @jdduke Can this be merged?"]}, {"number": 31505, "title": "Tensorflow 2.0 does not use GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): yes\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia MX250 4Gb\r\n\r\n**Describe the current behavior**\r\nI'm playing around with tf2.0 while mostly still working on tf1.14. I have created two conda environments - one for tf1.14 and the other one for tf2.0 - so all hardware and drivers are absolutely the same. Tf1.14 is working good and uses gpu as it supposed to, but tf2.0 runs on cpu.\r\n\r\n**Code to reproduce the issue**\r\nif i run:\r\n```python\r\nfrom tensorflow.python.client import device_lib\r\ndevice_lib.list_local_devices()\r\n\r\ni get an output with tf2.0:\r\n\r\n[name: \"/device:CPU:0\"\r\n device_type: \"CPU\"\r\n memory_limit: 268435456\r\n locality {\r\n }\r\n incarnation: 9820328803404595310, name: \"/device:XLA_CPU:0\"\r\n device_type: \"XLA_CPU\"\r\n memory_limit: 17179869184\r\n locality {\r\n }\r\n incarnation: 7262628900557013132\r\n physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\r\n device_type: \"XLA_GPU\"\r\n memory_limit: 17179869184\r\n locality {\r\n }\r\n incarnation: 4554198733675012648\r\n physical_device_desc: \"device: XLA_GPU device\"]\r\n\r\nand \r\n\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\n \r\ngives False. \r\nSame code in tf1.14 gives the following:\r\n\r\n[name: \"/device:CPU:0\"\r\n device_type: \"CPU\"\r\n memory_limit: 268435456\r\n locality {\r\n }\r\n incarnation: 11125515865111208996, name: \"/device:XLA_CPU:0\"\r\n device_type: \"XLA_CPU\"\r\n memory_limit: 17179869184\r\n locality {\r\n }\r\n incarnation: 7071753861411904965\r\n physical_device_desc: \"device: XLA_CPU device\", name: \"/device:GPU:0\"\r\n device_type: \"GPU\"\r\n memory_limit: 3333029888\r\n locality {\r\n   bus_id: 1\r\n   links {\r\n   }\r\n }\r\n incarnation: 17329402577157719353\r\n physical_device_desc: \"device: 0, name: GeForce MX250, pci bus id: 0000:02:00.0, compute capability: 6.1\", name: \"/device:XLA_GPU:0\"\r\n device_type: \"XLA_GPU\"\r\n memory_limit: 17179869184\r\n locality {\r\n }\r\n incarnation: 17993919851972383985\r\n physical_device_desc: \"device: XLA_GPU device\"]\r\n\r\nAnd:\r\n\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\n\r\nGives True\r\n```", "comments": ["Are you sure your GPU matches the required CUDA\u00ae Compute Capability 3.5 or higher? I couldn't find it in the list on the [website of nvidia](https://developer.nvidia.com/cuda-gpus).", "i could't find my gpu there as well. But apparently it's supported since it's working with tensorflow 1.14 - i can successfully train models and it works much faster than on cpu and i can see the gpu load. And also nvidia-smi shows cuda version:\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.40       Driver Version: 430.40       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce MX250       Off  | 00000000:02:00.0 Off |                  N/A |\r\n| N/A   43C    P8    N/A /  N/A |      4MiB /  4042MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n", "Did you install TF 1.14, 2.0 from source or binary?\r\nAlso did you install TF-GPU version for both versions of TF?\r\nCan you please check with [```tf.test.is_built_with_cuda```](https://www.tensorflow.org/api_docs/python/tf/test/is_built_with_cuda) for both TF versions?", "1) I install tensorflow 2.0 using pip (since conda install is not available for this version) and tensorflow 1.14 using conda.\r\n2) Yes, both gpu versions\r\n3) Just did  - both return True", "Thanks. I suspect, you haven't added cuda path to the TF 2.0 environment. You need to manually add this after doing ```pip install```.", "can you elaborate a bit more on how to do it? I've tried to google it and couldn't find any info on how to add cuda path to tensorflow ", "Sure. Take a look at https://www.tensorflow.org/install/gpu#linux_setup", "Could this happen because @andrey999333 uses cuda 10.1, while the pip wheel was built for cuda 10.0 ?\r\n@ymodak do you know why TensorFlow 2.0 on pypi is not yet built for cuda 10.1, library released in February ?"]}, {"number": 31504, "title": "NotImplementedError: tf.GradientTape.gradients() does not support graph control flow operations like tf.cond or tf.while at this time. Use tf.gradients() instead. If you need this feature, please file a feature request at https://github.com/tensorflow/tensorflow/issues/new", "body": "loss_object = tf.keras.losses.CategoricalCrossentropy()\r\n\r\ndef create_adversarial_pattern(input_image, input_label):\r\n  with tf.GradientTape() as tape:\r\n    tape.watch(input_image)\r\n    prediction = model(input_image)\r\n    print(prediction.shape)\r\n    loss = loss_object(input_label, prediction)\r\n    print('Loss:',loss)\r\n\r\n  # Get the gradients of the loss w.r.t to the input image.\r\n  \r\n  gradient = tape.gradient(loss, input_image)\r\n  print('................')\r\n  print('Gradient tensor:',gradient)\r\n  # Get the sign of the gradients to create the perturbation\r\n  signed_grad = tf.sign(gradient)\r\n\r\n  return signed_grad\r\n\r\n\r\nperturbations = create_adversarial_pattern(new_img, y_pred1)\r\nprint(perturbations)\r\n\r\n\r\n\r\nHello, can anyone help me to solve the error???  Why am I not getting a tensor shape of gradient\r\n(1,299,299,3). I followed https://www.tensorflow.org/beta/tutorials/generative/adversarial_fgsm\r\nlnk to do my code. TIA ", "comments": ["@Bakibiiiillah ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!\r\n", "Thanks a lot @oanush for your gentle reply, actually I am working on kaggle, the version of tensorflow I haven't checked yet, actually  I have to generate adversarial sample using fast gradient sign method for diabetic retinopathy dataset, but I got some error related with tensorflow. It's raelly very important for me too solve as it is my undergraduate thesis topic. The version of tensorflow is 1.14.0\r\n[diabetic-retinopathy.zip](https://github.com/tensorflow/tensorflow/files/3497142/diabetic-retinopathy.zip)\r\nActually last three or four cells of above code snippet contains the problem related with creating adversarial sample for my diabetic retinopathy dataset using fast gradient sign method(for tensorflow).\r\nAgain many many thanks for your gentle reply , it's really very important to solve the issues, \r\nso I'll eagerly wait for your reply@oanush\r\n\r\n", "@oanush , further for any question you can ask me any time, it's really important to solve.\r\nTIA", "@oanush \r\nThis screenshot provides name of my kaggle dataset\r\n![dr](https://user-images.githubusercontent.com/50144400/62947534-827eb780-be04-11e9-873c-d22742f4ed2d.PNG)\r\n", "I have the same problem...anyone can help?", "*typeerror:image data cannot be converted to float.*\n*how to solve this problem in kaggle to show a tensor image*\n\nOn Sun, Aug 25, 2019 at 6:55 PM sarsbug <notifications@github.com> wrote:\n\n> I have the same problem...anyone can help?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31504?email_source=notifications&email_token=AL6SJEG5FGHZZLUIU4EYED3QGJ6M5A5CNFSM4IKY5IKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5CS5EY#issuecomment-524627603>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AL6SJECM4YXSAXIV7EKQ56TQGJ6M5ANCNFSM4IKY5IKA>\n> .\n>\n", "@Bakibiiiillah Without a minimal code snippet we can do very little to help you solve your problem.\r\nFrom the error message following post may help you resolve it;\r\nhttps://stackoverflow.com/questions/32302180/typeerror-image-data-can-not-convert-to-float\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31504\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31504\">No</a>\n", "Did you get the problem resolved?", "I didn't get the problem resolve but I don't need to solve it now. You may\nclose the conversation. Thank\n\n\nOn Fri, Sep 27, 2019, 3:20 AM maithal <notifications@github.com> wrote:\n\n> Did you get the problem resolved?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31504?email_source=notifications&email_token=AL6SJEHJ3LYRLPT53OIU4BLQLURRJA5CNFSM4IKY5IKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7W7KYI#issuecomment-535688545>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AL6SJEHOLG4OFRG77AE3BF3QLURRJANCNFSM4IKY5IKA>\n> .\n>\n"]}, {"number": 31503, "title": "Tensorize scale in non-constant weight initializer.", "body": "Currently warmstart VocabInfo with non-constant initializer breaks. This change fixes that. I can add unit tests if necessary.", "comments": ["Please add unit tests.", "Hi alextp:\r\n  I have a new pull request here with the unit tests. How can I \"merge\" with this one?\r\nhttps://github.com/tensorflow/tensorflow/compare/master...yunjiangster:fix_warmstart?expand=1\r\n  Best,\r\n  John", "git merge? This pull request will show whatever is in the branch you're\npulling from, so just merge the tests into your branch.\n\nOn Thu, Aug 15, 2019 at 10:01 AM Yunjiang Jiang <notifications@github.com>\nwrote:\n\n> Hi alextp:\n> I have a new pull request here with the unit tests. How can I \"merge\" with\n> this one?\n>\n> https://github.com/tensorflow/tensorflow/compare/master...yunjiangster:fix_warmstart?expand=1\n> Best,\n> John\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31503?email_source=notifications&email_token=AAABHRPQK7SJYHH2Z6GMBRLQEWDXRA5CNFSM4IKYXBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4MLXCY#issuecomment-521714571>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPFEPCGITHIWZ2G3L3QEWDXRANCNFSM4IKYXBKQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp The earlier pull request was created via browser edit. I can't figure out how to merge with the new change, so I created a new pull request here with the unit tests. Feel free to suggest better ways: https://github.com/tensorflow/tensorflow/pull/31648"]}, {"number": 31502, "title": " module 'tensorflow._api.v2.train' has no attribute 'AdamOptimizer'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please provide all the information asked by the template. It will help us assist you better.\r\nFrom the error message looks like you are trying to use adam optimizer in TF 2.0. In that case,\r\nYou may try  [```tf.optimizers.Adam()```](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers)\r\n", "If I change Adam = tf.train.AdamOptimizer to  Adam = tf.optimizers.Adam(), there is the following error. \r\n\r\ngenerating GAN...\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-0decb4cd863c> in <module>\r\n     66 \r\n     67 print('generating GAN...')\r\n---> 68 gan_feed = gan(generator, discriminator)\r\n\r\n<ipython-input-7-0decb4cd863c> in gan(g, d)\r\n     23 \r\n     24     lr, b1 = 1e-4, .2 # otherwise won't converge.\r\n---> 25     optimizer = Adam(lr,beta1=b1)\r\n     26 \r\n     27     grad_loss_wd = optimizer.compute_gradients(dloss, d.trainable_weights)\r\n\r\nTypeError: 'Adam' object is not callable\r\n\r\n"]}, {"number": 31501, "title": "Can't translate saved model to MLIR", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary for training, Source for translation\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.0/7.4.1\r\n- GPU model and memory: P100\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI trained the official resnet imagenet model (r1) until it exported a saved_model.pb and associated variables folder\r\n\r\nI then copied both of those into a separate directory and verified that the model was saved correctly by loading it and inferencing a few images, which returned the correct results.\r\n\r\nI cloned tensorflow and called the following command in the tensorflow directory to translate the saved model to MLIR\r\n\r\n`bazel run //tensorflow/compiler/mlir:tf-mlir-translate -- --graphdef-to-mlir --tf-input-arrays=input_tensor --tf-input-shapes=32,244,244,3 --tf-input-data-types=DT_FLOAT --tf-output-arrays=ArgMax ~/MLIR/saved_model.pb -o ~/MLIR/saved_model.mlir`\r\n\r\nHowever, I got the following error\r\n\r\n> INFO: Analysed target //tensorflow/compiler/mlir:tf-mlir-translate (0 packages loaded, 0 targets configured).\r\n> INFO: Found 1 target...\r\n> Target //tensorflow/compiler/mlir:tf-mlir-translate up-to-date:\r\n>   bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate\r\n> INFO: Elapsed time: 0.571s, Critical Path: 0.00s\r\n> INFO: 0 processes.\r\n> INFO: Build completed successfully, 1 total action\r\n> INFO: Running command line: bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --graphdef-to-mlir '--tf-input-arrays=input_tensor' '--tf-input-shapes=32,244,244,3' '--tf-input-data-types=DT_FLOAT' '--tf-output-arrays=ArgMax' /home/alber\r\n\r\n_It got cut off by the edge of the terminal above_\r\n\r\n> INFO: Build completed successfully, 1 total action\r\n> 2019-08-09 16:59:17.399053: E tensorflow/compiler/mlir/tensorflow/utils/import_utils.cc:66] Error parsing Protobuf: /home/albert/MLIR/saved_model.pbtxt\r\n> 2019-08-09 16:59:17.399429: E tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate.cc:81] Graph import failed: Invalid argument: Could not parse input file\r\n\r\nI tried converting the saved_model.pb to a .pbtxt and running the same command, but I got the same error\r\n\r\nI've uploaded the contents of the .pbtxt file here: [http://m.uploadedit.com/bbtc/1565397330160.txt](http://m.uploadedit.com/bbtc/1565397330160.txt)\r\n", "comments": ["Thanks for the report, importing savedmodel is WIP (should see changes soon). For now you can try freezing the input savedmodel as workaround until then.", "+1 to what @jpienaar said. It's WIP right now. ", "Is there any WIP patch that you could share or some sort of timeline on when those changes may happen?", "The implementation has already started, e.g., https://github.com/tensorflow/tensorflow/commit/4080ca3bf3e0434cba9036a7e789cdd0d443fd87 is a CL for preparation. We expect to push out more CLs soon. SavedModel has lots of details that need to be sorted out to be properly represented in MLIR and we are discussing on that. It won't be enabled all at once with one CL; it will be gradually implemented. :)", "Sean is actively working on SavedModel support actually. So assign to him. :) ", "For everyone interested, Sean will talk about his work on SavedModel support in Nov 21's [MLIR open design meeting](https://docs.google.com/document/d/1y_9f1AbfgcoVdJh4_aM6-BaSHvrHl8zuA5G4jv_94K8/edit). ", "Hi, you are using --graphdef-to-mlir not --savedmodel-to-mlir, so you are not even using the savedmodel converter. Please try with --savedmodel-to-mlir. Also, --tf-input-arrays,--tf-input-shapes,-tf-input-data-types,--tf-output-arrays aren't needed with the SavedModel importer.\r\n\r\nAlso, btw, the link for the pbtxt file you uploaded doesn't seem to be working. ", "Just saw this was an older issue. So --savedmodel-to-mlir didn't even exist back when it was filed :)", "@albertNod \r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31501\">No</a>\n"]}, {"number": 31500, "title": "CUDA dll check not reporting correct version in comments", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nLine 76 in tensorflow/python/platform/self_check.py\r\n        except OSError:\r\n          raise ImportError(\r\n              \"Could not find %r. TensorFlow requires that this DLL be \"\r\n              \"installed in a directory that is named in your %%PATH%% \"\r\n              \"environment variable. Download and install CUDA %s from \"\r\n              \"this URL: https://developer.nvidia.com/cuda-90-download-archive\"\r\n              % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\n\r\nThe URL directs you to version 9 but the current dll checks for version 10. \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Duplicate issue"]}, {"number": 31499, "title": "MultiWorkerMirroredStrategy stuck in all_reduce", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave\r\n- TensorFlow installed from (source or binary): PyPi\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: none. (MacOS)\r\n- GPU model and memory: none. (MacOS)\r\n\r\n**Describe the current behavior**\r\nI am setting up a multi-worker cluster with two workers and I want to reduce the value of a tensor from each worker. Tensorflow is simply blocking on the collective all reduce operation (tensorflow.python.ops.collective_ops.all_reduce) and never synchronizes between workers.\r\n\r\n**Describe the expected behavior**\r\nI expect the collective op to return the sum of the tensor across each worker. More specifically, I expect the code below to print the value `[[2.,3.]]`\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport json\r\nimport os\r\nimport sys\r\nfrom multiprocessing import Process\r\nimport tensorflow as tf\r\nfrom tensorflow.distribute.cluster_resolver import TFConfigClusterResolver\r\n\r\ntf.logging.set_verbosity('DEBUG')\r\n\r\ndef test_dist(task_id):\r\n    resolver = TFConfigClusterResolver()\r\n    cluster = resolver.cluster_spec()\r\n    server = tf.distribute.Server(\r\n        cluster, job_name=\"worker\", task_index=task_id)\r\n\r\n    dist = tf.distribute.experimental.MultiWorkerMirroredStrategy(\r\n        tf.distribute.experimental.CollectiveCommunication.RING)\r\n\r\n    print('num replicas', dist.num_replicas_in_sync)\r\n\r\n    with tf.device('/job:worker/task:{0}/device:CPU:0'.format(task_id)):\r\n        t = tf.Variable([1.0,3.0*task_id], dtype=tf.float32, name='myvar')\r\n\r\n    def sum_deltas_fn(v):\r\n        return tf.identity(v)\r\n\r\n    with dist.scope():\r\n        all_ts = dist.experimental_run_v2(sum_deltas_fn, args=[t])\r\n        delta_sums_results = dist.reduce(tf.distribute.ReduceOp.SUM, all_ts)\r\n\r\n        sess = tf.compat.v1.Session(server.target)\r\n        sess.run(tf.compat.v1.global_variables_initializer())\r\n\r\n        print('tensor', delta_sums_results)\r\n        print('tensor value', sess.run(delta_sums_results))\r\n\r\ntest_dist(int(sys.argv[1]))\r\n```\r\n\r\nI run two workers in one terminal each:\r\n```\r\n# Terminal 1\r\nexport TF_CONFIG='{ \"cluster\": { \"worker\": [\"localhost:8027\", \"localhost:8028\"] }, \"task\": {\"type\": \"worker\", \"index\": 0} }'\r\npython collective_reduce.old.py 0\r\n```\r\n\r\n```\r\n# Terminal 2\r\nexport TF_CONFIG='{ \"cluster\": { \"worker\": [\"localhost:8027\", \"localhost:8028\"] }, \"task\": {\"type\": \"worker\", \"index\": 1} }'\r\npython collective_reduce.old.py 1\r\n```\r\n\r\n**Other info / logs**\r\nThe output is:\r\n\r\nworker 1\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0809 16:15:59.427072 4541060544 deprecation_wrapper.py:119] From collective_reduce.old.py:8: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n\r\n2019-08-09 16:15:59.428197: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-09 16:15:59.431492: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8027, 1 -> localhost:8028}\r\n2019-08-09 16:15:59.432571: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:8027\r\nI0809 16:15:59.470151 4541060544 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nW0809 16:15:59.471471 4541060544 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0809 16:15:59.472136 4541060544 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['localhost:8027', 'localhost:8028']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.RING\r\nnum replicas 2\r\nI0809 16:15:59.488581 4541060544 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0809 16:15:59.489030 4541060544 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\ntensor Tensor(\"allreduce/CollectiveReduce:0\", shape=(2,), dtype=float32, device=/job:worker/replica:0/task:0/device:CPU:0)\r\n```\r\n\r\nworker 2\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0809 16:16:07.349786 4473202112 deprecation_wrapper.py:119] From collective_reduce.old.py:8: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n\r\n2019-08-09 16:16:07.350931: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-09 16:16:07.354157: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8027, 1 -> localhost:8028}\r\n2019-08-09 16:16:07.355424: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:8028\r\nI0809 16:16:07.392420 4473202112 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nW0809 16:16:07.393295 4473202112 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0809 16:16:07.393734 4473202112 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['localhost:8027', 'localhost:8028']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.RING\r\nnum replicas 2\r\nI0809 16:16:07.408463 4473202112 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0809 16:16:07.408880 4473202112 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\ntensor Tensor(\"allreduce/CollectiveReduce:0\", shape=(2,), dtype=float32, device=/job:worker/replica:0/task:1/device:CPU:0)\r\n```\r\n\r\nWhether this is a bug or me doing something wrong, it is very unclear in the docs how to do anything with `MultiWorkerMirroredStrategy`.\r\nAlso, the example with setting the env variable directly with os.environ here https://www.tensorflow.org/guide/distribute_strategy#setting_up_tf_config_environment_variable\r\ndoes not register, and the ClusterSpec will be empty", "comments": ["Apparently, the code works for tf 2.0", "@grananqvist If this was resolved, please close the issue. Thanks!", "@jvishnuvardhan Right.. I was looking for what change there was? Is it possible to make whatever fix there was into TF 1.x ?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31499\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31499\">No</a>\n", "@grananqvist I think it is better if you can run the code in `tf-nightly` and check whether the issue persists there or not. If it persists, please open a new issue for TF1.x and reference the current issue so that community will get benefited. Thanks!", "I saw the same issue as described here, and it seems to be caused by a missing configuration of the `collective_group_leader` in the session config used by the server. It works for me if I do this:\r\n```python\r\n    sess_config = tf.ConfigProto()\r\n    # The line below sets `sess_config.experimental.collective_group_leader`\r\n    sess_config = dist.update_config_proto(sess_config)\r\n    # Pass the updated config to the server\r\n    server = tf.distribute.Server(\r\n        cluster, job_name=\"worker\", task_index=task_id, config=sess_config)\r\n```", "So I got this working for TF2.0, and thanks to @hakos I got it working for our TF1.x version as well. \r\nNow reopening this with a question because it still doesn't work without @hakos suggestion:\r\n\r\nIs this a bug, or intended? That I have to update the distribution strategy with `update_config_proto` with a default `tf.ConfigProto` to make it work, i.e. the reduce operation is unblocked and completed.", "@grananqvist  Tensorflow 1.x is not actively supported. Also,Could you please check with latest TF version and let us know .Please feel free  to reopen issue if necessary.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31499\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31499\">No</a>\n"]}, {"number": 31498, "title": "TF2.0beta1 - Not JSON Serializable when using tf.keras.experimental.export_saved_model", "body": "Most code is from one of the tensorflow 2.0 beta guides: [Writing layers and models with TensorFlow Keras](https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example)\r\n\r\n**Describe the current behavior**\r\n\r\nTypeError: ('Not JSON Serializable:', b'\\n\\x06Square\\x12\\x06Square\\x1a\\x0fz_mean/Identity*\\x07\\n\\x01T\\x12\\x020\\x01')\r\n\r\n**Describe the expected behavior**\r\n\r\nSave model correctly.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\n# Get training data.\r\n(x_train, _), _ = tf.keras.datasets.mnist.load_data()\r\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\r\n\r\noriginal_dim = 784\r\nintermediate_dim = 64\r\nlatent_dim = 32\r\n\r\ndef sampling(inputs):\r\n    z_mean, z_log_var = inputs\r\n    batch = tf.shape(z_mean)[0]\r\n    dim = tf.shape(z_mean)[1]\r\n    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n    return z_mean + tf.exp(0.5 * z_log_var) * epsilon    \r\n\r\n# Define encoder model.\r\noriginal_inputs = tf.keras.Input(shape=(original_dim,), name='encoder_input')\r\nx = layers.Dense(intermediate_dim, activation='relu')(original_inputs)\r\nz_mean = layers.Dense(latent_dim, name='z_mean')(x)\r\nz_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\r\nz = tf.keras.layers.Lambda(sampling)((z_mean, z_log_var))\r\nencoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\r\n\r\n# Define decoder model.\r\nlatent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\r\nx = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\r\noutputs = layers.Dense(original_dim, activation='sigmoid')(x)\r\ndecoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\r\n\r\n# Define VAE model.\r\noutputs = decoder(z)\r\nvae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name='vae')\r\n\r\n# Add KL divergence regularization loss.\r\nkl_loss = - 0.5 * tf.reduce_mean(\r\n    z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\r\nvae.add_loss(kl_loss)\r\n\r\n# Train.\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\nvae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\r\nvae.fit(x_train, x_train, epochs=3, batch_size=64)\r\n\r\n# Save model.\r\ntf.keras.experimental.export_saved_model(vae, 'vae_functional_saved_model')\r\n```\r\n**Other info / logs**\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-4eb0216166eb> in <module>\r\n----> 1 tf.keras.experimental.export_saved_model(vae, 'vae_functional_saved_model')\r\n\r\n~/.miniconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py in export_saved_model(model, saved_model_path, custom_objects, as_text, input_signature, serving_only)\r\n    167 \r\n    168   try:\r\n--> 169     _export_model_json(model, saved_model_path)\r\n    170   except NotImplementedError:\r\n    171     logging.warning('Skipped saving model JSON, subclassed model does not have '\r\n\r\n~/.miniconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py in _export_model_json(model, saved_model_path)\r\n    175 def _export_model_json(model, saved_model_path):\r\n    176   \"\"\"Saves model configuration as a json string under assets folder.\"\"\"\r\n--> 177   model_json = model.to_json()\r\n    178   model_json_filepath = os.path.join(\r\n    179       saved_model_utils.get_or_create_assets_dir(saved_model_path),\r\n\r\n~/.miniconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in to_json(self, **kwargs)\r\n   1447     model_config = self._updated_config()\r\n   1448     return json.dumps(\r\n-> 1449         model_config, default=serialization.get_json_type, **kwargs)\r\n   1450 \r\n   1451   def to_yaml(self, **kwargs):\r\n\r\n~/.miniconda/envs/tf2/lib/python3.7/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\r\n    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\r\n    237         separators=separators, default=default, sort_keys=sort_keys,\r\n--> 238         **kw).encode(obj)\r\n    239 \r\n    240 \r\n\r\n~/.miniconda/envs/tf2/lib/python3.7/json/encoder.py in encode(self, o)\r\n    197         # exceptions aren't as detailed.  The list call should be roughly\r\n    198         # equivalent to the PySequence_Fast that ''.join() would do.\r\n--> 199         chunks = self.iterencode(o, _one_shot=True)\r\n    200         if not isinstance(chunks, (list, tuple)):\r\n    201             chunks = list(chunks)\r\n\r\n~/.miniconda/envs/tf2/lib/python3.7/json/encoder.py in iterencode(self, o, _one_shot)\r\n    255                 self.key_separator, self.item_separator, self.sort_keys,\r\n    256                 self.skipkeys, _one_shot)\r\n--> 257         return _iterencode(o, 0)\r\n    258 \r\n    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\r\n\r\n~/.miniconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/util/serialization.py in get_json_type(obj)\r\n     67     return dict(obj)\r\n     68 \r\n---> 69   raise TypeError('Not JSON Serializable:', obj)\r\n\r\nTypeError: ('Not JSON Serializable:', b'\\n\\x06Square\\x12\\x06Square\\x1a\\x0fz_mean/Identity*\\x07\\n\\x01T\\x12\\x020\\x01')\r\n```", "comments": ["The code seems working fine on with tf-nightly-gpu-1.15.0.dev20190809\r\n", "It also seems to work with tf-nightly-2.0-preview-2.0.0.dev20190811. \r\n\r\nIn this version `tf.keras.experimental.export_saved_model` is deprecated. \r\n``` python\r\nPlease use `model.save(..., save_format=\"tf\")` or `tf.keras.models.save_model(..., save_format=\"tf\")`.\r\n```\r\n\r\nI think in earlier versions of TF, the serialization of TF objects returned a binary but keras uses a .json. So there was this problem. I don't know, but maybe\r\n\r\n``` python\r\nkl_loss = - 0.5 * tf.reduce_mean(\r\n    z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\r\nvae.add_loss(kl_loss)\r\n```\r\n\r\nor \r\n\r\n``` python\r\nvae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\r\n```\r\n\r\nare the functions that caused this error in serialization. At that time, I've tried putting everything in Lambda layers to avoid the binary serialization.\r\n\r\nEven though there are some warnings about loading / saving the optimizer when using this `tf.keras.experimental` there's no error and/or warning when using the `model.save(..., save_format=\"tf\")` or `tf.keras.models.save_model(..., save_format=\"tf\")`. Both with `save_format='tf'` or `save_format='h5'`\r\n\r\nSo it's a good idea to use at least this nightly version for TF and change the `tf.keras.experimental` to the non-deprecated versions if you want to use the 2.0 version.", "This is fixed with latest tf-2.0-nightly build  '2.0.0-dev20190812'. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31498\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31498\">No</a>\n"]}, {"number": 31497, "title": "Roll back experimental_compile from 2.0", "body": "Roll back experimental_compile from 2.0", "comments": ["We need to cherry-pick this into 2.0 release."]}, {"number": 31496, "title": "[INTEL MKL] Fix the error 'install_pip_packages.sh: line 20: pip2: command not found", "body": "This PR is to fix the `cpu` image build job here: https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu/1190/console", "comments": ["@agramesh1 ", "Looks like none of `install_bootstrap_deb_packages.sh`, `install_deb_packages.sh` installs `pip/pip3` prior to running `install_pip_packages.sh` and that's why the `Jenkins` job is failing.\r\nSo we have a couple of options either installing it during `install_deb_packages.sh` or using the `toolchain` scripts, but it looks like this is cleaner and all those other scripts need `python-pip` or `python3-pip` packages one way or another.", "https://github.com/tensorflow/tensorflow/commit/e545a51504503a86c94b9a17c8e0081444d359d7 @ashahba Hi Ebi, master has the fix now it seems.", "Closing this since the changes to remove calls to `easy_install` has been reverted."]}, {"number": 31495, "title": "fix #31494: export_lib.get_temp_export_dir returns incorrect value", "body": "See issue: https://github.com/tensorflow/tensorflow/issues/31494.\r\nIt contains both the fix and a new test.", "comments": []}, {"number": 31494, "title": "export_lib.get_temp_export_dir returns incorrect value with mixed bytes and str", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MacBook Pro\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nThe return value of export_lib.get_temp_export_dir is mixed with string and bytes, where the bytes portion is in literal form of temp-b'1234567890', including the letter b and the quotes, these will then become part of the directory name created.\r\n\r\n**Describe the expected behavior**\r\nThe return value should be temp-1234567890.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tensorflow_estimator.python.estimator.export import export_lib\r\nfrom tensorflow.python.lib.io import file_io\r\nimport time\r\n\r\nbase1 = \"/tmp/test/export_base\"\r\ntemp1 = export_lib.get_temp_export_dir(export_lib.get_timestamped_export_dir(base1))\r\nprint(\"temp1: \" + temp1.decode(\"utf-8\"))\r\nfile_io.recursive_create_dir(temp1)\r\narr = os.listdir(base1)\r\nprint(arr)\r\nos.rmdir(temp1)\r\n\r\n```\r\n**Other info / logs**\r\n\r\nOutput of above code:\r\n\r\n```\r\ntemp1: /tmp/test/export_base/temp-b'1565380472'\r\n[\"temp-b'1565380472'\"]\r\n\r\n```\r\n\r\nAs you can see the b'' became literal.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31494\">No</a>\n"]}, {"number": 31493, "title": "Error making prediction using `Models with multiple inputs and outputs` example code", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab. \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: whatever Colab has. \r\n- GPU model and memory: NA (GPU not enabled)\r\n\r\n**Describe the current behavior**\r\n\r\nI'm running the example code here: \r\nhttps://www.tensorflow.org/beta/guide/keras/functional#models_with_multiple_inputs_and_outputs\r\nThis runs successfully but if I try to make a prediction with the model by running\r\n```\r\nmodel(title_data, body_data, tags_data)\r\n```\r\nI get the following error: \r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-37-6db901800232> in <module>()\r\n----> 1 model.call(tf.constant(title_data), tf.constant(body_data), tf.constant(tags_data))\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\r\n    903     output_shapes = []\r\n    904     for x in self.outputs:\r\n--> 905       assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)\r\n    906       tensor = tensor_dict[str(id(x))]\r\n    907       output_shapes.append(x.shape)\r\n\r\nAssertionError: Could not compute output Tensor(\"priority/Identity:0\", shape=(None, 1), dtype=float32)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error! \r\n\r\n**Code to reproduce the issue**\r\n\r\nI can reproduce what I think is the same error using: \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput_a = tf.keras.layers.Input((4,))\r\ninput_b = tf.keras.layers.Input((4,))\r\n\r\noutput = tf.keras.layers.Concatenate()([input_a, input_b])\r\n\r\nmodel = tf.keras.Model(inputs = (input_a, input_b), outputs = output)\r\n\r\na = np.random.rand(3, 4).astype(np.float32)\r\nb = np.random.rand(3, 4).astype(np.float32)\r\n\r\npred = model(a, b)\r\n```\r\n", "comments": ["So turns out I needed to do e.g. \r\n```\r\npred = model(a, b)\r\n```\r\nThe error message could be more informative!"]}]