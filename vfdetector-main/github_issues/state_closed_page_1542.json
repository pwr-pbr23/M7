[{"number": 6664, "title": "Max pool grad grad", "body": "Adds gpu and cpu kernels for max pool grad. Supports NHWC and NCHW layouts for gpu kernel and only NHWC for cpu kernel. Additionally registers gradient for python `MaxPoolGradGrad` which is just `MaxPoolGrad`. Registers gradient for python `AvgPoolGrad` which is `AvgPool` evaluated on `top_diff`.\r\n\r\nContinuation of #6299 ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I did some refactoring for `MaxPoolingOp` which now uses functors and templates and registers kernels using `TF_CALL_*`. Since currently there more kernels for gpu then cpu, functors are not generalized to devices. Also adding `double` required a minor tweak to `stream_executor` (merely duplicating methods for `double`). I also added python tests with `double` now. This should fix #547\r\n\r\nI also updated registered ops to use more general type - `realnumbertype`. Note, since I am not using default type for `T` ops compatibility test fails. Therefore, I have not updated yet `ops.pbtxt`.\r\n\r\nI also would like to add `MaxPool3dGradGrad` (currently I added only forward declaration for it).\r\n\r\nAs this pull request is getting quite big, I would appreciate some comments.", "looking forward to this, it should fix https://github.com/fchollet/keras/issues/4694 in keras when it arrives ", "I updated 3d kernels. Cpu kernel supports only NHWC and gpu kernel supports both. Currently, python tests for 3d pooling check only NHWC because gpu kernels for pooling and pooling grad only work with this format.\r\n\r\nSomeone can correct me if I am wrong, but cudnn 5 supports both formats of tensors (NHWC and NCHW). However, both pooling 2d and 3d kernels with cudnn backend always convert tensors from NHWC to NCHW format.", "This is really nice @aam-at: we're waiting for Benoit to do an initial review of this code (he's been on vacation the past week or so).  ", "I dropped commit changing ops.pbtxt which required rebase.", "Any update on reviewing this?", "@vrv @benoitsteiner max pool grad grad definitely looks like some nice changes, would appreciate the review + merge!", "I'll kick off a test anyway, but will wait for @benoitsteiner to review.\r\n\r\n@tensorflow-jenkins test this please", "The build was broken due to the changes in the upstream (191658d54f90ac03c15b339326129cd52d1f56a3). It is a bit confusing that testing is performed after merging with upstream, though, quite reasonable.\r\n\r\nI updated pooling with doubles to call new cudnn loading mechanism. So build should be successful now. @vrv Can you rerun tests again?", "@tensorflow-jenkins test this please\r\n\r\n(I don't know the status of our builds right now, so just trying anyway since it's the weekend and the load is low)", "Looks like the sanity check is failing:\r\n\r\n```\r\n=== Sanity check step 3 of 6: do_buildifier (buildifier check) ===\r\n\r\nRunning do_buildifier on 157 files\r\n\r\ntensorflow/core/kernels/BUILD # listsort unsafesort sort:tf_kernel_library.hdrs\r\n\r\nbuildifier took 0 s\r\n\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n2648d2647\r\n<         \"pooling_ops_common.h\",\r\n2649a2649\r\n>         \"pooling_ops_common.h\",\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n\r\n```", "Buildifier requires sorted list of headers and sources. So I fixed that. It's my\r\nfirst pull request to tensorflow, so I am a bit confused with the requirements\r\nfor various tests. Thanks, @vrv!", "@tensorflow-jenkins test this please\r\n\r\n@gunan one thing that would be cool would be to have the sanity checks run automatically when users push their PRs -- but then we'd control the rest of the testing.  ", "@aam-at this is a pretty epic first PR to TensorFlow, so thank you for your patience :)", "Tests are passing, except for macos. I don't have access to macos and I am not\r\nsure why the build is failing. Thank you for your patience too, @vrv!", "I think it's a infrastructure failure, the fact that the other tests pass suggests this is ready for review!", "I think this can be an issue in build files. But bazel 0.4.3 has a bug in exception propagation, which make us see unhelpful messages. I will upgrade what I can access and rerun tests.", "Jenkins, test this please", "All tests are passing now. Thanks for the help! I will wait for review now.", "ping for @benoitsteiner for this week.", "@vrv @benoitsteiner Bumping this item since last post was 10 days ago and it is waiting on review. :-)", "@vrv @benoitsteiner any update on this?", "@andydavis1 or @rmlarsen any takers? :)   I am swamped with a lot of other things :(", "@rmlarsen, do you have time to look at this change?", "I'll take a look a this sometime next week.", "@aam-at what's the status of this?", "@jlebar would you please review the stream executor changes in this PR?", "@martinwicke It implements MaxPoolingGradGrad kernels for CPU and GPU. Tests are passing on my machine.", "@aam-at Thanks for the updates. Could you please rebase to resolve the conflicts and address the remaining comments.", "@rmlarsen I have finished rebase. One commit was dropped, since `TensorFormat` handles now 5d tensors. Regarding `mask` parameter, I am not sure if we should rename this parameter because it can introduce further confusion:\r\n1) some kernels were borrowed from caffe\r\n2) not clear how to rename `NoMask` methods.", "There are still (or again) conflicts. @rmlarsen if you can resolve the naming issues, we can move this PR along.", "@aam-at OK, let's stick with mask then.", "(if this is the last sticking point, make sure to update your review, and I'll switch to just caring about the tests)", "@aam-at please resolve the conflict so we can test this more thoroughly.", "@rmlarsen I did rebase again. Currently, there are no conflicts, so you can run the tests.", "@aam-at Thanks!", "@tensorflow-jenkins test this please", "@aam-at it looks like there are some broken tests (backwards compatibility and XLA pooling tests) that need attention.", "@rmlarsen I fixed backward compatibility tests. Xla pooling ops test should be also fixed.", "@tensorflow-jenkins test this please ", "@aam-at Excellent. Thank you for this large contribution!", "@vrv @rmlarsen Thank you for the help!", "@aam-at it appears that the two tests:\r\n\r\n_testMaxPoolGradGradSamePadding2_1(data_format, use_gpu)\r\n\r\nand \r\n\r\n_testMaxPoolGradGradSamePadding3_1(data_format, use_gpu)\r\n\r\nare flaky when use_gpu is True, e.g.:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/grte/v4/k8-linux/lib/python2.7/unittest/case.py\", line 331, in run\r\n    testMethod()\r\n  File \"/build/work/0bfa20861d2fc543528291b5513e5ecb/google3/runfiles/google3/third_party/tensorflow/python/kernel_tests/pooling_ops_test.py\", line 1293, in testMaxPoolGradGrad\r\n    self._testMaxPoolGradGradSamePadding2_1(data_format, use_gpu)\r\n  File \"/build/work/0bfa20861d2fc543528291b5513e5ecb/google3/runfiles/google3/third_party/tensorflow/python/kernel_tests/pooling_ops_test.py\", line 1258, in _testMaxPoolGradGradSamePadding2_1\r\n    use_gpu=use_gpu)\r\n  File \"/build/work/0bfa20861d2fc543528291b5513e5ecb/google3/runfiles/google3/third_party/tensorflow/python/kernel_tests/pooling_ops_test.py\", line 813, in _ConstructAndTestSecondGradient\r\n    self.assertLess(err, err_tolerance)\r\n  File \"/usr/grte/v4/k8-linux/lib/python2.7/unittest/case.py\", line 932, in assertLess\r\n    self.fail(self._formatMessage(msg, standardMsg))\r\n  File \"/build/work/0bfa20861d2fc543528291b5513e5ecb/google3/runfiles/google3/testing/pybase/basetest.py\", line 1205, in fail\r\n    return super(TestCase, self).fail(self._formatMessage(prefix, msg))\r\n  File \"/usr/grte/v4/k8-linux/lib/python2.7/unittest/case.py\", line 412, in fail\r\n    raise self.failureException(msg)\r\n**AssertionError: 1.99932861328125 not less than 0.01**\r\n\r\nPerhaps there's a bug for padding equal to SAME? Do you have any idea what the underlying cause might be?", "The problem was caused by a race due to using buffer forwarding instead of allocation in MaxPoolingGradGradOp(). I'll submit a fix.", "@rmlarsen It was night in my timezone. Thanks for the fix. I added `forward_input_or_allocated_output` in c1b6aa4b6bbb2e80e228b3ecb9f2877f4e5dc26a. Shouldn't we replace buffer forwarding to allocating output for 3d pooling grad grad too? I am actually not sure why `forward_input_or_allocated_output` creates race conditions."]}, {"number": 6662, "title": "New feature: Total Variation (image_ops)", "body": "This is a pull-request for the new feature discussed in https://github.com/tensorflow/tensorflow/issues/6397\r\n\r\nI have made a short Python Notebook for testing this:\r\n\r\nhttps://gist.github.com/Hvass-Labs/ac4ef0074fd182c6778532ba222d8c37\r\n\r\nI can add a unit-test when the API / semantics have been agreed upon. I suppose I have to build TensorFlow locally in order to run a unit-test? Have you made a guide for doing this? I have some horrible experiences trying to build other open-source projects in the past.\r\n\r\n----\r\n\r\nPS: You may want to have your legal department go over the CLA agreement. It is legalistic gibberish and I had to read parts of it several times to sort-of understand it. Example: \"For legal entities, the entity making a Contribution and all other entities that control, are controlled by, or are under common control with that entity are considered to be a single Contributor. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\"\r\n", "comments": ["Can one of the admins verify this patch?", "The API and semantics look good. Please do add a unit test before we begin reviewing the code. Thank you.\r\n\r\nWe do have a guide for building from source. Please take a look here:\r\nhttps://www.tensorflow.org/get_started/os_setup#installing_from_sources", "I have now added some unit-tests to the pull-request. I have tried to follow the style in `image_ops_test.py`\r\n\r\nI have a few remarks:\r\n\r\n- It appears that some of the tests in `image_ops_test.py` merely test the TensorFlow implementation against an equivalent numpy implementation. I'm not sure I agree that this is a good way of testing, because you're likely to make the same mistakes in both implementations. Nevertheless, I didn't want to handcraft large test-cases, so I did the same :-) This has been supplemented with a few small hand-made test-cases, and I also tested the function using a Python Notebook - which is a superior kind of testing, but it cannot be automated.\r\n\r\n- I couldn't quite figure out the naming conventions used for the private test-functions but it is presumably of minor importance how I named private functions in these unit-tests. You may rename them if you wish.\r\n\r\n- I also wanted to test if the proper exception was raised for incorrect arguments, but `assertRaises()` didn't really seem suited for that purpose.\r\n\r\n- As usual, my code-comments are more verbose. I believe this is very helpful to those of us who don't work on TensorFlow-core every day, as it makes it easier for us to grasp what the intention is with the code.\r\n\r\n- I have not tested using bazel. Officially bazel only supports Ubuntu 15 and I use Ubuntu 16. I did some internet searches and found out that bazel installation may involve several steps and dependencies, and it could be difficult or impossible to remove if something went wrong. I didn't want to take the risk of breaking my Linux or TensorFlow installation just to run this unit-test, as it would take me several days to reinstall everything. So I ran the test-code in the same Python Notebook provided above, and just hacked it slightly with a few imports so it could run there. It should run in bazel. Please test this.\r\n", "Jenkins, test this please.", "The test fails:\r\n`Traceback (most recent call last):\r\n  File \"/workspace/pip_test/tests/image_ops_test.py\", line 2431, in testTotalVariationHandmade\r\n    self._test(a, tot_var)\r\n  File \"/workspace/pip_test/tests/image_ops_test.py\", line 2301, in _test\r\n    y = image_ops.total_variation(images=x_tf)\r\nAttributeError: 'module' object has no attribute 'total_variation`\r\n\r\n", "To make the symbol available in the image_ops namespace, I think you'll have to add the @@total_variation  callout to image_ops.py: https://github.com/Hvass-Labs/tensorflow/blob/02da06577be86ed095cde20fc674e7604e99894d/tensorflow/python/ops/image_ops.py#L164\r\n\r\n", "cc: @martinwicke @gunan so they know this is an API being introduced", "thanks @vrv, sgtm", "Thanks for your help and patience.\r\n\r\nI have now tried to export the function in the image_ops namespace, as you said.\r\n\r\nI'm still reluctant to install bazel and build tensorflow from source in case it breaks my working installation, because I'm currently not planning on making a lot of contributions to tf-core. So I'll see if I can get Jenkins to test it as well - I'm assuming it is a bot:\r\n\r\nJenkins, test this please.\r\n\r\n(Reminds me of this :-) https://www.youtube.com/watch?v=MFqxdvggAxM ", "Jenkins, test this please.", "I changed the capitalization of Total Variation in the comment, as you requested. I prefer it in capitals so it stands out, but I won't argue :-)\r\n\r\nThanks for the help and I'm glad you like the contribution!\r\n", "Jenkins, test this please.", "Jenkins, test this please.\r\n", "BTW Bazel takes me about 2 mins to install using following commands:\r\n\r\n```\r\nsudo add-apt-repository ppa:webupd8team/java\r\nsudo apt-get update\r\nsudo apt-get install oracle-java8-installer\r\n\r\necho \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\r\ncurl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -\r\nsudo apt-get update && sudo apt-get install bazel\r\nsudo apt-get upgrade bazel\r\nsudo apt-get upgrade bazel --fix-missing\r\n\r\n```", "Also, you can always use docker to create an isolated container which should not affect your working setup at all.\r\nYou can always download devel docker images and work with them.", "Jenkins, test this please.", "It seems that two of the tests are failing. I looked at the error message which was VERY long. I searched for `total_variation` but it does not appear in the errors, so I don't know if the errors are related to my contributed code? Also, the other tests seem fine. Should I do anything?\r\n\r\nThanks for the tips on installing bazel. My concern was both whether it might break Linux - and if it would somehow override my working installations of TensorFlow. If I'm only making one or two contribs to tf-core then it didn't seem worth the risk. But I've noted your installation instructions in case I'll need to build tf in the future.\r\n", "The error is this:\r\n\r\n```\r\n(482 / 518) Python test-on-install FAILED (32988 ms): tensorflow/python/training/basic_session_run_hooks_test.py\r\n  Log @: /workspace/pip_test/tests/logs/tensorflow/python/training/basic_session_run_hooks_test.py.log\r\n============== BEGINS failure log content ==============\r\n....W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 32 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\nFW tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n..W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n..W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n..W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n..W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n........W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n..W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n...W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.....W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n.W tensorflow/compiler/jit/xla_gpu_device.cc:48] Failed to create XLA_GPU device: Not found: could not find registered platform with name: \"CUDA\"\r\n..\r\n======================================================================\r\nFAIL: test_save_secs_saves_periodically (__main__.CheckpointSaverHookTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/workspace/pip_test/tests/basic_session_run_hooks_test.py\", line 343, in test_save_secs_saves_periodically\r\n    self.global_step.name))\r\nAssertionError: 3 != 4\r\n\r\n----------------------------------------------------------------------\r\nRan 47 tests in 30.475s\r\n\r\nFAILED (failures=1)\r\n============== ENDS failure log content ==============\r\n```", "@gunan @hawkinsp it's running XLA in Linux CPU Tests?", "@drpngx XLA is linked in at the moment, but it should not actually be doing anything in this test. The warning is benign and comes from device initialization. It should probably be lowered in severity; it just means you don't have a GPU, which this builder does not.\r\n\r\nI think that test is simply flaky (it uses sleep() and expects a certain number of events to occur in a certain time). Perhaps XLA made something a touch slower, but perhaps not.\r\n\r\nEither way I think @gunan is working on disabling XLA in the regular builds and adding a separate XLA-only build, which should fix any problems.", "Jenkins, test this please.\n\nOn Jan 11, 2017 8:36 AM, \"Peter Hawkins\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> XLA is linked in at the moment, but\n> it should not actually be doing anything in this test. The warning is\n> benign and comes from device initialization. It should probably be lowered\n> in severity; it just means you don't have a GPU, which this builder does\n> not.\n>\n> I think that test is simply flaky (it uses sleep() and expects a certain\n> number of events to occur in a certain time). Perhaps XLA made something a\n> touch slower, but perhaps not.\n>\n> Either way I think @gunan <https://github.com/gunan> is working on\n> disabling XLA in the regular builds and adding a separate XLA-only build,\n> which should fix any problems.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6662#issuecomment-271918787>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbalQhFhgrldDx4mq7uzR3AygPIBiks5rRQUMgaJpZM4LbfiL>\n> .\n>\n", "144150966 is supposed to fix XLA linking in all builds. So builds should recover once that is pushed to github.", "Jenkins, test this please."]}, {"number": 6661, "title": "InvalidArgumentError: for tensor bool tensorflow==0.12.1", "body": "Hi,\r\n\r\nI have experimented with tensorflow==0.12.1 and I have following problem:\r\n\r\nI am trying to run:\r\n```\r\ninp = tf.placeholder(tf.float32, [None, 10, 10, 3], name='inp')\r\ntraining = tf.placeholder(tf.bool, name='training')\r\nx1 = np.random.random([20, 10, 10, 3])\r\n\r\nconfig = tf.ConfigProto(device_count={'GPU': 0})\r\nwith tf.Session(config=config) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    result = var.eval({inp: x1, is_training: True}, sess)\r\n```\r\n\r\nBut I'm getting following stack trace:\r\n```\r\n  File \"myscript.py\", line 35, in tf_run\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'is_training' with dtype bool\r\n\t [[Node: is_training = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'training', defined at:\r\n    File \"myscript.py\", line 30, in tf_run\r\n    training = tf.placeholder(tf.bool, name='training')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1587, in placeholder\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2043, in _placeholder\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n```\r\n\r\nThis problem appears on:\r\n`Ubuntu 16.04.1 - tensorflow GPU installation`\r\n`Mac - tensorflow CPU only installation`\r\n\r\nWhen I try to downgrade to `tensorflow==0.11.0` and replace line `sess.run(tf.global_variables_initializer())` by `sess.run(tf.initialize_all_variables())`, then everything works well.", "comments": ["You need a boolean Tensor. Try `tf.constant(True)`.", "@inflation I've tried changing the line `result = var.eval({inp: x1, is_training: True}, sess)` to `result = var.eval({inp: x1, is_training: tf.condtant(True)}, sess)` even to `result = var.eval({inp: x1, is_training: tf.constant(True, dtype=tf.bool)}, sess)`, but both possibilities end up with `InvalidArgumentError:`.", "This kind of usage question is best asked on [Stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow). Github issues are for bug reports and installation issues.", "@michaelisard It looks more like a bug in tensorflow or at least backwards incompatibility. So from my point of view it's more an issue than question for stackoverflow. But if you wish, then here is the stackoverflow question http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1.", "@ziky90 did you find any solution to this ? I am facing the same issue in tensorflow 1.0.1 on Ubuntu 16.04.1 . I think this issue should be opened again unless there is a clear solution."]}, {"number": 6660, "title": "using TensorFlow on Windows Server 2012", "body": "I'm using TensorFlow on Windows Server 2012, TensorFlow version is 0.12.1\r\nI've install CUDA (CUDA Version 8.0.44), and install cuDnn (cuDNN v5.1 (August 10, 2016), for CUDA 8.0).\r\n\r\nthat cuDnn is build for windows 10.\r\n\r\nBut when I run my tensorflow code is get some error about:\r\n```\r\nam_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_IN\r\nTERNAL_ERROR\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_B\r\nAD_PARAM\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\kernels\\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(\r\n&algorithms)\r\n```\r\n\r\nIt seems the cuDNN version is not correct, but I tried cuDNN v5.0, also got this exception.\r\n\r\ndoes anyone know how to fix this issue?\r\nThanks very much.", "comments": ["ps, tensorflow will report load cudnn64_5.dll successfully:\r\n```\r\ndirname(inspect.getfile(tensorflow)))'\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfu\r\nlly opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfu\r\nlly opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfu\r\nlly opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfu\r\nlly opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfu\r\nlly opened CUDA library curand64_80.dll locally\r\n```", "I will try to reproduce the problem when I find the time.\r\nBut could you try installing windows 7 build of cudnn and see if that resolves the issue?", "Hi, @gunan , using cuDNN for windows 7 still got the same issue.", "This seems to be the same as #6509\r\nLet's follow the other issue to merge all discussion.", "Closing duplicate issue, as the other one seems to have more information.\r\nPlease follow #6059 for resolution.", "@weixsong How could you install tensorflow on windows server 2012.\r\nI have tried it many times and it didn't work. \r\nHere is my case:\r\nFirst I installed python 3.5.* but it gave me error and I found that windows server 2012 only supports python 2.7.* and then I installed python 2.7.*. \r\nAfter installing python 2.7, I installed tensorflow but I got an error that there are no support version and I found that tensorflow on windows server only supports python 3.5 and 3.6\r\nDo you have any advice?", "after spending hours I am able to fix this issue here is my answer on stackoverflow https://stackoverflow.com/a/50475864/1996802", "> @weixsong How could you install tensorflow on windows server 2012.\r\n> I have tried it many times and it didn't work.\r\n> Here is my case:\r\n> First I installed python 3.5.* but it gave me error and I found that windows server 2012 only supports python 2.7.* and then I installed python 2.7.*.\r\n> After installing python 2.7, I installed tensorflow but I got an error that there are no support version and I found that tensorflow on windows server only supports python 3.5 and 3.6\r\n> Do you have any advice?\r\n\r\n@normansiboro Are you sure about what you wrote in your comment, so according to you windows server 2012 R2 can't run Tensorflow as tf is not there  for that os ?\r\n\r\nOr were you able to find a workaround for it?"]}, {"number": 6659, "title": "Is it possible to implement Sparse Cross Entropy or Sparse Softmax Cross Entropy with Smooth Threshold to deal with large loss?", "body": "I use `sparse_softmax_cross_entropy_with_logits` in Seq2Seq task with large vocabulary.\r\nBut sometime I got a large loss, such as 1000. It's too large for optimization. So I think to add a smooth threshold in sparse_softmax_cross_entropy_with_logits may deal with this problem.\r\n```python\r\nsm = tf.nn.softmax(logit)\r\nsm = sm + threshold\r\nce = sparse_cross_entropy(sm, target)\r\n```\r\nBTW, I clip `logit` for smoothness now. But I'm not sure if it's a good idea.\r\n```python\r\nlogit = clip(logit, -threshold, threshold)\r\n```\r\nDoes anyone have better solution to deal with large loss or will implement Sparse Cross Entropy?\r\nThanks so much!\r\n\r\n", "comments": ["This kind of usage question is best asked on [Stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow). Github issues are for bug reports and installation issues."]}, {"number": 6658, "title": "Update zlib url in r0.12 branch", "body": "(Similar to change done to master, to make r0.12 buildable).\r\n\r\nFixes #6594", "comments": ["Unrelated failure, merging."]}, {"number": 6657, "title": "fix zlib dependency (current one fails with 403 on ./configure)", "body": "Looks like 1.2.8 got deleted, ./configure fails with\r\nERROR: /local_home/yaroslav/tensorflow_dbg.git/tensorflow/tensorflow/core/BUILD:970:1: no such package '@zlib_archive//': Error downloading [http://zlib.net/zlib-1.2.8.tar.gz] to /local_home/yaroslav/.cache/bazel/_bazel_yaroslav/687fadd894268346c74cc86e6f287d8c/external/zlib_archive/zlib-1.2.8.tar.gz: GET returned 404 Not Found and referenced by '//tensorflow/core:lib_internal'.", "comments": ["Can one of the admins verify this patch?", "Closing since there's another PR that fixes this\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/6612#issuecomment-270563522"]}, {"number": 6656, "title": "http://zlib.net/zlib-1.2.8.tar.gz is down, breaks configure", "body": "Need something like https://github.com/yaroslavvb/tensorflow/commit/fbc8bef52fc20d1c26290979f956e781bc863be8 to fix ./configure since \"http://zlib.net/zlib-1.2.8.tar.gz\" is down", "comments": ["Resolved in master with ecf97ee5f2e386be31fde8ca790d48786b969103, and in r0.12 with 1e317b1f7dc5ccf04fc51ac96d97f5bdaefa9af9"]}, {"number": 6655, "title": "fix zlib dependency (current one fails with 403 on ./configure)", "body": "Looks like 1.2.8 got deleted, ./configure fails with \r\nERROR: /local_home/yaroslav/tensorflow_dbg.git/tensorflow/tensorflow/core/BUILD:970:1: no such package '@zlib_archive//': Error downloading [http://zlib.net/zlib-1.2.8.tar.gz] to /local_home/yaroslav/.cache/bazel/_bazel_yaroslav/687fadd894268346c74cc86e6f287d8c/external/zlib_archive/zlib-1.2.8.tar.gz: GET returned 404 Not Found and referenced by '//tensorflow/core:lib_internal'.\r\n", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->"]}, {"number": 6654, "title": "min_pool", "body": "There is only max_pool and avg_pool, but min_pool op is a common op in Application, I should always write it by  myself. \r\nStrongly suggest to add this feature.", "comments": ["what about doing `-tf.nn.max_pool(-value, ksize, strides, padding)` when you need `min_pool`?", "that is a simple method, but still feel not intuitive\r\n", "what about creating a wrapper, you could do \r\n\r\n```\r\ndef min_pool(....):\r\n  return -tf.nn.max_pool(-value, ksize, strides, padding)\r\n```\r\n\r\nThis way you can just write `min_pool` directly."]}, {"number": 6653, "title": "Initialize variable failure when importing graph_def", "body": "In r0.12, I'm trying to write a graph_def at local and import it in another server. But when I `session.run(a)` after `session.run(global_variables_initializer())`, it raises a `FailedPreconditionError: Attempting to use uninitialized value a` .\r\n\r\n### Write graph\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(1, name='a', dtype=tf.int32)\r\nb = tf.Variable(2, name='b', dtype=tf.int32)\r\nc = a + b\r\nd = tf.Variable(c, name='d', dtype=tf.int32)\r\nsess = tf.Session()\r\ntf.train.write_graph(sess.graph, /tmp/PJT', 'test.pb', as_text=False)\r\n```\r\n### Import graph\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.framework import graph_pb2\r\n\r\ngraph_pb = '/home/mind/PJT/test.pb'\r\ngraph_def = graph_pb2.GraphDef()\r\n\r\nwith open(graph_pb, 'rb') as f:\r\n    graph_def.ParseFromString(f.read())\r\n\r\ntf.import_graph_def(graph_def, name='')\r\nsess = tf.Session()\r\na = sess.graph.get_tensor_by_name('a:0')\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(a)\r\n```\r\n### Error log\r\n```\r\nFailedPreconditionErrorTraceback (most recent call last)\r\n<ipython-input-43-574ff7291e3a> in <module>()\r\n      2 a = sess.graph.get_tensor_by_name('a:0')\r\n      3 sess.run(tf.global_variables_initializer())\r\n----> 4 sess.run(a)\r\n\r\n/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    764     try:\r\n    765       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 766                          run_metadata_ptr)\r\n    767       if run_metadata:\r\n    768         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    962     if final_fetches or final_targets:\r\n    963       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 964                              feed_dict_string, options, run_metadata)\r\n    965     else:\r\n    966       results = []\r\n\r\n/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1012     if handle is None:\r\n   1013       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1014                            target_list, options, run_metadata)\r\n   1015     else:\r\n   1016       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1032         except KeyError:\r\n   1033           pass\r\n-> 1034       raise type(e)(node_def, op, message)\r\n   1035 \r\n   1036   def _extend_graph(self):\r\n\r\nFailedPreconditionError: Attempting to use uninitialized value a\r\n\t [[Node: _send_a_0 = _Send[T=DT_INT32, client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=784291433964173963, tensor_name=\"a:0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](a)]]\r\n```\r\n\r\n@mrry Could you have a look?\r\n", "comments": ["Maybe this will work:\r\nadd `init` op(like `tf.global_variables_initializer`) to the original graph, and in the second graph import the original one and get this `init` op with `graph.get_operation_by_name` and run the op with `sess.run(init)`,\r\nafter this you can get any tensor with `graph.get_tensor_by_name` and run it out.", "@zakizhou Thanks for your advice. I tried it, and got some interesting and unreasonable results.\r\n### Write graph\r\n```python\r\nimport tensorflow as tf\r\na = tf.Variable(1, name='a', dtype=tf.int32)\r\nb = tf.Variable(2, name='b', dtype=tf.int32)\r\ninit_op = tf.global_variables_initializer() \r\nsess = tf.Session()\r\ntf.train.write_graph(sess.graph, '/tmp/PJT', 'test.pb', as_text=False)\r\n```\r\n### Import graph\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.framework import graph_pb2\r\n\r\ngraph_pb = '/tmp/PJT/test.pb'\r\ngraph_def = graph_pb2.GraphDef()\r\nwith open(graph_pb, 'rb') as f:\r\n    graph_def.ParseFromString(f.read())\r\ntf.import_graph_def(graph_def, name='')\r\nsess = tf.Session()\r\na = sess.graph.get_tensor_by_name('a:0')\r\nd = sess.graph.as_graph_element('d:0')\r\ninit_op = sess.graph.get_operation_by_name('init')\r\n\r\nsess.run(init_op)\r\nsess.run(a)\r\n```\r\nAt first time I run the `import graph` snippet, it raise same error as below:\r\n```\r\nFailedPreconditionError: Attempting to use uninitialized value b\r\n\t [[Node: b/read = Identity[T=DT_INT32, _class=[\"loc:@b\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](b)]]\r\n\t [[Node: b/Assign/_12 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_14_b/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op u'b/read', defined at:\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-e62bd92e6511>\", line 1, in <module>\r\n    tf.import_graph_def(graph_def, name='')\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/mind/anaconda2/envs/tf_0.12/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value b\r\n\t [[Node: b/read = Identity[T=DT_INT32, _class=[\"loc:@b\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](b)]]\r\n\t [[Node: b/Assign/_12 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_14_b/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n```\r\n\r\nThe most unreasonable thing is when I separately run `sess.run(init_op)` again, everything goes well. I don't know why it works, and I think you can reproduce it.\r\n### Run again\r\n```python\r\nsess.run(init_op)\r\nsess.run(a)\r\n```\r\noutput:\r\n```\r\n1\r\n```", "@DjangoPeng Personally I seldom use low level functions like `tf.import_graph_def`, instead I use `tf.train.import_meta_graph`(which internelly calls `import_graph_def`), especially after v0.12, this function supports much more things than before like changing the inputs of the original graph and will do all the dirty work for you including import graph def and restore variables from checkpoints.", "@zakizhou I had a try of [tf.train.import_meta_graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.train.import_meta_graph.md), and I always restore fail. Could you show me a use case? I attached my test snippet below:\r\n### Save model\r\n```python\r\nimport tensorflow as tf\r\na = tf.Variable(1, name='a', dtype=tf.int32)\r\nb = tf.Variable(2, name='b', dtype=tf.int32)\r\nc = a + b\r\ninit_op = tf.global_variables_initializer() \r\nsaver = tf.train.Saver(tf.trainable_variables())\r\nsess = tf.Session()\r\nsess.run(init_op)  #  I have to run it twice to avoid the same error above.\r\nsess.run(d)\r\nsaver.save(sess, '/tmp/PJT/model-0')\r\n```\r\nThen I get the model file as below:\r\n```\r\nmodel-0.meta\r\nmodel-0.index\r\nmodel-0.data-00000-of-00001\r\n```\r\n### Restore model\r\n```python\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n    new_saver = tf.train.import_meta_graph('/tmp/PJT/model-0.meta')\r\n    new_saver.restore(sess, 'model-0.data-00000-of-00001') # It raise NotFoundError\r\n```\r\nError log:\r\n```\r\nNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for model-0.data-00000-of-00001\r\n...\r\n```\r\n", "change `new_saver.restore(sess, 'model-0.data-00000-of-00001')` to `new_saver.restore(sess, 'model-0')` and have a try, **take care of the path**\r\n\r\nI successfully restored with the following code\r\n\r\n```\r\nimport tensorflow as tf\r\na = tf.Variable(1, name='a', dtype=tf.int32)\r\nb = tf.Variable(2, name='b', dtype=tf.int32)\r\nc = a + b\r\ninit_op = tf.global_variables_initializer()\r\nsaver = tf.train.Saver(tf.trainable_variables())\r\nsess = tf.Session()\r\nsess.run(init_op)  #  I have to run it twice to avoid the same error above.\r\nsaver.save(sess, 'data/model-0')\r\n```\r\n\r\nrestore:\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nnew_saver = tf.train.import_meta_graph('data/model-0.meta')\r\nnew_saver.restore(sess, 'data/model-0')\r\n```\r\n**just drop the suffix of your ckpt file**", "@zakizhou It works! thanks for your reply!\r\n@mrry  BTW, it looks like  `tf.import_meta_graph` is more friendly and easier than `tf.import_graph_def` for TensorFlow user. ", "try this:\r\n```python\r\ngd = sess.graph.as_graph_def()\r\nfor node in gd.node:\r\n    if node.op == 'RefSwitch':\r\n        node.op = 'Switch'\r\n        for index in range(len(node.input)):\r\n            if 'moving_' in node.input[index]:\r\n                node.input[index] = node.input[index] + '/read'\r\n    elif node.op == 'AssignSub':\r\n        node.op = 'Sub'\r\n        if 'use_locking' in node.attr: del node.attr['use_locking']\r\n    elif node.op == 'AssignAdd':\r\n        node.op = 'Add'\r\n        if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\r\nconstant_graph = graph_util.convert_variables_to_constants(sess, gd, ['output_res'])\r\nwith tf.gfile.FastGFile('./pb_model/model.pb', mode='wb') as f:\r\n    f.write(constant_graph.SerializeToString())\r\n```\r\n\r\nref: https://github.com/davidsandberg/facenet/issues/161"]}, {"number": 6652, "title": "https://www.tensorflow.org/robots.txt is disallowing all Search Engines crawling www.tensorflow.org.", "body": "[https://www.tensorflow.org/robots.txt] \r\nUser-agent: *\r\nDisallow: /\r\n\r\nCan you please allow Search Engines ?\r\n\r\nThanks,\r\nFabrice Canel\r\nMicrosoft Bing", "comments": ["I think this is still not resolved. I still do not see r0.12 results in bing.\r\n@dr4b @wolffg ", "Yeah, I think this one needs to go to Wolff -- sorry for not redirecting earlier.", "@wolffg is this resolved?", "This was fixed some time ago.\r\n\r\n[https://www.tensorflow.org/robots.txt]\r\nUser-agent: *\r\nDisallow: /s/results*\r\nSitemap: https://www.tensorflow.org/sitemap.xml\r\n\r\nIf we need to change it again, please re-open or re-file."]}, {"number": 6651, "title": "change tensorboard html data request path to relative", "body": "Tensorboard use a  absolute path  for data request like `/data/runs`, `/data/graph`, `/data/logdir`.\r\n\r\nIf someone want to deploy tensorboard with a **path** `http://host/tensorboard/`, the data xmlhttprequest would be  `http://host/data/runs`,  thus give not found 404 errors and tensorboard doesn't work properly.\r\n\r\nTo make the data request with `http://host/tensorboard/data/runs`, a relative  xmlhttprequest url for data request should be used.  The default `dataDir = '/data';` will change to `dataDir = 'data';`\r\n\r\nThe default  dataDir to `dataDir = 'data'` will always work whenever `tf-tensorboard.html` is placed or any context path is set.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@googlebot \r\nI signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Changing the dist/tf-tensorboard.html file alone isn't enough. Can you please change the Typescript source too (I believe it will be router.ts)", "@danmane  I have done it, thanks for your reminding.", "@dandelionmane Can you take another look, please?", "Sorry for losing track of this. I already made the change (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/components/tf_backend/router.ts), and we will be re-generating the tf-tensorboard.html file with this change and more this week, so I'm going to close the pull request."]}, {"number": 6650, "title": "`histogram_fixed_width` type errors and GPU issues", "body": "With the following script:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n    image = tf.zeros([200, 200, 3], dtype=tf.int32)\r\n    hist_range = tf.constant([0, 255], dtype=tf.int32)\r\n    hist = histogram_fixed_width(image, hist_range, nbins=16, dtype=tf.int32)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(hist)\r\n```\r\n\r\nGives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"hist.py\", line 5, in <module>\r\n    hist = tf.histogram_fixed_width(image, hist_range, nbins=16, dtype=tf.int32)\r\n  File \"/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/ops/histogram_ops.py\", line 84, in histogram_fixed_width\r\n    indices = math_ops.floor(nbins_float * scaled_values, name='indices')\r\n  File \"/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 805, in binary_op_wrapper\r\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\r\n  File \"/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 651, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 589, in _TensorTensorConversionFunction\r\n    % (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype float64: 'Tensor(\"histogram_fixed_width/scaled_values:0\", shape=(120000,), dtype=float64)'\r\n```\r\n\r\nI believe the issue here is this line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/histogram_ops.py#L75\r\n\r\nIt should say `to_double` instead of `to_float`. However, even if you fix this, you then get this error: \r\n\r\n```\r\nCaused by op u'histogram_fixed_width', defined at:\r\n  File \"hist.py\", line 79, in <module>\r\n    hist = histogram_fixed_width(image, hist_range, nbins=16, dtype=tf.int32)\r\n  File \"hist.py\", line 74, in histogram_fixed_width\r\n    name=scope)\r\n  File \"/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2952, in unsorted_segment_sum\r\n    num_segments=num_segments, name=name)\r\n  File \"/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2392, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/wcrichto/.env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'histogram_fixed_width': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: histogram_fixed_width = UnsortedSegmentSum[T=DT_INT32, Tindices=DT_INT32, _device=\"/device:GPU:0\"](histogram_fixed_width/ones_like, histogram_fixed_width/Cast, histogram_fixed_width/nbins)]]\r\n```\r\n\r\nI ran this both with the TF version from `pip install tensorflow-gpu` as well as when building from source with `--config=cuda` and got this error. However, the source makes it seem like this should work: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/segment_reduction_ops.cc#L323\r\n", "comments": ["@sherrym can you see what's going wrong here?", "I have the same issue. For me, the following work-around helps for now: cast the first two arguments of `tf.histogram_fixed_width` to `float32`.", "Automatically closing due to recent activity.\r\nPlease feel free to reopen if this is still an issue.\r\nYou can also send a pull request fixing the problem, if you have a proposed fix.", "This is still an issue in TF 1.2\r\n\r\nTo reproduce, the following is enough:\r\n\r\n    L = 10\r\n    values = tf.constant(list(range(L)), dtype=tf.int32)\r\n    histo_vals = tf.constant([0, L - 1], dtype=values.dtype)\r\n    tf.histogram_fixed_width(values=values, value_range=histo_vals, nbins=L)"]}, {"number": 6649, "title": "v0.12 outputs worse result than v0.11, running same code of lstm", "body": "", "comments": ["I'm afraid there isn't enough detail here. Please open a new issue with the issue template filled in (OS, setup, code you are running, etc.)"]}, {"number": 6648, "title": "wide_n_deep tutorial issues", "body": "Please help. Running into an issue running the wide_n_deep.py tutorial.\r\n\r\n**Tensorflow Version:**\r\n$ python -c 'import tensorflow as tf; print(tf.__version__)'  # for Python 2\r\n=> 0.12.1\r\n\r\n**wide_n_deep_tutorial.py was downloaded from here:** https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/learn/wide_n_deep_tutorial.py)\r\n\r\n**Command and output**\r\n$ python wide_n_deep_tutorial.py --model_type=wide\r\n=>\r\n```\r\nTraining data is downloaded to /var/folders/7y/s1y_wxt93xj5l784mcn2sf380000gn/T/tmpvWXvU2\r\nTest data is downloaded to /var/folders/7y/s1y_wxt93xj5l784mcn2sf380000gn/T/tmpL4LH0x\r\nmodel directory = /var/folders/7y/s1y_wxt93xj5l784mcn2sf380000gn/T/tmpFbBGZe\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py:446 in fit.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py:446 in fit.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py:446 in fit.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nTraceback (most recent call last):\r\n  File \"wide_n_deep_tutorial.py\", line 208, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"wide_n_deep_tutorial.py\", line 204, in main\r\n    train_and_eval()\r\n  File \"wide_n_deep_tutorial.py\", line 197, in train_and_eval\r\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py\", line 446, in fit\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 191, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 355, in fit\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 690, in _train_model\r\n    features, labels = input_fn()\r\n  File \"wide_n_deep_tutorial.py\", line 197, in <lambda>\r\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\r\n  File \"wide_n_deep_tutorial.py\", line 159, in input_fn\r\n    for k in CATEGORICAL_COLUMNS}\r\n  File \"wide_n_deep_tutorial.py\", line 159, in <dictcomp>\r\n    for k in CATEGORICAL_COLUMNS}\r\nTypeError: __init__() got an unexpected keyword argument 'dense_shape'\r\n```\r\n\r\n**Running on Mac Sierra**", "comments": ["Am having the exact same issue [#6591](https://github.com/tensorflow/tensorflow/issues/6591) am on Ubuntu 16.04 LTS x64", "Try the example on the v0.12 branch.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/examples/learn/wide_n_deep_tutorial.py\r\n\r\ntf.TensorShape has breaking change in master branch (rename argument shape -> dense_shape. see https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/OePXmC9kJ7o/SRErOoYCDQAJ for more other API changes), and the examples catch up these changes. So examples in master branch could not work with 0.12 or older TensorFlow runtime.\r\n", "@nagachika yes I am able to run with the tutorial code from the v0.12 branch. Thanks!"]}, {"number": 6647, "title": "TypeError: Cannot create initializer for non-floating point type.", "body": "### Environment info\r\nOperating System:\r\ntensorflow docker(ubuntu 14.04)\r\n\r\n### Version\r\ntensorflow (0.12.1)/ tensorflow-gpu (0.12.1)\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nFrom official example:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification_character_rnn.py\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n# tensorflow-gpu (0.12.1)\r\n\r\ndefault dataset  'dbpedia' and my data set both get\r\n```\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpUlkI0V\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'save_summary_steps': 100, '_num_ps_replicas': 0, '_task_type': None, '_environment': 'local', '_is_chief': True, 'save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f26f2317750>, 'tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1\r\n}\r\n, '_task_id': 0, 'tf_random_seed': None, 'keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', 'save_checkpoints_steps': None, '_master': '', 'keep_checkpoint_max': 5}\r\nWARNING:tensorflow:From <ipython-input-1-f55d1ba92119>:85 in main.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From <ipython-input-1-f55d1ba92119>:85 in main.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\n\r\n\r\nTypeErrorTraceback (most recent call last)\r\n<ipython-input-1-f55d1ba92119> in <module>()\r\n     93 \r\n     94 \r\n---> 95 tf.app.run(main=main)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.pyc in run(main, argv)\r\n     41   # Call the main function, passing through any arguments\r\n     42   # to the final program.\r\n---> 43   sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n\r\n<ipython-input-1-f55d1ba92119> in main(unused_argv)\r\n     83 \r\n     84   # Train and predict\r\n---> 85   classifier.fit(x_train, y_train, steps=100)\r\n     86   y_predicted = [\r\n     87       p['class'] for p in classifier.predict(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)\r\n    189             _call_location(), decorator_utils.get_qualified_name(func),\r\n    190             func.__module__, arg_name, date, instructions)\r\n--> 191       return func(*args, **kwargs)\r\n    192     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\r\n    193         func.__doc__, date, instructions)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\r\n    353                              steps=steps,\r\n    354                              monitors=monitors,\r\n--> 355                              max_steps=max_steps)\r\n    356     logging.info('Loss for final step: %s.', loss)\r\n    357     return self\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\r\n    697       # cases, but will soon be deleted after the subclasses are updated.\r\n    698       # TODO(b/32664904): Update subclasses and delete the else-statement.\r\n--> 699       train_ops = self._get_train_ops(features, labels)\r\n    700       if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature\r\n    701         train_op = train_ops.train_op\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)\r\n   1050       `ModelFnOps` object.\r\n   1051     \"\"\"\r\n-> 1052     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n   1053 \r\n   1054   def _get_eval_ops(self, features, labels, metrics):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)\r\n   1021         model_fn_results = self._model_fn(features, labels, mode=mode)\r\n   1022     else:\r\n-> 1023       model_fn_results = self._model_fn(features, labels)\r\n   1024 \r\n   1025     if isinstance(model_fn_results, model_fn_lib.ModelFnOps):\r\n\r\n<ipython-input-1-f55d1ba92119> in char_cnn_model(features, target)\r\n     31     # Apply Convolution filtering on input sequence.\r\n     32     conv1 = tf.contrib.layers.convolution2d(\r\n---> 33         byte_list, N_FILTERS, FILTER_SHAPE1, padding='VALID')\r\n     34     # Add a RELU for non linearity.\r\n     35     conv1 = tf.nn.relu(conv1)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.pyc in func_with_args(*args, **kwargs)\r\n    175       current_args = current_scope[key_func].copy()\r\n    176       current_args.update(kwargs)\r\n--> 177     return func(*args, **current_args)\r\n    178   _add_op(func)\r\n    179   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.pyc in convolution(inputs, num_outputs, kernel_size, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\r\n    838                                        regularizer=weights_regularizer,\r\n    839                                        collections=weights_collections,\r\n--> 840                                        trainable=trainable)\r\n    841     outputs = nn.convolution(input=inputs,\r\n    842                              filter=weights,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.pyc in func_with_args(*args, **kwargs)\r\n    175       current_args = current_scope[key_func].copy()\r\n    176       current_args.update(kwargs)\r\n--> 177     return func(*args, **current_args)\r\n    178   _add_op(func)\r\n    179   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/variables.pyc in model_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device)\r\n    242                   initializer=initializer, regularizer=regularizer,\r\n    243                   trainable=trainable, collections=collections,\r\n--> 244                   caching_device=caching_device, device=device)\r\n    245 \r\n    246 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.pyc in func_with_args(*args, **kwargs)\r\n    175       current_args = current_scope[key_func].copy()\r\n    176       current_args.update(kwargs)\r\n--> 177     return func(*args, **current_args)\r\n    178   _add_op(func)\r\n    179   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/variables.pyc in variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device)\r\n    206                                        trainable=trainable,\r\n    207                                        collections=collections,\r\n--> 208                                        caching_device=caching_device)\r\n    209 \r\n    210 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n   1022       collections=collections, caching_device=caching_device,\r\n   1023       partitioner=partitioner, validate_shape=validate_shape,\r\n-> 1024       custom_getter=custom_getter)\r\n   1025 \r\n   1026 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    848           collections=collections, caching_device=caching_device,\r\n    849           partitioner=partitioner, validate_shape=validate_shape,\r\n--> 850           custom_getter=custom_getter)\r\n    851 \r\n    852   def _get_partitioned_variable(self,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    344           reuse=reuse, trainable=trainable, collections=collections,\r\n    345           caching_device=caching_device, partitioner=partitioner,\r\n--> 346           validate_shape=validate_shape)\r\n    347 \r\n    348   def _get_partitioned_variable(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\r\n    329           initializer=initializer, regularizer=regularizer, reuse=reuse,\r\n    330           trainable=trainable, collections=collections,\r\n--> 331           caching_device=caching_device, validate_shape=validate_shape)\r\n    332 \r\n    333     if custom_getter is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\r\n    675         dtype=variable_dtype,\r\n    676         validate_shape=validate_shape,\r\n--> 677         expected_shape=shape)\r\n    678     self._vars[name] = v\r\n    679     logging.vlog(1, \"Created variable %s with shape %s and init %s\", v.name,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.pyc in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope)\r\n    222           name=name,\r\n    223           dtype=dtype,\r\n--> 224           expected_shape=expected_shape)\r\n    225 \r\n    226   def __str__(self):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.pyc in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape)\r\n    325               # with the variable itself.\r\n    326               self._initial_value = ops.convert_to_tensor(\r\n--> 327                   initial_value(), name=\"initial_value\", dtype=dtype)\r\n    328               assert_expected_shape()\r\n    329 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in <lambda>()\r\n    663       else:\r\n    664         init_val = lambda: initializer(\r\n--> 665             shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n    666         variable_dtype = dtype.base_dtype\r\n    667 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/initializers.pyc in _initializer(shape, dtype, partition_info)\r\n    118     \"\"\"Initializer function.\"\"\"\r\n    119     if not dtype.is_floating:\r\n--> 120       raise TypeError('Cannot create initializer for non-floating point type.')\r\n    121     # Estimating fan_in and fan_out is not possible to do perfectly, but we try.\r\n    122     # This is the right thing for matrix multiply and convolutions.\r\n\r\nTypeError: Cannot create initializer for non-floating point type.\r\n\r\n```\r\n", "comments": ["relate to this https://github.com/tensorflow/tensorflow/issues/6342", "@ilblackdragon didn't you fix something similar to this?", "The same bug is triggered with the text_classification_character_cnn.py example.\r\n\r\nDebian 9, Python 3.5.3rc1, Tensorflow 0.12.1 (cpu)\r\n\r\n...\r\n  File \"/home/simon/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 850, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/home/simon/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 346, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/home/simon/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 331, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/home/simon/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 677, in _get_single_variable\r\n    expected_shape=shape)\r\n  File \"/home/simon/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/home/simon/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 327, in _init_from_args\r\n    initial_value(), name=\"initial_value\", dtype=dtype)\r\n  File \"/home/simon/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 665, in <lambda>\r\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n  File \"/home/simon/.local/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/initializers.py\", line 120, in _initializer\r\n    raise TypeError('Cannot create initializer for non-floating point type.')\r\nTypeError: Cannot create initializer for non-floating point type.\r\n", "Duplicate of #6342. See linked PR for a possible solution.", "Pls, I have write the simple code and I faced some problem\r\n\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\r\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, epochs=3\r\n\r\n\r\nTraceback (most recent call last)\r\n<ipython-input-11-b5d801a94bb2> in <module>\r\n     10 model.add(tf.keras.layers.Dense(10, activation="]}, {"number": 6646, "title": "A suggested improvement for tf.nn.embedding_lookup_sparse() (with code)", "body": "The sp_ids and sp_weights parameters in [embedding_lookup_sparse()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L203) can be merged into a single parameter sp_mat which has clearer semantics. In sp_ids and sp_weights, cols are not utilized. However, in sp_mat, we will use row as instance, col as embedding ids, and its entry/value as weight. This makes better sense and reduce parameters for the function. I have written up the function attached below (slightly modified from original code), and also provided some test cases. Hope you may consider it.\r\n\r\n```\r\ndef embedding_lookup_sparse(params, sp_mat,\r\n                            partition_strategy=\"mod\",\r\n                            name=None,\r\n                            combiner=None,\r\n                            max_norm=None):\r\n  \"\"\"Computes embeddings for the given ids and weights.\r\n\r\n  This op assumes that there is at least one id for each row in the dense tensor\r\n  represented by sp_mat (i.e. there are no rows with empty features, if so, \r\n  put 0.0 in sp_mat entry), and that all the indices of sp_mat are in\r\n  canonical row-major order.\r\n\r\n  It also assumes that all id values lie in the range [0, p0), where p0\r\n  is the sum of the size of params along dimension 0.\r\n\r\n  Args:\r\n    params: A single tensor representing the complete embedding tensor,\r\n      or a list of P tensors all of same shape except for the first dimension,\r\n      representing sharded embedding tensors.  Alternatively, a\r\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\r\n      element must be appropriately sized for the given `partition_strategy`.\r\n    sp_mat: N x M SparseTensor of zero or non-zero weights, \r\n      where N is typically batch size and M is the embedding table size.\r\n    partition_strategy: A string specifying the partitioning strategy, relevant\r\n      if `len(params) > 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\r\n      is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\r\n    name: Optional name for the op.\r\n    combiner: A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\r\n      and \"sum\" are supported.\r\n      \"sum\" computes the weighted sum of the embedding results for each row.\r\n      \"mean\" is the weighted sum divided by the total weight.\r\n      \"sqrtn\" is the weighted sum divided by the square root of the sum of the\r\n      squares of the weights.\r\n    max_norm: If not None, each embedding is normalized to have l2 norm equal\r\n      to max_norm before combining.\r\n\r\n  Returns:\r\n    A dense tensor representing the combined embeddings for the sparse ids. \r\n    For each row in the dense tensor represented by sp_mat, the op looks up \r\n    the embeddings for all (non-zero) ids in that row, multiplies them by the\r\n    corresponding weight, and combines these embeddings as specified.\r\n\r\n    In other words, if\r\n\r\n      shape(combined params) = [p0, p1, ..., pm]\r\n\r\n    and\r\n\r\n      shape(sp_mat) = [d0, d1, ..., dn]\r\n\r\n    then\r\n\r\n      shape(output) = [d0, d1, ..., dn-1, p1, ..., pm].\r\n\r\n    For instance, if params is a 10x20 matrix, and sp_mat is\r\n\r\n      [0, 0]: 1.0\r\n      [0, 1]: 3.0\r\n      [1, 0]: 0.0\r\n      [2, 3]: 1.0\r\n\r\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\r\n\r\n      output[0, :] = (params[0, :] * 1.0 + params[1, :] * 3.0) / (1.0 + 3.0)\r\n      output[1, :] = params[0, :] * 0.0 / div_protect\r\n      output[2, :] = params[3, :] * 1.0 / 1.0\r\n\r\n  Raises:\r\n    TypeError: If sp_mat is not a SparseTensor.\r\n    ValueError: If combiner is not one of {\"mean\", \"sqrtn\", \"sum\"}.\r\n  \"\"\"\r\n  if combiner is None:\r\n    logging.warn(\"The default value of combiner will change from \\\"mean\\\" \"\r\n                 \"to \\\"sqrtn\\\" after 2016/11/01.\")\r\n    combiner = \"mean\"\r\n  if combiner not in (\"mean\", \"sqrtn\", \"sum\"):\r\n    raise ValueError(\"combiner must be one of 'mean', 'sqrtn' or 'sum'\")\r\n  if isinstance(params, variables.PartitionedVariable):\r\n    params = list(params)  # Iterate to get the underlying Variables.\r\n  if not isinstance(params, list):\r\n    params = [params]\r\n  if not isinstance(sp_mat, sparse_tensor.SparseTensor):\r\n    raise TypeError(\"sp_mat must be SparseTensor\")\r\n\r\n  with ops.name_scope(name, \"embedding_lookup_sparse\",\r\n                      params + [sp_mat]) as name:\r\n    segment_ids = sp_mat.indices[:, 0]\r\n    if segment_ids.dtype != dtypes.int32:\r\n      segment_ids = math_ops.cast(segment_ids, dtypes.int32)\r\n\r\n    ids = sp_mat.indices[:, 1]\r\n\r\n    embeddings = embedding_lookup(\r\n        params, ids, partition_strategy=partition_strategy, max_norm=max_norm)\r\n\r\n    weights = sp_mat.values\r\n    if weights.dtype != embeddings.dtype:\r\n      weights = math_ops.cast(weights, embeddings.dtype)\r\n\r\n    # Reshape weights to allow broadcast\r\n    ones = array_ops.fill(\r\n        array_ops.expand_dims(array_ops.rank(embeddings) - 1, 0), 1)\r\n    bcast_weights_shape = array_ops.concat_v2(\r\n        [array_ops.shape(weights), ones], 0)\r\n\r\n    orig_weights_shape = weights.get_shape()\r\n    weights = array_ops.reshape(weights, bcast_weights_shape)\r\n\r\n    # Set the weight shape, since after reshaping to bcast_weights_shape,\r\n    # the shape becomes None.\r\n    if embeddings.get_shape().ndims is not None:\r\n      weights.set_shape(orig_weights_shape.concatenate(\r\n          [1 for _ in range(embeddings.get_shape().ndims - 1)]))\r\n\r\n    embeddings *= weights\r\n\r\n    div_protect = 1e-32  # would not work for float16 or float8\r\n    if combiner == \"sum\":\r\n      embeddings = math_ops.segment_sum(embeddings, segment_ids, name=name)\r\n    elif combiner == \"mean\":\r\n      embeddings = math_ops.segment_sum(embeddings, segment_ids)\r\n      weight_sum = math_ops.segment_sum(weights, segment_ids)\r\n      embeddings = math_ops.div(embeddings, weight_sum + div_protect, name=name)\r\n    elif combiner == \"sqrtn\":\r\n      embeddings = math_ops.segment_sum(embeddings, segment_ids)\r\n      weights_squared = math_ops.pow(weights, 2)\r\n      weight_sum = math_ops.segment_sum(weights_squared, segment_ids)\r\n      weight_sum_sqrt = math_ops.sqrt(weight_sum)\r\n      embeddings = math_ops.div(embeddings, weight_sum_sqrt + div_protect, name=name)\r\n    else:\r\n      assert False, \"Unrecognized combiner\"\r\n\r\n    return embeddings\r\n```\r\n\r\nTest cases\r\n\r\n```\r\nsp_mat = [np.array([(0, 0), (0,1), (1, 0), (2, 1), (3, 0), (3, 1)]), np.array((0.5, 0.5, 1, 1, 0, 0)), (-1, -1)]\r\nwith tf.Graph().as_default():\r\n    with tf.Session() as sess:\r\n        idx = tf.sparse_placeholder(dtype=tf.float32)\r\n        emb = tf.Variable(initial_value=np.random.random((100, 2)).astype('float32'))\r\n        y = embedding_lookup_sparse(emb, idx, combiner='mean')\r\n        sess.run(tf.global_variables_initializer())\r\n        result = sess.run([y], feed_dict={idx: sp_mat})[0]\r\n\r\nassert (result[0] == (result[1] + result[2]) / 2).all()\r\nassert (result[3] == np.array([0, 0])).all()\r\n```\r\n", "comments": ["@theweiho what do you think?", "It's not clear to me that this would make more sense - especially for the base use case where users simply have some IDs (without weights). Considering how the `tensorflow.Example` input data is probably storing the IDs as a repeated `bytes_list` or `int64_list` and how the weights are usually stored as a parallel `Feature`, it seems this change would force users to do an additional conversion into this matrix format.\r\n\r\nAlso, given existing uses of this externally and internally, I'm not sure it's worth the API churn.\r\n\r\nThoughts, @ysuematsu ?\r\n", "@theweiho For the record, sp_mat will have the same format as sp_ids, just entries will present weights instead of ids, none weight is the same as that sp_mat id entries being const. like 1.", "Suppose the IDs are stored as \r\nExample 1: `int64_list { value: 7 value: 101 value: 420 }`\r\nExample 2: `int64_list { value: 1 value: 8888 }`\r\n\r\nThey'd currently be read in as\r\n```\r\nSparseTensor(\r\n  indices=[[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],\r\n  values=[7, 101, 420, 1, 8888],\r\n  dense_shape=[2, 3])\r\n```\r\nby `parse_example()`, and can be used directly with `embedding_lookup_sparse()`. \r\n\r\nWith this change, the user will need to do an additional conversion to change it to \r\n```\r\nSparseTensor(\r\n  indices=[[0, 7], [0, 101], [0, 420], [1, 1], [1, 8888]],\r\n  values=[1, 1, 1, 1, 1],\r\n  dense_shape=[2, 8889])\r\n```\r\nbefore being able to pass it into `embedding_lookup_sparse()`. This seems to make things more complicated and error-prone?\r\n", "@theweiho that's a great example! And in this case, users need to make the changes. Well, that's expected since we're talking about changing the API. However, users don't have to keep useless col indices as in current one. more importantly, if you want to add weights based on the new API, you only need to change values, and don't need to construct and pass additional parameter. BTW, it seems dense_shape can be set to (-1, -1) and it would still work fine.", "I'd expect that to be the most common use case, though? Even if there are useless col indices, they're usually just generated for the user by `parse_example()` .\r\n\r\nI agree that the new API would make it cleaner to add arbitrary weights - though I'd also expect most cases dealing with weights to have the weights stored in their input data as well - like\r\nExample 1: float_list { value: 0.3 value: 0.6 value: 0.1 }\r\nExample 2: float_list { value: 0.8 value: 0.2 }\r\nwhich would be automatically converted into SparseTensor by `parse_example()`.\r\n\r\nI'm not sure making the most common use case more difficult (and requiring people to manipulate SparseTensor internals) would be a good API change - let's see what @ysuematsu thinks after he's back from vacation.\r\n\r\n(In addition, fully changing the API would require a deprecation period, updating existing users, then fixing any missed breakages after the full change. Just adding another arg to the fn - allowing ppl to pass in either sp_ids/sp_weights or sp_mat would be easier, but that'd increase the number of parameters and code complexity.)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I'm closing this due to lack of activity but please reopen if it still seems worth doing/discussing."]}, {"number": 6645, "title": "Error: Found more than one graph event per run", "body": "Hello,\r\n\r\nI am trying to debug an issue that I am having with a model. When I decide to kill the training process, I remove the tensorboard logs, and fix an issue. Afterwards I restart the process.\r\n\r\nSlowly, I seem to have accumulated tensorboard graphs, because I get the following Warning (multiple times)\r\n\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\n\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event\r\n\r\nAny idea how to resolve this issue?", "comments": []}, {"number": 6644, "title": "Error: Data loss: file is too short to be an sstable", "body": "Hi there,\r\n\r\nI'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm\r\n\r\nI am getting a \"Data loss\" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.\r\n\r\nCurrently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues. \r\n\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable\r\nTraceback (most recent call last):\r\n  File \"single_lm_train.py\", line 38, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"single_lm_train.py\", line 34, in main\r\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\r\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 111, in run_eval\r\n    while ckpt_loader.load_checkpoint():\r\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 45, in load_checkpoint\r\n    if load_from_checkpoint(self.saver, self.logdir):\r\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 27, in load_from_checkpoint\r\n    saver.restore(sess, ckpt.model_checkpoint_path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1388, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\ntensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable\r\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]\r\nCaused by op u'save/RestoreV2_15', defined at:  File \"single_lm_train.py\", line 38, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"single_lm_train.py\", line 34, in main\r\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\r\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 98, in run_eval\r\n    saver = tf.train.Saver(model.avg_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 624, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nDataLossError (see above for traceback): file is too short to be an sstable\r\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/repl\r\nica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha\r\npe_and_slices)]]\r\n\r\n```", "comments": ["Right before this error occurs, I get the following output. It seems like the checkpoint file is never created, and at some point a DataLossError is thrown. Any help is greatly appreciated!\r\n```\r\nNo checkpoint file found. Waiting...\r\nNo checkpoint file found. Waiting...\r\nNo checkpoint file found. Waiting...\r\nWaiting for a new checkpoint...\r\n```\r\n", "Would you fill in the issue template with your OS/setup, etc.? Thanks.", "Hi Michael,\r\n\r\nYes, here is the information\r\n\r\nOperating System: Ubuntu 16.04.1 LTS (GNU/Linux 4.4.0-53-generic x86_64)\r\n\r\nSetup:\r\n  tensorflow installed via \"pip install tensorflow-gpu\"\r\n  CUDA 8.0 & cuDNN 3.5\r\n  4 Tesla K80 (2 GPUs per card) @ 24GB per card\r\n\r\n\r\n", "That error message appears to mean one of your checkpoint files is <48 bytes in size. Is it possible that your training script failed or files got corrupted or truncated afterwards? If you regenerate the data, does the same error still happen?", "Hi Jart,\r\n\r\nThanks so much for your response.\r\n\r\nYes, the error is persistent. It seems like the Checkpoint file is being created through a deprecated class, and I can't figure out how to fix it.\r\n\r\nI'm now trying to simply rebuild the entire model from scratch and using the newest API documentation for version 0.12\r\n\r\nDo you think I could contact you directly via eMail or Google Hangout with questions? Just one time, not a repitive thing. My email is Lldenaurois@gmail.com just in case.\r\n\r\nAlso, I noticed that some of the documentation, e.g. Tutorials and How-Tos still refer to the older API, some of which is deprecated. I would like to contribute by updating the documentation. How would I go about that?", "Sorry, to your questio : The training script does not fail, but the eval script does. I think the checkpoint file is truncated because its calling a deprecated API call, which leads to the truncated checkpoint file.\r\n\r\nLike I said, I'm rebuding the entire model from scratch. Maybe once its done, we could add the source to the model zoo. I saw that lm_1b is in there, but only the eval script, not the training script.\r\n\r\nI contacted rafal about updating his repo, but he never answered. Is this something you guys think would be interesting?", "The repository you linked in the original comment appears to be doing a lot of checkpoint loading logic by hand. Please understand that TensorFlow is community supported and we also can't provide support for other people's projects.\r\n\r\nRight now I'm trying to determine if a TensorFlow API is responsible for writing corrupt data. If that's the case, then it's a bug, and we want to fix it. Anything you can do to help us better understand what the bug is, will be helpful. Possibly by providing a minimal reproducible example.\r\n\r\nAlso does [testLegacyInitOp()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/saved_model_test.py#L456) demonstrate what you're trying to do?", "Also would tf.train.NewCheckpointReader serve your needs?", "Hi Justine,\r\n\r\nThanks for the info. I'll try those out and get back to you with a minimum reprodubcible example (if there is a bug).\r\n\r\nThanks for the help!", "Closing due to lack of recent activity. Please update the issue if it persists and we will reopen.", "if you're getting this from a tf.data dataloader, then try deleting the cache file. This worked for me"]}, {"number": 6643, "title": "Build works; import does not", "body": "The latest build completes, but fails with an import error:  ImportError: No module named losses_impl\r\n\r\nthe complete message is here\r\n\r\n[melrobin@scorpion ~]$ python\r\nPython 2.7.11 (default, Sep 29 2016, 13:33:00) \r\n[GCC 5.3.1 20160406 (Red Hat 5.3.1-6)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow-0.12.1-py2.7-linux-x86_64.egg/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow-0.12.1-py2.7-linux-x86_64.egg/tensorflow/python/__init__.py\", line 102, in <module>\r\n    from tensorflow.python.ops.losses import losses\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow-0.12.1-py2.7-linux-x86_64.egg/tensorflow/python/ops/losses/losses.py\", line 40, in <module>\r\n    from tensorflow.python.ops.losses.losses_impl import *\r\nImportError: No module named losses_impl", "comments": []}, {"number": 6642, "title": "Tensorboard doesn't show anything in 0.12", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI was having IOErrors but after trying in different browsers. After reading issue [#4830](https://github.com/tensorflow/tensorflow/issues/4830), I decided to upgrade tensorflow to 0.12, even when it says the 0.11 is fixed.\r\n\r\n\r\nA [link](https://www.tensorflow.org/get_started/os_setup#pip_installation) to the pip package I installed: tensorflow-0.12.1-cp27-none-linux_x86_64.whl\r\n\r\n### Minimal reproducible example\r\nSo, the IOErrors did dissapeared but I stil can't show any graph or scalar or summary. After following the debug everything seems fine:\r\n\r\n```\r\nINFO:tensorflow:TensorBoard is in debug mode.\r\nINFO:tensorflow:Starting TensorBoard in directory /home/yunli/Documents/Research/articles/australian language/tensorFlow_use\r\nINFO:tensorflow:TensorBoard path_to_run is: {'/home/Documents/Research/articles/australian language/tensorFlow_use/logs': None}\r\nINFO:tensorflow:Event Multiplexer initializing.\r\nINFO:tensorflow:Event Multiplexer done initializing\r\nINFO:tensorflow:TensorBoard reload process beginning\r\nINFO:tensorflow:Starting AddRunsFromDirectory: /home/Documents/Research/articles/australian language/tensorFlow_use/logs\r\nINFO:tensorflow:Done with AddRunsFromDirectory: /home/Documents/Research/articles/australian language/tensorFlow_use/logs\r\nINFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer\r\nINFO:tensorflow:Beginning EventMultiplexer.Reload()\r\nINFO:tensorflow:Finished with EventMultiplexer.Reload()\r\nINFO:tensorflow:TensorBoard done reloading. Load took 0.001 secs\r\nINFO:tensorflow:TensorBoard is tag: 39\r\nStarting TensorBoard 39 on port 6006\r\n(You can navigate to http://127.0.1.1:6006)\r\n```\r\nHowever, I try again all the browsers (Chrome, Firefox and Chromium) and nothing appear. I'll provide one of the event files I [got](https://drive.google.com/file/d/0B_CAQhE3IU1Vc1Z0SDdoa3VXdEE/view?usp=sharing).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["What model are you trying to visualize?  What python model? How do I run it? Is it a standard example or your code? If it is your code, try one of the standard tutorial examples and make sure you can get that to work. Thanks!", "Thanks! It was my code based on the [tutorial](https://www.tensorflow.org/how_tos/summaries_and_tensorboard/). I tried the mnist_with_summaries.py and it is working.  In my code, I am using the suggested function :\r\n\r\n```\r\ndef variable_summaries(var):\r\n  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\r\n  with tf.name_scope('summaries'):\r\n    mean = tf.reduce_mean(var)\r\n    tf.summary.scalar('mean', mean)\r\n    with tf.name_scope('stddev'):\r\n      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\r\n    tf.summary.scalar('stddev', stddev)\r\n    tf.summary.scalar('max', tf.reduce_max(var))\r\n    tf.summary.scalar('min', tf.reduce_min(var))\r\n    tf.summary.histogram('histogram', var)\r\n```\r\nAnd I am calling it after the definition of certain operations. Also I am using the new functions to merge and write\r\n```\r\nmerged = tf.summary.merge_all()\r\ntrain_writer = tf.summary.FileWriter('./logs/train',sess.graph)\r\ninit = tf.global_variables_initializer()\r\n```\r\ninstead of the deprecated\r\n```\r\nmerged = tf.summary.merge_all()\r\ntrain_writer = tf.train.SummaryWriter('./logs/train',sess.graph)\r\ntf.initialize_all_variables()\r\n```\r\nAnd finally recording the summaries. There's no error when I run my code.\r\n```\r\nsummary, trnresult = sess.run([merged, accuracy], feed_dict={x: batch_xs,y: batch_ys,})\r\ntrain_writer.add_summary(summary, step)\r\n```\r\n\r\nWhat else can I be missing?\r\n", "This kind of usage question is best asked on [Stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow). Github issues are for bug reports and installation issues."]}, {"number": 6641, "title": "configure: Handle existing TF_CUDNN_VERSION correctly on OSX.", "body": "The configure script sets TF_CUDNN_EXT based on the desired CuDNN version. When TF_CUDNN_VERSION does not exist, the script correctly extracts the version from CuDNN's library symlink and uses it to populate TF_CUDNN_EXT. When TF_CUDNN_VERSION does exist, the script uses it to derive TF_CUDNN_EXT.\r\n\r\nUnfortunately, the logic for the latter case does not cover the special case in OSX naming. This commit adds logic to handle the special case in OSX.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "PR merged. Thanks, @pwnall ", "Thank you very much for the quick turnaround time!\r\n\r\nFWIW, this unblocks my work-in-progress [Homebrew formula](https://github.com/pwnall/homebrew-science/blob/tensorflow/tensorflow.rb).", "Good to know, @pwnall Thank you!\r\n\r\nCC @gunan @yifeif ", "@caisq Any chance this could be merged into the r1.0 branch? It'd be awesome if it'd be possible to install Tensorflow 1.0 with Homebrew.", "It will be merged to 1.0 branch before the next binary release.\nThanks for the contribution!\n\nOn Sat, Jan 14, 2017 at 4:07 PM, Victor Costan <notifications@github.com>\nwrote:\n\n> @caisq <https://github.com/caisq> Any chance this could be merged into\n> the r1.0 branch? It'd be awesome if it'd be possible to install Tensorflow\n> 1.0 with Homebrew.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6641#issuecomment-272663185>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOT8lCsyy5cvPphZgxmFrLZdeZPDHks5rSWMlgaJpZM4LbCgI>\n> .\n>\n"]}, {"number": 6640, "title": "not registered 'TensorArrayV2' on 12.1 on MacOS CPU", "body": "I'm getting\r\n`tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TensorArrayV2'\r\n` after upgrading to 0.12.1 on MacOS. Same code works on 0.11 on Mac and 0.12.1 on Ubuntu CPU\r\n\r\nThis op seems to be created by dynamic RNN code\r\n\r\n```\r\nnode {\r\n  name: \"global/RNN/TensorArray_1\"\r\n  op: \"TensorArrayV2\"\r\n  input: \"global/RNN/strided_slice_2\"\r\n  device: \"/cpu:0\"\r\n  attr {\r\n    key: \"clear_after_read\"\r\n    value {\r\n      b: true\r\n    }\r\n  }\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"dynamic_size\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_array_name\"\r\n    value {\r\n      s: \"global/RNN/dynamic_rnn/input_0\"\r\n    }\r\n  }\r\n\r\n\r\n```\r\nSorry don't have a shorter repro atm, but a long repro is to follow install instructions on https://github.com/openai/universe-starter-agent and run it with Mac CPU tensorflow        \r\n\r\n```\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.1-py3-none-any.whl\r\npip install -I --upgrade setuptools\r\npip install --upgrade $TF_BINARY_URL\r\npython train.py --num-workers 2 --env-id PongDeterministic-v3 --log-dir /tmp/pong\r\n```\r\n\r\n", "comments": ["Closing it for now since low-priority, will reopen with better reproducible example if this comes up again", "If you rebuild your graph using a newer version of tf Python, it'll point\nto the new TensorArray ops.\n\nOn Jan 6, 2017 8:47 PM, \"Yaroslav Bulatov\" <notifications@github.com> wrote:\n\n> Closed #6640 <https://github.com/tensorflow/tensorflow/issues/6640>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6640#event-914860585>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-6Rb967zkgg4Ie_ruProkicHIRAks5rPxjIgaJpZM4LbBgh>\n> .\n>\n"]}, {"number": 6639, "title": "Apply TensorFlow on ARFF", "body": "I have ARFF file or excel sheet with binary features(0,1) and class label also (0,1) can you help me to apply convolution neural network on my file to make classification with TensorFlow .\r\nThanks ", "comments": ["NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow."]}, {"number": 6638, "title": "Allow for different input signatures for different modes and additionally updating zlib fix", "body": "Reopening #5546 on request of @martinwicke \r\n\r\nChanges were reverted back for `estimator.py` which allows different input signatures under different operating modes (`EVAL`, `INFER`, `TRAIN`). \r\n\r\nAdditionally, I had to also fix weird zlib error which happened due to the updation of version.", "comments": ["Can one of the admins verify this patch?", "@martinwicke - here is the PR again. It's very small change.", "@abhitopia please sync and push.", "Note that there is zlib 11:\r\n\r\n\"Due to the bug fixes, any installations of 1.2.9 or 1.2.10 should be immediately replaced with 1.2.11.\"\r\n", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@drpngx - Reopened PR merged with master in #6887 "]}, {"number": 6637, "title": "Update deprecated function SummaryWriter.", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 6636, "title": "Tensorboard does not show any data", "body": "On latest git head, when running the mnist_with_summaries.py, tensorboard does not show any data:\r\n\r\n![distributions](https://cloud.githubusercontent.com/assets/5008257/21643171/2f91f656-d287-11e6-8e83-d9af45e7c978.png)\r\n![histogram](https://cloud.githubusercontent.com/assets/5008257/21643170/2f91d996-d287-11e6-99e3-f2371a84f448.png)\r\n![images](https://cloud.githubusercontent.com/assets/5008257/21643173/2f924f0c-d287-11e6-911e-34a7dc44a00a.png)\r\n![scalars](https://cloud.githubusercontent.com/assets/5008257/21643172/2f91f16a-d287-11e6-9734-c258b9973832.png)\r\n\r\nyou can find the generated events here:\r\n[events.zip](https://github.com/tensorflow/tensorflow/files/684693/events.zip)\r\n\r\n\r\nOperating System: Linux / x86_64\r\n\r\n**$ git rev-parse HEAD**\r\n7c36309c37b04843030664cdc64aca2bb7d6ecaa\r\n\r\n**$ bazel version**\r\nBuild label: 0.4.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 12:31:25 2016 (1482409885)\r\nBuild timestamp: 1482409885\r\nBuild timestamp as int: 1482409885\r\n\r\nthe mnist_with_summaries.py:\r\n./tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\r\n\r\nThe --inspect output seems to find the data:\r\n**$ tensorboard --logdir /tmp/tensorflow/mnist/logs/mnist_with_summaries/train/ --inspect**\r\n[inspect.txt](https://github.com/tensorflow/tensorflow/files/684700/inspect.txt)\r\n", "comments": ["This was due to a lagging chrome browser", "same problem"]}, {"number": 6635, "title": "Incorrect gradient when using tf.dynamic_stitch and tf.gather?", "body": "In Tensorflow 0.12, I find the discrepancy of gradients in two mathematically equivalent training procedures of LSTM, probably due to the use of tf.gather and tf.dynamic_stitch.  One is the normal procedure using the whole batch of training examples to unroll the LSTM in each step. The other first uses tf.gather to select ALL the examples of the whole batch in each step, then unroll the LSTM with those examples and finally use tf.dynamic_stitch to update the corresponding states and outputs.\r\n\r\nThese two procedures should be equivalent as they both essentially use the whole batch. However, the gradients of the same variables are significantly different.\r\n\r\nThe code is as follows (the core parts are essentially `# 1.` and  `# 2.`):\r\n\r\n```python\r\nbatch_size = 2\r\nnum_timesteps = 10\r\nvocab_size = 10\r\nnum_embedding_nodes = 32\r\nhidden_size = 128\r\nn_class = 2\r\nlearning_rate = 0.001\r\ninputs = tf.placeholder(tf.int64, [batch_size, num_timesteps])\r\ntargets = tf.placeholder(tf.int64, [batch_size])\r\nembedding = tf.get_variable(\"embedding\", [vocab_size, num_embedding_nodes])\r\nx = tf.nn.embedding_lookup(embedding, inputs)\r\nw_predict = tf.get_variable(\"w_predict\", [hidden_size, n_class])\r\nb_predict = tf.get_variable(\"b_predict\", [n_class])\r\n\r\nwith tf.variable_scope('lstm') as lstm_scope:\r\n  cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=False)\r\n  state = cell.zero_state(batch_size, dtype=tf.float32)\r\n  state1 = cell.zero_state(batch_size, dtype=tf.float32)\r\n  for t in range(num_timesteps):\r\n    if t == 0:\r\n      output, state = cell(x[:, t, :], state)\r\n      lstm_scope.reuse_variables()\r\n      output1, state1 = cell(x[:, t, :], state1)\r\n    else:\r\n      lstm_scope.reuse_variables()\r\n      # 1. normal lstm \r\n      output, state = cell(x[:, t, :], state)\r\n      # 2. lstm using tf.gather and tf.dynamic_stitch to select all samples from batch\r\n      idx_select = tf.range(batch_size)\r\n      tmp_output, tmp_state = cell(tf.gather(x[:, t, :], idx_select), tf.gather(state1, idx_select))\r\n      output1 = tf.dynamic_stitch([tf.range(batch_size), idx_select], [output1, tmp_output])\r\n      state1 = tf.dynamic_stitch([tf.range(batch_size), idx_select], [state1, tmp_state])\r\nlogits = tf.nn.xw_plus_b(output, w_predict, b_predict)\r\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets, name=None)\r\nlogits1 = tf.nn.xw_plus_b(output1, w_predict, b_predict)\r\ncross_entropy1 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, targets, name=None)\r\ntrainable = tf.trainable_variables()\r\ngrad = tf.gradients(cross_entropy, trainable)\r\ngrad1 = tf.gradients(cross_entropy1, trainable)\r\n\r\n######## gradient log\r\ncg = []\r\nfor g in grad:\r\n  if g is not None:\r\n    cg += [g]\r\ncg1 = []\r\nfor g in grad1:\r\n  if g is not None:\r\n    cg1 += [g]\r\n\r\noptimizer = tf.train.AdamOptimizer(learning_rate)\r\ntrain_op = optimizer.apply_gradients(zip(grad, trainable))\r\ninit_op = tf.initialize_all_variables()\r\n\r\n\r\n\"\"\" Training \"\"\"\r\n## arbitrary synthetic data\r\n# we use two training examples, each with length 10\r\nmy_input = np.array([[3,4,6,8,3,5,8,9,2,4], [4,2,3,8,5,2,2,3,6,1]])\r\nmy_target = np.array([0,1])\r\nsess = tf.Session()\r\nsess.run(init_op)\r\n\r\nepoch = 0\r\nwhile epoch < 5:\r\n  epoch += 1\r\n  fetches = [train_op, cg, cg1]\r\n  outputs = sess.run(fetches, feed_dict={inputs: my_input, targets: my_target})\r\n  gradients = outputs[1] \r\n  gradients1 = outputs[2] \r\n  print 'epoch %d:' % epoch \r\n    for i, g in enumerate(gradients):\r\n      if i > 0:\r\n\tprint('norm of gradient of var %d: %f' % (i, LA.norm(gradients[i])))\r\n\tprint('norm of gradient1 of var %d: %f' % (i, LA.norm(gradients1[i])))\r\n```\r\n\r\nThe sample output is:\r\n```\r\nepoch 1:\r\nnorm of gradient of var 1: 0.644620\r\nnorm of gradient1 of var 1: 0.644620\r\nnorm of gradient of var 2: 0.393020\r\nnorm of gradient1 of var 2: 0.393020\r\nnorm of gradient of var 3: 0.838759\r\nnorm of gradient1 of var 3: 102.815338\r\nnorm of gradient of var 4: 0.435841\r\nnorm of gradient1 of var 4: 44.867126\r\nepoch 2:\r\nnorm of gradient of var 1: 0.613848\r\nnorm of gradient1 of var 1: 0.611423\r\nnorm of gradient of var 2: 0.355387\r\nnorm of gradient1 of var 2: 0.351987\r\nnorm of gradient of var 3: 0.797761\r\nnorm of gradient1 of var 3: 96.391121\r\nnorm of gradient of var 4: 0.397020\r\nnorm of gradient1 of var 4: 39.937107\r\nepoch 3:\r\nnorm of gradient of var 1: 0.603118\r\nnorm of gradient1 of var 1: 0.603118\r\nnorm of gradient of var 2: 0.318260\r\nnorm of gradient1 of var 2: 0.318260\r\nnorm of gradient of var 3: 0.773661\r\nnorm of gradient1 of var 3: 93.290131\r\nnorm of gradient of var 4: 0.366684\r\nnorm of gradient1 of var 4: 36.636879\r\nepoch 4:\r\nnorm of gradient of var 1: 0.607643\r\nnorm of gradient1 of var 1: 0.607643\r\nnorm of gradient of var 2: 0.280101\r\nnorm of gradient1 of var 2: 0.280101\r\nnorm of gradient of var 3: 0.763007\r\nnorm of gradient1 of var 3: 92.295441\r\nnorm of gradient of var 4: 0.340769\r\nnorm of gradient1 of var 4: 33.630474\r\nepoch 5:\r\nnorm of gradient of var 1: 0.622874\r\nnorm of gradient1 of var 1: 0.619443\r\nnorm of gradient of var 2: 0.239731\r\nnorm of gradient1 of var 2: 0.235509\r\nnorm of gradient of var 3: 0.757205\r\nnorm of gradient1 of var 3: 93.335388\r\nnorm of gradient of var 4: 0.312203\r\nnorm of gradient1 of var 4: 30.536301\r\n```\r\n\r\nWe can see that the gradient and gradient1 of var3 have significantly different norms in every epoch, which should be the same. So is var4. Those two are the trainable variables of LSTM. In fact, if the sequence length is 50 instead of 10, the discrepancy is even much larger.\r\n\r\nCould anybody tell me why it is the case?\r\n\r\n### Environment info\r\nOperating System: ubuntu 14.04\r\n", "comments": ["Is this on CPU?", "@michaelisard , yes, on CPU.", "@rmlarsen any idea why this might be happening?", "@rmlarsen Were you planning to respond based on the tag you added?  Someone should probably track this down.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 6634, "title": "Nestable custom_getters", "body": "In TF 0.12 (and probably in earlier versions too), only one custom_getter is active at a time:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef postfix_name(postfix):\r\n    def custom_getter(getter, name, *args, **kwargs):\r\n        return getter(\"{}{}\".format(name, postfix), *args, **kwargs)\r\n    return custom_getter\r\n\r\nwith tf.Graph().as_default():\r\n    with tf.variable_scope(\"A\", custom_getter=postfix_name(\"_A\")):\r\n        with tf.variable_scope(\"B\"):\r\n            var1 = tf.get_variable(\"var1\", [])\r\n        with tf.variable_scope(\"C\", custom_getter=postfix_name(\"_C\")):\r\n            var2 = tf.get_variable(\"var2\", [])\r\n\r\n# Current functionality:\r\nassert tf.VERSION == '0.12.1'\r\nassert var1.name == 'A/B/var1_A:0'\r\nassert var2.name == 'A/C/var2_C:0'\r\n```\r\n\r\nThis is surprising to the user \u2013 at least to me it was \u2013 and limits the usefulness of custom_getters.\r\n\r\nI propose that nested custom_getters are applied recursively instead, like this:\r\n\r\n```python\r\n# Proposal:\r\nassert var1.name == 'A/B/var1_A:0'\r\nassert var2.name == 'A/C/var2_C_A:0'\r\n```\r\n\r\nThis would be useful since we often want to apply different transformations to weights before they are used. Here's a pseudo-code of a real-life use case:\r\n\r\n```python\r\nwith tf.variable_scope(\"weight_normed\", custom_getter=weight_normed):\r\n    with tf.variable_scope(\"model\", custom_getter=track_variables):\r\n        real_out = model(x)\r\n    with tf.variable_scope(\"model\", custom_getter=simulate_variables):\r\n        simulated_out = model(x)\r\n```", "comments": ["Possible workaround:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef default_custom_getter(getter, *args, **kwargs):\r\n    return getter(*args, **kwargs)\r\n\r\ndef postfix_name(postfix):\r\n    getter_wrapper = tf.get_variable_scope().custom_getter or default_custom_getter\r\n    def custom_getter(getter, name, *args, **kwargs):\r\n        return getter_wrapper(getter, \"{}{}\".format(name, postfix), *args, **kwargs)\r\n    return custom_getter\r\n\r\nwith tf.Graph().as_default():\r\n    with tf.variable_scope(\"A\", custom_getter=postfix_name(\"_A\")):\r\n        with tf.variable_scope(\"B\"):\r\n            var1 = tf.get_variable(\"var1\", [])\r\n        with tf.variable_scope(\"C\", custom_getter=postfix_name(\"_C\")):\r\n            var2 = tf.get_variable(\"var2\", [])\r\n\r\n# Workaround functionality:\r\nassert var1.name == 'A/B/var1_A:0'\r\nassert var2.name == 'A/C/var2_C_A:0'\r\n```", "@ebrevdo what do you think?", "This would be a useful feature and in line with how nested variable scope work.  PRs welcome.", "@ebrevdo I do agree with you. And I'm working on it. ", "Seems fixed already? The original code now prints:\r\n```\r\nA/B/var1_A:0\r\nA/C/var2_C_A:0\r\n```", "I guess I fixed this a while back."]}]