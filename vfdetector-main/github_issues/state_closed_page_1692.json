[{"number": 2140, "title": "Feature request: support symbolic links for tensorboard", "body": "It would be nice if one could use symbolic links to organize groups of training files. Tensorboard does not search over symbolic links for event files because the following line 178 of python/summary/event_multiplexer.py:\n\n``` python\nsubdirs = [\n        subdir\n        for (subdir, files) in io_wrapper.ListRecursively(path)\n        if list(filter(event_accumulator.IsTensorFlowEventsFile, files))\n]\n```\n\ndoes not walk across symbolic links. This would be easy to fix by adding a symlink option to ListRecursively().\n\nCurrent difficulties: \n1) Over time, the number of distinct training runs in a directory can grow to over 10, at which point tensorboard becomes slow and too information rich. Reorganizing a subset with symbolic links would be quite nice.\n\n2) Sometimes it would be nice to compare a small subset of training runs that exist in different directories. Copying these files with `cp -r` is quite slow.\n", "comments": ["Hi @Russell91. This request sounds quite reasonable to me. If you send in a pull request that enables this, I would happily accept it. I think the best thing to do would be to have gfile.Walk https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/default/_gfile.py#L400 follow symlinks by default (we can just pass follow_links=True to the os.walk call)\n"]}, {"number": 2139, "title": "conditional rbm ", "body": "Will conditional rbm ever be available in tensor flow library?\n\n[Here](http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf) is the original paper\n", "comments": ["This question is better asked in the discussion forum.   This context is for bug-like issues, thanks!\n\nhttps://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/discuss\n"]}, {"number": 2138, "title": "embedding_attention_seq2seq fails / embedding_rnn_seq2seq works", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: Cuda 7.0 and CUDNN 6.5 v4\n\nSo when I use a simple Embedding RNN Sequence to Sequence Model like this\n\n```\n# choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        # embedding model\n        with tf.variable_scope(\"train_test\"):\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n        with tf.variable_scope(\"train_test\", reuse = True):\n            self.dec_outputs_tst, _ = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n```\n\nThe above implementation works perfectly, but when I just change the model from simple embedding seq2seq to Embedding Attention Seq2Seq, like this,\n\n```\n\n        # choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        with tf.variable_scope(\"train_test\"):\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_attention_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n        with tf.variable_scope(\"train_test\", reuse = True):\n            self.dec_outputs_tst, _ = seq2seq.embedding_attention_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n\n```\n\nI get segmentation fault, with absolutely no information. My memory does not run out, neither my CPU, as I tried this with \n\n```\nbatch_size =1 \nsee.memory_dim = 1\n```\n\nand still got the same segmentation fault. \n\nI get the same error, and the above configuration can certainly not eat my RAM.\n\nThis is a potential bug, if I am not getting something worng. The LSTM and GRU cell just takes the size of the hidden layer as parameter, which is a scaler. \n\nTHE BUG REPORT \n\nThe Debug result\n\n```\n(gdb) run train_script_lstm_attn.py \nStarting program: /lusr/bin/python train_script_lstm_attn.py\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n[New Thread 0x7fffd7a25700 (LWP 45650)]\n[New Thread 0x7fffd7224700 (LWP 45651)]\n[New Thread 0x7fffd4a23700 (LWP 45652)]\n[New Thread 0x7fffd2222700 (LWP 45653)]\n[New Thread 0x7fffcfa21700 (LWP 45654)]\n[New Thread 0x7fffcd220700 (LWP 45655)]\n[New Thread 0x7fffcaa1f700 (LWP 45656)]\n[Thread 0x7fffcaa1f700 (LWP 45656) exited]\n[Thread 0x7fffcfa21700 (LWP 45654) exited]\n[Thread 0x7fffd7a25700 (LWP 45650) exited]\n[Thread 0x7fffd2222700 (LWP 45653) exited]\n[Thread 0x7fffd7224700 (LWP 45651) exited]\n[Thread 0x7fffcd220700 (LWP 45655) exited]\n[Thread 0x7fffd4a23700 (LWP 45652) exited]\n[New Thread 0x7fffcaa1f700 (LWP 45661)]\n[New Thread 0x7fffcd220700 (LWP 46103)]\n[New Thread 0x7fffcfa21700 (LWP 46104)]\n[New Thread 0x7fffd2222700 (LWP 46105)]\n[New Thread 0x7ffed22bf700 (LWP 46106)]\n[New Thread 0x7ffed1abe700 (LWP 46107)]\n[New Thread 0x7ffed12bd700 (LWP 46108)]\n[New Thread 0x7ffed0abc700 (LWP 46109)]\n[New Thread 0x7ffec3fff700 (LWP 46110)]\n[New Thread 0x7ffec37fe700 (LWP 46111)]\n[New Thread 0x7ffec2ffd700 (LWP 46112)]\n[New Thread 0x7ffeb77ff700 (LWP 46114)]\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN Black\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.98\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\n[New Thread 0x7ffeb6ffe700 (LWP 46115)]\n[New Thread 0x7ffeb67fd700 (LWP 46116)]\n[New Thread 0x7ffea2bff700 (LWP 46117)]\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX TITAN Black\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.98\npciBusID 0000:42:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN Black, pci bus id: 0000:42:00.0)\n[New Thread 0x7ffea23fe700 (LWP 46118)]\n[New Thread 0x7ffea1bfd700 (LWP 46119)]\n[New Thread 0x7ffea13fc700 (LWP 46120)]\n[New Thread 0x7ffea0bfb700 (LWP 46121)]\n[New Thread 0x7ffe8bfff700 (LWP 46122)]\n[New Thread 0x7ffe8b7fe700 (LWP 46123)]\n[New Thread 0x7ffe8affd700 (LWP 46124)]\n[New Thread 0x7ffe8a7fc700 (LWP 46125)]\n[New Thread 0x7ffe89ffb700 (LWP 46126)]\n[New Thread 0x7ffe897fa700 (LWP 46127)]\n[New Thread 0x7ffe88ff9700 (LWP 46128)]\n[New Thread 0x7ffe7bfff700 (LWP 46129)]\n[New Thread 0x7ffe3d23c700 (LWP 46167)]\n\nProgram received signal SIGSEGV, Segmentation fault.\n__memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143\n2143    ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S: No such file or directory.\n```\n\nThe Backtrace is attached below\n\n```\n(gdb) backtrace\n#0  __memmove_ssse3_back () at ../sysdeps/x86_64/multiarch/memcpy-ssse3-back.S:2143\n#1  0x00007fffedd56ae1 in tensorflow::Tensor::FromProto(tensorflow::Allocator*, tensorflow::TensorProto const&) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007fffedc4577f in tensorflow::ThreadPoolDevice::MakeTensorFromProto(tensorflow::TensorProto const&, tensorflow::AllocatorAttributes, tensorflow::Tensor*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fffecea4f76 in tensorflow::ConstantOp::ConstantOp(tensorflow::OpKernelConstruction*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fffecea50f2 in tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fffedd408bd in tensorflow::CreateOpKernel(tensorflow::DeviceType, tensorflow::DeviceBase*, tensorflow::Allocator*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fffedc1caf4 in tensorflow::CreateNonCachedKernel(tensorflow::Device*, tensorflow::FunctionLibraryRuntime*, tensorflow::NodeDef const&, int, tensorflow::OpKernel**) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fffedc10c97 in std::_Function_handler<tensorflow::Status (tensorflow::NodeDef const&, tensorflow::OpKernel**), tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*)::{lambda(tensorflow::NodeDef const&, tensorflow::OpKernel**)#2}>::_M_invoke(std::_Any_data const&, tensorflow::NodeDef const&, tensorflow::OpKernel**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fffedc287de in tensorflow::(anonymous namespace)::ExecutorImpl::Initialize() ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00007fffedc292b3 in tensorflow::NewLocalExecutor(tensorflow::LocalExecutorParams const&, tensorflow::Graph const*, tensorflow::Executor**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00007fffedc1608d in tensorflow::DoConstantFolding(tensorflow::ConstantFoldingOptions const&, tensorflow::Device*, tensorflow::Graph*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#11 0x00007fffedc36dda in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tensorflow::Device*, tensorflow::Graph**) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#12 0x00007fffeda06d9d in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#13 0x00007fffeda07e6a in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std---Type <return> to continue, or q <return> to quit---\n::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#14 0x00007fffeda0a992 in tensorflow::DirectSession::Run(std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#15 0x00007fffedc0b7c7 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#16 0x00007fffedc0bc11 in TF_Run ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#17 0x00007fffece8dff5 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#18 0x00007fffece8e661 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#19 0x00007fffece7a4d7 in _wrap_TF_Run ()\n   from /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#20 0x000000000049968d in call_function (oparg=<optimized out>, pp_stack=0x7fffffffdb20) at ../Python/ceval.c:4020\n#21 PyEval_EvalFrameEx (f=f@entry=\n    Frame 0xe341a40, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 628, in _run_fn (session=<SwigPyObject at remote 0x7fffb3547b70>, feed_dict={}, fetch_list=[], target_list=['init'], options=None, run_metadata=<TF_Buffer(this=<SwigPyObject at remote 0x7ffe3d2864e0>) at remote 0x7ffe496ff990>), \n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#22 0x00000000004a1c9a in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x0, \n    kwcount=<optimized out>, kws=<optimized out>, argcount=238295616, args=<optimized out>, locals=0x0, \n    globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252\n#23 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526\n#24 0x0000000000505f96 in PyObject_Call (func=<function at remote 0x7ffe3d39d938>, arg=<optimized out>, kw=<optimized out>)\n    at ../Objects/abstract.c:2529\n#25 0x000000000049b07a in ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, \n    pp_stack=0x7fffffffdd60, func=<function at remote 0x7ffe3d39d938>) at ../Python/ceval.c:4333\n---Type <return> to continue, or q <return> to quit---\n#26 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe341820, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 644, in _do_call (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Ne...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2705\n#27 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25330, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#28 0x000000000049ab45 in fast_function (nk=0, na=8, n=<optimized out>, pp_stack=0x7fffffffdf50, \n    func=<function at remote 0x7fffc3e28de8>) at ../Python/ceval.c:4116\n#29 call_function (oparg=<optimized out>, pp_stack=0x7fffffffdf50) at ../Python/ceval.c:4041\n#30 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe3415e0, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 637, in _do_run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neu...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#31 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25230, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#32 0x000000000049ab45 in fast_function (nk=0, na=7, n=<optimized out>, pp_stack=0x7fffffffe140, \n    func=<function at remote 0x7fffc3e28d70>) at ../Python/ceval.c:4116\n#33 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe140) at ../Python/ceval.c:4041\n#34 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0xe33d060, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 564, in _run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variabl---Type <return> to continue, or q <return> to quit---\nes': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#35 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e25030, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0)\n    at ../Python/ceval.c:3252\n#36 0x000000000049ab45 in fast_function (nk=0, na=6, n=<optimized out>, pp_stack=0x7fffffffe330, \n    func=<function at remote 0x7fffc3e28cf8>) at ../Python/ceval.c:4116\n#37 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe330) at ../Python/ceval.c:4041\n#38 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x1648a420, for file /u/harshal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py, line 340, in run (self=<InteractiveSession(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <Neural...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#39 0x00000000004a090c in PyEval_EvalCodeEx (co=0x7fffc3e22a30, globals=<optimized out>, locals=<optimized out>, \n    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x7fffc3e24608, defcount=3, \n    closure=0x0) at ../Python/ceval.c:3252\n#40 0x0000000000499a52 in fast_function (nk=0, na=2, n=<optimized out>, pp_stack=0x7fffffffe520, \n    func=<function at remote 0x7fffc3e28b18>) at ../Python/ceval.c:4116\n#41 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe520) at ../Python/ceval.c:4041\n#42 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x7ffe3d280de0, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 125, in __start_session (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_t---Type <return> to continue, or q <return> to quit---\nype_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#43 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe670, \n    func=<function at remote 0x7fffb4449b18>) at ../Python/ceval.c:4106\n#44 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe670) at ../Python/ceval.c:4041\n#45 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x7fffb3511050, for file /v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py, line 62, in form_model_graph (self=<NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__'...(truncated), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#46 0x0000000000499ef2 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffe7c0, \n    func=<function at remote 0x7fffb4449938>) at ../Python/ceval.c:4106\n#47 call_function (oparg=<optimized out>, pp_stack=0x7fffffffe7c0) at ../Python/ceval.c:4041\n#48 PyEval_EvalFrameEx (f=f@entry=Frame 0x7ffff7ebf7b0, for file train_script_lstm_attn.py, line 11, in <module> (), \n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\n#49 0x00000000004a1634 in PyEval_EvalCodeEx (closure=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, \nPython Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: \n    locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:3252\nPython Exception <class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte: \n#50 PyEval_EvalCode (locals=<unknown at remote 0x7ffff7ebf928>, globals=, co=0x7ffff7ec9130) at ../Python/ceval.c:667\n#51 run_mod.42576 (mod=mod@entry=0x9c1f30, filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", \n    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), \n---Type <return> to continue, or q <return> to quit---\n    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), flags=flags@entry=0x7fffffffe970, \n    arena=arena@entry=0x9aa9c0) at ../Python/pythonrun.c:1370\n#52 0x000000000044e4a5 in PyRun_FileExFlags (fp=fp@entry=0x976cd0, \n    filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", start=start@entry=257, \n    globals=globals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), \n    locals=locals@entry={'review_summary_file': 'extracted_data/review_summary.csv', 'gru': <module at remote 0x7fffb442f9b8>, 'out_file': 'result/test_results_lstm_with_attention.csv', '__builtins__': <module at remote 0x7ffff7f90b08>, '__file__': 'train_script_lstm_attn.py', 'lstm_net': <NeuralNet(dec_memory=<Tensor(_consumers=[<Operation(_graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={'variables': [<Variable(_variable=<Tensor(_consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_consumers=[], _value_index=0, _shape=<TensorShape(_dims=[<Dimension(_value=130088) at remote 0x7ffea024e310>, <Dimension(_value=200) at remote 0x7ffea024e190>]) at remote 0x7ffea024e290>, _op=<...>, _dtype=<DType(_type_enum=101) at remote 0x7fffc463a810>) at remote 0x7ffea024e3d0>], _control_flow_context=None, _id_value=1022, _original_op=None, _traceback=[('train_script_lstm_attn.py', 11, '<module>', {...}), ('/v/filer4b/v20q001/harshal/NLP/Final_Project/deep-summarization/algorithms/lstm.py', 60, 'form_mo...(truncated), closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)\n    at ../Python/pythonrun.c:1356\n#53 0x000000000044ec9f in PyRun_SimpleFileExFlags (fp=fp@entry=0x976cd0, filename=<optimized out>, \n    filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffe970)\n    at ../Python/pythonrun.c:948\n#54 0x000000000044ed9b in PyRun_AnyFileExFlags (fp=fp@entry=0x976cd0, \n---Type <return> to continue, or q <return> to quit---\n    filename=filename@entry=0x7fffffffed54 \"train_script_lstm_attn.py\", closeit=closeit@entry=1, \n    flags=flags@entry=0x7fffffffe970) at ../Python/pythonrun.c:752\n#55 0x000000000044f904 in Py_Main (argc=<optimized out>, argv=0x7fffffffeb28) at ../Modules/main.c:640\n#56 0x00007ffff7818ec5 in __libc_start_main (main=0x44f9c2 <main>, argc=2, argv=0x7fffffffeb28, init=<optimized out>, \n    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffeb18) at libc-start.c:287\n#57 0x0000000000578c4e in _start ()\n(gdb) \n\n```\n", "comments": ["did you look into this @lukaszkaiser @poxvoculi \n", "Sorry, I couldn't replicate it yet (didn't try on exactly the same settings though). Could you try with batch_size=2 and/or memory_dim=2? Does it happen as well, or is it specific to both being 1?\n", "yes it happens with that as well. Did it work for you? If yes, can you share your script that got it to work? I am not using separate decoder and encoder model. Just the wrapper code of run_seq2seq for both simple embedding and attention embedding.\n", "Hmm, it generally works, e.g. the translation model from the seq2seq tutorial. Can you share your full code so I can reproduce the problem?\n", "here is the repository. It is a work in progress.\nhttps://github.com/harpribot/deep-summarization\n", "The models are stored in models directory. To run the code, you will have to run \n\npython train_script_gru_simple_attn.py\n\nThis will trigger the code for attention mechanism on a RNN with simple GRU cell. \n\nThe code of interest lies in the models directory. I apologize it has not been documented, as it is still a work in progress. You might want to reduce the memory and batch size from train_script_\\* to make it runnable on a GPU. (It takes north of 8 gb of memory)\n", "Hmm, this repo is quite a lot of indirection to go through. Could you try to minimize the bug example in some way, provide, e.g., a single file?\n\nOne thing to note: there is a known outstanding bug that causes segfaults like the one you reported when scipy is imported after tensorflow. I think someone is working on that, could it be that some of your imports is causing the same problem?\n", "I can, but I have a busy week as its end of semester. I am sorry that I did not write a single script for scalability and reusability reason. \n\nI will try and write one big script after my finals to see if I can reproduce this. Tensor flow is the first thing I import in all my classes. I guess I can try importing Tensorflow at the end and see if it helps. \n\nBut I will need some time on this. I will get back to you in a week or so.\n", "Hi harpibot, any update on this? Let me know if you still see the problem!\n", "Automatically closing due to lack of recent activity. Please reopen if it is still an issue.\n", "Hi all, I have exactly the same issue: embedding_rnn_seq2seq works fine but embedding_attention_seq2seq gives me segfault when running tensorflow.initialize_all_variables() .\n\nI have enough ram and this segfault happens upon changing batch size, embedding dim or memory dim - for both GRU and LSTM cells.\n\nThe core dump tells me \n\"\"\"\nSegfault happened at:07ff864e935b <__memmove_avx_unaligned+427>: mov %esi,(%rdi)\nPC (0x07ff864e935b) ok\nsource \"%esi\" ok\ndestination \"(%rdi)\" (0x00000000) not located in a known VMA region (need writable region)!\n\"\"\"\n\nIt seems like tensorflow is using an uninitialized pointer, but it's not easy to debug where.\n\nI am running tensorflow on i7-6700K and 980-Ti. \n\nHas there been any progress on this? How can I help you solve this issue?\n", "Can you make a small example where this happens regularly? That'd be very helpful, since for now I cannot reproduce it. Are you using TF 0.10rc0?\n", "Thank you for your quick reply. I was using TF 0.9.0 when I posted the first comment. I upgraded to 0.10 and with the new version the error message is more clear.\n\nThe issue is an out of memory problem: with embedding_attention_seq2seq an additional tensor is created with size [num_symbols x  (num_symbols+memory_dim)]. Since my vocabolary size is ~100000 I run out of memory. \n\nMy guess is that this issue should be closed because it works as designed. \n\nI could ask you why the additional tensor is not shaped as [embedding_dim x (embedding_dim + memory_dim)], but this would be out of the current problem :smiley: \n"]}, {"number": 2137, "title": "Issue 2066: Fix the conv for stride > ksize case.", "body": "Issue 2066: Fix the conv for stride > ksize case. Passed all the tests and locally verified it fixed the problem. \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "I think this is a known flaky test.\n\n@zheng-xq: shouldn't we add a test to conv_ops_test.py for this?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2136, "title": "internal to github 2016/4/27", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Looks like this is ready to merge?\n"]}, {"number": 2135, "title": "Diagnostic / assessment tools for network", "body": "It would be convenient to have some tools out of the box for assessing networks complexity, computational cost, resources use etc without need for the user to do it 'by hand'. Do you think the following would be possible to implement in a general way?\n- getting a command to know how many parameters will be optimized by a tf.train node, and how many floating operations are required\n- more generally, getting a command to know how many operations are required to execute a node with sess.run\n- getting a set of commands to evaluate memory, cuda cores and bandwith use (both absolute value and proportion of the device) by a node on gpu, and cpu use. A little bit as mentioned here: http://stackoverflow.com/questions/34629613/how-to-profile-tensorflow-networks\n", "comments": ["Yes, it would be very useful to have better tools to analyze and predict TensorFlow program costs, in terms of time, memory and other resource needs.\n\nTensorFlow programs can be analyzed both statically and dynamically.  Your questions seem to me to suggest that maybe the properties in which you're interested can be discovered through static analysis.  \n\nBy and large, through static analysis one can determine the Tensor shapes and hence sizes of Variables.  Whether or not a given Variable functions as a model parameter is another question.    \n\nThe maximum memory required to execute a program is harder to get through static analysis because it depends on the extent to which memory intensive operations not otherwise constrained may overlap in time.  This may be non-deterministic, or determined by the runtime by rules that go beyond the denotational semantics of TensorFlow.  For example, there is the possibility of leaving values in memory for a long time, or recomputing them on demand, when the cost of doing so is less than the cost of leaving the memory occupied.  A version of this strategy is swapping values between CPU and GPU memory to free GPU memory for a period.\n\nDetermining the number of operations required to execute a node is also difficult for static analysis, so long as the Op implementing the node is opaque to the analysis, i.e. if you're not able to examine the actual machine instructions generated by the compiler.  In this case it seems more practical to dynamically measure the time or cycles consumed by an Op and use that to estimate its cost.\n\nBetter runtime profiling tools would be a welcome addition.\n"]}, {"number": 2134, "title": "tf.image.extract_glimpse does not work as expected", "body": "tf.image.extract_glimpse function does not work as expected. In some cases, the extracted glimpses are the same regardless of offset parameters. Please see the below code to reproduce.\n### Environment info\n\nOperating System: Ubuntu 14.04 \n\nInstalled version of CUDA and cuDNN: CUDA 7.5, cuDNN R4\n\nIf installed from sources, provide the commit hash: cf1659d\n### Steps to reproduce\n\nTry to run the code\n\n```\nimport tensorflow as tf\nimport numpy as np\n\ninput_img = np.arange(25).reshape((1,5,5,1))\n\nfirst_glimpse = tf.image.extract_glimpse(input_img, [3,3], [[1,1]],\n                                    centered=False, normalized=False)\nsecond_glimpse = tf.image.extract_glimpse(input_img, [3,3], [[2,1]],\n                                    centered=False, normalized=False)\n\nsess = tf.Session()\n\nprint first_glimpse.eval(session=sess)[0,:,:,0]\nprint second_glimpse.eval(session=sess)[0,:,:,0]\n```\n\nResults are\n\n```\nfirst_glimpse = [[  0.   1.   2.]\n                 [  5.   6.   7.]\n                 [ 10.  11.  12.]]\nsecond_glimpse = [[  0.   1.   2.]\n                  [  5.   6.   7.]\n                  [ 10.  11.  12.]]\n```\n\nIs this a bug? Or am I missing something?\n", "comments": ["One more observation: \ntf.image.extract_glimpse generates some noises (based on argument uniform_noise=True or False) on pixels outside of glimpse window. To my knowledge these noises should be random values, however it is not changed over calls. \n", "@benoitsteiner: Eigen's ExtractGlimpse code does seem broken: it builds its own internal RNG without any seed input from outside. \n", "@benoitsteiner Friendly ping to update this bug when you have a chance, thanks!\n", "@benoitsteiner Another friendly request to look at this. To be clear, the bug we are talking about is that the offsets of `extract_glimpse` are not working as expected.\r\n\r\nI reproduced the bug using this [slightly larger example](https://gist.github.com/CNugteren/5313ebbd497ddba9016cc2ea09745894) and I got [this output](https://gist.github.com/CNugteren/4d4a236dc1e98784fca8301c098daa51). I cannot make sense of it, and it becomes less clear when trying other input sizes (e.g. 4x4). Also I believe that the bug is also present when the indices are floating point and `normalized=True`.\r\n\r\nNote that there is also an issue open on [StackOverflow](http://stackoverflow.com/questions/36039429/tensorflow-extract-glimpse-offset).", "There is a work-around in case `centered=True` and `normalized=False`. In that case the offsets range from minus the size to plus the size of the tensor. I therefore wrote a wrapper that is more intuitive to numpy users, using pixel coordinates starting at (0,0):\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef extract_glimpse_numpy_like(input, glimpse_size, glimpse_offsets, name=None, uniform_noise=None):\r\n    \"\"\"\r\n    Works like in numpy with pixel coordinates starting at (0, 0), returns:\r\n       input[:, glimpse_offset[0] : glimpse_offset[0] + glimpse_size[0],\r\n                glimpse_offset[1] : glimpse_offset[1] + glimpse_size[1], :]\r\n    \"\"\"\r\n    assert(len(glimpse_size) == 2)\r\n    assert(len(glimpse_offsets) == 2)\r\n    input_size = tuple(input.get_shape().as_list())  # includes batch and number of channels\r\n    corrected_offsets = [2 * glimpse_offsets[0] - input_size[1] + glimpse_size[0],\r\n                         2 * glimpse_offsets[1] - input_size[2] + glimpse_size[1]]\r\n    corrected_offsets = np.tile(corrected_offsets, input_size[0])  # repeat it for each batch\r\n    corrected_offsets_t = tf.reshape(tf.constant(corrected_offsets, dtype=tf.float32), shape=(input_size[0], 2))\r\n    return tf.image.extract_glimpse(input, glimpse_size, corrected_offsets_t, centered=True, normalized=False,\r\n                                    uniform_noise=uniform_noise, name=name)\r\n```\r\nNote that this function assumes that the size of the tensor is known and that the offset is the same for each batch.\r\n\r\nThe original example would then return two subsequent glimpses when changed to use the above function:\r\n```\r\ninput_img = np.arange(25).reshape((1,5,5,1))\r\ninput_tensor = tf.constant(input_img, dtype=tf.float32)\r\nfirst_glimpse = extract_glimpse_numpy_like(input_tensor, [3,3], [0,0])\r\nsecond_glimpse = extract_glimpse_numpy_like(input_tensor, [3,3], [1,0])\r\n\r\nsess = tf.Session()\r\nprint first_glimpse.eval(session=sess)[0,:,:,0]\r\nprint second_glimpse.eval(session=sess)[0,:,:,0]\r\n```", "This demonstrates the problem for me with `normalized=False` and`centered=False` (notice the off by 1):\r\n```python\r\narr = np.zeros((1,11,11, 1), dtype=np.float32)\r\narr[0, 5,5] = 1\r\narr = tf.constant(arr)\r\nglim = tf.image.extract_glimpse(arr, (1,1), offsets=[(6,6)], normalized=False, centered=False)\r\nsess.run(glim)\r\n```\r\n```\r\n> array([[[[ 1.]]]], dtype=float32)\r\n```", "Looks like @benoitsteiner won't get to this himself.  Let me know if anyone wants to tackle it.  I'm happy to answer questions about how to mirror the seed semantics of the rest of the random ops.", "Perhaps this is another issue, or I am misunderstanding this, but it seems like there is an off by one issues as well?", "Does this issue is always unsolved ? ", "Is this bug fixed yet? Also, is there a way to have pad with ZEROS instead of random noise??", "I think the issue about offset is located in:\r\nhttps://github.com/tensorflow/tensorflow/blob/ca3bc0f1c2f917cf6e7c49d58f5ec604a9af9367/tensorflow/core/kernels/eigen_attention.h#L84-L98\r\n\r\nCreated a PR #12829 to fix it with the following changes:\r\n```\r\n      if (normalized_) {\r\n        // Un-normalize coordinates back to pixel space if normalized.\r\n        x *= input_width;\r\n        y *= input_height;\r\n        if (centered_) {\r\n          // Un-center if coordinates are centered on the image center.\r\n          x /= 2.0f;\r\n          y /= 2.0f;\r\n          x += input_width / 2.0f;\r\n          y += input_height / 2.0f;\r\n          // Remove half of the glimpse window.\r\n          x -= width_ / 2.0f;\r\n          y -= height_ / 2.0f;\r\n        }\r\n      } else {\r\n        if (centered_) {\r\n          x += input_width / 2.0f;\r\n          y += input_height / 2.0f;\r\n        }\r\n      }\r\n```", "@AroMorin Currently the  `tf.image.extract_glimpse` allows you to specify a boolean (`True/False`) to indicate if uniform or gaussian noise is to be used. We could not extend the boolean to indicate the zero background.\r\n\r\nIt might be possible to add an additional flag (e.g, `noise_type: string` etc) so that other noise types could be specified. But that will require API changes.\r\n\r\nI can work on adding zero padding if tensorflow team is OK with api changes for `tf.image.extract_glimpse`.", "It is possible to use extract_glimpse with zero padding by playing around with resize_image_with_crop_or_pad first. \r\n\r\nIf the img_batch variable is a tensor of shape [batch_size,img_width,img_height,channels] you could do first something like this:\r\n\r\n```python\r\n # unstack images in batch\r\nimg_batch_unstacked = tf.unstack(img_batch , axis= 0)\r\n\r\n # stack images on channels\r\nconcat_on_channel_batch = tf.concat(img_batch_unstacked, axis= 2)\r\n\r\n # pad the image with max glimpse width/height\r\nresized_img_batch = tf.image.resize_image_with_crop_or_pad(\r\n    image= concat_on_channel_batch,\r\n    target_width= conf.get_padded_img_width(), \r\n    target_height= conf.get_padded_img_height() \r\n)\r\n\r\n # undo the opeartions to get the original batch\r\n # first split images on channels \r\nsplited_on_channel_batch = tf.split( \r\n    resized_img_batch, \r\n    num_or_size_splits=conf.batch_size, \r\n    axis=2\r\n)\r\n\r\n # combine the images back to the original shape\r\nimg_batch_padded = tf.stack(\r\n    splited_on_channel_batch,\r\n    axis=0\r\n)\r\n```\r\n\r\nNow you need to modify the offsets variable so that it goes from -x to x where x is a new boundary depending on the original image dimensions and the padding dimensions. You can calculate it like: \r\n\r\n```python\r\noffset = offset * original_image_dimension / padded_image_dimension\r\n```\r\n\r\nFinally for the extract_glimpse use : \r\n\r\n```python\r\nglimpses = tf.image.extract_glimpse(\r\n    input=img_batch_padded,\r\n    size = glimpse_dim,\r\n    offsets=offset,\r\n    centered=True,\r\n    normalized=True\r\n)\r\n```\r\n\r\nI'm not sure if this is the best solution but it keeps all calculations in Tensorflow what was my goal. Hope this helps until the fix for zero padding get implemented. ", "If I extract multiple glimpses from an IMG how can I make sure they are non overlapping? thanks\r\n"]}, {"number": 2133, "title": "Both logs of single- and multi-GPU training are same in the CNN tutorial", "body": "This is not about tensorflow source code, but the tutorial page for convolutional neural networks.\nIn the [tutorial](https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html#training-a-model-using-multiple-gpu-cards), the train logs of both single gpu and multiple gpus are exactly same.\nI think it should be changed.\n", "comments": []}, {"number": 2132, "title": "Fix sec_per_batch in a multi-GPU CIFAR-10 example", "body": "In a multi-GPU CIFAR-10 example, 'sec_per_batch' should be computed in the same way as a single GPU example. It's identical with the [inception-v3 example](https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py#L346-L351).\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "closing due to inactivity and lack of CLA\n"]}, {"number": 2131, "title": "control_dependencies of ExponentialMovingAverage in cifar10_multi_gpu_train.py", "body": "There are two ExponentialMovingAverage in cifar10_multi_gpu_train.py.  \n- For model parameters https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py#L220  \n  Should the `apply` operation of ExponentialMovingAverage be called after the update of parameters? \n\n``` python\nwith tf.control_dependencies([apply_gradient_op]):\n    train_op = variable_averages.apply(tf.trainable_variables())\n```\n- For loss\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py#L105-L106  \n  Are these two lines redundant? Loss value won't change within a single run.\n\nP.S. The document of ExponentialMovingAverage (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L172-L177) says\n\n> `````` python\n> maintain_averages_op = ema.apply([var0, var1])\n>   # Create an op that will update the moving averages after each training\n>   # step.  This is what we will use in place of the usual training op.\n> with tf.control_dependencies([opt_op]):\n>     training_op = tf.group(maintain_averages_op)```\n> ``````\n\nIf we want to update the moving averages after the training step, should we call `ema.apply` within the context of tf.control_dependencies([opt_op])?\n", "comments": ["1. By doing the following:\n   with tf.control_dependencies([apply_gradient_op]):\n     train_op = variable_averages.apply(tf.trainable_variables())\n\nThe parameters are updated before the variable_averages.\n1. This is multi-gpu run. We are trying to return a snapshot of total_loss in case it's being updated by the other thread.\n2. I don't fully understand your question. Are you asking whether the implementation you quoted is correct, or you are trying to do something else different, and wonder how you should do it?\n\nThanks,\nSherry \n", "Hi Sherry,\n\nSuppose there is a parameter `p` initialized by 0. During training time the value of `p` are  \n0 -> 1 -> 2 -> 3 -> 4  \nIs it possible that the result of `ExponentialMovingAverage` become the moving average of 0, 1, 1, 3, 4 when we didn't specify the dependencies between `variable_averages.apply` and `apply_gradient_op`? \n\n---\n\nI cannot figure out under what circumstances will `total_loss` be modified by the other thread within a single `sess.run`.\nWhy we need\n\n``` python\nwith tf.control_dependencies([loss_averages_op]):\n    total_loss = tf.identity(total_loss)\n```\n\nto ensure `loss_averages_op` be executed before using `total_loss`?\n", "This fits better as a question on StackOverflow, since it doesn't seem to be about a fixable issue in TensorFlow.\n"]}, {"number": 2130, "title": "coord.request_stop() doesn't stop the threads", "body": "### Environment info\n\nOperating System: Arch Linux\n\nInstalled version of CUDA and cuDNN: cuda 7.5 cuDNNv4\n\nIf installed from sources, provide the commit hash:\ncommit cf1659d1c233f8ddbee13fd298464d76e58bdccb\n### Steps to reproduce\n\n```\nimport tensorflow as tf\n\nqueue_size = 100\nwith tf.Graph().as_default():\n  sess = tf.Session()\n  queue = tf.FIFOQueue(capacity=queue_size, dtypes=tf.int32)\n  enqueue_placeholder = tf.placeholder(dtype=tf.int32)\n  enqueue_op = queue.enqueue(enqueue_placeholder)\n  dequeue_op = queue.dequeue()\n  for f in range(queue_size):\n    sess.run([enqueue_op], feed_dict={enqueue_placeholder: f})\n  queue.close()\n\n  dequeue_op = tf.reshape(dequeue_op, shape=[1])\n  queue_batch = tf.train.batch([dequeue_op], batch_size=1, num_threads=1, capacity=64)\n\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  for i in range(queue_size):\n    print(sess.run([queue_batch]))\n\n  coord.request_stop()\n  coord.join(threads, stop_grace_period_secs=5)\n  sess.close()\n```\n### What have you tried?\n\nhttps://www.tensorflow.org/versions/r0.8/how_tos/reading_data/index.html#creating-threads-to-prefetch-using-queuerunner-objects\n\nAccording to documentation coord.request_stop() should stop the threads but in this case\nwe get a deadlock in coord.join().\n### Logs or other output that would be helpful\n\n```\n...\n[array([[97]], dtype=int32)]\n[array([[98]], dtype=int32)]\n[array([[99]], dtype=int32)]\nTraceback (most recent call last):\n  File \"test_queue.py\", line 26, in <module>\n    sess.close()\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3213, in get_controller\n    yield default\n  File \"test_queue.py\", line 25, in <module>\n    coord.join(threads, stop_grace_period_secs=5)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py\", line 289, in join\n    \" \".join(stragglers))\nRuntimeError: ('Coordinator stopped with threads still running: %s', 'Thread-1')\n```\n\nIf I put sess.close() before coord.join() the threads are killed but there is still a weird warning which should not exist because I called queue.close() before.\n\n```\n...\nW tensorflow/core/kernels/queue_base.cc:300] _0_fifo_queue: Skipping cancelled dequeue attempt with queue not closed\n```\n", "comments": ["I can reproduce. Thanks for giving self-contained repro, this is super helpful. Looking\n", "Oh...you are calling \"queue.close()\", but that actually returns an op which needs to be run to do anything. You need to do `sess.run(q.close())`. Since your first queue is not closed, your \"batch\" queue is waiting forever for something to be added to the first queue.\n\nFurthermore, this wait is happening in C++ mutex, so `stop_grace_period_secs` is useless -- the queue runner thread checks for \"stop_requested\" between session `run` calls, but because dequeue op never returns, it's stuck inside `session.run` forever.\n\nHere's the simpler example I made for myself to reproduce this\n\n```\ndef create_session():\n  \"\"\"Resets local session, returns new InteractiveSession\"\"\"\n\n  config = tf.ConfigProto(log_device_placement=True)\n  config.gpu_options.per_process_gpu_memory_fraction=0.3 # don't hog all vRAM\n  config.operation_timeout_in_ms=5000   # terminate on long hangs\n  sess = tf.InteractiveSession(\"\", config=config)\n  return sess\n\ntf.reset_default_graph()\nq = tf.FIFOQueue(4, tf.string)\nenqueue_val = tf.placeholder(dtype=tf.string)\nenqueue_op = q.enqueue(enqueue_val)\nsize_op = q.size()\ndequeue_op = q.dequeue()\nsess = create_session()\ndef enqueueit(val):\n  sess.run([enqueue_op], feed_dict={enqueue_val:val})\n  print \"queue1 size: \", sess.run(size_op)\nenqueueit(\"1\")\nenqueueit(\"2\")\nenqueueit(\"3\")\n#sess.run(q.close())\n\ndequeue_op.set_shape([])\nqueue2 = tf.train.batch([dequeue_op], batch_size=1, num_threads=1, capacity=1)\nthreads = tf.train.start_queue_runners()\n\ndef dequeueit():\n  print \"queue1 size: \", sess.run(size_op)\n  print \"queue2 size before: \", sess.run(\"batch/fifo_queue_Size:0\")\n  print \"result: \", sess.run(queue2)\n  print \"queue2 size after: \", sess.run(\"batch/fifo_queue_Size:0\")\n\ndequeueit()\ndequeueit()\ndequeueit()\ncoord.request_stop()\ncoord.join(threads, stop_grace_period_secs=5)\n```\n", "So I guess the problem is that there's no way to stop a thread from Python if a thread is stuck inside of a `session.run` call. One work-around is to create session with `config.operation_timeout_in_ms` as above\n", "closing since it doesn't seem to be possible to fix this besides the work-around given\n", "Thanks a lot @yaroslavvb! I somehow missed the queue.close() bug.\n\nI fixed my training code and everything works now.\nI need to handle data manualy in order to perform validation after each epoch as I have only one Titan X :)\nThere was one more thing I had to fix in my training code.\nIn all official examples the summary operation is run like this:\n\n```\n      if step % 100 == 0:\n        summary_str = sess.run(summary_op)\n        summary_writer.add_summary(summary_str, step)\n```\n\nThis means that every 100th image is skipped in applying the training operation because ordinary summary_op includes the loss operation and subsequently the whole forward pass.\nI uderstand this is not an issue when using the shuffled version of tf.train.string_input_producer but  it becomes a problem when handing queues manually because summary_op will dequeue and you need to count it as a full iteration.\nI fixed it by simply running summary_op and train_op jointly:\n\n```\n      if step % 100 == 0:\n        _, loss_value, summary_str = sess.run([train_op, loss, summary_op])\n        summary_writer.add_summary(summary_str, global_step_val)\n      else:\n        _, loss_value = sess.run([train_op, loss])\n```\n\nI think it would be useful if Tensorflow documentation or source code contaned an example of how to train and validate (after each epoch) the model on single GPU. A lot of people have limiting resources and they can't reserve one GPU only for validation. If you want I can send you a simplified version of my implementation.\n", "You are saying we have documentation that tells people to train/validate on separate GPUs? Which one is it? Meanwhile, you could also post your implementation as a comment here, and I'll reference as it comes up in issues/stackoverflow questions, etc\n", "Documentation is great. It was a wrong impression on my side regarding code examples. There are examples on how to train and evaluate on single GPU like:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py\n\nMy mistake was that I was using an older version of Slim wrapper which didn't have support for variable reuse which lead me to completely forget about reuse mechanism. This was a problem because some layers behave differently in inference mode (like batch normalization) and there was no option to define two versions of the same network using different is_training flag in older Slim. In the end I wrote a code which saves, rebuilds and restores a graph from scratch each epoch which is still a good workaround as the time to do this is negligible compared to all the forward and backward passes. Now I found out that Slim supports variable reuse and I just tested it and it works.\n\nThanks for help!\n", "To add to, and summarize this discussion: to get a clean shutdown, this was I needed to run at shutdown. If you don't \"cancel_pending_enqueues\", the threads still hang in enqueue ops (tf 1.0).\r\n\r\n```\r\ncoord.request_stop()\r\nsess.run(model.queue.close(cancel_pending_enqueues=True))\r\ncoord.join(threads)\r\n```", "@davidparks21 I solve the problem \"RuntimeError: ('Coordinator stopped with threads still running: %s', 'Thread-1') \" using your method. But I met a new error: RuntimeError: Graph is finalized and cannot be modified.Can you help me ? Thanks"]}, {"number": 2129, "title": "core dump when import tensorflow ", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Centos 7\n\nInstalled version of CUDA and cuDNN: cuda 7.5 cuDNNv4\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\ntotal 987720\n-rw-r--r--. 1 root root  28585480 Aug 16  2015 libcublas_device.a\nlrwxrwxrwx. 1 root root        16 Sep  9  2015 libcublas.so -> libcublas.so.7.5\nlrwxrwxrwx. 1 root root        19 Sep  9  2015 libcublas.so.7.5 -> libcublas.so.7.5.18\n-rwxr-xr-x. 1 root root  23938736 Aug 16  2015 libcublas.so.7.5.18\n-rw-r--r--. 1 root root  28220076 Aug 16  2015 libcublas_static.a\n-rw-r--r--. 1 root root    322936 Aug 16  2015 libcudadevrt.a\nlrwxrwxrwx. 1 root root        16 Sep  9  2015 libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx. 1 root root        19 Sep  9  2015 libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x. 1 root root    383336 Aug 16  2015 libcudart.so.7.5.18\n-rw-r--r--. 1 root root    720192 Aug 16  2015 libcudart_static.a\n-rwxr-xr-x. 1 root root  61453024 Apr 27 15:55 libcudnn.so\n-rwxr-xr-x. 1 root root  61453024 Apr 27 15:55 libcudnn.so.4\n-rwxr-xr-x. 1 root root  61453024 Apr 27 15:55 libcudnn.so.4.0.7\n-rw-r--r--. 1 root root  62025862 Apr 27 15:55 libcudnn_static.a\nlrwxrwxrwx. 1 root root        15 Sep  9  2015 libcufft.so -> libcufft.so.7.5\nlrwxrwxrwx. 1 root root        18 Sep  9  2015 libcufft.so.7.5 -> libcufft.so.7.5.18\n-rwxr-xr-x. 1 root root 111231960 Aug 16  2015 libcufft.so.7.5.18\n-rw-r--r--. 1 root root 115104400 Aug 16  2015 libcufft_static.a\nlrwxrwxrwx. 1 root root        16 Sep  9  2015 libcufftw.so -> libcufftw.so.7.5\nlrwxrwxrwx. 1 root root        19 Sep  9  2015 libcufftw.so.7.5 -> libcufftw.so.7.5.18\n-rwxr-xr-x. 1 root root    447664 Aug 16  2015 libcufftw.so.7.5.18\n-rw-r--r--. 1 root root     42206 Aug 16  2015 libcufftw_static.a\nlrwxrwxrwx. 1 root root        17 Sep  9  2015 libcuinj64.so -> libcuinj64.so.7.5\nlrwxrwxrwx. 1 root root        20 Sep  9  2015 libcuinj64.so.7.5 -> libcuinj64.so.7.5.18\n-rwxr-xr-x. 1 root root   5751400 Aug 16  2015 libcuinj64.so.7.5.18\n-rw-r--r--. 1 root root   1649726 Aug 16  2015 libculibos.a\nlrwxrwxrwx. 1 root root        16 Sep  9  2015 libcurand.so -> libcurand.so.7.5\nlrwxrwxrwx. 1 root root        19 Sep  9  2015 libcurand.so.7.5 -> libcurand.so.7.5.18\n-rwxr-xr-x. 1 root root  51765952 Aug 16  2015 libcurand.so.7.5.18\n-rw-r--r--. 1 root root  51992564 Aug 16  2015 libcurand_static.a\nlrwxrwxrwx. 1 root root        18 Sep  9  2015 libcusolver.so -> libcusolver.so.7.5\nlrwxrwxrwx. 1 root root        21 Sep  9  2015 libcusolver.so.7.5 -> libcusolver.so.7.5.18\n-rwxr-xr-x. 1 root root  37034328 Aug 16  2015 libcusolver.so.7.5.18\n-rw-r--r--. 1 root root  16613348 Aug 16  2015 libcusolver_static.a\nlrwxrwxrwx. 1 root root        18 Sep  9  2015 libcusparse.so -> libcusparse.so.7.5\nlrwxrwxrwx. 1 root root        21 Sep  9  2015 libcusparse.so.7.5 -> libcusparse.so.7.5.18\n-rwxr-xr-x. 1 root root  36816424 Aug 16  2015 libcusparse.so.7.5.18\n-rw-r--r--. 1 root root  44445334 Aug 16  2015 libcusparse_static.a\nlrwxrwxrwx. 1 root root        14 Sep  9  2015 libnppc.so -> libnppc.so.7.5\nlrwxrwxrwx. 1 root root        17 Sep  9  2015 libnppc.so.7.5 -> libnppc.so.7.5.18\n-rwxr-xr-x. 1 root root    427168 Aug 16  2015 libnppc.so.7.5.18\n-rw-r--r--. 1 root root     20664 Aug 16  2015 libnppc_static.a\nlrwxrwxrwx. 1 root root        14 Sep  9  2015 libnppi.so -> libnppi.so.7.5\nlrwxrwxrwx. 1 root root        17 Sep  9  2015 libnppi.so.7.5 -> libnppi.so.7.5.18\n-rwxr-xr-x. 1 root root  63516808 Aug 16  2015 libnppi.so.7.5.18\n-rw-r--r--. 1 root root  90106200 Aug 16  2015 libnppi_static.a\nlrwxrwxrwx. 1 root root        14 Sep  9  2015 libnpps.so -> libnpps.so.7.5\nlrwxrwxrwx. 1 root root        17 Sep  9  2015 libnpps.so.7.5 -> libnpps.so.7.5.18\n-rwxr-xr-x. 1 root root   6047400 Aug 16  2015 libnpps.so.7.5.18\n-rw-r--r--. 1 root root   8647292 Aug 16  2015 libnpps_static.a\nlrwxrwxrwx. 1 root root        16 Sep  9  2015 libnvblas.so -> libnvblas.so.7.5\nlrwxrwxrwx. 1 root root        19 Sep  9  2015 libnvblas.so.7.5 -> libnvblas.so.7.5.18\n-rwxr-xr-x. 1 root root    456112 Aug 16  2015 libnvblas.so.7.5.18\nlrwxrwxrwx. 1 root root        24 Sep  9  2015 libnvrtc-builtins.so -> libnvrtc-builtins.so.7.5\nlrwxrwxrwx. 1 root root        27 Sep  9  2015 libnvrtc-builtins.so.7.5 -> libnvrtc-builtins.so.7.5.18\n-rwxr-xr-x. 1 root root  22408512 Aug 16  2015 libnvrtc-builtins.so.7.5.18\nlrwxrwxrwx. 1 root root        15 Sep  9  2015 libnvrtc.so -> libnvrtc.so.7.5\nlrwxrwxrwx. 1 root root        18 Sep  9  2015 libnvrtc.so.7.5 -> libnvrtc.so.7.5.17\n-rwxr-xr-x. 1 root root  18199288 Aug 16  2015 libnvrtc.so.7.5.17\nlrwxrwxrwx. 1 root root        18 Sep  9  2015 libnvToolsExt.so -> libnvToolsExt.so.1\nlrwxrwxrwx. 1 root root        22 Sep  9  2015 libnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\n-rwxr-xr-x. 1 root root     37936 Aug 16  2015 libnvToolsExt.so.1.0.0\n-rw-r--r--. 1 root root     25840 Aug 16  2015 libOpenCL.so\nlrwxrwxrwx. 1 root root        12 Sep  9  2015 libOpenCL.so.1 -> libOpenCL.so\ndrwxr-xr-x. 2 root root      4096 Sep  9  2015 stubs\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. write a python script test.py which only has one line: import tensorflow\n\n2, gdb python\n\n3,  run test.py\n\n4,  core dumped, use bt to get stacktrace\n### What have you tried?\n1. install from pip\n   2, install form source\n   3, try cudnn V4, V5\n### Logs or other output that would be helpful\n\nstacktrace from gdb\n\nrogram received signal SIGSEGV, Segmentation fault.\n0x00007fffd50e0220 in PyArray_API () from /usr/lib64/python2.7/site-packages/numpy/core/multiarray.so\n(gdb) bt\n#0  0x00007fffd50e0220 in PyArray_API () from /usr/lib64/python2.7/site-packages/numpy/core/multiarray.so\n#1  0x00007fffc513f3c4 in initspecfun () from /usr/lib64/python2.7/site-packages/scipy/special/specfun.so\n#2  0x00007ffff7b09eb9 in _PyImport_LoadDynamicModule () from /lib64/libpython2.7.so.1.0\n#3  0x00007ffff7b07f91 in import_submodule () from /lib64/libpython2.7.so.1.0\n#4  0x00007ffff7b0848f in ensure_fromlist () from /lib64/libpython2.7.so.1.0\n#5  0x00007ffff7b08cca in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0\n#6  0x00007ffff7aef3bf in builtin___import__ () from /lib64/libpython2.7.so.1.0\n#7  0x00007ffff7a5f073 in PyObject_Call () from /lib64/libpython2.7.so.1.0\n#8  0x00007ffff7af0fd7 in PyEval_CallObjectWithKeywords () from /lib64/libpython2.7.so.1.0\n#9  0x00007ffff7af2aa3 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#10 0x00007ffff7af718d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#11 0x00007ffff7af7292 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\n#12 0x00007ffff7b0707c in PyImport_ExecCodeModuleEx () from /lib64/libpython2.7.so.1.0\n#13 0x00007ffff7b072f8 in load_source_module () from /lib64/libpython2.7.so.1.0\n#14 0x00007ffff7b07f91 in import_submodule () from /lib64/libpython2.7.so.1.0\n#15 0x00007ffff7b081dd in load_next () from /lib64/libpython2.7.so.1.0\n#16 0x00007ffff7b08bbe in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0\n#17 0x00007ffff7aef3bf in builtin___import__ () from /lib64/libpython2.7.so.1.0\n#18 0x00007ffff7a5f073 in PyObject_Call () from /lib64/libpython2.7.so.1.0\n#19 0x00007ffff7af0fd7 in PyEval_CallObjectWithKeywords () from /lib64/libpython2.7.so.1.0\n#20 0x00007ffff7af2aa3 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#21 0x00007ffff7af718d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#22 0x00007ffff7af7292 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\n#23 0x00007ffff7b0707c in PyImport_ExecCodeModuleEx () from /lib64/libpython2.7.so.1.0\n#24 0x00007ffff7b072f8 in load_source_module () from /lib64/libpython2.7.so.1.0\n#25 0x00007ffff7b0878a in load_package () from /lib64/libpython2.7.so.1.0\n#26 0x00007ffff7b07f91 in import_submodule () from /lib64/libpython2.7.so.1.0\n#27 0x00007ffff7b081dd in load_next () from /lib64/libpython2.7.so.1.0\n#28 0x00007ffff7b08bf8 in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0\n#29 0x00007ffff7aef3bf in builtin___import__ () from /lib64/libpython2.7.so.1.0\n#30 0x00007ffff7a5f073 in PyObject_Call () from /lib64/libpython2.7.so.1.0\n#31 0x00007ffff7af0fd7 in PyEval_CallObjectWithKeywords () from /lib64/libpython2.7.so.1.0\n#32 0x00007ffff7af2aa3 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#33 0x00007ffff7af718d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#34 0x00007ffff7af7292 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\n#35 0x00007ffff7b0707c in PyImport_ExecCodeModuleEx () from /lib64/libpython2.7.so.1.0\n#36 0x00007ffff7b072f8 in load_source_module () from /lib64/libpython2.7.so.1.0\n#37 0x00007ffff7b07f91 in import_submodule () from /lib64/libpython2.7.so.1.0\n#38 0x00007ffff7b081dd in load_next () from /lib64/libpython2.7.so.1.0\n#39 0x00007ffff7b08bf8 in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0\n", "comments": ["Your stack trace shows a failure in numpy, yet it sounds like your source program is just the one line.  What if you import numpy first, ie.  place\nimport numpy\nabove \nimport tensorflow\n?\n", "possibly related: https://github.com/tensorflow/tensorflow/issues/2034\n", "I seem to be having this problem too, or maybe the one in #2034. Tensorflow runs just fine, as long as numpy has already been imported.\n\nI'm using cuda 7.5, cudnn 5.0, numpy 1.11.0, python 2.7.11, and tensorflow HEAD (c38157c7cccf4c06e4d0d3f1f72ed490798dcfe5). Everything was compiled by me.\n\nTest case\n\n```\n$ python\nPython 2.7.11 (default, Apr 25 2016, 20:02:17)\n[GCC 4.9.1] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.5.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.5 locally\nzsh: segmentation fault (core dumped)  python\n```\n\nBut it doesn't crash as long as numpy is imported first:\n\n```\n$ python\nPython 2.7.11 (default, Apr 25 2016, 20:02:17)\n[GCC 4.9.1] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import numpy\n>>> import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.5.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.5 locally\n>>>\n```\n\nGDB stacktrace of the segfault.\n\n```\n$ gdb python\nGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)\nCopyright (C) 2010 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-redhat-linux-gnu\".\nFor bug reporting instructions, please see:\n<http://www.gnu.org/software/gdb/bugs/>...\nReading symbols from /work/01813/roller/maverick/packages/python/bin/python...done.\n(gdb) r\nStarting program: /work/01813/roller/maverick/packages/python/bin/python\n\n[Thread debugging using libthread_db enabled]\nPython 2.7.11 (default, Apr 25 2016, 20:02:17)\n[GCC 4.9.1] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\n>>> import tensorflow\nwarning: File \"/opt/apps/gcc/4.9.1/lib64/libstdc++.so.6.0.20-gdb.py\" auto-loading has been declined by your `auto-load safe-path' set to \"/usr/share/gdb/auto-load:/usr/lib/debug:/usr/bin/mono-gdb.py\".\nTo enable execution of this file add\n        add-auto-load-safe-path /opt/apps/gcc/4.9.1/lib64/libstdc++.so.6.0.20-gdb.py\nline to your configuration file \"/home/01813/roller/.gdbinit\".\nTo completely disable this security protection add\n        set auto-load safe-path /\nline to your configuration file \"/home/01813/roller/.gdbinit\".\nFor more information about this security protection see the\n\"Auto-loading safe path\" section in the GDB manual.  E.g., run from the shell:\n        info \"(gdb)Auto-loading safe path\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.5 locally\nMissing separate debuginfo for /usr/lib64/libcuda.so.1\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.5.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.5 locally\nDetaching after fork from child process 10971.\nMissing separate debuginfo for /work/01813/roller/maverick/packages/python/lib/python2.7/site-packages/scipy/special/../.libs/libgfortran-ed201abd.so.3.0.0\n\nProgram received signal SIGSEGV, Segmentation fault.\n0x00002aaac9d91a40 in PyArray_API () from /work/01813/roller/maverick/packages/python/lib/python2.7/site-packages/numpy-1.11.0-py2.7-linux-x86_64.egg/numpy/core/multiarray.so\nMissing separate debuginfos, use: debuginfo-install glibc-2.12-1.166.el6_7.1.x86_64 keyutils-libs-1.4-5.el6.x86_64 krb5-libs-1.10.3-42.el6.x86_64 libcom_err-1.41.12-22.el6.x86_64 libselinux-2.0.94-5.8.el6.x86_64 libuuid-2.17.2-12.18.el6.x86_64 ncurses-libs-5.7-4.20090207.el6.x86_64 openssl-1.0.1e-42.el6_7.4.x86_64 readline-6.0-4.el6.x86_64 zlib-1.2.3-29.el6.x86_64\n```\n\nEdit: I tried following the advice of #1373 and reinstalling everything, but it didn't help.\n", "@poxvoculi , the program has only one line \"import tensorflow\"\n", "@stephenroller , mine is different, I got a core dump even import tensorflow, have you solved this issue?\n", "it seems work now if I  \"import numpy\" before \"import tensorflow\".\naccording to #2034\n", "Yes! Incredibly import numpy before tensorflow fixes this issue. What's weird is this issue started happening today, and I've run this same code 50+ times before this started happening.\n", "I had the same segfault and even I tried import numpy before import tensorflow, it did not work. My environment is ubuntu 16.04, python 3.5, and TF 0.9 in a docker container. Anyone knows?\n", "Same issue, even I import numpy before tensorflow.\r\ntensorflow 1.2\r\nIssue happening after VM copy on other machine...\r\n\r\nI suspect this related to the source compiled version on machine 1 (some Intel CPU).\r\nWhen copying on different hardware the source compiled is not working anymore.....\r\n\r\n", "Hmm... another interesting thing I met is that I need to import numpy before tensorflow and tensorflow before pytorch\r\n\r\nIf not, I got `segmentation fault (core dumped)`", "tensorflow 1.6\r\n\r\n```\r\n(gdb) run test.py\r\nStarting program: /usr/bin/python test.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff3d42700 (LWP 4263)]\r\n\r\nThread 1 \"python\" received signal SIGILL, Illegal instruction.\r\n0x00007fffea257880 in std::pair<std::__detail::_Node_iterator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, false, true>, bool> std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, std::allocator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> > >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> > >(std::integral_constant<bool, true>, std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> >&&) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n```\r\n\r\n```\r\n(gdb) bt\r\n#0  0x00007fffea257880 in std::pair<std::__detail::_Node_iterator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, false, true>, bool> std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, std::allocator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> > >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> > >(std::integral_constant<bool, true>, std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> >&&) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#1  0x00007fffea259735 in tensorflow::UnaryVariantOpRegistry::RegisterDecodeFn(std::string const&, std::function<bool (tensorflow::Variant*)> const&) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#2  0x00007fffea234a7c in tensorflow::variant_op_registry_fn_registration::UnaryVariantDecodeRegistration<tensorflow::Tensor>::UnaryVariantDecodeRegistration(std::string const&) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fffea1ae165 in _GLOBAL__sub_I_tensor.cc () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007ffff7de76ba in call_init (l=<optimized out>, argc=argc@entry=2, argv=argv@entry=0x7fffffffdbc8, env=env@entry=0xa99dc0) at dl-init.c:72\r\n#5  0x00007ffff7de77cb in call_init (env=0xa99dc0, argv=0x7fffffffdbc8, argc=2, l=<optimized out>) at dl-init.c:30\r\n#6  _dl_init (main_map=main_map@entry=0xedcf10, argc=2, argv=0x7fffffffdbc8, env=0xa99dc0) at dl-init.c:120\r\n#7  0x00007ffff7dec8e2 in dl_open_worker (a=a@entry=0x7fffffffbbc0) at dl-open.c:575\r\n#8  0x00007ffff7de7564 in _dl_catch_error (objname=objname@entry=0x7fffffffbbb0, errstring=errstring@entry=0x7fffffffbbb8, mallocedp=mallocedp@entry=0x7fffffffbbaf, \r\n    operate=operate@entry=0x7ffff7dec4d0 <dl_open_worker>, args=args@entry=0x7fffffffbbc0) at dl-error.c:187\r\n#9  0x00007ffff7debda9 in _dl_open (file=0x7fffef22ba54 \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\", mode=-2147483646, \r\n    caller_dlopen=0x51ad19 <_PyImport_GetDynLoadFunc+233>, nsid=-2, argc=<optimized out>, argv=<optimized out>, env=0xa99dc0) at dl-open.c:660\r\n#10 0x00007ffff75ecf09 in dlopen_doit (a=a@entry=0x7fffffffbdf0) at dlopen.c:66\r\n#11 0x00007ffff7de7564 in _dl_catch_error (objname=0xa5ee40, errstring=0xa5ee48, mallocedp=0xa5ee38, operate=0x7ffff75eceb0 <dlopen_doit>, args=0x7fffffffbdf0) at dl-error.c:187\r\n#12 0x00007ffff75ed571 in _dlerror_run (operate=operate@entry=0x7ffff75eceb0 <dlopen_doit>, args=args@entry=0x7fffffffbdf0) at dlerror.c:163\r\n#13 0x00007ffff75ecfa1 in __dlopen (file=<optimized out>, mode=<optimized out>) at dlopen.c:87\r\n#14 0x000000000051ad19 in _PyImport_GetDynLoadFunc ()\r\n#15 0x000000000051a8e4 in _PyImport_LoadDynamicModule ()\r\n#16 0x00000000005b7b1b in ?? ()\r\n#17 0x00000000004bc3fa in PyEval_EvalFrameEx ()\r\n#18 0x00000000004c136f in PyEval_EvalFrameEx ()\r\n#19 0x00000000004b9ab6 in PyEval_EvalCodeEx ()\r\n#20 0x00000000004b97a6 in PyEval_EvalCode ()\r\n#21 0x00000000004b96df in PyImport_ExecCodeModuleEx ()\r\n#22 0x00000000004b2b06 in ?? ()\r\n#23 0x00000000004a4ae1 in ?? ()\r\n#24 0x00000000004a4513 in PyImport_ImportModuleLevel ()\r\n#25 0x00000000004a59e4 in ?? ()\r\n#26 0x00000000004a577e in PyObject_Call ()\r\n#27 0x00000000004c5e10 in PyEval_CallObjectWithKeywords ()\r\n#28 0x00000000004be6d7 in PyEval_EvalFrameEx ()\r\n#29 0x00000000004b9ab6 in PyEval_EvalCodeEx ()\r\n#30 0x00000000004b97a6 in PyEval_EvalCode ()\r\n#31 0x00000000004b96df in PyImport_ExecCodeModuleEx ()\r\n#32 0x00000000004b2b06 in ?? ()\r\n#33 0x00000000004a4ae1 in ?? ()\r\n#34 0x00000000004a7fd5 in ?? ()\r\n#35 0x00000000004a3f38 in PyImport_ImportModuleLevel ()\r\n#36 0x00000000004a59e4 in ?? ()\r\n#37 0x00000000004a577e in PyObject_Call ()\r\n#38 0x00000000004c5e10 in PyEval_CallObjectWithKeywords ()\r\n#39 0x00000000004be6d7 in PyEval_EvalFrameEx ()\r\n#40 0x00000000004b9ab6 in PyEval_EvalCodeEx ()\r\n#41 0x00000000004b97a6 in PyEval_EvalCode ()\r\n#42 0x00000000004b96df in PyImport_ExecCodeModuleEx ()\r\n#43 0x00000000004b2b06 in ?? ()\r\n#44 0x00000000004b402c in ?? ()\r\n#45 0x00000000004a4ae1 in ?? ()\r\n#46 0x00000000004a4513 in PyImport_ImportModuleLevel ()\r\n#47 0x00000000004a59e4 in ?? ()\r\n#48 0x00000000004a577e in PyObject_Call ()\r\n#49 0x00000000004c5e10 in PyEval_CallObjectWithKeywords ()\r\n#50 0x00000000004be6d7 in PyEval_EvalFrameEx ()\r\n---Type <return> to continue, or q <return> to quit---\r\n#51 0x00000000004b9ab6 in PyEval_EvalCodeEx ()\r\n#52 0x00000000004b97a6 in PyEval_EvalCode ()\r\n#53 0x00000000004b96df in PyImport_ExecCodeModuleEx ()\r\n#54 0x00000000004b2b06 in ?? ()\r\n#55 0x00000000004b402c in ?? ()\r\n#56 0x00000000004a4ae1 in ?? ()\r\n#57 0x00000000004a3e84 in PyImport_ImportModuleLevel ()\r\n#58 0x00000000004a59e4 in ?? ()\r\n#59 0x00000000004a577e in PyObject_Call ()\r\n#60 0x00000000004c5e10 in PyEval_CallObjectWithKeywords ()\r\n#61 0x00000000004be6d7 in PyEval_EvalFrameEx ()\r\n#62 0x00000000004b9ab6 in PyEval_EvalCodeEx ()\r\n#63 0x00000000004eb30f in ?? ()\r\n#64 0x00000000004e5422 in PyRun_FileExFlags ()\r\n#65 0x00000000004e3cd6 in PyRun_SimpleFileExFlags ()\r\n#66 0x0000000000493ae2 in Py_Main ()\r\n#67 0x00007ffff7810830 in __libc_start_main (main=0x4934c0 <main>, argc=2, argv=0x7fffffffdbc8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffdbb8)\r\n    at ../csu/libc-start.c:291\r\n#68 0x00000000004933e9 in _start ()\r\n```"]}, {"number": 2128, "title": "TensorFlow does not import in ipython when using virtualenv", "body": "I have installed tensorflow and ipython inside of the virtualenv.\nWhen I run ipython and try to import tensorflow by `import tensorflow as tf` I got following stack trace:\n\n```\nTypeError                                 Traceback (most recent call last)\n<ipython-input-1-41389fad42b5> in <module>()\n----> 1 import tensorflow as tf\n\n/Users/janzikes/.virtualenvs/research/lib/python2.7/site-packages/tensorflow/__init__.py in <module>()\n     21 from __future__ import print_function\n     22\n---> 23 from tensorflow.python import *\n\n/Users/janzikes/.virtualenvs/research/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>()\n     47\n     48 try:\n---> 49   from tensorflow.core.framework.graph_pb2 import *\n     50 except ImportError:\n     51   msg = \"\"\"%s\\n\\nError importing tensorflow.  Unless you are using bazel,\n\n/Users/janzikes/.virtualenvs/research/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py in <module>()\n     14\n     15\n---> 16 from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n     17 from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2\n     18 from tensorflow.core.framework import versions_pb2 as tensorflow_dot_core_dot_framework_dot_versions__pb2\n\n/Users/janzikes/.virtualenvs/research/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py in <module>()\n     14\n     15\n---> 16 from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n     17 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n     18 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n\n/Users/janzikes/.virtualenvs/research/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py in <module>()\n     14\n     15\n---> 16 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n     17 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n     18\n\n/Users/janzikes/.virtualenvs/research/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py in <module>()\n     20   package='tensorflow',\n     21   syntax='proto3',\n---> 22   serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB/\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\x62\\x06proto3')\n     23 )\n     24 _sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTypeError: __init__() got an unexpected keyword argument 'syntax'\n```\n\nI was trying to search for previous answers, but only thing that I have found was this [stackoverflow question](http://stackoverflow.com/questions/34270632/tensorflow-and-ipython-on-virtualenv) without any answer.\n\n**Note:**\nWhen I run just python and then run `import tensorflow as tf`, then everything works well, the same for anaconda.\nI am on Mac and I have installed `tensorflow=0.8.0`\n", "comments": ["I had the same issue, turned out I still had `protobuf` installed in my path with brew. \nShould try to get the version of protobuf in the path with:\n\n`protoc -v`\n\nif version < 3 then `brew uninstall protobuf`\n", "@ziky90: Does @callicles's fix solve your issue?  \n", "@girving \nI've switched to anaconda and I get back to this issue right now. \nAnd I'm not able to reproduce the issue in virtualenv, so I consider it as solved.\n"]}, {"number": 2127, "title": "tf.nn.dynamic_rnn fails when batch_size is 0", "body": "### Environment info\n\nOperating System: Ubuntu 15.10\n\nInstalled version of CUDA and cuDNN: \n\n```\n-rw-r--r-- 1 root root    322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 09:48 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 09:48 /usr/local/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  8 14:12 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  8 14:12 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nCommit hash: 8d310bfcffcd46418d68dd535fb0fbcfee74b8a0\n### Steps to reproduce\n\nRun the following test case:\n\n```\nimport tensorflow as tf\n\noutputs, state = tf.nn.dynamic_rnn(\n  tf.nn.rnn_cell.BasicRNNCell(1),\n  tf.zeros((0, 1, 1)),\n  dtype=tf.float32)\n\nwith tf.Session() as sess:\n  initializer = tf.random_uniform_initializer(-0.1, 0.1)\n  tf.initialize_all_variables().run()\n  print sess.run([state])\n```\n\nWhen TensorFlow is compiled with `-c dbg`, this fails with the following message:\n\n```\npython: external/eigen_archive/eigen-eigen-0823d98fdde7/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h:124: Eigen::internal::TensorIntDivisor<T, div_gt_one>::TensorIntDivisor(T) [with T = long int; bool div_gt_one = false]: Assertion `divider > 0' failed.\n```\n\nUsing gdb, we get the following backtrace:\n\n```\n(gdb) bt\n#0  0x00007ffff7826267 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:55\n#1  0x00007ffff7827eca in __GI_abort () at abort.c:89\n#2  0x00007ffff781f03d in __assert_fail_base (fmt=0x7ffff7980fe8 \"%s%s%s:%u: %s%sAssertion `%s' failed.\\n%n\", assertion=assertion@entry=0x7fffeb82f7fe \"divider > 0\",\n    file=file@entry=0x7fffeb82f738 \"external/eigen_archive/eigen-eigen-0823d98fdde7/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h\", line=line@entry=124,\n    function=function@entry=0x7fffeb860080 <Eigen::internal::TensorIntDivisor<int, false>::TensorIntDivisor(int)::__PRETTY_FUNCTION__> \"Eigen::internal::TensorIntDivisor<T, div_gt_one>::TensorIntDivisor(T) [with T = int; bool div_gt_one = false]\") at assert.c:92\n#3  0x00007ffff781f0f2 in __GI___assert_fail (assertion=0x7fffeb82f7fe \"divider > 0\", file=0x7fffeb82f738 \"external/eigen_archive/eigen-eigen-0823d98fdde7/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h\", line=124,\n    function=0x7fffeb860080 <Eigen::internal::TensorIntDivisor<int, false>::TensorIntDivisor(int)::__PRETTY_FUNCTION__> \"Eigen::internal::TensorIntDivisor<T, div_gt_one>::TensorIntDivisor(T) [with T = int; bool div_gt_one = false]\") at assert.c:101\n#4  0x00007fffe9173cf5 in Eigen::internal::TensorIntDivisor<int, false>::TensorIntDivisor (this=0x7fff3dff9e40, divider=0)\n    at external/eigen_archive/eigen-eigen-0823d98fdde7/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h:124\n#5  0x00007fffe9793fd5 in Eigen::TensorEvaluator<Eigen::TensorSlicingOp<Eigen::DSizes<long, 3> const, Eigen::DSizes<long, 3> const, Eigen::TensorMap<Eigen::Tensor<float const, 3, 1, int>, 16> const> const, Eigen::GpuDevice>::TensorEvaluator (this=0x7fff3dff9f20, op=..., device=...) at external/eigen_archive/eigen-eigen-0823d98fdde7/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:352\n#6  0x00007fffe979370a in Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 3, 1, int>, 16>, Eigen::TensorSlicingOp<Eigen::DSizes<long, 3> const, Eigen::DSizes<long, 3> const, Eigen::TensorMap<Eigen::Tensor<float const, 3, 1, int>, 16> const> const> const, Eigen::GpuDevice>::TensorEvaluator (this=0x7fff3dff9f00, op=..., device=...)\n    at external/eigen_archive/eigen-eigen-0823d98fdde7/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:106\n#7  0x00007fffe97930b3 in Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 3, 1, int>, 16>, Eigen::TensorSlicingOp<Eigen::DSizes<long, 3> const, Eigen::DSizes<long, 3> const, Eigen::TensorMap<Eigen::Tensor<float const, 3, 1, int>, 16> const> const> const, Eigen::GpuDevice, false>::run (expr=..., device=...)\n    at external/eigen_archive/eigen-eigen-0823d98fdde7/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:233\n#8  0x00007fffe9792bd2 in Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 3, 1, int>, 16>, Eigen::GpuDevice>::operator=<Eigen::TensorSlicingOp<Eigen::DSizes<long, 3> const, Eigen::DSizes<long, 3> const, Eigen::TensorMap<Eigen::Tensor<float const, 3, 1, int>, 16> const> > (this=0x7fff3dffa040, other=...) at external/eigen_archive/eigen-eigen-0823d98fdde7/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35\n#9  0x00007fffe9792570 in tensorflow::functor::Split<Eigen::GpuDevice, float>::operator() (this=0x7fff3dffa1e0, d=..., output=..., input=..., slice_indices=..., slice_sizes=...)\n    at tensorflow/core/kernels/split_lib_gpu.cu.cc:37\n#10 0x00007fffe96be665 in tensorflow::TensorArrayUnpackOp<Eigen::GpuDevice, float>::Compute (this=0xe44040, ctx=0x7fff3dffab20) at tensorflow/core/kernels/tensor_array_ops.cc:753\n#11 0x00007fffeaf8e561 in tensorflow::BaseGPUDevice::Compute (this=0x2ba8110, op_kernel=0xe44040, context=0x7fff3dffab20) at tensorflow/core/common_runtime/gpu/gpu_device.cc:388\n#12 0x00007fffeb1b22eb in tensorflow::(anonymous namespace)::ExecutorState::Process (this=0xdf4290, tagged_node=..., scheduled_usec=0) at tensorflow/core/common_runtime/executor.cc:1092\n#13 0x00007fffeb1bdbeb in std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)>::operator()<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode&, long long&, void> (this=0x7fff200008c0, __object=0xdf4290) at /usr/include/c++/4.9/functional:569\n#14 0x00007fffeb1bd03e in std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::__call<void, 0ul, 1ul, 2ul>(<unknown type in /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so, CU 0xf323471, DIE 0xf3a21e7>, std::_Index_tuple<0ul, 1ul, 2ul>) (this=0x7fff200008c0, __args=<unknown type in /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so, CU 0xf323471, DIE 0xf3a21e7>)\n    at /usr/include/c++/4.9/functional:1264\n#15 0x00007fffeb1bb69a in std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::operator()<, void>(void) (this=0x7fff200008c0) at /usr/include/c++/4.9/functional:1323\n#16 0x00007fffeb1b98b6 in std::_Function_handler<void(), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/4.9/functional:2039\n#17 0x00007fffe8e4788c in std::function<void ()>::operator()() const (this=0x7fff3dffadd0) at /usr/include/c++/4.9/functional:2439\n#18 0x00007fffeb450454 in tensorflow::thread::ThreadPool::Impl::WorkerLoop (this=0x2bb28f0) at tensorflow/core/lib/core/threadpool.cc:196\n#19 0x00007fffeb44fe95 in tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int)::{lambda()#1}::operator()() const ()\n    at tensorflow/core/lib/core/threadpool.cc:123\n#20 0x00007fffeb45086d in std::_Function_handler<void(), tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, const tensorflow::ThreadOptions&, const string&, int)::<lambda()> >::_M_invoke(const std::_Any_data &) (\n    __functor=...) at /usr/include/c++/4.9/functional:2039\n#21 0x00007fffe8e4788c in std::function<void ()>::operator()() const (this=0x2bb4c48) at /usr/include/c++/4.9/functional:2439\n#22 0x00007fffeb471f5e in std::_Bind_simple<std::function<void ()> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x2bb4c48) at /usr/include/c++/4.9/functional:1700\n#23 0x00007fffeb471ea3 in std::_Bind_simple<std::function<void ()> ()>::operator()() (this=0x2bb4c48) at /usr/include/c++/4.9/functional:1688\n#24 0x00007fffeb471e20 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() (this=0x2bb4c30) at /usr/include/c++/4.9/thread:115\n#25 0x00007fffdc3bc030 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#26 0x00007ffff7bc26aa in start_thread (arg=0x7fff3dffb700) at pthread_create.c:333\n#27 0x00007ffff78f7e9d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\n\n(gdb) f 10\n#10 0x00007fffe96be665 in tensorflow::TensorArrayUnpackOp<Eigen::GpuDevice, float>::Compute (this=0xe44040, ctx=0x7fff3dffab20) at tensorflow/core/kernels/tensor_array_ops.cc:753\n753           functor::Split<Device, T>()(ctx->eigen_device<Device>(), tensor_value_i_t,\n(gdb) p this->name()\n$1 = (const std::string &) @0xdc49e0: {static npos = <optimized out>, _M_dataplus = {<std::allocator<char>> = {<__gnu_cxx::new_allocator<char>> = {<No data fields>}, <No data fields>},\n    _M_p = 0x2bee918 \"RNN/TensorArrayUnpack\"}}\n```\n", "comments": ["Thanks for the detailed report.   \n", "tensorflow/core/kernels/tensor_array_ops.cc:753 is the key line from the stack trace.  @ebrevdo is the author, but is on vacation at the moment.  Other calls to functor::Split() can be found at tensorflow/core/kernels/split_op.cc:172 and tensorflow/core/kernels/unpack_op.cc:86.  Looks like @girving might have some familiarity.\n", "The Split kernel assumes its inputs are nonempty, and `TensorArrayUnpackOp` doesn't check this.  It should.\n", "@ebrevdo: Would you have time to look at this, now you're back? (Or did it get fixed in the mean time?)\n", "Looks like it was fixed in https://github.com/tensorflow/tensorflow/commit/b7f7fe26fd2e15978d84a1f02e983faf7324a131.\n"]}, {"number": 2126, "title": "Auto device placement for distributed runtime", "body": "In a distributed TF setting, we need to place variables and ops to different devices. This is annoying to manually assign each variable and op, especially when we have GPU resources in our environment.\n\nTF offer a context named `tf.train.replica_device_setter` which place variables to ps devices in round-robin manner, and this is helpful but not enough. @mrry can you shed some light on the auto distributed devices placement problem?\n", "comments": ["@bhack this is the best I can do for now, pls feel free to correct me and provide some more thoughts. thx.\n", "Thank you @windreamer. I also  find  this topic quite crucial for the \"user experience\" when we have distributed devices. Generally on cluster we could have really unbalanced GPU/CPU scalar resources ratio and there are different considerations to do for data parallel and model parallel approaches. So it is really interesting to have some feedbacks. /cc @mtamburrano @lenlen\n", "@vrv is working on improving the device placement so that policies will be more declarative, rather than binding to specific devices at graph construction time. This should address some of the limitations of `tf.train.replica_device_setter()`. If you had specific requirements, it would be good to collect them here, so that we could take them into consideration in the redesign.\n", "See the discussion on that issue. The problem is most likely caused by the fact that `tf.train.SyncReplicasOptimizer` bakes the number of workers into the checkpoint, which is another issue that should probably be addressed.\n", "Indeed, this is a much bigger project that we'll chip away at over time (optimizing for runtime across a distributed cluster is kind of a hard problem, and we're not good enough at it that manual placement typically produces better results).  We'd love to do a better job of this, so we'll be looking at this over the next little while.  Happy to brainstorm ideas here.\n", "/cc @robertnishihara @maxpumperla\n", "It would be better if parameter can be partitioned into pieces and well managed.\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions.", "@drpngx From last @vrv comment we are waiting for news from google on this. Any progress or public roadmap?", "@benoitsteiner is working on better cost models for auto-assignment I think, but I don't know what the expected roadmap for that work is.\r\n\r\nC++ shape inference theoretically makes it possible to do an initial-placement algorithm that is better than the simple_placer, but right now the python runtime still maintains shape information that is not propagated to C++.  I think we need to wait for the control flow ops to be implemented via the C-API before we could make any more progress on initial static shape inference -- I believe the C-API stuff is being worked on.  There are probably some other ways to start playing around with this though (e.g., via add_shapes argument to as_graph_def()), but nobody I know is working on that.", "I don't really know how you are solving this in production or managed cloud :) I know that you cannot disclose it but I don't think that you are handling manual placement.", "We are now able to predict memory usage and performance for each operation of a TensorFlow graph. With this data we can start the actual work on optimizing TensorFlow models. We're completing work on a first optimizer that improves the performance of several models by about 20%, and we've also started to find a good device placement in parallel.\r\nWe'll start open sourcing this work as it matures. It's hard to predict how quickly research projects will succeed so unfortunately I don't have a roadmap to share. ", "@benoitsteiner Thank you for the update. We can leave this open to track relevant progress.", "/cc @jhseu", "@vrv I've seen your deleted comment. Another hypothesis is a secret army of manual [Oompa-Loompa](http://roalddahl.wikia.com/wiki/Oompa_Loompas) device placers ;)", "Indeed, Oompa-Loompa's are not simple people.\r\n\r\n(My deleted comment was in response to \"I don't really know how you are solving this in production\" to which I responded \"hacks and common deployment scenarios\".  I didn't see all the other comments after and it looked out of context ;)", ":smile:", "Since #4552 was closed and referenced here, I would just like to point out that there's a separate issue having nothing to do with distributed computing, and that is to prevent TF from scheduling too many concurrent operations at once such that it runs out of memory (on a single GPU). I have run into this problem many times where every single op can fit onto memory but because TF is overly aggressive about concurrency I end up not being able to run the model. This isn't about efficiency, but simply about avoiding unnecessary OOM errors.", "May I know the progress on this issue? Auto placement is a really nice shot for TF guys to benefit its distributed execution model. We have tried to do some kind of alike work by mixing data-parallelism with model-parallelism to get a trade-off between computation and communication and see promising improvement against standard data-parallelism implementation, also we are trying to automate this work. So maybe there are some potential collaboration with this issue.\r\n\r\n@yuefengz @benoitsteiner ", "We've made a lot of progress on the placement side, which will be detailed in our upcoming ICML paper. \r\n\r\nWe've also automated data-parallelism in the  [grappler auto parallelizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/auto_parallel.cc). We've tested it successfully on several models and we'll start documenting it as soon as possible.\r\n\r\nWe're open to a collaboration. Is there any documentation or paper you can point us to to get started thinking about what a common solution could look like ?", "/cc @klueska Any feedback?", "Got the name of the paper: *Device Placement Optimization with Reinforcement Learning*. Looking forward to the preprint!", "@benoitsteiner\r\n\r\nThanks for the nice progress.\r\nOur work haven't been recorded into any paper since we are still on the way with it. \r\nAs to the grappler auto parallelizer, we do have taken a look at its implementation. It is nice and somehow is what we want, although currently it only support naive BSP auto placement(Is my understanding correct?). My question is that why not to implement the grappler auto-parallelizer in python side, since in python side, there are more graph metadata and the processing overhead should be less than implementing it in c++ side.\r\n\r\nAs to the starting point of our solution, we are working on designing a cost model to evaluate the potential placement solution to help find a sub-optimal one since we think the auto-placement problem actually is a NP-hard problem, it is non-trivial to find the optimal solution. \r\nWe have added hybrid-parallelism for AlexNet into python side(actually transform the placement problem into a graph re-written problem), which is a mixture of data-parallelism and model parallelism and the speed-up against standard implementation is significant. The implementation overhead actually is not quite large. And also we are trying to follow this strategy to make it as automatic as possible, also as general as possible. \r\n\r\nNot sure how far your device placement optimization work has gone.\r\n\r\nAs to your nice paper(Device Placement Optimization with Reinforcement Learning) to be published on ICML 2017, will you put it on arXiv before the conference? \r\n\r\nThanks\r\n", "Our [Device Placement Optimization with Reinforcement Learning](https://arxiv.org/abs/1706.04972) paper has just been published on arXiv.\r\n\r\n", "@benoitsteiner \r\n\r\nThanks for the update, already taken a go-through of your nice paper and really appreciate your fancy RL-based idea illustrated in the paper.\r\nIt looks that so far the device placement optimization is still time consuming(around 20 hour with 80 GPUs), so will you integrate it into the TF master revision? Also may I know inside Google how often you will use this RL-based automatic placement solution to accelerate the model training process?\r\n\r\nThanks\r\n", "hi, why not take a little step forward,  like users should explicitly declare where the computation should happens (cpu, gpu, ps), but just with a 'virtual' device id, then tf automatically schedule the sub graphs to any free devices, or a preferred device list would be even better for migration between distrubuted, multi-gpus, single gpu and cpu only. And I think this would be much easier for us to try those training tf models in our own environment.\r\nPS, I really don't want to try the distributed tf after I checked the tutorials, -,-", "I read the paper and this looks incredible, so promising. I only wish there was a bit more detail on the RL training procedure. My I ask when would we be able to read some more documentation on this topic?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "This issue could have some connection with https://github.com/tensorflow/k8s/issues/35 /cc @jlewi ", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "How the device placement is handled for the new high level api like `estimators`. Expecially with `tf.estimator.train_and_evaluate` where users don't handle `model_fn` manually to make explicit device assignment.", "This last topic could impact also https://github.com/tensorflow/k8s/issues/61", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue as it's too broad. For the common case where graphs have one compute device and many parameter server devices we currently support automatic placement. For more complicated cases it's an open research problem how to do this.", "@benoitsteiner    I have read your  Device Placement Optimization with Reinforcement Learning paper, but I have some questions about it. First, this paper has given the cost function and used adam optimizer to update parameters, what's the use of formula (3) in this paper? Second, how can I calculate the probability of placement P? Finally, is the code for this paper open source? Thank you.", "https://ai.googleblog.com/2020/12/end-to-end-transferable-deep-rl-for.html", "https://arxiv.org/abs/2201.12023", "> https://arxiv.org/abs/2201.12023\r\n\r\nHello, could I ask if there is a plan of open source the code? I am curious about what your paper mentioned :\r\n\r\n\r\n> Since there are only less than 80 kinds of common primitive operators in DL graphs, we can manually enumerate the possible parallel algorithms for all primitive operators.\r\n\r\nIs there any doc or code relating to this?\r\n\r\nEdit: Find one github repo related, but seems the author has not made code public:\r\nhttps://github.com/alpa-projects"]}, {"number": 2125, "title": "Taking gradient of tf.cond(..., tf.nn.dyamic_rnn(...)) raises exception", "body": "### Environment info\n\nOperating System: Ubuntu 15.10\n\nInstalled version of CUDA and cuDNN: \n\n```\n-rw-r--r-- 1 root root    322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 09:48 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 09:48 /usr/local/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  8 14:12 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  8 14:12 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nCommit hash: 8d310bfcffcd46418d68dd535fb0fbcfee74b8a0\n### Steps to reproduce\n\nRun the following test case:\n\n```\nimport tensorflow as tf\n\nout = tf.cond(tf.constant(False),\n        lambda: tf.nn.dynamic_rnn(tf.nn.rnn_cell.BasicRNNCell(1),\n            tf.zeros((1, 1, 1)),\n            dtype=tf.float32)[1],\n        lambda: tf.zeros((1, 1, 1))\n)\n\ntvars = tf.trainable_variables()\ngrads = tf.gradients(tf.reduce_sum(out), tvars)\n```\n\nThe above gives the following traceback:\n\n```\nTraceback (most recent call last):\n  File \"cond_rnn_bug.py\", line 11, in <module>\n    grads = tf.gradients(tf.reduce_sum(out), tvars)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 481, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py\", line 254, in _TanhGrad\n    return grad * (1 - math_ops.square(y))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1655, in square\n    return _op_def_lib.apply_op(\"Square\", x=x, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 694, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2153, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1157, in __init__\n    self._control_flow_context.AddOp(self)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1330, in AddOp\n    self._AddOpInternal(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1352, in _AddOpInternal\n    self.AddValue(x)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1288, in AddValue\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 662, in GetRealValue\n    real_value = self.AddBackPropAccumulatedValue(h_value, value)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 612, in AddBackPropAccumulatedValue\n    history_value = _SwitchRefOrTensor(history_value, pred)[branch]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 306, in _SwitchRefOrTensor\n    return ref_switch(data, pred, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_control_flow_ops.py\", line 275, in ref_switch\n    return _op_def_lib.apply_op(\"RefSwitch\", data=data, pred=pred, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 449, in apply_op\n    as_ref=input_arg.is_ref).dtype.name\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 565, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 162, in constant\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 336, in make_tensor_proto\n    raise ValueError(\"None values not supported.\")\nValueError: None values not supported.\n```\n### What have you tried?\n\nRunning tf.nn.dynamic_rnn outside the lambda works, but my understanding is that the two are not semantically equivalent (although I'm not sure exactly in which ways).\n\nThat is, the following works:\n\n```\nimport tensorflow as tf\n\nrnn_out = tf.nn.dynamic_rnn(tf.nn.rnn_cell.BasicRNNCell(1),\n            tf.zeros((1, 1, 1)),\n            dtype=tf.float32)[1]\nout = tf.cond(tf.constant(False),\n        lambda: rnn_out,\n        lambda: tf.zeros((1, 1, 1))\n)\n\ntvars = tf.trainable_variables()\ngrads = tf.gradients(tf.reduce_sum(out), tvars)\n```\n", "comments": ["They are different in that rnn_out is run every time even if they values are not used.\n\nThe \"correct way\" is that tf.nn.dynamic_run is inside the function.   This is so that the gradient is called on the dynamic_run otherwise the gradient is being ignored.\n", "@yuanbyu: Could you take a look?\n", "@yuanbyu Friendly ping, can you update this bug with the status, thanks!\n", "This probably has been addressed in the current version.  Close for now. Reopen if the issue still exists."]}, {"number": 2124, "title": "Ref #2063: Adding validation summary writer in ValidationMonitor", "body": "", "comments": ["CC @terrytangyuan \n"]}, {"number": 2123, "title": "Different models for training and testing", "body": "```\n# choose RNN/GRU/LSTM cell\n        self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        # embedding model\n        if not self.attention:\n            self.dec_outputs, self.dec_memory = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length)\n            self.dec_outputs_tst, _ = seq2seq.embedding_rnn_seq2seq(\\\n                            self.enc_inp, self.dec_inp, self.cell, \\\n                            self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n```\n\nI want to have feed_previous = False for the Training (self.dec_outputs), while feed_previous=True for Testing(self.dec_output_tst). This is the reasonable case, in any seq2seq translation model.\n\nHowever, when I try to run it I get following error. This is exactly not a bug, but there is absolutely no source of information on this\n\nThe error is as follows:\n\n```\nFile \"/Users/harshal/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py\", line 320, in embedding_rnn_seq2seq\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n  File \"/Users/harshal/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 143, in run\n    (output, state) = call_cell()\n  File \"/Users/harshal/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 136, in <lambda>\n    call_cell = lambda: cell(input_, state)\n  File \"/Users/harshal/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 609, in __call__\n    initializer=initializer)\n  File \"/Users/harshal/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 334, in get_variable\n    collections=collections)\n  File \"/Users/harshal/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 257, in get_variable\n    collections=collections, caching_device=caching_device)\n  File \"/Users/harshal/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 118, in get_variable\n    name, \"\".join(traceback.format_list(tb))))\nValueError: Variable embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n```\n\nNow I know that I have to use variable scoping, but exactly where is the question. I apologize if I have missed on something\n", "comments": ["Okay so the solution was this:\n\n```\n# choose RNN/GRU/LSTM cell\n        with tf.variable_scope(\"train_test\", reuse=True):\n            self.cell = rnn_cell.LSTMCell(self.memory_dim)\n\n        # embedding model\n        if not self.attention:\n            with tf.variable_scope(\"train_test\"):\n                self.dec_outputs, self.dec_memory = seq2seq.embedding_rnn_seq2seq(\\\n                                self.enc_inp, self.dec_inp, self.cell, \\\n                                self.vocab_size, self.vocab_size, self.seq_length)\n            with tf.variable_scope(\"train_test\", reuse = True):\n                self.dec_outputs_tst, _ = seq2seq.embedding_rnn_seq2seq(\\\n                                self.enc_inp, self.dec_inp, self.cell, \\\n                                self.vocab_size, self.vocab_size, self.seq_length, feed_previous=True)\n```\n", "With the above code I am getting following error.\r\n\r\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x000001D51315E390> with a different variable scope than its first use.  First use of cell was with scope 'train_test/embedding_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell', this attempt is with scope 'train_test/embedding_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([LSTMCell(...)] * num_layers), change to: MultiRNNCell([LSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n\r\nThanks in advance for your help.", "Any progress?"]}, {"number": 2122, "title": "MacOS successful installed, fail to import", "body": "### Environment info\n\nOperating System:\nOSX EI Capitan, 10.11.4\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nNone\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   tensorflow-0.8.0-py2-none-any.whl for mac\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   \u279c  examples python -c \"import tensorflow; print(tensorflow.**version**)\".\n   File \"<string>\", line 1\n     import tensorflow; print(tensorflow.**version**).\n                                                     ^\n   SyntaxError: invalid syntax\n### What have you tried?\n1. pip uninstall, reinstall , however, still failed.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n> > > import tensorflow as tf\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 49, in <module>\n> > >     from tensorflow.core.framework.graph_pb2 import *\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n> > >     from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n> > >     from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n> > >     from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n> > >     serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB/\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\x62\\x06proto3')\n> > > TypeError: __init__() got an unexpected keyword argument 'syntax'\n", "comments": ["https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#mac-os-x-typeerror-init-got-an-unexpected-keyword-argument-syntax\n"]}, {"number": 2121, "title": "TF freezes while running.", "body": "Operating System: CentOS7\nInstalled version of CUDA and cuDNN: 7.5.18; 4.0.7\nPackage: Python2 GPU nightly built on Apr 23.\n\nWhile I'm training an inception-like model on multi GPUs, the training totally freezed after hours.\nThen I could see one of the GPUs stay at a 99~100% GPU utilization and a 0% GPU memory utilization. Others stayed at 0%:\n\n```\n$ nvidia-smi --query-gpu=temperature.gpu,clocks.current.sm,power.draw,utilization.gpu,utilization.memory,memory.free --format=csv | column -t -s ,\ntemperature.gpu   clocks.current.sm [MHz]   power.draw [W]   utilization.gpu [%]   utilization.memory [%]   memory.free [MiB]\n53                987 MHz                   71.80 W          0 %                   0 %                      38 MiB\n30                324 MHz                   14.33 W          0 %                   0 %                      38 MiB\n30                324 MHz                   14.81 W          0 %                   0 %                      38 MiB\n29                324 MHz                   14.34 W          0 %                   0 %                      38 MiB\n60                1151 MHz                  87.82 W          100 %                 0 %                      39 MiB\n54                987 MHz                   74.47 W          0 %                   0 %                      39 MiB\n28                324 MHz                   14.16 W          0 %                   0 %                      12264 MiB\n29                324 MHz                   14.06 W          0 %                   0 %                      12264 MiB\n```\n\nThis problem happened to me several times, but still hard to reproduce (happen after hours of training).\n\nI attached the process with `gdb -p [proc]` and did stack trace on thread 1 (there are a lot other threads):\n\n```\n#0  0x00007fda5a3156d5 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n#1  0x00007fda506ee9ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\n#2  0x00007fda33a1ed13 in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, long long) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fda33a28634 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fda33a2a8e2 in tensorflow::DirectSession::Run(std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fda33c2ff87 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fda33c303d1 in TF_Run () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fda32da7855 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fda32da7ec1 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00007fda32d93d17 in _wrap_TF_Run () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00007fda5a606aa4 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#11 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#12 0x00007fda5a594f68 in function_call () from /lib64/libpython2.7.so.1.0\n#13 0x00007fda5a5700b3 in PyObject_Call () from /lib64/libpython2.7.so.1.0\n#14 0x00007fda5a6032f7 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#15 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#16 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#17 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#18 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#19 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#20 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#21 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#22 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#23 0x00007fda5a606860 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#24 0x00007fda5a606860 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#25 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#26 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#27 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#28 0x00007fda5a6081c2 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\n#29 0x00007fda5a6215ff in run_mod () from /lib64/libpython2.7.so.1.0\n#30 0x00007fda5a6227be in PyRun_FileExFlags () from /lib64/libpython2.7.so.1.0\n#31 0x00007fda5a623a49 in PyRun_SimpleFileExFlags () from /lib64/libpython2.7.so.1.0\n#32 0x00007fda5a634b9f in Py_Main () from /lib64/libpython2.7.so.1.0\n#33 0x00007fda59861b15 in __libc_start_main () from /lib64/libc.so.6\n#34 0x0000000000400721 in _start ()\n```\n\nThe process doesn't respond to Ctrl-C (SIGINT), but can be killed by SIGTERM.\nI'm happy to try other methods to gather more information about the problem.\n", "comments": ["This kind of problem is very hard to diagnose.  Given that we have a long track record of running inception variants on TensorFlow, it's unlikely to be a simple logic error in the TensorFlow source.  Are you running any Ops you've defined yourself, or custom kernels?  If not, I would suspect either a mutual problem between TF and CUDA 7.5 or something in your hardware environment.  To diagnose that, you probably need to use the NVIDIA tools and also anything else that might indicate problems with the PCIe bus(es) to which your GPUs are attached.\n\nHow many GPUs is your model using?\nWhen a GPU freezes like this, is it always the same one?  If so, try running the model without that one, just using the others.\nTake a look at the full\n  nvidia-smi -q\noutput.  It might be that the memory utilization number is bogus.  \nIf you can try using CUDA 7.0, that might be worth a shot.\n\nThe first thread that you've traced is just the python thread that's issued a session run request.  The model executes within the C++/CUDA binary.  However, if the GPU has gotten into some kind of infinite loop, you won't see much indication of why in any of the thread stacks: you'll likely be able to see that an executor thread is waiting for some Op to terminate, but not which one.  Although it's unlikely to be of much help, if you're willing to modify the source and recompile you can log the names of ops before/after execution by modifying common_runtime/executor.cc.\n", "Thanks a lot for all these advice! To answer your questions, I'm not using custom kernels/ops. I'm running with 6 GPUs and it's not the same one that freezes.\nI'm not familiar with what nvidia tools that could help with a problem like this. I'll also try and see what I can find with different software versions or a different machine.\n", "I have the same issue as well. I use a similar set up to OP (but ubuntu 14.04 on 2 Titan X). I have been trying to run `https://github.com/tensorflow/models/tree/master/inception`.\n\nOriginally, I would run into an unhandled `CUDA_ERROR_LAUNCH_TIMEOUT` a few hours in\n\n```\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1\n```\n\n`CUDA_ERROR_LAUNCH_TIMEOUT` is the result of watchdog killing a compute thread that took too long. \n\nTo address this, I disabled watchdog by adding `options \"Interactive\" \"0\"` to my devices in `xorg.conf`.\nThis results in behavior mentioned by the OP when I tried to run `https://github.com/tensorflow/models/tree/master/inception`\n\nWhen I came back in the morning, one of the gpus is at 100% with all its memory in use while the other appears idle. Another thing I noted was that the 100% used GPU was at a much lower temperature (45C)  than when tensorflow is operating normally (80C). Also, my GUI was not responsive, though I could ssh in.\n\nAs for reproducibility, it happened to me two out of two times, 7:30 in and 7:50 in respectively, running at ~49 examples/second.\n", "Hi, are you using cuda 7.5 or 7.0? I only tried it once after switching to 7.0 and it didn't happen to me that time.\n", "@ppwwyyxx tyvm for the update. I was on cuda 7.5. I will try cuda 7.0.\n", "More updates: using cuda 7.5, the same behavior occurs even when num_gpu is set to 1. \nSubsequently, I reproduced a similar behavior using cuda 7.0 with num_gpu set to 2, though this time it took about 9hours before freezing. \n", "I've observed similar behavior using a Titan X.\n\nOS: Ubuntu 16.04\nNVIDIA driver: 361.42\nCuda: 7.5.18\nCudnn: 4.0.7\nTensorFlow: 0.8.0 from `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl`\n\nnvidia-smi reports:\n\n```\njason@ithaca [~] $ nvidia-smi\nSat May 21 09:47:45 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 361.42     Driver Version: 361.42         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  On   | 0000:01:00.0     Off |                  N/A |\n| 22%   45C    P8    28W / 250W |  11406MiB / 12287MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n...\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      3504    C   python                                       11379MiB |\n...\n+-----------------------------------------------------------------------------+\n```\n\nNote the 100% GPU usage but low fan speed (22%), low temperature (45C), and low power draw (28W). This is likely related to the GPU ending up in `P8` mode, which is a lower power mode. From the [nvidia docs](http://docs.nvidia.com/gameworks/content/gameworkslibrary/coresdk/nvapi/group__gpupstate.html):\n\n```\nP0/P1 - Maximum 3D performance\nP2/P3 - Balanced 3D performance-power\nP8 - Basic HD video playback\nP10 - DVD playback\nP12 - Minimum idle power consumption\n```\n\nAfter remaining in this frozen state for half a day, eventually this error was produced:\n`tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT`\nsuggesting some relation between this issue and #2117.\n", "@zheng-xq: Any ideas here, including ideas for further experiments to get more information? \n", "@ppwwyyxx, @liuyipei, @yosinski, in order to debug this problem, it is very useful to list all threads information, even better with call stacks, at least for the unique ones. Also please run the same experiment with environmental variable \"CUDA_LAUNCH_BLOCKING=1\" set. \n\nThat would help us pin-point where the hang is happening. \n\nThere are a number of problems described here. Not sure all of them are the same one. But on the surface, \"query event: CUDA_ERROR_LAUNCH_TIMEOUT\" implies a \"cuEventQuery\" call fails with a \"CUDA_ERROR_LAUNCH_TIMEOUT\" status. Since \"cuEventQuery\" itself is non-blocking, it is quite strange that it could fail with timeout. So another possibility is some other kernel is failing, and cuEventQuery is the one that detects that. The \"CUDA_LAUNCH_BLOCKING=1\" should tell us which Cuda calls is making the trouble. \n\nOne possibility is that we are deadlock within the Cuda library itself. Since we don't have source code, debugging that would be more difficult. \n", "@ppwwyyxx @liuyipei @yosinski Friendly ping, have any of you been able to follow @zheng-xq 's suggestion of running with CUDA_LAUNCH_BLOCKING=1, and collecting call stacks for all threads?\n", "Nope. The problem didn't happen to me in the last three months, so I wasn't investigating this.\n", "I have the same issue when I try to do an FFT. Here's the full backtrace with CUDA_LAUNCH_BLOCKING=1: http://pastebin.com/BbZSxVyG\n\nI'm on CUDA 7.5, will try installing 7.0 next.\n"]}, {"number": 2120, "title": "Branch 120862895", "body": "", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Test this please\n"]}, {"number": 2119, "title": "internal to upstream 2016/4/26", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2118, "title": "Default for tf.nn.conv2d_transpose output_shape", "body": "I am currently working with `tf.nn.conv2d_transpose`. From the past I am used to Caffe deconvolution layer and `tf.nn.conv2d_transpose` is kind of TensorFlow equivalent to it.\n\nMy question here is if someone could point me to detailed behaviour/documenatation of `tf.nn.conv2d_transpose`.\nParticularly I am confused by `output_shape` parameter. My question is, why there is needed the `output_shape` parameter. Isn't the output shape directly come from the conv2d_transpose operation?\nBased on how I understand it from Caffe and from [here](http://cs231n.stanford.edu/slides/winter1516_lecture13.pdf), computed as:\n\n```\nh = ((len(value[1]) - 1) * stride_h) + kernel_h - 2 * pad_h\nw = ((len(value[2]) - 1) * stride_w) + kernel_w - 2 * pad_w\n```\n\nWhat happens when I set `output_shape` smaller than `h` and `w`? Is the new layer being cropped?\n\nWhat happens when I set it higher? Is there being created padding with only zeros?\n", "comments": ["`output_shape` is needed because the shape of the output can't necessarily be computed from the shape of the input, specifically if the output is smaller than the filter and we're using `VALID` padding so the input is an empty image.  However, this degenerate case is unimportant most of the time, so it'd be reasonable to make the Python wrapper compute `output_shape` automatically if it isn't set.\n", "Ah, also the shape is ambiguous in cases with `stride > 1`.\n", "@girving I am still not sure why shape is ambiguous with `stride > 1`? Is this statement only valid for `padding=VALID` and `stride>1`?\n\nGiving particular example:\nFor example let's say that I have `stride=32`, `kernel=64` and `padding=SAME` and `value_w=value_h=16`. Then `output_shape` must be `[b, 544, 544, c]`. I don't see how I could get different `output_shape`.\n\n**EDIT:**\nI was trying to play a bit around with this, but I still have not found the case where `output_shape` would make sense.\nwhen I put there some that is different than computed by the formula, I'm getting:\n`InvalidArgumentError: Conv2DBackpropInput: Number of rows of out_backprop doesn't match computed:`\n", "`conv2d_transpose` is also used as the gradient of `conv2d`, so it needs to be able to spit out any shape that can be an input to `conv2d` for a given output shape of `conv2d`.  The formulas for the shape of the output of `conv2d` are\n\n```\noutput = (input - filter + stride) // stride  # VALID\noutput = (input + stride - 1) // stride  # SAME\n```\n\nIf `stride > 1`, neither of these are invertible.  However, I think it's an excellent idea to give `output_shape` a default value for `conv2d_transpose` in Python.\n", "Hi everyone,\nI have a question. How can I force certain dimensionality of the output of the conv2d_transpose layer ? My problem is that I use it for upsampling and I want to match the dimensionality of my labels and the output of the NN, for example if I have a feature map as Bx25x40xC how can I make it Bx100x160xC (i.e. upsample exactly 4x times) ?\n", "@amolchanov86 Questions like that should go on StackOverflow; Github issues are for bugs and feature requests.\n", "Well, it would be actually great to have such feature, since the conv2d_inverse is often used for the goal of upsampling anyway and one usually have a standard target size :) But, sure, you are right.\n", "I'm going to mark this as contributions welcome since there are some subtleties that I don't have time to investigate at the moment.  I'd be happy to review PRs if anyone wants to tackle this.  The subtlety is that  I'm not sure what the ideal default should be.\n", "@girving  I want to know the details of transpose_conv, Do you have more books or links to introduce the code of transpose_conv ? ", "i get the problem that why the B(batch_size) is fixed during both train and test. if i change the batch size during test, if will throw error.", "I made this function to calculate output shape of deconv2d, assuming stride==1. I'm a little unclear if stride > 1. \r\nJust something to kickstart this conversation again.\r\n```\r\ndef calculate_output_shape(in_layer, n_kernel, kernel_size, border_mode='same'):\r\n\t\"\"\"\r\n\tAlways assumes stride=1\r\n\t\"\"\"\r\n\tin_shape = in_layer.get_shape() # assumes in_shape[0] = None or batch_size\r\n\tout_shape = [s for s in in_shape] # copy\r\n\tout_shape[-1] = n_kernel # always true\r\n\tif border_mode=='same':\r\n\t\tout_shape[1] = in_shape[1]\r\n\t\tout_shape[2] = in_shape[2]\r\n\telif border_mode == 'valid':\r\n\t\tout_shape[1] = in_shape[1]+kernel_size - 1\r\n\t\tout_shape[2] = in_shape[2]+kernel_size - 1\r\n\treturn out_shape\r\n```", "@ncullen93 I believe if `stride > 1` you should pick the minimum consistent size, but it would take some more thought to make sure.", "one further question, why in the documentation the `output_shape` is referred as a 1D Tensor when it asks for a 4D Tensor instead?", "The *shape* of a tensor is itself a 1-D tensor.  For example, the shape of a 4-D tensor is a 1-D tensor with 4 elements.  This code may make the situation clearer:\r\n\r\n    >>> np.array(np.zeros((2,3,4,5)).shape).shape\r\n    (4,)", "all clear, sorry for the stupid question! :(", "> so it needs to be able to spit out any shape that can be an input to conv2d for a given output shape of conv2d\r\n\r\n@girving Why can't these shapes be inferred from the inputs? Could you provide a concrete example?", "@3rd3 Consider a forward convolution filter bigger than the input image, or a stride bigger than the input image, both with `padding='VALID'`.  Then the output will have size 0 or 1, respectively, independent of the input image.  Running this backwards, if we do a `conv2d_transpose` with input size 0 or 1, it could produce many possible output sizes.", "In tensorflow.slim implementation of conv_transpose, I find that output_shape can be calculated by this:\r\n```\r\nfrom tensorflow.python.layers import utils\r\nout_H = utils.deconv_output_length(in_H, ksize, padding, stride)\r\nout_W = utils.deconv_output_length(in_W, ksize, padding, stride)\r\n```", "I would say that you can invert 'input' and 'output' in the expression of conv2d output shapes:\r\nfor conv2d: \r\n```\r\noutput = (input - filter + stride) // stride  # VALID\r\noutput = (input + stride - 1) // stride  # SAME\r\n```\r\n\r\nso I would say that for conv2d_transpose:\r\n```\r\noutput = input * stride + filter - stride  # VALID\r\noutput = input * stride - stride + 1  # SAME \r\n```\r\n\r\nWhat do you think about that ?\r\nIf we transpose a conv2d the input become the output and output become the input.", "It is pretty simple to show that it is not invertible due to the integer division by stride.\r\nLet's consider the formula for conv2d with padding SAME:\r\n`output = (input + stride - 1) // stride`\r\nLet's say stride is 2 (a common case), then for an input size of 4 we get output size 2.\r\nBut for input size 3 we also get an output size of 2. \r\nSo **not invertible**.\r\nHence we need to specify the output size of the conv2d_transpose layer explicitly ", "@girving \r\ndeconv_output_length(input_length, filter_size, padding, stride) is defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/utils.py#L159)\r\n\r\nWhen padding is valid, input_length += max(filter_size - stride, 0), why to use `max` function?\r\n\r\nFor conv2d:\r\n```\r\noutput = (input - filter + stride) // stride  # VALID\r\noutput = (input + stride - 1) // stride  # SAME\r\n```\r\n\r\nFor conv2d_transpose:\r\n```\r\noutput = input * stride + filter - stride  # VALID\r\noutput = input * stride - stride + 1  # SAME \r\n```\r\n\r\nEven when filter_size is less than stride, I think output is also `input * stride + filter - stride` rather `input * stride`, so why to use `max`?\r\n", "@tongxyh I also get this error. If I use output_shape=[input_.get_shape()[0].value, 32, 32, 256], it throws an error \"Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 32, 32, 256]. Consider casting elements to a supported type\" when building the computation graph.\r\nHowever, If I use output_shape=[-1, 32, 32, 256], it would be OK when building the graph, but when the session is ran, there also gets an error...", "While the integer division is not invertible, we have no information about the pixels that \"belong\" to the residual part. So, the consideration of these extra pixels leads us to padding the output tensor. Am I correct?\r\n(Not sure if this question/thought should be posted here.)\r\n\r\ne.g.,\r\n\r\nTo think about this, I use \"input\" to denote the input of the convolution (non transposed) and \"output\" its output. Since we are only interested in shapes, output -> conv2d_transposed -> input.\r\n\r\nGiven input of shape [7, 7], kernel of shape [3, 3]. With valid padding, the output before striding is [5, 5]. If the stride is [3, 3] then the final output is of shape [2, 2].\r\n\r\nThe same setting with input of shape [6, 6] gives the same output shape [2, 2], where we find the invertibility of integer division.\r\n\r\nHowever, we actually do not have any information about the extra pixels in the first case. (extra pixels in [7, 7] compared to [6, 6]). So, when we want to conv2d_transpose back to the input image [7, 7], we can do nothing but zero paddings. (Maybe translated by a bias later?)", "I see it is not invertible, and the desired output shape for conv2d_transposed is necessary. \r\n\r\nBut it becomes a common issue when trying to integrate the model in a c program and execute the model via Tensorflow C API, because the model (with conv2d_transposed layer) is tied with the desired output shape which is usually relative to the input image size. And we have to prepare many model files for different possible input image size, it is not feasible.\r\n\r\nActually, for many cases, the model (for example, with encoder and decoder) is designed to be independent with the input image size, it is expected that a single model file can be used for all kinds of image sizes.\r\n\r\nSo, can we add an option for this layer to not set output_shape explicitly?\r\n- add a new parameter remainders, and so:\r\noutput = input * stride + remainder + filter - stride  # VALID\r\noutput = input * stride + remainder - stride + 1  # SAME \r\n\r\n- if the output_shape is none (or even add a new parameter flag), just do:\r\noutput = input * stride + filter - stride  # VALID\r\noutput = input * stride - stride + 1  # SAME ", "> i get the problem that why the B(batch_size) is fixed during both train and test. if i change the batch size during test, if will throw error.\r\n\r\n", "Hi @ziky90! We are checking to see if you still need help this feature request , have you tried latest stable version 2.6 yet as many features have been added in latest versions.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 2117, "title": "Failed to memcpy from device to host: CUDA_ERROR_LAUNCH_TIMEOUT when running CIFAR10 after 113K steps", "body": "I tried to run the cifar10 model provided from TF with GPU support. I was able to successfully install Tensorflow from source (with GPU) and also was able to run the `cifar10_train.py` with utilizing my GPU. However, after step=113330, I encountered the following error which is probably related to async memcpy from device to host. As my graphic card compute capability is 5.2, I thought it should not be due to compute capability conflicts. \n### Similar issues\n#1477\n#1060\n\nBut my error is slightly different\n### Environment info\n\nOperating System: Ubuntu 14.04\nGPU:  GM200 - GeForce GTX TITAN X (rev a1)\nTensorflow 0.8 (installed from source)\nInstalled version of CUDA and cuDNN: \ncuda 7.5\ncudnn 7.0 (v4)\nusing Anaconda virtual env\n### Logs\n\n```\n2016-04-25 22:10:14.937118: step 113330, loss = 0.74 (1276.4 examples/sec; 0.100 sec/batch)\n\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1197] failed to enqueue async memcpy from device to host: CUDA_ERROR_LAUNCH_TIMEOUT; host dst: 0x7fbdd0001680; GPU src: 0xb06c84600; size: 3=0x3\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1099] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_TIMEOUT :: No stack trace available\nE tensorflow/stream_executor/stream.cc:272] Error recording event in stream: error recording CUDA event on stream 0x1efe980: CUDA_ERROR_LAUNCH_TIMEOUT; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1099] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_TIMEOUT :: No stack trace available\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1099] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_TIMEOUT :: No stack trace available\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1099] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_TIMEOUT :: No stack trace available\nI tensorflow/stream_executor/stream.cc:826] stream 0x1efe860 did not wait for stream: 0x1efd340\nAborted (core dumped)\n\n```\n", "comments": ["Is this error reproducible, i.e. does it occur repeatedly at roughly that number of steps?  Are there any other indications of unusual behavior before this error, e.g. signs of memory exhaustion?\n", "@poxvoculi \nIt is hard to reproduce it as it takes really long to reach 113K steps. I had it running overnight several times and sometimes my system reboots after core dumps. So I lost the step at which it failed.\nI am trying to keep track of it in a log file to see if there is a random failure or not.\nI did not notice any sign of memory exhaustion. Indeed, it was my initial guess. But what is the best way to ensure there is no memory bandwidth limit?\nMy current GPU memory usage at step=70K is 11501MiB with GPU utilization around %30-%45\n", "I have not personally seen this CUDA error previously.  The error likely is not directly related to the memcpy, but rather the memcpy is failing because the GPU is in a bad state.  Many CUDA driver commands are asynchronous: the error from a prior action is only fielded when trying to launch a subsequent action.  If it's a bad error (e.g. one we never expect to see), then a bad status will propagate back up to the top of TensorFlow. That's what's happening here.  NVIDIA documentation indicates that this error can result when a kernel takes too long to execute.  If you're not using any modified kernels and  are just running the provided model with the usual data, then no kernel should take an excessive amount of time unless something causes the GPU to not be able to make progress.  It's hard to guess what that might be.  TensorFlow uses its own external memory manager for GPU memory, so memory exhaustion _should_ trigger a TF bad status exception on the CPU side, and never trigger a GPU error.  Since this error only showed up after a long period, I don't think it's impossible that it could be a hardware issue, e.g. an un-corrected memory error (IIRC, most consumer NVIDIA GPUs don't use ECC memory).  \n", "Yes it takes a long time. And I figured out that it is not happened at the exact step. I re-ran the training this time with TF built using the whl file. And so far 238K step is reached without core dump.\nCould it be according to building from source?\nI did not change anything with respect the code, though.\n", "@hamidb: Building from source is fine.  Unfortunately, it sounds like this will be impossible to debug since it can't be reproduced, so I'll close for now.  Please reopen if reproduction becomes possible! \n", "I get the same bug. \r\n2017-07-13 22:05:28.022704: I tensorflow/stream_executor/stream.cc:1500] stream 0x5da97e0 did not wait for stream: 0x5da95b0\r\n2017-07-13 22:05:28.022741: I tensorflow/stream_executor/stream.cc:4087] stream 0x5da97e0 did not memcpy host-to-device; source: 0x203861300\r\n2017-07-13 22:05:28.022808: F tensorflow/core/common_runtime/gpu/gpu_util.cc:343] CPU->GPU Memcpy failed", "the same \r\n2018-12-11 00:31:09.918511: E tensorflow/stream_executor/cuda/cuda_driver.cc:1130] failed to enqueue async memcpy from host to device: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure; GPU dst: 0x4232ce0b00; host src: 0x7f1f10244780; size: 1597440=0x186000\r\n2018-12-11 00:31:09.918525: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2018-12-11 00:31:09.918534: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2018-12-11 00:31:09.918534: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2018-12-11 00:31:09.918568: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\n\r\n", "got a solution\r\n\r\ncudnn64_6.dll is missing in the Toolkit - bin folder. You have to copy it there. ", "My solution related to the fact I was using \r\n`from numba import cuda\r\ncuda.select_device(0)`\r\n\r\nYou have to use the latter line before interacting with tensorflow or keras at all. ", "Has anyone found a fix to this yet? I'm encountering just the same problem\r\n```\r\n2020-04-04 07:32:27.274747: I tensorflow/stream_executor/stream.cc:1990] [stream=000002027C991AE0,impl=000002027F822190] did not wait for [stream=000002027C9908E0,impl=000002027F822160]\r\n2020-04-04 07:32:27.278532: I tensorflow/stream_executor/stream.cc:4938] [stream=000002027C991AE0,impl=000002027F822190] did not memcpy host-to-device; source: 000002026F97A000\r\n2020-04-04 07:32:27.282116: F tensorflow/core/common_runtime/gpu/gpu_util.cc:340] CPU->GPU Memcpy failed\r\n```", "@Terkea \r\nI have strange problem - identical dockerfile, on one machine it works (GTX1050TI), on laptop with MX250 i get the same error as you.\r\n\r\nI found here #35029 that it could be linked with nvidia drivers - 430 with GTX and 440 with MX250.\r\n", "```python\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.Session(config=config)\r\n```\r\nadding those lines on top of my file right below the imports fixed this in my case. ", "@Terkea That worked for me, thanks so much!!!", "> @Terkea That worked for me, thanks so much!!!\r\n\r\nglad I could help", "@Terkea Thank you so much. Your solution works in my case as well."]}, {"number": 2116, "title": "Change from using platform.default._gfile to platform.gfile in learn \u2026", "body": "\u2026datasets\n", "comments": []}, {"number": 2115, "title": "Tutorial yields \"tensorboard: Command not found\"", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Mac OS X Yosemite, 10.10.5\nMacPorts\npython 2.7\n\nInstalled version of CUDA and cuDNN:  None\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   % sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl\n   Collecting tensorflow==0.8.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl\n   Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl (19.3MB)\n     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 19.3MB 53kB/s \n   Requirement already up-to-date: six>=1.10.0 in /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from tensorflow==0.8.0)\n   Collecting protobuf==3.0.0b2 (from tensorflow==0.8.0)\n   Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB)\n     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 2.0MB/s \n   Collecting wheel (from tensorflow==0.8.0)\n   Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\n     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 10.1MB/s \n   Collecting numpy>=1.10.1 (from tensorflow==0.8.0)\n   Downloading numpy-1.11.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.9MB)\n     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.9MB 265kB/s \n   Collecting setuptools (from protobuf==3.0.0b2->tensorflow==0.8.0)\n   Downloading setuptools-20.10.1-py2.py3-none-any.whl (509kB)\n     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 512kB 1.9MB/s \n   Installing collected packages: setuptools, protobuf, wheel, numpy, tensorflow\n   Found existing installation: setuptools 19.2\n     Uninstalling setuptools-19.2:\n       Successfully uninstalled setuptools-19.2\n   Found existing installation: numpy 1.10.4\n     DEPRECATION: Uninstalling a distutils installed project (numpy) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n     Uninstalling numpy-1.10.4:\n       Successfully uninstalled numpy-1.10.4\n   Successfully installed numpy-1.11.0 protobuf-3.0.0b2 setuptools-20.10.1 tensorflow-0.8.0 wheel-0.29.0\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   % python -c \"import tensorflow; print(tensorflow.**version**)\"\n   0.8.0\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1: Read  the documentation at https://www.tensorflow.org/versions/r0.8/how_tos/summaries_and_tensorboard/index.html:\n\"Launching TensorBoard\n\nTo run TensorBoard, use the command\n\ntensorboard --logdir=path/to/log-directory\"\n\n2: Do what it says\n\n```\n% tensorboard --logdir=path/to/log-directory\ntensorboard: Command not found.\n```\n### What have you tried?\n\n1: Try rehash\n\n```\n% rehash\n% tensorboard --logdir=path/to/log-directory\ntensorboard: Command not found.\n```\n\n2:\n\n```\n% locate tensorboard\n%\n```\n\n3:\n\n```\n% which tensorboard\ntensorboard: Command not found.\n\n```\n\n4:\n\nCheck Google, find StackExchange post \"How to install tensorboard\"\nhttp://stackoverflow.com/questions/33634008/how-to-install-tensorboard\nComments suggest that if one installs via pip (as I did), then tensorboard is available on the command-line, but it is not.\nAlternate invocation of tensorboard via python is not the same as in the Tutorial but does work....\n\n```\n% python /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/tensorboard/tensorboard.py --logdir=/tmp/mnist_logs\nStarting TensorBoard 16 on port 6006\n(You can navigate to http://0.0.0.0:6006)...\n```\n\nand everything works.\n\n...Conclusion is that either online Tutorial documentation is incorrect and needs revision, or pip installation does not perform as advertised, or...user error.\n\nWorkarounds:\n\n1.\n\n```\n% alias tensorboard 'python /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/tensorboard/tensorboard.py'\n% tensorboard --logdir=/tmp/mnist_logs\n```\n\n2.\n\n```\n`echo '#! /usr/bin/env python' > tmp.txt ; cat tmp.txt /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/tensorboard/tensorboard.py > tensorboard ; chmod u+x tensorboard ; rm -f tmp.txt; sudo mv tensorboard /opt/local/bin/\n```\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Thanks for the detailed report.  It sounds like the pip installation didn't work for you for reasons that we can't reproduce.  Would you consider the virtualenv install?  Under that install you should end up with a small executable 'tensorboard' in the bin directory under the virtualenv directory.    Its contents should be\n\n#!<path to virtualenv directory>/bin/python\n\n# -_\\- coding: utf-8 -_-\n\nimport re\nimport sys\n\nfrom tensorflow.tensorboard.tensorboard import main\n\nif **name** == '**main**':\n    sys.argv[0] = re.sub(r'(-script.pyw|.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "Yes, after virtualenv install I find a \"tensorboard\" file in the bin directory.  Contents are just as you say.\n\nFigured out the problem:\nUninstalled & re-installed TensorFlow in my main Mac environment, and I learned something: As a MacPorts user, I'm used to running things from out of the path /opt/local/bin.  When you install a python package via MacPorts, that's where the executables go --- even if they're just symbolic links to files to a main python repository in /opt/local/Library/Frameworks/Python.framework/Versions/2.7/bin/\n\npip installs things into the latter directory, but apparently does NOT add the symbolic link to /opt/local/bin \n\nThis has never been an issue (or even come up) for me before, because I've only used pip to install (non-executable) packages which load from within python.\nIn conclusion, there _is_ a /opt/local/Library/Frameworks/Python.framework/Versions/2.7/bin/tensorboard\n\nThis is a pip /  MacPorts-SOP mismatch / user error*, and nothing to do with tensorboard in particular.  \n Please close this issue.   Thanks for your help.\n\n*my 'locate' database was in the process of updating but hadn't completed\n", "I faced same issue, and when I typed \r\n`which tensorboard`\r\nit returned nothing, and following solved my problem\r\n`python -m tensorflow.tensorboard --logdir=path/to/log-directory`", "1. make sure that you not trying to run tensorboard global, while your tensorflow is installed in a virtualenv (pycharm egg). Install tensorflow with some command line (pip install tensorflow) globally. \r\n2. sometimes port 6006 is not working while you using windows, i guess cause it is identified as \"Ports normally used by Trojans\" (WinHole port 808 and BadBlood port 6006).\r\nInstead ouf 6006 try this: \r\ntensorboard --logdir=PATH --host localhost --port 8088\r\n", "I faced with same issue. Ubuntu 19.04 tensorflow 1.13  \r\nThis works: `python -m tensorboard.main --logdir=logdir`", "> \r\n> \r\n> I faced with same issue. Ubuntu 19.04 tensorflow 1.13\r\n> This works: `python -m tensorboard.main --logdir=logdir`\r\n\r\nAlso worked for me on Windows 10.", "Quickest solution - \r\n\r\n`echo \"alias tensorboard='python3 -m tensorboard.main'\" >> ~/.bash_profile`\r\n\r\nAfter adding this to your .bash_profile you can use \r\n\r\n`tensorboard --logdir=/path`"]}, {"number": 2114, "title": "Don't reimport numpy from py_func", "body": "Numpy is supposed to be imported only at module initialization time.\nFix this by factoring our numpy import logic into lib/core/numpy.{h,cc}\nand using from both tf_session_helper.h and py_func.h.\n", "comments": []}, {"number": 2113, "title": "build doc enhancements", "body": "build doc enhancements, described in #2083 \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@martinwicke OK to merge?\n", "there are conflicts :(\n"]}, {"number": 2112, "title": "Branch 120798893", "body": "", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Jenkins, test this please!\n", "Come on Jenkins, test this, please!\n", "Jenkins, test this again, please.\n", "Jenkins? Test this please!\n"]}, {"number": 2111, "title": "Two versions of Grid LSTM cells", "body": "I noticed that there are two distinct versions of Grid RNN and LSTM cells in contrib:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/grid_rnn/python/ops/grid_rnn_cell.py\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py\n\nI realize that the RNN interface is still in flux, but I was wondering if these will be harmonized at some point?\n", "comments": ["This might be a good issue to take up on the discussion board:\n\nhttps://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/discuss\n"]}]