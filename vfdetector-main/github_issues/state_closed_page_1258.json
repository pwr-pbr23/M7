[{"number": 15404, "title": "DOCS: Update the description of the fused parameter.", "body": "Update the description of layers.batch_normalization and contrib.layers.python.layers.batch_norm to reflect the fact that fused=None is equivalent to fused=True. In fact, both functions check at the beginning if fused is None, and if yes they set it to True.", "comments": ["Can one of the admins verify this patch?", "This change is good -- the docs were clearly out of date. \r\n\r\nHowever, the actual behavior of batch_norm is a bit puzzling to me: If fused is None or True, it tries to use fused batch norm. I would have expected \r\n\r\nfused=None: Try fused, fall back on regular if not possible to use fused\r\nfused=True: Try fused, if not possible, raise\r\nfused=False: Don't try fused.\r\n\r\n@chrisying, do you have context on this (just calling you because you're in the comments)", "The current behavior is that fused=None and fused=True have the same behavior (try to use fused=True but revert to fused=False if the conditions are not met). I'm not entirely sure why this but might possibly be due to backwards compatibility? Not certain.\r\n\r\ntf.layers.batch_normalization should be cleaned-up in the future since there are quite a few optional parameters right now, many of which are not compatible. But that's a separate issue, this doc change is technically correct but I can't confirm if it's intended or not."]}, {"number": 15403, "title": "Computing gradients of loop variables return None", "body": "### Problem description\r\nI got `None` when computing gradient of the two loop variables that are supposed to have gradient..\r\n\r\n### Minimum code to reproduce the error\r\n```python\r\ndef loop_cond(i, *_):\r\n    with tf.control_dependencies([tf.Print(i, [i])]):\r\n        return i < 5\r\n\r\ndef loop_body(i, a, b):\r\n    c = tf.gradients(b, a)[0]\r\n    b = b + c\r\n    return i + 1, b,  b ** 2\r\n        \r\na = tf.constant(3.0)\r\nf_i, f_a, f_b = tf.while_loop(loop_cond, loop_body, [0, a, a])\r\n```\r\n\r\nIt seems to have failed in the first iteration as there was no output from print statement.\r\n\r\n### Complete logs\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-23-a766942a3da2> in <module>()\r\n      9 \r\n     10 a = tf.constant(3.0)\r\n---> 11 f_i, f_a, f_b = tf.while_loop(loop_cond, loop_body, [0, a, a])\r\n\r\n~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2814     loop_context = WhileContext(parallel_iterations, back_prop, swap_memory)  # pylint: disable=redefined-outer-name\r\n   2815     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n-> 2816     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2817     return result\r\n   2818 \r\n\r\n~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2638       self.Enter()\r\n   2639       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2640           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2641     finally:\r\n   2642       self.Exit()\r\n\r\n~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2588         structure=original_loop_vars,\r\n   2589         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2590     body_result = body(*packed_vars_for_body)\r\n   2591     if not nest.is_sequence(body_result):\r\n   2592       body_result = [body_result]\r\n\r\n<ipython-input-23-a766942a3da2> in loop_body(i, a, b)\r\n      5 def loop_body(i, a, b):\r\n      6     c = tf.gradients(b, a)[0]\r\n----> 7     b = b + c\r\n      8     return i + 1, b,  b ** 2\r\n      9 \r\n\r\n~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\r\n    883       if not isinstance(y, sparse_tensor.SparseTensor):\r\n    884         try:\r\n--> 885           y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\r\n    886         except TypeError:\r\n    887           # If the RHS is not a tensor, it might be a tensor aware object\r\n\r\n~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n    834       name=name,\r\n    835       preferred_dtype=preferred_dtype,\r\n--> 836       as_ref=False)\r\n    837 \r\n    838 \r\n\r\n~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n    924 \r\n    925     if ret is None:\r\n--> 926       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    927 \r\n    928     if ret is NotImplemented:\r\n\r\n~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    227                                          as_ref=False):\r\n    228   _ = as_ref\r\n--> 229   return constant(v, dtype=dtype, name=name)\r\n    230 \r\n    231 \r\n\r\n~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    206   tensor_value.tensor.CopyFrom(\r\n    207       tensor_util.make_tensor_proto(\r\n--> 208           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    209   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    210   const_tensor = g.create_op(\r\n\r\n~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    369   else:\r\n    370     if values is None:\r\n--> 371       raise ValueError(\"None values not supported.\")\r\n    372     # if dtype is provided, forces numpy array to be the type\r\n    373     # provided if possible.\r\n\r\nValueError: None values not supported.\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 7.11 (wheezy)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-4-g9283868 1.4.0\r\n- **Python version**:  3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@angersson I am not sure why you say that this is not a bug. The issue is that tf.gradients is returning a None value inside of a tf.while_loop, but not when called with the exact same variables outside of the loop:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ni = tf.constant(0)\r\na = tf.constant(3.0)\r\nb = tf.constant(5.0)\r\nprint(\"External gradient:\", tf.gradients(a, a)[0])     # Prints Tensor(\"gradients/Fill:0\", shape=(), dtype=float32)\r\n\r\ndef loop_body(i, a, b):\r\n    print(\"internal gradient:\", tf.gradients(b, a)[0]) # Prints None\r\n    return i + 1, b,  b ** 2\r\n\r\ntf.while_loop( lambda i,a,b: tf.less(i, 5), loop_body, [i, a, b]);\r\n```", "@mholzel This issue has been closed for a long time. If you think you've found a bug, please make a new issue that follows the issue submission guidelines. Thanks!"]}, {"number": 15402, "title": "Add complex64 and complex128 support for ApplyAdadelta kernel", "body": "This fix tries to address the issue raised in #13521 where the complex64 and complex128 support for ApplyAdadelta is missing. The test cases have also been updated.\r\n\r\nThis fix fixes #13521.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@ebrevdo Thanks for the review. I tried to enable complex for GPU but have the following `host function(\"__builtin_csqrtf\") from a device function` error.\r\n\r\nI am guessing Eigen does not have a complex version of `sqrt` on GPU yet.\r\n\r\nMaybe we could leave GPU part alone for now?\r\n\r\nIf I could be pointed to the place in Eigen to make the necessary change, I will be happy to submit a PR in Eigen.\r\n\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h(420): error: calling a __host__ function(\"__builtin_csqrtf\") from a __device__ function(\"Eigen::internal::EigenMetaKernelEval< ::Eigen::TensorEvaluator<const  ::Eigen::TensorAssignOp< ::Eigen::TensorMap< ::Eigen::Tensor< ::std::complex<float> , (int)1, (int)1, long> , (int)16,  ::Eigen::MakePointer> , const  ::Eigen::TensorCwiseBinaryOp< ::Eigen::internal::scalar_difference_op< ::std::complex<float> ,  ::std::complex<float> > , const  ::Eigen::TensorMap< ::Eigen::Tensor< ::std::complex<float> , (int)1, (int)1, long> , (int)16,  ::Eigen::MakePointer> , const  ::Eigen::TensorCwiseBinaryOp< ::Eigen::internal::scalar_product_op< ::std::complex<float> ,  ::std::complex<float> > , const  ::Eigen::TensorCwiseBinaryOp< ::Eigen::internal::scalar_product_op< ::std::complex<float> ,  ::std::complex<float> > , const  ::Eigen::TensorCwiseBinaryOp< ::Eigen::internal::scalar_product_op< ::std::complex<float> ,  ::std::complex<float> > , const  ::Eigen::TensorCwiseUnaryOp< ::Eigen::internal::scalar_sqrt_op< ::std::complex<float> > , const  ::Eigen::TensorCwiseBinaryOp< ::Eigen::internal::scalar_sum_op< ::std::complex<float> ,  ::std::complex<float> > , const  ::Eigen::TensorMap< ::Eigen::Tensor< ::std::complex<float> , (int)1, (int)1, long> , (int)16,  ::Eigen::MakePointer> , const  ::Eigen::TensorBroadcastingOp<const  ::Eigen::array<long, (unsigned long)1ul> , const  ::Eigen::TensorReshapingOp<const  ::Eigen::Sizes<(long)1l > , const  ::Eigen::TensorMap< ::Eigen::TensorFixedSize<const  ::std::complex<float> ,  ::Eigen::Sizes< > , (int)1, long> , (int)16,  ::Eigen::MakePointer> > > > > , const  ::Eigen::TensorCwiseUnaryOp< ::Eigen::internal::scalar_rsqrt_op< ::std::complex<float> > , const  ::Eigen::TensorCwiseBinaryOp< ::Eigen::internal::scalar_sum_op< ::std::complex<float> ,  ::std::complex<float> > , const  ::Eigen::TensorMap< ::Eigen::Tensor< ::std::complex<float> , (int)1, (int)1, long> , (int)16,  ::Eigen::MakePointer> , const  ::Eigen::TensorBroadcastingOp<const  ::Eigen::array<long, (unsigned long)1ul> , const  ::Eigen::TensorReshapingOp<const  ::Eigen::Sizes<(long)1l > , const  ::Eigen::TensorMap< ::Eigen::TensorFixedSize<const  ::std::complex<float> ,  ::Eigen::Sizes< > , (int)1, long> , (int)16,  ::Eigen::MakePointer> > > > > > , const  ::Eigen::TensorMap< ::Eigen::Tensor<const  ::std::complex<float> , (int)1, (int)1, long> , (int)16,  ::Eigen::MakePointer> > , const  ::Eigen::TensorBroadcastingOp<const  ::Eigen::array<long, (unsigned long)1ul> , const  ::Eigen::TensorReshapingOp<const  ::Eigen::Sizes<(long)1l > , const  ::Eigen::TensorMap< ::Eigen::TensorFixedSize<const  ::std::complex<float> ,  ::Eigen::Sizes< > , (int)1, long> , (int)16,  ::Eigen::MakePointer> > > > > > ,  ::Eigen::GpuDevice> , long, (bool)0> ::run\") is not allowed", "I'm guessing that the code is missing an .abs() or .abs2() somewhere. There is almost guaranteed no (current) use for complex square root.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@yongtang any luck debugging this?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the delay. Missed the notification. I will work on this issue in the next few days and hopefully get it addressed shortly.", "Let me close this PR for now. I will reopen once I figure out the way to fix it.\r\n\r\nSorry to all for the inconvenience."]}, {"number": 15401, "title": "module 'tensorflow.contrib' has no attribute 'lite'", "body": "Hello folks.\r\n\r\nEverytime I try to run the example fo TOCO:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nval = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])\r\nout = tf.identity(val, name=\"out\")\r\nwith tf.Session() as sess:\r\n  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])\r\n  open(\"test.tflite\", \"wb\").write(tflite_modeL)\r\n``` \r\n\r\nI get the error: **module 'tensorflow.contrib' has no attribute 'lite'**\r\n\r\nOS Platform and Distribution: Mac OS High Sierra\r\nTensorflow installed from pip version 1.4.1. \r\nPython version: 2.7.14 and 3.6.3 :: Anaconda custom (64-bit)\r\n\r\nThe informations about my system are those:\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin Leandros-MacBook-Pro.local 17.3.0 Darwin Kernel Version 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.2\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.37)\r\nTarget: x86_64-apple-darwin17.3.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Leandros-MacBook-Pro.local 17.3.0 Darwin Kernel Version 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nnumpydoc (0.7.0)\r\nprotobuf (3.5.0.post1)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.8)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\nSeams like tensorflow lite is not available for reason =/\r\n\r\nSorry if this is not a bug and I am just being dumb about how to make TOCO works. \r\n", "comments": ["Tensorflow 1.4.1 doesn't contain TensorFlow lite. you need to download the nightly whl to get it. We are cutting v1.5.0 soon which will contain it.", "You should be able to install the nightly with `pip install tf_nightly`. Once the 1.5 release comes out, the `tensorflow` package will have TF Lite.", "Oh. So it's not a bug. Thanks for the help!", "Am geetting TOCO_CONVERT error. . .Kindlyhelp. . .\r\n\r\n**AttributeError: module 'tensorflow.contrib.lite.python.lite' has no attribute 'toco_convert'**\r\n_see last two  lines for error causing code_\r\n//=================================\r\nimport tensorflow  as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nimport seaborn as sns\r\nimport matplotlib as lib\r\n\r\nfrom tensorflow.python.tools import freeze_graph\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\n\r\n\r\nprint(tf.__version__)\r\ndata=pd.read_csv('C:\\ml\\irisInAndroid\\iris.data',names=['f1','f2','f3','f4','f5'])\r\n\r\ns=np.asarray([1,0,0])\r\nve=np.asarray([0,1,0])\r\nvi=np.asarray([0,0,1])\r\n\r\ndata['f5']=data['f5'].map({'Iris-setosa':s,'Iris-versicolor':ve,'Iris-virginica':vi})\r\n\r\n\r\ndata=data.iloc[np.random.permutation(len(data))]\r\n\r\nprint(data)\r\n\r\ndata=data.reset_index(drop=True)\r\n\r\ntrainFeats=data.ix[0:105,['f1','f2','f3','f4']]\r\ntemp=data['f5']\r\ntrainlabels=temp[0:106]\r\n\r\ny=tf.placeholder(tf.float32,shape=[None, 3])\r\n\r\nm=tf.Variable(tf.zeros([4,3]))\r\nx=tf.placeholder(tf.float32,shape=[None,4],name=\"Input\")\r\nc=tf.Variable(tf.zeros([3]))\r\n\r\nmxc = tf.nn.softmax((tf.matmul(x, m) + c) ,name=\"output\")\r\n\r\nloss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(mxc), reduction_indices=[1]))\r\n\r\ntrain_step = tf.train.AdamOptimizer(0.01).minimize(loss)\r\n\r\nsess = tf.InteractiveSession()\r\ninit = tf.initialize_all_variables()\r\nsess.run(init)\r\n\r\n\r\n\r\n\r\nepoch=2000\r\nfor step in range(epoch):\r\n  print(sess.run([train_step,loss], feed_dict={x: trainFeats, y:[t for t in trainlabels.as_matrix()]}))\r\n\r\ntestData=data.ix[130,['f1','f2','f3','f4']]\r\ntestDataInFrormat=testData.reshape(1,4)\r\nprint(sess.run(tf.argmax(mxc),feed_dict={x:testDataInFrormat}))\r\n\r\ntf.train.write_graph(sess.graph_def,'pbtxtFiles/','savegraph.pbtxt',as_text=True)\r\n\r\ntf.train.Saver().save(sess,'pbtxtFiles/model.ckpt')\r\n\r\nMODEL_NAME = 'iris'\r\ninput_graph_path = 'pbtxtFiles/savegraph.pbtxt'\r\ncheckpoint_path = 'pbtxtFiles/model.ckpt'\r\ninput_saver_def_path = \"\"\r\ninput_binary = False\r\noutput_node_names = \"output\"\r\nrestore_op_name = \"save/restore_all\"\r\nfilename_tensor_name = \"save/Const:0\"\r\noutput_frozen_graph_name = 'pbtxtFiles/frozen_model_'+MODEL_NAME+'.pb'\r\noutput_optimized_graph_name = 'pbtxtFiles/optimized_inference_model_'+MODEL_NAME+'.pb'\r\nclear_devices = True\r\n\r\nfreeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\r\n                          input_binary, checkpoint_path, output_node_names,\r\n                          restore_op_name, filename_tensor_name,\r\n                          output_frozen_graph_name, clear_devices, \"\")\r\n\r\n\r\noutput_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n        sess.graph_def,\r\n        [\"Input\"], # an array of the input node(s)\r\n        [\"output\"], # an array of output nodes\r\n        tf.float32.as_datatype_enum)\r\n\r\ntflite_model=tf.contrib.lite.toco_convert(sess.graph_def,[x],[mxc])\r\nopen(\"wow.tflite\",\"w\").write(tflite_model)\r\n@leandroBorgesFerreira @aselle @angersson @ry @jmhodges @a-dai @a-doumoulakis @a7744hsc @aam-at @b-3-n @b0noI @bafu @bakunyo @c0g @caisq @callofdutyops @calpa @cameronphchen \r\nam getting .pb abd .ckpt", "my tensorflow version is 1.8.0,but still show this error ,\r\nmodule 'tensorflow.contrib' has no attribute 'lite',\r\nhave no idea...", "@jizhidezhaojun same with me.\r\nDid you handle the issue ?", "@jizhidezhaojun @naelabdeljawad doing a pip install of tensorflow-gpu-1.9.0rc1 worked for me! And following these [instructions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md):\r\n\r\nExample:\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(...)\r\n```", "2018-07-30 07:22:46.477633: F tensorflow/contrib/lite/toco/import_tensorflow.cc:2020] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\r\nAborted (core dumped)\r\n\r\nNone\r\n\r\nI have tensorflow version 1.9 and i do have tf_nightly installed . so when i convert keras model file to tflite i get this error !!.", "I've got the same issue: AttributeError: module 'tensorflow.contrib' has no attribute 'lite'.\r\nTensorflow-gpu 1.10 installed from pip 18.0\r\nWindows 10\r\nPython 3.6.\r\n\r\nDoes anybody have a solution?", "I am having the same issue as @DmitrySolntsev as well.", "I believe this is fixed in master or 0.11 RC. Please give that a try.\n-A\n\n\nOn Mon, Sep 10, 2018 at 7:30 AM Lionellye97 <notifications@github.com>\nwrote:\n\n> I am having the same issue as @DmitrySolntsev\n> <https://github.com/DmitrySolntsev> as well.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15401#issuecomment-419933748>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAT52sEEkx609qUG_fW-4HJLCR_7iAQwks5uZneOgaJpZM4RD0Hc>\n> .\n>\n", "I've tried with 1.11.0-rc1 and no longer get the 'AttributeError' but instead get the following error: \r\nImportError: No module named _tensorflow_wrap_toco\r\nAny ideas? ", "I also got the same issue, but got it fixed by installing tf_nightly using \"pip install tf_nightly\"\r\nPlease be sure that while installing, there shouldnt be any active python sessions or else the install will fail and the import of even the tensorflow will get u errors. \r\nclose all active python sessions and install tf_nightly. ", "at tensorflow 1.9.0 souce code, I read this code tensorflow\\contrib\\__init__.py\r\nif os.name != \"nt\":\r\n  from tensorflow.contrib.lite.python import lite\r\nit means tensorflow 1.9.0 not support lite for window;\r\nyou must update tensorflow version to 1.11.0 or use linux system to install tensorflow", "update TF.", "TF version = 1.12\r\ntfnightly = 1.13.0.dev20190104\r\npython 3.5\r\nModel [ ssd_mobilenet_v1_coco_2018_01_28](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz)\r\n\r\n````\r\nimport tensorflow as tf\r\nimport sys\r\ndef convert(frozen):\r\n\tgraph_def_file = frozen+\"/frozen_inference_graph.pb\"\r\n\tinput_arrays = [\"image_tensor\"]\r\n\toutput_arrays = [\"detection_boxes\",\"detection_scores\",\"detection_classes\",\"num_detections\"]\r\n\tconverter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(graph_def_file,input_arrays,output_arrays)#,input_shapes={\"image_tensor\":[None,300,300,3]})\r\n\ttflite_model = converter.convert()\r\n\topen(frozen+\"/model.tflite\", \"wb\").write(tflite_model)\r\n\r\nif __name__=='__main__':\r\n\tfrozen_path = sys.argv[1]\r\n\tconvert(frozen_path)\r\n````\r\n\r\nError\r\n\r\n> 2019-01-07 09:01:08.263004: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-01-07 09:01:08.268020: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300030000 Hz\r\n2019-01-07 09:01:08.268185: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x56b0400 executing computations on platform Host. Devices:\r\n2019-01-07 09:01:08.268255: I tensorflow/compiler/xla/service/service.cc:168]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-01-07 09:01:09.412404: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-01-07 09:01:09.412562: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-01-07 09:01:11.571194: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:584] Optimization results for grappler item: graph_to_optimize\r\n2019-01-07 09:01:11.571245: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   model_pruner: Graph size after: 5957 nodes (-3), 10020 edges (-3), time = 68.272ms.\r\n2019-01-07 09:01:11.571253: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   function_optimizer: Graph size after: 5957 nodes (0), 10020 edges (0), time = 13.564ms.\r\n2019-01-07 09:01:11.571259: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   constant folding: Graph size after: 5958 nodes (1), 10022 edges (2), time = 539.625ms.\r\n2019-01-07 09:01:11.571265: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   shape_optimizer: Graph size after: 5958 nodes (0), 10022 edges (0), time = 31.515ms.\r\n2019-01-07 09:01:11.571271: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   arithmetic_optimizer: Graph size after: 4306 nodes (-1652), 8412 edges (-1610), time = 243.152ms.\r\n2019-01-07 09:01:11.571280: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   loop_optimizer: Graph size after: 4289 nodes (-17), 8380 edges (-32), time = 43.908ms.\r\n2019-01-07 09:01:11.571289: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   dependency_optimizer: Graph size after: 4272 nodes (-17), 8358 edges (-22), time = 103.724ms.\r\n2019-01-07 09:01:11.571299: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   layout: Graph size after: 4272 nodes (0), 8358 edges (0), time = 15.684ms.\r\n2019-01-07 09:01:11.571309: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   memory_optimizer: Graph size after: 4272 nodes (0), 8358 edges (0), time = 263.292ms.\r\n2019-01-07 09:01:11.571319: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   model_pruner: Graph size after: 4258 nodes (-14), 8332 edges (-26), time = 43.246ms.\r\n2019-01-07 09:01:11.571329: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   function_optimizer: Graph size after: 4258 nodes (0), 8332 edges (0), time = 11.187ms.\r\n2019-01-07 09:01:11.571338: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   constant folding: Graph size after: 4258 nodes (0), 8332 edges (0), time = 305.64ms.\r\n2019-01-07 09:01:11.571348: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   shape_optimizer: Graph size after: 4258 nodes (0), 8332 edges (0), time = 22.274ms.\r\n2019-01-07 09:01:11.571358: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   arithmetic_optimizer: Graph size after: 4256 nodes (-2), 8328 edges (-4), time = 192.768ms.\r\n2019-01-07 09:01:11.571367: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586]   dependency_optimizer: Graph size after: 4256 nodes (0), 8328 edges (0), time = 100.01ms.\r\nTraceback (most recent call last):\r\n  File \"export_graph.py\", line 15, in <module>\r\n    convert(frozen_path)\r\n  File \"export_graph.py\", line 10, in convert\r\n    tflite_model = converter.convert()\r\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/lite/python/lite.py\", line 499, in convert\r\n    **converter_kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/lite/python/convert.py\", line 445, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/lite/python/convert.py\", line 208, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-01-07 09:01:12.809603: F tensorflow/core/framework/function.cc:1640] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for ReadVariableOp\r\nAborted (core dumped)", "I met the same problem when I used\r\n`import tensorflow as tf` and \r\n`tf.contrib.lite.TFLiteConverter.from_session(self.sess, [img], [out])`\r\n\r\nthen I tried\r\n`from tensorflow.contrib import lite`  and \r\n`lite.TFLiteConverter.from_session(self.sess, [img], [out])`\r\nand no longer get this problem. But I don't know why.\r\n\r\nI find a phenomena that may be related to causes. Before I solved problem, when I print 'tf.', the automatic matching behind this is 'v contrib', but why this contrib is a variable?", "@umbrellalalalala  It is mainly version issue.. Check https://www.tensorflow.org/lite/convert/python_api for your tensorflow version and use the apt python api", "@jennings1716  Thank you. But my tensorflow version is already 1.12.0.", "```\r\nimport tensorflow; print(tensorflow.__version__)\r\n1.14.0\r\n\r\n'lite' in dir(tf.contrib)\r\nFalse\r\n\r\n'lite' in dir(tf)\r\nTrue\r\n\r\n```\r\n\r\n"]}, {"number": 15400, "title": "setuptools error on upgrading to 1.4.1", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6 (normal install, NO virtualenv, NO anaconda, ...)\r\n- **Exact command to reproduce**: ```sudo pip3 install --upgrade tensorflow```\r\n\r\nWhen upgrading TF 1.4.0 to 1.4.1 using the command above, I get the error message:\r\n```\r\nCould not import setuptools which is required to install from a source distribution.\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/req/req_install.py\", line 387, in setup_py\r\n    import setuptools  # noqa\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/setuptools/__init__.py\", line 10, in <module>\r\n    from setuptools.extern.six.moves import filter, map\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/setuptools/extern/__init__.py\", line 1, in <module>\r\n    from pkg_resources.extern import VendorImporter\r\nModuleNotFoundError: No module named 'pkg_resources.extern'\r\n```\r\n\r\nI have been able to solve the problem with the following command:\r\n\r\n```\r\npip3 install -U setuptools\r\n```\r\nAfter this, running pip3 install --upgrade Tensorflow runs successfully.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Bazel version: irrelevant, not building from source\r\nCUDA/cuDNN version: irrelevant, not using GPU\r\nGPU model and memory: irrelevant, not using", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@gunan are we missing a dependency?", "The error message seems to be during the installation of tensorflow?\r\nUnless @yifeif sees something else in the error, I think this is a pip/setuptools bug, not a tensorflow one as the command the error is seen is a pip command.", "It sounds as if it may make sense to report this to pip/setuptools. Closing here, but please reopen if you think this is a TensorFlow bug."]}, {"number": 15399, "title": "Fix bug: leaky relu supports float64", "body": "Fix #15391.\r\n\r\n### How to test\r\n\r\n+ [x] add test case.\r\n+ [ ] pass all tests.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15398, "title": "Feature request: tf.info to show docstrings in jupyter notebook", "body": "When working with jupyter notebooks, it is really helpful to see the documentation within the notebook. Numpy has a function called `np.info` which takes a numpy function as argument and prints its docstring. For example, np.info(np.mean) prints the docstring for `np.mean`. It would be really helpful to have an equivalent `tf.info` for those of us who work with jupyter notebooks (and who doesn't?).", "comments": ["![1215-13 20 52](https://user-images.githubusercontent.com/1381301/34060943-ce5ca2fe-e19a-11e7-952e-dd2ed18d9152.png)\r\n", "Thanks @ppwwyyxx @dataplayer12 Please reopen if that does not solve your problem."]}, {"number": 15397, "title": "Tensorflow c++ memory leak - Valgrind", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 17.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **GCC/Compiler version (if compiling from source)**: 6.0.3\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Describe the problem**:\r\n\r\n\r\nI am executing simple tensorflow code to create graph def as shown below\r\n\r\n```\r\ntensorflow::NewSession (options, &session)\r\nReadBinaryProto (tensorflow::Env::Default(), \"/home/ashok/eclipseWorkspace/faceRecognition-x86_64/Data/models/optimized_facenet.pb\", &graph_def));\r\nsession->Create (graph_def);\r\n```\r\n\r\nBut when I run Valgrind as shown below\r\n```\r\nvalgrind --leak-check=full --show-leak-kinds=all --vex-guest-max-insns=25 ./faceRecognition-x86_64 -r -i\r\n```\r\n I get below errors\r\n\r\n```\r\n==12366== 16,000 bytes in 1 blocks are still reachable in loss record 47,782 of 47,905\r\n==12366==    at 0x4C2E19F: operator new(unsigned long) (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n==12366==    by 0xBF875DC: std::vector<tensorflow::CostModel::MemUsage, std::allocator<tensorflow::CostModel::MemUsage> >::reserve(unsigned long) (in /usr/lib/libtensorflow_cc.so)\r\n==12366==    by 0xBF90128: tensorflow::CostModel::InitFromGraph(tensorflow::Graph const&) (in /usr/lib/libtensorflow_cc.so)\r\n==12366==    by 0xBEE48D3: tensorflow::SimpleGraphExecutionState::InitBaseGraph(tensorflow::BuildGraphOptions const&) (in /usr/lib/libtensorflow_cc.so)\r\n==12366==    by 0xBEE52CF: tensorflow::SimpleGraphExecutionState::MakeForBaseGraph(tensorflow::GraphDef*, tensorflow::SimpleGraphExecutionStateOptions const&, std::unique_ptr<tensorflow::SimpleGraphExecutionState, std::default_delete<tensorflow::SimpleGraphExecutionState> >*) (in /usr/lib/libtensorflow_cc.so)\r\n==12366==    by 0xBE68B9D: tensorflow::DirectSession::MaybeInitializeExecutionState(tensorflow::GraphDef const&, bool*) (in /usr/lib/libtensorflow_cc.so)\r\n==12366==    by 0xBE68CF9: tensorflow::DirectSession::ExtendLocked(tensorflow::GraphDef const&) (in /usr/lib/libtensorflow_cc.so)\r\n==12366==    by 0xBE68FC7: tensorflow::DirectSession::Create(tensorflow::GraphDef const&) (in /usr/lib/libtensorflow_cc.so)\r\n==12366==    by 0x26B899: TensorFlow::initializeRecognition() (in /home/ashok/eclipseWorkspace/faceRecognition-x86_64/Debug/faceRecognition-x86_64)\r\n==12366==    by 0x24197D: RecognitionWithImages::RecognitionWithImages() (in /home/ashok/eclipseWorkspace/faceRecognition-x86_64/Debug/faceRecognition-x86_64)\r\n==12366==    by 0x12F27C: main (in /home/ashok/eclipseWorkspace/faceRecognition-x86_64/Debug/faceRecognition-x86_64)\r\n\r\n```\r\nThese type of errors are also generated when I do **session -> run ()** \r\n\r\nDue to the above issues, the memory needed to run the program keeps increasing as time passes and the application crashes due to insufficient memory after  a certain point of time.\r\n\r\nI have also posted the issue in stack overflow - https://stackoverflow.com/questions/47834054/tensorflow-c-memory-leak-valgrind", "comments": ["This looks like an issue on your end, as per the answer to the SO question you also posted.\r\n\r\nSince this is not a bug or feature request, I'm closing this issue. If you believe you've found a true bug, please file another issue. Thanks!"]}, {"number": 15395, "title": "OS X + GPU NVIDIA nccl dependency missing", "body": "[Comment](https://github.com/tensorflow/tensorflow/pull/14016#discussion_r156704387) migrated to a full issue here.\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.2 (Any)\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Master @HEAD\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A (CMake)\r\n- **GCC/Compiler version (if compiling from source)**: Xcode 8.3.3 (Any)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: `cmake -GXcode ...`\r\n\r\n### Describe the problem\r\n\r\nI'm trying to extend [CMake-Linux support for GPU-build](https://github.com/tensorflow/tensorflow/pull/14016) to finish CMake + OSX testing w/ (NVIDIA) GPU.   I had started with CUDA 8.0 and cuDNN 6.0, but given recent activity here, it seems better to target a recent master branch w/ CUDA 9.0 + cuDNN 7.0.  The OS X CUDA installation does not come with `nccl` (see `find` output below for contents).  NVIDIA has a (login based) `nccl` download for v2.0, but it only has Linux libraries.  I see three other options to support OS X + GPU Builds:\r\n\r\n1. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/nccl (build from bundled source v1.3.5)\r\n2. https://github.com/nvidia/nccl (build from source v1.?)\r\n3. add `option(tensroflow_USE_NCCL \"Use NCCL lib\" ON)` to make it `nccl` optional (I'm not sure how complicated this is) and update the CMake and source code w/ preprocessor definitions as needed\r\n\r\nShall I add the bundled `nccl` (v1.3.5?) files to the CMake build?  I see notes about migrating to v2.0 in commit comments, which seems to be closed source.  If that's the case, then it may be best to try to make any internal tensorflow `nccl` operations optional in order to support single GPU configurations to start with. \r\n\r\n`find /Developer/NVIDIA/CUDA-9.0/lib -name \"*.a\"`\r\n```\r\n/Developer/NVIDIA/CUDA-9.0/lib/libcublas_device.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libcublas_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libcudadevrt.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libcudart_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libcufft_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libcufftw_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libculibos.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libcurand_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libcusolver_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libcusparse_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppc_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppial_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppicc_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppicom_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppidei_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppif_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppig_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppim_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppist_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppisu_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnppitc_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnpps_static.a\r\n/Developer/NVIDIA/CUDA-9.0/lib/libnvgraph_static.a\r\n```\r\n\r\n### Source code / logs\r\n\r\nN/A\r\n", "comments": ["We don't officially support OSX + Nvidia, so I'm marking this as community support.\r\n\r\nPlease feel free to put together a PR! ", "I will take a look.  Do you know which version of nccl is currently required for master?", "I'm unfamiliar with the OSX build process myself, so I don't know, no.", "Closing this issue due to staleness. Please use the latest version of TensorFlow and build again.\r\nFeel free to report any issues you encounter with latest TensorFlow. Thanks!"]}, {"number": 15394, "title": "Fix typo of tf.abs docstring", "body": "Change \"'\", which causes incorrect highlight, to \"`\"\r\n\r\nAnd by the way, this is a typo I found when checking whether #9827 was resolved. The answer seems to be yes and we can close #9827 now.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15393, "title": "Build tf_core_kernels on With VS2015 stops with \"fatal error C1060: compiler is out of heap space\"", "body": "### System information\r\n- Build tf_core_kernels.vcxproj\r\n- Windows 10 Enterprise Build 1607\r\n- TensorFlow installed from source\r\n- TensorFlow version Release 1.4.0 from github\r\n- VS2015 Enterprise 14.0.25431 Update 3 \r\n\r\nCannot build tf_core_kernels.vcxproj (generated with CMake) because of a fatal error C1060: compiler is out of heap space. \r\n\r\n### Source code / logs\r\n1>------ Build started: Project: tf_core_kernels, Configuration: RelWithDebInfo x64 ------\r\n1>  training_ops.cc\r\n1>e:\\software\\tensorflow\\tensorflow\\contrib\\cmake\\build\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/Tensor/TensorBase.h(245): fatal error C1060: compiler is out of heap space\r\n1>  INTERNAL COMPILER ERROR in 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\x86_amd64\\CL.exe'\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: No\r\nOS Platform and Distribution: Windows 10 Enterprise Build 1607\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: \r\nFollowed instructions here:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake \r\nand then:\r\nMSBuild /p:Configuration=RelWithDebInfo ALL_BUILD.vcxproj", "I hit this out of heap space error before as well, this prones to happen when you don't have enough RAM.\r\n\r\nA workaround is to remove all `/MP`compile flag to make `cl.exe` create less processes and hence use less memory (works for me).\r\n\r\n- https://github.com/tensorflow/tensorflow/search?l=CMake&q=%2FMP&type=&utf8=%E2%9C%93\r\n- https://msdn.microsoft.com/en-us/library/bb385193.aspx", "If may because of two things:\r\n1. you don't have enough memory.\r\n\r\n2. you are using 32 bits toolchains.\r\n   Please upgrade cmake to 3.10.x and add \"-T host=x64\" to cmake command line args.\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "I compiled manually one file at a time and then it worked. But compiling the whole project is still an issue.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "In the [install instruction from tensorflow](https://www.tensorflow.org/install/source_windows), it's already been mentioned.\r\n `Building TensorFlow from source can use a lot of RAM. If your system is memory-constrained, limit Bazel's RAM usage with: --local_resources 2048,.5,1.0.`"]}, {"number": 15392, "title": "build_all_android.sh does not work on TF 1.4 branch", "body": "Run `build_all_android.sh` on the `1.4` branch and it will fail with:\r\n\r\n    cp: <PATH>/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public: No such file or directory", "comments": ["It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Seems like the `compile_nsync.sh` script failed.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@eaigner Does it work now?"]}, {"number": 15391, "title": "tf.nn.leaky_relu does not work with float64", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 26\r\n- **TensorFlow installed from (source or binary)**: binary, pip3 install tensorflow\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: Python 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: Doesn't apply\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\n\r\n`tf.nn.leaky_relu` does not work with float64. It only seems to work with float32. I can't think of a reason why it should not work with float64, so I consider this a bug, your mileage might vary. I'm also not familiar enough with tf code to fix it myself without any help. :-)\r\n\r\nIt seems at least [ops.py#L926](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/framework/ops.py#L926) should be in a `try` block just as [ops.py#L912](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/framework/ops.py#L912)\r\n\r\nAlso the unit tests only test float32.\r\n\r\n### Source code / logs\r\n\r\n```\r\nIn [1]: import tensorflow as tf\r\n/usr/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n\r\nIn [2]: c = tf.constant(5.0, dtype=tf.float32)\r\n\r\nIn [3]: lr = tf.nn.leaky_relu(c)\r\n\r\nIn [4]: c = tf.constant(5.0, dtype=tf.float64)\r\n\r\nIn [5]: lr = tf.nn.leaky_relu(c)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-71e83d060e7b> in <module>()\r\n----> 1 lr = tf.nn.leaky_relu(c)\r\n\r\n~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in leaky_relu(features, alpha, name)\r\n   1541     features = ops.convert_to_tensor(features, name=\"features\")\r\n   1542     alpha = ops.convert_to_tensor(alpha, name=\"alpha\")\r\n-> 1543     return math_ops.maximum(alpha * features, features)\r\n   1544 \r\n   1545 \r\n\r\n~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\r\n    883       if not isinstance(y, sparse_tensor.SparseTensor):\r\n    884         try:\r\n--> 885           y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\r\n    886         except TypeError:\r\n    887           # If the RHS is not a tensor, it might be a tensor aware object\r\n\r\n~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n    834       name=name,\r\n    835       preferred_dtype=preferred_dtype,\r\n--> 836       as_ref=False)\r\n    837 \r\n    838 \r\n\r\n~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n    924 \r\n    925     if ret is None:\r\n--> 926       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    927 \r\n    928     if ret is NotImplemented:\r\n\r\n~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)\r\n    772     raise ValueError(\r\n    773         \"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\" %\r\n--> 774         (dtype.name, t.dtype.name, str(t)))\r\n    775   return t\r\n    776 \r\n\r\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype float64: 'Tensor(\"Const_1:0\", shape=(), dtype=float64)'\r\n```", "comments": ["Hi, @jhzab . Perhaps the workaround works for you.\r\n\r\n```python\r\nimport tensorflow as tf\r\nc = tf.constant(5.0, dtype=tf.float64)\r\nalpha=tf.constant(0.2, dtype=tf.float64)\r\nlr = tf.nn.leaky_relu(c, alpha)\r\n```", "Ah, I didn't even consider that the `alpha` parameter might be causing the problem when giving a `float64` input. I was using my own leaky_relu implementation in the mean time, but your suggestions seems cleaner, thanks.\r\n\r\nI would have expected `alpha * features` to return a float64, since `alpha` is not set to some specific dtype. Am I missing something?", "`alpha` is converted to `float32` by default, I believe that's why dtype conflicts here. Yes, I agree that it's better to return a float64 for `alpha * features`, rather than raise those annoying message for user.", "Since @facaiy is looking at this, I'll tag it as contributions welcome.\r\n\r\nThanks @facaiy!"]}, {"number": 15390, "title": "Update C API test data comparison for s390x", "body": "Base64 encode/decode is not endian-dependent. The hash passed to Base64 will generate different string on big-endian platform. Correcting the test data comparison for s390x architecture. ", "comments": ["Can one of the admins verify this patch?", "@iganichev Thank you for your review comments. Code changes incorporated.", "Jenkins, test this please."]}, {"number": 15389, "title": "Fatal error while compiling Tensorflow with CUDA 9.1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04.3 LTS (GNU/Linux 4.4.0-104-generic x86_64)\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nunknown 1.4.0 (Source code is cloned from 798fa36d11119e6fdc13b90a14abfe1805e7de90)\r\n- **Python version**: \r\n3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n0.8.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.1\r\ncuDNN 7.0.5\r\n- **GPU model and memory**:\r\n2 * Tesla V100-PCIE-16GB\r\n- **Exact command to reproduce**:\r\nSee description below.\r\n\r\n### Describe the problem\r\nWhile trying to compile the latest TensorFlow(cloned from 798fa36d11119e6fdc13b90a14abfe1805e7de90), such error will be raised:\r\n```\r\nERROR: /home/ubuntu/tensorflow/tensorflow/contrib/seq2seq/BUILD:64:1: error while parsing .d file: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/seq2seq/_objs/python/ops/_beam_search_ops_gpu/tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.pic.d (No such file or directory)\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:0,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/contrib/seq2seq/kernels/beam_search_ops.h:19,\r\n                 from tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc:20:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:59:34: fatal error: math_functions.hpp: No such file or directory\r\n```\r\n\r\nIt turns out that in CUDA 9.1, `math_functions.hpp` is located at `cuda/include/crt/math_functions.hpp`, rather than `cuda/include/math_functions.hpp` (CUDA 9.0 does), which leads to this error.\r\n`ln -s /usr/local/cuda/include/crt/math_functions.hpp /usr/local/cuda/include/math_functions.hpp` will fix this problem and complete the compiling process.\r\n\r\n#### Reference\r\nhttps://stackoverflow.com/a/47807106/2666624\r\n\r\n### Source code / logs\r\nTraceback is available above.\r\n", "comments": ["I'll try to open a PR resolving this problem, after digging some source code.", "I can confirm it's a bug of [Eigen](http://eigen.tuxfamily.org/).\r\nSuch error will be raised while compiling Eigen ([commit](https://bitbucket.org/eigen/eigen/src/5c43a259f0b51963a00dd85e941a1fd044ac1cf4)) with `cmake .. -DEIGEN_TEST_CUDA=ON && make check -j4`:\r\n```\r\nIn file included from /home/ubuntu/eigen-eigen-5c43a259f0b5/Eigen/QR:11:0,\r\n                 from /home/ubuntu/eigen-eigen-5c43a259f0b5/test/main.h:272,\r\n                 from /home/ubuntu/eigen-eigen-5c43a259f0b5/test/cuda_basic.cu:23:\r\n/home/ubuntu/eigen-eigen-5c43a259f0b5/Eigen/Core:59:34: fatal error: math_functions.hpp: No such file or directory\r\n```\r\n", "https://stackoverflow.com/questions/43113508/math-functions-hpp-not-found-when-using-cuda-with-eigen --> second answer seems to be related", "@AlessioTonioni Yes, and I've already referenced this answer above.", "Can you provide an exact command to help replicate this?", "Also discussed in https://github.com/tensorflow/tensorflow/issues/15140", "@angersson The command is exactly what this [doc](https://www.tensorflow.org/install/install_sources) says:\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package `", "It's a known issue. I can reproduce it under Fedora 25 and CUDA 9.1.\r\n", "/CC @nluehr can you fix this in Eigen?", "Issued [PR](https://bitbucket.org/eigen/eigen/pull-requests/353) to Eigen which should resolve this issue.", "Make a softlink from /usr/local/cuda-9.1/include/crt/math_functions.hpp to /usr/local/cuda-9.1/include/math_functions.hpp can solve the problem for me.\r\n\r\n-- update --\r\nThanks for @icyblade 's reminder that this method has already been mentioned above.", "@JamesTheZ Which has already been mentioned above, twice.", "Visit http://www.python36.com/install-tensorflow141-gpu/ for step by step instructions to build tensorflow from source for cuda 9.1 and cudnn 7.0.5 on ubuntu x64.", "There are multiple fixes available for this issue. Eigen sources after hash 034b6c3 remove the direct reference to math_functions.hpp. So bumping the Eigen version is one fix. Also, TF PR #15739 (which has now been merged into master) works around the problem by adding the cuda/inlcude/crt sub-directory to INCLUDES.\r\n", "The `v1.5.0-rc0` release doesn't seems to include a fix before doing the release. I hope they'll add a fix for the next release candidate.", "/CC @gunan, is it possible for the fix to be included in 1.5?", "This is due to eigen.\r\nCorrect fix is here: https://github.com/tensorflow/tensorflow/pull/15796\r\n\r\n@av8ramit Once the PR above is merged can we cherrypick it to the release (just the 2nd commit in the PR is sufficient).", "I've merged it. @gunan sure, it will be in rc1", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "So, which version of eigen to use?\r\n"]}, {"number": 15388, "title": "cannot install with virtualenv python3.6", "body": "Have I written custom code      No\r\nOS Platform and Distribution .  MacOS 10.13.2\r\nTensorFlow installed from        conda  URL of the TensorFlow Python package\r\nTensorFlow version .                1.4\r\nBazel version                             N/A\r\nCUDA/cuDNN version              N/A\r\nGPU model and memory          N/A\r\nExact command to reproduce  I just follow Install with conda, My python was installed with conda, I think maybe that is the problem.\r\n\r\n\r\n\r\nvirtualenv --system-site-packages -p python3 ~/tensorflow\r\nRunning virtualenv with interpreter /usr/local/bin/python3\r\nUsing base prefix '/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6'\r\nNew python executable in /Users/xinhai/tensorflow/bin/python3.6\r\nAlso creating executable in /Users/xinhai/tensorflow/bin/python\r\nInstalling setuptools, pip, wheel...\r\n  Complete output from command /Users/xinhai/tensorflow/bin/python3.6 - setuptools pip wheel:\r\n  stringstringstringstringstringstringstringstring\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 7, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/__init__.py\", line 5, in <module>\r\n  File \"/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/logging/__init__.py\", line 28, in <module>\r\n    from string import Template\r\nImportError: cannot import name 'Template'\r\n\r\n\r\n\r\n...Installing setuptools, pip, wheel...done.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/site-packages/virtualenv.py\", line 2328, in <module>\r\n    main()\r\n  File \"/usr/local/lib/python2.7/site-packages/virtualenv.py\", line 713, in main\r\n    symlink=options.symlink)\r\n  File \"/usr/local/lib/python2.7/site-packages/virtualenv.py\", line 945, in create_environment\r\n    download=download,\r\n  File \"/usr/local/lib/python2.7/site-packages/virtualenv.py\", line 901, in install_wheel\r\n    call_subprocess(cmd, show_stdout=False, extra_env=env, stdin=SCRIPT)\r\n  File \"/usr/local/lib/python2.7/site-packages/virtualenv.py\", line 797, in call_subprocess\r\n    % (cmd_desc, proc.returncode))\r\nOSError: Command /Users/xinhai/tensorflow/bin/python3.6 - setuptools pip wheel failed with error code 1", "comments": ["I'm confused as to what here is TensorFlow related. Do you already have TF pip-installed into your system site packages?\r\n", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code No\r\nOS Platform and Distribution . MacOS 10.13.2\r\nTensorFlow installed from conda URL of the TensorFlow Python package\r\nTensorFlow version . 1.4\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce I just follow Install with conda, My python was installed with conda, I think maybe that is the problem.", ".... but...\r\nThis command you report using and having an issue with\r\n```\r\nvirtualenv --system-site-packages -p python3 ~/tensorflow\r\n```\r\nhas **nothing** to do with TensorFlow. Why are you reporting this issue here?"]}, {"number": 15387, "title": "from pb tf_dst_dtype == DT_UINT8 || tf_dst_dtype == DT_INT32 || tf_dst_dtype == DT_FLOAT  Abort trap: 6", "body": "cambridgedeMBP:tensorflow-master cambridge$ bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n>   --input_file=/Users/cambridge/Desktop/test/my_inception_v4_freeze.pb \\\r\n>   --output_file=/Users/cambridge/Desktop/test/my_inception_v4_freeze.pb.lite \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF \\\r\n>   --output_format=TFLITE \\\r\n>   --inference_type=FLOAT \\\r\n>   --input_shapes=1,299,299,3 \\\r\n>   --input_arrays=input \\\r\n>   --output_array=InceptionV4/Logits/Predictions\r\n2017-12-15 16:18:25.486891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:140] Unsupported data type in placehoder op: 7\r\n2017-12-15 16:18:25.487509: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: DecodeJpeg\r\n2017-12-15 16:18:25.487725: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: StridedSlice\r\n2017-12-15 16:18:25.487772: F tensorflow/contrib/lite/toco/import_tensorflow.cc:1160] Check failed: tf_dst_dtype == DT_UINT8 || tf_dst_dtype == DT_INT32 || tf_dst_dtype == DT_FLOAT \r\nAbort trap: 6", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 15386, "title": "Fix api usage in examples of GAN", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15385, "title": "Silence warning of graph_replace", "body": "This PR fixes #8661 \r\nThe warning is caused by access to deprecated graph key `VARIABLES`.\r\n\r\nUpdate:\r\nBecause it is hard to distinguish the deprecated from other graph keys, this fix is somewhat ad-hoc and should be removed after graph key `VARIABLES` is removed.", "comments": ["Can one of the admins verify this patch?", "Nagging Reviewer @purpledog: It has been 113 days with no activity and the `awaiting review` label was assigned. Can you please take a look?"]}, {"number": 15384, "title": "Can't convert nasnet pb to tflite: Pad not supported", "body": "cambridgedeMBP:tensorflow-master cambridge$ bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n>   --input_file=/Users/cambridge/Desktop/test/nasnet_mobile_freeze.pb \\\r\n>   --output_file=/Users/cambridge/Desktop/test/nasnet_mobile_freeze.lite \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF \\\r\n>   --output_format=TFLITE \\\r\n>   --inference_type=FLOAT \\\r\n>   --input_shapes=1,224,224,3 \\\r\n>   --input_arrays=input \\\r\n>   --output_array=final_layer/predictions\r\n2017-12-15 14:18:22.406487: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39 \r\n\r\n] Before general graph transformations: 2988 operators, 4360 arrays (0 quantized)\r\n2017-12-15 14:18:22.659788: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39 \r\n\r\n] After general graph transformations pass 1: 663 operators, 1419 arrays (0 quantized)\r\n2017-12-15 14:18:22.673152: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39 \r\n\r\n] Before dequantization graph transformations: 663 operators, 1419 arrays (0 quantized)\r\n2017-12-15 14:18:22.678500: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312 \r\n\r\n] Total transient array allocated size: 4938176 bytes, theoretical optimal value: 3182720 bytes.\r\n2017-12-15 14:18:22.685171: I tensorflow/contrib/lite/toco/toco_tooling.cc:264 \r\n\r\n] Estimated count of arithmetic ops: 1.14687 billion (note that a multiply-add is counted as 2 ops).\r\n2017-12-15 14:18:22.691550: F tensorflow/contrib/lite/toco/tflite/export.cc:192 \r\n\r\n] Unsupported operator: Pad\r\nAbort trap: 6", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Any update or solution? ", "Please try again. Pad is supported now. Let us know how it goes.", "Closing per @andrehentz's comment. If this is still an issue, let us know and we'll reopen."]}, {"number": 15383, "title": "Feature request: Use placeholders to specify the inputs of TFGAN model.", "body": "", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler \r\nThanks, but this is a feature request, not a  bug. Do I still need to fill out these fields?", "Are plans to implement this? It would simplify the model, because currently it is very difficult to manipulate with a saved a graph without placeholders for inputs.\r\n\r\nFor example, currently there are two different discriminators created (https://github.com/tensorflow/tensorflow/issues/15271#issuecomment-350994299) - one for real and one for fake data.\r\n\r\nWhen I want to save the model and use the trained discriminator later with custom data, I have to create a third discriminator before saving with a placeholder as follows:\r\n\r\n```\r\ndisc_in = tf.placeholder(tf.float32, name='disc_in', shape=(None, 28, 28, 1))\r\nwith tf.variable_scope(gan_model.discriminator_scope, reuse=True):\r\n    gan_model.discriminator_and_aux_fn(disc_in, None)\r\n```\r\n\r\nSo I end up with 3 different discriminators in the model. If TFGAN would use placeholders, we would need only one discriminator in the model.", "@petertulala \r\n\r\nDo you have the example how to use input from placeholder?"]}, {"number": 15382, "title": "CMake: optionally link to ZLIB as systemlib / shared objects.", "body": "If the user has ZLIB (and devel pkg) installed at the system\r\nand the user wants to keep using that ZLIB for tensorflow,\r\nthe cmake option \"-Dsystemlib_ZLIB=ON\" will allow to do so.\r\n\r\nAnother option \"-Dsystemlib_ALL=ON\" will turn on every\r\n\"systemlib_*\" options.\r\n\r\nThis requires PR #15381 because this exposes the need for -fPIC from png.\r\n\r\nThis PR is an implementation suggestion to the proposal #13061\r\n\r\nSigned-off-by: MyungJoo Ham <myungjoo.ham@samsung.com>", "comments": ["Can one of the admins verify this patch?", "@myungjoo Please resolve conflicts and take a look at the test failures.", "> Please resolve conflicts and take a look at the test failures.\r\n\r\n@rmlarsen Yes, I will resolve that.\r\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Ping @myungjoo \r\nOnce the conflicts are resolved I can merge this change.", "@gunan Rebased."]}, {"number": 15381, "title": "CMake: fix -fPIC control.", "body": "if / endif cannot be stated inside.\r\nUse tensorflow_ENABLE_POSITION_INDEPENDENT_CODE itself\r\nto express ON or OFF; it is already set as ON or OFF by\r\ntensorflow/contrib/cmake/CMakeLists.txt\r\n\r\nFixes #15380\r\n\r\nSigned-off-by: MyungJoo Ham <myungjoo.ham@samsung.com>", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15380, "title": "CMake: external package: PIC option not working", "body": "In many of external cmake files (```/tensorflow/contrib/cmake/external/*.cmake```), it tries to turn PIC on and off with:\r\n```\r\n      CMAKE_CACHE_ARGS\r\n                if(tensorflow_ENABLE_POSITION_INDEPENDENT_CODE)\r\n                        -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON\r\n                else()\r\n                        -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=OFF\r\n                endif()\r\n```\r\n\r\nHowever, the result (with cmake 3.5) is not what we wanted in CMakeCache:\r\n```\r\nCMAKE_POSITION_INDEPENDENT_CODE:BOOL=OFF; endif();\r\n```\r\nWhere it should be\r\n```\r\nCMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON\r\n```\r\nbecause ```tensorflow_ENABLE_POSITION_INDEPENDENT_CODE``` is ON.\r\n\r\nThis can be corrected by:\r\n```\r\n  -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=${tensorflow_ENABLE_POSITION_INDEPENDENT_CODE}\r\n```\r\n\r\nI'll send a PR after some testing along with other fixes (e.g., linking to ZLIB as shared object, not static linking)\r\n\r\n---- update requested from @tensorflowbutler ----\r\nHave I written custom code: No.\r\nOS Platform and Distribution: PR-tested at Ubuntu-16.04 and Tizen\r\nTensorFlow installed from: N/A\r\nTensorFlow version: github master branch of last week: 00f8b97fc601381546aea89315dee549bdbbbdfc\r\nBazel version: N/A (doing it without bazel)\r\nCUDA/cuDNN version: Tizen: 9/7 / Ubuntu: 8/6\r\nGPU model and memory: Titan Xp\r\nExact command to reproduce: N/A (it is about build)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "> Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\n\r\nHave I written custom code: No. / PR #15381 will fix this.\r\nOS Platform and Distribution: PR-tested at Ubuntu-16.04 and Tizen\r\nTensorFlow installed from: N/A (downloaded source only. built locally)\r\nTensorFlow version: github master branch of last week: 00f8b97\r\nBazel version: N/A (doing it without bazel)\r\nCUDA/cuDNN version: Tizen: 9/7 / Ubuntu: 8/6\r\nGPU model and memory: Titan Xp\r\nExact command to reproduce: N/A (it is about build)", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler \r\n> Nagging Awaiting Response: It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue?\r\n\r\nYes, this is still an issue unless #15381 is applied. You can fix the issue with #15381 or any similar changes; there are some syntax errors in the CMake scripts.\r\n", "The original poster has replied to this issue after the stat:awaiting response label was applied."]}, {"number": 15379, "title": "Only emit PTX code for the most recent CUDA compute architecture", "body": "When there is no gap in the compute architectures you are compiling\r\nagainst, there is no point in generating PTX for all architectures but\r\nthe latest.\r\n\r\nSee the CUDA documentation:\r\nhttp://docs.nvidia.com/cuda/volta-compatibility-guide/index.html#building-volta-compatible-apps-using-cuda-9-0\r\n\r\nSigned-off-by: Felix Abecassis <fabecassis@nvidia.com>\r\n\r\nSlightly decreases the size of the generated binaries.", "comments": ["Can one of the admins verify this patch?", "@zheng-xq @tfboyd WDYT?\r\nI think we included all this architectures with recommendation from NVIDIA, but XQ or Toby may have more background information on this.", "@benbarsdell @cliffwoolley were OK with this change.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Thank you to Alfred for the reminder.  I will try to look today and check with XQ and get back.  I want to triple verify due to slow startup times in the past that we resolved and even if just paranoid I want to be sure.", "@tfboyd any updates?\r\nCC @zheng-xq to check.", "I still own this and we need to add compute capability 7.  I will try to sort this out as part of that problem/need.", "@flx42 \r\n\r\nRunning a test now.  Question:  If there is a gap does your change create a problem?  I could and likely will test but it is a lot of effort to create all the builds.  Say I did 3.7 and 7.0, what would happen?  Amusingly I would do this on AWS as K80s and V100s are my target.  ", "Yes, in this case there will be an issue at runtime: there will be no code for your GPU compute architecture and no PTX to JIT.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I am removing myself.  I did some internal testing and opened bugs for @gunan .  We did get 7.0 added to the default builds.  I think we cannot accept this change because our compute versions are not guaranteed to be contiguous.  One solution looking at the build code is to look to see if the compute capabilities are contiguous and execute one path and if there is a break execute the other path.  I opened an issue asking the team to evaluate the size difference of doing them all contiguous vs. picking just the ones we wanted. Meaning is picking more expensive then doing them all correctly.", "Based on the last comment by @tfboyd, I will close this PR.\r\nSorry it took this long to finalize this @flx42 "]}, {"number": 15378, "title": "Fix PEP8", "body": "This PR fixes pep8-related issues.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15377, "title": "Add description for new PR workflow.", "body": "", "comments": []}, {"number": 15376, "title": "With tf-nightly-gpu, getting error: ImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: `tf-nightly-gpu` -- I can't import TensorFlow to check the version\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: Cuda 9.0, cuDNN 7.0.4\r\n- **GPU model and memory**: GTX 1080 8GB\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nvirtualenv --system-site-packages ~/tftest\r\nsource ~/tftest/bin/activate\r\npip install tf-nightly-gpu\r\npython -c 'import tensorflow'\r\n```\r\n\r\n### Describe the problem\r\nWhen I run the commands above, I get the following error:\r\n```\r\n(tftest) reedwm@reedwm2:~$ python -c 'import tensorflow'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/buildtools/current/sitecustomize/sitecustomize.py\", line 152, in SetupPathsAndImport\r\n    return real_import(name, globals, locals, fromlist, level)\r\n  File \"/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 73, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found (required by /usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nThis occurs if I `pip install tf-nightly-gpu==1.5.0.dev20171212`, which is the earliest version of `tf-nightly-gpu` it occurs on. When I pip install the previous version with `pip install tf-nightly-gpu==1.5.0.dev20171207`, the issue does not occur.\r\n\r\nThis issue is similar to #53 and #3127.\r\n\r\n/CC @gunan @jhseu @martinwicke, any ideas what the issue could be?\r\n", "comments": ["@jhseu I think this is caused by our nightly and release GPU builds having to move to ubuntu 16, because the there are no more ubuntu 14 docker images with cuda9.\r\n\r\nMaybe our builds are missing this flag: `--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"`", "I think the issue is that ubuntu 14.04 has a lower version of GLIBC. When build moves to ubuntu 16.04, some of the GLIBC symbols that is linked on is not available for GLIBC in 14.04.\r\n\r\nThe easiest path for the fix might be to upgrade from 14.04 to ubuntu 16.04.\r\n\r\nOtherwise I think you can try alternatives:\r\n1. Upgrade or install newer version of glibc on 14.04 (may typically cause some other issues on other software in your system, as glibc has a much bigger impact)\r\n2. Recompile TensorFlow from source on Ubuntu 14.04.\r\n3. Use docker to run inside the container.\r\n\r\nAlso if the build is run on Ubuntu 16.04 maybe the supported platform for binary install could be updated.\r\n\r\n", "@gunan I don't think that flag will work here. I think we'll just have to require Ubuntu 16.04 as the minimum for our builds.", "I have the same error on RedHat7.2. Any suggestion to fix it?", "Unfortunately your current workaround is to build from sources.\r\n#15458 should fix this, but we are blocked from merging that PR right now.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing since the pre-built binaries no longer support Ubuntu 14.04."]}, {"number": 15375, "title": "Performance  problem TF VS Keras", "body": "Hello , \r\nI just got huge difference in results using Keras (Back-end TensorFlow) and TensorFlow. I want to know if the difference in performances is normal .\r\n\r\nThe keras model produces a loss of 0.2\r\n`model = k.models.Sequential()\r\nmodel.add(k.layers.convolutional.Conv2D(64, kernel_size=(3,3), input_shape=(75,75,3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.convolutional.MaxPooling2D(pool_size=(3,3), strides=(2,2)))\r\nmodel.add(k.layers.Dropout(0.2))\r\nmodel.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\nmodel.add(k.layers.Dropout(0.2))\r\nmodel.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\nmodel.add(k.layers.Dropout(0.3))\r\nmodel.add(k.layers.convolutional.Conv2D(64, kernel_size=(3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\nmodel.add(k.layers.Dropout(0.3))\r\nmodel.add(k.layers.Flatten())\r\nmodel.add(k.layers.Dense(512))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.Dropout(0.2))\r\nmodel.add(k.layers.Dense(256))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.Dropout(0.2))\r\nmodel.add(k.layers.Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\nmypotim=Adam(lr=0.01, decay=0.0)\r\nmodel.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])`\r\n\r\nThe TensorFlow normal model produces 0.7 :\r\n\r\n`x = tf.placeholder(tf.float32, [None, 75,75,3], name=\"DNN_Input\") \r\nlearningRateIn= tf.placeholder(tf.float32)\r\nkeep_prob = tf.placeholder(tf.float32)\r\nisTrainPlace=tf.placeholder(tf.bool)\r\nwith tf.name_scope('conv_1'):  \r\n    conv_1=tf.layers.conv2d(x,64,[3,3],activation=tf.nn.relu)\r\n    batch_n1 = tf.contrib.layers.batch_norm(conv_1,center=True, scale=True, is_training=isTrainPlace, scope='bn1')\r\n    mpool_1=tf.layers.max_pooling2d(batch_n1,pool_size=(2,2),strides=(2,2))\r\n    dropout_1=tf.layers.dropout(mpool_1,rate=0.8,training=isTrainPlace)\r\nwith tf.name_scope('conv_2'):      \r\n    conv_2=tf.layers.conv2d(dropout_1,128,[3,3],activation=tf.nn.relu)\r\n    batch_n2 = tf.contrib.layers.batch_norm(conv_2, center=True, scale=True,is_training=isTrainPlace,scope='bn2')\r\n    mpool_2=tf.layers.max_pooling2d(batch_n2,pool_size=(2,2),strides=(2,2))\r\n    dropout_2=tf.layers.dropout(mpool_2,rate=0.8,training=isTrainPlace)\r\nwith tf.name_scope('conv_3'):      \r\n    conv_3=tf.layers.conv2d(dropout_2,128,[3,3],activation=tf.nn.relu)\r\n    batch_n3 = tf.contrib.layers.batch_norm(conv_3, center=True, scale=True,is_training=isTrainPlace,scope='bn3')\r\n    mpool_3=tf.layers.max_pooling2d(batch_n3,pool_size=(2,2),strides=(2,2))\r\n    dropout_3=tf.layers.dropout(mpool_3,rate=0.7,training=isTrainPlace)\r\nwith tf.name_scope('conv_4'):     \r\n    conv_4=tf.layers.conv2d(dropout_3,64,[3,3],activation=tf.nn.relu)\r\n    batch_n4 = tf.contrib.layers.batch_norm(conv_4,center=True, scale=True,is_training=isTrainPlace,scope='bn4')\r\n    mpool_4=tf.layers.max_pooling2d(batch_n4,pool_size=(2,2),strides=(2,2))\r\n    dropout_4=tf.layers.dropout(mpool_4,rate=0.7,training=isTrainPlace)\r\n    h4=tf.contrib.layers.flatten(dropout_4)\r\nwith tf.name_scope('dense_1'):      \r\n    y_dense_1=tf.layers.dense(h4,512,activation=tf.nn.relu)\r\n    batch_dense_1 = tf.contrib.layers.batch_norm(y_dense_1,center=True, scale=True,is_training=isTrainPlace, scope='bn5')\r\n    dropout_dense_1=tf.layers.dropout(batch_dense_1,rate=0.8,training=isTrainPlace)\r\nwith tf.name_scope('dense_2'):      \r\n    y_dense_2=tf.layers.dense(dropout_dense_1,256,activation=tf.nn.relu)\r\n    batch_dense_2 = tf.contrib.layers.batch_norm(y_dense_2, center=True, scale=True, is_training=isTrainPlace,scope='bn6')\r\n    dropout_dense_2=tf.layers.dropout(batch_dense_2 ,rate=0.8, training=isTrainPlace)\r\n    y_estimated=tf.layers.dense(dropout_dense_2,2)  `\r\n\r\nPS : the code above , is inspired from : [ https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d](url)\r\nCan anybody help me, please, to undersand , if it's normal or not ? does Keras, uses different tensorflow parameters than the default parameters of tensorflow ?\r\nThanks in advance.\r\nToetoe.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15374, "title": "tf.matching_files order of returned files", "body": "As far as I can tell from `https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.cc`, the order of filenames returned by `tf.matching_files` can be non-determinstic. \r\n\r\nIf that is correct, it would be nice if that were stated in the documentation (and also for `Dataset.list_files` and `train.match_filenames_once`).\r\n\r\nEven better would be to guarantee alphabetical order, but I am not sure about the performance overhead that would incur.\r\n\r\nThis would enable to process files given as e.g.\r\nA/1.png, A/2.png, ... and B/1.png, ... jointly by doing to match_files followed by a zip.\r\n\r\nHave I written custom code No\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from pip \r\nTensorFlow version 1.4.1\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "still an issue", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Yes, that's correct. Feel free to send a pull request to update the docs.\r\n\r\nCC @MarkDaoust ", "Added a PR #16633 for the doc update."]}]