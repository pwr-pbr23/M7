[{"number": 10103, "title": "models/slim download_and_convert_data.py, unicodeDecodeError", "body": "Windows 10 x64 build 1703\r\nAnaconda 4.3.1 with Python 3.6.1 x64\r\nGeforce GTX1080\r\nTensorflow-gpu 1.1.0\r\n\r\n![image](https://cloud.githubusercontent.com/assets/4515120/26315004/b640ffbe-3f4a-11e7-8474-2e399af66960.png)\r\n", "comments": ["@mrry Did you run into UnicodeDecodeError problems when working on the Windows port? If so, this might be an easy question for you.", "My guess is that the `tf.gfile.FastGFile(filenames[i], \"r\")` in the stack trace should really be `tf.gfile.FastGFile(filenames[i], \"rb\")`, because it seems to be barfing on a binary file, and Windows handles text and binary files differently.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 10102, "title": "Tensorflow gpu vs cpu problem", "body": "I am using Tenosrflow 1.0.1. I can run my program tensorflow installed on CPU, but when I run it on different computer tensorflow installed GPU, it says out of memory. What is the problem with the GPU Tensorflow. \r\n\r\nWhat is the reason I can run it on CPU but not on GPU? To run it on GUP I have to decrease the batch size and the image size of my data set. Why has the GPU tensorflow become very poor?\r\n\r\nThe error message for GPU Tensorflow is : \r\n\r\n`2017-05-04 14:24:03.511879: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:1152] Resource exhausted: OOM when allocating tensor with shape[1,6000,6000,64]\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1039, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1021, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,6000,6000,64]\r\n         [[Node: inference/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sub, inference/conv1_1_w/read)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"FCN.py\", line 225, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"FCN.py\", line 196, in main\r\n    sess.run(train_op, feed_dict=feed_dict)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,6000,6000,64]\r\n         [[Node: inference/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sub, inference/conv1_1_w/read)]]\r\n\r\nCaused by op 'inference/Conv2D', defined at:\r\n  File \"FCN.py\", line 225, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"FCN.py\", line 149, in main\r\n    pred_annotation, logits = inference(image, keep_probability)\r\n  File \"FCN.py\", line 84, in inference\r\n    image_net = vgg_net(weights, processed_image)\r\n  File \"FCN.py\", line 54, in vgg_net\r\n    current = utils.conv2d_basic(current, kernels, bias)\r\n  File \"C:\\Users\\admin\\Documents\\gray\\TensorflowUtils.py\", line 89, in conv2d_basic\r\n    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\"SAME\")\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 403, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,6000,6000,64]\r\n         [[Node: inference/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sub, inference/conv1_1_w/read)]]\r\n`\r\n", "comments": ["You don't have enough memory on your GPU to run the model, try reducing batch size or the model size", "I know as it is a memory problem but that problem is only when I run it on GPU. But, I can run it on CPU and it takes very long time to finish it. Why did it work on CPU but not on GPU? That is my question. ", "Because CPU version uses main memory. You have enough main memory but not enough GPU memory. PS, this list is for feature requests/bugs, general questions are better for stackoverflow"]}, {"number": 10101, "title": "fix typo in stepper_test function name", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10100, "title": "fix some annotation", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10099, "title": "Model", "body": "how to change human detection and track into car detection in TF Detect?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10098, "title": "graph and model", "body": "how to change human detection and track into car detection in TF Detect?", "comments": ["This list is for bugs/feature request for tensorflow, please ask this on stackoverflow"]}, {"number": 10097, "title": "Windows: Make TensorFlow build with Bazel again", "body": "1. [rules_closure](https://github.com/bazelbuild/rules_closure) doesn't work on Windows, since `//tensorflow/tensorboard` depends on it, we exclude it from dependencies of pip package for now.\r\nSent https://github.com/bazelbuild/rules_closure/pull/206 to fix one of the rules_closure problems on Windows.\r\n\r\n2. `base_rendezvous_mgr` needs `tf_opts` to build correctly.\r\n\r\n3. `bazel_test_lib.sh`: move `run_configure_for_cpu_build` function in front of `clean_output_base`, because we need the correct .bazelrc to be written before running bazel command.\r\n\r\n4. Excluded `//tensorflow/python/tools:saved_model_cli_test` and `//tensorflow/python/feature_column:feature_column_test` on Windows.\r\nThe reason is [`test_src_dir_path`](https://github.com/tensorflow/tensorflow/blob/1a1527ab1fcffd94f692b1301fdbe87903ce7bdb/tensorflow/python/platform/googletest.py#L133) doesn't work on Windows, because `TEST_SRCDIR` points to the runfiles directory which doesn't exist on Windows, and besides, we run tests from `py_test_dir` directory, so `org_tensorflow/tensorflow` should be `org_tensorflow/py_test_dir/tensorflow`.\r\nI couldn't find an easy fix for these problem, so skip the tests for now.\r\n\r\n\r\n", "comments": ["Manually triggered http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/9/console", "From http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/9/console:\r\n```\r\n05:17:37 //py_test_dir/tensorflow/python/estimator:dnn_test                       FAILED in 1 out of 2 in 66.5s\r\n05:17:37   C:/tmp/_bazel_system/7rgg5n4w/execroot/tensorflow-pr-win-bazel/bazel-out/vc_14_0_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/estimator/dnn_test/test.log\r\n05:17:37 //py_test_dir/tensorflow/python/estimator:linear_test                    FAILED in 1 out of 2 in 65.9s\r\n05:17:37   C:/tmp/_bazel_system/7rgg5n4w/execroot/tensorflow-pr-win-bazel/bazel-out/vc_14_0_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/estimator/linear_test/test.log\r\n05:17:37 //py_test_dir/tensorflow/python/kernel_tests:neon_depthwise_conv_op_test FAILED in 1 out of 2 in 9.5s\r\n05:17:37   C:/tmp/_bazel_system/7rgg5n4w/execroot/tensorflow-pr-win-bazel/bazel-out/vc_14_0_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/kernel_tests/neon_depthwise_conv_op_test/test.log\r\n05:17:37 //py_test_dir/tensorflow/python/kernel_tests:tensor_array_ops_test       FAILED in 3 out of 4 in 12.5s\r\n```\r\n\r\nSo the pip package build is fixed, and caught some failures from newly added tests.", "Jenkins test this please.", "The failures look unrelated. We can go ahead and merge this PR."]}, {"number": 10096, "title": "Cannot set Build Cost Model in Graph Options", "body": "`init_op = tf.global_variables_initializer();\r\nsess = tf.Session(tf.ConfigProto(graph_options=tf.GraphOptions(build_cost_model=1)));\r\nsess.run(init_op);`\r\nAfter I downloaded the updated repository I cannot get the _build_cost_model_ flag to work. I have attached the code snippet above.\r\nThe error I get is \r\n\r\n> Traceback (most recent call last):                                                                                                            File \"test_part.py\", line 55, in <module>                                                                                                     sess = tf.Session(tf.ConfigProto(graph_options=tf.GraphOptions(build_cost_model=1)))                                                      File \"/home/utagar/tensorflow_new_instrumented/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1292, in __init__                                                                                                                                             super(Session, self).__init__(target, graph, config=config)                                                                               File \"/home/utagar/tensorflow_new_instrumented/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 541, in __init__                                                                                                                                              raise TypeError('target must be a string, but got %s' % type(target))                                                                   TypeError: target must be a string, but got <class 'tensorflow.core.protobuf.config_pb2.ConfigProto'>                                       Exception AttributeError: \"'Session' object has no attribute '_session'\" in <bound method Session.__del__ of <tensorflow.python.client.session.Session object at 0x7fcef03fccd0>> ignored\r\n\r\nPlease suggest how to fix it.", "comments": []}, {"number": 10095, "title": "R1.1", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 10094, "title": "compile contrib/hvx failed.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.2.0-rc0\r\n- **Bazel version (if compiling from source)**:\r\n\r\n- **CUDA/cuDNN version**:\r\nnot used \r\n- **GPU model and memory**:\r\nnot used\r\n- **Exact command to reproduce**:\r\nI follow the commands in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx \r\nto build tensorflow that running hvx.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n I have tryed ndk-r13b,r10d,r14d. All of them produce the errors as follows:\r\n\r\nchecking whether to enable maintainer-specific portions of Makefiles... yes\r\nchecking build system type... x86_64-unknown-linux-gnu\r\nchecking host system type... arm-unknown-linux-androideabi\r\nchecking target system type... arm-unknown-linux-androideabi\r\nchecking for a BSD-compatible install... /usr/bin/install -c\r\nchecking whether build environment is sane... yes\r\nchecking for arm-linux-androideabi-strip... no\r\nchecking for strip... strip\r\nchecking for a thread-safe mkdir -p... /bin/mkdir -p\r\nchecking for gawk... no\r\nchecking for mawk... mawk\r\nchecking whether make sets $(MAKE)... yes\r\nchecking whether make supports nested variables... yes\r\nchecking whether UID '1000' is supported by ustar format... yes\r\nchecking whether GID '1000' is supported by ustar format... yes\r\nchecking how to create a ustar tar archive... gnutar\r\nchecking for arm-linux-androideabi-gcc...  arm-linux-androideabi-gcc --sysroot ../Qualcomm/Hexagon_SDK/3.0/tools/android-ndk-r10d/platforms/android-21/arch-arm\r\nchecking whether the C compiler works... no\r\nconfigure: error: in `/home/zhouzhan/tensorflow/tensorflow/contrib/makefile/downloads/protobuf':\r\nconfigure: error: C compiler cannot create executables\r\nSee `config.log' for more details\r\n\r\nConfig.log is:\r\n[config_log.txt](https://github.com/tensorflow/tensorflow/files/1018354/config_log.txt)\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@skye  Could you take a look at this please? Thanks.", "@satok16 is listed as the maintainer of contrib/hvx, perhaps he can comment", "Hi, sorry for the delay.\r\nI'm going to refactor this part (change has been already created, I'm cleaning up it now.)  Will be checked in a week.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Closing since it's been a while, hopefully the refactoring works out for you."]}, {"number": 10093, "title": "Update README.md", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "Jenkins test this please.", "I signed it!", "Hmm, googlebot didn't verify your CLA. Can you check your existing CLA data and verify that your email is set on your git commits. The links are in the googlebot reply above.", "@maciekcc i have verified my CLA information and my mail id on commit.They are the same.", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 10092, "title": "tf.matmul unexpected exception", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMacOS sierra and Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\non mac, installed using binary\r\non linux,installed using binary\r\n**TensorFlow version (use command below)**:\r\nv1.0.1\r\n- **Bazel version (if compiling from source)**:\r\n\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\n\r\nimport tensorflow as tf\r\nx = tf.placeholder(tf.float32, shape=(None, 1795,13))\r\nu = tf.Variable(tf.truncated_normal(shape=(13,5), stddev=0.1))\r\ny = tf.matmul(x,u)\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI expected y to go through, and be a tensor of shape=(?,1795,5)\r\ninstead, I got an exception:\r\nValueError: Shape must be rank 2 but is rank 3 for 'MatMul_1' (op: 'MatMul') with input shapes: [?,1795,13], [13,5].\r\n", "comments": ["The described behaviour is a documented feature of `tf.matmul`. According to the [online documentation of matmul](https://www.tensorflow.org/api_docs/python/tf/matmul), both tensors must have the same rank (this is what the exception says, even if a bit clumsily, it should have said \"both matmul arguments must have a same rank\").\r\n\r\nTo achieve what you expect, you have to \"copy\" `u` into a shape (same_batch_size_as_x, 13, 5).", "The problem is that x and u have different ranks and can not multiply.\r\nYou should define x, u as follows:\r\nx = tf.placeholder(tf.float32, shape=(batch_size, 1795,13))\r\nu=tf.Variable(tf.truncated_normal(shape=(batch_size, 13,5), stddev=0.1))\r\ny=tf.matmul(x,u)\r\n#where batch_size is the size of one batch."]}, {"number": 10091, "title": "tf.assign does not change the shape of variables correctly", "body": "Hi,\r\n\r\nI have discovered what I believe to be a bug with tf.assign. This occurs when I have a matrix with a shape, e.g. (2, 3), and assign it a new, larger matrix, e.g. with the shape (2, 4). When you get the shape of your updated variable, its shape has not changed i.e. (2, 3) is returned. Furthermore, if you use tf.add on your new matrix and a matrix with the same shape (2, 4) as the updated matrix an exception is thrown:\r\n`ValueError: Dimensions must be equal, but are 3 and 4 for 'Add_2' (op: 'Add') with input shapes: [2,3], [2,4].`\r\n\r\nSimilar exceptions are also thrown when you try and use tf.matmult, tf.subtract, etc. on your updated variable.\r\n\r\nBelow is an example of this issue.\r\n\r\n<pre><code>import tensorflow as tf\r\n\r\nimport numpy as np\r\n\r\n# Initialise some variables\r\nsess = tf.Session()\r\nx = tf.Variable(tf.ones([2, 3]))\r\ny = tf.Variable(tf.ones([2, 3]))\r\nz = tf.Variable(tf.ones([2, 4]))\r\nsess.run(tf.variables_initializer([x, y, z]))\r\n\r\n# Print information about the original matrix\r\nprint(x.get_shape())\r\nprint(x.eval(session=sess))\r\nprint(tf.add(x, y).eval(session=sess))\r\nprint()\r\n\r\n# Enlarge the matrix by assigning it a new set of values\r\nsess.run(tf.assign(x, np.ones([2, 4]), validate_shape=False))\r\n\r\n# Print information about the new matrix\r\nprint(x.get_shape())\r\nprint(x.eval(session=sess))\r\nprint()\r\n\r\n# Try add the updated matrix to a matrix with the same ORIGINAL shape\r\n# I would expect this to fail because I have changed the shape\r\ntry:\r\n  print(tf.add(x, y).eval(session=sess))\r\nexcept:\r\n  print(\"Could not add x and y\")\r\nprint()\r\n\r\n# Try add the updated matrix to a matrix with the same shape\r\n# I would NOT expect this to fail because they both have the same shapes\r\ntry:\r\n  print(tf.add(x, z).eval(session=sess))\r\nexcept:\r\n  print(\"Could not add x and z\")\r\nprint()\r\n\r\n# Try add the updated matrix to a matrix that had the same ORIGINAL\r\n# shape as it, but has been updated so their actual shapes are the same\r\na = tf.Variable(tf.ones([2, 3]))\r\nsess.run(tf.variables_initializer([a,]))\r\nsess.run(tf.assign(a, np.ones([2, 4]), validate_shape=False))\r\nprint(tf.add(x, a).eval(session=sess))\r\n</code></pre>\r\n\r\nThe output of this code is:\r\n\r\n<code><pre>(2, 3)\r\n[[ 1.  1.  1.]\r\n [ 1.  1.  1.]]\r\n[[ 2.  2.  2.]\r\n [ 2.  2.  2.]]\r\n\r\n(2, 3)\r\n[[ 1.  1.  1.  1.]\r\n [ 1.  1.  1.  1.]]\r\n\r\nCould not add x and y\r\n\r\nCould not add x and z\r\n\r\n[[ 2.  2.  2.  2.]\r\n [ 2.  2.  2.  2.]]\r\n</code></pre>", "comments": ["@mrry is `validate_shape=False` supposed to cause this?\r\n\r\n```pycon\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n>>> x = tf.Variable(tf.ones([2, 3]))\r\n>>> y = tf.Variable(tf.ones([2, 3]))\r\n>>> sess.run(tf.variables_initializer([x, y]))\r\n\r\n>>> print(tf.add(x, y).eval(session=sess))\r\n[[ 2.  2.  2.]\r\n [ 2.  2.  2.]]\r\n\r\n>>> sess.run(tf.assign(x, tf.ones([2, 4]), validate_shape=False))\r\narray([[ 1.,  1.,  1.,  1.],\r\n       [ 1.,  1.,  1.,  1.]], dtype=float32)\r\n\r\n>>> print(tf.add(x, y).eval(session=sess))\r\nInvalidArgumentError (see above for traceback): Incompatible shapes: [2,4] vs. [2,3]\r\n\r\n>>> x\r\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32_ref>\r\n>>> y\r\n<tf.Variable 'Variable_1:0' shape=(2, 3) dtype=float32_ref>\r\n```", "Right, I think `tf.assign(..., validate_shape=False)` is unsafe wrt shape inference (which, for sanity's sake, assume that, once inferred, shape information is never invalidated).\r\n\r\nIf you want to use `tf.assign(..., validate_shape=False)` to change the shape of a variable later, you must construct the variable as `x = tf.Variable(tf.ones([2, 3]), validate_shape=False)`.", "I'm sorry I do not fully understand you. Essentially I am trying to achieve is this:\r\n\r\n<pre><code>\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsess = tf.Session()\r\nx = tf.Variable(tf.truncated_normal([2, 4], stddev=0.04))\r\nsess.run(tf.variables_initializer([x,]))\r\n\r\n# Inefficient declaration of z because values are copied not shared\r\nz = tf.Variable(tf.concat((x, tf.truncated_normal([1, 4], stddev=0.04)), axis=0))\r\nsess.run(tf.variables_initializer([z,]))\r\n\r\nprint(tf.add(z, np.ones([3, 4])).eval(session=sess))\r\n</pre></code>\r\n\r\nExcept I do not want to waste space in memory because variable z contains the exact values found in x (plus a few extra) and I will never need the matrix x anymore. The reason why I want this functionality is because I want to declare a weight matrix in my neural network which will become larger each iteration but I do not want to waste space by having copies of the previous smaller matrix still in memory.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "> If you want to use tf.assign(..., validate_shape=False) to change the shape of a variable later, you must construct the variable as x = tf.Variable(tf.ones([2, 3]), validate_shape=False).\r\n\r\n@mrry Is there a way to change `validate_shape` at a later point? This would be useful to modify networks restored from a metagraph definition"]}, {"number": 10090, "title": "How can I update my tensorflow 1.0 to 1.2 or 1.1 at the window10.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI installed tensorflow at my window 10.\r\nIt is only 1.0 version and using python 3.5.\r\nHow can I update version 1.2 and can I use python 3.6?\r\nI already installed anaconda newest version.\r\nI installed it using anaconda prompt.\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["1.2 release is still not final.\r\nYou can follow our installation guide, and use the URLs to the pip packages in the installation guide to upgrade to 1.2.0-rc0\r\n\r\nAt the moment, we do not have windows 3.6 packages."]}, {"number": 10089, "title": "Scala API", "body": "Hi,\r\n\r\nI have open sourced a Scala API for TensorFlow that contains much more complete functionality than the Java API, [here](https://github.com/eaplatanios/tensorflow_scala). The README file in the repository contains information on what features are supported etc. I would really appreciate some feedback from interested parties in the community on the library.\r\n\r\nOne main limitation is that the library does not yet support while loops in the graph. This is due to being unable to implement gradient backpropagation using the current C API. @skye do you have any suggestions on how to proceed with respect to that? It would be really useful for implementing RNNs.\r\n\r\nOther cool stuff (such as fetching arbitrarily structured data from sessions -- e.g., lists of tensors, or maps of tensors, etc.) are supported and are for the most part strongly typed.\r\n\r\nCheers,\r\nAnthony", "comments": ["Super cool Anthony! This looks really great. FYI @asimshankar\r\n\r\nRegarding while loops, I still need to reimplement the backprop logic in the C API as we discussed in #9150. Given that we're both depending on this (I'm currently switching the Python API to use the C API), I'll start work on this ASAP. I can't make any promises as to when it will land, but hopefully within the next month depending on the complexity and what else comes up (I'll also be on vacation next week).", "@skye Thanks! :)\r\n\r\nRegarding the gradients, I'm currently not using the provided C API gradients, because it's missing a lot of them (my library still supports it but the default one is my implementation). I have instead implemented back-propagation in the Scala API (essentially porting the Python API code). In this case it would be great for your implementation to allow me to implement back-propagation over while loops in the Scala side, or to allow registering gradient functions through the C API (although I feel this is not easy to do). Do you have any thoughts on this?\r\n\r\nP.S. I'm actually also going on vacation in a couple weeks! Have a great time wherever you're going! :))", "Thank you @eaplatanios for bringing your work to the attention of the community. We appreciate what you're doing. We hope the Scala community will find TensorFlow helpful.\r\n\r\nHere's one way we might be able to support you. You could send us a pull request updating our documentation to point to your project. For example, the [Which client languages are supported in TensorFlow?](https://www.tensorflow.org/programmers_guide/faq) section in our Programmer's Guide. You could add a sentence that says something like, \"There is also an independently maintained interface for [Scala](https://github.com/eaplatanios/tensorflow_scala).\" \r\n\r\nAny thoughts @martinwicke?\r\n\r\nCC: @jdegoes", "@eaplatanios : This is wonderful. If you'd like, I'd be happy to also link to this from https://www.tensorflow.org/api_docs/ like we do for some other language bindings.", "@eaplatanios Nice work! I'll offer my assistance on the API for this library if you would like.", "@eaplatanios I'll have to think about the while loop backprop more. We have the same problem porting the Python while_loop to use the C API, since the C gradients aren't finished. @asimshankar and I had discussed the C loop backprop code calling back into the Python runtime to create the individual gradients, so maybe we can make that mechanism general enough to also use in the Scala API. I'll let you know when I come up with something more concrete, and let me know if you have any further thoughts.", "@jart Thanks! That sounds good. I can do that.\r\n\r\n@asimshankar Thanks! That would be great! :)\r\n\r\n@jdegoes Thanks! That would actually be helpful as there are some parts of the library I'm not happy with and would like to make more elegant. We can talk about it whenever you want and I can tell you my thoughts. :)\r\n\r\n@skye C backprop code calling back into the Python runtime to create the individual gradients sounds like what I was referring to although it's hard to make generic. One possible idea would be to allow creation of functions (i.e., subgraphs) in the C API, and then define control flow operations in terms of those functions. Then, a single function for returning the gradient of a while block should be sufficient for users that implement backprop in other languages. Or something along those lines.", "@skye How about the following proposal?\r\n\r\nI have not really looked much at how functions (i.e., subgraphs) are defined in graphs, but intuitively I think the following solution makes sense.\r\n\r\nCurrently, there exist gradient registration functions in the Python API and in the C++ API, as well as in my Scala API. Furthermore, the gradient back-propagation code is also defined in all three places. If we change the gradient registration functions to instead return a TF function (i.e., a sub-graph) that has the appropriate inputs and outputs, then that would make these sub-graphs \"transferrable\" between other language APIs and the C API.\r\n\r\nThis should allow for the back-propagation code to only exist in the C++ and then we could have C API functions for registering op gradients, which would only need the op type name and the gradient function (i.e., sub-graph in this case) as arguments. This will allow registering gradient functions for yet unsupported gradients in the C++ side, while also avoiding replication of the gradient back-propagation code.\r\n\r\nI hope my brief description makes sense.", "@eaplatanios : Use of functions would be a great way to try and make gradients defined in any one language available to others and is something we are interested in. However, the timeline for that isn't clear yet (as a first step there are a few things we have to do before TF functions themselves are made part of the public API, and even after that there are some missing features in TF functions that are needed before all gradient functions can be expressed). What @skye suggested (having the C API call back into Python for the gradients) is a means of decoupling the transition to using the C API for graph construction in Python from working out all of those issues.", "@asimshankar I see. I can see how this may work with Python, but how would you go about having the C API call back into Java or Scala functions for the gradients. Is there a simple way to do that with JNI? Or is there some more generic way that would allow you to do that with any language?\r\n\r\nOne way I see for doing this would be to have the C API expose a function interface that users of the API can implement and which is similar to the gradient function interface in the Python API or my Scala API. Then, users can implement that by having it lookup a functions registry in their language implementation that would return the gradients if there is a corresponding function registered. The C API function would be called whenever the C API needs gradients for some op. I also think it might be better for that function to be called first (for now), before looking for C++ gradient functions, in order to retain consistency in how the gradients are named in the constructed graph. Does that sound like something that's possible with the C API? I'm not very familiar with C to know if that's easy to do or not.\r\n\r\nYet another way may be similarly to how tensor deallocators are specified. The users could possible register gradient functions this way. In this case, they would only need to register a single function that delegates the gradient lookup to their own language implementation, which constructs the gradients and returns the result.", "It's so cool man, thank you for your giving:)", "@asimshankar I created a pull request to link to my repository from some of the documentation pages, but I could not figure out how to link to it from [https://www.tensorflow.org/api_docs/](https://www.tensorflow.org/api_docs/) as you suggested. Could you please help me with that?\r\n\r\n@skye Is there any update on the support for gradients over while loops, etc., that we discussed before? Thanks! :)\r\n\r\n@Moriadry Thanks! :)", "@eaplatanios : I believe that particular landing page isn't checked into github, but I will take care of adding that before the next release. Thanks!", "@eaplatanios: I've been working on the framework for generating the backprop graph of while loops, but haven't committed anything yet. I'll post here as I make progress, and once the general framework is in we'll have to address how to generate unimplemented C++ op gradients. ", "That's really good, thanks.\r\n\r\nWe have community version of Scala GRPC and now we have Scala API for tensorflow.\r\n\r\nHopefully Google could add those as official support.", "I doubt we'd add official support, given our Scala bench isn't very deep.\nHowever, we'd be excited to link to Scala bindings from tensorflow.org.\n\nOn Jul 17, 2017 14:30, \"Tianhao Li\" <notifications@github.com> wrote:\n\n> That's really good, thanks.\n>\n> We have community version of Scala GRPC and now we have Scala API for\n> tensorflow.\n>\n> Hopefully Google could add those as official support.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10089#issuecomment-315741828>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_XFft-iZB13d1kT4Y-L8v3momKizks5sO1P-gaJpZM4Nhz52>\n> .\n>\n", "@martinwicke That is totally understandable! Do you mean linking on [this page](https://www.tensorflow.org/api_docs/) for the 1.3 release? That would be great! :)", "@skye That sounds great! I look forward to the updates and I can certainly provide some feedback on how to to generate unimplemented C++ op gradients when time comes. :)", "Yes, I believe Asim already mentioned the possibility earlier.\n", "Ok that sounds great. You're right, I'm sorry I had forgotten about the previous message. In this case, let's go ahead and do this. :)\n\nOn Jul 21, 2017, 11:32 AM -0400, Martin Wicke <notifications@github.com>, wrote:\n> Yes, I believe Asim already mentioned the possibility earlier.\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "Please do send a PR. I believe all this is in GitHub. @wolffg can you\nconfirm?\n", "I tried to find that page before but couldn't. @asim mentioned before that that page may not be in github.\n\n\nOn Jul 21, 2017, 1:56 PM -0400, Martin Wicke , wrote:\n> Please do send a PR. I believe all this is in GitHub. @wolffg can you\n> confirm?\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "@martinwicke @eaplatanios : Yeah that page isn't in github, it's generated as part of the script that uploads to www.tensorflow.org. I've made a change there, so it should go out in the next website push", "@asimshankar Great, thanks Asim! :)", "@eaplatanios @asimshankar are there plans to combine the jni library in tensorflow_scala with the java interface ? tensorflow_scala exposes the Server interfaces which will help in implementing distributed training and inference using parameter servers deployed in Spark clusters.... ", "@debasish83 It's very easy to share the work I have done for my JNI bindings and I would be happy to collaborate and find a way to merge it into the main TensorFlow repository. However, most of the setup/management for distributed TF in my library is implemented on the Scala side. The C API does not expose much for that as a lot of it is only implemented in the Python API. However, it's easy to create a Java API to my Scala API if you're up for that. You would just have to work around my use of implicits in the Scala side which should not be too bad for distributed training. However, it might get tricky if you try to create a Java interface for my learn API (i.e., estimators, datasets, etc.) as I use a lot of implicits for strong type checking there. Note that the best way to perform distributed training currently in my library is through the learn API I provide (similar to the Python API except for the strong typing and some other features in which we diverge). I'd be happy to provide support if you're interested in working on something like that. :)", "@skye @asimshankar I have also finally added support for while loops, back propagation, and RNNs, by reimplementing pretty much all of the Python API functionality and adding some strong typing on top of it (with respect to loop variables, etc), and so I guess you can close this issue if you want as there isn't any remaining issue I can see. :)", "@eaplatanios I am fine to use scala API...like MXNet it will be useful to introduce a scala package in tensorflow to support distributed learning with spark/flink and model inference can continue using java/scala API...I will provide further feedbacks on https://github.com/eaplatanios/tensorflow_scala...", "@debasish83 Sounds good! I'd really appreciate your feedback. :) I'm currently in the process of adding a sequence-to-sequence modeling package as well as a neural machine translation module. I will be working for a paper deadline until December 15th and traveling, but I'll try to be as responsive as possible. :)", "Wow awesome @eaplatanios! I'll go ahead and close this issue, but feel free to continue the current discussion here."]}, {"number": 10088, "title": "Surfacing REGISTER_OP to the public C API", "body": "### Feature Request\r\n\r\nI am working on the .NET bindings for C# and as part of this process, I am replicating the C API test suite in C# to ensure everything works as expected.\r\n\r\nThe REGISTER_OP capability is available to C developers, and I would like to have this capability surfaced so my binding (and likely other bindings) can roll out these tests as well.\r\n\r\nThis is what I would like to be able to do from C#:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7d785f1e18af9d22d940f18aac6e8c9ffd268b22/tensorflow/c/c_api_test.cc#L1577\r\n", "comments": ["Thanks for asking :-) We're hard at work on something that will enable that @girving. ", "@drpngx That sounds great! Is there any time estimate for that? Thanks! :)", "Transferring issue ownership.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "See https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h#L424"]}, {"number": 10087, "title": "Cannot get TensorBoard example working", "body": "\"\"\"A very simple MNIST classifer, modified to display data in TensorBoard\r\n\r\nSee extensive documentation for the original model at\r\nhttp://tensorflow.org/tutorials/mnist/beginners/index.md\r\n\r\nSee documentaion on the TensorBoard specific pieces at\r\nhttp://tensorflow.org/how_tos/summaries_and_tensorboard/index.md\r\n\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport sys\r\n\r\nimport tensorflow as tf\r\n\r\n# Import data\r\nimport input_data\r\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\n\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\n\r\n\r\n# Create the model\r\nx = tf.placeholder(\"float\", [None, 784], name=\"x-input\")\r\nW = tf.Variable(tf.zeros([784,10]), name=\"weights\")\r\nw_hist = tf.summary.histogram(\"weights\", W)\r\nb = tf.Variable(tf.zeros([10], name=\"bias\"))\r\nb_hist = tf.summary.histogram(\"biases\", b)\r\nwith tf.name_scope(\"Wx_b\") as scope:\r\n  y = tf.nn.softmax(tf.matmul(x,W) + b)\r\n#y_hist = tf.histogram_summary(\"y\", y)\r\ny_hist = tf.summary.histogram(\"y\", y)\r\n\r\n# Define loss and optimizer\r\ny_ = tf.placeholder(\"float\", [None,10], name=\"y-input\")\r\nwith tf.name_scope(\"xent\") as scope:\r\n  cross_entropy = -tf.reduce_sum(y_*tf.log(y))\r\n  ce_summ = tf.summary.scalar(\"cross entropy\", cross_entropy) # use summary.scalar instead of tf.scalar_summary\r\nwith tf.name_scope(\"train\") as scope:\r\n  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\r\n\r\nwith tf.name_scope(\"test\") as scope:\r\n  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n  accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy) # use summary.scalar instead of tf.scalar_summary\r\n\r\nmerged = tf.summary.merge_all() # use tf.summary.merge_all instead merge_all_summaries\r\nwriter = tf.summary.FileWriter(\"/tmp/mnist_logs\", sess.graph_def) # # use tf.summary.FileWriter instead SummaryWriter\r\ntf.initialize_all_variables().run()\r\n\r\n# Test trained model\r\n\r\nfor i in range(1000):\r\n  if i % 10 == 0:  # Record summary data, and the accuracy\r\n    feed = {x: mnist.test.images, y_: mnist.test.labels}\r\n    result = sess.run([merged, accuracy], feed_dict=feed)\r\n    summary_str = result[0]\r\n    acc = result[1]\r\n    writer.add_summary(summary_str, i)\r\n    print(\"Accuracy at step %s: %s\" % (i, acc))\r\n  else:\r\n    batch_xs, batch_ys = mnist.train.next_batch(100)\r\n    feed = {x: batch_xs, y_: batch_ys}\r\n    sess.run(train_step, feed_dict=feed)\r\n\r\nprint(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))\r\n\r\n\r\n\r\n\r\n\r\nwhile running the above example, it ran successfully , but while calling tensorborad it gives the below error. Please help.\r\n\r\nAccuracy at step 980: 0.9151\r\nAccuracy at step 990: 0.9156\r\n0.9147\r\nAbhisheks-MacBook-Air:POC abhi$ tensorboard --logdir=/tmp/mnist_logs\r\nTraceback (most recent call last):\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 242, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 342, in load_dynamic\r\nreturn _load(spec)\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: .8.0.dylib\r\nReferenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nReason: image not found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/bin/tensorboard\", line 7, in \r\nfrom tensorflow.tensorboard.tensorboard import main\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/init.py\", line 24, in \r\nfrom tensorflow.python import *\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/init.py\", line 51, in \r\nfrom tensorflow.python import pywrap_tensorflow\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in \r\nraise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 242, in load_module\r\n @abhis01\r\n     \r\nabhis01 commented 43 minutes ago\r\ni am using python 2.7\r\n @abhis01\r\n     \r\nabhis01 commented 38 minutes ago\r\nHello, I am using these versions :--\r\n\r\nAbhisheks-MacBook-Air:tensorflow abhi$ pip show tensorflow\r\nName: tensorflow\r\nVersion: 1.1.0\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires: wheel, protobuf, numpy, mock, werkzeug, six\r\nAbhisheks-MacBook-Air:tensorflow abhi$ pip show protobuf\r\nName: protobuf\r\nVersion: 3.3.0\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: protobuf@googlegroups.com\r\nAuthor-email: protobuf@googlegroups.com\r\nLicense: 3-Clause BSD License\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires: setuptools, six\r\nAbhisheks-MacBook-Air:tensorflow abhi$ pip show six\r\nName: six\r\nVersion: 1.10.0\r\nSummary: Python 2 and 3 compatibility utilities\r\nHome-page: http://pypi.python.org/pypi/six/\r\nAuthor: Benjamin Peterson\r\nAuthor-email: benjamin@python.org\r\nLicense: MIT\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires:\r\nAbhisheks-MacBook-Air:tensorflow abhi$ python\r\nPython 2.7.13 (default, Apr 4 2017, 08:47:57)\r\n[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.", "comments": ["I have tested your code. I can show the tensorboard as follows:\r\n\r\n![ss](https://cloud.githubusercontent.com/assets/15058478/26352806/697d3bf2-3f8b-11e7-9e9e-581f58b1e085.PNG)\r\n\r\nMost of your codes are correct. The only issue is the path. Try to change the path in the following codes:\r\nwriter = tf.summary.FileWriter(\"/tmp/mnist_logs\", sess.graph_def) \r\n\r\nThen make sure you can find the path. For example,\r\nwriter = tf.summary.FileWriter(\"C:/Users/Labs/issue1\", sess.graph_def) \r\nthen use the terminal \r\n \r\n![image](https://cloud.githubusercontent.com/assets/15058478/26353031/4e36a5f8-3f8c-11e7-9feb-a6bc83f0b642.png)\r\n\r\n\r\nThen you can log into the tensorboard by localhost:6006", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@abhis01 Did you get this solved? I'm having the same problem! ", "yes\n\nOn Tue, Oct 31, 2017 at 2:19 PM, Abhirami Harilal <notifications@github.com>\nwrote:\n\n> @abhis01 <https://github.com/abhis01> Did you get this solved? I'm having\n> the same problem!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10087#issuecomment-340877460>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGF0DOo2LmAwegFbD0me7aTK5oskVf9_ks5sx3LIgaJpZM4NhzJe>\n> .\n>\n", "Yes, I consider the solution of the issue has been explained in the link\n\nBest regards,\n\nWenting Li\n\nDepartment of Electrical, Computer & Systems Engineering\nRensselaer Polytechnic Institute, Troy, NY 12180\n\n2017-10-31 18:48 GMT-04:00 abhis01 <notifications@github.com>:\n\n> yes\n>\n> On Tue, Oct 31, 2017 at 2:19 PM, Abhirami Harilal <\n> notifications@github.com>\n> wrote:\n>\n> > @abhis01 <https://github.com/abhis01> Did you get this solved? I'm\n> having\n> > the same problem!\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 10087#issuecomment-340877460>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AGF0DOo2LmAwegFbD0me7aTK5oskVf9_ks5sx3LIgaJpZM4NhzJe>\n> > .\n>\n> >\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10087#issuecomment-340929755>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AOXGLojuGVRyh8PD8H_WVLj4JLl7aB7Eks5sx6PTgaJpZM4NhzJe>\n> .\n>\n", "No, why import from Keras? I think it unnecessary to use that library, but\nonly the tensorflow is enough.\n\nBest regards,\n\nWenting Li\n\nDepartment of Electrical, Computer & Systems Engineering\nRensselaer Polytechnic Institute, Troy, NY 12180\n\n2018-02-22 13:29 GMT-05:00 Mostafa Elhoushi <notifications@github.com>:\n\n> Have you tried adding to the top of your script:\n>\n> from keras.callbacks import TensorBoard\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10087#issuecomment-367774276>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AOXGLuqmnra31bToanzguw_vl34pSr1rks5tXbHxgaJpZM4NhzJe>\n> .\n>\n", "Sorry for that @Wendy0601 I was fixing another problem. I will delete my comment."]}, {"number": 10086, "title": "Type check failed in piecewise_constant.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:archlinux\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:v1.1.0-rc2-1140-g513b1e4 1.1.0-rc2 \r\n\r\n```python\r\nglobal_step = tf.get_variable('gs', trainable=False, dtype=tf.int32, shape=[])\r\nglobal_step = tf.get_default_graph().get_tensor_by_name('gs:0')\r\nboundaries = [100000, 110000]\r\nvalues = [1.0, 0.5, 0.1]\r\nlearning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\r\n```\r\n```\r\n    learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\r\n  File \"/xxx/.local/lib/python3.6/site-packages/tensorflow/python/training/learning_rate_decay.py\", line 144, in piecewise_constant\r\n    b.dtype, x.dtype))\r\nValueError: Boundaries (<dtype: 'int32'>) must have the same dtype as x (<dtype: 'int32_ref'>).\r\n```\r\nI wonder if the check is over strict? Is there a reason to distinguish `int32` and `int32_ref` here?", "comments": ["@vrv Could you take a look please? Thanks.", "No, we should probably be checking base dtype there. Would you like to send a PR?"]}, {"number": 10085, "title": "pip install  --user  tensorflow==1.2.0rc0", "body": "Error occurred:\r\nNon-zero exit code(1)\r\n![untitled](https://cloud.githubusercontent.com/assets/25774969/26286789/70e3d8be-3e97-11e7-8e60-6bcb27257d52.jpg)\r\n\r\n", "comments": ["I had issues downloading via pip so just used the latest whl from the last stable build from jenkins.", "I am also unable to download via pip.\r\n\r\n    $ pip install tensorflow\r\n    Collecting tensorflow\r\n      Could not find a version that satisfies the requirement tensorflow (from versions: )\r\n    No matching distribution found for tensorflow", "Are you using Windows 10? CPU version? More information please.", "1) I tried downloading tensorflow on my local machine running Mac OS 10.10.5 on a 3GHz i7 using pip 9.0.1 for python 2.7 & 3.5 and both failed with the same \"Could not find a version...\" error.\r\n\r\n```\r\n$ pip --version\r\npip 9.0.1 from /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (python 3.5)\r\n$ pip install tensorflow\r\nCollecting tensorflow\r\n    Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n```\r\n```\r\n$ pip2 --version\r\npip 9.0.1 from /Library/Python/2.7/site-packages (python 2.7)\r\n$ pip2 install tensorflow\r\nCollecting tensorflow\r\n    Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n```\r\n\r\n2) I tried the same process on our linux cluster running CentOS 6.9: using pip 9.0.1 to install tensorflow for python 2.7 & 3.5. This time, I was able to successfully install tensorflow-1.2.0rc0 using pip for python 3.5, but the python 2.7 install failed.\r\n\r\n```\r\n$ pip2 --version\r\npip 9.0.1 from /srv/gsfs0/software/python/python-2.7/lib/python2.7/site-packages (python 2.7)\r\n$ pip2 install tensorflow==1.2.0rc0\r\nCollecting tensorflow==1.2.0rc0\r\n  Could not find a version that satisfies the requirement tensorflow==1.2.0rc0 (from versions: )\r\nNo matching distribution found for tensorflow==1.2.0rc0\r\n```\r\n\r\n```\r\n$ pip3 --version\r\npip 9.0.1 from /srv/gsfs0/software/python/3.5.1/lib/python3.5/site-packages (python 3.5)\r\n$ pip3 install tensorflow==1.2.0rc0\r\nCollecting tensorflow==1.2.0rc0\r\n  Downloading tensorflow-1.2.0rc0-cp35-cp35m-manylinux1_x86_64.whl (34.6MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 34.6MB 44kB/s\r\n...\r\nSuccessfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.2.0 tensorflow-1.2.0rc0\r\n```\r\n\r\n3) I also tried installing tensorflow on the stock python:2.7.13 Docker image running Debian GNU/Linux 8 (jessie), on the same mac from step 1. Here I was able to successfully install tensorflow-1.2.0rc0 for python 2.7.\r\n\r\n```\r\n# pip install tensorflow==1.2.0rc0\r\nCollecting tensorflow==1.2.0rc0\r\n  Downloading tensorflow-1.2.0rc0-cp27-cp27mu-manylinux1_x86_64.whl (34.6MB)\r\n...\r\neSuccessfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.2.0 tensorflow-1.2.0rc0\r\n```", "Same issue: Trying to `pip install tensorflow`, but get \r\n```\r\nCould not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n```\r\nI'm running on a Docker image (`python:3-alpine`). Weirdly, I can install it on the Docker image `python` and `python:3-slim`. Am I missing something with regard to `pip`?\r\n\r\n**Version Info:**\r\n```\r\n# uname -a\r\nLinux b2a048fac069 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2 (2017-04-30) x86_64 Linux\r\n```\r\n```\r\n# python --version\r\nPython 3.6.1\r\n```\r\n```\r\n# pip --version\r\npip 9.0.1 from /usr/local/lib/python3.6/site-packages (python 3.6)\r\n```\r\n", "Have you tried installing the .whl's directly from pypi and seeing if they work. For example a macos CPU build for python 3.5 might be [this .whl file here](https://pypi.python.org/packages/01/4b/46b6208bddd2ff076ba2ade287394a293be2ae3344917c2c69393bfeffb6/tensorflow-1.2.0rc0-cp35-cp35m-macosx_10_11_x86_64.whl#md5=ef588a86efadd4a8a2c540658373fbcc).\r\n\r\n[See the full list here](https://pypi.python.org/pypi/tensorflow) and see if you can install directly like this. Not a permanent solution but i've had pip get upset before and say there isn't a compatible version when i know that there is. Hope this helps!", "Yes try installing the whl file. It's possible that whatever version of pip inside IntelliJ is less than 8.1. See https://www.tensorflow.org/install/install_linux\r\n\r\nI'm not sure why other people in this thread who have pip 9 would have problems. Any thoughts @yifeif?", "Could you try `python -c \"from pip import pep425tags;print(pep425tags.supported_tags)\"` to check what are supported tags on your system?\r\n\r\n", "I am also unable to install tensorflow using the whl file.\r\n\r\n```\r\n$ pip install https://pypi.python.org/packages/b1/35/62fdae318152cc1860cd7e4155165cbda6944b5727f09b6fc5a53c96d486/tensorflow-1.2.0rc0-cp27-cp27m-macosx_10_11_x86_64.whl#md5=772ab0dd05fd7e1da59bb8d642fd126c\r\ntensorflow-1.2.0rc0-cp27-cp27m-macosx_10_11_x86_64.whl is not a supported wheel on this platform.\r\n```\r\n\r\n@yifeif Here are my supported tags:\r\n\r\n```\r\n$ python -c \"from pip import pep425tags;print(pep425tags.supported_tags)\"\r\n[('cp27', 'cp27m', 'macosx_10_10_x86_64'), ('cp27', 'cp27m', 'macosx_10_10_intel'), ('cp27', 'cp27m', 'macosx_10_10_fat64'), ('cp27', 'cp27m', 'macosx_10_10_fat32'), ('cp27', 'cp27m', 'macosx_10_10_universal'), ('cp27', 'cp27m', 'macosx_10_9_x86_64'), ('cp27', 'cp27m', 'macosx_10_9_intel'), ('cp27', 'cp27m', 'macosx_10_9_fat64'), ('cp27', 'cp27m', 'macosx_10_9_fat32'), ('cp27', 'cp27m', 'macosx_10_9_universal'), ('cp27', 'cp27m', 'macosx_10_8_x86_64'), ('cp27', 'cp27m', 'macosx_10_8_intel'), ('cp27', 'cp27m', 'macosx_10_8_fat64'), ('cp27', 'cp27m', 'macosx_10_8_fat32'), ('cp27', 'cp27m', 'macosx_10_8_universal'), ('cp27', 'cp27m', 'macosx_10_7_x86_64'), ('cp27', 'cp27m', 'macosx_10_7_intel'), ('cp27', 'cp27m', 'macosx_10_7_fat64'), ('cp27', 'cp27m', 'macosx_10_7_fat32'), ('cp27', 'cp27m', 'macosx_10_7_universal'), ('cp27', 'cp27m', 'macosx_10_6_x86_64'), ('cp27', 'cp27m', 'macosx_10_6_intel'), ('cp27', 'cp27m', 'macosx_10_6_fat64'), ('cp27', 'cp27m', 'macosx_10_6_fat32'), ('cp27', 'cp27m', 'macosx_10_6_universal'), ('cp27', 'cp27m', 'macosx_10_5_x86_64'), ('cp27', 'cp27m', 'macosx_10_5_intel'), ('cp27', 'cp27m', 'macosx_10_5_fat64'), ('cp27', 'cp27m', 'macosx_10_5_fat32'), ('cp27', 'cp27m', 'macosx_10_5_universal'), ('cp27', 'cp27m', 'macosx_10_4_intel'), ('cp27', 'cp27m', 'macosx_10_4_fat32'), ('cp27', 'cp27m', 'macosx_10_4_universal'), ('cp27', 'cp27m', 'macosx_10_3_fat32'), ('cp27', 'cp27m', 'macosx_10_3_universal'), ('cp27', 'cp27m', 'macosx_10_2_fat32'), ('cp27', 'cp27m', 'macosx_10_2_universal'), ('cp27', 'cp27m', 'macosx_10_1_fat32'), ('cp27', 'cp27m', 'macosx_10_1_universal'), ('cp27', 'cp27m', 'macosx_10_0_fat32'), ('cp27', 'cp27m', 'macosx_10_0_universal'), ('cp27', 'none', 'macosx_10_10_x86_64'), ('cp27', 'none', 'macosx_10_10_intel'), ('cp27', 'none', 'macosx_10_10_fat64'), ('cp27', 'none', 'macosx_10_10_fat32'), ('cp27', 'none', 'macosx_10_10_universal'), ('cp27', 'none', 'macosx_10_9_x86_64'), ('cp27', 'none', 'macosx_10_9_intel'), ('cp27', 'none', 'macosx_10_9_fat64'), ('cp27', 'none', 'macosx_10_9_fat32'), ('cp27', 'none', 'macosx_10_9_universal'), ('cp27', 'none', 'macosx_10_8_x86_64'), ('cp27', 'none', 'macosx_10_8_intel'), ('cp27', 'none', 'macosx_10_8_fat64'), ('cp27', 'none', 'macosx_10_8_fat32'), ('cp27', 'none', 'macosx_10_8_universal'), ('cp27', 'none', 'macosx_10_7_x86_64'), ('cp27', 'none', 'macosx_10_7_intel'), ('cp27', 'none', 'macosx_10_7_fat64'), ('cp27', 'none', 'macosx_10_7_fat32'), ('cp27', 'none', 'macosx_10_7_universal'), ('cp27', 'none', 'macosx_10_6_x86_64'), ('cp27', 'none', 'macosx_10_6_intel'), ('cp27', 'none', 'macosx_10_6_fat64'), ('cp27', 'none', 'macosx_10_6_fat32'), ('cp27', 'none', 'macosx_10_6_universal'), ('cp27', 'none', 'macosx_10_5_x86_64'), ('cp27', 'none', 'macosx_10_5_intel'), ('cp27', 'none', 'macosx_10_5_fat64'), ('cp27', 'none', 'macosx_10_5_fat32'), ('cp27', 'none', 'macosx_10_5_universal'), ('cp27', 'none', 'macosx_10_4_intel'), ('cp27', 'none', 'macosx_10_4_fat32'), ('cp27', 'none', 'macosx_10_4_universal'), ('cp27', 'none', 'macosx_10_3_fat32'), ('cp27', 'none', 'macosx_10_3_universal'), ('cp27', 'none', 'macosx_10_2_fat32'), ('cp27', 'none', 'macosx_10_2_universal'), ('cp27', 'none', 'macosx_10_1_fat32'), ('cp27', 'none', 'macosx_10_1_universal'), ('cp27', 'none', 'macosx_10_0_fat32'), ('cp27', 'none', 'macosx_10_0_universal'), ('py2', 'none', 'macosx_10_10_x86_64'), ('py2', 'none', 'macosx_10_10_intel'), ('py2', 'none', 'macosx_10_10_fat64'), ('py2', 'none', 'macosx_10_10_fat32'), ('py2', 'none', 'macosx_10_10_universal'), ('py2', 'none', 'macosx_10_9_x86_64'), ('py2', 'none', 'macosx_10_9_intel'), ('py2', 'none', 'macosx_10_9_fat64'), ('py2', 'none', 'macosx_10_9_fat32'), ('py2', 'none', 'macosx_10_9_universal'), ('py2', 'none', 'macosx_10_8_x86_64'), ('py2', 'none', 'macosx_10_8_intel'), ('py2', 'none', 'macosx_10_8_fat64'), ('py2', 'none', 'macosx_10_8_fat32'), ('py2', 'none', 'macosx_10_8_universal'), ('py2', 'none', 'macosx_10_7_x86_64'), ('py2', 'none', 'macosx_10_7_intel'), ('py2', 'none', 'macosx_10_7_fat64'), ('py2', 'none', 'macosx_10_7_fat32'), ('py2', 'none', 'macosx_10_7_universal'), ('py2', 'none', 'macosx_10_6_x86_64'), ('py2', 'none', 'macosx_10_6_intel'), ('py2', 'none', 'macosx_10_6_fat64'), ('py2', 'none', 'macosx_10_6_fat32'), ('py2', 'none', 'macosx_10_6_universal'), ('py2', 'none', 'macosx_10_5_x86_64'), ('py2', 'none', 'macosx_10_5_intel'), ('py2', 'none', 'macosx_10_5_fat64'), ('py2', 'none', 'macosx_10_5_fat32'), ('py2', 'none', 'macosx_10_5_universal'), ('py2', 'none', 'macosx_10_4_intel'), ('py2', 'none', 'macosx_10_4_fat32'), ('py2', 'none', 'macosx_10_4_universal'), ('py2', 'none', 'macosx_10_3_fat32'), ('py2', 'none', 'macosx_10_3_universal'), ('py2', 'none', 'macosx_10_2_fat32'), ('py2', 'none', 'macosx_10_2_universal'), ('py2', 'none', 'macosx_10_1_fat32'), ('py2', 'none', 'macosx_10_1_universal'), ('py2', 'none', 'macosx_10_0_fat32'), ('py2', 'none', 'macosx_10_0_universal'), ('cp27', 'none', 'any'), ('cp2', 'none', 'any'), ('py27', 'none', 'any'), ('py2', 'none', 'any'), ('py26', 'none', 'any'), ('py25', 'none', 'any'), ('py24', 'none', 'any'), ('py23', 'none', 'any'), ('py22', 'none', 'any'), ('py21', 'none', 'any'), ('py20', 'none', 'any')]\r\n```\r\n\r\nTags in text format:\r\n[('cp27', 'cp27m', 'macosx_10_10_x86_64'), ('cp27', 'cp27m', 'macosx_10_10_intel'), ('cp27', 'cp27m', 'macosx_10_10_fat64'), ('cp27', 'cp27m', 'macosx_10_10_fat32'), ('cp27', 'cp27m', 'macosx_10_10_universal'), ('cp27', 'cp27m', 'macosx_10_9_x86_64'), ('cp27', 'cp27m', 'macosx_10_9_intel'), ('cp27', 'cp27m', 'macosx_10_9_fat64'), ('cp27', 'cp27m', 'macosx_10_9_fat32'), ('cp27', 'cp27m', 'macosx_10_9_universal'), ('cp27', 'cp27m', 'macosx_10_8_x86_64'), ('cp27', 'cp27m', 'macosx_10_8_intel'), ('cp27', 'cp27m', 'macosx_10_8_fat64'), ('cp27', 'cp27m', 'macosx_10_8_fat32'), ('cp27', 'cp27m', 'macosx_10_8_universal'), ('cp27', 'cp27m', 'macosx_10_7_x86_64'), ('cp27', 'cp27m', 'macosx_10_7_intel'), ('cp27', 'cp27m', 'macosx_10_7_fat64'), ('cp27', 'cp27m', 'macosx_10_7_fat32'), ('cp27', 'cp27m', 'macosx_10_7_universal'), ('cp27', 'cp27m', 'macosx_10_6_x86_64'), ('cp27', 'cp27m', 'macosx_10_6_intel'), ('cp27', 'cp27m', 'macosx_10_6_fat64'), ('cp27', 'cp27m', 'macosx_10_6_fat32'), ('cp27', 'cp27m', 'macosx_10_6_universal'), ('cp27', 'cp27m', 'macosx_10_5_x86_64'), ('cp27', 'cp27m', 'macosx_10_5_intel'), ('cp27', 'cp27m', 'macosx_10_5_fat64'), ('cp27', 'cp27m', 'macosx_10_5_fat32'), ('cp27', 'cp27m', 'macosx_10_5_universal'), ('cp27', 'cp27m', 'macosx_10_4_intel'), ('cp27', 'cp27m', 'macosx_10_4_fat32'), ('cp27', 'cp27m', 'macosx_10_4_universal'), ('cp27', 'cp27m', 'macosx_10_3_fat32'), ('cp27', 'cp27m', 'macosx_10_3_universal'), ('cp27', 'cp27m', 'macosx_10_2_fat32'), ('cp27', 'cp27m', 'macosx_10_2_universal'), ('cp27', 'cp27m', 'macosx_10_1_fat32'), ('cp27', 'cp27m', 'macosx_10_1_universal'), ('cp27', 'cp27m', 'macosx_10_0_fat32'), ('cp27', 'cp27m', 'macosx_10_0_universal'), ('cp27', 'none', 'macosx_10_10_x86_64'), ('cp27', 'none', 'macosx_10_10_intel'), ('cp27', 'none', 'macosx_10_10_fat64'), ('cp27', 'none', 'macosx_10_10_fat32'), ('cp27', 'none', 'macosx_10_10_universal'), ('cp27', 'none', 'macosx_10_9_x86_64'), ('cp27', 'none', 'macosx_10_9_intel'), ('cp27', 'none', 'macosx_10_9_fat64'), ('cp27', 'none', 'macosx_10_9_fat32'), ('cp27', 'none', 'macosx_10_9_universal'), ('cp27', 'none', 'macosx_10_8_x86_64'), ('cp27', 'none', 'macosx_10_8_intel'), ('cp27', 'none', 'macosx_10_8_fat64'), ('cp27', 'none', 'macosx_10_8_fat32'), ('cp27', 'none', 'macosx_10_8_universal'), ('cp27', 'none', 'macosx_10_7_x86_64'), ('cp27', 'none', 'macosx_10_7_intel'), ('cp27', 'none', 'macosx_10_7_fat64'), ('cp27', 'none', 'macosx_10_7_fat32'), ('cp27', 'none', 'macosx_10_7_universal'), ('cp27', 'none', 'macosx_10_6_x86_64'), ('cp27', 'none', 'macosx_10_6_intel'), ('cp27', 'none', 'macosx_10_6_fat64'), ('cp27', 'none', 'macosx_10_6_fat32'), ('cp27', 'none', 'macosx_10_6_universal'), ('cp27', 'none', 'macosx_10_5_x86_64'), ('cp27', 'none', 'macosx_10_5_intel'), ('cp27', 'none', 'macosx_10_5_fat64'), ('cp27', 'none', 'macosx_10_5_fat32'), ('cp27', 'none', 'macosx_10_5_universal'), ('cp27', 'none', 'macosx_10_4_intel'), ('cp27', 'none', 'macosx_10_4_fat32'), ('cp27', 'none', 'macosx_10_4_universal'), ('cp27', 'none', 'macosx_10_3_fat32'), ('cp27', 'none', 'macosx_10_3_universal'), ('cp27', 'none', 'macosx_10_2_fat32'), ('cp27', 'none', 'macosx_10_2_universal'), ('cp27', 'none', 'macosx_10_1_fat32'), ('cp27', 'none', 'macosx_10_1_universal'), ('cp27', 'none', 'macosx_10_0_fat32'), ('cp27', 'none', 'macosx_10_0_universal'), ('py2', 'none', 'macosx_10_10_x86_64'), ('py2', 'none', 'macosx_10_10_intel'), ('py2', 'none', 'macosx_10_10_fat64'), ('py2', 'none', 'macosx_10_10_fat32'), ('py2', 'none', 'macosx_10_10_universal'), ('py2', 'none', 'macosx_10_9_x86_64'), ('py2', 'none', 'macosx_10_9_intel'), ('py2', 'none', 'macosx_10_9_fat64'), ('py2', 'none', 'macosx_10_9_fat32'), ('py2', 'none', 'macosx_10_9_universal'), ('py2', 'none', 'macosx_10_8_x86_64'), ('py2', 'none', 'macosx_10_8_intel'), ('py2', 'none', 'macosx_10_8_fat64'), ('py2', 'none', 'macosx_10_8_fat32'), ('py2', 'none', 'macosx_10_8_universal'), ('py2', 'none', 'macosx_10_7_x86_64'), ('py2', 'none', 'macosx_10_7_intel'), ('py2', 'none', 'macosx_10_7_fat64'), ('py2', 'none', 'macosx_10_7_fat32'), ('py2', 'none', 'macosx_10_7_universal'), ('py2', 'none', 'macosx_10_6_x86_64'), ('py2', 'none', 'macosx_10_6_intel'), ('py2', 'none', 'macosx_10_6_fat64'), ('py2', 'none', 'macosx_10_6_fat32'), ('py2', 'none', 'macosx_10_6_universal'), ('py2', 'none', 'macosx_10_5_x86_64'), ('py2', 'none', 'macosx_10_5_intel'), ('py2', 'none', 'macosx_10_5_fat64'), ('py2', 'none', 'macosx_10_5_fat32'), ('py2', 'none', 'macosx_10_5_universal'), ('py2', 'none', 'macosx_10_4_intel'), ('py2', 'none', 'macosx_10_4_fat32'), ('py2', 'none', 'macosx_10_4_universal'), ('py2', 'none', 'macosx_10_3_fat32'), ('py2', 'none', 'macosx_10_3_universal'), ('py2', 'none', 'macosx_10_2_fat32'), ('py2', 'none', 'macosx_10_2_universal'), ('py2', 'none', 'macosx_10_1_fat32'), ('py2', 'none', 'macosx_10_1_universal'), ('py2', 'none', 'macosx_10_0_fat32'), ('py2', 'none', 'macosx_10_0_universal'), ('cp27', 'none', 'any'), ('cp2', 'none', 'any'), ('py27', 'none', 'any'), ('py2', 'none', 'any'), ('py26', 'none', 'any'), ('py25', 'none', 'any'), ('py24', 'none', 'any'), ('py23', 'none', 'any'), ('py22', 'none', 'any'), ('py21', 'none', 'any'), ('py20', 'none', 'any')]", "@pbilling thanks for the tags info. Looks like you have Mac OS X 10.10 installed. The pypi binary requires to have at least 10.11. One workaround is that you can try our uploaded binaries [here](https://www.tensorflow.org/install/install_mac#python_27). For example:\r\n`pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.2.0rc0-py2-none-any.whl` \r\n", "@yifeif OK, good to know. I think a Docker solution will work for me, but thanks for following up!", "@ChakritRakuang For your issue, you are using the wrong python version.\r\nAs our installation guide for windows states, TF only has support for python 3.5 on windows.\r\n\r\nYou can search through old issues for the technical challenges that made us decide not support python 2.7 on windows.", "tf-nightly 1.14.1.dev20190304 has requirement protobuf>=3.6.1, but you'll have protobuf 3.0.0b2 which is incompatible.\r\ntb-nightly 1.14.0a20190304 has requirement protobuf>=3.6.0, but you'll have protobuf 3.0.0b2 which is incompatible.\r\nwhat should i do?\r\n"]}, {"number": 10084, "title": "Adding NewScopeWithGraph func to Scope struct. This enables the construction of an in-memory graph from a previously serialized form + new additions.", "body": "Found a use case that wasn't covered while developing with tensorflow in go.\r\n\r\nSince working with multiple-graphs is an anti-pattern:\r\nhttp://stackoverflow.com/questions/35955144/working-with-multiple-graphs-in-tensorflow\r\n\r\nneeded to reuse a previously trained GraphDef file (binary) and then make additions. \r\n\r\nWhat do you think?", "comments": ["Can one of the admins verify this patch?", "Thanks for the PR @ctava \r\n\r\nWorking with multiple-graphs is not an anti-pattern in the Go API (or C++, or Java, or C for that matter) as these APIs do not have an implicit global graph like the Python API does. You should not hesitate from using multiple Graph objects in the same Go program.\r\n\r\nIf that is the case, do you still want to pursue this PR?\r\n\r\nIf you really need to extend an existing graph, then something like this might be necessary. I'm a bit torn between adding a `Scope.ImportGraphDef` vs. this approach. Currently, the `Scope` API makes for a clear separation between construction and use of the graph which will break if you can create a `Scope` with an arbitrary graph. It may be okay to forgo that property, but still, it is something that we're giving up :). (Adding a `Scope.ImportGraphDef` won't do that).\r\n\r\nDo let us know if you need this functionality to extend a graph (as opposed to creating a different one) and we can take the PR further.\r\n\r\nThanks!\r\n\r\nFYI @jhseu ", "@asimshankar it says, we make assumption that one session has one graph:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h#L59\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/08cb0ba5431ef2508c6ca63c498023a84f7b4a18/tensorflow/go/op/scope.go#L55\r\nwhen calling finalized on a scope - it outputs 1 graph. \r\n\r\nNewSession: accepts a graph:\r\nhttps://github.com/tensorflow/tensorflow/blob/08cb0ba5431ef2508c6ca63c498023a84f7b4a18/tensorflow/go/session.go#L51\r\n\r\nyea. i'd like to build a base Tensorflow graph, persist it and have the ability to hydrate it and extend it on a case-by-case basis\r\n", "@ctava : Ah, sorry, from the discussion in the stackoverflow thread and the use of the phrase \"anti-pattern\" you alluded to, I thought you were referring to multiple graphs in a single process. Multiple graphs in a session isn't an \"anti-pattern\" (in that it is possible but discouraged), it simply isn't possible :)\r\n\r\nThanks for the follow up, and the use case of wanting to extend an existing graph is quite reasonable. Just wanted to clarify your original comments. Will take a look at the PR now.\r\n", "@asimshankar changes have been made to the TestScopeWithGraph unit test. its much more concise now. ", "Jenkins test this please.", "Jenkins test this please."]}, {"number": 10083, "title": "Nullptr check failed, when using TensorArray in combination with while_loop and swap_memory", "body": "Environment: TensorFlow-gpu r1.1 build from source on Windows 10\r\n\r\nI am using the python API of TensorFlow to train a variant of an LSTM.\r\nFor that purpose I use the `tf.while_loop` function to iterate over the time steps\r\nWith `swap_memory=True` and not limiting devices to cpu I get the following failure:\r\n\r\n`...tensorflow/tensorflow/core/framework/tensor.cc:885] Check failed: nullptr != b.buf_ (nullptr vs. 00...)`\r\n\r\nWith `swap_memory=False` or limiting devices to cpu this does not happen.\r\n\r\nThe part of my code, that causes this failure (when commenting it out, it works) is in the body of the while loop:\r\n\r\n    ...\r\n    h_gathered = h_ta.gather(tf.range(time))\r\n    h_gathered = tf.transpose(h_gathered, [1, 0, 2])\r\n    syn_t = self.syntactic_weights_ta.read(time)[:, :time]\r\n    syn_t = tf.expand_dims(syn_t, 1)\r\n    syn_state_t = tf.squeeze(tf.tanh(tf.matmul(syn_t, h_gathered)), 1)\r\n    ...\r\n\r\nwhere `time` is zero based and incremented after each step, `h_ta` is a TensorArray\r\n\r\n    h_ta = tf.TensorArray(\r\n            dtype=dtype,\r\n            size=max_seq_len,\r\n            clear_after_read=False,\r\n            element_shape=[batch_size, num_hidden],\r\n            tensor_array_name=\"fw_output\")\r\nand `self.syntactic_weights_ta` is also a TensorArray\r\n\r\n    self.syntactic_weights_ta = tf.TensorArray(\r\n            dtype=dtype,\r\n            size=max_seq_len,\r\n            tensor_array_name=\"fw_syntactic_weights\")\r\n    self.syntactic_weights_ta = self.syntactic_weights_ta.unstack(syntactic_weights)\r\n\r\nWhat I am trying to achieve in the code snippet is basically a weighted sum over the past outputs, stored in `h_ta`.\r\nIn the end I train the network with `tf.train.AdamOptimizer`.\r\n\r\nThe forward propagation seems to work, as inserting `tf.Print` commands in the sensitive part of the code works.\r\nBut I guess the backward propagation \r\n\r\nUnfortunately I could not find out which tensor's buffer points to nullptr.\r\n", "comments": ["@yuanbyu it's possible that swap_memory takes a tensor that we're keeping track of inside the TensorArray and moves it off the CPU to the GPU.  When the TensorArray tries to access it, it's no longer there.\r\n\r\n@davzha we'll need the complete debug log and an example code snippet to reproduce...", "I was able to reproduce the error running the following script.\r\nSince I followed this guide https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake\r\nto build tensorflow-gpu r1.1, it does not support debugging, though if you still require a debug log I'll try to rebuild it.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rnn.python.ops.lstm_ops import _lstm_block_cell\r\nimport numpy as np\r\n\r\n\r\nclass SALSTMloop(object):\r\n    \"\"\"Syntax aware LSTM loop, using `_lstm_block_cell`.\r\n    \"\"\"\r\n\r\n    def __init__(self,\r\n                 num_units,\r\n                 input_ta,\r\n                 syntactic_weights_ta,\r\n                 max_sen_len,\r\n                 batch_size,\r\n                 forget_bias=1.0):\r\n        self._num_units = num_units\r\n        self._forget_bias = forget_bias\r\n        self._names = {\r\n            \"W\": \"weights\",\r\n            \"b\": \"biases\",\r\n            \"W_syn\": \"weight_syntactic\",\r\n            \"b_syn\": \"bias_syntactic\",\r\n            \"wci\": \"w_i_diag\",\r\n            \"wco\": \"w_o_diag\",\r\n            \"wcf\": \"w_f_diag\",\r\n            \"scope\": \"lstm_cell\"\r\n        }\r\n        self.max_sen_len = max_sen_len\r\n        self.zero_output = tf.zeros([batch_size, num_units])\r\n        self.input_ta = input_ta\r\n        self.syntactic_weights_ta = syntactic_weights_ta\r\n\r\n    def __call__(self, time, prev_state, h_ta, scope=None):\r\n        with tf.variable_scope(scope or self._names[\"scope\"]):\r\n            x = self.input_ta.read(time)\r\n            input_size = x.get_shape()[1].value\r\n\r\n            w = tf.get_variable(self._names[\"W\"], [\r\n                input_size + self._num_units, self._num_units * 4\r\n            ])\r\n            b = tf.get_variable(\r\n                self._names[\"b\"], [w.get_shape().with_rank(2)[1].value],\r\n                initializer=tf.constant_initializer(0.0))\r\n            w_syn = tf.get_variable(self._names[\"W_syn\"], [\r\n                input_size + self._num_units, self._num_units\r\n            ])\r\n            b_syn = tf.get_variable(\r\n                self._names[\"b_syn\"],\r\n                [w_syn.get_shape().with_rank(2)[1].value],\r\n                initializer=tf.constant_initializer(0.0))\r\n            wci = wco = wcf = tf.zeros([self._num_units])\r\n\r\n            (cs_prev, h_prev) = prev_state\r\n            (_, cs, _, _, _, _, h) = _lstm_block_cell(\r\n                x,\r\n                cs_prev,\r\n                h_prev,\r\n                w,\r\n                b,\r\n                wci=wci,\r\n                wco=wco,\r\n                wcf=wcf,\r\n                forget_bias=self._forget_bias,\r\n                use_peephole=False)\r\n\r\n            h_gathered = h_ta.gather(tf.range(time))\r\n            h_gathered = tf.transpose(h_gathered, [1, 0, 2])\r\n            syn_t = self.syntactic_weights_ta.read(time)[:, :time]\r\n\r\n            syn_t = tf.expand_dims(syn_t, 1)\r\n\r\n            syn_state_t = tf.squeeze(tf.tanh(tf.matmul(syn_t, h_gathered)), 1)\r\n            gate_syn = tf.matmul(tf.concat([x, h_prev], 1), w_syn) + b_syn\r\n            state_h = tf.multiply(gate_syn, syn_state_t)\r\n\r\n            h = h + state_h\r\n\r\n            new_state = (cs, h)\r\n            return (time + 1), new_state, h_ta.write(time, h)\r\n\r\n\r\nmax_sen_len = 60\r\ndtype = tf.float32\r\nnum_hidden = 512\r\nbatch_size = 100\r\ninit_state = (\r\n    tf.constant(0.0, dtype=tf.float32, shape=[batch_size, num_hidden]), ) * 2\r\nfw_syn_ta = tf.TensorArray(\r\n    dtype=dtype,\r\n    size=max_sen_len,\r\n    tensor_array_name=\"syntactic_weights\",\r\n    clear_after_read=False)\r\nsyntactic_weights = tf.random_uniform([max_sen_len, batch_size, max_sen_len])\r\nfw_syn_ta = fw_syn_ta.unstack(syntactic_weights)\r\ninputs = tf.constant(\r\n    np.arange(batch_size * max_sen_len * max_sen_len),\r\n    shape=[max_sen_len, batch_size, max_sen_len],\r\n    dtype=tf.float32)\r\n\r\nfw_input_ta = tf.TensorArray(\r\n    dtype=dtype, size=max_sen_len,\r\n    tensor_array_name=\"fw_input\").unstack(inputs)\r\nfw_output_ta = tf.TensorArray(\r\n    dtype=dtype,\r\n    size=max_sen_len,\r\n    clear_after_read=False,\r\n    element_shape=[batch_size, num_hidden],\r\n    tensor_array_name=\"fw_output\")\r\nfw_loop = SALSTMloop(\r\n    num_units=num_hidden,\r\n    input_ta=fw_input_ta,\r\n    syntactic_weights_ta=fw_syn_ta,\r\n    max_sen_len=max_sen_len,\r\n    batch_size=batch_size)\r\ntime = tf.constant(0, dtype=tf.int32, name=\"time\")\r\n\r\n_, fw_final_state, fw_final_output_ta = tf.while_loop(\r\n    lambda time, *_: time < max_sen_len,\r\n    fw_loop,\r\n    loop_vars=[time, init_state, fw_output_ta],\r\n    swap_memory=True)\r\nfw_output = fw_final_output_ta.stack()\r\nfw_final_output_ta.close()\r\n\r\nfinal_output = fw_final_output_ta.stack()\r\n\r\nloss = tf.reduce_sum(final_output)\r\n\r\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\r\n\r\ninit_op = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    sess.run(init_op)\r\n    output, _ = sess.run([final_output, train_op])\r\n```", "@ebrevdo What's the status of this issue?", "I am running into this problem, did anyone manage to solve it?", "Nagging Assignee @ebrevdo: It has been 259 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The error is no longer reproducible with a recent version of TF (e.g. a nightly from the past month) - so I'm assuming the bug has been fixed."]}, {"number": 10082, "title": "Automatically convert inputs to tensors in Dataset.from_tensor_slices", "body": "Allows usage like:\r\n```python\r\ntf.contrib.data.Dataset.from_tensor_slices([['img1.jpg', 'img2.jpg'], [1, 2]])\r\n```\r\nOtherwise, the two input lists (2 of size 2 here) were flattened into 1 list (of size 4), which resulted in a \"list index out of range\" error.\r\n\r\nMy correction was inspired by how the `tf.train.slice_input_producer` is defined: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/input.py#L302\r\n\r\nAlso see pull request #10079.\r\n", "comments": ["Jenkins test this please.", "Jenkins test this please.", "@omoindrot ping?", "@drpngx I answered @mrry in the comments of the change, but I am waiting for his answer.\r\n\r\nI can make the simple change he requested (just modifying the error raised), but I'd like to find a better solution.", "ping @mrry "]}, {"number": 10081, "title": "Nightly binaries are not being built successfully", "body": "Seems like the nightly builds have been failing for a while. For example, Python 3 Mac GPU build history [here](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/). Is this something being worked on since it's been a month or so. ", "comments": ["According to @martinwicke Mac slaves are down (Google IO craziness?)\r\nI've put a macos version built from head 4 days here: https://github.com/yaroslavvb/tensorflow-community-wheels/issues/18", "Thanks! @yaroslavvb ", "If you are interested in mac-gpu pip packages, we are dropping support for them starting 1.2, as announced during 1.1 release.\r\nThe rest, we are now triaging all remaining issues.", "Thanks! Please keep me posted. ", "This was fixed long time ago but seems like it's failing again. For example, last successful [MAC CPU nightly build](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/) was on June 28th. Just in case you are not aware of this. ", "As nightlies run a much larger suite of tests, their breakages are expected. We try to fix them before each release.\r\n@av8ramit could you take a look into the failures to see if they are flakes or actual failures.\r\nPlease escalate to owners of components if you find actual breakages.", "http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/544/\r\nNightlies are passing now!", "Great. Thanks!\n\nOn Fri, Jul 7, 2017 at 2:28 PM, Amit Patankar <notifications@github.com>\nwrote:\n\n> http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/544/\n> Nightlies are passing now!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10081#issuecomment-313758354>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEEnSlbidVc-bYliDY8vx0JVHnkAwmBcks5sLnjUgaJpZM4NhqfE>\n> .\n>\n"]}, {"number": 10080, "title": "compilation errors due to missing op classes when using selective registration (cmake windows build)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 7\r\n- **TensorFlow installed from (source or binary)**:\r\nFrom source\r\n- **TensorFlow version (use command below)**:\r\ncommit 280374\r\n- **Bazel version (if compiling from source)**:\r\nn/a, using cmake build\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\n\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DCMAKE_INCLUDE_CURRENT_DIR:BOOL=ON -DCMAKE_CXX_FLAGS=-DSELECTIVE_REGISTRATION -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX -Dtensorflow_BUILD_PYTHON_BINDINGS:BOOL=OFF -Dtensorflow_BUILD_CONTRIB_KERNELS:BOOL=OFF -Dtensorflow_BUILD_SHARED_LIB:BOOL=ON \r\n\r\nMSBuild /p:Configuration=Release tensorflow.vcxproj\r\n\r\n### Describe the problem\r\n\r\nI'm building with cmake on Windows, using selective registration.  I've put ops_to_register.h in my build dir and specified -DCMAKE_INCLUDE_CURRENT_DIR=ON in my cmake command so that it would be added to the include path.\r\n\r\nDue (I'm assuming) to selective registration, the generated code omits many classes which would otherwise be generated in a full build.\r\n\r\nWhen compiling non-generated code, e.g. tensorflow\\cc\\gradients\\array_grad.cc, there are compile errors wherever it references the classes that were not generated due to selective registration, for example:\r\n\r\narray_grad.cc(51): error C2653: 'Unstack': is not a class or namespace name [...omitted...\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_cc.vcxproj]\r\n\r\nThere are many of these errors for various referenced classes, in various other source files such as:\r\ntensorflow\\cc\\gradients\\math_grad.cc\r\ntensorflow\\cc\\gradients\\nn_grad.cc\r\ntensorflow\\cc\\framework\\gradients.cc\r\n\r\nWhen using selective registration, what's the right approach to avoid these errors?\r\n", "comments": ["I'm not aware of anybody who's successfully using the selective registration feature with the CMake build... those build rules are designed to do the minimal amount possible to get the Windows PIP packages built while we wait for full Bazel for Windows support.\r\n\r\nI suspect we'd need more specific targets in the CMake build to support selective registration (e.g. excluding the C++ gradients code). You might be able to start by looking at the dependencies for the Bazel Android targets here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e4296aefff97e6edd3d7cee9a09b9dd77da4c034/tensorflow/core/BUILD#L835\r\n\r\nI'm going to mark this as contributions welcome, but please feel free to ask more questions on here if you're interested in pursuing this feature.", "As contrib folder is deprecated , this issue is not relevant."]}, {"number": 10079, "title": "Fix classification dataset example in readme", "body": "Using python lists results in an IndexError for me.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please."]}, {"number": 10078, "title": "I get some error when I'm using tensorflow at the window anaconda", "body": "> Please\r\n\r\n go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI installed it at the window.\r\nWhen I'm test sample code and print it describes messages.\r\nI repeated it.\r\nAt that time it succesfully well.\r\nHow can I solve this it?\r\nIs it installed well?\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n\r\n(C:\\Users\\KBS\\Anaconda3) C:\\Users\\KBS>activate tensorflow\r\n\r\n(tensorflow) C:\\Users\\KBS>python\r\nPython 3.5.3 | packaged by conda-forge | (default, May 12 2017, 16:16:49) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> a=tf.constant('a')\r\n>>> sess=tf.Session()\r\n>>> print(sess.run(a))\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nb'a'\r\n>>> print(sess.run(a))\r\nb'a'\r\n>>>\r\n", "comments": ["This error was only present in TensorFlow 1.0 for Windows. Upgrading to the latest stable release (1.1) or release candidate (1.2rc0) will get rid of these messages."]}, {"number": 10077, "title": "Error happens when using tensorboard:No handlers could be found for logger \"werkzeug\"", "body": "I install the tensorflow-gpu 1.1.0 by anaconda-pip, but every time i open http://0.0.0.0:6006 to start tensorboard the error happens.I have already installed  \"werkzeug\".\r\n\r\n`No handlers could be found for logger \"werkzeug\"`\r\n", "comments": ["i have the same problem\r\nhave u solved it ?", "I am also facing this issue. Has anybody solved this issue?", "Does this problem happen with 1.2.0rc0? What operating system are you guys using? ", "@jart I am using ubuntu-xenial-16.04 and installed tensorflow using \"sudo pip install --upgrade tensorflow\". Moreover issue came recently when I have upgraded tensorflow and also, even on new machine if i install it for the first time, issue comes.", "@jart \r\nThis problem also happen with 1.2.0rc1\r\nSystem: OSX 10.12.5\r\nPython: 2.7.13\r\ntensorflow: 1.2.0rc1", "@jart @phipps553 @abhishekg2389 \r\nI found a way to fix this\r\nIf use relative path like\r\n```\r\ntensorboard --logdir=./logs/\r\n```\r\nThis problem will happen\r\nBut if I use absolute path like\r\n```\r\ntensorboard --logdir=/Users/wanqianjun/Desktop/logs\r\n```\r\nIt works well", "@wqj97 It is still giving that problem with absolute path", "I move the logs folder to home dir (~) ,fix the problem ", "@abhishekg2389 \r\n```\r\ncd ~\r\ntensorboard --logdir= \"absolute path\"\r\n```", "@wqj97 @jewdore Your way fix the problem.Thanks a lot!", "I had the same problem and I suspect that having non-ASCII characters in the path was the cause, removing accents from the path solved the problem for me (in addition to using an absolute path).", "The true reason for this problem is that there shouldn't be Chinese words in the path"]}, {"number": 10076, "title": "Update RELEASE.md", "body": "typo correction", "comments": ["Can one of the admins verify this patch?", "Jenkins test this please."]}, {"number": 10075, "title": "Missing includes in whl-file (cmake)", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: latest master\r\n- **Bazel version (if compiling from source)**: CMAKE\r\n- **Exact command to reproduce**: MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\n\r\n### Describe the problem\r\n- Using the cmake build command results in the successfull build of the file:\r\ntensorflow\\contrib\\cmake\\build\\tf_python\\dist\\tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl\r\n- In the whl-file, the header files (obtained via setup.py) are missing.\r\n- The header files should be included in the whl file if I am not mistaken. I am not sure what the origin of the missing files is. \r\n\r\n### Source code / logs\r\nThe desired header files can be manually installed by calling \r\n```\r\npython tensorflow\\contrib\\cmake\\build\\tf_python\\setup.py install_headers\r\n```\r\n", "comments": ["I've added the header files to the cmake builds. They mirror the other pip packages in the latest nightly builds."]}, {"number": 10074, "title": "fatal problem with saving variables", "body": "I coded a simple feedforward neural network and it works very well.\r\n\r\nI tried to save the computation time, and created:\r\n**self.total_time** = tf.Variable(0, dtype = tf.float32, trainable = True, name = 'total_time')\r\nin the fnn class.\r\n\r\nand i tried to print the total training time per some training epoch.\r\n**I made it to grow with time:**\r\n\r\n**# Check & Print training time**\r\ntill_now = time.time() - start_time\r\nself.total_time += till_now\r\nprint_time(self.total_time.eval())\r\n\r\n**and the result look something like this :** \r\n\r\nEpoch :   0 | Evaluation :  115 | Learning Rate : 0.50\r\n-------------------------------------------------------\r\nTraining Loss :         0.040919\r\nValidation Loss :      0.0741969\r\nValidation Accuracy :      97.77%\r\nTotal time cost : 0.38 seconds\r\n\r\nEpoch :   1 | Evaluation :  116 | Learning Rate : 0.50\r\n-------------------------------------------------------\r\nTraining Loss :        0.0417941\r\nValidation Loss :       0.073841\r\nValidation Accuracy :      97.73%\r\nTotal time cost : 0.71 seconds\r\n\r\nEpoch :   2 | Evaluation :  117 | Learning Rate : 0.50\r\n-------------------------------------------------------\r\nTraining Loss :        0.0334573\r\nValidation Loss :      0.0745566\r\nValidation Accuracy :      97.75%\r\nTotal time cost : 1.01 seconds\r\n\r\nHowever, When i interrupt the training procedure and try to restore global variables and restart the training, I just lose the value of **variable total_time** and it initialized as 0 which is the value i first give to.\r\nI also checked tf.global_variables() include self.total_time.\r\n\r\nWhat is wrong?", "comments": ["I found that when **self.total_time += till_now**, the tensor name was changed.\r\nit can be solved with assign function"]}]