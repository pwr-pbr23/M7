[{"number": 27866, "title": "systemlibs: Unbundle more third_party libraries", "body": "series to enable more libs to be unbundled with TF_SYSTEM_LIBS.\r\n", "comments": []}, {"number": 27865, "title": "sysconfig: fix library name on macos", "body": "On darwin, the library is named libtensorflow_framework.2.dylib\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["Finally resolves #27430 (where it was noted that MacOS is failing to build)."]}, {"number": 27864, "title": "Tf lite cross compilation", "body": "This pull request add the support of cross compiling variables.\r\nIt allow to cross compile TFLite library and python module for platforms other than aarch64, Rpi and iOS.\r\nIt has been successfully tested with Yocto Openembedded environment to build TensorFlow Lite for the new STM32MP1 Microprocessor target from STMicroelectronics.\r\n\r\nBR\r\nVincent Abriou", "comments": ["Hello,\r\n\r\nHave you any update concerning this pull request?\r\n\r\nThanks\r\nBR\r\nVincent", "Can one of the admins verify this patch?", "Thanks Vincent for the contribution.\r\nIt seems that your PR is bit outdated to apply. Could you check if you still need a patch for your Yocto Openembedded environment with the recent TFLite release?", "Terry, I see that my patches could not be applied on the last version.\r\nI will update my patches according to the last TensorFlow version to check what is still missing.\r\nIf any missing stuff, I will push a new pull request.\r\n\r\nBR\r\nVincent ", "Got it. Then let's close this PR until it's needed again."]}, {"number": 27863, "title": "Log failure reason in org.tensorflow.TensorFlow.init()", "body": "Loading the native library can fail for various reasons, especially if it is being run on a machine other than where the tensorflow c++ code was compiled. Unfortunately it can be hard to determine the reason because Java seems to throw away the cause when it throws a `NoClassDefFoundError`.\r\n\r\nWe have worked around this for now by manually calling `TensorFlow.init()` in a try-catch block before calling any other tensorflow code. A better solution would be if the library logged the failure before rethrowing the exception.\r\n\r\nDo you think this is acceptable? I'm very happy to change the message or use some other method of logging.", "comments": []}, {"number": 27862, "title": "Dimension Mismatch error in [image_captioning.ipynb]", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:2.2.0 alpha\r\n- Doc Link: [https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/image_captioning.ipynb](url)\r\n\r\nI am running this example with the Google colab and getting this issue: \r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1818   try:\r\n-> 1819     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1820   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: Dimension 0 in both shapes must be equal, but are 1 and 32. Shapes are [1,1] and [32,1]. for 'rnn__decoder/concat' (op: 'ConcatV2') with input shapes: [1,1,256], [32,1,256], [] and with computed input tensors: input[2] = <-1>.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-40-bdc59db794fd> in <module>()\r\n      6 \r\n      7     for (batch, (img_tensor, target)) in enumerate(dataset):\r\n----> 8         batch_loss, t_loss = train_step(img_tensor, target)\r\n      9         total_loss += t_loss\r\n     10 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    412       # In this case we have created variables on the first call, so we run the\r\n    413       # defunned version which is guaranteed to never create variables.\r\n--> 414       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    415     elif self._stateful_fn is not None:\r\n    416       # In this case we have not created variables on the first call. So we can\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1285   def __call__(self, *args, **kwargs):\r\n   1286     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n-> 1287     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   1288     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1289 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   1609           relaxed_arg_shapes)\r\n   1610       graph_function = self._create_graph_function(\r\n-> 1611           args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\r\n   1612       self._function_cache.arg_relaxed[rank_only_cache_key] = graph_function\r\n   1613 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   1510             arg_names=arg_names,\r\n   1511             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 1512             capture_by_value=self._capture_by_value),\r\n   1513         self._function_attributes)\r\n   1514 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    692                                           converted_func)\r\n    693 \r\n--> 694       func_outputs = python_func(*func_args, **func_kwargs)\r\n    695 \r\n    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    316         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    318     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    319 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    684                   optional_features=autograph_options,\r\n    685                   force_conversion=True,\r\n--> 686               ), args, kwargs)\r\n    687 \r\n    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n    390     return _call_unconverted(f, args, kwargs)\r\n    391 \r\n--> 392   result = converted_f(*effective_args, **kwargs)\r\n    393 \r\n    394   # The converted function's closure is simply inserted into the function's\r\n\r\n/tmp/tmp3uq18dwt.py in tf__train_step(img_tensor, target)\r\n     15       dec_input_1 = ag__.converted_call('expand_dims', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_6, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (target[:, (i)], 1), {})\r\n     16       return dec_input_1, loss_1, hidden_1\r\n---> 17     dec_input, loss, hidden = ag__.for_stmt(ag__.converted_call(range, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (1, target.shape[1]), {}), None, loop_body, (dec_input, loss, hidden))\r\n     18   total_loss = loss / ag__.converted_call(int, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_7, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (target.shape[1],), {})\r\n     19   trainable_variables = encoder.trainable_variables + decoder.trainable_variables\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py in for_stmt(iter_, extra_test, body, init_state)\r\n     79     return _dataset_for_stmt(iter_, extra_test, body, init_state)\r\n     80   else:\r\n---> 81     return _py_for_stmt(iter_, extra_test, body, init_state)\r\n     82 \r\n     83 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py in _py_for_stmt(iter_, extra_test, body, init_state)\r\n     88     if extra_test is not None and not extra_test(*state):\r\n     89       break\r\n---> 90     state = body(target, *state)\r\n     91   return state\r\n     92 \r\n\r\n/tmp/tmp3uq18dwt.py in loop_body(loop_vars, dec_input_1, loss_1, hidden_1)\r\n     11     def loop_body(loop_vars, dec_input_1, loss_1, hidden_1):\r\n     12       i = loop_vars\r\n---> 13       predictions, hidden_1, _ = ag__.converted_call(decoder, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_4, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (dec_input_1, features, hidden_1), {})\r\n     14       loss_1 += ag__.converted_call(loss_function, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_5, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (target[:, (i)], predictions), {})\r\n     15       dec_input_1 = ag__.converted_call('expand_dims', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_6, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (target[:, (i)], 1), {})\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n    265 \r\n    266   if not options.force_conversion and conversion.is_whitelisted_for_graph(f):\r\n--> 267     return _call_unconverted(f, args, kwargs)\r\n    268 \r\n    269   # internal_convert_user_code is for example turned off when issuing a dynamic\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs)\r\n    186     return f.__self__.call(args, kwargs)\r\n    187 \r\n--> 188   return f(*args, **kwargs)\r\n    189 \r\n    190 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    610                       base_layer_utils.AutoAddUpdates(self,\r\n    611                                                       inputs)) as auto_updater:\r\n--> 612                 outputs = self.call(inputs, *args, **kwargs)\r\n    613                 auto_updater.set_outputs(outputs)\r\n    614 \r\n\r\n<ipython-input-33-dc6f8641457a> in call(self, x, features, hidden)\r\n     22 \r\n     23     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\r\n---> 24     x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\r\n     25 \r\n     26     # passing the concatenated vector to the GRU\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in concat(values, axis, name)\r\n   1269               tensor_shape.scalar())\r\n   1270       return identity(values[0], name=scope)\r\n-> 1271   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n   1272 \r\n   1273 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in concat_v2(values, axis, name)\r\n   1215   _attr_N = len(values)\r\n   1216   _, _, _op = _op_def_lib._apply_op_helper(\r\n-> 1217         \"ConcatV2\", values=values, axis=axis, name=name)\r\n   1218   _result = _op.outputs[:]\r\n   1219   _inputs_flat = _op.inputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    798         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    799                          input_types=input_types, attrs=attr_protos,\r\n--> 800                          op_def=op_def)\r\n    801       return output_structure, op_def.is_stateful, op\r\n    802 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n    450     return super(FuncGraph, self).create_op(\r\n    451         op_type, inputs, dtypes, input_types, name, attrs, op_def,\r\n--> 452         compute_device=compute_device)\r\n    453 \r\n    454   def capture(self, tensor, name=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(***failed resolving arguments***)\r\n   3477           input_types=input_types,\r\n   3478           original_op=self._default_original_op,\r\n-> 3479           op_def=op_def)\r\n   3480       self._create_op_helper(ret, compute_device=compute_device)\r\n   3481     return ret\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1981           op_def, inputs, node_def.attr)\r\n   1982       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\r\n-> 1983                                 control_input_ops)\r\n   1984 \r\n   1985     # Initialize self._outputs.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1820   except errors.InvalidArgumentError as e:\r\n   1821     # Convert to ValueError for backwards compatibility.\r\n-> 1822     raise ValueError(str(e))\r\n   1823 \r\n   1824   return c_op\r\n\r\nValueError: Dimension 0 in both shapes must be equal, but are 1 and 32. Shapes are [1,1] and [32,1]. for 'rnn__decoder/concat' (op: 'ConcatV2') with input shapes: [1,1,256], [32,1,256], [] and with computed input tensors: input[2] = <-1>.\r\n```\r\n\r\n**I checked different thing but can't able to solve this issue**\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["I am unable to access your` image_captioning.ipynb`. Hence, it is difficult to rectify your issue without seeing your code. From your error message I can suggest you that check dimensions of each layer. You did mistake in defining input shapes and corresponding output shapes. Concentrate in the below error lines. \r\n\r\n`InvalidArgumentError: Dimension 0 in both shapes must be equal, but are 1 and 32. Shapes are [1,1] and [32,1]. for 'rnn__decoder/concat' (op: 'ConcatV2') with input shapes: [1,1,256], [32,1,256], [] and with computed input tensors: input[2] = <-1>.`\r\n\r\n", "@muneeb699 Please provide the correct google colab link to reproduce the issue reported here. Thanks!\r\n", "Hi Here is the link :\r\n[https://colab.research.google.com/drive/1lORosF4OMpsG2iEhyJzAXikEQEcViWE2](url)\r\n\r\nI did change just the train test split and I got this error. Otherwise i didn't change anything", "@muneeb699 Still the colab link that you have provided is not correct. Please provide correct link. Thanks!", "Copy the link and paste it. It will not be open when you click it. \r\nhttps://colab.research.google.com/drive/1lORosF4OMpsG2iEhyJzAXikEQEcViWE2", "Your problem has been solved. Change **BATCH_SIZE = 48**\r\n\r\n```\r\nBATCH_SIZE = 48\r\nBUFFER_SIZE = 1000\r\nembedding_dim = 256\r\nunits = 512\r\nvocab_size = len(tokenizer.word_index) + 1\r\nnum_steps = len(img_name_train) // BATCH_SIZE\r\n# shape of the vector extracted from InceptionV3 is (64, 2048)\r\n# these two variables represent that\r\nfeatures_shape = 2048\r\nattention_features_shape = 64\r\n```", "Why this problem is occured in the first place? Because you just change the batch size and it gives an error. Doesn't seems right.\r\nI am experimenting with different dataset and same error appeared again if i do the train test split zero", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "The batch size missmatch was fixed in tensorflow/docs@9e7ea7c446e7f40b3d1"]}, {"number": 27861, "title": "Tensorflow 2.0 keras load_model does not restore step / epoch counters", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, see example below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0 alpha 0 - gpu\r\n- Python version: 3.5.3\r\n- CUDA/cuDNN version: Cuda 10\r\n- GPU model and memory: GeForce 960M \r\n\r\n**Describe the current behavior**\r\nI train a keras model and let it be saved by the `keras.callbacks.ModelCheckpoint`. After the training is finished, I want to continue it from the saved checkpoint. Therefore, I load the model with `keras.models.load_model` and run `fit` again. Unfortunately, it seems that the steps/epochs counters are not restored. \r\n\r\n**Describe the expected behavior**\r\nThe steps /epochs counters should be restored to allow continuation of training. If they are not restored, e.g. tensorboard cannot be used properly, since the new training will write its values to the same steps as the first training, instead of appending them. The result looks like this: \r\n\r\n![Unbenannt](https://user-images.githubusercontent.com/9267365/56139393-12b50900-5f99-11e9-9c4e-b553ab2c1589.PNG)\r\n\r\n\r\n**Code to reproduce the issue**\r\nRun the following code multiple times and have a look at the tensorboard results by running `tensorboard --logdir=test_outputs`.\r\n\r\n```\r\nimport os\r\nfrom tensorflow import keras\r\n\r\nlog_dir = 'test_outputs'\r\nmodel_file = os.path.join(log_dir, \"model.hdf5\")\r\n\r\n(x_train, y_train), _ = keras.datasets.mnist.load_data()\r\n\r\n\r\ndef create_model():\r\n\tmodel = keras.models.Sequential([\r\n\t\tkeras.layers.Flatten(input_shape=(28, 28), name=\"flatten\"),\r\n\t\tkeras.layers.Dense(128, activation='relu', name=\"dense1\"),\r\n\t\tkeras.layers.Dropout(0.2, name=\"dropout\"),\r\n\t\tkeras.layers.Dense(10, activation='softmax', name=\"dense2\")\r\n\t])\r\n\r\n\tmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\treturn model\r\n\r\n\r\nif os.path.exists(model_file):\r\n\tmodel = keras.models.load_model(model_file)\r\nelse:\r\n\tmodel = create_model()\r\n\r\nmodel.fit(x_train, y_train, epochs=5, callbacks=[keras.callbacks.TensorBoard(log_dir=log_dir), keras.callbacks.ModelCheckpoint(model_file)])\r\n```\r\n", "comments": ["FWIW\r\nit's possible to work around with model.fit (initial_epoch = ...)\r\nobviously initial_epoch has to be stored somewhere - custom callback or as a part of checkpoint filename", "This is the intended behavior; step and epoch are reset with call to model.fit. Optimizer.iterations should maintain its count, such that any learning rate adjustments reflect the model's history, but epoch count is designed to restart with new training loop, even when loaded from a serialized model.", "But how can I then continue training a model and see the correct graph in Tensorboard? If I restore a checkpoint, I expect to be able to continue working with it. This is a crucial feature!\r\n\r\n@karmel: Can you please explain, how we can handle this need?\r\n", "@karmel `Model.fit` allows an `initial_epoch` argument. Surely this is evidence that the epoch count is designed to support restarting from an existing checkpoint, even if it defaults to restarting from 0?\r\n\r\n@andreas-eberle I've had similar frustrations between this and `ModelCheckpoint`. I've ended up writing my own saving/restoration callback and (for the moment) hacking the `TensorBoard` callback to do what is, in my opinion, \"the right thing\", but an independent implementation is probably wise at some point to ensure forwards compatibility (I'm accessing private attrs).\r\n\r\nIt's hacky and gross and written for eager execution. There are slight issues here surrounding the final element of each epoch which might have fewer members than `batch_size`, but you might be able to adapt it enough to be suitable for your purposes.\r\n\r\n```python\r\nclass CheckpointManagerCallback(tf.keras.callbacks.Callback):\r\n    \"\"\"\r\n    Callback wraping `tf.train.CheckpointManager`.\r\n\r\n    Restores previous checkpoint `on_train_begin`\r\n\r\n    Example usage:\r\n    ```python\r\n    model = get_model(...)\r\n    model.compile(optimizer=optimizer, ...)\r\n    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\r\n    manager = tf.train.CheckpointManager(\r\n        checkpoint, '/tmp/my_model', max_to_keep=5)\r\n    callback = CheckpointManagerCallback(checkpoint, manager, period=1)\r\n\r\n    model.fit(..., callbacks=[callbacks])\r\n    ```\r\n    \"\"\"\r\n    def __init__(self, checkpoint, manager, period=1, save_on_train_end=True):\r\n        self._manager = manager\r\n        self._checkpoint = checkpoint\r\n        self._period = period\r\n        self._save_on_train_end = save_on_train_end\r\n        self._restored = False\r\n        self._epoch_count = None\r\n        self._last_save = None\r\n\r\n    def _on_begin(self):\r\n        if not self._restored:\r\n            self.restore()\r\n\r\n    def restore(self, save_path=None):\r\n        if save_path is None:\r\n            save_path = self._manager.latest_checkpoint\r\n        self._checkpoint.restore(save_path)\r\n        self._restored = True\r\n\r\n    def on_train_begin(self, logs=None):\r\n        self._on_begin()\r\n\r\n    def on_test_begin(self, logs=None):\r\n        self._on_begin()\r\n\r\n    def on_predict_begin(self, logs=None):\r\n        self._on_begin()\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        epochs_finished = epoch + 1\r\n        self._epoch_count = epochs_finished\r\n        if epochs_finished % self._period == 0:\r\n            self._save()\r\n\r\n    def on_train_end(self, logs=None):\r\n        if self._save_on_train_end:\r\n            self._save()\r\n\r\n    def _save(self):\r\n        if self._epoch_count is None:\r\n            return\r\n        if self._last_save != self._epoch_count:\r\n            self._manager.save(self._epoch_count)\r\n            self._last_save = self._epoch_count\r\n\r\ncheckpoint = tf.train.Checkpoint(\r\n    model=model, optimizer=model.optimizer)\r\nmanager = tf.train.CheckpointManager(\r\n    checkpoint, model_dir, max_to_keep=max_ckpts_to_keep)\r\nsaver_callback = cb.CheckpointManagerCallback(\r\n    checkpoint, manager, period=checkpoint_freq)\r\n\r\nsaver_callback.restore()\r\n\r\ntb_callback = tf.keras.callbacks.TensorBoard(\r\n    write_graph=False, log_dir=model_dir, update_freq=summary_freq)\r\n\r\ntb_callback._total_batches_seen = initial_train_steps\r\ntb_callback._samples_seen = initial_train_steps * batch_size\r\nif val_steps_per_epoch is not None:\r\n    initial_val_steps = initial_epoch*val_steps_per_epoch\r\n    tb_callback._total_val_batches_seen = initial_val_steps\r\n\r\nmodel.fit(callbacks=[saver_callback, tb_callback], initial_epoch=initial_epoch, ...)\r\n```\r\n\r\nComing from the `estimator` API it sure would be nice if keras had something close to feature parity at least by 2.0. Obvious issues like this being labelled as \"working as intended\" doesn't give me much confidence...", "Also think that the behavior is not intended... For instance, many *learning rate schedulers* will look at the *epoch* number to adjust the learning rate. So when continuing from a checkpoint, the learning rate will be completely wrong...", "I have the same issue. Restoring current epoch is important on some occasions. For example, if I use L1 cost in the first 1000 epochs, and turn into L2 cost afterwards. When I restore a ckpt, I'd like to know how many epochs have elapsed. So as per the doc, can this be used? In the training loop, step can be used to record epoch etc. [https://www.tensorflow.org/guide/checkpoint](url)\r\nWell, it is probably hard to use this way in model.fit(..), but I prefer writing custom training loops to using model.fit in tf2.x, since it has become much easier to do so than in tf1.x, and it gives us loads of power to control everything our own ways.\r\n```\r\nckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=net, iterator=iterator)\r\n## in the training loop, do that after each epoch\r\nckpt.step.assign_add(1)\r\n```", "Please reopen. It would be great to at least define the current epoch and step count. Otherwise the `tf.keras.callbacks.ModelCheckpoint(f'{self.checkpoint_dir}/{{epoch}}')`\r\noverwrites the existing checkpoints.", "+1, this seems like a bizarrely missing feature that renders training resumption impractical for real use cases, at least without cumbersome custom code/callbacks to manually save and restore the epoch."]}, {"number": 27860, "title": "AttributeError: module 'tensorflow.tools.api.generator.api.estimator' has no attribute 'SessionRunHook'", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 27859, "title": "Undef OPAQUE marco conflicting with TensorFlow symbol.", "body": "See issue #25773.\r\n@jlebar @rmlarsen.", "comments": ["I think Eigen is managed as a separate repository and copy/pasted into TF (?), so the PR should be against https://bitbucket.org/eigen/eigen/ ?\r\n\r\nBut @rmlarsen I also notice that third_party/eigen3 hasn't been updated since Nov 6 2018.  Not sure if that means that adding this one-line fix is going to require a massive Eigen upgrade?", "third_party/eigen3 is synced with upstream roughly weekly. Feel free to\nsend a PR for Eigen to fix this issue.\n\nOn Mon, Apr 15, 2019 at 8:35 AM Justin Lebar <notifications@github.com>\nwrote:\n\n> I think Eigen is managed as a separate repository and copy/pasted into TF\n> (?), so the PR should be against https://bitbucket.org/eigen/eigen/ ?\n>\n> But @rmlarsen <https://github.com/rmlarsen> I also notice that\n> third_party/eigen3 hasn't been updated since Nov 6 2018. Not sure if that\n> means that adding this one-line fix is going to require a massive Eigen\n> upgrade?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27859#issuecomment-483301402>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQH9DphGPPIkrMysygmWNmPAtXrCl9Yfks5vhJxYgaJpZM4cvkCX>\n> .\n>\n", "@rmlarsen [third_party/eigen3/unsupported/Eigen/CXX11/Tensor](https://github.com/tensorflow/tensorflow/blob/master/third_party/eigen3/unsupported/Eigen/CXX11/Tensor) is not the same as [src/default/unsupported/Eigen/CXX11/Tensor](https://bitbucket.org/eigen/eigen/src/default/unsupported/Eigen/CXX11/Tensor).\r\n\r\nThere are also commits like [this](https://github.com/tensorflow/tensorflow/commit/d0d975f8c3330b5402263b2356b038bc8af919a2#diff-dd67b23046f83602c7172fd9457a44c8) affecting [third_party/eigen3/unsupported/Eigen/CXX11/Tensor](https://github.com/tensorflow/tensorflow/blob/master/third_party/eigen3/unsupported/Eigen/CXX11/Tensor) on TensorFlow repo. @jlebar So I do not think it is managed as a separate repository.", "Ah, my bad!\n\nOn Tue, Apr 16, 2019 at 4:49 AM Stepiii <notifications@github.com> wrote:\n\n> @rmlarsen <https://github.com/rmlarsen>\n> third_party/eigen3/unsupported/Eigen/CXX11/Tensor\n> <https://github.com/tensorflow/tensorflow/blob/master/third_party/eigen3/unsupported/Eigen/CXX11/Tensor>\n> is not the same as src/default/unsupported/Eigen/CXX11/Tensor\n> <https://bitbucket.org/eigen/eigen/src/default/unsupported/Eigen/CXX11/Tensor>\n> .\n>\n> There are also commits like this\n> <https://github.com/tensorflow/tensorflow/commit/d0d975f8c3330b5402263b2356b038bc8af919a2#diff-dd67b23046f83602c7172fd9457a44c8>\n> affecting third_party/eigen3/unsupported/Eigen/CXX11/Tensor\n> <https://github.com/tensorflow/tensorflow/blob/master/third_party/eigen3/unsupported/Eigen/CXX11/Tensor>\n> on TensorFlow repo. @jlebar <https://github.com/jlebar> So I do not think\n> it is managed as a separate repository.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27859#issuecomment-483625573>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAJMh2Qp1pu_cvAGvljLIh_mEgIcAPG-ks5vhbjdgaJpZM4cvkCX>\n> .\n>\n", "@rmlarsen @jlebar  Will this PR fix the issue?", "Already fixed in 0621242."]}, {"number": 27858, "title": "tf.summary.tensor_summary no longer appears in the REPL, see it in the code? Docs need update? ", "body": "Not sure what the intention is as I see a lot of v1/v2 name changing.\r\n\r\ntf.summary.scalar is there but not tf.summary.tensor_summary\r\n\r\nIn [80]: tf.__version__\r\nOut[80]: '2.0.0-alpha0'", "comments": ["@cottrell Can you please provide the doc link which needs to be updated.  Thanks!", "I am not finding a \"deprecated\" notices here for example: \r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/summary/tensor_summary", "@cottrell Thanks for providing the details. ", "@cottrell `tf.summary.tensor_summary` is part of `TF1.x` version only and it will stay with TF1.x. I think there is no need for deprecation warning. For example, [`tf.session`](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/Session) is part of `TF1.x` only and hence there is no deprecation warning. \r\n\r\nPlease let us know what you think. Please close this issue if it was resolved. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 27857, "title": "tf.estimator.train_and_evaluate not run evaluate when distribute strategy is CollectiveAllReduceStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentOS Linux release 7.3.1611\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.13.1\r\n- Python version:\r\n2.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.13.1-0-g6612da8951', '1.13.1')\r\n\r\n**Describe the current behavior**\r\nWhen I try to run estimator in distribute with CollectiveAllReduceStrategy strategy, the train_and_evaluate api do not run evaluation after model save checkpoint.\r\n\r\n**Describe the expected behavior**\r\ntrain_and_evaluate should run evaluation after model save checkpoint\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nimport random\r\nimport os\r\nimport json\r\n\r\nclass Generator:\r\n    def __init__(self, mode, batch_size=100):\r\n        self._i = 0\r\n        self._mode = mode\r\n        self._batch_size = batch_size\r\n\r\n    def _get_random(self):\r\n        return random.uniform(0, 100)\r\n\r\n    def next_batch(self):\r\n        self._i += 1\r\n        if self._mode != tf.estimator.ModeKeys.TRAIN and self._i > 200:\r\n            raise StopIteration\r\n        features = {'a': [], 'b': []}\r\n        labels = []\r\n        for _ in xrange(self._batch_size):\r\n            label = 0.0\r\n            for key in features:\r\n                r = self._get_random()\r\n                features[key].append(r)\r\n                label += r\r\n            labels.append(label)\r\n        return features, labels\r\n\r\n    def output_types(self):\r\n        return ({'a': tf.float32, 'b': tf.float32}, tf.float32)\r\n\r\n    def output_shapes(self):\r\n        return ({'a': [None], 'b': [None]}, [None])\r\n\r\ndef _dataset(mode):\r\n    generator = Generator(mode)\r\n\r\n    def generate_data():\r\n        while True:\r\n            yield generator.next_batch()\r\n\r\n    return tf.data.Dataset.from_generator(\r\n            generator=generate_data,\r\n            output_types=generator.output_types(),\r\n            output_shapes=generator.output_shapes(),\r\n            args=[])\r\n\r\ndef _my_model_fn(features, labels, mode, params):\r\n    learning_rate = params['learning_rate']\r\n    keep_prob = params['keep_prob']\r\n    feature_columns = [tf.feature_column.numeric_column('a'),\r\n            tf.feature_column.numeric_column('b')]\r\n    dense_tensor = tf.feature_column.input_layer(features, feature_columns)\r\n    dense_tensor = tf.nn.dropout(dense_tensor, keep_prob=keep_prob)\r\n    for units in [64, 32]:\r\n        dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)\r\n    predictions = tf.layers.dense(dense_tensor, 1)\r\n    predictions = tf.squeeze(predictions, [1])\r\n    loss = tf.losses.absolute_difference(labels=labels, predictions=predictions)\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        accuracy_op = tf.metrics.accuracy(\r\n                labels=labels,\r\n                predictions=predictions,\r\n                name='accuracy_op')\r\n        eval_metric_ops = {'accuracy': accuracy_op}\r\n        spec = tf.estimator.EstimatorSpec(tf.estimator.ModeKeys.EVAL,\r\n                loss=loss,\r\n                eval_metric_ops=eval_metric_ops)\r\n    else:\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n        spec = tf.estimator.EstimatorSpec(tf.estimator.ModeKeys.TRAIN,\r\n                loss=loss,\r\n                train_op=train_op)\r\n    return spec\r\n\r\ndef _cluster():\r\n    return {'worker': ['localhost:2222', 'localhost:2223', 'localhost:2224']}\r\n\r\ndef _set_tf_config(index):\r\n    tf_config = {\r\n            'cluster': _cluster(),\r\n            'task': {'type': 'worker', 'index': index}}\r\n    os.environ['TF_CONFIG'] = json.dumps(tf_config)\r\n\r\ndef main(argv):\r\n    distribution = tf.contrib.distribute.CollectiveAllReduceStrategy()\r\n    config = tf.estimator.RunConfig(\r\n            save_checkpoints_steps=2000,\r\n            keep_checkpoint_max=1,\r\n            train_distribute=distribution,\r\n            eval_distribute=distribution)\r\n    model_dir = './model'\r\n    learning_rate = 1e-6\r\n    keep_prob = 0.75\r\n    estimator = tf.estimator.Estimator(\r\n            model_fn=_my_model_fn,\r\n            model_dir=model_dir,\r\n            config=config,\r\n            params={\r\n                'learning_rate': learning_rate,\r\n                'keep_prob': keep_prob\r\n            })\r\n    train_spec = tf.estimator.TrainSpec(\r\n            input_fn=lambda : _dataset(tf.estimator.ModeKeys.TRAIN),\r\n            max_steps=4000)\r\n    eval_spec = tf.estimator.EvalSpec(\r\n            input_fn=lambda : _dataset(tf.estimator.ModeKeys.EVAL))\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\nif __name__ == '__main__':\r\n    FLAGS = tf.app.flags.FLAGS\r\n    tf.app.flags.DEFINE_integer(\r\n        'index', 0, 'input task index')\r\n    _set_tf_config(FLAGS.index)\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.app.run()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I am also seeing this same problem with tf.contrib.distribute.MirroredStrategy(). Nearly the same exact setup as above, but reading from tfrecords instead of a generator. ", "Ping @yuefengz", "Btw could you try tf-nightly and reproduce the error?", "@byronyi, thanks for the reply. I tried installing tf-nightly into a clean virtual env (as well as the conda env that I normally use). Unfortunately, I'm unable to train an estimator with a distribution strategy (no distribution seems to work normally). A few details below (want to avoid adding too much off an unrelated issue).  Sorry this doesn't reproduce the error exactly or give you code that runs out of the box but I've included enough to hopefully give you an idea of what I'm doing\r\n\r\n1) MirroredStrategy fails, when being applied in the exact same way as before. \r\n```\r\nValueError: In-graph multi-worker training with `MirroredStrategy` is not supported.\r\n```\r\n\r\n2) Trying CollectiveAllReduce, it fails asking me to pass a tf.losses.reduction to the loss function\r\n\r\n```\r\nValueError: Please use `tf.keras.losses.Reduction.SUM` or `tf.keras.losses.Reduction.NONE` for loss reduction when losses are used with `tf.distribute.Strategy` outside of the built-in training loops.\r\n```\r\nTrying that out, I get this:\r\n```\r\nAttributeError: module 'tensorflow._api.v1.losses' has no attribute 'reduction'\r\n```\r\n\r\n3)Trying MultiWorkerAllReduce, I get:\r\n```\r\nAttributeError: 'MultiWorkerAllReduce' object has no attribute 'configure'\r\n```\r\n\r\n```\r\n\r\n\r\ndef autoencoder_dataset(filename, batch_size, interleave_cycle_length, input_context):\r\n\r\n    feature_description = {\r\n        'vendor_variant_id': tf.FixedLenFeature([], tf.int64, default_value=0),\r\n        'image': tf.FixedLenFeature([], tf.string, default_value=''),\r\n        'taxonomy_id': tf.FixedLenFeature([], tf.int64, default_value=-1)\r\n    }\r\n\r\n    def _parse_function(example_proto):\r\n\r\n        parsed = tf.parse_single_example(example_proto, feature_description)\r\n        vendor_variant_id = parsed['vendor_variant_id']\r\n        image = parsed['image']\r\n        taxonomy_id = parsed['taxonomy_id']\r\n\r\n        return vendor_variant_id, image, taxonomy_id\r\n\r\n    def scale_image(image):\r\n\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n\r\n        return image\r\n\r\n    def preprocess_image(vendor_variant_id, image, taxonomy_id):\r\n\r\n        decoded_image = tf.image.decode_image(image, channels=3)\r\n        decoded_image = tf.reshape(decoded_image, (224,224,3))\r\n\r\n        augmented_image = tf.image.random_flip_left_right(decoded_image, seed=919)\r\n        augmented_image = tf.image.random_flip_up_down(augmented_image, seed=919)\r\n\r\n        np.random.seed(919)\r\n        pixel_shifts = np.random.randint(-30, 30, size=2).tolist()\r\n        augmented_image = tf.contrib.image.translate(augmented_image, pixel_shifts)\r\n\r\n        augmented_scaled_image = scale_image(decoded_image)\r\n        scaled_image = scale_image(augmented_image)\r\n\r\n        return augmented_scaled_image, scaled_image\r\n\r\n    def autoencoder_dataset(tfrecord_path, batch_size, interleave_cycle_length, input_context):\r\n\r\n        files = tf.data.Dataset.list_files(tfrecord_path)\r\n\r\n        if input_context:\r\n            files = files.apply(\r\n                tf.data.experimental.filter_for_shard(input_context.num_input_pipelines,\r\n                                                      input_context.input_pipeline_id)\r\n                )\r\n\r\n        if interleave_cycle_length>1:\r\n            raw_dataset = files.apply(tf.contrib.data.parallel_interleave(\r\n                tf.data.TFRecordDataset,\r\n                cycle_length=interleave_cycle_length,\r\n                sloppy=True))\r\n        else:\r\n            raw_dataset = tf.data.TFRecordDataset(files)\r\n\r\n        shuffled_dataset = raw_dataset.shuffle(1000)\r\n        parsed_dataset = shuffled_dataset.map(_parse_function,\r\n                                              num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n\r\n        mapped_dataset = parsed_dataset.apply(\r\n            tf.data.experimental.map_and_batch(map_func=preprocess_image,\r\n                                          batch_size=batch_size,\r\n                                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n            )\r\n\r\n        return mapped_dataset.repeat()\r\n\r\n    return autoencoder_dataset(filename, batch_size, interleave_cycle_length, input_context)\r\n\r\n\r\ndef input_fn(params, mode, input_context=None):\r\n\r\n    batch_size = params['batch_size']\r\n    interleave_cycle_length = params['interleave_cycle_length']\r\n\r\n    dataset_path = params['train_dataset_path']\r\n    n_images = params['n_train_images']\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n\r\n        dataset = autoencoder_dataset(dataset_path, batch_size, 1, input_context)\r\n\r\n    elif mode == tf.estimator.ModeKeys.TRAIN:\r\n\r\n        dataset = autoencoder_dataset(dataset_path, batch_size, interleave_cycle_length, input_context)\r\n\r\n    return dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n\r\ndef activation_factory(params):\r\n\r\n    if params['activation'] == 'leaky':\r\n\r\n        return tf.keras.layers.LeakyReLU(0.01)\r\n\r\n    else:\r\n\r\n        return tf.keras.layers.LeakyReLU(0.0)\r\n\r\n\r\ndef model_fn(features, labels, params, mode):\r\n\r\n    n_hidden_1 = 16  # 1st hidden layer\r\n    n_hidden_2 = 12  # 2nd hidden layer\r\n    n_hidden_3 = 10  # 3rd hidden layer\r\n    n_hidden_4 = 8\r\n\r\n    convkernel = (3, 3)\r\n    poolkernel = (2, 2)\r\n\r\n    activation = activation_factory(params)\r\n\r\n    model = tf.keras.Sequential([\r\n\r\n        ## Encoder\r\n        tf.keras.layers.Conv2D(n_hidden_1, convkernel, activation='relu', input_shape=(224, 224, 3)),\r\n        tf.keras.layers.MaxPooling2D(poolkernel, padding='same'),\r\n        tf.keras.layers.Conv2D(n_hidden_2, convkernel, padding='same'),\r\n        activation,\r\n        tf.keras.layers.Conv2D(n_hidden_3, convkernel, padding='same'),\r\n        activation,\r\n        tf.keras.layers.MaxPooling2D(poolkernel, padding='same'),\r\n        tf.keras.layers.Conv2D(n_hidden_4, convkernel, activation='relu', padding='same'),\r\n        tf.keras.layers.MaxPooling2D(poolkernel, padding='same', name='bottleneck'),\r\n\r\n        ## Decoder\r\n        tf.keras.layers.Conv2D(n_hidden_4, convkernel, activation='relu', padding='same'),\r\n        tf.keras.layers.UpSampling2D(poolkernel),\r\n        tf.keras.layers.Conv2D(n_hidden_3, convkernel, padding='same'),\r\n        activation,\r\n        tf.keras.layers.Conv2D(n_hidden_2, convkernel, padding='same'),\r\n        activation,\r\n        tf.keras.layers.UpSampling2D(poolkernel),\r\n        tf.keras.layers.Conv2D(32, (1,1)),\r\n        activation,\r\n        tf.keras.layers.UpSampling2D(poolkernel),\r\n        tf.keras.layers.Conv2D(3, convkernel, activation='sigmoid', padding='same')\r\n\r\n        ])\r\n\r\n    global_step = tf.train.get_global_step()\r\n\r\n    logits = model(features, training=False)\r\n    predictions = {'logits': logits}\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n\r\n        return tf.estimator.EstimatorSpec(labels=labels,\r\n                                          predictions=predictions)\r\n\r\n    momentum = params['learning_rates_momentum']\r\n    boundaries = [e*(params['n_train_images']//params['batch_size']) for e in params['learning_rates_epoch_schedule']]\r\n\r\n    n_gpus = params['cluster_gpus']\r\n\r\n    if 'learning_rates_warmup' in params.keys() and params['learning_rates_warmup']:\r\n        scaled_rates = [r * n_gpus for r in params['learning_rates']]\r\n        scaled_rates = [params['learning_rates'][0]] + scaled_rates\r\n        learning_rate = manual_stepping(global_step, boundaries, scaled_rates, warmup=True)\r\n    else:\r\n        learning_rate = manual_stepping(global_step, boundaries, params['learning_rates'])\r\n\r\n    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\r\n                                           momentum=momentum)\r\n\r\n    loss = tf.keras.losses.MeanSquaredError()(labels, logits)\r\n\r\n    tf.summary.scalar('learning_rate', optimizer._learning_rate)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n\r\n        train_op = tf.contrib.training.create_train_op(loss,\r\n                                                       optimizer,\r\n                                                       summarize_gradients=True)\r\n\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            loss=loss,\r\n            train_op=train_op\r\n        )\r\n\r\n    else:\r\n\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss)\r\n\r\n\r\ndef set_tf_config(config, is_chief, worker_index=None):\r\n\r\n    tf_config = {}\r\n    tf_config['cluster'] = {}\r\n    tf_config['cluster']['chief'] = ['{}:{}'.format(config['chief']['ip'], str(config['chief']['port']))]\r\n    tf_config['cluster']['worker'] = []\r\n\r\n    for i,w in enumerate(config['workers']):\r\n\r\n        if is_chief:\r\n\r\n            is_chief = True\r\n\r\n            if not worker_index:\r\n\r\n                worker_index = 0\r\n\r\n        else:\r\n\r\n            if worker_index is None:\r\n\r\n                worker_index = i\r\n\r\n        tf_config['cluster']['worker'].append('{}:{}'.format(w['ip'], str(w['port'])))\r\n\r\n    if worker_index is None:\r\n        raise Exception('Instance public DNS name not found')\r\n\r\n    if is_chief:\r\n        tf_config['task'] = {'type': 'chief', 'index': worker_index}\r\n    else:\r\n        tf_config['task'] = {'type': 'worker', 'index': worker_index}\r\n\r\n    os.environ['TF_CONFIG'] = json.dumps(tf_config)\r\n\r\nsave_summary_steps = 1\r\nsave_checkpoints_steps = 1\r\n\r\nis_chief = False\r\nworker_index = None\r\ncluster_config = {\"workers\": [{\"ip\": \"localhost\", \"port\": 1111},\r\n                              {\"ip\": \"localhost\", \"port\": 1112}]}\r\n# Will infer worker index from cluster config if not passed as an arg\r\nset_tf_config(cluster_config, is_chief, worker_index)\r\n\r\n\r\nstrategy = tf.contrib.distribute.MirroredStrategy()\r\n\r\nrun_config = tf.estimator.RunConfig(\r\n    save_summary_steps=save_summary_steps,\r\n    save_checkpoints_steps=save_checkpoints_steps,\r\n    train_distribute=strategy,\r\n    eval_distribute=None\r\n)\r\n\r\ntrain_config = {\r\n    # Various parameters passed to my model_fn\r\n}\r\n\r\nclassifier = tf.estimator.Estimator(\r\n    model_fn=model_fn,\r\n    model_dir='...',\r\n    params=train_config,\r\n    config=run_config\r\n)\r\n\r\ntf.estimator.train_and_evaluate(\r\n    classifier,\r\n    train_spec=tf.estimator.TrainSpec(input_fn=input_fn,\r\n                                      max_steps=3),\r\n    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn,\r\n                                    steps=1,\r\n                                    start_delay_secs=0,\r\n                                    throttle_secs=0)\r\n)\r\n```\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1079-aws x86_64v) AND OSX Sierra 10.13.6\r\nTensorFlow installed from (source or binary): pip tf-nightly\r\nTensorFlow version (use command below): 1.13.1\r\nPython version: 3.6.5\r\nCUDA/cuDNN version: CUDA 10\r\nGPU model and memory: NVIDIA V100 Tensor Core GPU (AWS p3.2x large)", "Found the solution (to at least my problem and hopefully the one above). If you don't supply a separate machine/task an evaluator you need to specify one of your machines/tasks as \"master\" NOT \"chief\" or \"worker\". In _TrainingExecutor (which is called by the distribute coordinator) there are separate methods for run_chief and run_master, run_chief does not call estimator.evaluate while run_master does!! \r\n\r\n @wudixiaotie @byronyi", "@wudixiaotie Is this still an issue?", "Please note \"master\" is not officially supported by distribution strategy or Estimator. If you want to run evaluation, you need to have an \"evaluator\" task with `train_and_evaluate` API.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27857\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27857\">No</a>\n", "@devinkmoore Are you seeing any loss scaling related issue or is that resolved?", "> Please note \"master\" is not officially supported by distribution strategy or Estimator. If you want to run evaluation, you need to have an \"evaluator\" task with `train_and_evaluate` API.\r\n\r\n@yuefengz Any TF 2.x code examples to configure the \"evaluator\" task? Thanks.", "> Please note \"master\" is not officially supported by distribution strategy or Estimator. If you want to run evaluation, you need to have an \"evaluator\" task with `train_and_evaluate` API.\r\n\r\nDo you me in the tfconfig specify \"evaluator\" in addition to \"chief\", \"worker\", \"ps\"?"]}, {"number": 27856, "title": "get error message when use tf.summary.scalar() with TF 2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview: 2.0.0-dev20190413\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Yes, 11GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI get the following error message when I use tensorflow in GPU mode. When I use tensorflow in CPU mode, such error message doesn't occur.\r\n\r\n>AttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'summary_scope'\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect no error when I call tf.summary.scalar() in TF2.0 in both GPU and CPU mode.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```Python\r\n#!/usr/bin/python3\r\n\r\nimport tensorflow as tf;\r\n\r\ndef main():\r\n\r\n    log = tf.summary.create_file_writer('checkpoints');\r\n    with log.as_default():\r\n        tf.summary.scalar('loss',1., step = 0);\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    main();\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Just in case it might help, I got past this issue today by running\r\n``pip3 install --upgrade --force-reinstall tb-nightly``", "thx.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27856\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27856\">No</a>\n"]}, {"number": 27855, "title": "How to use libtensorflow-lite.a with C++ on Raspberry Pi 3?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Rasbian\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 3b+\r\n- TensorFlow installed from (source or binary):No\r\n- TensorFlow version:No\r\n- Python version:3.5\r\n- Installed using virtualenv? pip? conda?:No\r\n- Bazel version (if compiling from source):No\r\n- GCC/Compiler version (if compiling from source):No\r\n- CUDA/cuDNN version:No\r\n- GPU model and memory:No\r\n\r\n\r\nI have trained own model and converted to lite model. Then I have generated libtensorflow-lite.a with cross-compiling on Ubuntu 16.04. I am trying to find the header file of all the objects in the libtensorflow-lite.a. Is this correct ? How to use it on Raspberry Pi 3 ? \r\n\r\nIn addition, the path to the libtensorflow-lite.a generated when using cross-compilation is /tensorflow/lite/tools/make/gen/rpi_armv7l/lib is not the same as the official teaching path.\r\nDoes this compile fail?\r\n\r\n", "comments": ["you have to make a *.a file by yourself. follow this guide:https://tensorflow.google.cn/lite/guide/build_rpi", "> \u4f60\u5fc5\u9808\u81ea\u5df1\u88fd\u4f5c\u4e00\u500b* .a\u6587\u4ef6\u3002\u8acb\u6309\u7167\u4ee5\u4e0b\u6307\u5357\u64cd\u4f5c\uff1a[https](https://tensorflow.google.cn/lite/guide/build_rpi)\uff1a[//tensorflow.google.cn/lite/guide/build_rpi](https://tensorflow.google.cn/lite/guide/build_rpi)\r\n\r\n@jinyige I have made the *.a file in accordance with the official instructions. If I want to use this *.a file, should I need a .h file?\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 27854, "title": "Problem with Running GANEstimator: Type Error in eval_utils_impl.py", "body": "```\r\n\r\n---> 55   if grid_shape[0] * grid_shape[1] != int(input_tensor.shape[0]):\r\n     56     raise ValueError(\"Grid shape %s incompatible with minibatch size %i.\" %\r\n     57                      (grid_shape, int(input_tensor.shape[0])))\r\n\r\nTypeError: __int__ returned non-int (type NoneType)\r\n```\r\nSource: tensorflow/contrib/gan/python/eval/python/eval_utils_impl.py\r\n\r\nI am rebuilding GAN Estimator with MNIST Dataset:\r\n\r\ninput function is defined as:\r\n\r\n```python\r\ndef preprocess(img, label):\r\n    img = tf.cast(img, dtype=tf.float32)\r\n    img = tf.divide(img, 255.0)\r\n    if len(img.shape) == 2:\r\n        img = tf.expand_dims(img, 2)\r\n    prior = tf.random_normal([64])\r\n    return prior, img\r\n\r\n\r\ndef get_input_fn(is_train):\r\n    if is_train:\r\n        dataset = train\r\n    else:\r\n        dataset = test\r\n    tf_dataset = tf.data.Dataset.from_tensor_slices(dataset)\r\n\r\n    tf_dataset = tf_dataset.map(preprocess)\r\n        \r\n    def input_fn():\r\n        data = tf_dataset.batch(BATCH_SIZE)\r\n        dA_iterator = data.make_one_shot_iterator()\r\n        prior, d_data = dA_iterator.get_next()\r\n        \r\n        return (prior, d_data)\r\n    return input_fn\r\n\r\ninput_fn = get_input_fn(True)\r\n```\r\n\r\nGAN generators and discriminators:\r\n```python\r\n\r\ndef generator_fn(prior):\r\n    output = tf.keras.layers.Dense(1024)(prior)\r\n    output = tf.keras.layers.Dense(7*7*128)(output)\r\n    output = tf.reshape(output, (-1, 7, 7, 128))\r\n    output = tf.keras.layers.Conv2DTranspose(64, 4, 2, padding=\"same\")(output)\r\n    output = tf.keras.layers.Conv2DTranspose(32, 4, 2, padding=\"same\")(output)\r\n    output = tf.keras.layers.Conv2DTranspose(1, 4, padding=\"same\", activation=tf.nn.tanh)(output)\r\n    return output\r\n\r\ndef discriminator_fn(img, unused_conditioning):\r\n    output = tf.keras.layers.Conv2D(64, 4, 2)(img)\r\n    output = tf.keras.layers.Conv2D(128, 4, 2)(output)\r\n    output = tf.layers.flatten(output)\r\n    output = tf.layers.Dense(1024)(output)\r\n    output = tf.contrib.layers.layer_norm(output)\r\n    output = tf.layers.Dense(1)(output)\r\n    return output\r\n```\r\n\r\nEstimator is created as:\r\n\r\n```python\r\ngan_estimator = tfgan.estimator.GANEstimator(generator_fn=generator_fn, discriminator_fn=discriminator_fn,\\\r\n                             generator_loss_fn=tfgan.losses.wasserstein_generator_loss, \\\r\n                             discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\\\r\n                             generator_optimizer=tf.train.AdamOptimizer(0.001, 0.5),\\\r\n                             discriminator_optimizer=tf.train.AdamOptimizer(0.0001, 0.5),\\\r\n                             add_summaries=tfgan.estimator.SummaryType.IMAGES)\r\n\r\ngan_estimator.train(input_fn, max_steps=1000)\r\n```\r\n\r\nThe environment I am running on is:\r\n\r\nUbuntu 16.01\r\nPython 3.6.6\r\nTensorflow 1.13", "comments": ["@LutetiumX Could you provide full trace of the error. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27854\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27854\">No</a>\n"]}, {"number": 27853, "title": "Tensorflow variables not casting to ref type [BUG][TF 2.0]", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0.0-alpha\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n**Describe the current behavior**\r\nWhen passed to different Function, tf.Variable dtype doesn't cast to _ref type.\r\nFor example:\r\n```python3\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\na = tf.Variable(1.0, dtype=tf.float32)\r\ndef f(x):\r\n  print(x.dtype)\r\n```\r\nOutput: `<dtype: 'float32'>`\r\n**Describe the expected behavior** (according to tf r1.13)\r\n```python3\r\nimport tensorflow as tf\r\na = tf.Variable(1.0, dtype=tf.float32)\r\ndef f(x):\r\n  print(x.dtype)\r\n```\r\nOutput: `<dtype: 'float32_ref'>`\r\n\r\n**Code to reproduce the issue**\r\nGiven Above\r\n\r\n**Other info / logs**\r\nFor this reason, a lot Function Calls like `tensorflow.python.ops.gen_state_ops.assign_sub` are failing. This Leads to Failing of Keras Backend Calls, like `tensorflow.python.keras.backend.moving_average_update`, even in Graph Mode.\r\nTraceback created after Appending `tf.compat.v1.disable_eager_execution()` to the issue https://github.com/tensorflow/tensorflow/issues/27739, shows that assign_sub is failing as Tensor Object have no assign_sub. Upon Further Investigation, this error was found here:\r\nhttps://github.com/tensorflow/tensorflow/blob/0c464c70cef2369b6ef5c5e17dbd2cda2a6107fb/tensorflow/python/ops/state_ops.py#L159-L162", "comments": ["@captain-pool Hi, I am working on a project with Python 2, Keras 1.2.2 and Tensorflow 1.6.0 and I want to initialize a Dense layer with my own weight matrix. I am using the examples online:\r\n```\r\n    from keras import backend as K\r\n     \r\n    def my_init(shape, dtype=None):\r\n        return K.random_normal(shape, dtype=dtype)\r\n     \r\n    model.add(Dense(64, init=my_init))\r\n```\r\n\r\n and they are failing when the model starts training, at:\r\n```\r\n\r\n  if ref.dtype._is_ref_dtype:\r\n    return gen_state_ops.assign(\r\n        ref, value, use_locking=use_locking, name=name,\r\n        validate_shape=validate_shape)\r\n  return ref.assign(value)\r\n```\r\n`'Tensor' object has no attribute 'assign'`\r\nEven if I do: `dtype=tf.float32` in the definition of my_init(). If I delete `init=my_init` then during training the code does not reach `return ref.assign(value)` and works fine. Any help please?"]}, {"number": 27852, "title": "Problem building tensorflow on Ubuntu system with Bazel gpu flavor", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: Python 3.6.7\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): 0.23.2\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026\r\n- CUDA/cuDNN version: 9.2\r\n- GPU model and memory: GeForce GTX 1060 Max-Q GPU with 6GB of VRAM\r\n\r\n\r\n\r\nFaced tensorflow compilation problem with below details\r\n\r\n> user@user:/opt/work/work_tf/tensbazel build -j 8 --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n> Extracting Bazel installation...\r\n> Starting local Bazel server and connecting to it...\r\n> WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\n> DEBUG: /home/user/.cache/bazel/_bazel_user/f793b012987ffd570584b4a3e97b9108/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: \r\n> Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\n> ERROR: /home/user/.cache/bazel/_bazel_user/f793b012987ffd570584b4a3e97b9108/external/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/BUILD:27:1: no such target '@bazel_tools//tools/cpp:cc_flags': target 'cc_flags' not declared in package 'tools/cpp' defined by /home/user/.cache/bazel/_bazel_user/f793b012987ffd570584b4a3e97b9108/external/bazel_tools/tools/cpp/BUILD and referenced by '@com_github_googlecloudplatform_google_cloud_cpp//google/cloud:generate_build_info'\r\n> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\n> INFO: Elapsed time: 43.643s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (327 packages loaded, 8419 targets configured)\r\n>     Fetching @grpc; fetching 18s\r\n>     Fetching @com_google_absl; fetching 18s\r\n>     Fetching @aws; fetching 17s\r\n>     Fetching @llvm; fetching 15s\r\n>     Fetching @nasm; fetching 9s\r\n\r\n\r\n**Any other info / logs**\r\nThis problem is reported after merge \"Update Google Cloud C++ Client to the v0.8.1 release.\" (commit id : 0602e366015a85899e3f2cc645667dd308f4fbb5) changes.", "comments": ["To get around this problem, I had to upgrade bazel to version 0.24.1", "Thanks @wdirons with bazel 0.24.1 version tensorflow compilation problem is not exists. "]}, {"number": 27851, "title": "Moved pad reference and optimized ops into its own.", "body": "To Remove the dependency moving pad to it own file.", "comments": ["@aselle , I have committed padding support for tflite macro as well with the 2nd commit, Would love to see your comments on the same.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 could you please resolve the conflicts?  Thanks!", "@gbaned , thanks for pointing this out, i have resolved the merge conflicts.\r\n\r\nRegards\r\nAmit", "@aselle , i have re-based the code again , kindly have a look and provide your feedback.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "> @amitsrivastava78 Could you please resolve the conflicts? Thanks!\r\n\r\n@gbaned , conflict resolved.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27850, "title": "broadcast_to op: \"invalid shape to broadcast\" on 6D tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: x\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: CUDA 10, cudnn 7.5\r\n- GPU model and memory: Pascal Xp\r\n\r\n**Describe the current behavior**\r\n\r\n```python\r\na = tf.zeros([2, 100, 32, 32, 3])\r\nb = tf.broadcast_to( tf.expand_dims(a, 1), [2, 10, 100, 32, 32, 3] )\r\n\r\nsess = tf.InteractiveSession()     # or whatever session\r\nsess.run(b).shape\r\n```\r\n\r\ngives the error:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): invalid shape to broadcast from [2,1,100,32,32,3] to [2,10,100,32,32,3]\r\n         [[node BroadcastTo_1 (defined at <ipython-input-10-49d2a4dce7c3>:1) ]]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nShould run normally, giving the `[2, 10, 100, 32, 32, 3]` tensor.\r\n\r\n\r\n\r\n**Other Information**\r\n\r\nIt works for 5D tensor.\r\n\r\n```python\r\n>>> a = tf.zeros([2,100,32,32])\r\n>>> b = tf.broadcast_to( tf.expand_dims(a, 1), [2,10,100,32,32] )\r\n\r\n>>> sess.run(b).shape\r\n(2, 10, 100, 32, 32)\r\n```", "comments": ["Using `tf.tile` as a replacement of `tf.broadcast_to` works fine.\r\n\r\n```\r\ntf.tile(tf.expand_dims(a, 1), [1, 10, 1, 1, 1, 1])\r\n```", "With python 3.6.7 version am able to run the sample snippet and get actual expected o/p.\r\n\r\n>>> sess.run(b).shape\r\n(2, 10, 100, 32, 32, 3)\r\n", "@Dayananda-V Thanks for checking this on yourside. Can you please specify other environment information as well (TF version, it is built from source or from pip/conda, etc.)?", "This should be fixed as of 99121926a8d774f1e205c00108396bc955346e3d"]}, {"number": 27849, "title": "Update framework.py", "body": "Fix #27847", "comments": ["@ppwwyyxx can you please address Ubuntu Sanity build failures", "The test was now fixed", "It seems that the test failure now is unrelated to this PR.", "The copybara sync infrastructure seems to have failed to sync the latest change. I'll submit this PR through  a separate commit, with a unit test.", "FYI @ppwwyyxx The fixing change (based on your PR, but with a unit test) is just merged. Thank you for filing the issue and coming up with the fix.", "Here is the link: https://github.com/tensorflow/tensorflow/commit/e2d269edb9217411fc4119338df949e1a741432b"]}, {"number": 27848, "title": "Error filename logging with tf.logging.warn", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.0\r\n\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nstdout prints \r\n```\r\ntf_logging.py:warn - test warn\r\n<stdin>:<module> - test warning\r\n```\r\n**Describe the expected behavior**\r\nstdout prints\r\n```\r\n<stdin>:<module> - test warn\r\n<stdin>:<module> - test warning\r\n```\r\n**Code to reproduce the issue**\r\n``` python\r\nimport logging\r\nimport sys\r\nh = logging.StreamHandler(sys.stdout)\r\nformatter = logging.Formatter(\"%(filename)s:%(funcName)s - %(message)s\")\r\nh.setFormatter(formatter)\r\nlog = logging.getLogger()\r\nlog.addHandler(h)\r\nimport tensorflow as tf\r\ntf.logging.warn(\"test warn\")\r\ntf.logging.warning(\"test warning\")\r\n```\r\n**Other info / logs**\r\nsince the _logging.warn_ of python3 is the wrapper of _logging.warning_, code is [here](https://github.com/python/cpython/blob/master/Lib/logging/__init__.py#L1459), \r\nthis adds one frame making the [get_caller(4)](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/python/platform/tf_logging.py#L49) of tf.logging miss the _warn_ method.\r\n", "comments": ["Added PR #27871 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27848\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27848\">No</a>\n"]}, {"number": 27847, "title": "BUG: tfdbg session cannot be used with SessionRunHooks", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ArchLinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):b'v1.13.0-rc2-5-g6612da8' 1.13.1\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory:n/a\r\n\r\nThe following code:\r\n```python\r\n#-*- coding: utf-8 -*-\r\n#File:\r\n\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\na = tf.placeholder(tf.float32, [10])\r\nb = a + 1\r\nc = b * 2\r\n\r\nclass Hook(tf.train.SessionRunHook):\r\n    def before_run(self, _):\r\n        return tf.train.SessionRunArgs(fetches=c)\r\n\r\nclass Hook2(tf.train.SessionRunHook):\r\n    def before_run(self, _):\r\n        return tf.train.SessionRunArgs(fetches=b)\r\n\r\nsess = tf.Session()\r\nsess = tf_debug.LocalCLIDebugWrapperSession(sess)\r\n\r\nclass SessionCreator():\r\n    def create_session(self):\r\n        return sess\r\nfinal_sess = tf.train.MonitoredSession(session_creator=SessionCreator(), hooks=[Hook(), Hook2()])\r\n\r\nfinal_sess.run(b, feed_dict={a:np.arange(10)})\r\n```\r\n\r\nThrows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"tfdbg.py\", line 30, in <module>\r\n    final_sess.run(b, feed_dict={a:np.arange(10)})\r\n  File \"/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 676, in run                                                                            \r\n    run_metadata=run_metadata)\r\n  File \"/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1171, in run                                                                           \r\n    run_metadata=run_metadata)\r\n  File \"/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1270, in run                                                                           \r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/lib/python3.7/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run                                                                           \r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1327, in run                                                                           \r\n    run_metadata=run_metadata)\r\n  File \"/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1091, in run                                                                           \r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/debug/wrappers/framework.py\", line 463, in run                                                                              \r\n    empty_fetches = not nest.flatten(fetches)\r\n  File \"/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 2156, in Flatten                                                                       \r\n    return _pywrap_tensorflow_internal.Flatten(nested)\r\nTypeError: '<' not supported between instances of 'Hook' and 'str'\r\n```\r\n\r\nI believe this issue was introduced in https://github.com/tensorflow/tensorflow/commit/1f26c65254268730b7409f517d1ed1b554d01e50 a year ago. `flatten` cannot handle fetches created by hooks.\r\nThe fix will be to obtain `empty_fetches` in a smarter way.", "comments": ["Fixed by https://github.com/tensorflow/tensorflow/commit/e2d269edb9217411fc4119338df949e1a741432b", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27847\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27847\">No</a>\n"]}, {"number": 27845, "title": "Wrong derivatives for complex second order derivatives.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow==1.12.0\r\n- Python version: 3.6.8\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nDerivatives of non-holomorphic functions are incorrect when compared both against AD and finite differences.\r\n\r\n**Describe the expected behavior**\r\n\r\nDerivatives of non-holomorphic functions should becorrect.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n``` python\r\nimport numpy as onp\r\nimport autograd as ag\r\nimport autograd.numpy as anp\r\nimport numpy as onp\r\nimport tensorflow as tf\r\n\r\ninp = anp.array(2.0)\r\n\r\nprint(\"input\", inp)\r\n\r\ndef ag_fn(x):\r\n    real = anp.cos(x+2)\r\n    imag = anp.sin(x-1)\r\n    return anp.abs(real+1j*imag)\r\n\r\nag_hess = ag.hessian(ag_fn)\r\n\r\nprint(\"ag val:\", ag_fn(inp))\r\nprint(\"ag hess:\", ag_hess(inp))\r\n\r\ndef tf_fn(x):\r\n    real = tf.cos(x+2)\r\n    imag = tf.sin(x-1)\r\n    return tf.abs(tf.complex(real, imag))\r\n\r\n# tf_inp = tf.convert_to_tensor(inp)\r\ntf_inp = tf.placeholder(shape=tuple(), dtype=onp.float64)\r\n\r\nout_op = tf_fn(tf_inp)\r\n\r\ntf_grad = tf.gradients(out_op, tf_inp)[0]\r\ntf_hess = tf.hessians(out_op, tf_inp)[0]\r\n\r\nsess = tf.Session()\r\ndelta = 1e-7\r\n\r\n_, d0, tf_ad = sess.run([out_op, tf_grad, tf_hess], feed_dict={tf_inp: inp})\r\n_, d1, _ = sess.run([out_op, tf_grad, tf_hess], feed_dict={tf_inp: inp+delta})\r\n\r\nprint(\"tf_numerical derivative:\", (d1-d0)/delta)\r\nprint(\"tf_autodiff derivative:\", tf_ad)\r\n```\r\n\r\n```\r\ninput 2.0\r\nag val: 1.0655155566059393\r\nag hess: -0.25533014019223726\r\n2019-04-14 22:55:43.481283: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\ntf_numerical derivative: -0.25533013481293665\r\ntf_autodiff derivative: -1.0655155566059389\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAdditional information: https://github.com/google/jax/issues/603", "comments": ["Ran this on `tensorflow==2.0.0-dev20190327` and I get the same incorrect output.", "Thanks for the minimal code snippet to reproduce the issue. I was able to reproduce the behavior in TF 1.13 and latest nightly build.", "Thanks for filing the issue!\r\n\r\nIf I replace `tf.abs` on your example with a manual implementation (`tf.sqrt(real(x)*real(x) + imag(x)*imag(x))`) the values are identical, so I think this is a problem with the gradient for the `ComplexAbs` op.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27845\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27845\">No</a>\n", "Thanks for fixing this guys!"]}, {"number": 27844, "title": "quantization with multiple gpu training", "body": "when i try to quantization with multiple gpu training,   a error happened\r\n\r\nKeyError: \"The name 'tower_0/gradients/tower_0/MobileNet/conv_ds_13/pointwise_conv/BatchNorm/batchnorm_1/Rsqrt' refers to an Operation not in the graph.\"\r\n\r\nmy multiple gpu code\uff1a\r\n with tf.variable_scope(tf.get_variable_scope()):\r\n            for i in range(2):\r\n                with tf.device('/gpu:%d' % i):\r\n                    with tf.name_scope('tower_%d' % i) as scope1:\r\n                        X = x[int(i*BATCH_SIZE*0.5):int((i+1)*BATCH_SIZE*0.5)]\r\n                        Y = y_[int(i*BATCH_SIZE*0.5):int((i+1)*BATCH_SIZE*0.5)]\r\n\r\n                        with slim.arg_scope(mobilenet.mobilenet_arg_scope(weight_decay=0.00004)):\r\n                            logits, end_points = mobilenet.mobilenetv1(\r\n                                inputs=X,\r\n                                num_classes=136,\r\n                                is_training=is_training,\r\n                                width_multiplier=0.5,\r\n                                scope='MobileNet'\r\n                            )\r\n\r\n                        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\r\n                               tf.contrib.quantize.create_training_graph(quant_delay=get_quant_delay())\r\n\r\n                        loss = tf.losses.huber_loss(labels=Y, predictions=logits, delta=1.0, scope=scope1)\r\n                        tf.get_variable_scope().reuse_variables()\r\n\r\n                        if QUANTIZE:\r\n                            tf.contrib.quantize.create_training_graph(quant_delay=get_quant_delay())\r\n\r\n                        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\r\n                        with tf.control_dependencies(update_ops):\r\n                            grads = train_steps.compute_gradients(loss)\r\n                            tower_grads.append(grads)\r\n\r\n        grads = average_gradients(tower_grads)\r\n        train_op = train_steps.apply_gradients(grads, global_step=global_step)\r\n\r\n", "comments": ["@zzk88862   Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27844\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27844\">No</a>\n"]}, {"number": 27843, "title": "ImportError: cannot import name 'abs'", "body": "os: ubuntu 16.04 LTS\r\nprocessor : Intel\u00ae Xeon(R) CPU E5-1620 v4 @ 3.50GHz \u00d7 8 \r\nGPU : GeForce GTX 1080 Ti/PCIe/SSE2 \r\npython : 3.6 \r\ntensorflow 1.13.1\r\n\r\nhttps://github.com/hellochick/PSPNet-tensorflow\r\n\r\nrunfile('/data/segmentation-master/run_on_sequence.py', wdir='/data/segmentation-master')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-3-914d8c6a3fac>\", line 1, in <module>\r\n    runfile('/data/segmentation-master/run_on_sequence.py', wdir='/data/segmentation-master')\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py\", line 786, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"/data/segmentation-master/run_on_sequence.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 88, in <module>\r\n    from tensorflow.python import keras\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py\", line 24, in <module>\r\n    from tensorflow.python.keras import activations\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.activations import elu\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py\", line 21, in <module>\r\n    from tensorflow.python.keras._impl.keras import activations\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/activations.py\", line 23, in <module>\r\n    from tensorflow.python.keras._impl.keras import backend as K\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 37, in <module>\r\n    from tensorflow.python.layers import base as tf_base_layers\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 25, in <module>\r\n    from tensorflow.python.keras.engine import base_layer\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/__init__.py\", line 23, in <module>\r\n    from tensorflow.python.keras.engine.base_layer import InputSpec\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 35, in <module>\r\n    from tensorflow.python.keras import backend\r\n\r\n  File \"/home/dl-box/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/backend/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.backend import abs\r\n\r\nImportError: cannot import name 'abs'\r\n\r\n\r\n\r\n", "comments": ["@ashisham Could you check the following resources [1](https://stackoverflow.com/questions/51299194/importerror-cannot-import-name-abs) and [2](https://github.com/tensorflow/tensorflow/issues/20778) for any possible solution. Please check GitHub and Stackoverflow as there are many post describe solution to this issue. Please let me know how it progresses. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 27842, "title": "2.0 alpha build from source version failed when call model.fit()", "body": "**System information**\r\n- OS: windows10\r\n- TensorFlow installed from source\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Python version: 3.6.6\r\n- Installed using virtualenv? pip? conda?: venv\r\n- Bazel version (if compiling from source): Bazel 22.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX 1050 4G\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI had compiled TensorFlow 2.0.0-alpha0 GPU version on windows 10. It said build is completely sucessful. I installed the whl file to my python virtual environment. The print(tf.__version__) can print correct version info. But when I run this simple program it failed unexpectly.\r\nThe simple program as below:\r\n`\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\n\r\ncelsius_q = np.array([-40, -10, 0, 8, 15, 22, 38], dtype=float)\r\nfahrenfeit_a = np.array([-40, 14, 32, 46, 59, 72, 100], dtype=float)\r\nfor i, c in enumerate(celsius_q):\r\n    print('{0}C = {1}F'.format(c, fahrenfeit_a[i]))\r\n    \r\nlayer1 = tf.keras.layers.Dense(units=1, input_shape=[1])\r\nmodel = tf.keras.Sequential([layer1])\r\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))\r\nhistory = model.fit(celsius_q, fahrenfeit_a, epochs=500, verbose=False)\r\n`\r\n\r\n**Any other info / logs**\r\nThe error log as below:\r\n`\r\n\r\n(tfg) (base) D:\\adb>python tf2_demo.py\r\n-40.0C = -40.0F\r\n-10.0C = 14.0F\r\n0.0C = 32.0F\r\n8.0C = 46.0F\r\n15.0C = 59.0F\r\n22.0C = 72.0F\r\n38.0C = 100.0F\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0415 09:24:58.782853  1912 deprecation.py:506] From D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n2019-04-15 09:24:58.942776: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-04-15 09:24:59.943075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties:\r\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.30GiB\r\n2019-04-15 09:24:59.948936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0\r\n2019-04-15 09:25:00.987639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-15 09:25:00.991733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0\r\n2019-04-15 09:25:00.993541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N\r\n2019-04-15 09:25:00.995511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3011 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)                                                                                2019-04-15 09:25:11.523151: W tensorflow/core/common_runtime/bfc_allocator.cc:288] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.68GiB.  Current allocation summary follows.\r\n2019-04-15 09:25:11.528008: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (256):   Total Chunks: 36, Chunks in use: 34. 9.0KiB allocated for chunks. 8.5KiB in use in bin. 232B client-requested in use in bin.\r\n2019-04-15 09:25:11.533611: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (512):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.538796: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.3KiB allocated for chunks. 1.3KiB in use in bin. 1.0KiB client-requested in use in bin.                    2019-04-15 09:25:11.545015: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.549985: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.555897: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.562697: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (16384):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.                        2019-04-15 09:25:11.568183: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (32768):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.577286: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (65536):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.584167: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (131072):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.594069: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (262144):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.600881: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (524288):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.608271: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (1048576):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.615063: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (2097152):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.622202: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (4194304):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.629692: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (8388608):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.636440: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (16777216):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.643003: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (33554432):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.649099: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (67108864):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.655134: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (134217728):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.663212: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (268435456):     Total Chunks: 1, Chunks in use: 0. 2.94GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-15 09:25:11.668643: I tensorflow/core/common_runtime/bfc_allocator.cc:649] Bin for 5.68GiB was 256.00MiB, Chunk State:\r\n2019-04-15 09:25:11.675501: I tensorflow/core/common_runtime/bfc_allocator.cc:655]   Size: 2.94GiB | Requested Size: 4B | in_use: 0, prev:   Size: 256B | Requested Size: 28B | in_use: 1\r\n2019-04-15 09:25:11.679701: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400000 of size 1280\r\n2019-04-15 09:25:11.683459: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400500 of size 256\r\n2019-04-15 09:25:11.689566: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400600 of size 256\r\n2019-04-15 09:25:11.693736: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400700 of size 256\r\n2019-04-15 09:25:11.696694: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400800 of size 256\r\n2019-04-15 09:25:11.700230: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400900 of size 256\r\n2019-04-15 09:25:11.703917: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400A00 of size 256\r\n2019-04-15 09:25:11.707864: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400B00 of size 256\r\n2019-04-15 09:25:11.711760: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400C00 of size 256\r\n2019-04-15 09:25:11.717932: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400D00 of size 256\r\n2019-04-15 09:25:11.723400: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400E00 of size 256\r\n2019-04-15 09:25:11.727774: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400F00 of size 256\r\n2019-04-15 09:25:11.731836: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401000 of size 256\r\n2019-04-15 09:25:11.735367: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401100 of size 256\r\n2019-04-15 09:25:11.741123: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401200 of size 256\r\n2019-04-15 09:25:11.745619: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401300 of size 256\r\n2019-04-15 09:25:11.749705: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401400 of size 256\r\n2019-04-15 09:25:11.753128: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401500 of size 256\r\n2019-04-15 09:25:11.758698: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401600 of size 256\r\n2019-04-15 09:25:11.762093: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401700 of size 256\r\n2019-04-15 09:25:11.766516: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401800 of size 256\r\n2019-04-15 09:25:11.774200: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401900 of size 256\r\n2019-04-15 09:25:11.780786: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401A00 of size 256\r\n2019-04-15 09:25:11.785347: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401B00 of size 256\r\n2019-04-15 09:25:11.793098: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401C00 of size 256\r\n2019-04-15 09:25:11.799916: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401D00 of size 256\r\n2019-04-15 09:25:11.804645: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401E00 of size 256\r\n2019-04-15 09:25:11.812418: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401F00 of size 256\r\n2019-04-15 09:25:11.819120: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402000 of size 256\r\n2019-04-15 09:25:11.824766: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402100 of size 256\r\n2019-04-15 09:25:11.828505: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402200 of size 256\r\n2019-04-15 09:25:11.833054: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402300 of size 256\r\n2019-04-15 09:25:11.841134: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Free  at 0000000502402400 of size 256\r\n2019-04-15 09:25:11.848579: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402500 of size 256\r\n2019-04-15 09:25:11.855357: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Free  at 0000000502402600 of size 256\r\n2019-04-15 09:25:11.860341: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402700 of size 256\r\n2019-04-15 09:25:11.867202: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402800 of size 256\r\n2019-04-15 09:25:11.874567: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Free  at 0000000502402900 of size 3157422080\r\n2019-04-15 09:25:11.879528: I tensorflow/core/common_runtime/bfc_allocator.cc:677]      Summary of in-use Chunks by size:\r\n2019-04-15 09:25:11.883032: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 34 Chunks of size 256 totalling 8.5KiB\r\n2019-04-15 09:25:11.887446: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1280 totalling 1.3KiB\r\n2019-04-15 09:25:11.895103: I tensorflow/core/common_runtime/bfc_allocator.cc:684] Sum Total of in-use chunks: 9.8KiB\r\n2019-04-15 09:25:11.901440: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats:\r\nLimit:                  3157432729\r\nInUse:                        9984\r\nMaxInUse:                    10752\r\nNumAllocs:                      44\r\nMaxAllocSize:                 1280\r\n\r\n2019-04-15 09:25:11.916657: W tensorflow/core/common_runtime/bfc_allocator.cc:292] *___________________________________________________________________________________________________\r\n2019-04-15 09:25:11.923443: W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at dynamic_stitch_op.cc:132 : Resource exhausted: OOM when allocating tensor with shape[1525343649] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"tf2_demo.py\", line 13, in <module>\r\n    history = model.fit(celsius_q, fahrenfeit_a, epochs=500, verbose=False)\r\n  File \"D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 873, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 352, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3096, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1440, in __call__\r\n    run_metadata_ptr)\r\n  File \"D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 548, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1525343649] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node training/Adam/gradients/loss/dense_loss/mean_squared_error/Mean_grad/DynamicStitch}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n         [[GroupCrossDeviceControlEdges_0/training/Adam/Adam/Const/_72]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\n(tfg) (base) D:\\adb>import numpy as npimport matplotlib.pyplot as pltimport tensorflow as tfcelsius_q = np.array([-40, -10, 0, 8, 15, 22, 38], dtype=float)fahrenfeit_a = np.array([-40, 14, 32, 46, 59, 72, 100], dtype=float)for i, c in enumerate(celsius_q):    print('{0}C = {1}F'.format(c, fahrenfeit_a[i]))    layer1 = tf.keras.layers.Dense(units=1, input_shape=[1])model = tf.keras.Sequential([layer1])model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))history = model.fit(celsius_q, fahrenfeit_a, epochs=500, verbose=False)\r\n`\r\nBy the way this simple program I run successfully in TensorFlow 2.0.0-alpha0 binary version.\r\n", "comments": ["The source code version:\r\ngit clone https://github.com/tensorflow/tensorflow\r\ngit checkout v2.0.0-alpha0\r\n\r\nI found that the eager execution mode was turned of insteam turn on. Something wrong with this branch?", "@yt7589 The git clone link provided by you points to master (TF1.13.1). The correct link to TF2.0 is [here](https://github.com/tensorflow/tensorflow/tree/r2.0). \r\n\r\nThere is no issue with the \"simple code\"  as I was able to run it with TF1.12, TF1.13.1, and TF2.0. But, something wrong with the build. Could you build from the source again and see how the issue progresses. Thanks!", "I had built the r2.0 branch complete successfully. When I ran the same program its output as below:\r\n-40.0C = -40.0F\r\n-10.0C = 14.0F\r\n0.0C = 32.0F\r\n8.0C = 46.0F\r\n15.0C = 59.0F\r\n22.0C = 72.0F\r\n38.0C = 100.0F\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0419 19:57:08.641340  7520 deprecation.py:506] From D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n2019-04-19 19:57:09.095051: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-04-19 19:57:10.957178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties:\r\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.30GiB\r\n2019-04-19 19:57:10.987857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0\r\n2019-04-19 19:58:16.472440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-19 19:58:16.485270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0\r\n2019-04-19 19:58:16.493167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N\r\n2019-04-19 19:58:16.502242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3011 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-04-19 19:58:29.225684: W tensorflow/core/common_runtime/bfc_allocator.cc:288] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.49GiB.  Current allocation summary follows.\r\n2019-04-19 19:58:29.247239: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (256):   Total Chunks: 37, Chunks in use: 34. 9.3KiB allocated for chunks. 8.5KiB in use in bin. 232B client-requested in use in bin.\r\n2019-04-19 19:58:29.279712: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (512):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.297394: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.3KiB allocated for chunks. 1.3KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2019-04-19 19:58:29.313375: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.323326: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.330457: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.335402: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (16384):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.341108: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (32768):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.346667: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (65536):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.352183: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (131072):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.358436: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (262144):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.363626: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (524288):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.368867: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (1048576):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.374524: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (2097152):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.380065: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (4194304):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.385784: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (8388608):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.391331: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (16777216):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.396937: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (33554432):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.401965: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (67108864):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.407910: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (134217728):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.414755: I tensorflow/core/common_runtime/bfc_allocator.cc:633] Bin (268435456):     Total Chunks: 1, Chunks in use: 0. 2.94GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-04-19 19:58:29.420279: I tensorflow/core/common_runtime/bfc_allocator.cc:649] Bin for 5.49GiB was 256.00MiB, Chunk State:\r\n2019-04-19 19:58:29.425710: I tensorflow/core/common_runtime/bfc_allocator.cc:655]   Size: 2.94GiB | Requested Size: 0B | in_use: 0, prev:   Size: 256B | Requested Size: 28B | in_use: 1\r\n2019-04-19 19:58:29.430050: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400000 of size 1280\r\n2019-04-19 19:58:29.433774: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400500 of size 256\r\n2019-04-19 19:58:29.439711: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400600 of size 256\r\n2019-04-19 19:58:29.444074: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400700 of size 256\r\n2019-04-19 19:58:29.449969: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400800 of size 256\r\n2019-04-19 19:58:29.454293: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400900 of size 256\r\n2019-04-19 19:58:29.457711: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400A00 of size 256\r\n2019-04-19 19:58:29.461369: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400B00 of size 256\r\n2019-04-19 19:58:29.467627: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400C00 of size 256\r\n2019-04-19 19:58:29.471912: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400D00 of size 256\r\n2019-04-19 19:58:29.480252: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400E00 of size 256\r\n2019-04-19 19:58:29.486372: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502400F00 of size 256\r\n2019-04-19 19:58:29.492211: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401000 of size 256\r\n2019-04-19 19:58:29.495536: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401100 of size 256\r\n2019-04-19 19:58:29.499371: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401200 of size 256\r\n2019-04-19 19:58:29.504955: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401300 of size 256\r\n2019-04-19 19:58:29.509820: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401400 of size 256\r\n2019-04-19 19:58:29.515437: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401500 of size 256\r\n2019-04-19 19:58:29.519652: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401600 of size 256\r\n2019-04-19 19:58:29.526552: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401700 of size 256\r\n2019-04-19 19:58:29.532407: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401800 of size 256\r\n2019-04-19 19:58:29.536199: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401900 of size 256\r\n2019-04-19 19:58:29.542590: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401A00 of size 256\r\n2019-04-19 19:58:29.546253: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401B00 of size 256\r\n2019-04-19 19:58:29.551808: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401C00 of size 256\r\n2019-04-19 19:58:29.555652: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401D00 of size 256\r\n2019-04-19 19:58:29.563580: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401E00 of size 256\r\n2019-04-19 19:58:29.570183: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502401F00 of size 256\r\n2019-04-19 19:58:29.575297: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402000 of size 256\r\n2019-04-19 19:58:29.578721: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402100 of size 256\r\n2019-04-19 19:58:29.582637: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402200 of size 256\r\n2019-04-19 19:58:29.588516: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402300 of size 256\r\n2019-04-19 19:58:29.593694: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Free  at 0000000502402400 of size 256\r\n2019-04-19 19:58:29.596729: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402500 of size 256\r\n2019-04-19 19:58:29.600546: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Free  at 0000000502402600 of size 256\r\n2019-04-19 19:58:29.607571: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402700 of size 256\r\n2019-04-19 19:58:29.611909: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Free  at 0000000502402800 of size 256\r\n2019-04-19 19:58:29.617630: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Chunk at 0000000502402900 of size 256\r\n2019-04-19 19:58:29.621570: I tensorflow/core/common_runtime/bfc_allocator.cc:668] Free  at 0000000502402A00 of size 3157421824\r\n2019-04-19 19:58:29.629047: I tensorflow/core/common_runtime/bfc_allocator.cc:677]      Summary of in-use Chunks by size:\r\n2019-04-19 19:58:29.634695: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 34 Chunks of size 256 totalling 8.5KiB\r\n2019-04-19 19:58:29.638532: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1280 totalling 1.3KiB\r\n2019-04-19 19:58:29.645418: I tensorflow/core/common_runtime/bfc_allocator.cc:684] Sum Total of in-use chunks: 9.8KiB\r\n2019-04-19 19:58:29.648862: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats:\r\nLimit:                  3157432729\r\nInUse:                        9984\r\nMaxInUse:                    10752\r\nNumAllocs:                      44\r\nMaxAllocSize:                 1280\r\n\r\n2019-04-19 19:58:29.660593: W tensorflow/core/common_runtime/bfc_allocator.cc:292] *___________________________________________________________________________________________________\r\n2019-04-19 19:58:29.664828: W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at dynamic_stitch_op.cc:132 : Resource exhausted: OOM when allocating tensor with shape[1474944849] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"tf2.py\", line 13, in <module>\r\n    history = model.fit(celsius_q, fahrenfeit_a, epochs=500, verbose=False)\r\n  File \"D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 873, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 352, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3096, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1440, in __call__\r\n    run_metadata_ptr)\r\n  File \"D:\\abiz\\mnf\\dev\\tfg\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 548, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1474944849] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node training/Adam/gradients/loss/dense_loss/mean_squared_error/Mean_grad/DynamicStitch}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n         [[GroupCrossDeviceControlEdges_0/training/Adam/Adam/Const/_72]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nBest Regards", "I have a same question ~"]}, {"number": 27841, "title": "Cannot initialize two different models with the same checkpoint.", "body": "I'm trying to use TF Slim ResNet-v1-101 pre-trained model to initialize two input branches of model, with different inputs (RGB features[0] and depth map features[1] respectively) to further combine them and feed their joint result to some spatial convolution layers. In order to achieve that, I create separate scope for each of the branches and initialize the models with pre-trained checkpoint, excluding the last fully connected layer (`logits`) :\r\n\r\n**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 16.04:\r\n- TensorFlow version 1.13:\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GV100 32GB\r\n\r\n**Describe the current behavior**\r\n```\r\nwith tf.variable_scope(\"rgb_branch\"):\r\n    with tf.contrib.slim.arg_scope(resnet_v1.resnet_arg_scope()):\r\n        rgb_logits, end_points = resnet_v1.resnet_v1_101(features[0], self.num_classes, is_training=is_training)        \r\n\r\n        rgb_variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=['rgb_branch/resnet_v1_101/logits'])\r\n                \r\n        # strip scope name\r\n        rgb_assignment_map = { rgb_variables_to_restore[0].name.split(':')[0] : rgb_variables_to_restore[0]}\r\n        rgb_assignment_map.update({ v.name.split(':')[0].split('/', 1)[1] : v for v in rgb_variables_to_restore[1:] })\r\n      \r\n        tf.train.init_from_checkpoint(self.pre_trained_model_path, rgb_assignment_map)\r\n        \r\n        \r\nwith tf.variable_scope(\"depth_branch\"):\r\n    with tf.contrib.slim.arg_scope(resnet_v1.resnet_arg_scope()):     \r\n        depth_logits, end_points = resnet_v1.resnet_v1_101(features[1], self.num_classes, is_training=is_training)\r\n\r\n        depth_variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=['depth_branch/resnet_v1_101/logits'])\r\n                \r\n        depth_assignment_map = { depth_variables_to_restore[0].name.split(':')[0] : depth_variables_to_restore[0]}\r\n        depth_assignment_map.update({ v.name.split(':')[0].split('/', 1)[1] : v for v in depth_variables_to_restore[1:] })\r\n      \r\n        tf.train.init_from_checkpoint(self.pre_trained_model_path, depth_assignment_map)\r\n```\r\nDuring the second initialization (the depth branch), TF complains about shape of the logits layer as if it was never removed:\r\n\r\n`ValueError: Shape of variable rgb_branch/resnet_v1_101/logits/biases:0 ((3,)) doesn't match with shape of tensor resnet_v1_101/logits/biases ([1000]) from checkpoint reader.`\r\n\r\nThe problem doesn't occur when I initialize only one branch, and is tied only to the first of the branches defined - if I change the order of the branches, the above error would change to `depth_branch/resnet_v1_101/logits/biases:0`\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected - have two separate graphs, both initialized by the same checkpoint", "comments": ["This issue is better asked on tensorflow/models repo. Please post it on models repo from [here](https://github.com/tensorflow/models/issues/new)\r\nYou can also ask on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27841\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27841\">No</a>\n"]}, {"number": 27840, "title": "Proper way to build Tensorflow for arbitrary CUDA Compute Capability", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **docker image** (_tensorflow/tensorflow:devel-gpu-py3_) for building tensorflow\r\n- TensorFlow version: **1.13.0**\r\n- Python version: **3.5**\r\n- Installed using virtualenv? pip? conda?: **docker image**\r\n- Bazel version (if compiling from source): **0.23.2** _(default for the docker image)_\r\n- GCC/Compiler version (if compiling from source): **5.4.0 20160609** _(default for the docker image)_\r\n- CUDA/cuDNN version: **10.0**\r\n- GPU model and memory: [NVIDIA QUADRO K4200](https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/quadro-product-literature/DS-NV-Quadro-K4200-JUL24-US-NV-r-HR.pdf)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nFollowing the [build instructions for docker with GPU support with CUDA](https://www.tensorflow.org/install/source#gpu_support_2) leads to a pip wheel with support for CUDA Compute Capability 3.5, 5.2, 6.0, 6.1, 7.0.\r\n\r\nHowever, the GPU I am trying to set up Tensorflow for supports only CUDA Compute Capability 3.0. Going through the documentation I did not manage to find a clear instruction on how to enable support for non-default CUDA Compute Capability during the build process. Despite trying a couple of different approaches, I always got the following error:\r\n\r\n```python\r\n2019-04-14 22:58:01.822303: I tensorflow/compiler/xla/service/platform_util.cc:197] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0\r\n```\r\n\r\nwhen trying to execute the following command:\r\n```python\r\npython -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI tried to edit `/tensorflow_src/.tf_configure.bazelrc` in the docker container by having `TF_CUDA_COMPUTE_CAPABILITIES=\"3.0,3.5,5.2,6.0,6.1,7.0\"` before building, but this did not solve the problem.\r\nI set the respective shell variable `TF_CUDA_COMPUTE_CAPABILITIES=\"3.0,3.5,5.2,6.0,6.1,7.0\"` before configuring with bazel, but this also did not result in a working image.\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Have you tried to build with XLA turned off? It appears XLA requires a gpu with the minimal capability of 3.5: https://github.com/tensorflow/tensorflow/blob/adf8dac492adb33d4c71a4271f6f003a156eae22/tensorflow/compiler/xla/service/platform_util.cc#L36 ", "@wdirons, thank you for your fast response!\r\n\r\nI built tensorflow by modifying `build:xla --define with_xla_support=false` (it was true after the initial `./configure`, despite setting the use of XLA JIT to false) in `tensorflow_src/.tf_configure.bazelrc` and also by exporting `TF_CUDA_COMPUTE_CAPABILITIES=\"3.0\"`.\r\n\r\nThe built wheel installed successfully and seems to be working, I am using it now as a test.\r\nPerhaps these specifics can be somehow added in the documentation (or FAQ) and the issue can be closed. :) \r\n", "That is odd behavior, the behavior of ./configure setting it to true is expected, let me explain:\r\n\r\nWhen true is answered what is written to `.tf_configure.bazelrc` is:\r\n> build --define with_xla_support=true\r\n\r\nWhen `false` is answered what is wrrtten to `.tf_configure.bazelrc` is:\r\n> build:xla --define with_xla_support=true\r\n\r\nThe difference is in 'build' vs 'build:xla', bazel build automatically includes all build parameters, but only includes `build:xyz` parameters when `--config=xyz` is passed in via the bazel build command line", "In ./configure for master, but not in 1.13, is a warning about using XLA with TF_CUDA_COMPUTE_CAPABILITIES < 3.5.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure.py#L1021-L1023", "Thank you for the elaborate replies, @wdirons!\r\n\r\nI am novice to `bazel` so was unaware when it includes `build:xyz` parameters in the build. Therefore I modified the `build:xla --define with_xla_support` from `true` to `false` and the build seems to have succeeded and to result in a working wheel.\r\n\r\nUsing the `tensorflow/tensorflow:devel-gpu-py3` docker image from a couple of weeks ago, I also received the `WARNING: XLA does not support CUDA compute capabilities lower than 3.5. Disable XLA when running on older GPUs.` warning. It was a bit confusing because in my case it was printed towards the end of the configuration procedure.\r\n\r\nI had not taken a look at the `configure.py` before. During the configuration procedure I was not prompted to specify a list of CUDA compute capabilities as many tutorials point out.\r\nNow I see that this message is contained in `ask_cuda_compute_capabilities` and it was not printed due to the `TF_CUDA_COMPUTE_CAPABILITIES` environment variable being already set up in the docker image.", "Closing this issue since its resolved. Feel free to reopen if have further questions. Thanks!", "Can't use tersonflow with K4200, so I have to use Pytorch. Good job, tersonflow team!\r\n```\r\n(tensorflow) \u279c  tensorflow python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n\r\n2019-10-23 18:52:56.886068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-10-23 18:52:56.918140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: Quadro K4200 major: 3 minor: 0 memoryClockRate(GHz): 0.784\r\npciBusID: 0000:03:00.0\r\n2019-10-23 18:52:56.920414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-23 18:52:56.949430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-23 18:52:56.964393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-23 18:52:56.969524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-23 18:52:57.004353: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-23 18:52:57.026557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-23 18:52:57.093799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-23 18:52:57.095530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Ignoring visible gpu device (device: 0, name: Quadro K4200, pci bus id: 0000:03:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\r\n2019-10-23 18:52:57.134106: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-23 18:52:57.269851: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294740000 Hz\r\n2019-10-23 18:52:57.274847: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5b7df80 executing computations on platform Host. Devices:\r\n2019-10-23 18:52:57.274902: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-10-23 18:52:57.334299: I tensorflow/compiler/xla/service/platform_util.cc:197] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0\r\n2019-10-23 18:52:57.342662: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\n```", "> Can't use tersonflow with K4200, so I have to use Pytorch. Good job, tersonflow team!\r\n> \r\n> ```\r\n> (tensorflow) \u279c  tensorflow python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n> \r\n> 2019-10-23 18:52:56.886068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n> 2019-10-23 18:52:56.918140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\n> name: Quadro K4200 major: 3 minor: 0 memoryClockRate(GHz): 0.784\r\n> pciBusID: 0000:03:00.0\r\n> 2019-10-23 18:52:56.920414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n> 2019-10-23 18:52:56.949430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n> 2019-10-23 18:52:56.964393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n> 2019-10-23 18:52:56.969524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n> 2019-10-23 18:52:57.004353: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n> 2019-10-23 18:52:57.026557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n> 2019-10-23 18:52:57.093799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-10-23 18:52:57.095530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Ignoring visible gpu device (device: 0, name: Quadro K4200, pci bus id: 0000:03:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\r\n> 2019-10-23 18:52:57.134106: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2019-10-23 18:52:57.269851: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294740000 Hz\r\n> 2019-10-23 18:52:57.274847: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5b7df80 executing computations on platform Host. Devices:\r\n> 2019-10-23 18:52:57.274902: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n> 2019-10-23 18:52:57.334299: I tensorflow/compiler/xla/service/platform_util.cc:197] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0\r\n> 2019-10-23 18:52:57.342662: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\n> ```\r\n\r\nPyTorch 1.5 also does not support compute capability <3.5", "In order to get custom CC during compilation I'd suggest simply editing the ./configure.py file (https://github.com/tensorflow/tensorflow/blob/master/configure.py)\r\n\r\n```\r\n...\r\n_DEFAULT_CUDA_COMPUTE_CAPABILITIES = '3.5,7.0'\r\n...\r\n```"]}, {"number": 27839, "title": "com.ai.tensorflow A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x28 in tid 17491 (m.ai.tensorflow)", "body": "<em>it has a crash in tensorflow lite android App\r\n **--------- beginning of crash\r\ncom.ai.tensorflow A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x28 in tid 17491\r\n (m.ai.tensorflow)**\r\n</em>\r\n\r\n**System information**\r\n- Have I use tensorflow demo app\r\n**tensorflow/tensorflow/lite/examples/android/**\r\n [lite android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android)\r\n- the issue happens on mobile device:android 5.0.2\r\n\r\n\r\n\r\n** How this error is happen**\r\nTFLiteObjectDetectionAPIModel.java\r\n\r\n1.`tfLite = new Interpreter(loadModelFile(assetManager, modelFilename));`\r\nafter get the tfLite, and then to detect the image\r\n2.when it wants to stop the tflite by:\r\n`tfLite.close();`\r\n\r\n3.if it's running image detect by : `tfLite.runForMultipleInputsOutputs(inputArray, outputMap);`\r\n  and this timing I call the\uff1a`tfLite.close();`\r\nthen this error happened\r\n\r\n**all the error information\uff1a**\r\n04-15 03:34:43.581 17441-17491/com.ai.tensorflow A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x28 in tid 17491 (m.ai.tensorflow)\r\n04-15 03:34:43.586 17441-17488/com.ai.tensorflow A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x28 in tid 17488 (m.ai.tensorflow)\r\n04-15 03:34:43.596 17441-17490/com.ai.tensorflow A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x10 in tid 17490 (m.ai.tensorflow)\r\n04-15 03:34:43.685 11849-11849/? I/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n04-15 03:34:43.685 11849-11849/? I/DEBUG: Revision: '0'\r\n04-15 03:34:43.685 11849-11849/? I/DEBUG: ABI: 'arm'\r\n04-15 03:34:43.686 11849-11849/? I/DEBUG: pid: 17441, tid: 17491, name: m.ai.tensorflow  >>> com.ai.tensorflow <<<\r\n04-15 03:34:43.686 11849-11849/? I/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x28\r\n04-15 03:34:43.708 11849-11849/? I/DEBUG:     r0 00000028  r1 0000000c  r2 0000000f  r3 00000000\r\n04-15 03:34:43.709 11849-11849/? I/DEBUG:     r4 0000000f  r5 b908ef30  r6 9f26b1f4  r7 9f565cf8\r\n04-15 03:34:43.709 11849-11849/? I/DEBUG:     r8 b910d888  r9 0000000c  sl 00000001  fp 00000000\r\n04-15 03:34:43.710 11849-11849/? I/DEBUG:     ip 00000001  sp 9f565cc0  lr a42ee2af  pc a42ed98c  cpsr 200d0030\r\n04-15 03:34:43.710 11849-11849/? I/DEBUG: backtrace:\r\n04-15 03:34:43.710 11849-11849/? I/DEBUG:     #00 pc 0005198c  /data/app/com.ai.tensorflow-1/lib/arm/libtensorflowlite_jni.so\r\n04-15 03:34:43.710 11849-11849/? I/DEBUG:     #01 pc 000522ab  /data/app/com.ai.tensorflow-1/lib/arm/libtensorflowlite_jni.so\r\n04-15 03:34:43.710 11849-11849/? I/DEBUG:     #02 pc 000d6bd3  /data/app/com.ai.tensorflow-1/lib/arm/libtensorflowlite_jni.so\r\n04-15 03:34:43.710 11849-11849/? I/DEBUG:     #03 pc 000f5a7f  /data/app/com.ai.tensorflow-1/lib/arm/libtensorflowlite_jni.so\r\n04-15 03:34:43.710 11849-11849/? I/DEBUG:     #04 pc 00012ff7  /system/lib/libc.so (__pthread_start(void*)+30)\r\n04-15 03:34:43.710 11849-11849/? I/DEBUG:     #05 pc 00010fcf  /system/lib/libc.so (__start_thread+6)\r\n\r\n\r\n", "comments": ["@lovenaturelovelifedevelop Could you provide more details on the issue and its context? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27839\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27839\">No</a>\n"]}, {"number": 27838, "title": "[TF 2.0 docs] Add example for EarlyStopping", "body": "I think a short example like this is very useful for user who don't have much experience and want to use a callback for their first time.", "comments": ["If the responses are positive I will create examples for the other callbacks, too. ;)", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27838) for more info**.\n\n<!-- need_author_consent -->", "Thank you again @kyscg! I like the interactive style and will try to make the next examples in a similiar way!", "@rthadur, gentle ping to review and set CLA's to yes", "@aweers can you please sign CLA ", "@rthadur I signed the CLA ;)", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27838) for more info**.\n\n<!-- cla_yes -->", "@rthadur @rchao Gentle ping to review", "@rchao I updated the file with your suggestions", "But somehow I don't see a new commit? Only if I try to edit the file again, I see the changes I made...", "> But somehow I don't see a new commit? Only if I try to edit the file again, I see the changes I made...\r\n\r\nThanks! I don't see it either. Please let me know once the commit is made.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27838) for more info**.\n\n<!-- need_author_consent -->", "@rchao @rthadur now the commit is made, don't know why it took some time... But CLA check failed again, so it need to be set by you ;)", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27838) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 27837, "title": "[TF 2.0 API Docs] Added Documentation to tf.keras.activations.selu", "body": "Fix #27657 by adding required information to tf.keras.activations.selu [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py) and added examples of FNN instead of ConvNets because SELU finds better use there. Also added an image of the function, hoping the markdown renders properly. ", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27837) for more info**.\n\n<!-- need_author_consent -->", "@kyscg @rthadur @googlebot any ways to confirm the CLA if all co-authors of the commit have already signed it (Even though individual commits are made at later stages)? Is this level of granularity necessary (for each commit?) just asking out of curiosity", "I think it is necessary, there is no other way of knowing whether all the co-authors are aware of a proposed merge. It's not that complex either.", "@rthadur, gentle ping to review and set CLA's to yes", "Sorry again @kyscg but as @ekerazha pointed out, the `kernel_initializer='lecun_normal'` has been used. Please re-review", "@Bharat123rox thank you for your contribution , please make sure all the contributors have signed CLA", "Yes, all contributors have signed the CLA @Bharat123rox and @kyscg and are\nwilling to merge contributions in source code\n\nOn Mon, 15 Apr 2019, 10:47 pm rthadur, <notifications@github.com> wrote:\n\n> @Bharat123rox <https://github.com/Bharat123rox> thank you for your\n> contribution , please make sure all the contributors have signed CLA\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27837#issuecomment-483339145>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AMwu8bRdkJk3ghjI5SoIE8CFeLyjjBnQks5vhLQUgaJpZM4cuvp2>\n> .\n>\n", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27837) for more info**.\n\n<!-- cla_yes -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27837) for more info**.\n\n<!-- need_author_consent -->", "@gbaned @rthadur please change CLAs to yes", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27837) for more info**.\n\n<!-- cla_yes -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27837) for more info**.\n\n<!-- need_author_consent -->", "@rthadur @gbaned please add the `cla:yes` labels and @tanzhenyu please re-review", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27837) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 27836, "title": "XLA: specify num_partitions in xla_cpu_runtime_ParallelForkJoin", "body": "I am interested in the LLVM IR obtained by launching a Tensorflow test program with the `--xla_dump_ir_to` flag. In particular, I would like XLA to create a certain number of copies of a function that can run in parallel on multiple CPUs. \r\n(I know this sounds like a support question rather than an issue, but nobody could answer me on StackOverflow until now, so I figured that maybe what I am looking for is a feature that still does not exist)\r\n\r\nI have a test program that includes these lines\r\n`with tf.device(\"device:XLA_CPU:0\"):`\r\n`       y = tf.nn.conv2d(X, weights_tensor, strides=[1, 1, 1, 1], padding='SAME')`\r\n`    sess.run(tf.global_variables_initializer())`\r\n\r\nAnd in the output `.ll` files XLA generates a call to\r\n`call void @__xla_cpu_runtime_ParallelForkJoin(i8* %6, i8* %run_options, i8** null, i8** %buffer_table, i64* %prof_counters, i32 56, i64* getelementptr inbounds ([224 x i64], [224 x i64]* @parallel_convolution_parallel_dimension_partitions, i32 0, i32 0), i32 2, i8* bitcast (void (i8*, i8*, i8**, i8**, i64*, i64*)* @parallel_convolution to i8*))`\r\nwhich is a good starting point.\r\n\r\nBut now I want to specify how many copies of that function I want. What I have tried up to now is to use these options\r\n`config = tf.ConfigProto()`\r\n`config.device_count={\"CPU\": 5}`\r\n`config.intra_op_parallelism_threads=5`\r\n`config.inter_op_parallelism_threads=5`\r\nand they still lead to the same call, with the `num_partitions` argument set to 56 (number of CPUs on my machine).\r\n\r\nMoreover, a different test program with\r\n`with tf.device(\"device:XLA_CPU:0\"):`\r\n`                y=tf.layers.max_pooling2d(inputs=x, pool_size=[3, 3], strides=3)`\r\n`        sess.run(y,{x: reshaped_input_array})`\r\ndoes not generate any call to `xla_cpu_runtime_ParallelForkJoin` at all.\r\n\r\nIs it possible to force XLA in any way to produce parallel copies of a function, and to specify how many copies?\r\n\r\n\r\n### System information\r\n- OS Platform and Distribution: Linux 4.19.0-4-amd64 #1 SMP Debian 4.19.28-2 (2019-03-15)\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 2.7.16\r\n- Bazel version: 0.21.0\r\n- GCC version: 8.3.0\r\n", "comments": ["@jlebar ", "> Is it possible to force XLA in any way to produce parallel copies of a function, and to specify how many copies?\r\n\r\nNo, exactly how XLA generates LLVM IR for a particular op is an implementation detail.\r\n\r\nIf you're interested in contributing patches to add this feature, xla-dev@googlegroups.com would be a place to start the discussion."]}]