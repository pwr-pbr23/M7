[{"number": 788, "title": "Fix missing dependency declarations error #469 #772", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins, test this please.\n", "Merged\n"]}, {"number": 787, "title": "Fix typos in a testcase name and comments of core/common_runtime module.", "body": "Please note that this PR renames the following test case.\n\n```\n-TEST(EventMgr, ManySmallTensorsSeperateCallsFlushed) {\n+TEST(EventMgr, ManySmallTensorsSeparateCallsFlushed) {\n```\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 785, "title": "OOM error when using dropout", "body": "I thought I might have been going crazy so I only got around to posting this just now. I've got a GPU that can handle a batch size of 160 on my network, but I run it at 128. Thats just to show that I've got about 20% space left on my GPU ram when the network starts training. If I am using dropout, after about 100,000 iterations i get an OOM error.\n\n```\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 128 } dim { size: 40 } dim { size: 40 } dim { size: 64 }\n         [[Node: range_1/_1993 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_25333_range_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n```\n\nIs it possible that there could be a memory leak somewhere?\n", "comments": ["TensorFlow doesn't do dropout inplace to my understanding, so in the worse case scenario, it can use double the memory (assuming you place dropout everywhere in your graph).\n", "I feel like a memory leak would likely occur much earlier than 100,000 iterations -- the logs should produce a dump of the OOM information, which could help quickly identify whether there's an obvious memory leak.\n\nMost likely it's just that you're already operating pretty close to the memory capacity of the device and that the order of node execution caused the system to allocate memory in a suboptimal order.\n\nAs mentioned on the [roadmap](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/roadmap.md), we're working on improving the memory consumption and reporting, so stay tuned.\n", "I don't think there's enough information to diagnose here.  Feel free to reopen if more details emerge.\n"]}, {"number": 784, "title": "TensorBoard cannot feed to placeholder x-input on MNIST (for merged only)", "body": "I am running a slightly different version of mnist+relog because I recently installed TF on my computer and it does not have access to Internet (and I wanted to do the Image processing part myself).\nIt runs completely fine without TensorBoard and gives me 0.9166 accuracy on Test set which is not too bad for simple relog on unrolled images.\nHowever when I add in the for loop the part where you feed feed_dist to merged (which is the fusion of all the summaries) I got the following error message:  \n\n```\nInvalidArgumentError: You must feed a value for placeholder tensor 'x-input_4' with dtype float\n     [[Node: x-input_4 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n     [[Node: x-input_7/_23 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_152_x-input_7\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nCaused by op u'x-input_4', defined at:\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/spyderlib/widgets/externalshell/start_ipython_kernel.py\", line 188, in <module>\n    __ipythonkernel__.start()\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py\", line 374, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 151, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 840, in start\n    handler_func(fd_obj, events)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py\", line 252, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py\", line 213, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py\", line 362, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py\", line 181, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2868, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2978, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3032, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-48-28d84e6d5ef3>\", line 1, in <module>\n    runfile('/home/jeandut/mnist_jean_tf.py', wdir='/home/jeandut')\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 682, in runfile\n    execfile(filename, namespace)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 78, in execfile\n    builtins.execfile(filename, *where)\n  File \"/home/jeandut/mnist_jean_tf.py\", line 64, in <module>\n    x=tf.placeholder(tf.float32, [None, 784],name=\"x-input\")\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 673, in placeholder\n    name=name)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 463, in _placeholder\n    name=name)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/jeandut/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n```\n\nIt is therefore :   \n- me badly implementing tensorboard part (but as I copied almost exactly what you did in the mnist tutorial I cannot find where it the bug is)  \n- a bug in my installation of tensorflow somehow...  \n- something else  \n\nMy code:\n\n```\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Jan 14 13:06:44 2016\n\n@author: jeandut\n\"\"\"\n#from tensorflow.examples.tutorials.mnist import input_data\n#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\nimport tensorflow as tf\nimport os\nimport random\nimport numpy as np\nfrom array import array\nimport struct\nimport matplotlib.pyplot as plt\nimport time\n\n#I imported myself the decompressed -ubyte files from mnist cause I do not have internet they are at the path I wrote replace with appropriate path to reproduce\nos.chdir('/home/jeandut/Bureau/Step1/')\n\nwith open(\"train-labels.idx1-ubyte\") as file:\n    magic, size = struct.unpack(\">II\",file.read(8))\n    train_labels_data=np.asarray(array(\"B\",file.read()))\nwith open(\"t10k-labels.idx1-ubyte\") as file:\n    magic, size = struct.unpack(\">II\",file.read(8))\n    test_labels_data=np.asarray(array(\"B\",file.read()))\nwith open(\"train-images.idx3-ubyte\") as file:\n    magic, size, rows, cols =struct.unpack(\">IIII\",file.read(16))\n    train_images_data=np.reshape(np.asarray(array(\"B\",file.read())),(size,rows,cols))\nwith open(\"t10k-images.idx3-ubyte\") as file:\n    magic, size, rows, cols =struct.unpack(\">IIII\",file.read(16))\n    test_images_data=np.reshape(np.asarray(array(\"B\",file.read())),(size,rows,cols))\n\n\n\n\n\nfor i in range(10):\n   plt.imshow(train_images_data[i,:])\n   plt.show()\n   print(train_labels_data[i])\n\n\ntrain_images=np.reshape(train_images_data,(60000,28*28)).astype(np.float32)*1/255\ntest_images=np.reshape(test_images_data,(10000,28*28)).astype(np.float32)*1/255\n\ntrain_labels=np.zeros((60000,10),dtype=np.float32)\ntest_labels=np.zeros((10000,10),dtype=np.float32)\n\nfor i in range(60000):\n    a=train_labels_data[i]\n    train_labels[i,a]=1.\n\nfor j in range(10000):\n    b=test_labels_data[j]\n    test_labels[j,b]=1.\n\n\n\nsess=tf.Session()\n\n\nx=tf.placeholder(tf.float32, [None, 784],name=\"x-input\")\nW=tf.Variable(tf.zeros([784, 10]),name=\"weights\")\nb=tf.Variable(tf.zeros([10]),name=\"bias\")\n\n\nwith tf.name_scope(\"Wx_b\") as scope:\n    y=tf.nn.softmax(tf.matmul(x,W) + b)\n\n\nw_hist=tf.histogram_summary(\"weights\",W)\nb_hist=tf.histogram_summary(\"bias\",b)\ny_hist=tf.histogram_summary(\"y\",y)\n\n\ny_ =tf.placeholder(tf.float32, [None, 10], name=\"y-input\")\n\ny_hist=tf.histogram_summary(\"y\",y)\nwith tf.name_scope(\"xent\") as scope:\n\n    cross_entropy= -tf.reduce_sum(y_*tf.log(y))\n    ce_summ=tf.scalar_summary(\"cross_entropy\", cross_entropy)\n\n\nwith tf.name_scope(\"train\") as scope:\n    train_step=tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n\n\nwith tf.name_scope(\"train\") as scope:\n    correct_prediction =tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n\n\n    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n    accuracy_summary=tf.scalar_summary(\"accuracy\",accuracy)\n\nmerged=tf.merge_all_summaries()\nwriter=tf.train.SummaryWriter(\"tmp/mnist_logs\",sess.graph_def)\n\n\n\n\n\n\ninit=tf.initialize_all_variables()\nsess.run(init)\n\n\nfor i in range(1000):\n    if i % 10 == 0:\n        feed={x:test_images, y_: test_labels}\n        result=sess.run([merged, accuracy],feed_dict=feed)\n        summary_str=result[0]\n        acc=result[1]\n        writer.add_summary(summary_str, i)\n        print(\"Accuracy at step %s: %s\" % (i,acc))\n    else:\n        index=np.random.randint(60000-1,size=100)\n        batch_xs, batch_ys = train_images[index,:], train_labels[index]\n        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\n\n\n\n\nprint(sess.run(accuracy, feed_dict={x: train_images, y_: train_labels}))\n\n\n\n```\n\nI posted the question on stack but nobody seems to be able to solve it ! Please help !\n", "comments": ["I find out that once you run it once in spyder it prevents you from runing it again on the same session\n"]}, {"number": 783, "title": "Gradient computation erroneously returns None", "body": "``` python\nIn [5]: tf.gradients(tf.constant(5), tf.Variable(0))\nOut[5]: [None]\n```\n\nThe derivative of 5 with respect to x should be 0.\n", "comments": ["I'm not sure this isn't the desired behavior.  Returning `None` makes it explicit that there is no graph connection between the two.\n", "If it's called \"gradients\" then I expect it to compute gradients... The gradient of e.g. 5 with respect to some vector v is most certainly the zero vector.\n\nAlso just from a practicality standpoint, you should be able to compute gradients and then perform mathematical operations with them without having to worry about something unexpectedly becoming a non-`Tensor` and causing an exception to be raised. In some TF code I wrote recently I had to make the following function to avoid this bug:\n\n``` python\ndef _compute_gradients(tensor, var_list):\n  grads = tf.gradients(tensor, var_list)\n  return [grad if grad is not None else tf.zeros_like(var)\n          for var, grad in zip(var_list, grads)]\n```\n", "Yep, I agree that this is a bug; my comment was poorly worded.  @mrry: Do we use the `None` feature anywhere that changing this would break?  It could certainly cause some code to get slower, which is a potential concern.\n", "I'm pretty sure this is intended behavior. [`Optimizer.apply_gradients()`](https://github.com/tensorflow/tensorflow/blob/b159e211860e26e3dbae26989163b9da55e3f430/tensorflow/python/training/optimizer.py#L239) uses the fact that individual gradients may be `None` to remove assignment ops. While this would be an easy case for a graph rewriter, it's not something we currently do, so I'd be a little worried about regressions if various variables suddenly started being updated every time.\n\nSince `tf.gradients()` is mostly hidden behind the optimizers, and the behavior isn't currently documented, we could consider a breaking change to the API, but I think we'd want to retain the existing functionality. Perhaps it could be controlled by an optional arg?\n", "Hmm, I get the efficiency argument. I agree that a keyword arg could be a suitable workaround. E.g.\n\n``` python\ntf.gradients(tf.constant(5), tf.Variable(0), return_zeros=True)\n```\n", "It looks like there are hundreds of direct uses of `tf.gradients` within Google, so I don't think a silent performance breaking change is okay.  If we're going to change the default behavior, I think the only way would be to make the special `Zeros` class and give it suitable arithmetic overloads.  That way, anyone who doesn't realize and treats it as a normal tensor in a way that doesn't take advantage of zeros will throw an exception.\n\nFor now, how about a `return_zeros` argument that defaults to `False`?\n", "SGTM, and going for a keyword arg would allow for easy deprecation once/if the `Zeros` class gets implemented.\n", "Writing this now.\n", "Actually, there's a wrinkle: `None` is used to indicate a variety of different things:\n1. There is no connection from input to output.\n2. There is a connection, but it's through a discrete variable with meaningless gradients.\n3. There is a connection, but it's through an op that doesn't have an implemented gradient.\n\n(3) in particular would be very bad to replace with zeros.\n", "Ahh yeah I would think only (1) is a bug. Do you think it would be better to raise separate exceptions instead of returning `None` for both (2) and (3)? Allow the user to handle them separately?\n\nOne thing I could imagine is adding a `return_none` flag instead of a `return_zeros` flag, with a default value of `True`. When set to false, it raises `GradientOfDiscreteVariableError` for (2) and `NoImplementedGradientError` for (3), and returns an appropriately shaped zero Tensor for (1). When set to `True` it maintains the status quo and returns `None` in all cases. And then eventually plan to deprecate the `return_none=True` path.\n\nWhat do you think? Obviously I'd be happy to help out.\n", "That's sounds reasonable, but unfortunately it would require adjusting the gradient functions, since individual gradient functions also use `None` for both (1) and (2).\n\n@mrry: What do the think about the `Zeros` solution? \n", "I have the same problem, I do not kwon core implementation about tf and theano, but I think that the param disconnected_inputs in  theano.gradient.grad() would be good\n", "I'm going to close this for now.  It's a marginal benefit with an unfortunately large amount of work, since introducing `Zeros` or similar will break a lot of downstream code.\n", "I've been using this work-around\n\ndef replace_none_with_zero(l):\n  return [0 if i==None else i for i in l]\n\ngrads = replace_none_with_zero(tf.gradients([grads[0]], [x, y]))\n\nOn Tue, Aug 9, 2016 at 4:39 PM, Geoffrey Irving notifications@github.com\nwrote:\n\n> Closed #783 https://github.com/tensorflow/tensorflow/issues/783.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/783#event-750619754, or mute\n> the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHM0gn5SXMbEBDAZ_iSIYWmRFNQO4ks5qeQ-qgaJpZM4HFmRr\n> .\n", "I think I've just ran into this:\r\n\r\n```\r\nwith tf.Graph().as_default():\r\n  with tf.Session() as sess:\r\n    x = tf.constant([1.0])\r\n    x_double = 2*x\r\n    print sess.run(tf.hessians(x_double, x))\r\n\r\n# ValueError: None values not supported.\r\n```\r\n\r\n=\\", "> Actually, there's a wrinkle: `None` is used to indicate a variety of different things:\r\n> \r\n> 1. There is no connection from input to output.\r\n> 2. There is a connection, but it's through a discrete variable with meaningless gradients.\r\n> 3. There is a connection, but it's through an op that doesn't have an implemented gradient.\r\n> \r\n> (3) in particular would be very bad to replace with zeros.\r\n\r\n@girving, excuse me Sir,  May I ask: There are two session run in my certain project, and the first run outputs the middle-results that is inputed into the second run, and this two step-run are respectively the first part and the next part of a complete whole network. After the second run, I get the result of loss and I try to compute all the parameters in both two parts of the network above, by **_tf.gradients_**, and then **_optimizer.apply_gradients_**. But, as you declared, lt's not possible to achieve this, cause there is no direct graph connection between this two session run. And now I'm in the dilemma cause by this problem. So if there is any suggestion that you can give to me? Thank u so much. You can make some further detail about my problem if necessary.", "@scofield7419 Your question is unrelated to this issue.  Please use stackoverflow for questions like this instead.", "Then I take it the official documentation of tf.gradients() is erroneous?\r\n\r\n> `unconnected_gradients` determines the value returned for each x in xs if it is unconnected in the graph to ys. By default this is None to safeguard against errors. MAthematically these gradients are zero which can be requested using the 'zero' option. tf.UnconnectedGradients provides the following options and behaviors:\r\n> ```\r\n> a = tf.ones([1, 2])\r\n> b = tf.ones([3, 1])\r\n> g1 = tf.gradients([b], [a], unnconnected_gradients='none')\r\n> sess.run(g1)  # [None]\r\n> \r\n> g2 = tf.gradients([b], [a], unconnected_gradients='zero')\r\n> sess.run(g2)  # [array([[0., 0.]], dtype=float32)]\r\n> ```\r\n\r\nThere doesn't seem to be such a keyword argument though.", "I met this problem when using BN layer(tf.layers.batch_normalization)\r\nWhen I calculate the gradients using opt.compute_gradients(), and it shows the None gradient comes from `((None, <tf.Variable 'VLAD/cluster_bn/gamma:0' shape=(512,) dtype=float32_ref>),)`\r\n\r\nHowever, the gradients of a BN layer is supposed to be:\r\n```\r\n((<tf.Tensor 'gradients/VLAD/vlad_bn0/batchnorm/mul_grad/tuple/control_dependency_1:0' shape=(64,) dtype=float32>, <tf.Variable 'VLAD/vlad_bn0/gamma:0' shape=(64,) dtype=float32_ref>),)\r\n\r\n((<tf.Tensor 'gradients/VLAD/vlad_bn0/batchnorm/sub_grad/tuple/control_dependency:0' shape=(64,) dtype=float32>, <tf.Variable 'VLAD/vlad_bn0/beta:0' shape=(64,) dtype=float32_ref>),)\r\n\r\n```\r\nand the traceback is as follow:\r\n\r\nTraceback (most recent call last):\r\n\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper\r\n>     preferred_dtype=default_dtype)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1094, in internal_convert_to_tensor\r\n>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 217, in _constant_tensor_conversion_function\r\n>     return constant(v, dtype=dtype, name=name)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 196, in constant\r\n>     value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\", line 424, in make_tensor_proto\r\n>     raise ValueError(\"None values not supported.\")\r\n> ValueError: None values not supported.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 524, in _apply_op_helper\r\n>     values, as_ref=input_arg.is_ref).dtype.name\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1094, in internal_convert_to_tensor\r\n>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 217, in _constant_tensor_conversion_function\r\n>     return constant(v, dtype=dtype, name=name)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 196, in constant\r\n>     value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\", line 424, in make_tensor_proto\r\n>     raise ValueError(\"None values not supported.\")\r\n> ValueError: None values not supported.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"train.py\", line 192, in <module>\r\n>     main()\r\n>   File \"train.py\", line 188, in main\r\n>     Trainer.train()\r\n>   File \"train.py\", line 136, in train\r\n>     grads = self.average_gradients(tower_grads)\r\n>   File \"train.py\", line 61, in average_gradients\r\n>     expanded_g = tf.expand_dims(g, 0)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 136, in expand_dims\r\n>     return gen_array_ops.expand_dims(input, axis, name)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2020, in expand_dims\r\n>     \"ExpandDims\", input=input, dim=axis, name=name)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 528, in _apply_op_helper\r\n>     (input_name, err))\r\n> ValueError: Tried to convert 'input' to a tensor and failed. Error: None values not supported.", "> If it's called \"gradients\" then I expect it to compute gradients... The gradient of e.g. 5 with respect to some vector v is most certainly the zero vector.\r\n> \r\n> Also just from a practicality standpoint, you should be able to compute gradients and then perform mathematical operations with them without having to worry about something unexpectedly becoming a non-`Tensor` and causing an exception to be raised. In some TF code I wrote recently I had to make the following function to avoid this bug:\r\n> \r\n> ```python\r\n> def _compute_gradients(tensor, var_list):\r\n>   grads = tf.gradients(tensor, var_list)\r\n>   return [grad if grad is not None else tf.zeros_like(var)\r\n>           for var, grad in zip(var_list, grads)]\r\n> ```\r\n\r\nHow does this affect the computation since we are just setting them to Zero?"]}, {"number": 782, "title": "Adds GPU CUDA kernel for scatter ops", "body": "This adds GPU CUDA kernels for scatter_add, scatter_sub, and scatter_update.\n\nTesting on a local machine with a weak graphics card already shows good improvement.  For about 20M floats, split as a 2Mx10 matrix, 1M size for idx, the op on a weak 3-core GPU is about twice as fast as on a 12-core CPU.  On very small data sets (1000 floats) the GPU code seems a bit slower (20%).\n\nSome notes:\n1. range checking in CUDA kernel currently not implemented.  It's not possible to include `OpKernelContext` since it includes protos hence nvcc fails to compile.  I can do it as a method in the .h/.cc files.  However, it seems rather inefficient as for low-dimensional input it is basically as slow as doing the entire scatter op sequentially on CPU.  Would it make sense to run the entire kernel, ignore bad indices, and report after?  This does mean the parameters may be updated when the code does fail because the modifications are happening in place.  Advice?<br>\n2. There's a weird bug in `scatter_op_gpu.cu.cc` where I cannot use `TF_CALL_GPU_NUMBER_TYPES`.  I suspect it has to do with the `defined(__ANDROID__)` magic in `register_types.h` interacting badly with nvcc, but cannot confirm.\n", "comments": ["Can one of the admins verify this patch?\n", "There are a lot of unrelated commits in this PR -- can you separate them out into individual PRs?  Specifically, fixing the typos and such can be done as a separate PR, and then the main GPU impl + test can be in a single commit for reviewing.\n", "Done.\n", "Btw, with this change, I could re-enable word2vec example GPU support: https://github.com/fpmchu/tensorflow/commit/121784b62da64f3a799cc0eb6776ca4f35594a9f\n\nHowever, on my machine, this made the example _slower_.  Either it's because my card is weak, or perhaps the example needs to be optimized for using GPU?  I'm not sure if it's my code that's slow...\n", "Quite possibly adding this adds a bunch of transfers back and forth, which\nare slow. Optimizing the example for GPU would fix that.\n\nMartin\nOn Fri, Jan 15, 2016 at 18:52 Frank Chu notifications@github.com wrote:\n\n> Btw, with this change, I could re-enable word2vec example GPU support:\n> fpmchu@121784b\n> https://github.com/fpmchu/tensorflow/commit/121784b62da64f3a799cc0eb6776ca4f35594a9f\n> \n> However, on my machine, this made the example _slower_. Either it's\n> because my card is weak, or perhaps the example needs to be optimized for\n> using GPU? I'm not sure if it's my code that's slow...\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/782#issuecomment-172149174\n> .\n", "@keveman After reading more, it seems like Thrust's scatter does not support repeated items.  [Thrust's doc](https://thrust.github.io/doc/group__scattering.html) says:\n\n> If the same index appears more than once in the range [map, map + (last - first)), the result is undefined\n\nLooking at [their CUDA code](https://github.com/thrust/thrust/blob/67bbec34a7fdb9186aea6c269870b517e42e4388/thrust/system/detail/generic/scatter.inl), I don't think they do anything special to handle repeated indices either.\n\nSimilarly, numpy also says the behavior is undefined.  From [numpys ndarray's doc](http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)\n\n> For advanced assignments, there is in general no guarantee for the iteration order. This means that if an element is set more than once, it is not possible to predict the final result\n\nIn fact, as written, `scatter_ops_test.py` would fail right now when repeated indices exist.  For example,\n\n```\n$ n = np.array([[1, 2], [3, 4], [5, 6]])\n$ n[np.array([1, 1])] += np.array([[10, 10], [20, 20]])\n$ print(n)\n[[ 1  2]\n [23 24]\n [ 5  6]]\n```\n\nNow of course, I agree this should be supported, if only because it's already written into the contract.  I just want to point it out since I would have to make some changes to the test as well.\n", "@fpmchu, I apologize, I should have checked Thrust's documentation before sending you on a wild goose chase. But you are right, since we have committed to this in the documentation, it'll be nice to support repeated indices. My belief is, the performance may not be terrible if you turned the `+=` to `atomicAdd`. Do you mind giving it a shot and running the benchmarks?\n", "Ok, added atomic ops and range check.\n\nFor atomics, the benchmark code as is doesn't really run on GPU (not that I can tell).  I wrote a custom script to benchmark, and it shows very little difference in speed between the non-atomic and the atomic version, for the common case when there are no repeats.  I think this is to be expected as the atomics are lock free.  It might be slow for when there are repeat indices, but that really seems like a odd use case anyway...\n\nI had to add atomicAdd/atomicExch to support double, because CUDA does not support them directly.  The code is copied straight from CUDA's own manual.  I put them in `cuda_kernel_helper.h`.  Not sure if that's a good idea and/or it's the right place.\n\nFor range checks, I wrote a simple CUDA kernel for doing range checks, and also added tests.\n\nBtw, I'm not checking for CUDA errors, and it seems like I should.  I'm following examples from bias_ops though, and in there there's also no error checking... Any suggestions?\n", "fpmchu@, thanks for doing this, this is awesome! Almost there, we'll merge this soon.\n", "I fixed the `atomicAdd` ovelroad.  PTAL.\nHowever, I did consider it before, and I felt overloading might be slightly better because that forces a compilation error when one compiles against a new CUDA version.  This means the code will be forced to be fixed, instead of using an inefficient version.  That said, a custom wrapper isn't that bad so I did it anyway.\n", "As for `scatter_update`, sorry that I didn't check the doc on it.  No, the current version will not implement\n\n> If indices contains duplicate entries, lexicographically later entries override earlier entries.\n\nIt'll be quite tricky to do that.  Off the top of my head, I guess one can first sort then remove duplicates (or do a max).  It'll be inefficient and is tailored to a case that probably won't be used.  How about changing documentation?\n", "Two more questions\n1. In my implementation I'm using template specialization.  I could easily use function overload.  They behave the same way I think, so not sure what's preferred.\n2. Any comment on error checking?\n", "- Changing the documentation for `scatter_update` to note that one of the values corresponding to the duplicate indices will be written at that index.\n- The compiler looks for function overloads before it looks for full specializations, so I usually write overloads. But in this case, both function overload and full specialization should behave the same, so I am leaving it up to you.\n", "Done and done.\nAfter talking to @martinwicke , he says just to change doc in state_ops.cc.  Tests pass locally.  Anything else?\n", "Jenkins, test this please.\n", "Added a note in RELEASE.md.  PTAL.\n", "Can you smash all of these commits into one large commit?\n", "Done.\n", "Thanks, looking through this now.\n", "Can you do some merging of the registration macros in `scatter_op.cc`?  Certainly add and sub always have the same set of types, and for gpu it looks like everything has the same set of types.  Taking advantage of this would dramatically shorten that section and make it easier to read and modify.  For example, there would be only one `#if GOOGLE_CUDA` line in `scatter_op.cc`.\n", "I'm likely being a bit dense here, but what file defines the GPU implementations?  I can't find anything in your new single CL.\n\nOnce I do find it: we may need to be more specific about what happens for `ScatterUpdate` on larger types.  Currently you guarantee that one of the indices is used (which admittedly is what I said to do), but that's not remotely actually guaranteed for either indices which refer to slices or even to `complex128` on CPU if we were to switch to threads.\n", "Also, not sure if `complex64` is an issue yet, but once it is we'll want a primitive which is slightly weaker than atomic add / sub, since it's fine to add both components at unrelated points in the sequence.\n", "Sorry I missed some files during the squash.  Re-pushed now including the kernel.\n", "Hi, I made several changes based on comments.  PTAL.\n", "Done.  I also rebased to master to resolve some conflicts.\n", "@vrv, @martinwicke, @keveman, @girving, the GPU portion of the code LGTM. \n\nPlease feel free to move forward if you think other parts are good as well.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Can one of the admins verify this patch?\n", "I have fixed the test, and rebased + squashed.  Please test again\n", "Jenkins, test this please\n", "Hold on, this may be a tool failure.\n", "Jenkins, test this please.\n", "After talking with @martinwicke, we've decided to temporarily disabled the GPU range check test because gpu_pip test does not run on a GPU and is failing.\n\nSquashed and rebased.  PTAL.\n", "Jenkins, test this please.\n"]}, {"number": 781, "title": "rotate an image for data augmentation", "body": " I would like to rotate an image from a random angle, for data augmentation. But I don't find this transformation in the tf.image module.\n\nsee: http://stackoverflow.com/questions/34801342/tensorflow-how-to-rotate-an-image-for-data-augmentation\n", "comments": ["I will extend this request to more transformation like https://github.com/BVLC/caffe/pull/2252\n", "/cc @ducha-aiki if interested to follow this.\n", "@bhack thanks! Actually, I could think about implementing it...but need to get time for at least installing TF.\n", "@ducha-aiki are you going to implement it? I'm interested to contribute this functionality. \n\nShould I implement it first then send a pull request? Or is someone going to be assigned this. Sorry for the questions. I'm new to how contributions work.\n", "@jkschin not now, so you can try first :)\n\n> Should I implement it first then send a pull request? \n\nYes, usually it is done like this.\n", "Alright. What I intend to do would be to first implement some form of smart padding, so not much information is lost. If we look at python PIL, when we rotate the image, some parts are left black. We just have to pad the image by a scale of sqrt(2), do a rotation, find the center and extract the same sized square.\n\nThis is how I intend to do it, suggestions and improvements are welcome!\n\nEDIT: Or should I just implement a no-frills rotate first? Parts will be blacked out.\n", "@jkschin There is a little discussion about it in https://github.com/BVLC/caffe/pull/2252\nThe following ones are implemented in different libraries:\n- pad with zeros\n- pad with random noise\n- pad with mean pixel (nice for images)\n- pad with border mirroring.\n", "@ducha-aiki thanks!\n\nAnother question. Is it practice to assume a square image? Or do we have to factor in rectangular use cases as well?\n", "@jkschin TensorFlow is very flexible instrument, so rectangular as well.\n", "Probably we'll need also GPU ops version at some point like https://github.com/tensorflow/tensorflow/pull/588\n", "What about the case when we get, for example, an 256x256 image and we want to extract a random rotated 224x224 crop from WITHIN the image? In this case we don't need padding. Should we have in mind this use case?\n\nOff course, not all rotation angles will be available depending on the original images size and on the target rotated crop size, and also depending where the center of the crop was randomly chosen.\n", "It would be reasonable to allow to specify the size of rotated image, extracted from the original one, and throw an error if the some parts of it lie outside the original image. Additionally a user can specify the default pixel value to fill in the outside areas. In this case an error is not thrown. I had [an implementation](https://github.com/sdemyanov/ConvNet/blob/master/c%2B%2B/cuda/sources/cuda_util.cu) of a Transform function, that performed all flipping, translation, rotation and scaling in a single function. Maybe some code from there still can be useful.\n", "Throwing an error (if part of the new image is outside the original image) is not acceptable for my workflow.\n", "Was this ever implemented?\n", "I haven't written C++ code in a few years but I threw this together because this is the op I need, it should do the trick. I might throw together a cuda kernel at some point if I have the time.\n\n``` c++\n#define EIGEN_USE_THREADS\n\n#include <algorithm>\n#include <memory>\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/bounds_check.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/platform/logging.h\"\n\n\nnamespace tensorflow {\n\nREGISTER_OP(\"CropRotatedRectangleOp\")\n    .Input(\"image: T\")\n    .Input(\"size: int32\")\n    .Input(\"center: float\")\n    .Input(\"angle: float\")\n    .Attr(\"T: {uint8, int8, int16, int32, int64, half, float, double}\")\n    .Output(\"rotated_image: float\")\n    .Doc(R\"doc(\nCrops a rotated rectangle from an image.\n\nimage: 3-D with shape `[height, width, channels]`.\nsize: A 1-D int32 Tensor of 2 elements: `new_height, new_width`. The\n  size of the rectangles to be cropped.\ncenter: A 1-D float Tensor of 2 elements: `center_x, center_y`. The center\n  of the rectangles to be cropped.\nangle: A 0-D float Tensor representing the angle that the rectangles are\n  rotated.\nrotated_image: 3-D with shape `[new_height, new_width, channels]`.\n)doc\");\n\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename Device, typename T>\nclass CropRotatedRectangleOp : public OpKernel {\n public:\n  explicit CropRotatedRectangleOp(OpKernelConstruction* context) :\n      OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input_tensor = context->input(0);\n    OP_REQUIRES(context, input_tensor.dims() == 3,\n                errors::InvalidArgument(\"image must be 3-dimensional\",\n                                        input_tensor.shape().DebugString()));\n\n    const Tensor& shape_t = context->input(1);\n    OP_REQUIRES(context, shape_t.dims() == 1,\n                errors::InvalidArgument(\"shape_t must be 1-dimensional\",\n                                        shape_t.shape().DebugString()));\n    OP_REQUIRES(context, shape_t.NumElements() == 2,\n                errors::InvalidArgument(\"shape_t must have two elements\",\n                                        shape_t.shape().DebugString()));\n\n    const Tensor& center_t = context->input(2);\n    OP_REQUIRES(context, center_t.dims() == 1,\n                errors::InvalidArgument(\"center_t must be 1-dimensional\",\n                                        center_t.shape().DebugString()));\n    OP_REQUIRES(context, center_t.NumElements() == 2,\n                errors::InvalidArgument(\"center_t must have two elements\",\n                                        center_t.shape().DebugString()));\n\n    const Tensor& angle_t = context->input(3);\n\n    auto Svec = shape_t.vec<int32>();\n    const int64 out_height = internal::SubtleMustCopy(Svec(0));\n    const int64 out_width = internal::SubtleMustCopy(Svec(1));\n    const int64 channels = input_tensor.dim_size(2);\n    OP_REQUIRES(context, out_height > 0 && out_width > 0,\n                errors::InvalidArgument(\"output dimensions must be positive\"));\n    OP_REQUIRES(context, channels > 0,\n                errors::InvalidArgument(\"image must have at least 1 channel\"));\n    OP_REQUIRES(context,\n                input_tensor.dim_size(0) > 0 && input_tensor.dim_size(1) > 0,\n                errors::InvalidArgument(\"input must be of non-zero size\"));\n\n    typename TTypes<T, 3>::ConstTensor input_data = input_tensor.tensor<T, 3>();\n    const int64 in_height = static_cast<int32>(input_tensor.dim_size(0));\n    const int64 in_width = static_cast<int32>(input_tensor.dim_size(1));\n\n    const float angle = angle_t.scalar<float>()();\n    const float cosa = cos(angle);\n    const float sina = sin(angle);\n\n    auto Cvec = center_t.vec<float>();\n    const float center_x = Cvec(0);\n    const float center_y = Cvec(1);\n    const float offset_x =\n      center_x - (out_width * cosa - out_height * sina) / 2.0f;\n    const float offset_y =\n      center_y - (out_width * sina + out_height * cosa) / 2.0f;\n\n    Tensor* output_tensor = NULL;\n    OP_REQUIRES_OK(context, context->allocate_output(0,\n      TensorShape({out_height, out_width, channels}), &output_tensor));\n    typename TTypes<float, 3>::Tensor output_data =\n      output_tensor->tensor<float, 3>();\n\n    for (int64 y = 0; y < out_height; ++y) {\n      float ysina = y * sina;\n      float ycosa = y * cosa;\n\n      for (int64 x = 0; x < out_width; ++x) {\n        float src_x = x * cosa - ysina + offset_x;\n        float src_y = x * sina + ycosa + offset_y;\n\n        if (src_x >= 0.0f && src_x < in_width-1 &&\n            src_y >= 0.0f && src_y < in_height-1) {\n          int64 src_x0 = floor(src_x);\n          int64 src_x1 = src_x0 + 1;\n          int64 src_y0 = floor(src_y);\n          int64 src_y1 = src_y0 + 1;\n\n          float sx = src_x - src_x0;\n          float sy = src_y - src_y0;\n          float s00 = (1.0f-sx) * (1.0f-sy);\n          float s10 = sx * (1.0f-sy);\n          float s01 = (1.0f-sx) * sy;\n          float s11 = sx * sy;\n\n          for (int64 c = 0; c < channels; ++c) {\n            output_data(y, x, c) =\n              s00 * float(input_data(src_y0, src_x0, c)) +\n              s10 * float(input_data(src_y0, src_x1, c)) +\n              s01 * float(input_data(src_y1, src_x0, c)) +\n              s11 * float(input_data(src_y1, src_x1, c));\n          }\n        } else {\n          for (int64 c = 0; c < channels; ++c) {\n            output_data(y, x, c) = 0.0f;\n          }\n        }\n      }\n    }\n  }\n};\n\n#define REGISTER_KERNEL(T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"CropRotatedRectangle\")\\\n                              .Device(DEVICE_CPU)     \\\n                              .TypeConstraint<T>(\"T\") \\\n                              .HostMemory(\"size\"),    \\\n                          CropRotatedRectangleOp<CPUDevice, T>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNEL);\n\n#undef REGISTER_KERNEL\n\n}  // namespace tensorflow\n```\n", "When I try to use this by defining it as a user op I get:\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: No OpKernel was registered to support Op 'CropRotatedRectangleOp' with these attrs\n     [[Node: CropRotatedRectangleOp = CropRotatedRectangleOp[T=DT_FLOAT](CropRotatedRectangleOp/image, CropRotatedRectangleOp/size, CropRotatedRectangleOp/center, CropRotatedRectangleOp/angle)]]\n```\n", "It would be cool to have full set of affine transforms available: http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.AffineTransform.\n", "This is my implementation. Images and Targets should be in NCHW layout, shift_mat, scale_mat and mirror_mat are (N, 2) arrays with the transformation parameters for each image, angle_mat is (N, 1) array, defval is the value to fill when the source pixel is outside the original image.\n\n```\n#define THREADS_PER_DIM_1                  16\n#define THREADS_PER_DIM_2                  16\n#define IMAGES_PER_THREAD                  32\n#define CHANNELS_PER_THREAD                32\n\n__global__\nvoid kTransform(float* imgs, float* targets,\n  int imgSize1, int imgSize2, int trgSize1, int trgSize2,\n  int numChannels, int numImages, int channelBlocks,\n  float *shift_mat, float *scale_mat, float *mirror_mat, float *angle_mat, float defval) {\n\n  const int targetIdx1 = blockIdx.x * blockDim.x + threadIdx.x;\n  const int targetIdx2 = blockIdx.y * blockDim.y + threadIdx.y;\n  const int channelBlockIdx = blockIdx.z % channelBlocks;\n  const int imageBlockIdx = blockIdx.z / channelBlocks;\n  // HW layout\n  const int targetIdx = targetIdx1 * trgSize2 + targetIdx2;\n\n  if (targetIdx1 >= trgSize1 || targetIdx2 >= trgSize2) {\n    return;\n  }\n\n  const int imgPixels = imgSize1 * imgSize2;\n  const int trgPixels = trgSize1 * trgSize2;\n\n  float buffer[IMAGES_PER_THREAD][CHANNELS_PER_THREAD];\n\n  const float imgHalf1 = (float) imgSize1 / 2 - 0.5;\n  const float imgHalf2 = (float) imgSize2 / 2 - 0.5;\n  const float trgHalf1 = (float) trgSize1 / 2 - 0.5;\n  const float trgHalf2 = (float) trgSize2 / 2 - 0.5;\n\n  // #pragma unroll\n  const int first_im = imageBlockIdx * IMAGES_PER_THREAD;\n  const int last_im = MIN(first_im + IMAGES_PER_THREAD, numImages);\n  const int first_ch = channelBlockIdx * CHANNELS_PER_THREAD;\n  const int last_ch = MIN(first_ch + CHANNELS_PER_THREAD, numChannels);\n  for (int i = first_im; i < last_im; ++i) {\n    // indices for (n, 2) param arrays\n    const int i0 = 2*i, i1 = 2*i+1;\n    const float angcos = (float) cos(angle_mat[i]);\n    const float angsin = (float) sin(angle_mat[i]);\n    const float xi1 = (targetIdx1 - trgHalf1) * scale_mat[i0]; // scale[0];\n    const float xi2 = (targetIdx2 - trgHalf2) * scale_mat[i1]; //scale[1];\n    float x1 = xi1 * angcos - xi2 * angsin + imgHalf1 + shift_mat[i0]; //shift[0];\n    float x2 = xi1 * angsin + xi2 * angcos + imgHalf2 + shift_mat[i1]; //shift[1];\n    if (mirror_mat[i0] > 0.5) x1 = imgSize1 - 1 - x1;\n    if (mirror_mat[i1] > 0.5) x2 = imgSize2 - 1 - x2;\n    if (0 <= x1 && x1 <= imgSize1 - 1 &&\n        0 <= x2 && x2 <= imgSize2 - 1) {\n      const int xu1 = (int) (x1 + 0.5);\n      const int xu2 = (int) (x2 + 0.5);\n      const int xp1 = MIN(xu1 + 1, imgSize1 - 1);\n      const int xp2 = MIN(xu2 + 1, imgSize2 - 1);\n      // HW layout\n      const int imgPx11 = xu1 * imgSize2 + xu2;\n      const int imgPx21 = xu1 * imgSize2 + xp2;\n      const int imgPx12 = xp1 * imgSize2 + xu2;\n      const int imgPx22 = xp1 * imgSize2 + xp2;\n      for (int c = first_ch; c < last_ch; ++c) {\n        // NCHW or NCWH layout\n        const int cur_shift = (i * numChannels + c) * imgPixels;\n        const int imgInd11 = cur_shift + imgPx11;\n        const int imgInd21 = cur_shift + imgPx21;\n        const int imgInd12 = cur_shift + imgPx12;\n        const int imgInd22 = cur_shift + imgPx22;\n        const float vl = (x1 - (float) xu1) * imgs[imgInd21] + ((float) xu1 + 1 - x1) * imgs[imgInd11];\n        const float vh = (x1 - (float) xu1) * imgs[imgInd22] + ((float) xu1 + 1 - x1) * imgs[imgInd12];\n        buffer[i-first_im][c-first_ch] = (x2 - (float) xu2) * vh + ((float) xu2 + 1 - x2) * vl;\n      }\n    } else {\n      for (int c = 0; c < numChannels; ++c) {\n        buffer[i-first_im][c-first_ch] = defval;\n      }\n    }\n  }\n\n  // shift to the current pixel\n  targets += targetIdx;\n   #pragma unroll\n  for (int i = first_im; i < last_im; ++i) {\n     #pragma unroll\n    for (int c = first_ch; c < last_ch; ++c) {\n      targets[(i * numChannels + c) * trgPixels] = buffer[i-first_im][c-first_ch];\n    }\n  }\n}\n\nvoid _transformActs(const MatGPU &images, MatGPU &targets,\n                    int imgSize1, int imgSize2,\n                    int trgSize1, int trgSize2,\n                    const MatGPU &shift_mat, const MatGPU &scale_mat,\n                    const MatGPU &mirror_mat, const MatGPU &angle_mat, float defval) {\n\n\n  int numImages = images.size1_;\n\n  int imgPixels = imgSize1 * imgSize2;\n  mexAssert(images.size2_ % imgPixels == 0, \"ta2\");\n  int numChannels = images.size2_ / imgPixels;\n\n  mexAssert(targets.size1_ == numImages, \"ta1\");\n  mexAssert(targets.size2_ == trgSize1 * trgSize2 * numChannels, \"ta3\");\n\n  cudaStream_t stream = MatGPU::_defaultStream;\n\n  dim3 threads(THREADS_PER_DIM_1, THREADS_PER_DIM_2);\n\n  int imageBlocks = DIVUP(numImages, IMAGES_PER_THREAD);\n  int channelBlocks = DIVUP(numChannels, CHANNELS_PER_THREAD);\n  dim3 blocks(DIVUP(trgSize1, THREADS_PER_DIM_1),\n              DIVUP(trgSize2, THREADS_PER_DIM_2),\n              imageBlocks * channelBlocks);\n\n  kTransform<<<blocks, threads, 0, stream>>>\n    (images.data_, targets.data_, imgSize1, imgSize2, trgSize1, trgSize2,\n     numChannels, numImages, channelBlocks,\n     shift_mat.data_, scale_mat.data_, mirror_mat.data_, angle_mat.data_, defval);\n\n  mexAssert(cudaGetLastError() == cudaSuccess, \"_transformActs: kernel execution failed\");\n}\n```\n", "While it would be useful to have mirrored images in the null areas for classification tasks, it is counterproductive for pose estimation tasks. It would be nice to have multiple options.\n", "@ringw let me know if you need any assistance on this one.\n", "@kofd Thanks! I'm adding same-size output/fill with 0s which is enough for my personal needs. Let me know if you want to add certain features once that's in.\n", "In addition to rotations I'm also interested in applying generalized affine and projective transformations to images.\n\nStackoverflow also has [another implementation of rotation as a tensorflow graph written using existing ops in python](http://stackoverflow.com/a/40483687/99379).\n", "I think chaining so many ops will be prohibitively slow; it might be faster to just use py_func and scipy, etc.\n\nI was adding an ImageAffineTransform op to implement rotations, but I'll add extra parameters to support general projective transforms. I'll add a tf.contrib.image.rotate function, and probably also expose a tf.contrib.image.projective_transform function where you can pass in the parameters for any transform.\n\nThat seems to cover most of the cases in the linked BVLC/caffe#2252, once we support different size/padding modes for the output. If it's not possible to implement the noise policies described there with tf.random_\\* plus a couple of extra ops, then we might want to add noise ops too in tf.contrib.image.\n", "@ringw Sounds fantastic! Not trying to pile on the requests, but any chance of being able to specify separate input and output tensors with different dimensions for this as well? (or a crop that will work seamlessly with whatever transform parameterization you choose)\n", "@ahundt I created a bilinear implementation of the stack overflow approach and its about 20 times slower than PIL on a cpu machine. It might still be worth using for gpu though.\r\n\r\nedit: got it down to 10 times slower", "@kofd it would be an intermediate runtime step for me so avoiding a round trip out of the gpu may still be worthwhile. Were you thinking of putting it up somewhere?", "```python\r\ndef rotate(image, angle):\r\n    with tf.name_scope('rotate'):\r\n        image = tf.cast(image, tf.float32)\r\n        angle = angle / 180 * math.pi\r\n        shape = image.get_shape().as_list()\r\n        assert len(shape) == 3, \"Input needs to be 3D.\"\r\n        image_center = np.array([x/2 for x in shape][:-1])\r\n\r\n        coord1 = tf.cast(tf.range(shape[0]), tf.float32)\r\n        coord2 = tf.cast(tf.range(shape[1]), tf.float32)\r\n\r\n        # Create vectors of those coordinates in order to vectorize the image\r\n        coord1_vec = tf.tile(coord1, [shape[1]])\r\n\r\n        coord2_vec_unordered = tf.tile(coord2, [shape[0]])\r\n        coord2_vec_unordered = tf.reshape(coord2_vec_unordered, [shape[0], shape[1]])\r\n        coord2_vec = tf.reshape(tf.transpose(coord2_vec_unordered, [1, 0]), [-1])\r\n\r\n        # center coordinates since rotation center is supposed to be in the image center\r\n        coord1_vec_centered = coord1_vec - image_center[0]\r\n        coord2_vec_centered = coord2_vec - image_center[1]\r\n\r\n        coord_new_centered = tf.cast(tf.pack([coord1_vec_centered, coord2_vec_centered]), tf.float32)\r\n\r\n        # Perform backward transformation of the image coordinates\r\n        rot_mat_inv = tf.expand_dims(tf.pack([tf.cos(angle), tf.sin(angle), -tf.sin(angle), tf.cos(angle)]), 0)\r\n        rot_mat_inv = tf.cast(tf.reshape(rot_mat_inv, shape=[2, 2]), tf.float32)\r\n        coord_old_centered = tf.matmul(rot_mat_inv, coord_new_centered)\r\n\r\n        # Find neighbors in old image\r\n        coord1_old_nn = coord_old_centered[0, :] + image_center[0]\r\n        coord2_old_nn = coord_old_centered[1, :] + image_center[1]\r\n\r\n        # Clip values to stay inside image coordinates\r\n        outside_ind1 = tf.logical_or(tf.greater(coord1_old_nn, shape[0]-1), tf.less(coord1_old_nn, 0))\r\n        outside_ind2 = tf.logical_or(tf.greater(coord2_old_nn, shape[1]-1), tf.less(coord2_old_nn, 0))\r\n        outside_ind = tf.logical_or(outside_ind1, outside_ind2)\r\n\r\n        coord1_vec = tf.boolean_mask(coord1_vec, tf.logical_not(outside_ind))\r\n        coord2_vec = tf.boolean_mask(coord2_vec, tf.logical_not(outside_ind))\r\n\r\n        # Coordinates of the new image\r\n        coord_new = tf.transpose(tf.cast(tf.pack([coord1_vec, coord2_vec]), tf.int32), [1, 0])\r\n\r\n        coord1_old_nn0 = tf.floor(coord1_old_nn)\r\n        coord2_old_nn0 = tf.floor(coord2_old_nn)\r\n        sx = coord1_old_nn - coord1_old_nn0\r\n        sy = coord2_old_nn - coord2_old_nn0\r\n        coord1_old_nn0 = tf.cast(coord1_old_nn0, tf.int32)\r\n        coord2_old_nn0 = tf.cast(coord2_old_nn0, tf.int32)\r\n        coord1_old_nn0 = tf.boolean_mask(coord1_old_nn0, tf.logical_not(outside_ind))\r\n        coord2_old_nn0 = tf.boolean_mask(coord2_old_nn0, tf.logical_not(outside_ind))\r\n        coord1_old_nn1 = coord1_old_nn0 + 1\r\n        coord2_old_nn1 = coord2_old_nn0 + 1\r\n        interp_coords = [\r\n            ((1.-sx) * (1.-sy), coord1_old_nn0, coord2_old_nn0),\r\n            (    sx  * (1.-sy), coord1_old_nn1, coord2_old_nn0),\r\n            ((1.-sx) *     sy,  coord1_old_nn0, coord2_old_nn1),\r\n            (    sx  *     sy,  coord1_old_nn1, coord2_old_nn1)\r\n        ]\r\n\r\n        interp_old = []\r\n        for intensity, coord1, coord2 in interp_coords:\r\n            intensity = tf.transpose(tf.reshape(intensity, [shape[1], shape[0]]))\r\n            coord_old_clipped = tf.transpose(tf.pack([coord1, coord2]), [1, 0])\r\n            interp_old.append((intensity, coord_old_clipped))\r\n\r\n        channels = tf.split(2, shape[2], image)\r\n        image_rotated_channel_list = list()\r\n        for channel in channels:\r\n            channel = tf.squeeze(channel)\r\n            interp_intensities = []\r\n            for intensity, coord_old_clipped in interp_old:\r\n                image_chan_new_values = tf.gather_nd(channel, coord_old_clipped)\r\n\r\n                channel_values = tf.sparse_to_dense(coord_new, [shape[0], shape[1]], image_chan_new_values,\r\n                                                    0, validate_indices=False)\r\n\r\n                interp_intensities.append(channel_values * intensity)\r\n            image_rotated_channel_list.append(tf.add_n(interp_intensities))\r\n\r\n        image_rotated = tf.transpose(tf.pack(image_rotated_channel_list), [1, 2, 0])\r\n\r\n        return image_rotated\r\n```", "If anyone is OK with this performance, I also had [an implementation](https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/models/image_sample.py#L45) of bilinear sampling similar to @kofd but works for batch of images.  I used it to learn parameterized rotation/translation.", "Sorry for the delay! I'm trying to get a basic implementation in as soon as\nI can, but I'm hunting down a bizarre test failure right now. I can\ndefinitely add efficient bilinear interpolation as part of the C++ op. The\nunderlying op will take an input tensor (a batch of images), a\ntransformation matrix, and the size of the output, so it should be possible\nto get whichever output mode you need.\n\nOn Thu, Dec 1, 2016 at 5:28 PM Yuxin Wu <notifications@github.com> wrote:\n\nIf anyone is OK with this performance, I also had an implementation\n<https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/models/image_sample.py#L45>\nof bilinear sampling similar to @kofd <https://github.com/kofd> but works\nfor batch of images, which I used to learn parameterized\nrotation/translation.\n\n\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/781#issuecomment-264315262>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ABU8HJuFUrKH6v9S1lpoBjpy_k0ovOqFks5rD0oXgaJpZM4HFgkQ>\n.\n", "@ringw cool, 4x4 which allows for translations as well? Probably shouldn't matter because all the channels would be modified in the same way, but will different channel depths (1,3,4...) be ok too? ", "I think a 3x3 projective matrix will allow for translation (x, y translation are in the third column). The image tensor will always have to be 4D to disambiguate between multiple batches or multiple channels, but you can have 1 or more channels and the transformation will be applied to just the x, y coordinates, not the channels (the channels in each image have the same transform applied to them).\r\n\r\nHere's the documentation for the new op (it will support interpolation and custom output sizes in a follow-up):\r\n\r\n> Applies the given transform to each of the images.\r\n>\r\n> Input `image` is a `Tensor` of rank 4, where the axes are image number, rows,\r\ncolumns, and channels. Input `transforms` is a num_images x 8 or 1 x 8 matrix,\r\nwhere each row corresponds to a 3 x 3 projective transformation matrix, with the\r\nlast entry assumed to be 1. If there is one row, the same transformation will be\r\napplied to all images.\r\n>\r\n> If the passed-in transform is `(a0, a1, a2, b0, b1, b2, c0, c1)`, then it maps\r\nthe *output* point `(x, y)` to a transformed *input* point\r\n`(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`, where\r\n`k = c0 x + c1 y + 1`. If the transformed point lays outside of the input\r\nimage, the output pixel is set to 0.\r\n>\r\n> images: 4D `Tensor`, input image(s).\r\n>\r\n> transforms: 2D `Tensor`, projective transform(s) to apply to the image(s).\r\n\r\ntf.contrib.image.rotate will be a convenience wrapper that takes a 1D tensor of angles, and passes the corresponding rotation matrices to the op.", "ah yes, silly me, a 2d image can be projected with a 3x3 matrix. Are the parameters there supposed to be in tuple form? If so, could you explain the reason for that vs other structures?  Thanks!", "Oh, I was just referring to one row in the transforms tensor there. It will be an array of length 8, or n x 8 if using batches. I'll update the documentation to make it clear that it's not meant to represent a Python tuple.", "I'm following this thread and waiting anxiously for the feature to find its way into the next release of tensorflow... ", "The first version should be going into master later today, if you feel like building from source!", "@ringw The rotation op works great. Any chance to add interpolation?", "@aaronpries as per commit message \"Mirror and interpolation will be added in follow-ups.\"", "Sorry for the delay! I'll get back to adding bilinear interpolation soon.", "It would also be cool to be able to get the rotation matrices created by `tf.contrib.image.rotate` without actually preforming the transformation. That way along the lines of [`skimage.transform.AffineTransform`](https://github.com/tensorflow/tensorflow/issues/781#issuecomment-243181271) these affine transformations can be chained together to get a new transformation matric.", "@cancan101 I've added `angles_to_projective_transforms` and `compose_transforms` functions, let me know if that covers what you're thinking of!", "Yes, exactly! Does it make sense to open a new ticket to track the feature request to addba resize method and output size parameter, or keep using this thread? ", "Probably better to keep image rotation and transformation in this ticket. There are already tf.image.resize_* methods that will probably stay unchanged. We might want to keep the existing ops for all of those, but we can also have a helper to create a transform for resizing.", "Sorry, to be clear, I was referring to your comment above :\r\n>it will support interpolation and custom output sizes in a follow-up", "Sure! Yeah I think it still makes sense to track those in this issue. I'll\nprobably look into interpolation next week.\n\nOn Mar 24, 2017 2:32 PM, \"Alex Rothberg\" <notifications@github.com> wrote:\n\n> Sorry, to be clear, I was referring to your comment above :\n>\n> it will support interpolation and custom output sizes in a follow-up\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/781#issuecomment-289107946>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABU8HEC_Tv6wYXedkkLAHxm74q10epfWks5rpAwhgaJpZM4HFgkQ>\n> .\n>\n", "Is there a way to randomly rotate images, a `random_rotation` similar to `random_saturation` for augmentation purposes? I've looked and looked and can't seem to find anything for such an obvious use case.", "I would say to just chain it, something like this:\r\n\r\n    tf.contrib.image.rotate(images, tf.random_uniform([num_images], maxval=2*math.pi))\r\n\r\nBut since we already have `random_saturation`, probably best if I add a `random_rotation` too for consistency.", "@ringw cool looks like this in, I assume that currently the output dimensions are fixed?", "Yeah, the output is currently the same size as the input. I'll add an output_size arg, and then add an \"expand to fit\" option for rotate--are there any other cases that would be useful for rotation?", "Only thing I can think of is an arbitrary channel count, not sure if it is already there or not. For example, that would be useful for transforming a one hot encoding when doing image segmentation. Thanks for the awesome work!", "It already does accept arbitrary (num_images, height, width, num_channels)\nshaped tensors!\n\nOn Jun 7, 2017 2:04 AM, \"Andrew Hundt\" <notifications@github.com> wrote:\n\nOnly thing I can think of is an arbitrary channel count, not sure if it is\nalready there or not. For example, that would be useful for transforming a\none hot encoding when doing image segmentation.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/781#issuecomment-306647933>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ABU8HIJQVjEpLDcq6uPHLUzK65GLly-Aks5sBekXgaJpZM4HFgkQ>\n.\n", "@ringw I wrote `translations_to_projective_transforms` which acts just as your `angles_to_projective_transforms` but for translations. Also `scales_to_projective_transforms` for scaling. I'll write `translate` and `scale` wrappers and send in a PR later today.", "@sampepose Link to PR?", "@sampepose, I don't see scales_to_projective_transforms in https://www.tensorflow.org/api_docs/python/tf/contrib/image/rotate yet, has it not landed? ", "I think it might be https://github.com/tensorflow/tensorflow/pull/12306, which is still open", "@astromme @ahundt #12306 is translation, but I don't think @sampepose created a PR for scales_to_projective_transforms", "Any news on scales_to_projective_transforms?\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "I will write scales_to_projective_transforms, but otherwise I'm not planning on making any further improvements any time soon. Please open specific new issues if there is any other image support that you would like.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ringw I've been looking at [image_ops in tf.contrib.image](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/image/python/ops/image_ops.py) in tf 1.5 and it looks amazing! Thanks! I'm giving it a try now. :+1: \r\n\r\nWhat's definition of identity here in terms of `a0, a1, ...`? I assume all 0 except `a0, b1, c2 == 1`?\r\n\r\nWhat do you think of a function `identity_projective_transform()`?\r\n\r\nIdentity makes setting up a function that uses these utilities a bit simpler to define and less error prone, since the actual transform function requires at least one transform to be defined as input. Of course `rotate(0)`, etc is identity `identity_projective_transform()` would be just for clarity. :-)", "\r\n**Update:** No point doing the matrix below manually because I just found:\r\n```python\r\n    tf.contrib.image._flat_transforms_to_matrices()\r\n    tf.contrib.image._transform_matrices_to_flat()\r\n```\r\nNot public, but they can be copied locally if needed.\r\n\r\n**previous question:**\r\nAlso is the following correct if I want to define an actual coordinate index in the input image, apply the transform, and get the same coordinate in the output image?\r\n\r\n```python\r\n\r\n        # transform_matrix is an\r\n        # 8 element projective transform matrix\r\n        t = transform_matrix\r\n        projection = tf.convert_to_tensor(\r\n            [[t[0], t[1], t[2]],\r\n             [t[3], t[4], t[5]],\r\n             [t[6], t[7], 1])\r\n        coordinate = tf.transpose(tf.convert_to_tensor(\r\n            coordinate[0],\r\n            coordinate[1],\r\n            1\r\n        ))\r\n        output_coordinate = projection * coordinate\r\n        output_coordinate = coordinate[:2]\r\n```", "Thanks Andrew!\r\n\r\nYes, the identity in our representation is [1, 0, 0, 0, 1, 0, 0, 0] which corresponds to the 3x3 identity matrix. Makes sense to add an `identity_projective_transform` helper!\r\n\r\nI think we can make `_flat_transforms_to_matrices`, etc, public too. The catch is that the matrix transforms output coordinates to input coordinates, because at each output pixel, we compute the input pixel to read from. So you would probably need to call `tf.linalg.inv` on the `projection` matrix.\r\n\r\nI think tf.transpose doesn't do anything to 1D tensors, and `*` does elementwise multiplication, so I would recommend:\r\n\r\n```\r\n        # transform_matrix is an\r\n        # 8 element projective transform matrix\r\n        t = transform_matrix\r\n        # Always creates batches of matrices\r\n        reverse_transform = tf.contrib.image.flat_transforms_to_matrices(t)[0, :, :]\r\n        forward_transform = tf.linalg.inv(reverse_transform)\r\n        coordinate = tf.transpose([[coordinate[0], coordinate[1], 1]])\r\n        output_coordinate = tf.matmul(forward_transform, coordinate)\r\n        output_coordinate = coordinate[:2, 0]\r\n```", "Closing since this issue appears to be resolved with tf.contrib.rotate() and the matrix conversion routines.  Thanks @ringw.", "After removal of `tf.contrib` on TF2.0, this issue will rise again.\r\nWhat is the recommended way for random rotation on TF2.0?\r\n`tf.keras.preprocessing.image.random_rotation` doesn't work under `@tf.function` without users' effort.\r\nI want TF to contain native `rotate()` or `random_rotate()` under `tf.image`.", "Image transformations have moved to tensorflow_addons (https://github.com/tensorflow/addons/tree/master/tensorflow_addons/image).", "> I want TF to contain native rotate() or random_rotate() under tf.image.\r\n\r\nI agree. It doesn't seem right to me that something so basic as being able to rotate an image isn't included natively within TensorFlow. ", "This should be added natively to tf 2.0.", "Just a reference note for those that come across this, be sure to check if this is fixed or if it matters for your application:\r\nhttps://hackernoon.com/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35"]}, {"number": 780, "title": "Potential to add tf.image.rotate?", "body": "I experimented with the CIFAR10 example. I noticed that they have very good transformation functions. For example, flipping up down and left right. Would it be good if we add a rotate function? For example:\n\ntf.image.rotate(image,angle)\n\nwhere angle is the degrees to rotate by in the anti-clockwise direction. \n", "comments": ["See also https://github.com/tensorflow/tensorflow/issues/781\n", "I'm going to close this in favor of #781\n"]}, {"number": 779, "title": "Use augmented assignment statements and fix typos in mnist/word2vec python code", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins, test this please.\n", "I guess the failure of `MacOS CPU Tests` is irrelevant to this PR since the master branch doesn't pass the same test. I'm wondering if the failure of `default` is meaningful.\n", "Yeah it's fine. I'll merge tomorrow\n", "I see. Thank you, @vrv .\n", "Merged\n"]}, {"number": 778, "title": "installation failed on redhat 5.7", "body": "I installed tf from source code on redhat 5.7(I know it is a very old version), when I try to import tensorflow, it gives: anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: ELF file OS ABI invalid\nHow can I solve that?\n", "comments": ["@maplewizard If you have root access, you may upgrade glib and be able to install bazel, then use it to compile tensorflow. If you don't have root access, as far as I know it's very difficult to install bazel and tensorflow on redhat 5 or redhat 6.\n", "@digitalsword yes. When I  am trying to install bazel, it said glib version missing, but I cannot compile glib sucessfully. Some strange error occur...\n", "@maplewizard  check your glibc version \n\nldd --version \nor\n/lib64/libc.so.6\nOn 32-bit Red Hat based systems:/lib/libc.so.6\n", "@bellaj The glibc version is too low. and I failed in compling it on the machine. I have no idea, maybe it cannot install on an old system.\n", "check your kernel version  uname -r\n", "It's also a very low version 2.6.32-220.23.2\n", "I am able to compile bazel using gcc 4.8 installed at custom path. https://github.com/bazelbuild/bazel/issues/760\n\nBut I am still not able to compile tensorflow with the bazel binary on a redhat 6.7 system.\n\n```\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /nfs/02/h212/.cache/bazel/_bazel_h212/7a2b15405f00fa358f0e0720e30182fe/external/gemmlowp/BUILD:77:1: C++ compilation of rule '@gemmlowp//:eight_bit_int_gemm' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 31 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ngcc: unrecognized option '-no-canonical-prefixes'\ncc1plus: error: unrecognized command line option \"-std=c++11\"\ncc1plus: error: unrecognized command line option \"-fno-canonical-system-headers\"\ncc1plus: warning: unrecognized command line option \"-Wno-free-nonheap-object\"\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 1.992s, Critical Path: 1.13s\n```\n\n```\n-bash-4.1$ ldd bazel_head_latest/output/bazel\n    linux-vdso.so.1 =>  (0x00007fffd6386000)\n    librt.so.1 => /lib64/librt.so.1 (0x00000032c2e00000)\n    libz.so.1 => /lib64/libz.so.1 (0x00000032c2a00000)\n    libstdc++.so.6 => /usr/local/gcc/4.8.4/lib64/libstdc++.so.6 (0x00002b82e38cb000)\n    libgcc_s.so.1 => /usr/local/gcc/4.8.4/lib64/libgcc_s.so.1 (0x00002b82e3bd4000)\n    libc.so.6 => /lib64/libc.so.6 (0x00000032c1a00000)\n    libpthread.so.0 => /lib64/libpthread.so.0 (0x00000032c2200000)\n    /lib64/ld-linux-x86-64.so.2 (0x00000032c1600000)\n    libm.so.6 => /lib64/libm.so.6 (0x00000032c260\n```\n", "try the docker container \n", "In fact, docker require that the kernel is above 3.1.  So it's not possible to do that....\n", "so upgrad your kernel \n", "@bellaj Thanks for your advice. However, the system cannot be modified. I will try to find other methods.\n", "@maplewizard I am in the same situation, I do not have root access. I managed to build bazel binary using a new version (4.8) of gcc installed at custom path. But when I use the bazel binary to compile tensorflow, it does invoke the old gcc compiler from the old system instead of the newer gcc I installed.\n", "@digitalsword It seems no solution until now. I have no idea.\n", "if you have many gcc version you could use update-alternatives. \n", "@digitalsword: It sounds like this is a bazel issue rather than a TensorFlow issue if you can't get bazel to compile using the right gcc.  Sorry to redirect, but I don't think we'll be able to fix it on our end. \n"]}, {"number": 777, "title": "Added generate_checkpoint_state() which returns a checkpoint proto", "body": "with model_checkpoint_path containing absolute path or path relative\nto save_dir.\n\nFixes #751.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins, test this please.\n"]}, {"number": 776, "title": "assertion failed: status == cudaSuccess in convolutional.py", "body": "When trying to run convolutional.py on a Ubuntu 14.04 system with Titan, and CUDA Toolkit 7 and CUDNN 6.5, I get this error trying to run convolutional.py :  \n\n (tensorflow_env)sghosh@halaklin:~/anaconda/envs/tensorflow_env/lib/python2.7/site-packages/tensorflow$ python -m tensorflow.models.image.mnist.convolutional\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: \nname: GeForce GTX TITAN\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.8755\npciBusID 0000:02:00.0\nTotal memory: 6.00GiB\nFree memory: 5.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 4.86GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0x600200000 extends to 0x737401000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8\nassertion failed: status == cudaSuccess in function void Eigen::initializeDeviceProp() at third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceType.h:464\nAborted (core dumped)\n", "comments": ["Did anyone else have the same issue as I did ? Any suggestions ?\n", "This assertion comes from Eigen. Assigning Benoit to take a look. It is helpful if Eigen prints out the failed status. \n", "Hello @benoitsteiner  could you please take a look at this bug ? We've been blocked for quite sometime on this. Do you think migrating to a new release of Eigen could resolve the problem ?\n", "Eigen will not print error messages to stderr when it fails to initialize the CUDA runtime (see https://bitbucket.org/eigen/eigen/commits/8bfe1ed866fbd09954d65a644584ce5d1f741aa7 for details). If you pull the latest version of TensorFlow from github and return your model you should get some information to help pinpoint the root cause of the issue.\n", "I'm going to close this. Please reopen if the problem persists and more diagnostic information is available.\n"]}, {"number": 775, "title": "TensorBoard \"Split on underscores\" does not split on underscores", "body": "No output to the terminal. No nothing. Running Chrome on Ubuntu 14.04 at 669790ac.\n", "comments": ["Thanks for the report, I've pushed a fix.\n"]}, {"number": 774, "title": "Fix typos in a string and comments of python and C++ code in `models`", "body": "", "comments": ["Can one of the admins verify this patch?\n", "thank you, @vrv \n"]}, {"number": 773, "title": "How to get the gradients of activations?", "body": "In cifar10 example, the gradients of parameters can be got:\ngrads_and_vars = opt.compute_gradients(loss)\nfor grad, var in grads_and_vars:\n    # ...\n\nIs there any way to get the gradients of activations(not the parameters)\uff0c and watch them in Tensorboard?\n", "comments": ["This is a question better suited for Stack Overflow, since this isn't a bug / new feature request.\n"]}, {"number": 772, "title": "Can't install current MASTER from source", "body": "Hi!\n\nI was trying to install a fresh install of the tensorflow master from source, but always run into an error:\n\n```\n$ bazel build -c opt --config=cuda  --verbose_failures --spawn_strategy=standalone //tensorflow/tools/pip_package:build_pip_package\n```\n\nEventually fails with:\n\n```\nERROR: /system/user/bioinf01/tom/sources/tensorflow/tensorflow/python/BUILD:71:1: C++ compilation of rule '//tensorflow/python:py_func_lib' failed: crosstool_wrapper_driver_is_not_\ngcc failed: error executing command \n  (cd /system/user/unterthi/.cache/bazel/_bazel_unterthi/b4214462455bc5801962f5dfb9d41d3b/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/system/apps/biosoft/bazel-0.1.1/bazel-bin/src:/system/apps/biosoft/jdk1.8.0_40/bin:/usr/local/cuda/bin:/system/apps/biosoft/R-3.2.0/bin:/system/apps/biosoft/caffe_py351/d\nistribute/bin:/system/apps/biosoft/lmdb-0.9.17/bin:/system/apps/biosoft/protobuf-3.0.0-alpha-3.1_py351/bin:/system/apps/biosoft/boost_1_59_0_py351/bin:/system/apps/biosoft/python-3\n51/bin:/usr/local/cuda/bin:/system/apps/biosoft/R-3.2.0/bin:/system/apps/biosoft/caffe_py351/distribute/bin:/system/apps/biosoft/lmdb-0.9.17/bin:/system/apps/biosoft/protobuf-3.0.0\n-alpha-3.1_py351/bin:/system/apps/biosoft/boost_1_59_0_py351/bin:/system/apps/biosoft/python-351/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/system\n/user/unterthi/bin:/system/user/unterthi/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-fre\ne-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/\nprotobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-o\npt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem ex\nternal/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/eigen3 -isystem exte\nrnal/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-eigen-5651786d5e59 -isystem third_party/py/numpy/numpy_incl\nude -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-py3-opt/genfiles/util/pyth\non/python_include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_li\nnux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorfl\now/python/lib/core/py_func.pic.d -fPIC -c tensorflow/python/lib/core/py_func.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/\npy_func.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /system/user/unterthi/.cache/bazel/_bazel_unterthi/b4214462455bc5801962f5dfb9d41d3b/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/system/apps/biosoft/bazel-0.1.1/bazel-bin/src:/system/apps/biosoft/jdk1.8.0_40/bin:/usr/local/cuda/bin:/system/apps/biosoft/R-3.2.0/bin:/system/apps/biosoft/caffe_py351/d\nistribute/bin:/system/apps/biosoft/lmdb-0.9.17/bin:/system/apps/biosoft/protobuf-3.0.0-alpha-3.1_py351/bin:/system/apps/biosoft/boost_1_59_0_py351/bin:/system/apps/biosoft/python-3\n51/bin:/usr/local/cuda/bin:/system/apps/biosoft/R-3.2.0/bin:/system/apps/biosoft/caffe_py351/distribute/bin:/system/apps/biosoft/lmdb-0.9.17/bin:/system/apps/biosoft/protobuf-3.0.0\n-alpha-3.1_py351/bin:/system/apps/biosoft/boost_1_59_0_py351/bin:/system/apps/biosoft/python-351/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/system\n/user/unterthi/bin:/system/user/unterthi/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-fre\ne-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -isystem google/\nprotobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-o\npt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem ex\nternal/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/eigen3 -isystem exte\nrnal/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-eigen-5651786d5e59 -isystem third_party/py/numpy/numpy_incl\nude -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local_linux-py3-opt/genfiles/util/pyth\non/python_include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_li\nnux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.o' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorfl\now/python/lib/core/py_func.pic.d -fPIC -c tensorflow/python/lib/core/py_func.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/\npy_func.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 118.584s, Critical Path: 32.92s\n```\n\nRunning that last command myself to see the GCC error message gives:\n\n```\n$ third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstac\nk-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iqu\note bazel-out/local_linux-py3-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem externa\nl/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-py3-\nopt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/lo\ncal_linux-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-5651786d5e59 -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive/eigen-\neigen-5651786d5e59 -isystem third_party/py/numpy/numpy_include -isystem bazel-out/local_linux-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_includ\ne -isystem bazel-out/local_linux-py3-opt/genfiles/util/python/python_include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"\n' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.o' -MD -MF bazel-out/local_lin\nux-py3-opt/bin/tensorflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.d -fPIC -c tensorflow/python/lib/core/py_func.cc -o bazel-out/local_linux-py3-opt/bin/tens\norflow/python/_objs/py_func_lib/tensorflow/python/lib/core/py_func.pic.o\nIn file included from ./tensorflow/core/public/tensor.h:19:0,\n                 from ./tensorflow/python/lib/core/py_func.h:20,\n                 from tensorflow/python/lib/core/py_func.cc:16:\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:90: fatal error: external/eigen_archive/eigen-eigen-5651786d5e59/unsupported/Eigen/CXX11/Tensor: No such file or directory\n #include \"external/eigen_archive/eigen-eigen-5651786d5e59/unsupported/Eigen/CXX11/Tensor\"\n                                                                                          ^\ncompilation terminated.\n```\n", "comments": ["Interesting! I checked out the master (a2c5e74) today and it does compile without cuda. Did you try without cuda?\n\nHowever, my custom operations that worked in 0.5.0 fail with the same message. Would be very interested to find out why\n\n```\nIn file included from ./tensorflow/core/public/tensor.h:19:0,\n                 from ./tensorflow/core/framework/device_base.h:28,\n                 from ./tensorflow/core/framework/op_kernel.h:24,\n                 from tensorflow/core/user_ops/my_first_kernel.cc:3:\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:90: fatal error: external/eigen_archive/eigen-eigen-5651786d5e59/unsupported/Eigen/CXX11/Tensor: No such file or directory\n #include \"external/eigen_archive/eigen-eigen-5651786d5e59/unsupported/Eigen/CXX11/Tensor\"\n```\n\nUpdate: Probably related to this report: http://stackoverflow.com/questions/34689114/where-is-the-external-code-unsupported-eigen-cxx11-tensor-for-tensorflow-githu/34753456#34753456\n", "1) did you run ./configure ?\n\n2) Can you try a newer version of bazel?  I think 0.1.1 is too old now.\n", "Thanks for the advise. Yes I did run `./configure`, but I used bazel 0.1.1.\n\n I tried again with both bazel 0.1.2 and bazel 0.1.3 and today's master. Howver I always get the following error:\n\n```\nERROR: /system/user/bioinf01/tom/apps/tensorflow/tensorflow/core/BUILD:148:1: undeclared inclusion(s) in rule '//tensorflow/core:gpu_runtime':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/common_runtime/gpu/gpu_device_factory.cc':\n  '/usr/local/cuda-7.5/include/cuda.h'\n  '/usr/local/cuda-7.5/include/cufft.h'\n  '/usr/local/cuda-7.5/include/cuComplex.h'\n  '/usr/local/cuda-7.5/include/vector_types.h'\n  '/usr/local/cuda-7.5/include/builtin_types.h'\n  '/usr/local/cuda-7.5/include/device_types.h'\n  '/usr/local/cuda-7.5/include/host_defines.h'\n  '/usr/local/cuda-7.5/include/driver_types.h'\n  '/usr/local/cuda-7.5/include/surface_types.h'\n  '/usr/local/cuda-7.5/include/texture_types.h'\n  '/usr/local/cuda-7.5/include/cuda_runtime.h'\n  '/usr/local/cuda-7.5/include/host_config.h'\n  '/usr/local/cuda-7.5/include/channel_descriptor.h'\n  '/usr/local/cuda-7.5/include/cuda_runtime_api.h'\n  '/usr/local/cuda-7.5/include/cuda_device_runtime_api.h'\n  '/usr/local/cuda-7.5/include/driver_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.hpp'.\n____[858 / 1,221] Still waiting for 1 job to complete:\n      Running (spawn):\n        Compiling tensorflow/core/kernels/cwise_op_gpu_square.cu.cc, 10 s\n____Building complete.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n", "Weirdly, running the same command again for a 2nd time, I get a slightly different error:\n\n```\n$ bazel build --config=cuda --verbose_failures --spawn_strategy=standalone -c opt //tensorflow/tools/pip_package:build_pip_package \nWARNING: Output base '/system/user/unterthi/.cache/bazel/_bazel_unterthi/ca09685feaa24bba7598707e311fa383' is on NFS. This may lead to surprising failures and undetermined behavior.\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /system/user/bioinf01/tom/apps/tensorflow/tensorflow/stream_executor/BUILD:5:1: undeclared inclusion(s) in rule '//tensorflow/stream_executor:stream_executor':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cuda_diagnostics.cc':\n  '/system/user/bioinf01/tom/apps/tensorflow/tensorflow/stream_executor/cuda/cuda_diagnostics.h'.\n\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 15.271s, Critical Path: 13.78s\n```\n", "@schmiflo adding hdrs = glob([\"user_ops/**/*.h\"]) to the //tensorflow/core:user_ops_op_lib BUILD rule might fix your custom ops problem (Update: actually that looks like something else, but this should fix an undeclared inclusion(s) error if you use custom headers.)\n\n@untom I got a similar \"undeclared inclusion(s)\" error with bazel 0.1.3, but downgrading to a precompiled 0.1.1 seemed to fix the problem (note, however, that I am not trying to build the pip package).\n", "@untom +1, I got the same problem\n", "@untom, @JianbangZ\n`ERROR: /system/user/bioinf01/tom/sources/tensorflow/tensorflow/python/BUILD:71:1: C++ compilation of rule '//tensorflow/python:py_func_lib' failed: crosstool_wrapper_driver_is_not_\ngcc failed: error executing command`\nhas been fixed in #733. You can try the latest master. And\n`undeclared inclusion(s) in rule '//tensorflow/core:gpu_runtime': this rule is missing dependency declarations` \ncan be fixed by adding\n`unfiltered_cxx_flag: \"-fno-canonical-system-headers\"` at https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/CROSSTOOL#L110\nfor more info, you can checkout #469\n", "Does somebody want to send us a PR for that?\n", "@vrv I'll do it.\n", "Thanks @frankyjuang !  We just merged it -- let us know if the problems are still present by commenting and we'll reopen.\n", "You're welcome.\n", "Thanks for looking into this. I now run into a different error. Not sure if the other error masked this one or if this is a completely different issue (let me know if I should file this separately):\n\n```\n$ bazel build --config=cuda --verbose_failures --spawn_strategy=standalone -c opt //tensorflow/tools/pip_package:build_pip_package \nWARNING: Output base '/system/user/unterthi/.cache/bazel/_bazel_unterthi/dd424fe506e42444112be8dec7db846b' is on NFS. This may lead to surprising failures and undetermined behavior.\n........\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /data01/bioinf/tom/apps/tensorflow/tensorflow/python/BUILD:247:1: Converting to Python 3: tensorflow/python/ops/functional_ops.py failed: 2to3 failed: error executing command \n  (cd /system/user/unterthi/.cache/bazel/_bazel_unterthi/dd424fe506e42444112be8dec7db846b/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local_linux-py3-opt/genfiles/python3/tensorflow/python/ops --write-unchanged-files tensorflow/python/ops/functional_ops.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: 2to3 failed: error executing command \n  (cd /system/user/unterthi/.cache/bazel/_bazel_unterthi/dd424fe506e42444112be8dec7db846b/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local_linux-py3-opt/genfiles/python3/tensorflow/python/ops --write-unchanged-files tensorflow/python/ops/functional_ops.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 87.057s, Critical Path: 11.50s\n```\n\nFor what it's worth, I'm trying to use Python 3.5.1 (custom build, not the system installation). What I find really puzzling is the file bazel is trying to execute (`2to3`):\n\n```\n$ cat /system/user/unterthi/.cache/bazel/_bazel_unterthi/dd424fe506e42444112be8dec7db846b/tensorflow/bazel-out/host/bin/tools/python/2to3\n#!/bin/bash\n\nexit 1\n```\n\nI changed this file to `exit 0` instead, but this again leaves me with an error:\n\n```\n$ bazel build --config=cuda --verbose_failures --spawn_strategy=standalone -c opt //tensorflow/tools/pip_package:build_pip_package \nWARNING: Output base '/system/user/unterthi/.cache/bazel/_bazel_unterthi/dd424fe506e42444112be8dec7db846b' is on NFS. This may lead to surprising failures and undetermined behavior.\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /data01/bioinf/tom/apps/tensorflow/third_party/gpus/cuda/BUILD:122:1: declared output 'third_party/gpus/cuda/lib64/libcufft.so.7.5' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).\nERROR: /data01/bioinf/tom/apps/tensorflow/third_party/gpus/cuda/BUILD:122:1: not all outputs were created.\nERROR: /data01/bioinf/tom/apps/tensorflow/tensorflow/python/BUILD:247:1: output 'python3/tensorflow/python/ops/functional_ops.py' was not created.\nERROR: /data01/bioinf/tom/apps/tensorflow/tensorflow/python/BUILD:247:1: not all outputs were created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 3.169s, Critical Path: 1.73s\n```\n\n(In case you're wondering: I changed all references of CUDA libraries to 7.5, since installing 7.0 on this system is not an option. `libcufft.so.7.5` is in a path that is included in LD_LIBRARY_PATH)\n", "Bazel tried to run 2to3 if it sees a python file that doesn't have the srcs_version = \"PY2AND3\" annotation. As it turns out that's the case for functional_ops.py (and a couple of other targets in python/BUILD.\n\nCan you add the annotation to those targets and see if that works for you?\n", "I actually noticed this was likely a duplicate of #791, the workaround described there solved the problem for me, as well!\n", "+1 \nI run into the same problem when running\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\nbazel 0.1.1 and 0.1.3 give this error immediately\n\n```\njava.lang.NoClassDefFoundError: com/jcraft/jsch/JSchException\n        at org.eclipse.jgit.transport.JschConfigSessionFactory.getSession(JschConfigSessionFactory.java:109)\n        at org.eclipse.jgit.transport.SshTransport.getSession(SshTransport.java:136)\n        at org.eclipse.jgit.transport.TransportGitSsh$SshFetchConnection.<init>(TransportGitSsh.java:262)\n        at org.eclipse.jgit.transport.TransportGitSsh.openFetch(TransportGitSsh.java:161)\n        at org.eclipse.jgit.transport.FetchProcess.executeImp(FetchProcess.java:136)\n        at org.eclipse.jgit.transport.FetchProcess.execute(FetchProcess.java:122)\n        at org.eclipse.jgit.transport.Transport.fetch(Transport.java:1138)\n        at org.eclipse.jgit.api.FetchCommand.call(FetchCommand.java:130)\n        at org.eclipse.jgit.api.CloneCommand.fetch(CloneCommand.java:193)\n        at org.eclipse.jgit.api.CloneCommand.call(CloneCommand.java:133)\n        at com.google.devtools.build.lib.bazel.repository.GitCloneFunction.compute(GitCloneFunction.java:127)\n        at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:933)\n        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:499)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```\n\nbazel 0.1.2 fails to build protobuf. \n\n```\nINFO: Found 1 target...\nINFO: From Compiling google/protobuf/src/google/protobuf/extension_set.cc [for host]:\nsrc/main/tools/namespace-sandbox.c:633: execvp(argv[0], argv): No such file or directory\nERROR: /home/zer0n/src/tensorflow/google/protobuf/BUILD:29:1: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /home/zer0n/.cache/bazel/_bazel_zer0n/752539f897d4732a3a650ab733666abd/tensorflow && \\\n  exec env - \\\n    PATH=/home/zer0n/src/torch/install/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-out/host/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers '-frandom-seed=bazel-out/host/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/extension_set.o' -MD -MF bazel-out/host/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/extension_set.d -c google/protobuf/src/google/protobuf/extension_set.cc -o bazel-out/host/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/extension_set.o): crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /home/zer0n/.cache/bazel/_bazel_zer0n/752539f897d4732a3a650ab733666abd/tensorflow && \\\n  exec env - \\\n    PATH=/home/zer0n/src/torch/install/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-out/host/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers '-frandom-seed=bazel-out/host/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/extension_set.o' -MD -MF bazel-out/host/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/extension_set.d -c google/protobuf/src/google/protobuf/extension_set.cc -o bazel-out/host/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/extension_set.o).\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 1.338s, Critical Path: 0.33s.\n```\n"]}, {"number": 771, "title": "Had issues running ./configure and bazel build commands", "body": "I did not get any error when I ran ./configure but when i ran \n\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n\nI get following error \n\nSending SIGTERM to previous Bazel server (pid=2919)... done.\n........\nINFO: Found 1 target...\nINFO: From Compiling external/re2/re2/compile.cc:\nsrc/main/tools/namespace-sandbox.c:633: execvp(argv[0], argv): No such file or directory\nERROR: /home/ushnish/.cache/bazel/_bazel_ushnish/f442d11296e91085e26406d3023541ae/external/re2/BUILD:9:1: C++ compilation of rule '@re2//:re2' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 34 argument(s) skipped).\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 9.317s, Critical Path: 1.97s\n\nI was actually able to solve this problem and successfully run TF with GPUs by following exactly the instructions on this page, which did not mention ./configure or the bazel build command\n\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/jRkkvsB1iWA\n\nPlease reflect these instructions or some modified version of the instructions on the Getting Started page of Tensorflow as this particular step caused a lot of grief :)\n", "comments": ["https://www.tensorflow.org/versions/master/get_started/os_setup.html#optional-install-cuda-gpus-on-linux\n\nMost of the information is there, including ./configure and bazel, if building from sources.\n\nIn your case you probably need to pass --spawn_strategy=standalone  to the bazel command line flag -- I guess we should add that to our tutorials but that seems like a temporary bazel bug that I was hoping would get resolved soon...\n", "@lberki: do you think we should just add --spawn_strategy=standalone to our build instructions? Is that likely to be unnecessary in the future?\n", "Adding  --spawn_strategy=standalone to every `bazel build` command works for me. I think you should add \" --spawn_strategy=standalone\" to the install instructions.\n\nThis is the error I got when using bazel build without --spawn_strategy=standalone\n\n```\nINFO: From Compiling google/protobuf/src/google/protobuf/wire_format_lite.cc:\nsrc/main/tools/namespace-sandbox.c:633: execvp(argv[0], argv): No such file or directory\nERROR: /home/user/tensorflow/google/protobuf/BUILD:29:1: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 42 argument(s) skipped).\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 3.294s, Critical Path: 0.65s\n```\n"]}, {"number": 770, "title": "fix error in coordinator documentation", "body": "There is a missing comma in the Coordinator() documentation.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks!\n"]}, {"number": 769, "title": "\"Deep MNIST for Experts\" too complex for second tutorial.", "body": "The end of the first tutorial, \"MNIST for ML Beginners\", has this paragraph:\n\n> What matters is that we learned from this model. Still, if you're feeling a bit down about these results, check out [the next tutorial](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts) where we do a lot better, and learn how to build more sophisticated models using TensorFlow!\n\nBut the next tutorial is \"Deep MNIST for Experts\", and it does not explain things nearly as well as the first tutorial. For example, the second tutorial starts differing at the `Build a Multilayer Convolutional Network` section, but the very first paragraph under `Weight Initialization` does not explain the concepts it is using:\n\n> To create this model, we're going to need to create a lot of weights and biases. One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons.\" Instead of doing this repeatedly while we build the model, let's create two handy functions to do it for us.\n\nWhat are \"ReLU neurons\"? Why are we using them? What _is_ a convolutional network even?\n\nI would say, either\n1. This tutorial should be expanded to clarify/explain things better for beginners, or\n2. This tutorial should be moved to later in the list of tutorials, and\n3. The first tutorial, \"MNIST for Beginners,\" should not say `the next tutorial` but instead say `a later tutorial on MNIST for Experts`.\n\nThanks!\n", "comments": ["Seconded. I think an explanation of conv nets would be helpful in the context of MNIST digits.\n", "The TensorFlow tutorials cannot replace a deep learning or machine learning textbook or course. We may add pointers to additional resources as those become available.\n", "Not only this tutorial is complicated, all these tutorials are complicated.\nFor example, what I want to do is:\n1. load a cvs of image file names, and the classification labels associated with each of them\n2. load a folder of images\n3. **Have the neuron network up and running !!! no matter how bad the accuracy !!!**\n4. Improve the accuracy\n\nBut Tensorflow gives me already at the beginning a 4-file MNIST tutorial, with complicated graphs and complicated mechanisms for loading files into the network\n"]}, {"number": 768, "title": "Added generate_checkpoint_state_proto().", "body": "Added generate_checkpoint_state_proto() which returns a checkpoint proto with model_checkpoint_path containing absolute path or path relative to save_dir.\n\nAdded unit tests.\n\nFixes #751.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins: test this please.\n", "LG, can you squash the commits?  Then I'll merge.\n", "Can one of the admins verify this patch?\n"]}, {"number": 767, "title": "rnn_cell.py: Allow user to explicitly set forget bias in LSTMCell", "body": "", "comments": ["Can one of the admins verify this patch?\n", "LGTM\n", "I think it would be good to have a test for this -- right now if we removed it, no tests would fail.\n", "@vrv Added the option to the existing test, seemed useless to add a complete new test just for that option\n", "Would this test fail if the dtype of the cell is float64?\n\nOn Mon, Jan 18, 2016 at 8:51 AM, Fabrizio Milo notifications@github.com\nwrote:\n\n> @vrv https://github.com/vrv Added the option to the existing test,\n> seemed useless to add a complete new test just for that option\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/767#issuecomment-172584885\n> .\n", "Closing due to lack of activity.\n", "@vrv I think it just need to be tested by Jenkins and then is good to go. what is missing ?\n", "An answer to https://github.com/tensorflow/tensorflow/pull/767#issuecomment-172597373 :)\n", "@ebrevdo I don't think it would fail. I guess you are referring to the 1.0. I think it will be casted to the right type in the `f + self._forget_bias`. How would you create a test with a cell with dtype=float64 ? \n", "This change looks good to me, I think it will work as said. Can we test it and get it in?\n", "Can one of the admins verify this patch?\n", "Ok, well then let's address the conflict / squash, then we'll test and merge.\n", "rebased\n", "Hmm, so not saying this change isn't helpful, but doesn't [Jozefowicz 2016](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf) suggest that you would never really want to have a forget bias initialized to anything other than 1.0 as it outperforms all other LSTMs? Would there ever be a foreseeable instance where you would not want to have that be the case?\n", "@rafaljozefowicz \n", "> Would there ever be a foreseeable instance where you would not want to have that be the case?\n\nI think 1.0 is the best default value for a variety of tasks. But it's hard to be sure that there is no better starting bias on some particular task. And some people want to faithfully replicate previous work that used bias=0. So it's definitely good to have this option!\n", "Okay, rebase / address the conflict and we'll test / merge.\n", ":cool: Thanks!\n", "@vrv rebased.\n", "@tensorflow-jenkins: test this please\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 766, "title": "Running TensorFlow on GTX 480 (Compute capability 2.0)", "body": "I'm trying to make tensorflow run on an older GPU (GTX 480). So I removed the compute checking condition in gpu_device.cc like this:\n\n``` C\n       // Only GPUs with no less than the minimum supported compute capability is\n      // accepted.\n      //if (device_capability < min_supported_capability) {\n      //  LOG(INFO) << \"Ignoring gpu device \"\n      //            << \"(\" << GetShortDeviceDescription(i, desc) << \") \"\n      //            << \"with Cuda compute capability \" << device_capability\n      //            << \". The minimum required Cuda capability is \"\n      //            << min_supported_capability << \".\";\n      //  continue;\n      //}\n```\n\nAnd run the cifar10_train.py\nI got the errors as following: \n\n``` C\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2424 get requests, put_count=2259 evicted_count=1000 eviction_rate=0.442674 and unsatisfied allocation rate=0.521865\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110\nW tensorflow/core/common_runtime/executor.cc:1075] 0x206ad80 Compute status: Invalid argument: Indices are not valid: not lexicographically sorted or containing repeats.\n     [[Node: SparseToDense = SparseToDense[T=DT_FLOAT, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](concat/_195, SparseToDense/output_shape/_197, SparseToDense/sparse_values/_199, SparseToDense/default_value/_201)]]\nW tensorflow/core/common_runtime/executor.cc:1075] 0x35962a0 Compute status: Invalid argument: Indices are not valid: not lexicographically sorted or containing repeats.\n     [[Node: SparseToDense = SparseToDense[T=DT_FLOAT, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](concat/_195, SparseToDense/output_shape/_197, SparseToDense/sparse_values/_199, SparseToDense/default_value/_201)]]\n     [[Node: SparseToDense/_203 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_474_SparseToDense\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\n```\n\nI'm guessing if tensorflow is trying to allocate more memory than supported?\n", "comments": ["1) I would not modify the source code like that: try following the instructions here https://www.tensorflow.org/versions/master/get_started/os_setup.html#optional-install-cuda-gpus-on-linux to enable other compute capabilities.\n\n2) The error is:\n\n```\nIndices are not valid: not lexicographically sorted or containing repeats.\n     [[Node: SparseToDense = SparseToDense[T=DT_FLOAT, Tindices=DT_INT32, valid\n```\n\nMake sure to read the documentation of SparseToDense to see what contract your code is violating.\n\nFeel free to comment again if you think there's an actual bug or installation problem and I'll re-open. Otherwise, StackOverflow might be a better place for this type of question.\n"]}, {"number": 765, "title": "Fix typos in tutorials", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thank you, @vrv \n"]}, {"number": 764, "title": "Fix Python 3 compilation issues", "body": "This fixes the build error when using Python 3 as the return types for import array operations are int in Python 3 and void in Python 2 (as discussed in #733).\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "I'll merge once you add the comment, and [squash the commits](rebaseandsqua.sh) (or amend the existing one).\n", "I take that back, your other problem may well be related. \n\nThe #if thing must not be in the header (since we will not know about NUMPY_IMPORT_ARRAY_RETVAL until after numpy/arrayobject.h has been included). Move it to the c file, below the includes, and you should be fine.\n", "I fixed it and added the comment, but it still does not build under Linux. The build breaks at the same place as it broke before the fix, and I could not figure out what happened as the error message returned by bazel only states which rule and command failed. I will test on OS X right now.\n", "Jenkins, test this please.\n", "Fortunately, the Jenkins build produced a decent enough error message -- can you see if that's related to your failure or even the same?\n", "Yes, it seems to be the same issue.\n", "Error is enlightening: \n\ntensorflow/python/lib/core/py_func.cc:29:31: error: operator '==' has no left operand\n #if NUMPY_IMPORT_ARRAY_RETVAL == NULL\n\nChecking for empty defines is nasty business, look at this: http://stackoverflow.com/questions/3781520/how-to-test-if-preprocessor-symbol-is-defined-but-has-no-value\n", "I was expecting the error message to be below the red \"ERROR\" message, not above it. Now it's clear!\n\nWould it be acceptable to use PY_MAJOR_VERSION for the comparison instead of NUMPY_IMPORT_ARRAY_RETVAL, just to avoid adding macro black magic just for this? I think it will make the code clearer.\n", "I fixed this by using PY_MAJOR_VERSION (as mentioned above), but now there's another build error that is not related to this code:\n\n```\nbazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'PyObject* _wrap_TF_GetOpList(PyObject*, PyObject*)':\nbazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc:4636:72: error: 'PyString_FromStringAndSize' was not declared in this scope\n       reinterpret_cast<const char*>((&result)->data), (&result)->length);\n```\n\n`PyString_FromStringAndSize` does not exist in Python 3, and this is dealt with in the SWIG interface `platform/base.i`, but not in `client/tf_session.i`.\n", "Can you fix it in tf_session the same way it's fixed in base.i? I think that's the right thing to do. Might as well make this PR \"Fix Python 3 compilation issues\" since we're almost there already.\n", "This is now fixed but the build is still breaking with the same error:\n\n```\nbazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc:4639:72: error: 'PyString_FromStringAndSize' was not declared in this scope\n       reinterpret_cast<const char*>((&result)->data), (&result)->length);\n```\n\nSince I have both Python 2 and 3 installed, is there a chance bazel is passing the wrong includes/libs to GCC? I tried cleaning and reconfiguring the build to no avail.\n", "Jenkins, test this please.\n", "Just making another log for myself. :)\n", "I guess the problem was having both Python 2 and 3, since tests passed on Linux and Android but not Mac? I don't know how the environment used by Jenkins looks like, but I'm on Ubuntu 14.04 with python-dev and python3-dev installed.\n", "The Mac build is broken right now, unrelated to this. :/ \n\nIt's strange because the compiler shouldn't even see the PyString_ version any more. Can you remove the ifdef altogether (and keep only the Py3 branch) and see if that works compiling with Python3? Those two are the only occurrences of this function, so I feel like for some reason the ifdef doesn't work as expected (maybe because it's evaluated by swig, and we need a %{ %} around it).\n", "Removing the Python 2 only branches works. According to the docs SWIG [should not mess with C preprocessor stuff](http://www.swig.org/Doc3.0/SWIGDocumentation.html#SWIG_nn6), but it seems that's wrong :)\n\nI think `%{ }` only works for things to be included in the interface header, isn't it?\n", "To add to the weirdness of this issue: something (maybe SWIG?) adds the following macros to pywrap_tensorflow.cc:\n\n```\n#if PY_VERSION_HEX >= 0x03000000\n\n#define PyClass_Check(obj) PyObject_IsInstance(obj, (PyObject *)&PyType_Type)\n#define PyInt_Check(x) PyLong_Check(x)\n#define PyInt_AsLong(x) PyLong_AsLong(x)\n#define PyInt_FromLong(x) PyLong_FromLong(x)\n#define PyInt_FromSize_t(x) PyLong_FromSize_t(x)\n#define PyString_Check(name) PyBytes_Check(name)\n#define PyString_FromString(x) PyUnicode_FromString(x)\n#define PyString_Format(fmt, args)  PyUnicode_Format(fmt, args)\n#define PyString_AsString(str) PyBytes_AsString(str)\n#define PyString_Size(str) PyBytes_Size(str)    \n#define PyString_InternFromString(key) PyUnicode_InternFromString(key)\n#define Py_TPFLAGS_HAVE_CLASS Py_TPFLAGS_BASETYPE\n#define PyString_AS_STRING(x) PyUnicode_AS_STRING(x)\n#define _PyLong_FromSsize_t(x) PyLong_FromSsize_t(x)\n\n#endif\n```\n\nThese should be defined and avoid the issue without any other workarounds but that never happens, which probably means both PY_VERSION_HEX and PY_MAJOR_VERSION are not defined correctly. The script in `util/python/python_config.sh` seems to be working properly, as the links to python_include and python_lib are correct. \n", "I see that this list conspicuously does not include\n`PyString_FromStringAndSize`.\n\nMaybe this is a swig problem? What version of swig is this, and does\npython3 maybe need a newer version of it?\n\nOn Wed, Jan 13, 2016 at 3:08 PM Jo\u00e3o Felipe Santos notifications@github.com\nwrote:\n\n> To add to the weirdness of this issue: something (maybe SWIG?) adds the\n> following macros to pywrap_tensorflow.cc:\n> \n> #if PY_VERSION_HEX >= 0x03000000\n> \n> #define PyClass_Check(obj) PyObject_IsInstance(obj, (PyObject *)&PyType_Type)\n> #define PyInt_Check(x) PyLong_Check(x)\n> #define PyInt_AsLong(x) PyLong_AsLong(x)\n> #define PyInt_FromLong(x) PyLong_FromLong(x)\n> #define PyInt_FromSize_t(x) PyLong_FromSize_t(x)\n> #define PyString_Check(name) PyBytes_Check(name)\n> #define PyString_FromString(x) PyUnicode_FromString(x)\n> #define PyString_Format(fmt, args)  PyUnicode_Format(fmt, args)\n> #define PyString_AsString(str) PyBytes_AsString(str)\n> #define PyString_Size(str) PyBytes_Size(str)\n> #define PyString_InternFromString(key) PyUnicode_InternFromString(key)\n> #define Py_TPFLAGS_HAVE_CLASS Py_TPFLAGS_BASETYPE\n> #define PyString_AS_STRING(x) PyUnicode_AS_STRING(x)\n> #define _PyLong_FromSsize_t(x) PyLong_FromSsize_t(x)\n> \n> #endif\n> \n> These should be defined and avoid the issue without any other workarounds\n> but that never happens, which probably means both PY_VERSION_HEX and\n> PY_MAJOR_VERSION are not defined correctly.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/764#issuecomment-171466757\n> .\n", "I guess I was too tired and did not see `PyString_FromStringAndSize` was missing, sorry!\n\nMy Linux box is on Ubuntu 14.04, which ships SWIG 2.0.11, but with SWIG 3.0.8 on my Mac I got the same auto-generated macros. However, SWIG's at fault: if you check line 208 on `tensorflow/python/client/tf_session.i`, we added a SWIG typemap which uses an `#if PY_MAJOR_VERSION`. This is what gets translated into the broken code, since the SWIG-generated file has a similar call with blank lines in the places the branched-out `#if` lines are.\n\nCode in `tf_session.i`:\n\n```\n%typemap(out) TF_Buffer TF_GetOpList {\n#if PY_MAJOR_VERSION < 3\n  $result = PyString_FromStringAndSize(\n#else\n  $result = PyUnicode_FromStringAndSize(\n#endif\n      reinterpret_cast<const char*>($1.data), $1.length);\n}\n```\n\nCode in `pywrap_tensorflow.cc`:\n\n```\n  {\n    resultobj = PyString_FromStringAndSize(\n\n\n\n      reinterpret_cast<const char*>((&result)->data), (&result)->length);\n  }\n```\n\nTurns out [SWIG was preprocessing](http://www.swig.org/Doc1.3/Preprocessor.html#Preprocessor_nn9) the `#if PY_MAJOR_VERSION` as you suspected, and you have to use `%#` instead of `#` to make it forward preprocessing directives to the generated code. This is fixed in the last commit and the code now compiles successfully on OS X and Linux. \n", "Splendid. Can you squash, please?\n", "Jenkins, test this please.\n", "Done!\n", "Jenkins, test this please.\n", "Merged\n"]}, {"number": 763, "title": "Non-blocking queuing and running on thread pool", "body": "Addresses #551 and should address #583\n\nFor a benchmark on running the December Inception model 100 times (20 in parallel at a time) on a 32 vcore CPU, results are:\n\nCurrent implementation (baseline)\n\n```\n211.17user 154.92system 0:15.42elapsed 2373%CPU (0avgtext+0avgdata 950024maxresident)k\n0inputs+0outputs (0major+186279minor)pagefaults 0swaps\n```\n\nLock free implementation\n\n```\n204.62user 27.12system 0:09.08elapsed 2551%CPU (0avgtext+0avgdata 737888maxresident)k\n0inputs+0outputs (0major+122158minor)pagefaults 0swaps\n```\n\nSo throughput is improved by ~35% and system overhead is greatly reduced.\n\nThis is a port of existing code, which will be cleaned up for style and better integrated.  I'm interested in a preliminary review of approach, applicability and interest in getting it fully merged for the moment.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "CLA was signed as Datacratic Inc.\n", "(waiting for the corp CLA to be added before looking at this)\n", "(Corporate CLA was submitted this morning... I guess it's still working its way through the pipes as we haven't gotten a countersigned version back yet).\n", "(Yeah, looking internally it still seems to have not pushed, sorry about the delay).\n", "FYI the CLA came back.\n", "Can you check whether it shows up on https://cla.developers.google.com/clas\n ?\n\nOn Thu, Jan 14, 2016 at 8:42 AM Jeremy Barnes notifications@github.com\nwrote:\n\n> FYI the CLA came back.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/763#issuecomment-171696248\n> .\n", "@martinwicke I checked internally, it has been signed.  Not sure what magic words to trigger to the googlebot.\n\nLet me try a magic incantation:\n\nI signed it!\n\nDatacratic Inc\n", "I signed it!\n\n@martinwicke it does show up at the URL in your previous comment.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "I'll be using reviewable to review this.  Review comments/discussion will be at:\n\nhttps://reviewable.io/reviews/tensorflow/tensorflow/763\n", "There is a fair bit more information in Reviewable, including a benchmark on mnist training which is nearly twice as fast (CPU-only) with this PR.  It looks like Reviewable is a little bit of a black hole for comments; would it be possible for somebody to have a look?\n", "The latest commit implements the full set of feedback on Reviewable.\n", "Addressed latest comments on Reviewable.\n", "Addressed latest comments on Reviewable.\n", "Addressed latest comments on Reviewable.\n", "Addressed latest comments on Reviewable and rebased and squashed onto latest HEAD.\n", "What is the status of this?\n", "Can one of the admins verify this patch?\n", "@bhack This patch (or at least, where I originally branched off) works, is running in production at http://mldb.ai/, and does increase the CPU-only speed of Tensorflow by a factor of 2, in both training and prediction, from C++ and Python.  It's pretty safe to use, since it's disabled by default and needs to be turned on by an environment variable.\n\nHowever, I don't see a viable path to getting it merged.  Several days of work went into pulling it into shape based upon the reviewer's comments (especially @dvyukov who had excellent comments and a deep review).  Unfortunately most of the comments were lost in Reviewable when the patch was rebased for merge, and there was no clarity on how many separate reviewers it would need to pass (with most review comments focusing on the more trivial aspects of style rather than a deep review of the substance).  So it ended up at a bit of a dead end.\n\nMore importantly, I feel we need some input from the project leadership.  Concurrently with reviewing my patch, @dvyukov proposed #967 which fixes slightly different problems (optimizing jobs submitted from a single thread rather than multiple threads as in this patch), but he also identified #932 and #933 which show some more fundamental issues in the thread pool model and Eigen.  We agreed that we needed a clear direction on which benchmarks or situations we were interested in optimizing for to decide, but it looks like the TF team are busy putting TF everywhere as their day-jobs whilst managing one of the most popular OSS projects on Github and we haven't heard back :)\n\nMore seriously, some more structured guidance for non-Google committers would be helpful, as there is a lot of assumed knowledge of the Google development processes that external committers just don't have.  For example, it seems that Googlers look at each other's employee number to understand relative seniority when there is conflicting advice, but those aren't available on Github.  It would have also been helpful to know that reviewers will default to reviewing coding style before passing to deeper reviews, and so one should focus on getting that right before thinking about the functionality.\n\nI would also not recommend using Reviewable until it has greatly matured; my experience there was very poor.  I can see it has merits as a tool, but needs some more work and I have issues with the PR metadata and comments not being visible on Github.\n\nSince I ran out of time to work on this, I decided to put it aside until I had a clear indication that it was worth working on and had a chance of getting merged.\n\nThe bright side is that MLDB's thread pool improved from the review comments, so in some ways I found a way to get Google employees to review code from another project :)\n\nThe version in mldbai/tensorflow on the master branch is more up to date, if ever you want to use it, though it's based on an earlier hash.\n", "@jeremybarnes: I'm sorry to hear about the experience -- we were hoping Reviewable would be a useful tool but GitHub's lack of permission control made it impossible for Reviewable to do its job.  We won't be using Reviewable or any other system until GitHub can improve the permission system.\n\nSecondly, your PR went through a few reviews because we thought it was fine (tests were passing, etc).  But we realized this wasn't something to take lightly, and so we had @dvyukov help (FYI he's less \"senior\" than Jeff and Sanjay :P) -- and he unearthed that the Eigen use of threadpools is deadlock prone given our ThreadPool implementation, which was a much more urgent and important thing to fix.  I was under the impression that this was communicated to you, I'm sorry.  He's actually been working on changing Eigen multi-threading for a few weeks, and will hopefully have something soon.\n\nSo the upshot is that your PR was super useful in finding out these issues; we'll credit you for helping us out!\n", "@vrv Thanks for the update, and I agree that the Eigen changes are important to deal with as whatever thread pool is used needs to meet the requirements of the interface.  Are you planning on fixing this issue as part of that work?  I'm asking for guidance as to whether this PR has any chance of being merged or not.  If not, we should probably close it.\n\nI'd also ask you to consider allowing for the thread pool implementation to be pluggable, so that when TF is embedded within another application it could defer to the application's inbuilt thread pool.\n", "So the first step is to fix Eigen, as we've agreed.  The second step is to make sure we have benchmarks that allow us to evaluate the merits of any implementation: we don't have those right now.  Only after that we'll be able to compare implementations of ThreadPool.\n\nI (personally) would be happy to see pluggable ThreadPool implementations.  I'm not sure the way we suggested it in this PR is ideal anymore: I am thinking it might be better to have a ThreadPoolFactory and registration mechanism like we have for Devices and Session implementations, so that anyone can dynamically register their ThreadPool implementation, rather than requiring it be checked into our codebase and selected via environment variables.  If you would be interested in helping out on making something like the ThreadPoolFactory, that would be awesome.  Otherwise, we'll try to find time to do this ourselves and then anyone can plug in their implementation if they so desire.  We can then use this to evaluate your ThreadPool and accept it as the default if the benchmarks and metrics show it shines :)\n", "There definitely was lack of communication. Sorry.\n\nAnd Reviewable was not helping as well. I basically can't make any sense out of the interface. I have no idea what's pending, what's answered, when, etc.\n\nI am working on eigen contraction issue. It blocks any non-trivial thread pool implementation. @jeremybarnes you were lucky to not hit deadlocks. But if we would continue to improve your implementation, we would hit them as well (e.g. if we steal half of elements from remote queue which would effectively randomize execution order).\nUnfortunately it turned out to be not completely trivial. Contraction performance characteristics are also tightly tied to execution order (e.g. packed rhs reuse while it is hot in cache). And any asynchronous continuation-based implementation shuffles execution order. So most time is taken by ensuring that there are no significant performance degradations on any of hundreds of different matrix configurations, thread counts and instruction sets (sse, avx, fma).\n\n@jeremybarnes It would be great if you provide one or several benchmarks representative of workloads you are interested in. I know that you asked for acknowledgement from somebody from core tensorflow team and got no response. Anybody from tensorflow?\n\nRe pluggable thread pools, I would very much warn you against this idea.\nWhat it will do is it will fragment users, provoke components that work with one thread pool but does not work with another and disperse optimization efforts.\nThread pool implementation must not (in theory) differ by provided semantics, they can only differ by buginess level and performance/scalability. That's not what users much choose between. We need a single thread pool that is tested as much as possible, works with all components and provides best performance/scalability. There is no need for anything else.\nI understand that there can be different implementations that provide different performance for different algorithms. But what we should concentrate on it tuning the single implementation to support broader class of algorithms well. We are nowhere close to being able to reasonedly conclude that that is not possible, and the only choice we have is to fragment user base.\n", "@dvyukov Thank you for the update. What do you think about the switchable [Thread/Openmp approach adopted by Openblas](https://github.com/xianyi/OpenBLAS/wiki/faq#multi-threaded)?\n", "@bhack do we have this problem in tensorflow?\nHow would it look like? User manually partitions dataset and submits parts from within a openmp parallel for loop? I can hardly imagine it...\n", "I don't know if we could have this kind of problems with multiple Session.run calls.\n", "Work submitted from several sessions should go to a single thread pool, so it should not be an issue.\n\nAlso, user-level parallelization can be insufficient. If you run N sessions, N does not correlate with number of cores. So we need to parallelize anyway.\n\nAlso, tensorflow should do its best to determine proper parallelism level for each task. If user submits a tiny task, it should not be parallelized regardless of whether user just have tiny amount of data to process or user does own higher-level parallelization. So in the latter case parallelism will be turned off automatically.\n", "As of change `ab02c5`, we consider the CPU scaling issue generally resolved, so I'm going to close this PR. Please file new issues with benchmarks and steps to reproduce if you still see any performance issues.\n"]}, {"number": 762, "title": "tutorials_example_trainer working for cpu not gpu (built from source)", "body": "I am trying to install TF from source WITHOUT INTERNET so far I have been able to advance thanks to @damienmg by replacing git repos by local repos for re2,gemmlowp,libpng,jpegsvrc and eigen and modifying WORKSPACE accordingly.  \nThe `bazel build -c opt  //tensorflow/cc:tutorials_example_trainer` cmd for tutorials_example_trainer works fine (with a few warnings due to comparison between signed and unsigned entities and index subscript above array bounds).\nAnd `bazel-bin/tensorflow/cc/tutorials_example_trainer` produces the expected result.\nHowever when I clean up with `bazel clean --expunge` and rerun ./configure for GPU support and then run:  \n`bazel build -c opt config=cuda //tensorflow/cc:tutorials_example_trainer`  \nI get the error messages :  \n`'@gemmlowp//:eight_bit_int_gemm' failed: crosstool_wrapper_driver-is_not_gcc failed: error executing command third_party/gpus/crosstool/cleang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D FORTIFY_SOURCE=1' fstack-protector -fpIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object`  \nor the same with `'@re2//:re2'`\n", "comments": ["It seems like it is a PATH problem however I do not understand as the dependencies I installed locally worked for the non gpu support !\nSomeone ?\n", "I was hoping to get rid of the gemmlowp dependency soon (it doesn't look like we are actually using it), but re2 will likely stick around for a bit.  Not sure what the problem would be.\n", "Can you try \n\n```\nbazel build -c opt config=cuda --spawn_strategy=standalone --verbose_failures //tensorflow/cc:tutorials_example_trainer\n```\n\nand if that doesn't work, post the entire logs?\n", "I think I already built in in standalone and that did not work but I am gonna retry and post the log just in case or maybe I only did it for the building of the wheel and not the example.\n", "Since I posted this question I chose to install TF from the already created wheel available online instead of rebuilding it from scratch with bazel (because of this issue and also because of the 72 dependencies of tensorboard...I built it without GPU support and without Tensorboard but I was unhappy). I am a bit afraid that the competition of two installations would endanger the library itself therefore if you agree I will close this issue with no further explanations. Sorry for the waste of time.\n", "We'd be happy to help you debug the issue, but we understand if you just want to use the pip package instead.\n", "Being a relative beginner in Linux I think I am gonna stick with this install for now.\nI only installed from source because I heard somewhere on github that it could be better.\nWhat do you think ?\nYou all however have been incredibly helpful through the installation !\nIf I had to comment I would say that the tricks `bazel clean--expunge` and `spawnstrategy=standalone` should be more emphasized on Tensorflow installation page for beginners like me !\nPS : I reran with `spawnstrategy=standalone` and got another error with `undeclared inclusion(s) in rule '//tensorflow/core:gpu_runtime': this rule is missing dependency declarations for the following files` maybe it is because I am using the old version of bazel compatible with Java7...\n", "Hi @jeandut  , I am trying to instal TF from source without internet access now. Could you tell me how to replace git repos by local repos? Thanks!\n", "@cswhjiang see [#587](https://github.com/bazelbuild/bazel/issues/587) issue on bazel repo.\n"]}, {"number": 761, "title": "Optimizer got only float32?", "body": "Got some big number so thought dtype float64 must be able to embrace a larger range. Everything looks fine until it comes to define the optimisation. I got this error:\n\n```\nsite-packages/tensorflow/python/training/optimizer.py(343)_assert_valid_dtypes()\n    342             \"Invalid type %r for %s, expected: %s.\" % (\n--> 343                 dtype, t.name, [v for v in valid_dtypes]))\n    344 \n\nipdb> l\n    338     for t in tensors:\n    339       dtype = t.dtype.base_dtype\n    340       if dtype not in valid_dtypes:\n    341         raise ValueError(\n    342             \"Invalid type %r for %s, expected: %s.\" % (\n--> 343                 dtype, t.name, [v for v in valid_dtypes]))\n    344 \n    345   # --------------\n    346   # Methods to be implemented by subclasses if they want to use the\n    347   # inherited implementation of apply_gradients() or compute_gradients().\n    348   # --------------\n\nipdb> t\n<tensorflow.python.framework.ops.Tensor object at 0x110b40630>\nipdb> t.dtype.base_dtype\ntf.float64\nipdb> t.dtype\ntf.float64\n```\n\nLooks like only tf.float32 accepted in this snippet? Can this be fixed?\n", "comments": ["For a hacky (unsupported) workaround, you could try the [answer to this StackOverflow question](http://stackoverflow.com/a/33702428/3574081).\n", "@mrry: Is there a reason to not change _valid_dtypes on our end?\n", "there's likely a lot more holes in float64 support, I ran into the following ops being float32-only in 10 minutes of porting: image_summary, max_pool and Square\n", "Yep, changing `_valid_dtypes` on our end means fixing such things.  Are the other reasons?\n", "@girving `_valid_dtypes` now includes float64, changed in #2389.\n", "Thanks @siddharth-agrawal !  Someone can comment if for some reason #2389 doesn't fix this\n", "Seems still a warning appears for Adam optimizer in 1.0.1 version"]}, {"number": 760, "title": "word2vec_basic.py line 54, yield instead of return", "body": "```\n# Read the data into a string.\ndef read_data(filename):\n  f = zipfile.ZipFile(filename)\n  for name in f.namelist():\n    return f.read(name).split() ### SHOULD BE: yeild  f.read(name).split()?\n  f.close()\n```\n\nOringial sample data got one big txt file in the zip file. \n\n```\ndef read_data(filename):\n  with zipfile.ZipFile(filename) as f:\n    for name in f.namelist():\n      yield f.read(name).split() \n```\n", "comments": ["(Feel free to send us a pull request to fix this if you'd like)\n", "@fivejjs: Would you be up for sending a PR?  Your suggested code is indeed much better.\n", "this looks like it has been changed already by a different commit\n"]}, {"number": 759, "title": "ImportError: No module named tensorflow after installing from sources", "body": "I installed TF from sources because I do not have an internet access. I used @damienmg tricks to replace git repositories by internal sources for eigen, six, gemmlowp, re2, libpng and jpegsvrc.\nFinally the line `bazel build -c opt //tensorflow/cc:tutorials_example_trainer` compiled successfully (however there were several warnings of index subscript above limits and comparison between signed and unsigned (I figure it is because it is a beta version of eigen)).  \nAnd I ran the example `bazel-bin/tensorflow/cc/tutorials_example_trainer`  \nIt ran and the end line was :  \n`000002/000005 lambda=2.000000 x =[0.894427 -0.447214] y =[1.788854 -0.894427]`  \nHowever when I try to run python wherever `import tensorflow` fails with error no module named tensorflow (I am using anaconda if it is important and when I ran the configure the path for python was correct).  \nWhere do you think I got it wrong ?\n", "comments": ["My mistake I did not indeed install it through pip yet therefore the result\n"]}, {"number": 758, "title": "[RBM implemention] Problem with GraphDef cannot be larger than 2GB ?", "body": "Hi, everyone\nI'm trying to implement RBM with tensorflow, here is the code:\n\nrbm.py\n\n\"\"\" An rbm implementation for TensorFlow, based closely on the one in Theano \"\"\"\n\nimport tensorflow as tf\nimport math\n\ndef sample_prob(probs):\n    \"\"\"Takes a tensor of probabilities (as from a sigmoidal activation)\n       and samples from all the distributions\"\"\"\n    return tf.nn.relu(\n        tf.sign(\n            probs - tf.random_uniform(probs.get_shape())))\n\nclass RBM(object):\n    \"\"\" represents a sigmoidal rbm \"\"\"\n\n```\ndef __init__(self, name, input_size, output_size):\n    with tf.name_scope(\"rbm_\" + name):\n        self.weights = tf.Variable(\n            tf.truncated_normal([input_size, output_size],\n                stddev=1.0 / math.sqrt(float(input_size))), name=\"weights\")\n        self.v_bias = tf.Variable(tf.zeros([input_size]), name=\"v_bias\")\n        self.h_bias = tf.Variable(tf.zeros([output_size]), name=\"h_bias\")\n\ndef propup(self, visible):\n    \"\"\" P(h|v) \"\"\"\n    return tf.nn.sigmoid(tf.matmul(visible, self.weights) + self.h_bias)\n\ndef propdown(self, hidden):\n    \"\"\" P(v|h) \"\"\"\n    return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(self.weights)) + self.v_bias)\n\ndef sample_h_given_v(self, v_sample):\n    \"\"\" Generate a sample from the hidden layer \"\"\"\n    return sample_prob(self.propup(v_sample))\n\ndef sample_v_given_h(self, h_sample):\n    \"\"\" Generate a sample from the visible layer \"\"\"\n    return sample_prob(self.propdown(h_sample))\n\ndef gibbs_hvh(self, h0_sample):\n    \"\"\" A gibbs step starting from the hidden layer \"\"\"\n    v_sample = self.sample_v_given_h(h0_sample)\n    h_sample = self.sample_h_given_v(v_sample)\n    return [v_sample, h_sample]\n\ndef gibbs_vhv(self, v0_sample):\n    \"\"\" A gibbs step starting from the visible layer \"\"\"\n    h_sample = self.sample_h_given_v(v0_sample)\n    v_sample = self.sample_v_given_h(h_sample)\n    return  [h_sample, v_sample]\n\ndef cd1(self, visibles, learning_rate=0.1):\n    \" One step of contrastive divergence, with Rao-Blackwellization \"\n    h_start = self.propup(visibles)\n    v_end = self.propdown(h_start)\n    h_end = self.propup(v_end)\n    w_positive_grad = tf.matmul(tf.transpose(visibles), h_start)\n    w_negative_grad = tf.matmul(tf.transpose(v_end), h_end)\n\n    update_w = self.weights.assign_add(learning_rate * (w_positive_grad - w_negative_grad))\n\n    update_vb = self.v_bias.assign_add(learning_rate * tf.reduce_mean(visibles - v_end, 0))\n\n    update_hb = self.h_bias.assign_add(learning_rate * tf.reduce_mean(h_start - h_end, 0))\n\n    return [update_w, update_vb, update_hb]\n\ndef reconstruction_error(self, dataset):\n    \"\"\" The reconstruction cost for the whole dataset \"\"\"\n    err = tf.stop_gradient(dataset - self.gibbs_vhv(dataset)[1])\n    return tf.reduce_sum(err * err)\n```\n###### \n\nrbm_MNIST_test.py\n\nimport tensorflow as tf\nimport numpy as np\nimport rbm\nimport input_data\n\ndef build_model(X, w1, b1, wo, bo):\n    h1 = tf.nn.sigmoid(tf.matmul(X, w1)+b1)\n    model = tf.nn.sigmoid(tf.matmul(h1, wo)+bo)\n    return model\n\ndef init_weight(shape):\n    return tf.Variable(tf.random_normal(shape, mean=0.0, stddev=0.01))\n\ndef init_bias(dim):\n    return tf.Variable(tf.zeros([dim]))\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n\nX = tf.placeholder(\"float\", [None, 784])\nY = tf.placeholder(\"float\", [None, 10])\n\nrbm_layer = rbm.RBM(\"mnist\", 784, 500)\n\nfor i in range(10):\n    print \"RBM CD: \", i\n    rbm_layer.cd1(trX)\n\nrbm_w, rbm_vb, rbm_hb = rbm_layer.cd1(trX)\n\nwo = init_weight([500,10])\nbo = init_bias(10)\npy_x = build_model(X, rbm_w, rbm_hb, wo, bo)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\npredict_op = tf.argmax(py_x, 1)\n\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\n\nfor i in range(10):\n    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\n        sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n    print i, np.mean(np.argmax(teY, axis=1) ==\n                     sess.run(predict_op, feed_dict={X: teX, Y: teY}))\n###### \n\nbut here comes the error:\n\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1626, in as_graph_def\n    raise ValueError(\"GraphDef cannot be larger than 2GB.\")\nValueError: GraphDef cannot be larger than 2GB.\n\nCan someone help me to solve?\n", "comments": ["Looks like @keveman answered this question on StackOverflow, so I'll close this issue.\n"]}]