[{"number": 31735, "title": "Create build shell script to make installation and setup of TensorFlo\u2026", "body": "In this commit, II created a build.sh file which makes installing and setting up TensorFlow easier for beginners and pros. This file supports all computer platforms.", "comments": ["I am skeptic about this script, but I will read it more closely once I have more time.\r\nFor example, on windows running a \"bash\" script is a problem of its own. \r\n\r\n@angerson @yifeif to see if they have any comments.", "Also @mihaimaruseac as I see \"sudo\" calls in the script.", "I think this is better as a tutorial / file outside of the repo, as it currently makes several assumption about the environment it is going to run in (i.e: what happens if someone wants to use TensorFlow on Fedora? ArchLinux? Gentoo? NixOS?; `apt` won't work on either of these and the script would just be one extra file in the directory).", "Maybe I should change the script?", "Can one of the admins verify this patch?", "Ok, thanks, everyone! That's a good suggestion. I found this: https://www.tensorflow.org/install/source much better than my script. Until next time!"]}, {"number": 31734, "title": "Add int64 support for resource_gather", "body": "This PR tries to address the issue raised in #31696 where resource_gather on GPU does not support int64. The reason was that only half, float, double have been registered on GPU for ResourceGather kernel.\r\n\r\nThis PR registers additional kernel supports (no int32 though as int32 on GPU are emulated on CPU)\r\n\r\nThis fix fixes #31696.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang can you please check build failures ?", "Is this change available on tf-2.2?\r\n\r\nI'm still facing the issue.\r\n\r\n*** tensorflow.python.framework.errors_impl.NotFoundError: No registered 'ResourceGatherNd' OpKernel for 'GPU' devices compatible with node {{node ResourceGatherNd}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: Tindices=DT_INT32, dtype=DT_INT64\r\n\t.  Registered:  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_RESOURCE]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_RESOURCE]; Tindices in [DT_INT32]", "@mingruimingrui Can you open a new issue and provides details such as OS version, install, etc? That will help getting the issue resolved. You can ping me once a new issue is opened."]}, {"number": 31733, "title": "Error when subclassing for stock example: Expected D2 of index to be 2 got 3 at position 1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes. The code is derived from one of the integration test cases.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nStock example in Google Colab https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/keras/feature_columns.ipynb\r\n- TensorFlow version (use command below):\r\ntf.version.GIT_VERSION = v2.0.0-beta0-16-g1d91213fe7\r\ntf.version.VERSION = 2.0.0-beta1\r\n- Python version:\r\n3.6\r\n\r\n**Describe the current behavior**\r\nThe following `feature_column` creates the issue when subclassing `keras.models.Model`, which works alright with `sequential` API.\r\n```python\r\ncrossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\ncrossed_feature = feature_column.indicator_column(crossed_feature)\r\n```\r\n\r\n**Describe the expected behavior**\r\nNot failing with `feature_column.crossed_column` when subclassing. The integration test here, https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/feature_columns_integration_test.py#L161-L183, didn't catch this problem.\r\n\r\n**Code to reproduce the issue**\r\nRunning the following at the end of the notebook will reproduce the error.\r\n```python\r\nclass FeatureColumnDsModel(tf.keras.models.Model):\r\n    def __init__(self, feature_columns, **kwargs):\r\n        super().__init__(name=None, **kwargs)\r\n        self._input_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n        self._dense1 = tf.keras.layers.Dense(128, activation='relu')\r\n        self._dense2 = tf.keras.layers.Dense(128, activation='relu')\r\n        self._output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\r\n        \r\n    def call(self, features):\r\n        x = self._input_layer(features)\r\n        x = self._dense1(x)\r\n        x = self._dense2(x)\r\n        y = self._output_layer(x)\r\n        return y\r\n    \r\nmodel = FeatureColumnDsModel(feature_columns)\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'],)\r\n\r\nmodel.fit(train_ds,\r\n          validation_data=val_ds,\r\n          epochs=5)\r\n```\r\n\r\n**Other info / logs**\r\n```bash\r\nEpoch 1/5\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-27-fa952c9e276b> in <module>()\r\n     21 model.fit(train_ds,\r\n     22           validation_data=val_ds,\r\n---> 23           epochs=5)\r\n\r\n9 frames\r\n/tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    641         max_queue_size=max_queue_size,\r\n    642         workers=workers,\r\n--> 643         use_multiprocessing=use_multiprocessing)\r\n    644 \r\n    645   def evaluate(self,\r\n\r\n/tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/engine/training_generator.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    692         shuffle=shuffle,\r\n    693         initial_epoch=initial_epoch,\r\n--> 694         steps_name='steps_per_epoch')\r\n    695 \r\n    696   def evaluate(self,\r\n\r\n/tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    262 \r\n    263       is_deferred = not model._is_compiled\r\n--> 264       batch_outs = batch_function(*batch_data)\r\n    265       if not isinstance(batch_outs, list):\r\n    266         batch_outs = [batch_outs]\r\n\r\n/tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n    916       self._update_sample_weight_modes(sample_weights=sample_weights)\r\n    917       self._make_train_function()\r\n--> 918       outputs = self.train_function(ins)  # pylint: disable=not-callable\r\n    919 \r\n    920     if reset_metrics:\r\n\r\n/tensorflow-2.0.0b1/python3.6/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3508         value = math_ops.cast(value, tensor.dtype)\r\n   3509       converted_inputs.append(value)\r\n-> 3510     outputs = self._graph_fn(*converted_inputs)\r\n   3511 \r\n   3512     # EagerTensor.numpy() will often make a copy to ensure memory safety.\r\n\r\n/tensorflow-2.0.0b1/python3.6/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n    570       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n    571           list(kwargs.keys()), list(self._arg_keywords)))\r\n--> 572     return self._call_flat(args)\r\n    573 \r\n    574   def _filtered_call(self, args, kwargs):\r\n\r\n/tensorflow-2.0.0b1/python3.6/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    669     # Only need to override the gradient in graph mode and when we have outputs.\r\n    670     if context.executing_eagerly() or not self.outputs:\r\n--> 671       outputs = self._inference_function.call(ctx, args)\r\n    672     else:\r\n    673       self._register_gradient()\r\n\r\n/tensorflow-2.0.0b1/python3.6/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    443             attrs=(\"executor_type\", executor_type,\r\n    444                    \"config_proto\", config),\r\n--> 445             ctx=ctx)\r\n    446       # Replace empty list with None\r\n    447       outputs = outputs or None\r\n\r\n/tensorflow-2.0.0b1/python3.6/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError:  Expected D2 of index to be 2 got 3 at position 1\r\n\t [[node feature_column_ds_model/dense_features_7/age_bucketized_X_thal_indicator/SparseCross (defined at <ipython-input-27-fa952c9e276b>:23) ]] [Op:__inference_keras_scratch_graph_32547]\r\n\r\nFunction call stack:\r\nkeras_scratch_graph\r\n```", "comments": ["@Guzzii \r\nIs it possible for you to try latest TF versions (!pip install tf-nightly-2.0-preview==2.0.0.dev20190818)and let us know whether the issue persists? There were lots of performance improvements in the latest versions. I am not seeing the mentioned error in nightly versions.Thanks!", "Thanks for the reply. I am trying to see if the error still exists in the latest version. The following code block still doesn't import the tf-nightly.\r\n```python\r\ntry:\r\n    # %tensorflow_version only exists in Colab.\r\n    %tensorflow_version 2.x\r\nexcept Exception:\r\n    pass\r\n\r\nimport tensorflow as tf\r\n```\r\nWithout the `try except` block, it import version 1.12. Any suggestions on how to import the nightly build on colab? Thanks ", "@Guzzii \r\nI tried with nightly versions ('2.0.0-dev20190819') in colab and i am not seeing any error. I am able to reproduce the issue with TF 2.0.0-beta1 version. Please install  (`!pip install tf-nightly-2.0-preview==2.0.0.dev20190819)`and try again and let me know if the issue still persists. Thanks!\r\n![image](https://user-images.githubusercontent.com/51902062/63321485-83e93c00-c33e-11e9-9332-ac3d09d4ee46.png)\r\n", "This should be fixed with tf2 nightly.", "@Guzzii \r\nCan you try tf2-nighly and let us know if the issue still persists. Thanks!\r\n", "@ravikyram I confirm the problem is fixed in `tf-nightly-2.0-preview==2.0.0.dev20190819`.", "I am closing the issue since the query is been resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31733\">No</a>\n"]}, {"number": 31732, "title": "Tensorflow serving Batching configuration docs has bunch of broken links", "body": "\r\nBroken URL : https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration\r\n\r\nAll the link under batching configuration section return 404. \r\n\r\nExample links : \r\n\r\n1. https://www.tensorflow.org/tfx/batching/README#servers_with_multiple_models_model_versions_or_subtasks\r\n\r\n2. https://www.tensorflow.org/tfx/batching/README\r\n\r\n3. https://www.tensorflow.org/tfx/batching/README#batch_scheduling_parameters_and_tuning\r\n\r\n", "comments": ["@aaur0 Thanks for finding the issue.\r\nAs this is more related to TF serving, please post this in [TF serving repo](https://github.com/tensorflow/serving/issues). Thanks!", "@aaur0, Closing the issue, please post this issue in TF serving repository. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31732\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31732\">No</a>\n"]}, {"number": 31731, "title": "tf.linalg.sqrtm() returns nan for invertible matrix", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **windows 10**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **r1.14**\r\n- Python version: **3.6.8**\r\n- CUDA/cuDNN version: **No**\r\n\r\n**Describe the current behavior**\r\ntf.linalg.sqrtm() returns nan, even though input matrix is invertible. \r\nWhen the input matrix is constructed using numpy operations instead and then converted to tensor, the  behavior is as expected.\r\n\r\nThe issue can be reproduced with the following code, both .csv's can be downloaded from: https://www.kaggle.com/gemartin/world-bank-data-1960-to-2016/downloads/world-bank-data-1960-to-2016.zip/1\r\n\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\n\r\ndef load_word_bank_dataset():\r\n    \"\"\"\r\n    This function loads the World Bank Data and return it as NxD numpy arrays\r\n    \"\"\"\r\n    # Load and drop rows with missing values\r\n    fert_rate = pd.read_csv(r'./fertility_rate.csv').dropna()\r\n    life_exp = pd.read_csv(r'./life_expectancy.csv').dropna()\r\n\r\n    # Keep only countries that appears in both tables\r\n    valid_countries = list(set(fert_rate['Country Code']) & set(life_exp['Country Code']))\r\n    fert_rate = fert_rate[fert_rate['Country Code'].isin(valid_countries)]\r\n    life_exp = life_exp[life_exp['Country Code'].isin(valid_countries)]\r\n\r\n    # Create numpy arrays from the years columns\r\n    years_str_list = [str(year) for year in range(1960, 2017)]\r\n    fert_rate = fert_rate[years_str_list].to_numpy()\r\n    life_exp = life_exp[years_str_list].to_numpy()\r\n\r\n    return life_exp, fert_rate\r\n\r\ndef cca_loss(x1, x2, r1=0.0, r2=0.0):\r\n    # Extract dimensions\r\n    N, D1 = x1.shape\r\n    _, D2 = x2.shape\r\n    scale_mat = np.eye(N) - 1.0 / N * np.ones((N, N))\r\n    scale_factor = tf.cast(1. / N, dtype=tf.float32)\r\n\r\n    # This part doesn't work\r\n    scale_mat_tensor = tf.convert_to_tensor(scale_mat, dtype=tf.float32)\r\n    x1_T_tensor = tf.convert_to_tensor(x1.T, dtype=tf.float32)\r\n    h1_bar_1_tensor = tf.matmul(x1_T_tensor, scale_mat_tensor)\r\n    cov_11_1_tensor = scale_factor * tf.matmul(h1_bar_1_tensor, h1_bar_1_tensor, adjoint_b=True)\r\n    cov_11_1_sqrt = tf.linalg.sqrtm(cov_11_1_tensor)  # Returns nan's\r\n\r\n    # This part works\r\n    h1_bar_np = np.matmul(x1.T, scale_mat)\r\n    h1_bar_2_tensor = tf.convert_to_tensor(h1_bar_np, dtype=tf.float32)\r\n    cov_11_2_tensor = scale_factor * tf.matmul(h1_bar_2_tensor, h1_bar_2_tensor, adjoint_b=True)\r\n    cov_11_2_sqrt = tf.linalg.sqrtm(cov_11_2_tensor)  # Returns real valued matrix\r\n\r\n    return -1\r\n\r\nif __name__ == '__main__':\r\n    tf.compat.v1.enable_eager_execution()\r\n    # Load data\r\n    fertility_rate, life_expectancy = load_word_bank_dataset()\r\n    loss = cca_loss(fertility_rate, life_expectancy, r1=0., r2=0.)\r\n```", "comments": ["@itsikad ,\r\nCan you please provide the complete code necessary to reproduce the issue in editable format.Thanks", "@oanush \r\nThanks for your response.\r\nI added code to reproduce the issue.", "I continued investigating this issue, I believe the root cause is that the matrix is ill posed. It's minimal eigenvalue is 10^-6,  the decomposition algorithm that is used to calculate its square root might get small negative values or fail to converge.\r\nThe difference between the two pieces of code is that numpy calculation are with float64 and tf with float32.\r\nI'm closing this issue."]}, {"number": 31730, "title": "Create global C++/.h/.proto .clang-format", "body": "This commit creates a .clang-format file for global C++/.h/.proto .clang-format. This code can be changed later on. This just makes the TensorFlow project simpler on the fact of code formatting.\r\n\r\n### FORMAT OPTIONS\r\nStyle Guide: Google\r\nThe .clang-format file follows Google's style guide.", "comments": ["I think our problem before was, internally we have different versions of clang-tidy running. So when people ran clang-format, sometimes it may be incompatible with external clang-tidy.\r\n\r\n@yifeif you explored this a while back. Do you know if this config captures the commonalities of internal and external clang-tidy?", "Can one of the admins verify this patch?", "@yifeif @gunan What is the internal problem? I can fix my PR based on your suggestions.", "The problem is while externally a \"Google format\" is available, we discovered that it never matches internally. So any clang-format we tried before has failed the internal checks.", "We are now auto-formatting all code that is checked in. So for C++ code, whatever you do at home is fine, we'll auto-format anyway.\r\n\r\nUsing the Google style clang-format will work fine for that purpose, the differences are slight at best. \r\n\r\nI'd rather not check this into the repo though, because what will happen is that if you reformat code you didn't write, that'll cause larger than necessary diffs. \r\n\r\nFor that reason, I'll close this PR. \r\n\r\nThank you!", "Ok, sure, no problem!"]}, {"number": 31729, "title": "Make ConcreteFunction Compatible with Pickle", "body": "**System information**\r\n- TensorFlow version (you are using): `2.0.0-beta1`\r\n- Are you willing to contribute it (Yes/No): No \r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSee [issue 31421](https://github.com/tensorflow/tensorflow/issues/31421#issue-478107883) for more details. \r\n\r\nA `ConcreteFunction`, specifically, a `SavedModel`'s default serving `ConcreteFunction` that is used for `SavedModel` predictions. \r\n\r\n```python\r\nsaved_model = tf.saved_model.load(export_dir, tags=['serve'])\r\n\r\n# infer is not serializable (it cannot be pickled)\r\ninfer = saved_model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n```\r\n\r\nThe serialization error is \r\n```sh\r\n_pickle.PickleError: can't pickle repeated message fields, convert to list first\r\n```\r\n\r\n**Will this change the current api? How?**\r\nNot to my knowledge \r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to deploy a TF 2.0 `SavedModel` or use a `ConcreteFunction` in (Py)Spark or any other distributed system that relies on pickling user-defined functions \r\n\r\n**Any Other info.**\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/31421#issuecomment-522273057", "comments": ["@devstein,\r\nSorry for the delayed response. Can you please check the documentation for [Loading the Model with Signatures](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) let us know if the issue still persists? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 31728, "title": "New Version of NVIDIA CUDA (10.1 Update 2)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): master\r\n- Are you willing to contribute it (Yes/No): Yes, I have a system that supports CUDA 10.1u2 and am willing to test issues with it.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** NVIDIA has released the newest version of CUDA, 10.1u2, and I would like to know if anyone familiar with NVIDIA/CUDA programming knows of any major changes that we would need to make in the future. Otherwise, I am available to compile and test on a system with a newer NVIDIA card to see how it works.\r\n\r\n**Will this change the current api? How?** Not applicable.\r\n\r\n**Who will benefit with this feature?** Newer hardware support.\r\n\r\n**Any Other info.**\r\n", "comments": ["I can **confirm** that a build of TensorFlow, using the 2.0 API, works in the following configuration at commit b0fee96b1dd893fe3fea6b746cf1965c8ca9f114 (current HEAD of branch `r2.0`):\r\n* CUDA 10.1u2\r\n* cuDNN 7.6.2\r\n* TensorRT 5.1.5.0\r\n* NCCL 2.4.8\r\n* MKL-DNN 0.20\r\n* Bazel 0.24.1\r\n* GCC 8.2.1\r\n* Red Hat Enterprise Linux 8.0", "@nhubbard ,\r\nCan you confirm if the feature requested is working after the commit [b0fee96](https://github.com/tensorflow/tensorflow/commit/b0fee96b1dd893fe3fea6b746cf1965c8ca9f114) ?Thanks!", "It is working completely. Both the integrated tests and the tests that I use to test features for my use case are functioning as expected.", "Closing since the issue is resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31728\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31728\">No</a>\n"]}, {"number": 31727, "title": "Change http://mirror.tensorflow.org URL to HTTPS", "body": "", "comments": ["Changes have been merged internally , waiting for auto-merge to happen.\r\n\r\n"]}, {"number": 31726, "title": "Update HTTP mirror.tensorflow.org URL to HTTPS", "body": "This replaces http://mirror.tensorflow.org with https://storage.googleapis.com/mirror.tensorflow.org.", "comments": []}, {"number": 31725, "title": "Error loading tensorflow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.14.0\r\n- **Python version**: 3.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 10.1 - 10.1\r\n- **GPU model and memory**: Nvidia Quadro K5000 - 4Gb\r\n- **Exact command to reproduce**: import tensorflow as tf\r\n\r\n### Describe the problem\r\nError loading Tensorflow. Does not provide a clue about which DLL is failing to initialize.\r\n\r\n### Source code / logs\r\n> import tensorflow as tf\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Error en una rutina de inicializaci\u00f3n de biblioteca de v\u00ednculos din\u00e1micos (DLL).\r\nFailed to load the native TensorFlow runtime.\r\n\r\nCalling from a program:\r\n>python style.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Javier\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Javier\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Javier\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Error en una rutina de inicializaci\u00f3n de biblioteca de v\u00ednculos din\u00e1micos (DLL).\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"style.py\", line 9, in <module>\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Error en una rutina de inicializaci\u00f3n de biblioteca de v\u00ednculos din\u00e1micos (DLL).\r\nFailed to load the native TensorFlow runtime.", "comments": ["Solved here #13715", "> Solved here #13715\r\n\r\nI cannot see the solution. Cuda/Cudnn seems to be working fine. Can you help on the troubleshooting steps? ", "In conclusion, install this https://www.microsoft.com/en-us/download/details.aspx?id=48145", "TF 1.13 and above support cuda 10.0. Please rollback to cuda 10.0\r\nSee https://www.tensorflow.org/install/gpu#software_requirements\r\nAlso update your environment paths to cuda 10\r\nSee #31643", "I did rollback to cuda 10.0 and changed the paths. The problem persists. \r\n\r\nIs there a way to troubleshoot the issue and see what DLL is causing the error?", "> I did rollback to cuda 10.0 and changed the paths. The problem persists.\r\n> \r\n> Is there a way to troubleshoot the issue and see what DLL is causing the error?\r\n\r\nOpen ...\\Lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd use D[ependency Walker](http://www.dependencywalker.com/), it will show you the DLL dependency tree, you will find which DLL cause the problem. You can then install the needed CUDA or vcredist version, or add some path to system variable.\r\n\r\nHere is the dependency tree look like:\r\n![58224686-43831d80-7d51-11e9-9610-9fa5820ffa7e](https://user-images.githubusercontent.com/52397990/63266893-8e5bf500-c2ae-11e9-96ec-8377ee2de4a3.png)\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31725\">No</a>\n"]}, {"number": 31724, "title": "Tensorflow distribute strategy with custom layers gives an error when using Adam optimizer", "body": "`<em>Please` make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below):1.14.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am getting an error when I use a model with custom layers with trainable weights. It works fine without the distribute strategy, and also works fine if the custom layer is replaced by  an existing tf.Keras layer. The error appears to be related to Adam optimizer finding an empty var list when it tries to colocate variables. \r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n`\r\n      \r\n     \r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@capilano Can you please attach a minimal code snippet which can reproduce this issue?", "This was related to my weights not being created properly when using self.add_variable inside a subclassed Keras layer. I was using adam via tensorflow,  which was returning a weird error, but when I switched to the Keras version, I got a no gradients defined error,which I backtracked to self.add_variable not defining weights as I expected.\r\nMy weights were initialized something like this,\r\nself.std = tf.sqrt(2.) /tf.prod(shape[: -1]) ...(compile time constant,shape=[])\r\nself.custom_weight = self.add_weight(...) * self.std\r\nI changed this to \r\nself.custom_weight = tf.Variable(init_value * self.std) which solved the problem\r\nThe model gets created correctly,the weights are created and reported as trainable when I print the summary(both cases), but throws an error later when the training starts, I think because self.add_weight expects a constant and not a Tensor\r\n", "Can you please paste the complete code that you are running? And please paste the `weird` error logs also. Without that it will hard to say what's going on. @capilano ", "@srihari-humbarwadi  Like I mentioned in the post above, the error was due to the weights not being created properly, and thus there were no gradients to back propogate. \r\nI solved the problem using tf.Variable instead of layer.add_variable , although both are equivalent, the second one apparently does not work in some cases(like the example I posted above)\r\nWhen I posted the issue, I felt the problem was elsewhere,because there are no errors generated during the creation of the model and the summary does print out correct parameters. \r\nI think the issue can be closed, but if you need a code snippet, to reproduce the original issue, I can provide one.", "I am closing this issue since the query is been resolved. Please, feel free to reopen if the issue still persists.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31724\">No</a>\n"]}, {"number": 31723, "title": "TFLite Micro: initial ESP32 support", "body": "Ported the Arduino library to ESP32 platform.\r\n\r\nModified debug logging and examples 'hello_world' and 'micro_speech' to work with ESP32 peripherals.\r\n\r\nThe ESP32 runs FreeRTOS and the stack size of the default task is too small to fit several kb of model and buffers, so this implementation uses dynamic memory allocations and puts the model and audio buffers ('micro_speech' example) on the heap.\r\n\r\nUnfortunately there is no Makefile support by Espressif, so this will be an arduino library for now.\r\n\r\nThe code was developed on a lolin_d32_pro board with SPH0640 I2S microphone breakout board wired up to the following pins:\r\n\r\n- BCLK: 26\r\n- LRCL: 25\r\n- DOUT: 22", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31723) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31723) for more info**.\n\n<!-- ok -->", "Can one of the admins verify this patch?", "I see there has been a new commit e9a3aa1 introducing esp-idf support.\r\nUnfortunately this doesn't seem to address memory allocation either.", "@h3ndrik Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 31722, "title": "Get prediction values from tensorflow lite android", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: Samsung A3 2016\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.0/7.4\r\n- GPU model and memory: Nvidia Geforce 840m 3 Go\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI want to get values from output tensor with input an image to predict the eye region landmarks.\r\nI have predicted image from frozen graph model and Tensorflow lite using python script and it works fine.\r\nThe problem is with the Android platform, I got false and different values compared to values from python script\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nPython script\r\n\r\n```python\r\ndata = np.asarray( img, dtype=\"float32\" ) \r\n\r\n# Inference on input data normalized to [0, 1]\r\ninputImg = np.expand_dims(data,0).astype(np.float32)\r\ninput_details = interpreter.get_input_details()\r\ninterpreter.set_tensor(input_details[0]['index'], inputImg)\r\n\r\ninterpreter.invoke()\r\n\r\noutput_details = interpreter.get_output_details()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n\r\n```\r\n\r\nJava code ( Android Environment )\r\n```java\r\nprivate float[][][][] bitmapToInputArray() {\r\n        // [START mlkit_bitmap_input]\r\n        Bitmap bitmap= getYourInputImage();\r\n        bitmap = Bitmap.createScaledBitmap(bitmap, 112, 112, true);\r\n\r\n        int batchNum = 0;\r\n        float[][][][] input = new float[1][112][112][3];\r\n        for (int x = 0; x < 112; x++) {\r\n            for (int y = 0; y < 112; y++) {\r\n                int pixel = bitmap.getPixel(x, y);\r\n                // Normalize channel values to [-1.0, 1.0]. This requirement varies by\r\n                // model. For example, some models might require values to be normalized\r\n                // to the range [0.0, 1.0] instead.\r\n                input[batchNum][x][y][0] = (Color.red(pixel) - 127) / 128.0f;\r\n                input[batchNum][x][y][1] = (Color.green(pixel) - 127) / 128.0f;\r\n                input[batchNum][x][y][2] = (Color.blue(pixel) - 127) / 128.0f;\r\n                Log.i(\"Input\",\"input\"+input[batchNum][x][y]);\r\n\r\n            }\r\n        }\r\n        // [END mlkit_bitmap_input]\r\n\r\n      return input;\r\n    }\r\n\r\n private void useInferenceResult(float[] probabilities) throws IOException {\r\n        String[] result=new String[80];\r\n        String x=\"\";\r\n        String y=\"\";\r\n        ArrayList<Point> listpoint= new ArrayList<Point>();\r\n\r\n        double viewWidth = canvas.getWidth();\r\n        double viewHeight = canvas.getHeight();\r\n        double imageWidth = mutableBitmap.getWidth();\r\n        double imageHeight = mutableBitmap.getHeight();\r\n        Log.i(\"viewWidth\",\"viewwidth \"+viewWidth);\r\n        Log.i(\"viewHeight\",\"viewheight \"+viewHeight);\r\n        Log.i(\"imagewidth\",\"imagewidth \"+imageWidth);\r\n        Log.i(\"imaageHeigh\",\"imageheigh \"+imageHeight);\r\n        double scale = Math.min(viewWidth / imageWidth, viewHeight / imageHeight);\r\n        Log.i(\"Scale\",\"Scale\"+scale);\r\n        try {\r\n            for (int i = 0; i < probabilities.length; i++) {\r\n\r\n                Log.i(\"MLKit\", String.format(\"%1.8f\", probabilities[i]));\r\n                float i1 = probabilities[i];\r\n                Log.i(\"floaaat\", \"\" + i1);\r\n            }\r\n        }\r\n```\r\n\r\n\r\n\r\n**Other info / logs**\r\n+ That what I should get : ( this is from python script using Tensorflow lite model ):\r\n\r\n>[[0.3323875  0.19654518 0.3430611  0.17367488 0.3671013  0.16478491\r\n  0.39022946 0.17491257 0.399131   0.19814822 0.3900888  0.22294888\r\n  0.36448467 0.23006389 0.34123936 0.21979642 0.30286688 0.20466375\r\n  0.31792122 0.19202346 0.33909258 0.18460245 0.3652809  0.18118384\r\n  0.39127237 0.18584773 0.40995973 0.19605562 0.42233318 0.21089557\r\n  0.40651116 0.2182863  0.38454503 0.22073016 0.3601514  0.22152397\r\n  0.3375371  0.21898884 0.31742495 0.2137229  0.628366   0.21445222\r\n  0.63888156 0.19114998 0.6626687  0.18333182 0.6856534  0.1955274\r\n  0.69444335 0.22068903 0.6838143  0.2438955  0.66011566 0.2518409\r\n  0.63507193 0.2414121  0.60496974 0.22573914 0.6209617  0.21241865\r\n  0.6422507  0.20397455 0.6683734  0.20254172 0.6917013  0.2092008\r\n  0.7084733  0.21974358 0.7189961  0.23459744 0.70730895 0.24100597\r\n  0.69019526 0.24282111 0.6691431  0.24188581 0.6451422  0.23857626\r\n  0.6218671  0.2344214 ]]\r\n\r\n+ Those values are from Android Studio ( java using Log.i) :\r\n\r\n\r\n>2019-08-17 14:47:50.617 21349-21349/com.example.irisdetection I/MLKit: 0,23961355\r\n2019-08-17 14:47:50.620 21349-21349/com.example.irisdetection I/MLKit: 0,25104424\r\n2019-08-17 14:47:50.621 21349-21349/com.example.irisdetection I/MLKit: 0,28179651\r\n2019-08-17 14:47:50.622 21349-21349/com.example.irisdetection I/MLKit: 0,31467810\r\n2019-08-17 14:47:50.623 21349-21349/com.example.irisdetection I/MLKit: 0,33257431\r\n2019-08-17 14:47:50.624 21349-21349/com.example.irisdetection I/MLKit: 0,32645294\r\n2019-08-17 14:47:50.625 21349-21349/com.example.irisdetection I/MLKit: 0,29138848\r\n2019-08-17 14:47:50.626 21349-21349/com.example.irisdetection I/MLKit: 0,25581932\r\n2019-08-17 14:47:50.627 21349-21349/com.example.irisdetection I/MLKit: 0,19593856\r\n2019-08-17 14:47:50.628 21349-21349/com.example.irisdetection I/MLKit: 0,21698779\r\n2019-08-17 14:47:50.631 21349-21349/com.example.irisdetection I/MLKit: 0,24266151\r\n2019-08-17 14:47:50.632 21349-21349/com.example.irisdetection I/MLKit: 0,27562365\r\n2019-08-17 14:47:50.633 21349-21349/com.example.irisdetection I/MLKit: 0,30823168\r\n2019-08-17 14:47:50.635 21349-21349/com.example.irisdetection I/MLKit: 0,33465266\r\n2019-08-17 14:47:50.636 21349-21349/com.example.irisdetection I/MLKit: 0,35355449\r\n2019-08-17 14:47:50.637 21349-21349/com.example.irisdetection I/MLKit: 0,34009647\r\n2019-08-17 14:47:50.638 21349-21349/com.example.irisdetection I/MLKit: 0,31358159\r\n2019-08-17 14:47:50.640 21349-21349/com.example.irisdetection I/MLKit: 0,28156102\r\n2019-08-17 14:47:50.642 21349-21349/com.example.irisdetection I/MLKit: 0,25063315\r\n2019-08-17 14:47:50.643 21349-21349/com.example.irisdetection I/MLKit: 0,21878451\r\n2019-08-17 14:47:50.644 21349-21349/com.example.irisdetection I/MLKit: 0,69623101\r\n2019-08-17 14:47:50.646 21349-21349/com.example.irisdetection I/MLKit: 0,70167470\r\n2019-08-17 14:47:50.646 21349-21349/com.example.irisdetection I/MLKit: 0,73317540\r\n2019-08-17 14:47:50.648 21349-21349/com.example.irisdetection I/MLKit: 0,76974392\r\n2019-08-17 14:47:50.649 21349-21349/com.example.irisdetection I/MLKit: 0,79195201\r\n2019-08-17 14:47:50.651 21349-21349/com.example.irisdetection I/MLKit: 0,78359401\r\n2019-08-17 14:47:50.652 21349-21349/com.example.irisdetection I/MLKit: 0,75674009\r\n2019-08-17 14:47:50.653 21349-21349/com.example.irisdetection I/MLKit: 0,71786618\r\n2019-08-17 14:47:50.654 21349-21349/com.example.irisdetection I/MLKit: 0,66782737\r\n2019-08-17 14:47:50.655 21349-21349/com.example.irisdetection I/MLKit: 0,68930006\r\n2019-08-17 14:47:50.656 21349-21349/com.example.irisdetection I/MLKit: 0,71668541\r\n2019-08-17 14:47:50.657 21349-21349/com.example.irisdetection I/MLKit: 0,75279719\r\n2019-08-17 14:47:50.658 21349-21349/com.example.irisdetection I/MLKit: 0,78872705\r\n2019-08-17 14:47:50.659 21349-21349/com.example.irisdetection I/MLKit: 0,81867975\r\n2019-08-17 14:47:50.661 21349-21349/com.example.irisdetection I/MLKit: 0,83806717\r\n2019-08-17 14:47:50.662 21349-21349/com.example.irisdetection I/MLKit: 0,82371044\r\n2019-08-17 14:47:50.664 21349-21349/com.example.irisdetection I/MLKit: 0,79749656\r\n2019-08-17 14:47:50.665 21349-21349/com.example.irisdetection I/MLKit: 0,76317006\r\n2019-08-17 14:47:50.666 21349-21349/com.example.irisdetection I/MLKit: 0,72700304\r\n2019-08-17 14:47:50.667 21349-21349/com.example.irisdetection I/MLKit: 0,69159627\r\n\r\nHow can I solve this problem??\r\n", "comments": ["A common issue with train/eval skew is the difference in pre-processing. Have you compared the inputs to the model if they're the same?", "The input is an image... \r\nFor Android, I have copied the image and make them on the drawable folder. \r\nI don't have any idea how to compare the input.\r\nI tried print values of the image array, but I get very different values, I'm not sure if this comparison is true or no.\r\nPlease see this new issue that I have created for more informations.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31722\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31722\">No</a>\n"]}, {"number": 31721, "title": "[lite] failed on  define=with_select_tf_ops=true", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code \r\n\u2019 \r\ncc_binary(\r\n    name = \"Prophet.so\",\r\n    srcs = [\"minimal.h\", \"Prophet.cc\",   ],\r\n    linkopts = tflite_linkopts() + select({\r\n        \"//tensorflow:android\": [\r\n            \"-pie\",  # Android 5.0 and later supports only PIE\r\n            \"-lm\",  # some builtin ops, e.g., tanh, need -lm\r\n        ],\r\n        \"//conditions:default\": [],\r\n    }),\r\n    deps = [\r\n        \"//tensorflow/lite:framework\",\r\n        \"//tensorflow/lite/delegates/flex:delegate\", [ on | off]\r\n        \"//tensorflow/lite/c:c_api_internal\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n        \"//tensorflow/lite:builtin_op_data\",\r\n        \"//tensorflow/lite/schema:schema_fbs\",\r\n    ],\r\n    linkshared = True,\r\n)\r\n- OS Platform and Distribution (vmware Linux Ubuntu 18.04 x4 on window10x64):\r\n- TensorFlow installed from (source  ):github 20190817\r\n- TensorFlow version (): 1.14.0\r\n- Python version:3.7.4\r\n- Bazel version (if compiling from source):0.26\r\n- GCC/Compiler version (if compiling from source): 7.4 binary\r\n- CUDA/cuDNN version: cpu\r\n- GPU model and memory: cpu\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\r\n**Describe the current behavior**\r\nTarget //tensorflow/lite/examples/l17:Prophet.so failed to build\r\n**Describe the expected behavior**\r\nbuild successed\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n bazel build --config monolithic --cxxopt=-std=c++11 -c opt  \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n--define=with_select_tf_ops=true   --cpu=armeabi-v7a  --verbose_failures\\\r\n  //tensorflow/lite/examples/l17:Prophet.so\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nbazel cmd log\r\n`\r\nERROR: /root/tensorflow-master/tensorflow/core/kernels/BUILD:6576:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 254): clang failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/510cb3499e6983b92a09020af9102ff3/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=29.0.2 \\\r\n    ANDROID_NDK_API_LEVEL=18 \\\r\n    ANDROID_NDK_HOME=/root/vender/android-ndk-r17c \\\r\n    ANDROID_SDK_API_LEVEL=18 \\\r\n    ANDROID_SDK_HOME=/root/vender/android-sdk \\\r\n    PATH='~/anaconda3/bin:/root/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin' \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/root/anaconda3/bin/python \\\r\n    PYTHON_LIB_PATH=/root/anaconda3/lib/python3.7/site-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang '-D__ANDROID_API__=18' -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/cwise_op_not_equal_to_2.pic.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/cwise_op_not_equal_to_2.pic.o' -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -iquote . -iquote bazel-out/armeabi-v7a-opt/bin -iquote external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/armeabi-v7a-opt/bin/external/nsync -iquote external/com_google_protobuf -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_protobuf -iquote external/zlib_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/zlib_archive -iquote external/eigen_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-v7a-opt/bin/external/local_config_sycl -iquote external/double_conversion -iquote bazel-out/armeabi-v7a-opt/bin/external/double_conversion -iquote external/farmhash_archive -iquote bazel-out/armeabi-v7a-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/armeabi-v7a-opt/bin/external/fft2d -iquote external/gemmlowp -iquote bazel-out/armeabi-v7a-opt/bin/external/gemmlowp -isystem external/nsync/public -isystem bazel-out/armeabi-v7a-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/armeabi-v7a-opt/bin/external/com_google_protobuf/src -isystem external/zlib_archive -isystem bazel-out/armeabi-v7a-opt/bin/external/zlib_archive -isystem external/eigen_archive -isystem bazel-out/armeabi-v7a-opt/bin/external/eigen_archive -isystem external/double_conversion -isystem bazel-out/armeabi-v7a-opt/bin/external/double_conversion -isystem external/farmhash_archive/src -isystem bazel-out/armeabi-v7a-opt/bin/external/farmhash_archive/src '-std=c++11' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-mfpu=neon' -DTENSORFLOW_MONOLITHIC_BUILD -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-18/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/core/kernels/cwise_op_not_equal_to_2.cc -o bazel-out/armeabi-v7a-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/cwise_op_not_equal_to_2.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nclang: error: unable to execute command: Killed\r\nclang: error: clang frontend command failed due to signal (use -v to see invocation)\r\nAndroid (4691093 based on r316199) clang version 6.0.2 (https://android.googlesource.com/toolchain/clang 183abd29fc496f55536e7d904e0abae47888fc7f) (https://android.googlesource.com/toolchain/llvm 34361f192e41ed6e4e8f9aca80a4ea7e9856f327) (based on LLVM 6.0.2svn)\r\nTarget: armv7-none-linux-android\r\nThread model: posix\r\nInstalledDir: external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin\r\nclang: note: diagnostic msg: PLEASE submit a bug report to http://llvm.org/bugs/ and include the crash backtrace, preprocessed source, and associated run script.\r\nclang: note: diagnostic msg:\r\n********************\r\n\r\nPLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:\r\nPreprocessed source(s) and associated run script(s) are located at:\r\nclang: note: diagnostic msg: /tmp/cwise_op_not_equal_to_2-9cbb9b.cpp\r\nclang: note: diagnostic msg: /tmp/cwise_op_not_equal_to_2-9cbb9b.sh\r\nclang: note: diagnostic msg:\r\n\r\n********************\r\nTarget //tensorflow/lite/examples/l17:Prophet.so failed to build\r\n\r\n`\r\n\r\n\r\n", "comments": ["It's interesting that it crashes at printf not `float re=output[0];`\r\n\r\nDoes it crash if you try to access to output[0] before invoke? And what's the value of output? Is it nullptr?", "foud type error.\r\nfail: \r\n       *result=interpreter->typed_output_tensor<int>(0)[0];  // fail\r\nok:\r\n        TfLiteTensor* outputTensor=interpreter->tensor(interpreter->outputs()[0]);\r\n        auto re=outputTensor->data.i32;\r\nwhy?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31721\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31721\">No</a>\n"]}, {"number": 31720, "title": "Only the first 30 ops will run on the GPU, and the remaining 1 on the CPU", "body": "I am building my codes with tensorflow_lite_gpu.framework on iPhone 7. The codes is just like Google recommends:\r\n\r\n```\r\n_model = FlatBufferModel::BuildFromFile(modelPathCString);\r\n        ops::builtin::BuiltinOpResolver resolver;\r\n        InterpreterBuilder(*_model, resolver)(&_interpreter);\r\n        _delegate = NewGpuDelegate(nullptr);  // default config\r\n        _interpreter->ModifyGraphWithDelegate(_delegate);\r\n...//Other codes\r\n```\r\nWhen I use the model mobilenet_v1_1.0_224.tflite which Google provides, I get warning: `WARNING: 25 cannot be handled by this delegate.  Only the first 30 ops will run on the GPU, and the remaining 1 on the CPU` from console after excuting `_interpreter->ModifyGraphWithDelegate(_delegate)`. But the model deeplabv3_257_mv_gpu.tflite is good which Google provides too. As for my own model, the result is like mobilenet_v1_1.0_224.tflite does. Someone please help me.\r\n\r\nPS: Here is my podfile: \r\n```\r\nplatform :ios, '10.0'\r\ntarget \"SpeechExample\" do\r\npod 'TensorFlowLiteGpuExperimental'\r\nend\r\n```", "comments": ["@LukeChow \r\n\r\n25 in the error message is probably referring to the op code 25, which is SOFTMAX.  It's been a while, but IIRC there's been some problems with SOFTMAX which should be solved by now.  However, the bad part is that there has been no recent cocoapod release with the fix in it.  Is it possible for you to try it out with the latest master branch of TF?", "> \r\n> 25 in the error message is probably referring to the op code 25, which is SOFTMAX. It's been a while, but IIRC there's been some problems with SOFTMAX which should be solved by now. However, the bad part is that there has been no recent cocoapod release with the fix in it. Is it possible for you to try it out with the latest master branch of TF?\r\n\r\n@impjdi \r\nI am trying to build Tensorflow master source code. By the way, with my own\r\n model, I get warning `WARNING: 82 cannot be handled by this delegate`, which is Reduce Max op defined in file builtin_ops.h. But my model does not contain this op. I uploaded an attachment that contains information of layers used in my model. Could you please help me. Thank you very much. [layers.txt](https://github.com/tensorflow/tensorflow/files/3524494/layers.txt)", "@impjdi Where can I get the operator op code list? I need it to debug my code.", "@yxchng you can take a look at `delegates/gpu/common/model_builder.cc`  There is a huge switch-case near the end of the file.", "@LukeChow Sorry, this thread dropped off my radar.  It's possible that TOCO replaces certain graph patterns with a different op.", "@LukeChow,\r\n\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to `2.6` which is latest stable version of TF and let us know if the issue still persists in newer versions.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31720\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31720\">No</a>\n"]}, {"number": 31719, "title": "keras.backend.gradients shows error as tf.gradients", "body": "**System information**\r\ntensorflow 2.0b1\r\nwindows 10\r\nanaconda python 3.7\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n![20190817164347](https://user-images.githubusercontent.com/27112868/63209122-87e34700-c10f-11e9-8040-38dbdda1d351.png)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.layers as layers\r\nimport tensorflow.keras.backend as K\r\n\r\ntf.keras.backend.set_floatx('float64')\r\n\r\n\r\n\r\ndef grad( y, x ):\r\n   V =  tf.keras.layers.Lambda( lambda z: K.gradients( z[ 0 ], z[ 1 ] ), output_shape = [1] )( [ y, x ] )\r\n   return V\r\n\r\n\r\n\r\nfixed_input = keras.Input( shape=(2,) )\r\n\r\na = fixed_input * fixed_input\r\n\r\nb = grad( a, fixed_input )\r\n\r\n\r\nmodel = keras.Model( inputs = [ fixed_input ], outputs = [ b ] )\r\n\r\n\r\n\r\nc = model( tf.constant(2.0, dtype=tf.float64))\r\n```\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nrunfile('F:/2019/temp1.py', wdir='F:/2019')\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0817 16:46:12.144755  7996 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>>: AssertionError: \r\nReloaded modules: tmpd51tclm6\r\nWARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>>: AssertionError: \r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-763868c416b6>\", line 1, in <module>\r\n    runfile('F:/2019/temp1.py', wdir='F:/2019')\r\n\r\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 826, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"F:/2019/temp1.py\", line 28, in <module>\r\n    c = model( tf.constant(2.0, dtype=tf.float64))\r\n\r\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 712, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 753, in call\r\n    return self._run_internal_graph(inputs, training=training, mask=mask)\r\n\r\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 895, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n\r\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 712, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 785, in call\r\n    return self.function(inputs, **arguments)\r\n\r\n  File \"F:/2019/temp1.py\", line 12, in <lambda>\r\n    V =  tf.keras.layers.Lambda( lambda z: K.gradients( z[ 0 ], z[ 1 ] ), output_shape = [1] )( [ y, x ] )\r\n\r\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3568, in gradients\r\n    loss, variables, colocate_gradients_with_ops=True)\r\n\r\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 158, in gradients\r\n    unconnected_gradients)\r\n\r\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\", line 504, in _GradientsHelper\r\n    raise RuntimeError(\"tf.gradients is not supported when eager execution \"\r\n\r\nRuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\r\n", "comments": ["Hi @zwenju, it's because you are using functional API so the standard way to get the output from functional API is using `model.predict`. If you call the model via `__call__` the model will run eagerly, thus get the errors. So the workable example should be:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.layers as layers\r\nimport tensorflow.keras.backend as K\r\n\r\ntf.keras.backend.set_floatx('float64')\r\n\r\n\r\ndef grad(y, x):\r\n    V = layers.Lambda(lambda z: K.gradients(\r\n        z[0], z[1]), output_shape=[1])([y, x])\r\n    return V\r\n\r\n\r\nfixed_input = keras.Input(shape=(2,))\r\na = fixed_input * fixed_input\r\nb = grad(a, fixed_input)\r\n\r\nmodel = keras.Model(inputs=[fixed_input], outputs=[b])\r\n\r\n# c = model(tf.constant(2.0, dtype=tf.float64))\r\nc = model.predict(tf.constant([[2.0, 1.0]], dtype=tf.float64))\r\nprint(c)\r\n```\r\n\r\nIf you do want to subclass a `tf.keras.Model` instead of functional API for flexibility, you could obtain the gradient in this way:\r\n```python\r\nclass Toy(tf.keras.Model):\r\n    def call(self, x):\r\n        return x * x\r\n\r\ntoy = Toy()\r\n\r\nx = tf.constant([2.0, 1.0], dtype=tf.float64)\r\nwith tf.GradientTape() as tape:\r\n    # Remember to watch non-variable tensor\r\n    tape.watch(x)\r\n    y = toy(x)\r\nprint(tape.gradient(y, x))\r\n```\r\n\r\nThanks", "Great Thanks\uff01\r\nI tested you solutions, It works well.\r\n"]}, {"number": 31718, "title": "Fix VarianceScaling initializer for warmstart", "body": "To replace https://github.com/tensorflow/tensorflow/pull/31648 which was incorrectly submitted for r1.14.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31718) for more info**.\n\n<!-- need_author_cla -->", "How do I fix the cla/google check?", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31718) for more info**.\n\n<!-- cla_yes -->", "Can one of the admins verify this patch?", "For old releases we are only accepting patches for security vulnerabilities, as they are the only ones that cause new (patch) releases to be made. As such, we cannot accept PRs which are not against master (and cherry-pick PRs against the main development branch for a new release).\r\n\r\nClosing this PR accordingly.", "> For old releases we are only accepting patches for security vulnerabilities, as they are the only ones that cause new (patch) releases to be made. As such, we cannot accept PRs which are not against master (and cherry-pick PRs against the main development branch for a new release).\r\n> \r\n> Closing this PR accordingly.\r\n\r\nSame problem exists in r1.14. Care to make a similar patch?", "Unfortunately, since this PR is not a security vulnerability, it cannot go into r1.14 either.\r\n\r\nInstead, this fix should be made against master, so it can be picked up on the future releases. As there is a branch cut for r2.0 (and soon r1.15), you might then decide to cherry-pick the PR against master to each of those two branches and the release owners can decide if they want the patch or not."]}, {"number": 31717, "title": "TF-TRT API: Add build method. Not return function from convert. Calibrate in convert.", "body": "- Add TrtGraphConverterV2.build method. Every time this function is called, new TRT engines get built in the cache if needed.\r\n- Do not return function from convert. Exposing the function currently is not safe due to save() method.\r\n- Calibrate in convert method.\r\n- Update tests accordingly.", "comments": ["Changes have been merged internally , waiting for auto-merge to happen."]}, {"number": 31716, "title": "add mean absolute percentage error metric for estimator", "body": "MAPE(mean absolute percentage error) is widely used in many machine learning tasks. Even though this is a implementation under `tf.keras.metrics` , it's not compatible with `tf.Estimator`. I added my implementation, hope it is useful for others too.", "comments": ["tf contrib will not make it into any future releases of tensorflow, so closing this PR."]}, {"number": 31715, "title": "2.0 cherry-pick request: tf.data fix", "body": "Add non-determinstic seed code path for RandomSeedGenerator to match TF 1.X behavior.\r\n\r\nFixes: #31706\r\nPiperOrigin-RevId: 263878374", "comments": []}, {"number": 31714, "title": "add mean absolute percentage error metric for estimator", "body": "MAPE (mean absolute percentage error) is widely used in machine learning tasks. Even though 'tf.keras' already has the implementation of MAPE, it's not compatible with `tf.estimator`  metric.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31714) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31714) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 31713, "title": "Add API docs for LMDBDataset", "body": "This PR adds documentation to the API definition for `tf.data.experimental.LMDBDataset`. I documented the input argument and added a summary and a long-form `description` field.", "comments": ["Can one of the admins verify this patch?", "@jsimsa can you please help review this PR"]}, {"number": 31712, "title": "r2.0 cherry-pick request: [Intel MKL] Upgrading curl to 7.65.3 to fix CVE-2019-5443", "body": "Description from the original PR https://github.com/tensorflow/tensorflow/pull/31675:\r\n\r\n# [CVE-2019-5443](https://nvd.nist.gov/vuln/detail/CVE-2019-5443)\r\n\r\n**NVD:** 2019/07/02 - CVSS v2.0 Base Score: 4.6 - CVSS v3.0 Base Score: 7.8\r\n\r\n`A non-privileged user or program can put code and a config file in a known non-privileged path (under C:/usr/local/) that will make curl <= 7.65.1 automatically run the code (as an openssl \"engine\") on invocation. If that curl is invoked by a privileged user it can do anything it wants.`\r\n\r\n## References to Advisories, Solutions, and Tools\r\nSource | Link | Type\r\n---- | ---- | ----\r\nMLIST | [www.openwall.com](http://www.openwall.com/lists/oss-security/2019/06/24/1) | Mailing List, Patch, Third Party Advisory\r\nBID | [www.securityfocus.com](http://www.securityfocus.com/bid/108881) | Third Party Advisory, VDB Entry\r\nMISC | [curl.haxx.se](https://curl.haxx.se/docs/CVE-2019-5443.html) | Patch, Vendor Advisory\r\n\r\n", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31712) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31712) for more info**.\n\n<!-- cla_yes -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31712) for more info**.\n\n<!-- need_author_consent -->", "Manually setting the CLA to yes again since the other commit that I didn't author is already merged in the master branch.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31712) for more info**.\n\n<!-- cla_yes -->", "@googlebot I consent."]}, {"number": 31711, "title": "pip3 install tensorflow -> Packages Do Not Match Hashes from Requirements File", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster 7/10\r\n- Raspberry Pi 4\r\n- TensorFlow installed from (source or binary): Installed using wheels\r\n- TensorFlow version: 1.13 or whatever is latest on piwheels\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: PIP\r\n\r\n> THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\r\n>     h5py from https://www.piwheels.org/simple/h5py/h5py-2.9.0-cp37-cp37m-linux_armv7l.whl#sha256=38657cba8b4689142e203ac4cac0b3f64dfe2f5634067f956dbd3abed7ea3a0a (from keras-applications>=1.0.8->tensorflow):\r\n>         Expected sha256 38657cba8b4689142e203ac4cac0b3f64dfe2f5634067f956dbd3abed7ea3a0a\r\n>              Got        2feb5218ab87862f2367a335f4cc315c87fc397c3534e10c08d723cfaeda3a15\r\n> \r\n\r\nI did a basic PIP install:\r\n\r\n`pip3 install tensorflow`\r\n\r\nIt seems there's a mismatch in the info regarding it installing TensorFlow 1.13 but mentioning Tensorboard 1.14???\r\n\r\n> pip3 install tensorflow\r\n> Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\n> Collecting tensorflow\r\n>   Downloading https://www.piwheels.org/simple/tensorflow/tensorflow-1.13.1-cp37-none-linux_armv7l.whl (93.2MB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 93.2MB 5.1kB/s \r\n> Collecting absl-py>=0.7.0 (from tensorflow)\r\n>   Downloading https://www.piwheels.org/simple/absl-py/absl_py-0.7.1-py3-none-any.whl (117kB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 122kB 238kB/s \r\n> Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\r\n>   Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.2MB 162kB/s \r\n> Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.32.3)\r\n> Collecting keras-preprocessing>=1.0.5 (from tensorflow)\r\n>   Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 1.8MB/s \r\n> Collecting wrapt>=1.11.1 (from tensorflow)\r\n>   Downloading https://www.piwheels.org/simple/wrapt/wrapt-1.11.2-cp37-cp37m-linux_armv7l.whl (68kB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 205kB/s \r\n> Collecting grpcio>=1.8.6 (from tensorflow)\r\n>   Downloading https://www.piwheels.org/simple/grpcio/grpcio-1.23.0-cp37-cp37m-linux_armv7l.whl (13.7MB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13.7MB 38kB/s \r\n> Collecting protobuf>=3.6.1 (from tensorflow)\r\n>   Downloading https://files.pythonhosted.org/packages/0d/2e/d4b1b67c264ce6722def110f2715461e9b4d49647952750c82c6b247bfa6/protobuf-3.9.1-py2.py3-none-any.whl (432kB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 440kB 644kB/s \r\n> Collecting astor>=0.6.0 (from tensorflow)\r\n>   Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\r\n> Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.12.0)\r\n> Collecting keras-applications>=1.0.8 (from tensorflow)\r\n>   Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 3.6MB/s \r\n> Collecting termcolor>=1.1.0 (from tensorflow)\r\n>   Downloading https://www.piwheels.org/simple/termcolor/termcolor-1.1.0-py3-none-any.whl\r\n> Collecting gast>=0.2.0 (from tensorflow)\r\n>   Downloading https://www.piwheels.org/simple/gast/gast-0.2.2-py3-none-any.whl\r\n> Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\r\n>   Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 491kB 598kB/s \r\n> Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.2)\r\n> Collecting google-pasta>=0.1.6 (from tensorflow)\r\n>   Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 4.2MB/s \r\n> Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\r\n>   Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\r\n>     100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 2.9MB/s \r\n> Requirement already satisfied: werkzeug>=0.11.15 in /usr/lib/python3/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\r\n> Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.6.1->tensorflow) (40.8.0)\r\n> Collecting h5py (from keras-applications>=1.0.8->tensorflow)\r\n>   Downloading https://www.piwheels.org/simple/h5py/h5py-2.9.0-cp37-cp37m-linux_armv7l.whl (4.6MB)\r\n>     75% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        | 3.4MB 945kB/s eta 0:00:02\r\n> ", "comments": ["@AeroWRX Just to verify, did you follow the instructions mentioned in the Tensorflow official [website](https://www.tensorflow.org/install/pip). Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31711\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31711\">No</a>\n"]}, {"number": 31710, "title": "r2.0-CherryPick:Fix unwanted deprecation warning in optimizers v2", "body": "Get rid of unwanted deprecation warning in optimizer v2.\r\n\r\nCurrently, optimizers need to access variable.constraint. Doing so raises a user-visible deprecation warning. As a result, every time an optimizer is used in TF 2.0, a deprecation warning is raised, which is not an acceptable user-facing behavior.", "comments": []}, {"number": 31709, "title": "Tensorflow 2.0 is much slower than pytorch for large matrix assignment", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow 2.0 beta\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0/cuDNN 7\r\n- GPU model and memory: GeForce RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nIn one of my research, I need to assign values to large sparse matrices. Since tensorflow does not support value assignment/update to large matrices , I have to use a lot of tf.stack()/tf.concat() functions. \r\n\r\nI compared the same function implemented in tensorflow 2.0 and pytorch 1.1.0, and the execution time for tensorflow was much slower than pytorch. The execution times are listed below:\r\npytorch: 0.0036 secs\r\ntf 2.0    : 0.1734 secs\r\n\r\n**Describe the expected behavior**\r\nIs there a way to optimize the tensorflow codes to have comparable performance?\r\n\r\n**Code to reproduce the issue**\r\n```\r\n## Tensorflow 2.0\r\nimport tensorflow as tf\r\nimport time\r\nimport numpy as np\r\n\r\ndef skew(x):\r\n    x = tf.reshape(x, [-1, 1])\r\n    z1 = tf.stack([tf.zeros(1), -x[2], x[1]], 1)\r\n    z2 = tf.stack([x[2], tf.zeros(1), -x[0]], 1)\r\n    z3 = tf.stack([-x[1], x[0], tf.zeros(1)], 1)\r\n    X = tf.concat([z1, z2, z3], 0)\r\n    return X\r\n\r\ndef propagate(Rot, v, p, g):\r\n    v_skew_rot = tf.matmul(skew(v), Rot)\r\n    p_skew_rot = tf.matmul(skew(p), Rot)\r\n\r\n    F0 = tf.zeros([3, 3])\r\n    F1 = tf.concat([F0, skew(g), F0], 0)\r\n    F2 = tf.concat([F0, F0, tf.eye(3)], 0)\r\n    F3 = tf.zeros([9, 3])\r\n    F4 = tf.concat([-Rot, -v_skew_rot, -p_skew_rot], 0)\r\n    F5 = tf.concat([F0, -Rot, F0], 0)\r\n    F6 = tf.zeros([9, 6])\r\n    F7 = tf.concat([F1, F2, F3, F4, F5, F6], 1)\r\n    F = tf.concat([F7, tf.zeros([12, 21])], 0)\r\n\r\n    G0 = tf.zeros([3, 12])\r\n    G1 = tf.concat([Rot, v_skew_rot, p_skew_rot, tf.zeros([12, 3])], 0)\r\n    G2 = tf.concat([F0, Rot, F0, tf.zeros([12, 3])], 0)\r\n    G3 = tf.concat([G0, G0, G0, tf.eye(12)], 0)\r\n    G = tf.concat([G1, G2, G3], 1)\r\n    return F, G\r\n\r\nif __name__ == '__main__':\r\n    Rot = tf.eye(3)\r\n    v = np.array([0.5, 0, 0], dtype=np.float32)\r\n    p = np.array([1.5, 0, 0], dtype=np.float32)\r\n    g = np.array([0, 0, -9.80655], dtype=np.float32)\r\n    start = time.time()\r\n    P = propagate(Rot, v, p, g)\r\n    print(\"Propagate function takes {} secs\".format(time.time() - start))\r\n```\r\n\r\n```\r\n## pytorch 1.1.0\r\nimport torch\r\nimport time\r\nimport numpy as np\r\n\r\n\r\ndef skew(x):\r\n    X = torch.Tensor([[0, -x[2], x[1]],\r\n                      [x[2], 0, -x[0]],\r\n                      [-x[1], x[0], 0]])\r\n    return X\r\n\r\ndef propagate(Rot_prev, v_prev, p_prev, g):\r\n    F = torch.zeros(21, 21)\r\n    G = torch.zeros(21, 18)\r\n    v_skew_rot = skew(v_prev).mm(Rot_prev)\r\n    p_skew_rot = skew(p_prev).mm(Rot_prev)\r\n\r\n    F[:3, 9:12] = -Rot_prev\r\n    F[3:6, :3] = skew(g)\r\n    F[6:9, 3:6] = torch.eye(3)\r\n    F[3:6, 12:15] = -Rot_prev\r\n    F[3:6, 9:12] = -v_skew_rot\r\n    F[6:9, 9:12] = -p_skew_rot\r\n\r\n    G[:3, :3] = Rot_prev\r\n    G[3:6, 3:6] = Rot_prev\r\n    G[3:6, :3] = v_skew_rot\r\n    G[6:9, :3] = p_skew_rot\r\n    G[9:12, 6:9] = torch.eye(3)\r\n    G[12:15, 9:12] = torch.eye(3)\r\n    G[15:18, 12:15] = torch.eye(3)\r\n    G[18:21, 15:18] = torch.eye(3)\r\n    return F, G\r\n\r\nif __name__ == '__main__':\r\n    Rot = torch.eye(3)\r\n    v = np.array([0.5, 0, 0], dtype=np.float32)\r\n    p = np.array([1.5, 0, 0], dtype=np.float32)\r\n    g = np.array([0, 0, -9.80655], dtype=np.float32)\r\n    start = time.time()\r\n    P = propagate(Rot, v, p, g)\r\n    print(\"Propagate function takes {} secs\".format(time.time() - start))\r\n```", "comments": ["I have tried on colab with TF version 2.0 beta1 , and nightly version 2.0.0.dev20190818 was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1CkKJmRKzZPAp7sQrXmzzGnp73iLda6Uv) here.Thanks!", "You can use tf.tensor_scatter_nd_add etc to update slices without concats, which are dog slow.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31709\">No</a>\n", "> You can use tf.tensor_scatter_nd_add etc to update slices without concats, which are dog slow.\r\n\r\n@alextp thanks for your suggestion, but I think this may not be a proper solution for this issue. In this particular case, I need to assign several small matrices (e.g. 3x3) to a large matrix (e.g. 21x21). Something similar to this:\r\n`\r\nF[:3, 9:12] = -Rot_prev\r\n    F[3:6, :3] = skew(g)\r\n    F[6:9, 3:6] = torch.eye(3)\r\n    F[3:6, 12:15] = -Rot_prev\r\n    F[3:6, 9:12] = -v_skew_rot\r\n    F[6:9, 9:12] = -p_skew_rot\r\n`\r\nIf I use tf.tensor_scatter_nd_add etc functions, I need to create 9 indices and separate 9 values from each small matrix? This is not very convenient nor efficient. \r\n\r\nPlease direct if I didn't understand or use this function properly.", "I think scatter_nd_add can let you get one index for a whole submatrix (hence the nd part)", "> I think scatter_nd_add can let you get one index for a whole submatrix (hence the nd part)\r\n\r\n@alextp From api guide, I only saw two ways to use this function: either add individual elements to a tensor by index, or insert entire slices of a higher rank tensor all at once. \r\n\r\nCould you give an example how to use this function to implement this? Thanks!\r\n```\r\nF = tf.zeros([21, 21])\r\na = tf.ones([3, 3])\r\nF[:3, 9:12] = a\r\n```", "Indeed this type of slicing on multiple dimensions at once doesn't work well with the `tensor_scatter_nd_*` family of functions.\r\n\r\nI will leave it open and unassigned to see if someone wants to contribute a similar solution to those but for the slicing operations.", "The tf.data API helps to build flexible and efficient input pipelines.", "If this issue is still open, I would like to look into it. May I?", "@guuava  Still open, I think, so go for it. I don't expect the solution to be trivial though and encourage you to write up a lightweight RFC proposal", "Sure. I will draw up a proposal by the end of the week and share with you here ", "bump", "@yzhang93 pytorch is faster than tensorflow also they don't have extremely breaking changes as tensorflow do. tensorflow provides tf_upgrade but it doesn't even do its job", "https://colab.research.google.com/drive/18QFNQY1OtFBWI5Y2GYBbl6dsjdXZkb5K#scrollTo=kW1zIf11Jjup\r\n\r\nI have re-tested the code above using the latest of TensorFlow 2.4.1 and PyTorch 1.8.0+cu101\r\nThis is the result:\r\n```\r\nFor TensorFlow 2.4.1\r\nAverage Time: 0.004553 secs --> (Minimum Time: 0.004054 secs --- Maximum Time: 0.010114 secs)\r\nFor PyTorch 1.8.0+cu101\r\nAverage Time: 0.000164 secs --> (Minimum Time: 0.000146 secs --- Maximum Time: 0.001017 secs)\r\nDifferent Time consuming (TF / PyTorch):\r\nAverage Case: Absolute Time: 0.004389 secs ---> Relative in Time: 27.7869x\r\nMinimum Case: Absolute Time: 0.003908 secs ---> Relative in Time: 27.6938x\r\nMaximum Case: Absolute Time: 0.009097 secs ---> Relative in Time: 9.9484x\r\n```", "@yzhang93 \r\nCould you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 31708, "title": "r2.0-CherryPick: More test and bug fixes.", "body": "", "comments": []}, {"number": 31707, "title": "Fix unwanted deprecation warning in optimizer v2", "body": "", "comments": []}, {"number": 31706, "title": "TF 2.0 Dataset.shuffle() is deterministic", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tf-nightly-gpu-2.0-preview==2.0.0.dev20190815\r\n- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview==2.0.0.dev20190815\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`Dataset.shuffle()` returns results in the same order across invocations and across runs.\r\n\r\n**Describe the expected behavior**\r\nPreviously, `Dataset.shuffle()` would return randomized results.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n(TF 2.0)\r\n```python\r\nimport tensorflow as tf\r\nds = tf.data.Dataset.from_tensor_slices(list(range(10)))\r\nprint([x.numpy() for x in ds])\r\nfor i in range(10):\r\n    print([x.numpy() for x in ds.shuffle(10)])\r\n```\r\n\r\n```\r\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]\r\n[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]\r\n[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]\r\n[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]\r\n[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]\r\n[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]\r\n[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]\r\n[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]\r\n[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]\r\n[1, 9, 0, 7, 2, 3, 8, 5, 6, 4]\r\n```\r\n\r\n(TF 1.14)\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nds = tf.data.Dataset.from_tensor_slices(list(range(10)))\r\nprint([x.numpy() for x in ds])\r\nfor i in range(10):\r\n    print([x.numpy() for x in ds.shuffle(10)])\r\n```\r\n\r\n```\r\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n[1, 8, 3, 5, 6, 0, 7, 2, 9, 4]\r\n[7, 0, 8, 5, 1, 6, 4, 3, 9, 2]\r\n[4, 5, 3, 0, 7, 1, 2, 6, 9, 8]\r\n[6, 1, 9, 4, 3, 2, 8, 7, 0, 5]\r\n[3, 0, 4, 6, 5, 2, 8, 9, 7, 1]\r\n[5, 9, 3, 4, 0, 6, 7, 1, 2, 8]\r\n[4, 8, 2, 1, 0, 9, 3, 6, 5, 7]\r\n[3, 5, 7, 0, 2, 4, 8, 6, 9, 1]\r\n[8, 0, 6, 2, 7, 3, 4, 5, 9, 1]\r\n[8, 4, 2, 6, 5, 0, 1, 7, 9, 3]\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\ncc @martinwicke", "comments": ["@jsimsa This does look problematic. I recall some changes, but maybe those went further than we wanted?", "This is working as intended. In TF 2.0, the shuffling order is determined by a resource (created for each `shuffle` transformation) whose initial state is derived from the graph-level random seed.\r\n\r\nTo retain the TF 1.X behavior, one can do:\r\n\r\n```\r\nds = tf.data.Dataset.from_tensor_slices(list(range(10)))\r\nprint([x.numpy() for x in ds])\r\nds = ds.shuffle(10)\r\nfor i in range(10):\r\n  print([x.numpy() for x in ds])\r\n```\r\n\r\nThis will reuse the state that is used to determine the shuffle order across epochs (resulting in reshuffling between epochs).\r\n\r\nOr, you can change set the seed used for creating the resource that controls the shuffle order (or use `tf.random.set_seed()`):\r\n\r\n```\r\nds = tf.data.Dataset.from_tensor_slices(list(range(10)))\r\nprint([x.numpy() for x in ds])\r\nfor i in range(10):\r\n  print([x.numpy() for x in ds.shuffle(10, seed=i)])\r\n```\r\n", "@jsimsa, thanks for the quick reply!  Is it expected that random seed is kept the same across process invocations though?", "I was wrong. This was not working as intended. I have tracked down the issue and have a pending fix for this, that will be cherry picked into the 2.0 release. After the fix, your original program works the same between TF 1.X and TF 2.0. Thank you for reporting the issue.", "@jsimsa, thanks!"]}]