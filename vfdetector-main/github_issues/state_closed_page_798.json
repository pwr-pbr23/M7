[{"number": 29599, "title": "model.fit_generator(verbose=0/2) prints multiple lines per epoch during validation.", "body": "Custom code: Yes (included below)\r\nOS Platform: Debian GNU/Linux 9.8 (stretch) (GNU/Linux 4.9.0-8-amd64 x86_64)\r\nTensorFlow installed from: conda install -c anaconda tensorflow-gpu\r\nTensorFlow version: 1.13.1\r\nPython version: 3.7.1\r\nCUDA/cuDNN version: 10.0\r\nGPU model and memory: Tesla P4 7611MiB\r\n\r\n**Describe the current behavior**\r\nWhen verbose=0, it should be silent, but it prints out multiple lines during validation. It's same for verbose=2.\r\n\r\n**Describe the expected behavior**\r\nIt should be \"0 = silent, 1 = progress bar, 2 = one line per epoch.\"\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.layers import Input, Dense, Flatten\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom tensorflow.keras.callbacks import Callback\r\nfrom tensorflow.keras.utils import Sequence\r\n\r\nclass CustomCallback(Callback):\r\n\tdef __init__(self, patience=10, restore_best=True, dir=\"weights\"):\r\n\t\tsuper().__init__()\r\n\t\tself.best_score = 0.0\r\n\t\tself.best_epoch = 0\r\n\t\tself.best_weights = None\r\n\t\tself.patience = patience\r\n\t\tself.restore_best = restore_best\r\n\r\n\tdef on_epoch_end(self, epoch, logs={}):\r\n\t\tscore = logs[\"val_acc\"]\r\n\t\tif score>self.best_score:\r\n\t\t\tself.best_score = score\r\n\t\t\tself.best_epoch = epoch\r\n\t\t\tself.best_weights = self.model.get_weights()\r\n\t\t\tprint(f\"\\nBest Score: {self.best_score*100:.2f}\")\r\n\t\telse: print(f\"\\nBest Epoch: {self.best_epoch+1}, Best Score: {self.best_score*100:.2f}\")\r\n\r\n\t\tif self.patience>0 and epoch-self.best_epoch >= self.patience:\r\n\t\t\tprint(f\"\\nStopping... Best Epoch: {self.best_epoch+1}, Best Score: {self.best_score*100:.2f}\")\r\n\t\t\tself.stopped_epoch = epoch\r\n\t\t\tself.model.stop_training = True\r\n\t\t\tif self.restore_best: \t\tself.model.set_weights(self.best_weights)\r\n    \r\nclass DataGenerator(Sequence):\r\n\tdef __init__(self,x, y, batch_size=32):\r\n\t\tself.x = x\r\n\t\tself.y = y\r\n\t\tself.batch_size = batch_size\r\n\t\tself.on_epoch_end()\r\n\r\n\tdef __len__(self):\r\n\t\treturn int(np.ceil(len(self.x) / float(self.batch_size)))\r\n\r\n\tdef on_epoch_end(self):\r\n\t\tself.indexes = np.arange(len(self.x))\r\n\t\tnp.random.shuffle(self.indexes)\r\n\t\r\n\tdef __getitem__(self, index):\r\n\t\tindexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n\t\tx_batch = self.x[indexes]\r\n\t\ty_batch = self.y[indexes]\r\n\t\treturn x_batch, y_batch\r\n\r\ndef dense_model(input_shape, output_shape):\r\n\tinputs = Input(input_shape)\r\n\tx = Flatten()(inputs)\r\n\tx = Dense(300, activation='relu')(x)\r\n\tx = Dense(100, activation=\"relu\")(x)\r\n\toutputs = Dense(output_shape, activation='softmax')(x)\r\n\tmodel = Model(inputs=inputs, outputs=outputs)\r\n\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\r\n\treturn model\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nheight, width = 28, 28\r\ninput_shape = (height, width)\r\noutput_shape = 10\r\nx_train = x_train/255\r\nx_test = x_test/255\r\ny_train = to_categorical(y_train, output_shape)\r\ny_test = to_categorical(y_test, output_shape)\r\nmodel = dense_model(input_shape, output_shape)\r\nhistory = model.fit_generator(DataGenerator(x_train, y_train), epochs=12, validation_data=DataGenerator(x_test, y_test), verbose=0)\r\n```\r\n\r\n**Other info / logs**\r\n``\r\nEpoch 1/12\r\n1/313 [..............................] - ETA: 17s - loss: 0.1513 - acc: 0.9688\r\n25/313 [=>............................] - ETA: 1s - loss: 0.1436 - acc: 0.9525\r\n48/313 [===>..........................] - ETA: 0s - loss: 0.1517 - acc: 0.9479\r\n...\r\n``\r\n", "comments": ["@jsl303 Please provide minimal code snippet to reproduce the issue. Thanks!", "Yes, I provided the code.", "@jsl303 I tried reproducing the issue on colab with Tf-gpu 1.13.1 but I am unable to reproduce it. Can you help me to reproduce the reported issue. Thanks!", "It's strange. In Notebook environment, nothing gets printed as expected.\n\nHowever, if I run as a script, it prints out the validation.\n\nI also tried on colab, nothing gets printed.\n\nI tried on Kaggle as a python script as well as my local machine, then \nvalidation result gets printed.\n\nMaybe Jupyter suppresses the output?\n\nCould you try on your local machine with just command line? It's a very \nsimple, so it shouldn't take long at all.\n\nThanks!\n\n", "@jsl303 I tried running in Spyder IDE, Jupyter and google colab and I don't see those lines of output. It may be due to some logging settings. \r\n\r\nSorry, I'm going to close this issue because I don't think it's feasible for us to fix anything in Tensorflow to resolve. Feel free to open a new feature request or re-open if you feel strongly about that. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29599\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29599\">No</a>\n"]}, {"number": 29598, "title": "[ROCm] Adding ROCm support for scatter ops", "body": "This PR adds ROCm support for the scatter ops.\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n-------------------------------------------\r\n\r\n@tatianashp @whchung \r\n", "comments": []}, {"number": 29597, "title": "Compiling from source, can't find any working combination", "body": "GPU 1080Ti\r\ncuda 10.1\r\ncuDNN 7.6.0\r\nnccl 2.4.7\r\nUbuntu 18.04.2\r\nPython 3.6\r\nTensorflow r1.12, r1.13, r1.14\r\nBazel 0.17.2, 0.18.0, 0.25.2, 0.26.1\r\n\r\nr1.14 + 0.26.1\r\ndowngrade bazle to 0.25.2\r\n\r\nr1.14 + 0.25.2\r\nThis error:\r\n`Traceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 497, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 489, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 448, in find_cuda_config\r\n    _get_default_cuda_paths(cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 164, in _get_default_cuda_paths\r\n    ] + _get_ld_config_paths()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 144, in _get_ld_config_paths\r\n    match = pattern.match(line.decode(\"ascii\"))\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 27: ordinal not in range(128)\r\nAsking for detailed CUDA configuration...\r\n`\r\nI try any other combination of tensorflow ans bazel but I always get errors in different places.\r\n\r\nr1.13 + 0.25.2\r\ndowngrate to 0.21.0\r\n\r\n./configure work\r\n\r\nError at compiling:\r\n\r\n`bazel build --config=opt --config=mkl --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n`ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in /home/thomasd/tensorflow/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/thomasd/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1556\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/thomasd/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1302, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, cuda_config)\r\n\tFile \"/home/thomasd/tensorflow/third_party/gpus/cuda_configure.bzl\", line 840, in _find_libs\r\n\t\t_find_cuda_lib(\"cublas\", repository_ctx, cpu_value, c..., ...)\r\n\tFile \"/home/thomasd/tensorflow/third_party/gpus/cuda_configure.bzl\", line 752, in _find_cuda_lib\r\n\t\tauto_configure_fail((\"Cannot find cuda library %s\" %...))\r\n\tFile \"/home/thomasd/tensorflow/third_party/gpus/cuda_configure.bzl\", line 342, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cuda library libcublas.so.10.1\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': in /home/thomasd/tensorflow/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/thomasd/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1556\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/thomasd/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1302, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, cuda_config)\r\n\tFile \"/home/thomasd/tensorflow/third_party/gpus/cuda_configure.bzl\", line 840, in _find_libs\r\n\t\t_find_cuda_lib(\"cublas\", repository_ctx, cpu_value, c..., ...)\r\n\tFile \"/home/thomasd/tensorflow/third_party/gpus/cuda_configure.bzl\", line 752, in _find_cuda_lib\r\n\t\tauto_configure_fail((\"Cannot find cuda library %s\" %...))\r\n\tFile \"/home/thomasd/tensorflow/third_party/gpus/cuda_configure.bzl\", line 342, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cuda library libcublas.so.10.1\r\nINFO: Elapsed time: 5.369s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n    Fetching @local_config_cuda; fetching\r\n`\r\n\r\nr1.12 + 0.21.0\r\n\r\n.configure work\r\n\r\nError at compiling:\r\n\r\n`ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\nUse --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\nUse --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\nINFO: Elapsed time: 1.986s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    Fetching @io_bazel_rules_closure; fetching`\r\n\r\nI try any other combination of tensorflow and bazel but I always get errors in different places.\r\n", "comments": ["This fix in master: https://github.com/tensorflow/tensorflow/pull/28209, is needed for 1.14.\r\n\r\n1.14 is the only release than can be compiled with CUDA 10.1, if you want to compile 1.13, you'll need to use CUDA 10.0"]}, {"number": 29596, "title": "Post Training Quantization slower in latest tf-nightly vs tf 1.10", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tensorflow==1.10.0 vs tf-nightly==1.14.1.dev20190606\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI have converted the mobilenetv2_1.0_224_frozen.pb model to tflite and performed post training quantization in two virtual environments - one with tf-nightly==1.14.1.dev20190606 (following the instructions from here: https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/lite/tutorials/post_training_quant.ipynb) and one with tensorflow==1.10.0.\r\n\r\nDuring inference, I have observed that the model converted with the latest tensorflow version is ~2x slower\r\n\r\n**Describe the expected behavior**\r\nI expect that the latest post training quantization should remain as fast as the one in version 1.10.\r\n \r\n**Code to reproduce the issue**\r\nThe scripts used for conversion:\r\n[conversion_scripts.zip](https://github.com/tensorflow/tensorflow/files/3272376/conversion_scripts.zip)\r\n\r\nFor inference I have used tflite 1.10 and tflite 2.0.\r\n\r\nI have used the profiling functionality of tflite and observed that:\r\n   - in tf 1.10 there are 49 DEQUANTIZE Nodes vs 15 DEQUANTIZE Nodes for tf 1.14.1.dev20190606\r\n   - CONV_2D is ~3x slower for the converted model in tf 1.14.1.dev20190606 (when running a float model CONV_2D seems similar in both tf versions)\r\n   - by looking at the run order of the nodes it seems that only the depthwise nodes are quantized in tf 1.14.1.dev20190606 but in tf 1.10 expand, project and depthwise are quantized.\r\n\r\nHere is the Summary by node type for tf 1.10:\r\n```\r\nNumber of nodes executed: 115\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]     [times called]\r\n\t       DEPTHWISE_CONV_2D\t       17\t   151.612\t   68\r\n\t                 CONV_2D\t       36\t   125.676\t    144\r\n\t              DEQUANTIZE\t       49\t     7.872\t     \t      196\r\n\t                     ADD\t       10\t     0.288\t            40\r\n\t         AVERAGE_POOL_2D\t        1\t     0.056\t    \t   4\r\n\t                 SOFTMAX\t        1\t     0.028\t     \t        4\r\n\t                 RESHAPE\t        1\t     0.004\t     \t        4\r\n\r\n```\r\n\r\nAnd the Summary by node type for tf 1.14.1.dev20190606:\r\n```\r\n\r\nNumber of nodes executed: 81\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t   [times called]\r\n\t                 CONV_2D\t       36\t   380.280\t        144\r\n\t       DEPTHWISE_CONV_2D\t       17\t   160.624\t   68\r\n\t                     ADD\t       10\t     0.428\t     \t       40\r\n\t         AVERAGE_POOL_2D\t        1\t     0.056\t    \t        4\r\n\t                 SOFTMAX\t        1\t     0.028\t     \t        4\r\n\t              DEQUANTIZE\t       15\t     0.016\t     \t       60\r\n\t                 RESHAPE\t        1\t     0.004\t               4\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["What device are you benchmarking on? It is likely that the quantized benchmark numbers on x86 (desktop) are slower (especially because the graph with fewer Dequantize nodes seems right to me, since Conv2d's should have \"hybrid\" quantized implementation).\r\n\r\nTo get an accurate performance measure you will need to benchmark on an optimized mobile device.\r\n\r\n(also note, i don't believe this tool was complete in 1.10).", "Thank you for your response! \r\n\r\nI have run the benchmarks on desktop and on a Raspberry Pi 3B.", "Hi @BrindusaNiculae !\r\nIt seems you are using older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions as many features and bug fixes has been done in newer versions and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29595, "title": "Installation of Tensorflow 2.0.0.dev20190607 on Ubuntu  :AttributeError: module 'tensorflow_core' has no attribute 'compiler'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GCP \"Ubuntu\"VERSION=\"18.04.2 LTS (Bionic Beaver)\"\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: TF 2.0 nightly tf-nightly-2.0-preview==2.0.0.dev20190607\r\n- Python version: 3.6.6 \r\n- Installed using virtualenv? pip? conda?: conda virtual env\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\nInstallation of TF is fine but it crashes during the import of Tensorflow 2.0.0.dev20190607 on GCP Ubuntu. The exact same works on Mac OS.\r\n\r\n```\r\nPython 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16)\r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/miniconda3/envs/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow/__init__.py\", line 93, in <module>\r\n    from tensorflow_core import *\r\nAttributeError: module 'tensorflow_core' has no attribute 'compiler'\r\n```\r\npackages related to TF after the installation:\r\n\r\n> \r\n> tb-nightly                1.14.0a20190610          pypi_0    pypi\r\n> tensorflow-estimator-2-0-preview 1.14.0.dev2019060900          pypi_0    pypi\r\n> tensorflow-metadata       0.13.0                   pypi_0    pypi\r\n> tf-nightly-2-0-preview    2.0.0.dev20190607          pypi_0    pypi\r\n> tfds-nightly              1.0.2.dev201906050105          pypi_0    pypi\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nmore environment_2_0.yml\r\nchannels:\r\n- defaults\r\n- conda-forge\r\ndependencies:\r\n- python=3.6.6\r\n- numpy=1.14.5\r\n- scipy=1.1.0\r\n- scikit-learn=0.20.1\r\n- tornado=4.5.3\r\n- jupyter=1.0.0\r\n- jupyterlab=0.34.9\r\n- jupyter_contrib_nbextensions=0.5.0\r\n- jupyter_nbextensions_configurator=0.4.0\r\n- pandas=0.24.2\r\n- matplotlib\r\n- nomkl # Only for Mac user\r\n- notebook=5.6.0\r\n- ipykernel=4.10.0\r\n- ipywidgets=7.4.1\r\n- nbconvert=5.4.0\r\n- watermark\r\n- pytest=3.6.2\r\n- autopep8\r\n- pylint\r\n- pep8=1.7.1\r\n- pylama=7.4.3\r\n- nbdime\r\n- google-auth\r\n- pillow\r\n- scikit-image=0.14.0\r\n- seaborn=0.9.0\r\n- lime\r\n- google-cloud-storage\r\n- google-cloud-bigquery\r\n- psutil\r\n- pip=19.1\r\n- pip:\r\n  - tfds-nightly==1.0.2.dev201906050105\r\n  - tf-nightly-2.0-preview==2.0.0.dev20190607\r\n```\r\n\r\n```\r\nconda env create -f environment_2_0.yml -n env_gcp_dl_2_0_nightly\r\nconda activate env_gcp_dl_2_0_nightly\r\n```\r\n", "comments": ["To add that using an older version: 2.0.0.dev20190605 is working fine", "That's my mistake, but it should be fixed with a more recent nightly. See #29536", "@mihaimaruseac thanks for the info and you are right. The issue is fixed with tf-nightly-2.0-preview==2.0.0.dev20190608. It seems quite some nightly  were not build for MacOS and Linux. I was Looking for MacOS and didn't see that for Linux 2.0.0.dev20190608 was present and not having issue. Closing."]}, {"number": 29594, "title": "sparse.to_dense don't work when used to dataset element", "body": "`def _parse_function(example_proto):\r\n\r\n  features = {\"feature0\":tf.io.VarLenFeature(tf.int64),\r\n              \"feature1\": tf.io.FixedLenFeature([1], tf.int64)}\r\n  parsed_features = tf.io.parse_single_example(example_proto, features)\r\n  print(parsed_features['feature0'])\r\n  return tf.sparse.to_dense(parsed_features['feature0']), parsed_features['feature1']\r\n\r\ndataset = tf.data.TFRecordDataset(filename)\r\ndataset = dataset.map(_parse_function)`\r\n\r\nerror like below\r\n`  File \"/home/yi/bin/Miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py\", line 68, in _convert_to_sparse_tensor\r\n    raise TypeError(\"Input must be a SparseTensor.\")\r\nTypeError: Input must be a SparseTensor.`\r\n", "comments": ["Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29594\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29594\">No</a>\n"]}, {"number": 29593, "title": "tf.estimator.train_and_evaluate fails to print anything (loss/ accuracy) when used with  tf.keras.estimator.model_to_estimator [tf2.0 beta]", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): 2.0.0-beta0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/ 7.5\r\n- GPU model and memory: 11gb GTX 1080ti\r\n\r\n**Describe the current behavior**\r\nWhen training an estimator which is got from a tf.keras.Model instance, the tf.estimator.train_and_evaluate method fails to log anything on the stdout.\r\n```\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\r\n```\r\nHowever, if set the above flag, i can loss being logged (similar to that of API v1)\r\n\r\n```\r\nI0610 16:24:54.069874 140233725445952 basic_session_run_hooks.py:260] loss = 0.9756932, step = 600 (4.162 sec)\r\n```\r\n**Describe the expected behavior**\r\nlog the metrics and loss as described in the official docs\r\n[https://www.tensorflow.org/beta/guide/migration_guide]()\r\n```\r\nW0608 04:37:53.280840 139724414916352 estimator.py:1811] Using temporary folder as model directory: /tmp/tmp4rht4njh W0608 04:37:54.037474 139724414916352 deprecation.py:323] From /tmpfs/src/tf_docs_env/lib/python3.5/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version. Instructions for updating: Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts. W0608 04:37:58.233872 139724414916352 deprecation.py:323] From /tmpfs/src/tf_docs_env/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version. Instructions for updating: Use standard file APIs to check for files with this prefix. \r\n({'accuracy': 0.678125, 'global_step': 25, 'loss': 1.4507575}, [])\r\n```\r\n**Code to reproduce the issue**\r\n```\r\nfrom tensorflow.keras.applications.xception import Xception\r\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint('Tensorflow version', tf.__version__)\r\n\r\ndef input_fn(training=False):\r\n    batch_size = 8\r\n    def preprocess_map_func(image, label):\r\n        image = tf.image.resize(image, size=[299, 299])\r\n        image.set_shape([None, None, 3])\r\n        image /=  127.5\r\n        image -= 1\r\n        return image, label\r\n    \r\n    def input_():\r\n        if training:\r\n            dataset = tfds.load(name='cats_vs_dogs', as_supervised=True, split=[\"train\"])[0]\r\n            train_dataset = dataset.skip(3000)\r\n            train_dataset = train_dataset.map(preprocess_map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n            train_dataset = train_dataset.shuffle(1024).batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)\r\n            return train_dataset\r\n        else:\r\n            dataset = tfds.load(name='cats_vs_dogs', as_supervised=True, split=[\"train\"])[0]\r\n            test_dataset = dataset.take(3000)\r\n            test_dataset = test_dataset.map(preprocess_map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n            test_dataset = test_dataset.shuffle(1024).batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)\r\n            return test_dataset\r\n    return input_\r\n\r\nepochs = 10\r\nsamples = 23000\r\nbatch_size = 8\r\n\r\nbase_model = Xception(input_shape=(299, 299, 3), include_top=False, weights='imagenet')\r\ny = GlobalAveragePooling2D()(base_model.output)\r\ny = Dense(units=1, activation='linear', kernel_initializer='he_normal')(y)\r\nbase_model.trainable = False\r\nmodel = tf.keras.Model(base_model.input, y)\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(model)\r\ntrain_spec = tf.estimator.TrainSpec(input_fn(training=True), max_steps=epochs * samples//batch_size)\r\neval_spec = tf.estimator.EvalSpec(input_fn(training=False), steps=3000//batch_size)\r\ntf.estimator.train_and_evaluate(estimator, \r\n                                train_spec=train_spec, \r\n                                eval_spec=eval_spec)\r\n\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@gadagashwini @jvishnuvardhan ", "any updates?", "@srihari-humbarwadi Can you check with `TF2.0` and let us know whether it was resolved your issue? I ran it in `TF2.0` and I see the following output (partial) that includes `loss` at different steps.\r\n\r\n```\r\nINFO:tensorflow:loss = 1.3160852, step = 0\r\nINFO:tensorflow:loss = 1.3160852, step = 0\r\nINFO:tensorflow:global_step/sec: 15.5583\r\nINFO:tensorflow:global_step/sec: 15.5583\r\nINFO:tensorflow:loss = 0.26208925, step = 100 (6.430 sec)\r\nINFO:tensorflow:loss = 0.26208925, step = 100 (6.430 sec)\r\nINFO:tensorflow:global_step/sec: 16.0867\r\nINFO:tensorflow:global_step/sec: 16.0867\r\nINFO:tensorflow:loss = 8.357772, step = 200 (6.216 sec)\r\nINFO:tensorflow:loss = 8.357772, step = 200 (6.216 sec)\r\nINFO:tensorflow:global_step/sec: 15.8081\r\nINFO:tensorflow:global_step/sec: 15.8081\r\nINFO:tensorflow:loss = 1.9635394, step = 300 (6.326 sec)\r\nINFO:tensorflow:loss = 1.9635394, step = 300 (6.326 sec)\r\nINFO:tensorflow:global_step/sec: 15.7007\r\nINFO:tensorflow:global_step/sec: 15.7007\r\nINFO:tensorflow:loss = 0.044729847, step = 400 (6.370 sec)\r\nINFO:tensorflow:loss = 0.044729847, step = 400 (6.370 sec)\r\nINFO:tensorflow:global_step/sec: 15.5839\r\nINFO:tensorflow:global_step/sec: 15.5839\r\nINFO:tensorflow:loss = 0.24973816, step = 500 (6.420 sec)\r\nINFO:tensorflow:loss = 0.24973816, step = 500 (6.420 sec)\r\n```\r\n\r\nPlease check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ceabd9bc9d5f4447a94d8c26344669a1/tf29593.ipynb) and let us know what you think. If your issue was resolved by `TF2.0`, then please close the issue. thanks!", "Looks like its working now, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29593\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29593\">No</a>\n"]}, {"number": 29592, "title": "Custom Op Placement on TPUs", "body": "Hello,\r\n\r\nI'd like to place one portion of my model on a single core of a TPU and another portion on another core (as opposed to the usual full-model replica method).\r\n\r\nWhat is the correct way to do this? I understand that I'll need to do away with TPUEstimator and use a tpu.rewrite() along with tf.Sessions, but I'm not sure the correct way to specify placement for a TPU core.\r\n\r\nIs it as simple as `with tf.device(\"TPU:<core_id>\"):`?", "comments": ["Presumably people working on MTF have already solved how to do this, so maybe Noam, Ryan, or another would be the best people to have answer.", "Can you please help us with some more information as in platform ( operating system, architecture ), TensorFlow version you are using. This will help us to proceed faster. Thanks! ", "TF 1.13.1\r\nTPU-v2\r\n\r\nI suppose this easiest thing to provide would be to show a colab that's confusing me: https://colab.research.google.com/drive/1Y9PgINAT_zhjz4xXbpC5tTMRzg7aSH1e\r\n\r\nI had imagined that running on all 8 cores would have been many times faster than running on a single core, but I'm not seeing that here.", "@aidangomez I think the issue here is that the computation in your Colab is so small that the TPU compilation overhead (etc...) dominates. I changed `for _ in range(5)` to `for _ in range(500)`, then I get:\r\n\r\n```\r\nRunning ops\r\nOne core: 44.88167405128479\r\nEight cores: 25.88580870628357\r\n```\r\nIt would also be helpful to [take a profile](https://stackoverflow.com/questions/53254056/running-cloud-tpu-profiler-in-google-colab-environment?rq=1) and see what the utilization is like on each core.\r\n\r\n", "Going to close this for now, but feel free to re-open if this persists with larger computations and it would be helpful to take a profile as well."]}, {"number": 29591, "title": "Broken Link", "body": "https://www.tensorflow.org/images/models/pose_estimation.gif\r\n\r\n\r\nthis Link is broken", "comments": ["Could you please give some more information or fill the issue template? \r\nWhere does this link exist in the docs? (i.e. Where did you click it and realised it's broken?)", "Hi dear Anestis, Yes. I was just browsing TensorFlow docs on the link below:\nhttps://www.tensorflow.org/lite/models/pose_estimation/overview\n\nBut there is a broken link in the middle of the page\nhttps://www.tensorflow.org/images/models/pose_estimation.gif\n\nOn Mon, Jun 10, 2019 at 8:29 PM Anestis Sakerlis <notifications@github.com>\nwrote:\n\n> Could you please give some more information?\n> Where does this link exist in the docs? (i.e. Where did you click it and\n> realised it's broken?)\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/29591?email_source=notifications&email_token=ADRWKGLZV7W745LLD4S2BNDPZZ27NA5CNFSM4HWR4D6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXKJBXI#issuecomment-500469981>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADRWKGNHNRJX7PUL4ULRICLPZZ27NANCNFSM4HWR4D6A>\n> .\n>\n", "I believe it is supposed to point in this [link](https://raw.githubusercontent.com/irealva/tfjs-models/master/posenet/demos/camera.gif) and this [docs file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/pose_estimation/overview.md) should be changed. We should wait for a tensorflower before opening a PR. \r\nWould you like to submit it if that's the case?", "Thanks for the report!\r\nThat link should point here: https://www.tensorflow.org/images/lite/models/pose_estimation.gif\r\nWould someone be able to make a PR?"]}, {"number": 29590, "title": "Does tFlite support input shape=[1,32,None,3]", "body": "**System information**\r\n Linux Ubuntu 16.04\r\ntf-cpu-1.13.1\r\n\r\nI use tensorflow train a crnn+ctc OCR  model,the width of textline is Variable,but when I convert pb to tflite,ValueError: None is only supported in the 1st dimension Tensor 'input_images' has invalid shape [1, 32, None, 3]\u3002\r\n\r\n", "comments": ["I believe the converter currently can't handle unknown dimension other than the batch size dimension. You could try use a fixed length (such as max length) instead.", "if use a fixed length(max width),the run time will be too long for small width", "I see.\r\n\r\nCurrently dynamic input shape is not supported in tflite. However a walkaround could be:\r\n\r\n1) set the unknown dimension to a fixed value during conversion.\r\n2) then try interpreter.resize_tensor_input() method to resize the input tensor size at inference.\r\n\r\nIt's not guaranteed that this path will always work as expected though, however it's no harm if you can give it a try. Thanks!", "@haozha111  Seems your method does not work. \r\nAfter calling\r\n```\r\n interpreter.resize_tensor_input(\r\n```\r\n and \r\n\r\n```\r\ninterpreter.set_tensor(input_details[0]['index'], im)\r\n```\r\n\r\nthe program will crash with\r\n```\r\nProcess finished with exit code 139 (interrupted by signal 11: SIGSEGV)\r\n```", "Hi Melody, sorry that it doesn't work for you. We are actively working on dynamic shapes in TF Lite, and hope to make the user experience better. Adding Nupur who is working in this area.", "Any news on dynamic shapes in TF Lite?", "I tried resize input_tensor.It would crash in seconds.\r\nAnd I tried 3 methods in  post_training_quantize ,interpreter.invoke  takes about 3 times before I used optimization method , even the model is  actually smaller.", "or at least can we have variable batch? I resize input/output tensors, but when call allocate_tensors (either from python or java) I get an error inside reshape operator that the input and output tensors don't have the same number of elements", "We added support for unknown dimensions in TensorFlow Lite today (55912083e2f16087c2f29394acf8a6a4811a2ce0). \r\n\r\nCan you try converting your model again with tonight's (1/31) `tf-nightly` once it's released (`pip install tf-nightly`). Convert the model with `experimental_new_converter = True`.\r\n\r\nWhen you load the model it should have an additional field `shape_signature` that contains the shape with any unknown dimensions marked with `-1`. `shape` will have those dimensions marked with `1`.\r\n\r\nYou can then call `ResizeInputTensor` with the desired shape when running the interpreter. The generated model will only work on the latest TensorFlow version (i.e. the interpreter on the `tf-nightly` version you are running).\r\n\r\nIf it does not work, can you provide a detailed error and repro instructions?", "@gargn I exported a .tflite file generated from AutoML. The default input is 224x224. Any ideas on how to change the input to 200x80 and then save and download a new .tflite file? I can't seem to find documentation on this? I know i can use `resize_tensor_input`, but then I have to do the inference at that time. I'd love to get a new tflite file that has a new input tensor all together. \r\n\r\nThoughts? ", "> We added support for unknown dimensions in TensorFlow Lite today ([5591208](https://github.com/tensorflow/tensorflow/commit/55912083e2f16087c2f29394acf8a6a4811a2ce0)).\r\n> \r\n> Can you try converting your model again with tonight's (1/31) `tf-nightly` once it's released (`pip install tf-nightly`). Convert the model with `experimental_new_converter = True`.\r\n> \r\n> When you load the model it should have an additional field `shape_signature` that contains the shape with any unknown dimensions marked with `-1`. `shape` will have those dimensions marked with `1`.\r\n> \r\n> You can then call `ResizeInputTensor` with the desired shape when running the interpreter. The generated model will only work on the latest TensorFlow version (i.e. the interpreter on the `tf-nightly` version you are running).\r\n> \r\n> If it does not work, can you provide a detailed error and repro instructions?\r\n\r\nI tried this with\r\n`tf.__version__ = '2.2.0-dev20200317'`\r\n\r\nand It's not working -- I believe it's because `uint8` dtype (i.e. coral USB compilation?) isn't supported yet. Can this be?\r\n\r\n```\r\nconverter = lite.TFLiteConverter.from_saved_model('saved_yolo/')  \r\nconverter.experimental_new_converter = True\r\n```\r\n\r\nMy data needs to be `uint8` so my tflite model can be converted to an Edge TPU model...\r\nbut it seems that the converter won't work with this data:\r\n```\r\nIn [27]: converter._is_unknown_shapes_allowed(fp32_execution=next(\r\n                 representative_dataset_gen())[0][0].dtype == np.float32)                                                                 \r\nOut[27]: False\r\n\r\nIn [28]: converter._is_unknown_shapes_allowed(fp32_execution=next(\r\n                  representative_dataset_gen())[0][0].dtype == np.uint8)                                                                   \r\nOut[28]: True\r\n```\r\n ```\r\nconverter.representative_dataset = representative_dataset_gen \r\nconverter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS_INT8] \r\nconverter.optimizations = [lite.Optimize.DEFAULT] \r\nconverter.inference_input_type = tf.uint8 \r\nconverter.inference_output_type = tf.uint8 \r\ntflite_model = converter.convert()                                                                                                                                                             \r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-18-c548bab089a8> in <module>\r\n----> 1 tflite_model = converter.convert()\r\n\r\n~/.pyenv/versions/3.7.6/envs/experimental_converter/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    556               \"None is only supported in the 1st dimension. Tensor '{0}' has \"\r\n    557               \"invalid shape '{1}'.\".format(\r\n--> 558                   _get_tensor_name(tensor), shape_list))\r\n    559         elif shape_list and shape_list[0] is None:\r\n    560           # Set the batch size to 1 if undefined.\r\n\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_1' has invalid shape '[None, None, None, 3]'.\r\n```\r\n\r\nAs an aside, if any of you know how I can make the TF 1.x yolo model I'm trying to convert have fixed tensor shapes so I can compile, that would be amazing!\r\n\r\nI've only started learning TF now that the nice 2.x APIs are there for me use and don't know anything about TF 1.x idioms.", "@gargn \r\n> We added support for unknown dimensions in TensorFlow Lite today ([5591208](https://github.com/tensorflow/tensorflow/commit/55912083e2f16087c2f29394acf8a6a4811a2ce0)).\r\n> \r\n> Can you try converting your model again with tonight's (1/31) `tf-nightly` once it's released (`pip install tf-nightly`). Convert the model with `experimental_new_converter = True`.\r\n> \r\n> When you load the model it should have an additional field `shape_signature` that contains the shape with any unknown dimensions marked with `-1`. `shape` will have those dimensions marked with `1`.\r\n> \r\n> You can then call `ResizeInputTensor` with the desired shape when running the interpreter. The generated model will only work on the latest TensorFlow version (i.e. the interpreter on the `tf-nightly` version you are running).\r\n> \r\n> If it does not work, can you provide a detailed error and repro instructions?\r\n\r\nI've tried this in `2.2.0-rc2` with a keras model and I'm having issues also. Minimal repro:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import Model, Input\r\nfrom tensorflow.python.keras.layers import Conv2D\r\n\r\nprint('version:', tf.__version__)\r\n\r\ni = Input(shape=(None, None, 3))\r\nx = Conv2D(32, (3, 3))(i)\r\n\r\nm = Model(i, x)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(m)\r\n# MLIR enabled by default in 2.2.0, but check anyway:\r\nprint('MLIR enabled?', converter.experimental_new_converter)\r\ntflite_model = converter.convert()\r\n```\r\nOutput:\r\n```\r\nversion: 2.2.0-rc2\r\nMLIR enabled? True\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-f6a6483ac38d> in <module>\r\n     12 # MLIR enabled by default in 2.2.0, but check anyway:\r\n     13 print('MLIR enabled?', converter.experimental_new_converter)\r\n---> 14 tflite_model = converter.convert()\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    481               \"None is only supported in the 1st dimension. Tensor '{0}' has \"\r\n    482               \"invalid shape '{1}'.\".format(\r\n--> 483                   _get_tensor_name(tensor), shape_list))\r\n    484         elif shape_list and shape_list[0] is None:\r\n    485           # Set the batch size to 1 if undefined.\r\n\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_1' has invalid shape '[None, None, None, 3]'.\r\n```", "Having the same issue as adrian", "> @gargn\r\n> \r\n> > We added support for unknown dimensions in TensorFlow Lite today ([5591208](https://github.com/tensorflow/tensorflow/commit/55912083e2f16087c2f29394acf8a6a4811a2ce0)).\r\n> > Can you try converting your model again with tonight's (1/31) `tf-nightly` once it's released (`pip install tf-nightly`). Convert the model with `experimental_new_converter = True`.\r\n> > When you load the model it should have an additional field `shape_signature` that contains the shape with any unknown dimensions marked with `-1`. `shape` will have those dimensions marked with `1`.\r\n> > You can then call `ResizeInputTensor` with the desired shape when running the interpreter. The generated model will only work on the latest TensorFlow version (i.e. the interpreter on the `tf-nightly` version you are running).\r\n> > If it does not work, can you provide a detailed error and repro instructions?\r\n> \r\n> I've tried this in `2.2.0-rc2` with a keras model and I'm having issues also. Minimal repro:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from tensorflow.python.keras import Model, Input\r\n> from tensorflow.python.keras.layers import Conv2D\r\n> \r\n> print('version:', tf.__version__)\r\n> \r\n> i = Input(shape=(None, None, 3))\r\n> x = Conv2D(32, (3, 3))(i)\r\n> \r\n> m = Model(i, x)\r\n> converter = tf.lite.TFLiteConverter.from_keras_model(m)\r\n> # MLIR enabled by default in 2.2.0, but check anyway:\r\n> print('MLIR enabled?', converter.experimental_new_converter)\r\n> tflite_model = converter.convert()\r\n> ```\r\n> \r\n> Output:\r\n> \r\n> ```\r\n> version: 2.2.0-rc2\r\n> MLIR enabled? True\r\n> ---------------------------------------------------------------------------\r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-1-f6a6483ac38d> in <module>\r\n>      12 # MLIR enabled by default in 2.2.0, but check anyway:\r\n>      13 print('MLIR enabled?', converter.experimental_new_converter)\r\n> ---> 14 tflite_model = converter.convert()\r\n> \r\n> ~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n>     481               \"None is only supported in the 1st dimension. Tensor '{0}' has \"\r\n>     482               \"invalid shape '{1}'.\".format(\r\n> --> 483                   _get_tensor_name(tensor), shape_list))\r\n>     484         elif shape_list and shape_list[0] is None:\r\n>     485           # Set the batch size to 1 if undefined.\r\n> \r\n> ValueError: None is only supported in the 1st dimension. Tensor 'input_1' has invalid shape '[None, None, None, 3]'.\r\n> ```\r\n\r\nHaving the same issue here.", "We can specify a random non-batch size dimemsion for dynamic axis using tensorflow-2.2.0-rc3 tflite_convert.\r\n\r\nHowever, the tflite-runtime cannot recalculate the appropriate output tensor dimensions if our model needs to.\r\n\r\n```python\r\ninterpreter = tflite.Interpreter(model_path=\"densenet.tflite\")\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nwidth = 277 # Any number that is valid for the model\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (1, 32, width, 1))\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(input_details[0]['index'], X)\r\ninterpreter.invoke()\r\noutput = interpreter.get_tensor(output_details[0]['index'])\r\n```\r\n\r\nSomething like this will occur:\r\n```\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (3072 != 13698)\r\n```\r\nLooks like tflite not yet able to modify all internal operator input/output dimensions accordingly.", "@adriancaruana I think your issue was resolved already in recent `tf-nightly`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/18e7203d5deccb1f45306a00c7002ea0/29590.ipynb). Thanks!", "@james34602 Can you print `input_details` and `output_details`? Can you share the tflite file or model file? Thanks!", "@jvishnuvardhan \r\nAbsolutely!\r\n[https://drive.google.com/file/d/1ymi9BP6QQB1J0am3kkZLKdl6qQkFj_-f/view?usp=sharing](https://drive.google.com/file/d/1ymi9BP6QQB1J0am3kkZLKdl6qQkFj_-f/view?usp=sharing)\r\n\r\nI remove most parameters in the model, however, the I/O of the model remains identical to the original unshrink model file.\r\n\r\n---Python print start---\r\n**input_details**:\r\n [{'name': 'the_input', 'index': 44, 'shape': array(**[1, 32, 2112, 1]**), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}}]\r\n\r\n**output_details**:\r\n [{'name': 'out/truediv', 'index': 36, 'shape': array(**[]**, dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}}]\r\n\r\nTraceback (most recent call last):\r\n  File \"runDensenet.py\", line 15, in <module>\r\n    text = keras_densenet(image)\r\n  File \"C:\\Users\\XXX\\densenetCustomTrial\\densenet\\modelSing.py\", line 52, in predict\r\n    interpreter.allocate_tensors()\r\n  File \"C:\\Users\\XXX\\Anaconda3\\envs\\ocrtest\\lib\\site-packages\\tflite_runtime\\interpreter.py\", line 243, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"C:\\Users\\XXX\\Anaconda3\\envs\\ocrtest\\lib\\site-packages\\tflite_runtime\\tensorflow_wrap_interpreter_wrapper.py\", line 110, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (192 != 0)Node number 11 (RESHAPE) failed to prepare.`\r\n---Python print end---\r\n_Notice that:_\r\n1. **2112** ([1, 32, 2112, 1]) should be **None** in the first place, tricks are needed to get tflite_convert  working\r\n2. The tflite run completely correct if the input width is **2112**, but it's not when input != 2112.\r\n\r\nWe hope tensorflow team fix the problem...even Matlab MatConvNet can handle dynamic I/O shape...", "I used concrete_function to set input shape when converting saved_model to fflite format.\r\nThe conversion is successful but when I tried to inference in android, the output shape I got with converted model has shape 0. \r\nIn saved_model_cli, output_shape is: `(-1, -1, -1, 2)`", "@james34602 I printed the `input_details` and found that `None` (-1) is not present in the signature. Please check the `input_details`.\r\n\r\n`Input_details:  [{'name': 'the_input', 'index': 44, 'shape': array([   1,   32, 2112,    1], dtype=int32), 'shape_signature': array([   1,   32, 2112,    1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n`\r\nIf you had defined 3rd dimension as `None`, then the signature in the `input_details` should look like `'shape_signature': array([   1,   32, -1,    1], dtype=int32),`. Please check @gargn comment on `None` dimension [here](https://github.com/tensorflow/tensorflow/issues/29590#issuecomment-580951882).\r\n\r\nCan you update the model with `None` and convert again. [Here](https://colab.research.google.com/gist/jvishnuvardhan/5879cf2e7714c97a942e8ee63247829b/29590.ipynb) is a gist for your reference. Hope it helps. Thanks!", "@lynx97 Can you please create a new issue and provide a standalone code to reproduce the issue. Opening new issue is better and can be easy to follow for other users who are facing similar issue like you. Thanks!", "@jvishnuvardhan Thanks for attention.\r\nThe version of tensorflow I was using is tensorflow-2.2.0-rc3, sorry about that, I thought RC version contain some sort of experimental features, converting a model that contain non-batch None axis is not supported.", "Hello guys, @jvishnuvardhan \r\nThe tf-nightly can convert None axis in all of my test model, and work perfectly, thanks.\r\n```\r\ninterpreter = tflite.Interpreter(model_path=\"densenet/densenet.tflite\")\r\nX = img.reshape([1, 32, width, 1])\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (1, 32, width, 1))\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(input_details[0]['index'], X)\r\ninterpreter.invoke()\r\nY = interpreter.get_tensor(output_details[0]['index'])\r\n```", "@gds101054108 Can you please verify once and close the issue if this was resolved for you. Couple of other above who had similar issue like you confirmed that `None` in non-batch dimension work as expected. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> We added support for unknown dimensions in TensorFlow Lite today ([5591208](https://github.com/tensorflow/tensorflow/commit/55912083e2f16087c2f29394acf8a6a4811a2ce0)).\r\n> \r\n> Can you try converting your model again with tonight's (1/31) `tf-nightly` once it's released (`pip install tf-nightly`). Convert the model with `experimental_new_converter = True`.\r\n> \r\n> When you load the model it should have an additional field `shape_signature` that contains the shape with any unknown dimensions marked with `-1`. `shape` will have those dimensions marked with `1`.\r\n> \r\n> You can then call `ResizeInputTensor` with the desired shape when running the interpreter. The generated model will only work on the latest TensorFlow version (i.e. the interpreter on the `tf-nightly` version you are running).\r\n> \r\n> If it does not work, can you provide a detailed error and repro instructions?\r\n\r\nHi\uff01I am very curious for this trick, but I don't know how to achieve this. Could you give me a example? \r\n![image](https://user-images.githubusercontent.com/27290494/83724621-2b87ee80-a673-11ea-9999-09a98bc1b181.png)\r\nActually, I follow the guide of this guy, and it seems to work. So is the last step necessary?\r\n![image](https://user-images.githubusercontent.com/27290494/83724800-76a20180-a673-11ea-988a-05a8e624426f.png)\r\n", "hey @james34602  @jvishnuvardhan @gargn \r\nHey! did it work for None Type? \r\nI see that your input shape is [1, 32, width, 1],\r\nIs it going to work for something like [None,None,None,4]\r\nI tried it for [None,None,None,4], it did not work, any suggestions?\r\n\r\nAlso, [1, 32, width, 1] is not similar to [1, 32,None, 1] right?", "@purva98 \r\ntfnightly solve many problems, give it a shot.\r\n[1, 32, width, 1] is [1, 32,None, 1]\r\nwidth is not predetermined.", "Good morning\n\nOn Sun, Jun 7, 2020, 10:26 AM James Fung <notifications@github.com> wrote:\n\n> @purva98 <https://github.com/purva98>\n> tfnightly solve many problems, give it a shot.\n> [1, 32, width, 1] is [1, 32,None, 1]\n> width is not predetermined.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/29590#issuecomment-640176281>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIT525WESAGKJFSY3SWJ2W3RVNFKBANCNFSM4HWRWFCQ>\n> .\n>\n", "Hey @james34602 @jvishnuvardhan @gargn \r\n\r\nI have been facing similar issues listed in this thread.\r\nI have a model trained with tensorflow 1.15 and wanted to convert it to a tflite model, and have been using the ```tf-nightly``` package for it (version: 2.4.0-dev20200714).\r\n\r\nSince the model requires **dynamic input sizes** I  had set the width and height to ```None``` and after converting it to tflite this was the input_tensors : \r\n[{'dtype': numpy.float32,\r\n  'index': 0,\r\n  'name': 'input_image:0',\r\n  'quantization': (0.0, 0),\r\n  'quantization_parameters': {'quantized_dimension': 0,\r\n   'scales': array([], dtype=float32),\r\n   'zero_points': array([], dtype=int32)},\r\n  'shape': array([1, 1, 1, 3], dtype=int32),\r\n  'shape_signature': array([ 1, -1, -1,  3], dtype=int32),\r\n  'sparsity_parameters': {}}]\r\n\r\nFor resizing and setting the input data, I followed this snippet, here ```batch_image``` is of shape: [1, height, width, 3].\r\n\r\n```python\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (1, batch_image.shape[1], batch_image.shape[2], 3), strict=True)\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n```\r\n\r\nYou can see the input details shape has changed after executing the above snippet\r\n ```python \r\ninterpreter.get_input_details() \r\n```\r\n[{'dtype': numpy.float32,\r\n  'index': 0,\r\n  'name': 'input_image:0',\r\n  'quantization': (0.0, 0),\r\n  'quantization_parameters': {'quantized_dimension': 0,\r\n   'scales': array([], dtype=float32),\r\n   'zero_points': array([], dtype=int32)},\r\n  'shape': array([  1, 286, 300,   3], dtype=int32),\r\n  'shape_signature': array([ 1, -1, -1,  3], dtype=int32),\r\n  'sparsity_parameters': {}}]\r\n\r\nBut on executing ```interpreter.invoke()```, I am facing 2 issues:\r\n1. **Runtime error** on some images, these same images work perfectly fine without tflite.\r\n```python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-59-7d35ed1dfe14> in <module>()\r\n----> 1 interpreter.invoke()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in invoke(self)\r\n    522     \"\"\"\r\n    523     self._ensure_safe()\r\n--> 524     self._interpreter.Invoke()\r\n    525 \r\n    526   def reset_all_variables(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/kernel_util.cc:249 d1 == d2 || d1 == 1 || d2 == 1 was not true.Node number 49 (ADD) failed to prepare.\r\ntensorflow/lite/kernels/kernel_util.cc:249 d1 == d2 || d1 == 1 || d2 == 1 was not true.Node number 37 (ADD) failed to prepare.\r\n```\r\n2. Works on certain images **but very high execution time** (orders of 10x increase in time) than normally executing the model without tflite directly using the savedmodel format.\r\n\r\nI have put together a **Colab notebook** to reproduce both the situations. \r\nhttps://colab.research.google.com/drive/1OqzaBvzOCKOr6G-R7mQVPsnkhQm51jfY?usp=sharing\r\n\r\n**Model Files and Test Images** (for both cases):\r\nhttps://drive.google.com/file/d/1JErOmr9kyQYJFfTW9QwrZMJ6gM_3e9ZH/view?usp=sharing\r\n\r\n- Can you please help me and tell me the problem and possible solution for this.\r\n- Can you also please share what's the recommended way to use tflite for dynamic input size models with the performance gains similar to which tflite says we can achieve?", "- Mac OS\r\n- TF 2.3.0\r\n\r\nIt still has the problem.\r\n\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (416 != 0)Node number 11 (RESHAPE) failed to prepare.\r\n", "I have the same problem as @tjdevWorks .\r\nWhen using:\r\n```{python}\r\ninterpreter = tf.lite.Interpreter(model_path=path)\r\ninput_details = interpreter.get_input_details()\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (1, 640, 427, 3))\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\n```\r\n\r\nI get:\r\n```{python}\r\n---------------------------------------------------------------------------\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n\r\n<ipython-input-70-8ab2d11515fd> in <module>()\r\n     12 interpreter.set_tensor(input_details[0]['index'], input_data)\r\n     13 \r\n---> 14 interpreter.invoke()\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py in invoke(self)\r\n    538     \"\"\"\r\n    539     self._ensure_safe()\r\n--> 540     self._interpreter.Invoke()\r\n    541 \r\n    542   def reset_all_variables(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/kernel_util.cc:404 d1 == d2 || d1 == 1 || d2 == 1 was not true.Node number 85 (ADD) failed to prepare.\r\n\r\n```\r\n\r\n@jvishnuvardhan Does any news on this exist or can somebody provide some help?"]}, {"number": 29589, "title": "Adding missing header files to the micro_speech Makefile.inc-file.", "body": "Add missing header files to the micro_speech Makefile.inc to fix build\r\nerrors when compiling parts of micro_speech for Mbed OS.", "comments": ["@jenselofsson Can you please check build failures? Thanks!", "From what I can see, none of the errors are related to the changes in the Makefile in Tensorflow Lite Micro.", "@jenselofsson Could you please resolve the conflicts? Thanks!", "@jenselofsson  gentle ping to resolve the conflicts. Thanks!", "@gbaned Conflicts resolved.", "Can one of the admins verify this patch?", "Sorry it looks like this might have bit-rotted a bit. I cleaned up the patch and attached here. Do you want to verify this works? I think we patched in a few files recently.\r\n\r\nPlease @ me and we can get this merged - thank you for the fix!!\r\n\r\n", "@nkreeger I'll take a look at the commits and see if they're still doing anything useful. ", "@nkreeger It seems like the files added to the Makefile by this PR is already in the Makefile on master, so I'll close this PR."]}, {"number": 29588, "title": "Segmentation fault model_from_json", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Linux Ubuntu 16.04 LTS:\r\n- TensorFlow installed with pip install:\r\n- TensorFlow version 1.13.1\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.0/?\r\n- GPU model and memory: Tesla K80 11GB (even after disconnecting the card from VM app still crashed)\r\n\r\nTensorflow loads fine, yet program crashes with segmentation error at load_from_json action.\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy.core.multiarray\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom keras.models import load_model, model_from_json\r\n\r\njson_file = open(os.path.join('/'.join(FLAGS.model_path.split('/')[0:-1]), 'model.json'), 'r')\r\nloaded_model_json = json_file.read()\r\njson_file.close()\r\nmodel = model_from_json(loaded_model_json, custom_objects={'tf': tf, 'RESIZE_FACTOR': RESIZE_FACTOR})    <<<<<  This line crashes\r\nmodel.load_weights(FLAGS.model_path)\r\n```\r\n\r\nExactly the same code runs smoothly on my laptop with only CPU, windows 10 and python 3.7.\r\n", "comments": ["@derNorweg Could you provide a standalone code to reproduce the issue? You could make a gitHub gist and share the link. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29588\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29588\">No</a>\n"]}, {"number": 29587, "title": "when i install tensorflow2.0 ,I run it it told me that:", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\heyx\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\heyx\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\heyx\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\heyx\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\heyx\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\n", "comments": ["I dont think tensorflow 2.0 supports CUDA 10.1 yet. [Details here.](https://www.tensorflow.org/install/gpu)\r\nPlease install CUDA 10 and CuDNN 7.6 and try again. Please also close as this is not an issue with tensorflow itself but issue with your installed version.", "If you know Anaconda, you can follow this [guide ](https://medium.com/@shaolinkhoa/install-tensorflow-gpu-2-0-alpha-on-anaconda-for-windows-10-ubuntu-ced099010b21?source=friends_link&sk=0be0817547ef42e0670552c6814a423a) to install tensorflow 2 beta on anaconda. It will be easier to install.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29586, "title": "when i install tensorflow2.0 ,I run it it told me that:", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["duplicate #29587"]}, {"number": 29585, "title": "lite/micro/micro_vision: fix makefile typo", "body": "/cc @dansitu", "comments": ["Changes have been pushed by different commit already , thanks for your contribution , closing this "]}, {"number": 29584, "title": "TF 2.0 beta0: Can't use tf.keras.layers.LSTM on GPU.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Linux d8cb89680c63 4.14.79)\r\n- TensorFlow installed from (source or binary): binary (using pip)\r\n- TensorFlow version (use command below): tensorflow-gpu-2.0.0b0\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: V10.0.130\r\n- GPU model and memory: Tesla T4  15079MiB\r\n\r\n**Describe the current behavior**\r\nAfter `!pip install tensorflow-gpu==2.0.0-beta0`, and restart Google colab runtime. Try to fit the model, got the following log, then it just stay there 5 minutes plus, then start logging the train steps. Besides the training is very slow, so I suspect it is trained on CPU.\r\nBut it **can** train on GPU when I `!pip install tensorflow-gpu==2.0.0-alpha0`\r\n```\r\n2019-06-10 03:10:39.688276: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-06-10 03:10:39.775107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-10 03:10:39.775558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:00:04.0\r\n2019-06-10 03:10:39.798707: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-10 03:10:39.958916: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-10 03:10:40.032897: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-10 03:10:40.050421: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-10 03:10:40.183197: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-10 03:10:40.286011: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-10 03:10:40.620983: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-10 03:10:40.621231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-10 03:10:40.621760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-10 03:10:40.622122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-06-10 03:10:40.628272: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-10 03:10:40.800975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-10 03:10:40.804767: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29ac300 executing computations on platform CUDA. Devices:\r\n2019-06-10 03:10:40.804807: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\r\n2019-06-10 03:10:40.888533: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-06-10 03:10:40.888737: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29acbc0 executing computations on platform Host. Devices:\r\n2019-06-10 03:10:40.888768: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-10 03:10:40.889062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-10 03:10:40.889477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:00:04.0\r\n2019-06-10 03:10:40.889555: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-10 03:10:40.889580: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-10 03:10:40.889601: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-10 03:10:40.889623: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-10 03:10:40.889641: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-10 03:10:40.889659: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-10 03:10:40.889679: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-10 03:10:40.889776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-10 03:10:40.890166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-10 03:10:40.890578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-06-10 03:10:40.893860: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-10 03:10:40.895061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-10 03:10:40.895092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-06-10 03:10:40.895105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-06-10 03:10:40.902720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-10 03:10:40.903146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-10 03:10:40.903526: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2019-06-10 03:10:40.903571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\r\n2019-06-10 03:10:40.928132: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 388579200 exceeds 10% of system memory.\r\n2019-06-10 03:10:41.602882: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 388579200 exceeds 10% of system memory.\r\n2019-06-10 03:10:41.835533: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 388579200 exceeds 10% of system memory.\r\nI0610 03:10:42.373863 139896129251200 lstm.py:580] LSTM, act is tanh\r\nI0610 03:10:47.659146 139896129251200 lstm.py:716] build model -> done\r\nW0610 03:10:47.744854 139896129251200 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n\r\nEpoch 00001: LearningRateScheduler reducing learning rate to 0.000625.\r\nEpoch 1/4\r\n2019-06-10 03:10:50.361292: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n2019-06-10 03:10:50.708566: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n2019-06-10 03:10:51.075378: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-06-10 03:10:51.252533: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-10 03:10:52.298371: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 55296000 exceeds 10% of system memory.\r\n2019-06-10 03:10:57.025235: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 388579200 exceeds 10% of system memory.\r\n```\r\n**Describe the expected behavior**\r\nModel fitting should run successfully in GPU.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, PReLU\r\n...\r\n...\r\n    def build_lstm_model_customed(self, num_aux_targets, with_aux=False, loss='binary_crossentropy', metrics=None,\r\n                                  hidden_act='relu', with_BN=False):\r\n        logger.debug(f'model detail: loss {loss}, hidden_act {hidden_act}, with_BN {with_BN}')\r\n        words = Input(shape=(d.MAX_LEN,))  # (None, 180)\r\n        x = Embedding(*self.embedding_matrix.shape, weights=[self.embedding_matrix], trainable=False)(words)\r\n\r\n        x = SpatialDropout1D(0.2)(x)\r\n        logger.debug(\"LSTM, act is tanh\")\r\n        x = Bidirectional(LSTM(LSTM_UNITS, activation='tanh', return_sequences=True))(x)\r\n        x = Bidirectional(LSTM(LSTM_UNITS, activation='tanh', return_sequences=True))(x)\r\n\r\n        hidden = concatenate([\r\n            AttentionRaffel(d.MAX_LEN, name=\"attention_after_lstm\")(x),\r\n            GlobalMaxPooling1D()(x),\r\n            #GlobalAveragePooling1D()(x),\r\n        ])\r\n\r\n        activate_type = hidden_act\r\n        if activate_type == 'prelu':  # found it not working\r\n            hidden = add([hidden, PReLU()(Dense(DENSE_HIDDEN_UNITS, activation=None)(hidden))])\r\n            if with_BN: hidden = BatchNormalization()(hidden)\r\n            hidden = add([hidden, PReLU()(Dense(DENSE_HIDDEN_UNITS, activation=None)(hidden))])\r\n        else:\r\n            hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation=activate_type)(hidden)])\r\n            if with_BN: hidden = BatchNormalization()(hidden)\r\n            hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation=activate_type)(hidden)])\r\n\r\n        logit = Dense(1, activation=None)(hidden)\r\n        result = Activation('sigmoid')(logit)\r\n\r\n        if with_aux:\r\n            aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\r\n            model = Model(inputs=words, outputs=[result, aux_result])\r\n        else:\r\n            model = Model(inputs=words, outputs=result)\r\n\r\n        model.compile(loss=loss, optimizer='adam', metrics=metrics)\r\n\r\n        return model\r\n\r\n    model = build_lstm_model_customed(0)\r\n    model.fit(X,y)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #1 SMP Wed Dec 19 21:19:13 PST 2018\r\nos release version: 4.14.79+\r\nos platform: Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\r\nlinux distribution: ('Ubuntu', '18.04', 'bionic')\r\nlinux os distribution: ('Ubuntu', '18.04', 'bionic')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='d8cb89680c63', release='4.14.79+', version='#1 SMP Wed Dec 19 21:19:13 PST 2018', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nmesh-tensorflow          0.0.5                \r\nmsgpack-numpy            0.4.3.2              \r\nnumpy                    1.16.4               \r\nprotobuf                 3.7.1                \r\ntensorflow               1.13.1               \r\ntensorflow-estimator     1.13.0               \r\ntensorflow-gpu           2.0.0b0              \r\ntensorflow-hub           0.4.0                \r\ntensorflow-metadata      0.13.0               \r\ntensorflow-probability   0.6.0                \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.0.0-beta0\r\ntf.version.GIT_VERSION = v1.12.1-3259-gf59745a381\r\ntf.version.COMPILER_VERSION = 4.8.5\r\n```", "comments": ["@pennz Looks code snippet is incomplete. In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!\r\n", "@gadagashwini Thanks for your help. Testing with minimal code really helps. I tested with a minimal code snippet, and the LSTM can work in the GPU. This is the code snippet for testing LSTM: [url](https://colab.research.google.com/drive/1fanPZaoBZgfGLdLCnzba1ExLK6zot92T).\r\n\r\nSo the problem is somewhere else. Close this issue.\r\n\r\nThanks."]}, {"number": 29583, "title": "bulid form source 1.13.1 no such package '@org_sqlite//", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):centos7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:1.13.1\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:virtualenv\r\n- Bazel version (if compiling from source):0.25.3\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:\r\n\r\nwhen i build tf  from source and run this command:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nthere is a error :no such package '@org_sqlite//:\r\n`ERROR: /tmp/tf/tensorflow/tensorflow/contrib/summary/BUILD:65:1: no such package '@org_sqlite//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/www.sqlite.org/2019/sqlite-amalgamation-3280000.zip, https://www.sqlite.org/2019/sqlite-amalgamation-3280000.zip] to /home/tico/.cache/bazel/_bazel_tico/111e4735042cad380f0b87cff4bce3f6/external/org_sqlite/sqlite-amalgamation-3280000.zip: All mirrors are down: [GET returned 404 Not Found, sun.security.validator.ValidatorException: PKIX path bu`\r\n\r\ni think the  download url is wrong , when i open this url:\r\n http://mirror.tensorflow.org/www.sqlite.org/2019/sqlite-amalgamation-3280000.zip\r\nthe page can not open, and in the  http://mirror.tensorflow.org/ page ,for www.sqlite.org key,there only have this:\r\n\r\n```\r\n<Contents>\r\n<Key>\r\nwww.sqlite.org/2018/sqlite-amalgamation-3240000.zip\r\n</Key>\r\n<Generation>1553535068786424</Generation>\r\n<MetaGeneration>2</MetaGeneration>\r\n<LastModified>2019-03-25T17:31:08.786Z</LastModified>\r\n<ETag>\"4ea1e0c6e7e82cb0490d4753d6878698\"</ETag>\r\n<Size>2206612</Size>\r\n</Contents>\r\n```\r\n\r\nso , i  vi  the workspace.bzl  file  and  change the  sqlite download url http://mirror.tensorflow.org/www.sqlite.org/2019/sqlite-amalgamation-3280000.zip to  https://mirror.bazel.build/www.sqlite.org/2018/sqlite-amalgamation-3240000.zip\r\n,then it works!\r\n", "comments": ["We haven't generated yet the new mirror and the 1.13 branch is in a state for 1.13.2 patch release. sorry for the issues this raised. I expect that by the end of the week this should be fixed.", "Hi mihaimaruseac,\r\n\r\nI think the mirror is down again. I got the following error when building TensorFlow/2.0.0-beta1 from source.\r\n\"\"\"\r\nAn error occurred during the fetch of repository 'org_sqlite':\r\n   java.io.IOException: Error downloading [http://mirror.tensorflow.org/www.sqlite.org/2019/sqlite-amalgamation-3280000.zip, https://www.sqlite.org/2019/sqlite-amalgamation-3280000.zip] to /short/z00/yxs900/tmp/_bazel_yxs900/7d93e1dcd15cd969c36cafda24edd905/external/org_sqlite/sqlite-amalgamation-3280000.zip: All mirrors are down: [GET returned 404 Not Found, sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target]\r\n\"\"\"\r\n\r\nCheers\r\nYue", "Thank you for pointing this out. I will investigate and try to fix quick", "Hi mihaimaruseac,\r\nMany thanks to the prompt reply. Please let me know when it is fixed and I will resume compiling then. \r\nCheers\r\nYue", "The mirror should be fixed now."]}, {"number": 29582, "title": "TFLite build failed in 7348ee578693bb5aacdb45a1be746f0c933fc65f", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:7348ee578693bb5aacdb45a1be746f0c933fc65f\r\n- Python version:2.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI use tensorflow/tensorflow/lite/java/demo in Android studio.\r\nAndroid studio version:3.1.2 Andoird_NDK=r16b\r\nrun build, it comes error:\r\n<img width=\"1351\" alt=\"image\" src=\"https://user-images.githubusercontent.com/38650344/59168984-dbaf3e00-8b6a-11e9-98d5-c69cd43a6c2c.png\">\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nThe log info is:\r\nCould not download tensorflow-lite-gpu.aar (org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly)\r\n   > Could not get resource 'https://jcenter.bintray.com/org/tensorflow/tensorflow-lite-gpu/0.0.0-nightly/tensorflow-lite-gpu-0.0.0-nightly.aar'.\r\n      > Could not GET 'https://jcenter.bintray.com/org/tensorflow/tensorflow-lite-gpu/0.0.0-nightly/tensorflow-lite-gpu-0.0.0-nightly.aar'.\r\n         > Connect to d29vzk4ow07wi7.cloudfront.net:443 [d29vzk4ow07wi7.cloudfront.net/13.224.42.66, d29vzk4ow07wi7.cloudfront.net/13.224.42.221, d29vzk4ow07wi7.cloudfront.net/13.224.42.134, d29vzk4ow07wi7.cloudfront.net/13.224.42.149] failed: Read timed out\r\n\r\n", "comments": ["Now it occurs other problem: \u7a0b\u5e8f\u5305org.tensorflow.lite.nnapi\u4e0d\u5b58\u5728\r\nimport org.tensorflow.lite.nnapi.NnApiDelegate; error, can't find this \r\n", "Note that you need to include *both* the tensorflow-lite-gpu lib *and* the `tensorflow-lite` library in your gradle build. You might need to [clear your gradle cache](https://stackoverflow.com/questions/23025433/how-to-clear-gradle-cache) for the NNAPI library to appear.", "Thanks this problem has been solved"]}, {"number": 29581, "title": "Inconsistency in Input Pipeline code block for test_dataset", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/pix2pix.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe code block below appears under the heading `Input Pipeline`.\r\nIt appears to be an almost, but not quite, semantic copy of the code block above it for the \r\n`train_dataset`, except that, in this code block for the `test_dataset` we are shuffling the\r\n`train_dataset` again.\r\n\r\n```\r\ntest_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\r\n# shuffling so that for every epoch a different image is generated\r\n# to predict and display the progress of our model.\r\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE)\r\ntest_dataset = test_dataset.map(load_image_test)\r\ntest_dataset = test_dataset.batch(1)\r\n```\r\n\r\nSo, I think that this code block should be:\r\n\r\n```\r\ntest_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\r\n# shuffling so that for every epoch a different image is generated\r\n# to predict and display the progress of our model.\r\ntest_dataset = test_dataset.shuffle(BUFFER_SIZE)\r\ntest_dataset = test_dataset.map(load_image_test)\r\ntest_dataset = test_dataset.batch(1)\r\n```\r\n\r\nAlso note that the code block for the `train_dataset` has:\r\n\r\n```\r\ntrain_dataset = train_dataset.map(load_image_train,\r\n                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n```\r\n\r\nBut the code block for the `test_dataset` only has:\r\n```\r\ntest_dataset = test_dataset.map(load_image_test)\r\n```\r\n\r\nThe `AUTOTUNE ` is noted here, but not explained, so I can't easily comment on what its use,\r\nor lack of, actually means:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/experimental#AUTOTUNE\r\n\r\n### Submit a pull request?\r\n\r\nI can if this is actually incorrect.", "comments": ["@NathanDotTo Thanks for finding this. I think this is a typo `train_dataset = train_dataset.shuffle(BUFFER_SIZE)`. \r\n\r\nFor more details on AUTOTUNE check this [source](https://www.tensorflow.org/beta/guide/data_performance#pipelining). Thanks!", "@NathanDotTo,\r\nThe code snippet has been updated in the [latest version](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb) of the tutorial.\r\n\r\nCloud you please let us know if this still an issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29580, "title": "Fix tf.keras.dataset.mnist/reuters np v1.16.3", "body": "This fix add `allow_pickle=True` to fix the numpy issue in 1.16.3.\r\n\r\nSee keras-team/keras@3423197\r\nand keras-team/keras#12714\r\n\r\nfor related changes in keras-team/keras.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Could you please resolve the conflicts? Thanks!", "@gbaned Thanks, the PR has been updated.", "Can one of the admins verify this patch?"]}, {"number": 29579, "title": "pip_package: reorganize includes for tf.sysconfig.get_compile_flags()", "body": "When tensorflow was moved to tensorflow_core the includes were renamed\r\nas well and no longer worked. This renames them back and cleans up the\r\nincludes.\r\n\r\nThe previous fix for this moved the files around without the wheel\r\nmanifest being updated. This changes the file path in the header install\r\nso they are correct in the manifest.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>\r\n\r\n@angersson @mihaimaruseac @byronyi This fixes it on my machine but can you verify it too?\r\n", "comments": ["I'm not completely sure the bit for --nolegacy_external_runfiles is right. Can you guys verify that too?", "It works in the sense of get_compile_flags. Thanks, @perfinion!", "Fixed the pylint error (line too long) in ubuntu-sanity", "Thanks. I fixed some merge conflict internally, that's why Copybara didn't autoclose"]}, {"number": 29578, "title": "Tensorflow 2.0 graph slower than eager for dynamic batch sizes", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Linux, Colaboratory\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): unknown 2.0.0-beta0\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: Quadro M1000M, Colaboratory, ...\r\n\r\n**Describe the current behavior**\r\nPassing data with variable batch sizes to a training graph (as created by `tf.function`) causes the graph execution to be significantly slower than just executing the training in eager mode. It's also significantly slower than executing the same inputs with graph mode in TF1.x\r\n\r\nWhile passing data with variable batch sizes is uncommon on most DL tasks, it is required for tasks on Geometric Learning. For example, if your data is graphs, even if you fix the same number of graphs per batch, you can still get a varying number of nodes or edges.\r\n\r\n**Describe the expected behavior**\r\nGraph mode should always be faster than eager mode.\r\nGraph mode in TF2.0 should be at least approximately as fast as graph mode in TF 1.x.\r\n\r\n**Code to reproduce the issue**\r\n[Colab notebook available here](https://colab.research.google.com/drive/1xUVb0NAeOdefofgjKQhL_MB-XPILdSo_). Running with GPU makes eager mode significantly faster and the time difference more noticeable.\r\n", "comments": ["@mmv I was able to reproduce the issue with TF2.0-beta0. Thanks!", "I had a quick look at the colab. The code is fairly complex which makes it difficult to identify specific inefficiencies.\r\n\r\n@martinwicke could you route this to the appropriate person? Looks like a performance regression in graph mode.\r\n\r\nIt would be great to have a smaller reproducing example, but I believe that might be difficult to find.", "I've added another cell at the end of the Colab notebook to show that the graph-compiled function is only slow for inputs with sizes it hasn't yet seen. If dimensions are repeated, the execution time is fast, as expected.", "This is possibly related to #29075.", "mmv@ have you tested this with TensorFlow 2.0.0  that has been released, \r\nfix for https://github.com/tensorflow/tensorflow/issues/29075 that was referenced above has been cherrypicked into the r2.0 branch. \r\n", "@goldiegadde I ran the code (@mmv code) with `TF2.0.0` and `TF2.0.0b0`.  I noticed `TF2.0.0` is consistently faster than `TF2.0.0b0`.  Please check the [gist with `TF2.0`](https://colab.sandbox.google.com/gist/jvishnuvardhan/e16e60b3356be6284ca1bd84ebd2a1c9/tf2-bugs-graph-slower-than-eager.ipynb) and the [gist with `TF2.0.0b0`](https://colab.sandbox.google.com/gist/jvishnuvardhan/33f145cf86aece336a71da4924a85908/tf2-bugs-graph-slower-than-eager.ipynb). Thanks!", "This has been resolved with along with #29075 \u2014 I'll go ahead and close it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29578\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29578\">No</a>\n"]}, {"number": 29577, "title": "lite/micro/micro_vision: Missing separator at line 21.", "body": "Line separator is needed to get the makefile to work.\r\n \r\ne.g. \r\n```make -f tensorflow/lite/experimental/micro/tools/make/Makefile test``` \r\nwill return a **missing separator error**.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29577) for more info**.\n\n<!-- need_sender_cla -->", "Thank you for the PR, please sign CLA, otherwise we won't be able to merge", "- [x] `I signed it!` ", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29577) for more info**.\n\n<!-- ok -->", "Thanks for your contribution , similar changes were pushed to master , so closing this PR"]}, {"number": 29576, "title": "Different variable names in keras layers depending on how layer is used", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary (conda defaults channel)\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION = 1.13.1\r\ntf.version.GIT_VERSION = b'unknown'\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nI am getting two different variable names from keras layers when variables are built.\r\nIf I create a layer and call it on some input, everything works as expected.\r\nHowever, if I create a layer and call  `layer.build(input_shape)`, the variable names are different.\r\n**Describe the expected behavior**\r\nI expect to get the same variable names in a layer no matter how layer is used.\r\n\r\n**Code to reproduce the issue**\r\nCalling Keras layers on some input results in different variable names compared to calling layer.build:\r\n```python\r\nIn [1]: import tensorflow as tf                                                                                                                                \r\n\r\nIn [2]: layer1 = tf.keras.layers.Dense(3, name=\"dense_1\")                                                                                                      \r\n\r\nIn [3]: layer1.variables                                                                                                                                       \r\nOut[3]: []\r\n\r\nIn [4]: layer1.build((None, 3))                                                                                                                                \r\n\r\nIn [5]: layer1.variables                                                                                                                                       \r\nOut[5]: \r\n[<tf.Variable 'kernel:0' shape=(3, 3) dtype=float32>,\r\n <tf.Variable 'bias:0' shape=(3,) dtype=float32>]\r\n\r\nIn [6]: layer2 = tf.keras.layers.Dense(3, name=\"dense_2\")                                                                                                      \r\n\r\nIn [7]: layer2.variables                                                                                                                                       \r\nOut[7]: []\r\n\r\nIn [8]: layer2(tf.zeros((32, 3)))                                                                                                                              \r\nOut[8]: <tf.Tensor 'dense_2/BiasAdd:0' shape=(32, 3) dtype=float32>\r\n\r\nIn [9]: layer2.variables                                                                                                                                       \r\nOut[9]: \r\n[<tf.Variable 'dense_2/kernel:0' shape=(3, 3) dtype=float32>,\r\n <tf.Variable 'dense_2/bias:0' shape=(3,) dtype=float32>]\r\n```\r\nIn the first case, variable names are:\r\n```\r\n[<tf.Variable 'kernel:0' shape=(3, 3) dtype=float32>,\r\n <tf.Variable 'bias:0' shape=(3,) dtype=float32>]\r\n```\r\nwhile in the second case, they are:\r\n```\r\n[<tf.Variable 'dense_2/kernel:0' shape=(3, 3) dtype=float32>,\r\n <tf.Variable 'dense_2/bias:0' shape=(3,) dtype=float32>]\r\n```\r\n\r\nI expect the variables in the first layer to be prefixed with `dense_1/`.\r\n\r\n**Other info / logs**\r\nNone\r\n", "comments": ["The `Layer.__call__` automatically adds a parent name scope.", "Hello, is there any feedback available on this issue?", "@183amir `build` isn't called with a name_scope added, if you want this behavior, please do:\r\n\r\n```\r\nwith tf.name_scope(layer.name):\r\n  layer.build(...)\r\n```", "Thank you for the workaround but the reason that I have opened this issue is to ask for consistent behavior in layer variable names. Conceptually you might think that building a layer is the same thing as calling the layer on some input but you end up with two different behavior (variable names). Is there a reason behind this? I think having a consistent behavior is better than having to remember the different behavior.\r\n\r\nAlso, calling `.build` on a model that contains other layers seems to add the name_scope automatically for inner layers but not for itself. For example:\r\n```\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: layer1 = tf.keras.layers.Dense(3, name=\"dense_1\")\r\n\r\nIn [3]: model = tf.keras.Sequential([layer1], name=\"model\")\r\n\r\nIn [4]: model.build((None, 3))\r\n\r\nIn [5]: model.variables\r\nOut[5]:\r\n[<tf.Variable 'dense_1/kernel:0' shape=(3, 3) dtype=float32>,\r\n <tf.Variable 'dense_1/bias:0' shape=(3,) dtype=float32>]\r\n\r\nIn [6]: model_2 = tf.keras.Sequential([tf.keras.layers.Dense(3, name=\"dense_2\")], name=\"model_2\")\r\n\r\nIn [7]: model_2(tf.zeros((32, 3)))\r\nOut[7]: <tf.Tensor 'model_2/dense_2/BiasAdd:0' shape=(32, 3) dtype=float32>\r\n\r\nIn [8]: model_2.variables\r\nOut[8]:\r\n[<tf.Variable 'model_2/dense_2/kernel:0' shape=(3, 3) dtype=float32>,\r\n <tf.Variable 'model_2/dense_2/bias:0' shape=(3,) dtype=float32>]\r\n```\r\nThis is really confusing IMHO.", "I am trying to scope variable names in a nested manner (in tensorflow 2.3), but I am not able to do so:\r\n\r\n1. Only `tf.keras.layers.Layer` can be scoped in a nested manner, `tf.keras.Model` and `tf.keras.Sequential` do not work. For example:\r\n```python\r\nlayer = tf.keras.layers.Dense(2, name=\"dense\")\r\nmodel = tf.keras.Sequential([layer], name=\"model\")\r\nprint(model.built)\r\nwith tf.name_scope(\"parent\"):\r\n    with tf.name_scope(model.name):\r\n        model.build((None, 3))\r\nprint([v.name for v in model.variables])\r\n```\r\nprints:\r\n```\r\nFalse\r\n['dense/kernel:0', 'dense/bias:0']\r\n```\r\nAs you can see model name or parent scope name are not in variable names.\r\n\r\nAnother example:\r\n```python\r\nlayer = tf.keras.layers.Dense(2, name=\"dense\")\r\nmodel = tf.keras.Sequential([layer], name=\"model\")\r\nprint(model.built)\r\nwith tf.name_scope(\"parent\"):\r\n    model(tf.zeros((32, 3)))\r\nprint([v.name for v in model.variables])\r\n```\r\nprints\r\n```\r\nFalse\r\n['dense/kernel:0', 'dense/bias:0']\r\n```\r\nAs you can see model name or parent scope name are not in variable names.\r\n\r\nBut when the model is called on `tf.keras.Input`:\r\n```python\r\nlayer = tf.keras.layers.Dense(2, name=\"dense\")\r\nmodel = tf.keras.Sequential([layer], name=\"model\")\r\nprint(model.built)\r\nwith tf.name_scope(\"parent\"):\r\n    model(tf.keras.Input((2,)))\r\nprint([v.name for v in model.variables])\r\n```\r\nprints\r\n```\r\nFalse\r\n['model/dense/kernel:0', 'model/dense/bias:0']\r\n```\r\nwhere now the model name is in the variable names but parent name is still missing!\r\n\r\n2. Even if using only `tf.keras.layers.Layer`, it cannot be scoped when it is called on `tf.keras.Input`:\r\n```python\r\nlayer_built = tf.keras.layers.Dense(2, name=\"built\")\r\nlayer_tensor_called = tf.keras.layers.Dense(2, name=\"tensor_called\")\r\nlayer_input_called = tf.keras.layers.Dense(2, name=\"input_called\")\r\n\r\ninputs = tf.keras.Input((3,))\r\n\r\nwith tf.name_scope(\"parent\"):\r\n    \r\n    with tf.name_scope(layer_built.name):\r\n        layer_built.build((None, 3))\r\n    \r\n    layer_tensor_called(tf.zeros((32, 3)))\r\n    \r\n    layer_input_called(inputs)\r\n\r\nprint([v.name for v in layer_built.variables])\r\nprint([v.name for v in layer_tensor_called.variables])\r\nprint([v.name for v in layer_input_called.variables])\r\n```\r\nwhich prints:\r\n```\r\n['parent/built/kernel:0', 'parent/built/bias:0']\r\n['parent/tensor_called/kernel:0', 'parent/tensor_called/bias:0']\r\n['input_called/kernel:0', 'input_called/bias:0']\r\n```\r\n\r\nAs you can see, the line where the layer is called on an input layer: `layer_input_called(inputs)` ignores its parent scope.\r\n\r\nI have tried as many combinations as possible to get consistent nested variable names but it seems impossible!", "Testing this with 2.4.1, the behavior has changed again! But now at least using the Sequential class, you can get consistent (broken) naming:\r\n```python\r\nIn [3]: layer = tf.keras.layers.Dense(2, name=\"dense\")\r\n   ...: model = tf.keras.Sequential([layer], name=\"model\")\r\n   ...: print(model.built)\r\n   ...: with tf.name_scope(\"parent\"):\r\n   ...:     with tf.name_scope(model.name):\r\n   ...:         model.build((None, 3))\r\n   ...: print([v.name for v in model.variables])\r\nIn [4]: layer = tf.keras.layers.Dense(2, name=\"dense\")\r\n   ...: model = tf.keras.Sequential([layer], name=\"model\")\r\n   ...: print(model.built)\r\n   ...: with tf.name_scope(\"parent\"):\r\n   ...:     model(tf.zeros((32, 3)))\r\n   ...: print([v.name for v in model.variables])\r\nIn [5]: layer = tf.keras.layers.Dense(2, name=\"dense\")\r\n   ...: model = tf.keras.Sequential([layer], name=\"model\")\r\n   ...: print(model.built)\r\n   ...: with tf.name_scope(\"parent\"):\r\n   ...:     model(tf.keras.Input((2,)))\r\n   ...: print([v.name for v in model.variables])\r\n```\r\nall the 3 code variants above, print:\r\n```\r\nFalse\r\n['dense/kernel:0', 'dense/bias:0']\r\n```\r\n\r\nAs a workaround, I suggest people explicitly add the name scope, like the example below:\r\n```python\r\ndef Conv2D_BN(\r\n    filters,\r\n    kernel_size,\r\n    strides=1,\r\n    padding=\"same\",\r\n    activation=\"relu\",\r\n    use_bias=False,\r\n    name=None,\r\n    **kwargs,\r\n):\r\n    \"\"\"Utility class to apply conv + BN.\r\n\r\n    # Arguments\r\n        x: input tensor.\r\n        filters:\r\n        kernel_size:\r\n        strides:\r\n        padding:\r\n        activation:\r\n        use_bias:\r\n\r\n    Attributes\r\n    ----------\r\n    activation\r\n        activation in `Conv2D`.\r\n    filters\r\n        filters in `Conv2D`.\r\n    kernel_size\r\n        kernel size as in `Conv2D`.\r\n    padding\r\n        padding mode in `Conv2D`.\r\n    strides\r\n        strides in `Conv2D`.\r\n    use_bias\r\n        whether to use a bias in `Conv2D`.\r\n    name\r\n        name of the ops; will become `name + '/Act'` for the activation\r\n        and `name + '/BatchNorm'` for the batch norm layer.\r\n    \"\"\"\r\n    if name is None:\r\n        raise ValueError(\"name cannot be None!\")\r\n\r\n    layers = [\r\n        Conv2D(\r\n            filters,\r\n            kernel_size,\r\n            strides=strides,\r\n            padding=padding,\r\n            use_bias=use_bias,\r\n            name=f\"{name}/Conv2D\",\r\n        )\r\n    ]\r\n\r\n    if not use_bias:\r\n        bn_axis = 1 if K.image_data_format() == \"channels_first\" else 3\r\n        layers += [BatchNormalization(axis=bn_axis, scale=False, name=f\"{name}/BatchNorm\")]\r\n\r\n    if activation is not None:\r\n        layers += [Activation(activation, name=f\"{name}/Act\")]\r\n\r\n    return tf.keras.Sequential(layers, name=name, **kwargs)\r\n```\r\n\r\nlayer naming still remains broken:\r\n```python\r\nIn [7]: layer_built = tf.keras.layers.Dense(2, name=\"built\")\r\n   ...: layer_tensor_called = tf.keras.layers.Dense(2, name=\"tensor_called\")\r\n   ...: layer_input_called = tf.keras.layers.Dense(2, name=\"input_called\")\r\n   ...: \r\n   ...: inputs = tf.keras.Input((3,))\r\n   ...: \r\n   ...: with tf.name_scope(\"parent\"):\r\n   ...: \r\n   ...:     with tf.name_scope(layer_built.name):\r\n   ...:         layer_built.build((None, 3))\r\n   ...: \r\n   ...:     layer_tensor_called(tf.zeros((32, 3)))\r\n   ...: \r\n   ...:     layer_input_called(inputs)\r\n   ...: \r\n   ...: print([v.name for v in layer_built.variables])\r\n   ...: print([v.name for v in layer_tensor_called.variables])\r\n   ...: print([v.name for v in layer_input_called.variables])\r\n['parent/built/kernel:0', 'parent/built/bias:0']\r\n['parent/tensor_called/kernel:0', 'parent/tensor_called/bias:0']\r\n['input_called/kernel:0', 'input_called/bias:0']\r\n```", "@183amir Is this still an issue for you? I ran your code with `TF2.4.1` and see consistent names for `model1` and `model2` . [Here](https://colab.research.google.com/gist/jvishnuvardhan/d573c4de1f69615083f357d33dbd701e/untitled.ipynb) is a gist for you reference. Thanks!\r\n\r\nPlease verify once with recent TF versions and close the issue if this was resolved for you. Thanks!", "Dear @jvishnuvardhan, yes, this is still an issue.\r\nPlease run in your notebook and observe the inconsistent name of variables:\r\n```python\r\nlayer_built = tf.keras.layers.Dense(2, name=\"built\")\r\nlayer_tensor_called = tf.keras.layers.Dense(2, name=\"tensor_called\")\r\nlayer_input_called = tf.keras.layers.Dense(2, name=\"input_called\")\r\n\r\ninputs = tf.keras.Input((3,))\r\n\r\nwith tf.name_scope(\"parent\"):\r\n\r\n    with tf.name_scope(layer_built.name):\r\n        layer_built.build((None, 3))\r\n\r\n    layer_tensor_called(tf.zeros((32, 3)))\r\n\r\n    layer_input_called(inputs)\r\n\r\nprint([v.name for v in layer_built.variables])\r\nprint([v.name for v in layer_tensor_called.variables])\r\nprint([v.name for v in layer_input_called.variables])\r\n```\r\nwhich will print:\r\n```\r\n['parent/built/kernel:0', 'parent/built/bias:0']\r\n['parent/tensor_called/kernel:0', 'parent/tensor_called/bias:0']\r\n['input_called/kernel:0', 'input_called/bias:0']\r\n```\r\nas you can see, the last layer (`layer_input_called`) gets different variable names which is inconsistent. I am expecting this code example to work without workarounds. Hence, this issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I am awaiting a tensorflower.", "@183amir \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29576\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29576\">No</a>\n"]}, {"number": 29575, "title": " DLL load failed: The specified module could not be found", "body": "when running code with keras library \r\nit ran at the first time , but when running it at the second time this exception appeared\r\n\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\tf-1-9-36\\python.exe \"C:/Users/TOSHIBA/PycharmProjects/omnia text detection/CNN.py\"\r\nTraceback (most recent call last):\r\n  File \"C:/Users/TOSHIBA/PycharmProjects/omnia text detection/CNN.py\", line 4, in <module>\r\n    import numpy as np\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf-1-9-36\\lib\\site-packages\\numpy\\__init__.py\", line 140, in <module>\r\n    from . import _distributor_init\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf-1-9-36\\lib\\site-packages\\numpy\\_distributor_init.py\", line 34, in <module>\r\n    from . import _mklinit\r\nImportError: DLL load failed: The specified module could not be found.\r\n", "comments": ["@omniaabo Please provide TensorFlow version. Also, did you compile from source or install a binary? Thanks!\r\n", "@gadagashwini  \r\nthanks for replaying \r\nI used python version 3.6.8 instead of 3.7.1\r\nit works ", "@omniaabo Good to know that it resolved. Will close this issue. Thanks!"]}, {"number": 29574, "title": "Op type not registered 'swish_f32' in binary running", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-9-a62a8332a64b> in <module>\r\n     10       verbose=1,\r\n     11     use_multiprocessing=True,\r\n---> 12     workers=4)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1513         shuffle=shuffle,\r\n   1514         initial_epoch=initial_epoch,\r\n-> 1515         steps_name='steps_per_epoch')\r\n   1516 \r\n   1517   def evaluate_generator(self,\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    255 \r\n    256       is_deferred = not model._is_compiled\r\n--> 257       batch_outs = batch_function(*batch_data)\r\n    258       if not isinstance(batch_outs, list):\r\n    259         batch_outs = [batch_outs]\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1256         outputs = self.train_function(ins)  # pylint: disable=not-callable\r\n   1257       else:\r\n-> 1258         self._make_fit_function()\r\n   1259         outputs = self._fit_function(ins)  # pylint: disable=not-callable\r\n   1260 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in _make_fit_function(self)\r\n   2175     ]\r\n   2176     self._make_train_function_helper(\r\n-> 2177         '_fit_function', [self.total_loss] + metrics_tensors)\r\n   2178 \r\n   2179   def _make_test_function_helper(self, fn_name, outputs):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in _make_train_function_helper(self, fn_name, outputs)\r\n   2160             updates=updates,\r\n   2161             name='train_function',\r\n-> 2162             **self._function_kwargs)\r\n   2163         setattr(self, fn_name, fn)\r\n   2164 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py in function(inputs, outputs, updates, name, **kwargs)\r\n   3241       raise ValueError('Session keyword arguments are not support during '\r\n   3242                        'eager execution. You passed: %s' % (kwargs,))\r\n-> 3243     return EagerExecutionFunction(inputs, outputs, updates=updates, name=name)\r\n   3244 \r\n   3245   if kwargs:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py in __init__(self, inputs, outputs, updates, name)\r\n   3162         lifted_map = lift_to_graph.lift_to_graph(\r\n   3163             init_tensors=init_tensors, graph=exec_graph, sources=inputs,\r\n-> 3164             add_sources=True, handle_captures=True, base_graph=source_graph)\r\n   3165 \r\n   3166         inputs = [lifted_map[i] for i in inputs]\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/lift_to_graph.py in lift_to_graph(init_tensors, graph, sources, disallowed_placeholders, add_sources, handle_captures, base_graph)\r\n    313         continue\r\n    314 \r\n--> 315       _copy_non_source(op=op, graph=graph, op_map=op_map)\r\n    316 \r\n    317     return op_map\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/lift_to_graph.py in _copy_non_source(op, graph, op_map)\r\n    158         dtypes=[x.dtype for x in op.outputs],\r\n    159         attrs=op.node_def.attr,\r\n--> 160         name=op.name)\r\n    161   op_map[op] = copied_op\r\n    162   for i, o in enumerate(op.outputs):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/func_graph.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n    450     return super(FuncGraph, self).create_op(\r\n    451         op_type, inputs, dtypes, input_types, name, attrs, op_def,\r\n--> 452         compute_device=compute_device)\r\n    453 \r\n    454   def capture(self, tensor, name=None):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in create_op(***failed resolving arguments***)\r\n   3477           input_types=input_types,\r\n   3478           original_op=self._default_original_op,\r\n-> 3479           op_def=op_def)\r\n   3480       self._create_op_helper(ret, compute_device=compute_device)\r\n   3481     return ret\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1975     else:\r\n   1976       if op_def is None:\r\n-> 1977         op_def = self._graph._get_op_def(node_def.op)\r\n   1978       # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.\r\n   1979       # Refactor so we don't have to do this here.\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _get_op_def(self, type)\r\n   3864     with c_api_util.tf_buffer() as buf:\r\n   3865       # pylint: disable=protected-access\r\n-> 3866       c_api.TF_GraphGetOpDef(self._c_graph, compat.as_bytes(type), buf)\r\n   3867       # pylint: enable=protected-access\r\n   3868       data = c_api.TF_GetBuffer(buf)\r\n\r\nNotFoundError: Op type not registered 'swish_f32' in binary running on tensorflow-20190607-122214. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nMy running environment:\r\nTensorflow 2.0 alpha,\r\nPython 3.5.3\r\non Google Cloud Platform.\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Also let us know whether you are using CPU/GPU version of TF 2.0 alpha and information about the platform (architecture and operating system). Thanks!\r\n", "Most likely is that 'swish_f32' is a kind of user defined function (even though originally from tf.nn.swish) and stored in the GraphDef's library.function part. Somehow such function is removed during some actions, such as tf.graph_util.remove_training_nodes.", "tf.compat.v1.disable_eager_execution() fixed that issue for me but I'm using the Python API for tflite convert.\r\n\r\nEDIT: That's not helping at all though, because later I get errors with the tflite converter because it needs to be in eager execution.", "> tf.compat.v1.disable_eager_execution() fixed that issue for me but I'm using the Python API for tflite convert.\r\n> \r\n> EDIT: That's not helping at all though, because later I get errors with the tflite converter because it needs to be in eager execution.\r\n\r\nIt finally works for me. Thank you so much!", "it looks like there is a new way (in TF 2.0?) to register gradients that works in eager mode:\r\n\r\n```python\r\n@tf.custom_gradient\r\ndef swish(features):\r\n    @tf.function\r\n    def _grad(grad):\r\n        sigmoid_features = tf.math.sigmoid(features)\r\n        activation_grad = (sigmoid_features * (1.0 + features * (1.0 - sigmoid_features)))\r\n        return grad * activation_grad\r\n\r\n    return features * tf.nn.sigmoid(features), _grad\r\n\r\n```", "> Most likely is that 'swish_f32' is a kind of user defined function (even though originally from tf.nn.swish) and stored in the GraphDef's library.function part. Somehow such function is removed during some actions, such as tf.graph_util.remove_training_nodes.\r\n\r\nThank you skyflynil.  I encountered the same error when convert the efficientnet model (pb file) to tensorRT format. Removing **tf.graph_util.remove_training_nodes** before **convert_variables_to_constants** does resolve this error for me.\r\n\r\nI am not sure if this change affects the final performance in inference though.", "> > Most likely is that 'swish_f32' is a kind of user defined function (even though originally from tf.nn.swish) and stored in the GraphDef's library.function part. Somehow such function is removed during some actions, such as tf.graph_util.remove_training_nodes.\r\n> \r\n> Thank you skyflynil. I encountered the same error when convert the efficientnet model (pb file) to tensorRT format. Removing **tf.graph_util.remove_training_nodes** before **convert_variables_to_constants** does resolve this error for me.\r\n> \r\n> I am not sure if this change affects the final performance in inference though.\r\n\r\n@captainst I think you can still keep the remove _training_nodes part. There is just some post processing to workaround this issue.\r\noption 1: copy back the function part of the graphdef. suppose the graphdef before/after removing training nodes are graph_def and new_graph_def, you can do\r\n```python\r\nfor function_def in graph_def.library.function:\r\n     if function_def.signature.name == 'swish_f32':\r\n        new_graph_def.library.function.extend([copy.deepcopy(function_def)])\r\n```\r\noption 2:\r\ninline the swish_f32 into the graphdef by using sigmoid and mul, it might be useful for TensorRT.\r\n", "BTW, the new efficientnet for edge tpu seems to be a better choice for gpu device and tensorrt.", "@skyflynil Many thanks. I am using jetson NANO with classification stuffs. Although removing **tf.graph_util.remove_training_nodes** when freezing the keras model eliminates the error, the generated tensorRT model has problems and generate equal probabilities for each category (I was using softmax with 4 outputs), so the result is something like (0.25, 0.25, 0.25, 0.25). \r\nI tried to inference using the normal tensorflow .pb file and it gave correct results. So I think that probably there is some incongruence during the process **Keras model -> tensorflow pb -> tensorRT model**. Still need to figure out.", "> > > Most likely is that 'swish_f32' is a kind of user defined function (even though originally from tf.nn.swish) and stored in the GraphDef's library.function part. Somehow such function is removed during some actions, such as tf.graph_util.remove_training_nodes.\r\n> > \r\n> > \r\n> > Thank you skyflynil. I encountered the same error when convert the efficientnet model (pb file) to tensorRT format. Removing **tf.graph_util.remove_training_nodes** before **convert_variables_to_constants** does resolve this error for me.\r\n> > I am not sure if this change affects the final performance in inference though.\r\n> \r\n> @captainst I think you can still keep the remove _training_nodes part. There is just some post processing to workaround this issue.\r\n> option 1: copy back the function part of the graphdef. suppose the graphdef before/after removing training nodes are graph_def and new_graph_def, you can do\r\n> \r\n> ```python\r\n> for function_def in graph_def.library.function:\r\n>      if function_def.signature.name == 'swish_f32':\r\n>         new_graph_def.library.function.extend([copy.deepcopy(function_def)])\r\n> ```\r\n> \r\n> option 2:\r\n> inline the swish_f32 into the graphdef by using sigmoid and mul, it might be useful for TensorRT.\r\n\r\nActually, some functions for graph optimization cause that problem.\r\nThese functions are :\r\n\r\n- strip_unused_lib.strip_unused\r\n- graph_util.remove_training_nodes \r\n- fold_batch_norms\r\n\r\nThe following code must be after these optimized functions.\r\n`for function_def in graph_def.library.function:\r\n     if function_def.signature.name == 'swish_f32':\r\n        new_graph_def.library.function.extend([copy.deepcopy(function_def)])`\r\n\r\n"]}, {"number": 29573, "title": "Tensorflow 2 eager execution is off by default", "body": "This code:\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nprint(tf.executing_eagerly())\r\n```\r\nProduces this output:\r\n```\r\n2.0.0-beta0\r\nFalse\r\n```\r\n\r\nI build it from source.\r\n", "comments": ["@seven-dev: How are you building it? I believe you need to specify `--config=v2`. I tried the following in colob without any issue:\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n!pip install tensorflow-gpu==2.0.0-beta0\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\nprint(tf.executing_eagerly())\r\n```\r\n\r\nClosing for now. But feel free to re-open if I missed something", "I just followed the guide but I checked out the branch \"2.0.0-beta0\":  \r\nhttps://www.tensorflow.org/install/source  \r\n\r\nI can't find that flag anywhere in bazelrc files hmm...", "The documentation needs to be updated, and we are working on fixing it. Thanks for bringing this to our attention.", "the guide is updated, https://www.tensorflow.org/install/source\r\ncan you please try and let us know ?", "I will leave it building today in the afternoon and tomorrow I'll tell you how it went. Thank you for your help.", "Yeah, it works :) Thank you very much!  \r\n```python\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n2.0.0-beta1\r\n>>> print(tf.executing_eagerly())\r\nTrue\r\n```", "Why is eager execution turned on by default? It's much slower. Shouldn't it be up to the user's discretion to turn it on if needed? "]}, {"number": 29572, "title": "Initialize Layer with weights loaded from files", "body": "```python\r\nembedding = tf.keras.layers.Embedding(1000, 64, \r\n    initializer=tf.keras.initializer.FileInitializer(\"weights.npy\", verify_shape=True))\r\n```\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nPresently there is no way to directly tell Tensorflow to initialize the weights of a layer from a file without loading them up first and passing them. This is not required and can be done at the target device itself using the callback.\r\n**Will this change the current api? How?**\r\nThe current API remains unchanged.\r\n**Who will benefit with this feature?**\r\nAnyone doing NLP, Vision, who might want to specificly load the weights of a layer from a file. This could also help in sharing weights of a few layers of a complicated neural network. It is definitely beneficial when working with word2vec like embedding layers. \r\n\r\nIf it's fine, I would like to get assigned to this. ", "comments": ["@prannayk \r\nPlease post this issue on[ keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999"]}, {"number": 29571, "title": "tflite: Slicing isn't compatible with quantisation ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v1.12.1-3185-g1a4a0aee1f 1.13.1\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\nI'm trying to implement shufflenet v2 in Tensorflow 2.0 with tflite, which requires slicing.  This works fine with float precision, however when I turn on quantization I get an error. \r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\nchannels = 64\r\n# Slicing & quantization together results in an error\r\nuse_slice = True\r\nquantize = True\r\n\r\ninput = tf.keras.layers.Input(shape=(channels))\r\nx = input\r\nx *= x\r\nif use_slice:\r\n    x = x[:, ::2]\r\n\r\nmodel = tf.keras.Model(inputs=[input], outputs=[x])\r\nmodel.summary()\r\n\r\n\r\ndef _gen_input(channels):\r\n    return tf.constant(np.random.uniform(0, 1, size=(1, channels,)), dtype=tf.float32)\r\n\r\n# Test normal tensorflow forward pass\r\nmodel(_gen_input(channels))\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nif quantize:\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ndef representative_data_gen():\r\n    for _ in range(100):\r\n        yield [_gen_input(channels)]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\ntflite_model = converter.convert()\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], _gen_input(channels))\r\ninterpreter.invoke()\r\ntflite_results = interpreter.get_tensor(output_details[0]['index'])\r\n```\r\n\r\nIf I turn off either 'use_slice' or 'quantise', it works fine - however with both on I get the following error:\r\n\r\nRuntimeError: tensorflow/lite/kernels/dequantize.cc:67 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteFloat16 was not true.Node number 3 (DEQUANTIZE) failed to prepare.\r\n", "comments": ["Any updates on this?  Please let me know if there's any more debug info required.", "This looks to have been fixed - just gave this a retest with the latest tf 2.0 nightly", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29571\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29571\">No</a>\n"]}, {"number": 29570, "title": "hi, anyone know how to fix this?", "body": "\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Deep_Learning\\envs\\tensorflow\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["This related to #22794\r\n\r\nThere should be a helpful comment for your error.\r\n\r\nMaybe you should use CUDA 10.0. You can look at this comment\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/22794#issuecomment-500099017\r\n\r\n", "https://github.com/tensorflow/tensorflow/issues/22794#issuecomment-500099017\r\n\r\nTry this configurations, don't forget to set path in environment variables", "> This related to #22794\r\n> \r\n> There should be a helpful comment for your error.\r\n> \r\n> Maybe you should use CUDA 10.0. You can look at this comment\r\n> \r\n> [#22794 (comment)](https://github.com/tensorflow/tensorflow/issues/22794#issuecomment-500099017)\r\n\r\n\r\n\r\n> [#22794 (comment)](https://github.com/tensorflow/tensorflow/issues/22794#issuecomment-500099017)\r\n> \r\n> Try this configurations, don't forget to set path in environment variables\r\n\r\nHey, thank you guys, I downgrade my Cudnn as you said and it worked out. "]}]