[{"number": 16565, "title": "Feature Request: Separated Name Spaces of RNN Cells for hidden weights and recurrent states", "body": "This feature request is based on the following question of StackOverflow, if it's not appropriate I will close this issue.\r\n\r\nhttps://stackoverflow.com/questions/48506422/indices-and-slicing-of-tensorflows-global-variables-of-kernels-for-hidden-weigh\r\n\r\n    placeholders = {\"inputs\":tf.placeholder(tf.float32, shape=[None, None, 1000])}\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(80)\r\n    outs, states = tf.nn.dynamic_rnn(cell=cell, inputs=placeholders[\"inputs\"], dtype=tf.float32)\r\n\r\nThis graph building gives us tf.global_variables() list as follows:\r\n\r\n    [<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(1080, 320) dtype=float32_ref>, <tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(320,) dtype=float32_ref>]\r\n\r\nThe proposed new feature separates two name spaces of \"kernel\": hidden weights and recurrent states\r\nThe expected tf.gloabal_variables() list with new feature is as follows:\r\n\r\n     [<tf.Variable 'rnn/basic_lstm_cell/kernel/hidden_weights:0' shape=(1000, 320) dtype=float32_ref>,<tf.Variable 'rnn/basic_lstm_cell/kernel/recurrent_state:0' shape=(80, 320) dtype=float32_ref>,<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(320,) dtype=float32_ref>]\r\n\r\nIn the new feature, a kernel name space has two sub spaces.\r\nrnn/basic_lstm_cell/kernel/hidden_weights\r\nrnn/basic_lstm_cell/kernel/recurrent_state\r\n\r\n", "comments": []}, {"number": 16564, "title": "Ndlstm dynamic batch size", "body": "Apparently, lstm2d.separable_lstm doesn't accept dynamic batch sizes (number of images). This pull request addresses issue https://github.com/tensorflow/tensorflow/issues/16510", "comments": ["I tried it for my own project and it's taking long to get to a step with a batch size greater than 1 in training. Any other ideas on how to make the batch size for ndlstm dynamic? ", "It's now working as intended (thanks to the suggestion of @facaiy in the mentioned issue). Test please.", "I forgot to revert the changes I made in lstm2d_test.py that was working for my previous implementation of lstm2d.py. My bad. Test again please. ", "I wasn't able to run tests in my own machine before so I couldn't really figure out if it works or not. I managed to run the tests now and it's working perfectly.", "I got an [error](https://source.cloud.google.com/results/invocations/c4cd9871-91f0-4a4b-84d6-cf3918b1e7ea/targets/%2F%2Ftensorflow%2Fcontrib%2Fbayesflow:hmc_test/log) that seems to be totally unrelated to the module I'm working on. ", "please resolve conflicts", "I'm closing this pull request now since ndlstm is removed from tensorflow:master."]}, {"number": 16563, "title": "Minor refactor to remove redundant test class", "body": "There were two WeightNormLSTMCellTest classes that differed in formatting only.  Removed the (original) one at end of file since based on history, the version above it contained latest formatting updates.", "comments": ["This PR was small subset of changes since addressed in [PR 16647](https://github.com/tensorflow/tensorflow/pull/16647) which handles all recent sanity issues.."]}, {"number": 16562, "title": "Fix typo", "body": "fix typo", "comments": []}, {"number": 16561, "title": "Parse toco generated file (.tflite) in python?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI am using toco to optimize a frozen model (.pb). How do I read the .tflite file in python - something similar to tf.gfile.GFile('frozen.pb', 'rb')?\r\n", "comments": ["/CC @bjacob", "bump", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@santoshchilkunda, what exactly are you trying to do once you read it? Do you want to run inference on it. Do you want to analyze it?", "@aselle \r\nThanks for looking into this.\r\n\r\nI am trying to read/parse the model, convert it to our internal format, and then run inference on it.\r\n\r\nAs a workaround (to not being able to parse tflite), I set the output_format while running toco tool to TENSORFLOW_GRAPHDEF (i.e. both input and output formats are TENSORFLOW_GRAPHDEF). And then parse the generated protobuf.\r\n\r\nHowever, I see that protobuf is \"less\" optimized compared to tflite\r\n\r\nFollowing is the log when output_format is TENSORFLOW_GRAPHDEF:\r\nI tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 773 operators, 1072 arrays (0 quantized)\r\nI tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: **141 operators, 260 arrays** (0 quantized)\r\nI tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 141 operators, 260 arrays (0 quantized)\r\nI tensorflow/contrib/lite/toco/toco_tooling.cc:273] Estimated count of arithmetic ops: 3.01265 billion (note that a multiply-add is counted as 2 ops).\r\n\r\nFollowing is the log when output_format is set to TFLITE:\r\nI tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 773 operators, 1072 arrays (0 quantized)\r\nI tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: **84 operators, 203 arrays** (0 quantized)\r\nI tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 84 operators, 203 arrays (0 quantized)\r\nI tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 4014080 bytes, theoretical optimal value: 4014080 bytes.\r\nI tensorflow/contrib/lite/toco/toco_tooling.cc:273] Estimated count of arithmetic ops: 3.01265 billion (note that a multiply-add is counted as 2 ops).\r\n\r\nThe number of operators after graph transformation is different (141 v/s 84) while the estimated count of arithmetic ops is same (3.01265 billion).\r\nFrom a performance perspective, I am not completely sure if this means both the output formats are same or not.\r\n", "well you can use flatc to generate a python api that can read the tflite format.\r\nhttps://google.github.io/flatbuffers/flatbuffers_guide_use_python.html\r\nDoes that answer your question?\r\n", "I will try it and update. Thanks!\r\n\r\nRegarding the second part of my question, is there a performance difference between the two output formats?\r\n", "@santoshchilkunda, it is an inherent characteristic of the TensorFlow Lite flatbuffer format that it allows to represent the same neural network in fewer nodes than are needed in the TensorFlow GraphDef format, as you found from this logging. To find out more about what the difference is in your graph, use --dump_graphviz as explained there,\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#using---dump_graphviz", "dump_graphviz worked, thanks!\r\nStill working on flatc method that was suggested", "Another approach is to generate json from the flatbuffer using flatc. This is used by the tflite visualizer:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tools/visualize.py\r\nPlease close the issue if it is resolve. Thanks!\r\n", "Thanks for all the suggestions.\r\ndump_graphviz served my purpose. \r\nWill explore flat buffer method soon.", "> well you can use flatc to generate a python api that can read the tflite format.\r\n> https://google.github.io/flatbuffers/flatbuffers_guide_use_python.html\r\n> Does that answer your question?\r\n\r\n@aselle could you please make it a little bit clear for me \r\n\r\nI am using pyhon generated code to read and get node attributes from the model graph.\r\n\r\nI generated it this way (output is `tflite/` folder with autogenerated *.py files):\r\n`flatc -python tensorflow/tensorflow/lite/schema/schema.fbs`\r\n\r\nThan I read the model: \r\n\r\n```\r\n    from tflite.Model import Model\r\n    def read_tflite_model(file):\r\n        buf = open(file, \"rb\").read()\r\n        buf = bytearray(buf)\r\n        model = Model.GetRootAsModel(buf, 0)\r\n        return model\r\n\r\n```\r\nGetting model parameters:\r\n\r\n```\r\n    def print_model_info(model):\r\n        version = model.Version()\r\n        print(\"Model version:\", version)\r\n        description = model.Description().decode('utf-8')\r\n        print(\"Description:\", description)\r\n        subgraph_len = model.SubgraphsLength()\r\n        print(\"Subgraph length:\", subgraph_len)\r\n\r\n```\r\nThan I realized that graph nodes could be interpreted as `Tensor` object or `Operator` object.\r\n\r\n`Tensor` object stores quantization params, shape, tensor type (I don't understand the meaning of it yet)\r\n\r\n`Operator` object has `BuiltinOptions` and `CustomOptions` methods that seems to me should give me access to node parameters (such as paddings, dilations and all layer specific info)\r\n\r\nI tried to iterate over them since there are Inputs and Outputs methods. But failed. I don't understand w\r\n\r\nThis is my scratch:\r\n```\r\ndef print_nodes_info(model):\r\n    # what does this 0 mean? should it always be zero?\r\n    subgraph = model.Subgraphs(0)\r\n    operators_len = subgraph.OperatorsLength()\r\n    print('Operators length:', operators_len)\r\n\r\n    from collections import deque\r\n    nodes = deque(subgraph.InputsAsNumpy())\r\n\r\n    STEP_N = 0\r\n    MAX_STEPS = operators_len\r\n    print(\"Nodes info:\")\r\n    while len(nodes) != 0 and STEP_N <= MAX_STEPS:\r\n        print(\"MAX_STEPS={} STEP_N={}\".format(MAX_STEPS, STEP_N))\r\n        print(\"-\" * 60)\r\n\r\n        node_id = nodes.pop()\r\n        print(\"Node id:\", node_id)\r\n\r\n        tensor = subgraph.Tensors(node_id)\r\n        print(\"Node name:\", tensor.Name().decode('utf-8'))\r\n        print(\"Node shape:\", tensor.ShapeAsNumpy())\r\n\r\n        # which type is it? what does it mean?\r\n        type_of_tensor = tensor.Type()\r\n        print(\"Tensor type:\", type_of_tensor)\r\n\r\n        quantization = tensor.Quantization()\r\n        min = quantization.MinAsNumpy()\r\n        max = quantization.MaxAsNumpy()\r\n        scale = quantization.ScaleAsNumpy()\r\n        zero_point = quantization.ZeroPointAsNumpy()\r\n        print(\"Quantization: ({}, {}), s={}, z={}\".format(min, max, scale, zero_point))\r\n\r\n        # I do not understand it again. what is j, that I set to 0 here?\r\n        operator = subgraph.Operators(0)\r\n        for i in operator.OutputsAsNumpy():\r\n            nodes.appendleft(i)\r\n\r\n        STEP_N += 1\r\n\r\n    print(\"-\"*60)\r\n\r\n```\r\n\r\nPlease help me to get access the node attributes.\r\nThank you in advance for your help.\r\n\r\n\r\n", "Hi all\r\nIt's too late but it might be useful for someone who needs it.\r\nHere, you will find the TFLite Parser Python Package: https://github.com/jackwish/tflite\r\n\r\nThanks jackwish for the wonderful package."]}, {"number": 16560, "title": "Add remove_control_dependencies() graph_transform.", "body": "Add a graph_transform that can be used to remove control dependencies from the tensorflow graph. This can allow later passes such as strip_unused_nodes to do a better job.  ", "comments": ["@petewarden Does this look okay? Is the graph_transform tool still in active development?", "The \"Ubuntu CC\" check has been stuck since I opened the PR. Is there any way to retrigger it?", "There is an issue with include files not being available. I'm assuming this has to do with BUILD dependencies? @tadeegan can you take a look?", "Okay. Lets try again.  The includes need \"\" instead of <>"]}, {"number": 16559, "title": " Fixed iOS build script for all architectures, fixes #12904", "body": "Fixes the nightly iOS build, which has been suffering failures.", "comments": []}, {"number": 16558, "title": "Update README.md", "body": "added a friendly badge to get the latest android tensorflow version", "comments": []}, {"number": 16557, "title": "MKL: Fix for mkl_input conversion for MKL DNN. ", "body": "Fix also enables elementwise operations in MKL", "comments": ["@tensorflow-jenkins test this please.", "@gunan The Windows Cmake test failure doesn't seem to be due to this PR. Let  me know if you need any thing else."]}, {"number": 16556, "title": "tf.argmax appears to be functioning incorrectly on occasion", "body": "**EDIT**\r\n\r\n[Link to script and input/label data as pickle files to reproduce the error.](https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH)\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI have written my own code. My code base, data set and batch generating algorithm are quite large, so I am attempting to illustrate this as best as possible. If no mistake of mine can be seen in this post and the examples I have given, then I will provide further code/data. I have asked on StackOverflow, but have got not replies. If  I am missing something simple, then I'm sure it would have been pointed out on StackOverflow by now.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  \r\nManjaro 17.1.3 Kernel 4.14\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\npython pip\r\n\r\n- **TensorFlow version (use command below)**:\r\ntensorflow-gpu 1.5.0\r\n\r\n- **Python version**:\r\n3.6.4\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0\r\ncuDNN 7.0\r\n\r\n- **GPU model and memory**: \r\nNvidia GeForce GTX 1050 8GB\r\n\r\n### Describe the problem\r\ntf.argmax seems to be occasionally producing incorrect results when used on the last axis of a 3-dimensional tensor.\r\n\r\n### Source code / logs\r\nTo debug this, I have printed out the following operations:\r\n\r\n```\r\nprint(self.session.run(\r\n    tf.equal(tf.argmax(self.predictions, axis=-1),\r\n             tf.argmax(self.labelsUnrolled, axis=-1)),\r\n    self.batchDict))\r\nprint(\"\")\r\nprint(self.session.run(self.predictions, self.batchDict))\r\nprint(\"\")\r\nprint(self.session.run(self.labelsUnrolled, self.batchDict))\r\nprint(\"\\n********\\n\")\r\n```\r\n\r\nWhich on two consecutive iterations output the following:\r\n\r\n```\r\n[[ True  True  True]\r\n [ True  True  True]]\r\n\r\n[array([[0.06275553, 0.44493628, 0.42474008, 0.06756803],\r\n        [0.06320112, 0.49631155, 0.4021484 , 0.03833894],\r\n        [0.04378054, 0.59403986, 0.3236889 , 0.03849069]], dtype=float32), \r\narray([[8.1677590e-06, 9.9997127e-01, 2.0200867e-05, 3.6184446e-07],\r\n        [4.3686719e-06, 9.9992716e-01, 6.6905603e-05, 1.5286902e-06],\r\n        [1.3270236e-05, 9.9986196e-01, 1.1622251e-04, 8.5579613e-06]], dtype=float32)]\r\n\r\n[array([[0., 0., 1., 0.],\r\n        [0., 1., 0., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32),\r\n array([[0., 1., 0., 0.],\r\n        [0., 1., 0., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32)]\r\n\r\n********\r\n\r\n[[False False  True]\r\n [ True  True  True]]\r\n\r\n [array([[0.0466171 , 0.53605616, 0.37778312, 0.03954368],\r\n         [0.05007472, 0.4603508 , 0.44400516, 0.0455693 ],\r\n         [0.06134444, 0.38073638, 0.504286  , 0.05363319]], dtype=float32),\r\n array([[9.6363285e-05, 9.9861979e-01, 1.2741127e-03, 9.7381862e-06],\r\n         [1.6185455e-05, 9.9977034e-01, 2.0742408e-04, 6.0521238e-06],\r\n         [2.9893983e-05, 9.9954152e-01, 4.2436572e-04, 4.2021661e-06]], dtype=float32)]\r\n\r\n[array([[0., 0., 1., 0.],\r\n        [0., 0., 1., 0.],\r\n        [0., 0., 1., 0.]], dtype=float32), \r\n array([[0., 1., 0., 0.],\r\n        [0., 1., 0., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32)]\r\n\r\n********\r\n```\r\n\r\nIsn't the very first `True` in the first iteration incorrect? There are many more examples being executed where these are wrong (although it is right the majority of the time). Am I going wrong somewhere with the `tf.argmax` function?\r\n\r\nPrinting out the same operations, but not inside the session gives the following shapes:\r\n\r\n```\r\nTensor(\"Equal_175:0\", shape=(2, ?), dtype=bool)\r\n\r\n[<tf.Tensor 'unstack_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_2:1' shape=(?, 4) dtype=float32>]\r\n\r\n[<tf.Tensor 'unstack_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_1:1' shape=(?, 4) dtype=float32>]\r\n```\r\n\r\nIs it a problem that the number associated with the tensor name \"Equal_XXX:0\" is incrementing each iteration?\r\n\r\nI have also tried changing the axis argument in both argmax functions to `axis=2`, giving the \"Equal\" tensor a shape of (2, 3) again, but there are still similar errors.\r\n\r\nHere is an example:\r\n\r\n```\r\n[[False False  True]\r\n [ True  True False]]\r\n\r\n[array([[0.09075877, 0.41096467, 0.4460272 , 0.05224944],\r\n        [0.04962843, 0.43777955, 0.46654516, 0.04604685],\r\n        [0.07901238, 0.40768984, 0.46641603, 0.04688181]], dtype=float32),\r\n array([[0.04444276, 0.49195835, 0.42141557, 0.04218334],\r\n        [0.02372498, 0.47147286, 0.4679979 , 0.03680426],\r\n        [0.03707527, 0.435518  , 0.48937747, 0.03802926]], dtype=float32)]\r\n\r\n[array([[0., 0., 1., 0.],\r\n        [1., 0., 0., 0.],\r\n        [0., 0., 1., 0.]], dtype=float32),\r\n array([[0., 1., 0., 0.],\r\n        [0., 0., 1., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32)]\r\n```\r\n\r\nI would expect this to be:\r\n\r\n```\r\n[[ True False  True]\r\n [ True False False]]\r\n```\r\n\r\nI thought this may have been a problem with parallel computations on the GPU, but I tried the same execution on just my CPU and got the following, similar miscalculation, too:\r\n\r\n```\r\n[[False  True False]\r\n [False False False]]\r\n\r\n[array([[0.07774187, 0.40993363, 0.47022063, 0.04210386],\r\n        [0.04910654, 0.44086066, 0.46013904, 0.04989377],\r\n        [0.06700655, 0.37128285, 0.51324743, 0.04846317]], dtype=float32), \r\narray([[0.07584244, 0.3863555 , 0.5090046 , 0.02879751],\r\n        [0.06959026, 0.3221606 , 0.5715027 , 0.03674646],\r\n        [0.09042579, 0.32515866, 0.5385905 , 0.04582503]], dtype=float32)]\r\n\r\n[array([[0., 0., 1., 0.],\r\n        [0., 0., 1., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32),\r\narray([[0., 1., 0., 0.],\r\n        [0., 1., 0., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32)]\r\n```\r\n\r\n", "comments": ["Could you try to whittle that down to a simpler test case that's easy to repro? That would help. Thanks!", "@drpngx I have quickly generted a script which is a much simplified version of my project. I have also included two small pickle files to be fed into the `inputs` and `labels` placeholders of the script. Depending on your OS, I think it should load the pickles and run as is, but you may need some tinkering otherwise.\r\n\r\n[Here is a link to a zipped file containing the script, the inputs pickle file and the labels pickle file](https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH).\r\n\r\nWhat I have done is run 100 iterations of training on the same input and labels batch. For each iteration, I compare `tf.argmax` with `np.argmax` and print the results if they are different. Usually, each time I run the script, I get at least one difference in the Numpy and TensorFlow results, as shown below. Each time there is an error, results in the following format are printed:\r\n\r\n`tf.equal(tf.argmax(predictions, axis=-1), tf.argmax(labels, axis=-1))`\r\n\r\n`np.array_equal(np.argmax(predictions, axis=-1), np.argmax(labels, axis=-1))`\r\n\r\n`predictions`\r\n\r\n`labels`\r\n\r\nSample output:\r\n\r\n```\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n\r\n [[False]\r\n [False]] \r\n\r\n [[ True]\r\n [False]] \r\n\r\n [array([[0.34652075, 0.34789905, 0.0830554 , 0.2225248 ]], dtype=float32), array([[0.31778762, 0.35688218, 0.09286535, 0.23246484]], dtype=float32)] \r\n\r\n [array([[0., 1., 0., 0.]], dtype=float32), array([[0., 0., 1., 0.]], dtype=float32)] \r\n\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n\r\n [[ True]\r\n [False]] \r\n\r\n [[ True]\r\n [ True]] \r\n\r\n [array([[0.25822368, 0.38438326, 0.12220651, 0.23518656]], dtype=float32), array([[0.13360085, 0.28102002, 0.40999535, 0.17538387]], dtype=float32)] \r\n\r\n [array([[0., 1., 0., 0.]], dtype=float32), array([[0., 0., 1., 0.]], dtype=float32)] \r\n\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\n64\r\n65\r\n66\r\n67\r\n68\r\n69\r\n70\r\n71\r\n72\r\n73\r\n74\r\n75\r\n76\r\n77\r\n78\r\n79\r\n80\r\n81\r\n82\r\n83\r\n84\r\n85\r\n86\r\n87\r\n88\r\n89\r\n90\r\n91\r\n92\r\n93\r\n94\r\n95\r\n96\r\n97\r\n98\r\n99\r\n100\r\n```", "Just double checking: could you run\r\n`tfCorrect, preds, labs = session.run([correctPredictions, preds, labs], feed_dict)`? That makes it slightly easier to reason about.", "Ok, that seems to have fixed it. Could you explain why that is so? Is there some sort of parallelisation issues with when I run `AdamOptimizer` and then the three other operations in different `session.run` calls? I have always been calling all of my operations in separate `session.run` calls, and never knew that unexpectancies like this could come of it", "Also, is there a way to still call each operation in separate `session.run` calls, and avoid this problem? My entire model is written in an OOP fashion and so I retrieve the results of different operations through individual `session.run` calls in different, individual class methods", "I haven't looked at your code, but there are two possibilities:\r\n* requesting these variables changes global state, for instance, by assigning variables\r\n* similar to the first case, but specifically to random generators\r\n* the computation is non-deterministic, for instance, if you have parallel computation.\r\n\r\nI would need to look at the code again to guess what it is, but in general it's both more efficient and more reliable to use a single run call.\r\n\r\nThere is `partial_run` but it's experimental and I wouldn't recommend using this. If it is really an issue for you, I think this is the kind of scenario that Tensorflow Eager Mode was built for.\r\n\r\nI'll be closing this issue for admin purposes."]}, {"number": 16555, "title": "LSTMBlockFusedCell#call no longer supports list of tensors", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Unbuntu\r\n- **TensorFlow installed from (source or binary)**: soruce\r\n- **TensorFlow version (use command below)**: 'v1.5.0-0-g37aa430', '1.5.0'\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: Build label: 0.9.0\r\n\r\n### Describe the problem\r\n[Documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMBlockFusedCell#call) says, \"inputs: 3-D tensor with shape [time_len, batch_size, input_size] or a list of time_len tensors of shape [batch_size, input_size]\". However, as of 1.5.0, this call only works with a 3-D tensor. \r\n\r\n### Source code / logs\r\nWhen attempting to run with a list of 2-D tensors, I get the following:\r\n```\r\n...\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 629, in __call__\r\n    self._assert_input_compatibility(inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 1112, in _assert_input_compatibility\r\n    ' input tensors. Inputs received: ' + str(inputs))\r\nValueError: Layer rnn/lstm_cell expects 1 inputs, but it received 34 input tensors. Inputs received: ... \r\n```\r\n", "comments": ["/CC @ebrevdo can you take a look?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Yeah; sorry about that.  We need to change the documentation now.  Just feed in `tf.stack(inputs)` instead of `inputs`; and things should continue to work as before.", "Would you like to send a PR updating the documentation?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I submitted a PR which updates the documentation. It was not clear how to generate the documentation in order to verify the changes, but I feel confident that I found the correct lines to remove. ", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Seems like this is fixed."]}, {"number": 16554, "title": "Correct argument doc for BasicLSTMCell.call", "body": "", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please fix errors.", "@ebrevdo All checks have passed now. Thanks.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16553, "title": "Any way to pass a feedable placeholder for num_spectrogram_bins  in linear_to_mel_weight_matrix?", "body": "I need to run `auido->spectrogram->mel` graph for three different frame sizes. Ideally, I would have `frame_size` input tensor of the `stft` function be a placeholder and feed it in a loop. However, when computing the mel scale, tf does a value check for the number of bins, which according to my understanding, means that the number of bins needs to be specified during the graph construction.\r\nAny suggestions?\r\n\r\nBelow is the link to the problematic code\r\nhttps://github.com/tensorflow/tensorflow/blob/f7cbb757cf51143a1c7d9db5a812ac165941adf4/tensorflow/contrib/signal/python/ops/mel_ops.py#L72\r\n\r\n**Edit**: I am referencing the official tutorial.\r\nhttps://www.tensorflow.org/api_guides/python/contrib.signal#Computing_log_mel_spectrograms\r\nThe very first line essentially begins the problem chain. \r\n\r\n**Edit**: I just commented out the line below that initiates the value checks, and everything ran smoothly. I was able to feed in `frame_length`. \r\nhttps://github.com/tensorflow/tensorflow/blob/f7cbb757cf51143a1c7d9db5a812ac165941adf4/tensorflow/contrib/signal/python/ops/mel_ops.py#L151\r\n", "comments": ["/CC @rryan @dpwe, can you take a look?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@siryog90 @reedwm I take a look at the implementation of `linear_to_mel_weight_matrix`. It looks that `num_spectrogram_bins` is only passed to `math_ops.linspace`, which already performs validations in both shape function and kernel. For that I think it makes sense to remove the validation in python, so that non python int values could be used.\r\n\r\nCreate a PR #17404 for the fix. Please take a look."]}, {"number": 16552, "title": "Sampled softmax loss stops gradients on sampled classes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.3\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nCUDA 8 / cuDNN 6\r\n- **GPU model and memory**:\r\n4 x TITAN X (Pascal)\r\n\r\n### Describe the problem\r\n\r\nThe backbone of TensorFlow's sampled loss functions `nce_loss` and `sampled_softmax_loss` is a helper function called `_compute_sampled_logits`https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L961-L1139.\r\n\r\n`_compute_sampled_logits` takes as input:\r\n\r\n- weights and biases of the final layer,\r\n- the output labels\r\n- the inputs to the final layer inputs\r\n- the sampled values of the output layer\r\n- a few other things\r\n\r\nand returns the logits and labels of only the requested sampled labels.\r\n\r\nOne of the first ops executed is https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L1046-L1047 \r\n\r\nThis line seems like it is stopping gradients flowing back through the sampled values if i'm reading it correctly.\r\n\r\nShouldn't the gradients be stopped from flowing back through the _non-sampled values_ as opposed to the _sampled values_? Why are gradients being stopped at the sampled values? \r\n", "comments": ["@gouwsmeister could you take a look?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "`sampled` is only a list of integer ids of the sampled positive classes. The actual weights are selected/indexed further down. Applying stop_gradient on the ids does not prevent the actual weights to still be updated. ", "Closing since this seems to be correct behavior."]}, {"number": 16551, "title": "Branch 183683856", "body": "", "comments": []}, {"number": 16550, "title": "Add options to enable new features for cloud-tpu-profiler.", "body": "Add options for the user to manually include dataset ops in trace collection, and to automatically recapture the traces when no trace event is collected.\r\nAlso change tf.flags to absl.flags since the former is going to be deprecated. ", "comments": ["The bulk of the implementation has been submitted internally. "]}, {"number": 16549, "title": "Fix typos 'followings' 'optionanl'", "body": "", "comments": []}, {"number": 16548, "title": "AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.2\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 3.6.3\r\n- **GPU model and memory**: no GPU\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nclass my_RNNCell(tf.nn.rnn_cell.RNNCell):\r\n    def __init__(self):\r\n        super(my_RNNCell, self).__init__()\r\n        self._output_size = 2\r\n        self._state_size = 2\r\n\r\n    def __call__(self, tensor_in, state):\r\n\r\n        output = tf.layers.conv_2d(tensor_in, 1, [1, 10])\r\n\r\n        return output, output\r\n    \r\n    @property\r\n    def output_size(self):\r\n        return self._output_size\r\n    @property\r\n    def state_size(self):\r\n        return self._state_size\r\n\r\n\r\ntf.nn.dynamic_rnn(my_RNNCell(), inputs=tf.placeholder(shape=[None,2,100,3], dtype=tf.float32), initial_state= tf.placeholder(shape=[None,2], dtype=tf.float32))\r\n\r\n>>>\r\nAttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d'\r\n```\r\n", "comments": ["Can you try `tf.layers.conv2d`?", "No problems there:\r\n```python\r\n>>>tf.layers.conv2d(tf.placeholder(shape=[None,2,100,3], dtype=tf.float32), 1, [1, 10])\r\n<tf.Tensor 'conv2d/BiasAdd:0' shape=(?, 2, 91, 1) dtype=float32>\r\n```", "Great to see it's working for you!", "I'm sorry @drpngx , perhaps I wasn't clear: **my issue is still unsolved** (I can't initialize a `dynamic_rnn` using an `RNNCell` with a convolutional layer).\r\n\r\nI thought you had asked me to verify that `tf.layers.con2d` could create an op. Could you please reopen the issue? Can you reproduce it with the code that I posted?", "What is the error?", "The one on the title of this issue: ```AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d'```", "So you created that cell. Can you change it to `conv2d` instead of `conv_2d`?", "omg, I can't believe it was a typo... Ok, problem solved! \ud83d\ude05", "\r\n        top = tf.layers.conv2d(inputs,\r\n                               filters=filters,\r\n                               kernel_size=3,\r\n                               padding=padding,\r\n                               activation=activation,\r\n                               kernel_initializer=kernel_initializer,\r\n                               bias_initializer=bias_initializer,\r\n                               name=name)\r\n   top = tf.layers.conv2d(inputs,\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'layers'"]}, {"number": 16547, "title": "Spelling", "body": "", "comments": []}, {"number": 16546, "title": "Fix raw summary metrics for Estimators in python 3", "body": "It looked for `six.string_types` in the metrics, but should be `six.binary_type` since `tf.string` returns a `bytes()` object.\r\n\r\nAlso changes the test which missed this.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "py3 pip test failure is a known flake."]}, {"number": 16545, "title": "Graph transform : fold_constant issue since tf v 1.5", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux centos 7\r\n- **TensorFlow installed from (source or binary)**: with pip\r\n- **TensorFlow version (use command below)**: 1.5.0 (vs 1.4.1)\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: No\r\n- **GCC/Compiler version (if compiling from source)**: No\r\n- **CUDA/cuDNN version**: No\r\n- **GPU model and memory**: No\r\n- **Exact command to reproduce**:\r\n\r\nClone the tensorflow/models repo.\r\nthen \r\n```\r\ncd models/research/slim/\r\npython export_inference_graph.py \\\r\n  --model_name=inception_v1 \\\r\n  --image_size=224 \\\r\n  --output_file=/tmp/inception_v1.pb\r\n```\r\nThen freeze the graph : \r\n```\r\ncd tensorflow/tensorflow/python/tools/\r\npython freeze_graph.py \\\r\n--input_graph /tmp/inception_v1.pb \\\r\n--input_checkpoint /tmp/inception_v1.ckpt \\\r\n--output_graph /tmp/inception_v1_frozen.pb \\\r\n--input_binary True \\\r\n--output_node_names \"InceptionV1/Logits/Predictions/Reshape_1\"\r\n```\r\n\r\nThen in python do : \r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.core.framework import graph_pb2\r\nfrom tensorflow.python.platform import gfile\r\nimport tensorflow.tools.graph_transforms as graph_transforms\r\ngraph = graph_pb2.GraphDef()\r\nwith open(\"/tmp/inception_v1_frozen.pb\", 'rb') as f:\r\n    s = f.read()\r\n    graph.ParseFromString(s)\r\ngraph = graph_transforms.TransformGraph(graph,\r\n            [\"input\"], # inputs nodes\r\n            [\"InceptionV1/Logits/Predictions/Reshape_1\"], # outputs nodes\r\n            ['fold_constants()'])\r\nwith gfile.FastGFile(\"/tmp/inception_v1_frozen\"+\"_optimized.pbtxt\", \"w\") as f:\r\n    f.write(str(graph))\r\n```\r\n\r\n### Describe the problem\r\nI am using the graph_transform to fold constants in by graph saved as .pb.\r\nWhen I use the fold_constants() transformation, some inputs of some nodes are renamed but not the corresponding nodes in the whole graph. So the graph is no longer valid... \r\n\r\nI have an \"input\" placeholder in the graph.\r\nAnd the node connected to this placeholder as an input name \"input:0\" instead of \"input\".\r\n\r\nWith the version 1.4.1 of tensorflow, I didn't have this issue.\r\n\r\nTo reproduce, follow the instructions below, and take a look to the /tmp/inception_v1_frozen_optimized.pbtxt graph. And search le node named \"input\" it is the input placeholder. Then search node which has an input named \"input:0\". This name (\"input:0\") is node a node of the graph.", "comments": ["/CC @petewarden, can you take a look?", "Just ran into this today too.", "@mrfortynine Please post information on how we can reproduce the problem that you see.", "All information about how to reproduce the bug are given in the fist post ... Does someone know how to solve this issue ?", "The problem is still not solved...\r\n\r\nGiven this example : \r\n```\r\nname: \"input\"\r\nop: \"Placeholder\"\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nname: \"Pad\"\r\nop: \"Pad\"\r\ninput: \"input\"\r\ninput: \"Pad/paddings\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\n```\r\n\r\nThe fold_constant() transformation change the name of the input tensor from \"input\" to \"input:0\".\r\nBut the node named input is still named \"input\" and not \"input:0\". \r\nlike this : \r\n```\r\nname: \"input\"\r\nop: \"Placeholder\"\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nname: \"Pad\"\r\nop: \"Pad\"\r\ninput: \"input:0\"\r\ninput: \"Pad/paddings\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\n```\r\n\r\nSo, after this transformation of the graph, the connection between the input node \"input\" and the next node is broken.\r\n\r\n(The input placeholder is not connected any more to the Pad node...)", "@tensorflower-gardener Do you think this issue is related to your commit :   \r\nhttps://github.com/tensorflow/tensorflow/commit/38bcb3c02fbc5185d6c1fb7e8327a070284b66e4", "Closing this issue due to staleness. Please check with the latest version of TensorFlow. Feel free to reopen if issue still persists. Thanks!", "@ymodak this issue occurs in TF 1.14\r\nat least when using this set of transforms:\r\n```\r\nDEFAULT_TRANSFORMS = [\r\n    'remove_nodes(op=Identity, op=CheckNumerics)',\r\n    'merge_duplicate_nodes',\r\n    'strip_unused_nodes',\r\n    'fold_constants',\r\n    'flatten_atrous_conv',\r\n    'fold_batch_norms',\r\n    'fold_old_batch_norms',\r\n    'remove_control_dependencies',\r\n    'sort_by_execution_order'\r\n]\r\n```\r\nafter removing fold_constants from this list rename doesn't occur.\r\n\r\nI will solve it by manual rename of nodes in generated graph but it doesn't solve the problem for the other users."]}, {"number": 16544, "title": "deconv_output_length(input_length, filter_size, padding, stride)", "body": "deconv_output_length(input_length, filter_size, padding, stride) is defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/utils.py#L159)\r\n\r\nWhen padding is valid, input_length += max(filter_size - stride, 0), why to use `max` function?\r\nSee more details on #2118\r\n\r\nFor conv2d:\r\n```\r\noutput = (input - filter + stride) // stride  # VALID\r\noutput = (input + stride - 1) // stride  # SAME\r\n```\r\n\r\nFor conv2d_transpose:\r\n```\r\noutput = input * stride + filter - stride  # VALID\r\noutput = input * stride - stride + 1  # SAME \r\n```\r\n\r\nEven when filter_size is less than stride, I think output is also `input * stride + filter - stride` rather `input * stride`, so why to use `max`?\r\n\r\n \r\n", "comments": ["Could you comment on the original thread? If you're will to send a well-formed PR, maybe that's also another way. thanks!"]}, {"number": 16543, "title": "Adding tf.image.non_max_suppression_with_overlaps", "body": "This commit introduces the op `tf.image.non_max_suppression_overlaps`.\r\nIt allows to perform non-max-suppression with an overlap criterion different to IOU by providing an n-by-n matrix with precomputed overlap values for each box pair.\r\n\r\nIn this way it is possible to use a custom overlap criterion for NMS (eg. intersection-over-area) without implementing a specialized op.\r\n\r\nThere are currently no unit tests. If you are willing to integrate this pull request, I will copy and rewrite the existing unit tests for `NonMaxSuppressionV2`.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "signed cla", "CLAs look good, thanks!\n\n<!-- ok -->", "The latest commit changes the name and adds the missing unit tests.\r\nIs there a convenient way to check performance implications? ", "Thanks.  \r\n\r\nI'm not aware of any existing benchmark.\r\n\r\nIf you run through with a reasonable number of boxes (50000-100000?) before and after that would be sufficient.  I just want to double check it's not a major performance change for existing code.", "I use the following script to measure the average runtime with > 1000000 boxes:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport timeit\r\n\r\ndef create_boxgrid():\r\n    coord_linspace = np.linspace(start=0.0, stop=1.0, num=600)\r\n    x_grid, y_grid = np.meshgrid(coord_linspace, coord_linspace)\r\n    box_centers = np.stack((x_grid, y_grid), axis=2).reshape((-1,2))\r\n    \r\n    def create_boxes(width, height):\r\n        xmin = box_centers[:, 0] - width/2\r\n        xmax = box_centers[:, 0] + width/2\r\n        ymin = box_centers[:, 1] - height/2\r\n        ymax = box_centers[:, 1] + height/2\r\n        return np.stack((ymin, xmin, ymax, xmax), axis=1)\r\n    \r\n    return np.concatenate((create_boxes(0.2, 0.2), create_boxes(0.5, 0.3), create_boxes(0.3, 0.5)))\r\n\r\nif __name__ == '__main__':\r\n    boxes = create_boxgrid()\r\n    num_boxes = len(boxes)\r\n    scores = np.ones(shape=(num_boxes,), dtype=np.float32)\r\n    print('Number of boxes: {:d}'.format(num_boxes))\r\n\r\n    keep_idx = tf.image.non_max_suppression(boxes, scores, num_boxes)\r\n    with tf.Session() as sess:\r\n        def run():\r\n            return sess.run(keep_idx)\r\n        \r\n        # warmup\r\n        print('Warming up...')\r\n        for _ in range(1000):\r\n            np_keep_idx = run()\r\n        \r\n        num_runs = 100000\r\n        print('Measuring time for {:d} runs...'.format(num_runs))\r\n        elapsed_time = timeit.timeit(run, number=num_runs)\r\n        avg_time = elapsed_time / num_runs\r\n        print('Elapsed time: {:f} seconds'.format(elapsed_time))\r\n        print('Average time: {:f} seconds'.format(avg_time))\r\n        print('Keep {:d} boxes'.format(len(np_keep_idx)))\r\n```\r\n\r\nVersion in master:\r\n```\r\nNumber of boxes: 1080000\r\nWarming up...\r\nMeasuring time for 100000 runs...\r\nElapsed time: 5.595638 seconds\r\nAverage time: 0.000056 seconds\r\nKeep 410 boxes\r\n```\r\n\r\nNew version:\r\n```\r\nNumber of boxes: 1080000\r\nWarming up...\r\nMeasuring time for 100000 runs...\r\nElapsed time: 5.595950 seconds\r\nAverage time: 0.000056 seconds\r\nKeep 410 boxes\r\n```\r\n\r\nLooks like the performance is pretty much the same!", "(From API review: We'd be interested in @jch1 s opinion as well)", "Thanks for running the tests!  Looks good to me pending feedback from @jch1 .", "Nagging Assignees @rjpower, @jch1: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the delay.  Everything looks good from here.", "Sorry, haven't been available last week...\r\nIs there anything I should do from here on in order to get this merged?", "Nagging Assignee @rjpower: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Apologies, I didn't realize this was still assigned to me.\r\n\r\nI think you'll need to resolve the conflict and add a api pbtxt to get the tests passing.  I've assigned @gunan since he'll probably know more about this than I do.", "I added the missing api definitions and resolved the conflict. The code would be ready for another test run!", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@chrert could you pull rebase and push again?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I rebased and pushed again.\r\nTests are running locally.", "Fixed the pylint errors.", "Here is the API failure. Note that you have to run the following on a python 2 environment.\r\nOnce the API update is merged, I will direct this to tensorflow API reviewers to get an API change approval.\r\n\r\n```\r\nERROR:tensorflow:TensorFlow API backwards compatibility test\r\nThis test ensures all changes to the public API of TensorFlow are intended.\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test as follows to update\r\ntest goldens and package them with your change.\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\nYou will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\nERROR:tensorflow:1 differences found between API and golden.\r\nIssue 1\t: 'path: \"tensorflow.image\"\\ntf_module {\\n  member {\\n    name: \"ResizeMethod\"\\n   [truncated]... != 'path: \"tensorflow.image\"\\ntf_module {\\n  member {\\n    name: \"ResizeMethod\"\\n   [truncated]...\r\nDiff is 11284 characters long. Set self.maxDiff to None to see it.\r\n```\r\n", "Thanks, I've added the changes to the API golden files (I hope that's how it is supposed to be done).", "The change looks correct to me.\r\nI will rerun tests, and added this to the API review queue.", "Approval for API review.", "Thank you very much for the API review.\r\n@chrert May I ask one last conflict resolution from you, then I will merge this PR.", "Thanks for the approval!\r\nI resolved the conflict."]}, {"number": 16542, "title": "Cannot merge devices with incompatible jobs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04(kernel 4.10)\r\n- **TensorFlow installed from (source or binary)**: source code\r\n- **TensorFlow version (use command below)**: r1.4.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: No CUDA\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: var_rep = tf.Variable(var_concat, name=var_name, collections=[tf.GraphKeys.LOCAL_VARIABLES], trainable=False)\r\n\r\n### Describe the problem\r\nI am running distributed tensorflow with 2 parameter servers and 2 workers. I created some partioned variables by using \"tf.create_partitioned_variables()\". So the different parts of a variable will be placed on different ps. Then I want to concatenate them by using \"tf.concat()\" and assign the result of tf.concat() to a untrainable variable which lies in worker. But the error occurs as follows:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"distributed_vgg19.py\", line 230, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"distributed_vgg19.py\", line 188, in main\r\n    sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 726, in prepare_or_wait_for_session\r\n    max_wait_secs=max_wait_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 400, in wait_for_session\r\n    sess)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 483, in _try_run_local_init_op\r\n    sess.run(self._local_init_op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot colocate nodes 'fc8/fc8_biases_1/fc8/fc8_biases/part_1/read_fc8/fc8_biases_1_0' and 'fc8/fc8_biases/part_1: Cannot merge devices with incompatible jobs: '/job:ps/task:0' and '/job:worker/task:1'\r\n         [[Node: fc8/fc8_biases_1/fc8/fc8_biases/part_1/read_fc8/fc8_biases_1_0 = Identity[T=DT_FLOAT, _class=[\"loc:@fc8/fc8_biases/part_1\"], _device=\"/job:worker/task:1\"](fc8/fc8_biases_1/cond_1/Merge)]]\r\n\r\nCaused by op u'fc8/fc8_biases_1/fc8/fc8_biases/part_1/read_fc8/fc8_biases_1_0', defined at:\r\n  File \"distributed_vgg19.py\", line 230, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"distributed_vgg19.py\", line 136, in main\r\n    vgg.build(x, train_mode)\r\n  File \"/home/shuai/distributed_vgg19_part/vgg19.py\", line 96, in build\r\n    self.fc8 = self.fc_layer(self.relu7, 4096, 1000, \"fc8\")\r\n  File \"/home/shuai/distributed_vgg19_part/vgg19.py\", line 120, in fc_layer\r\n    weights, biases = self.get_fc_var(in_size, out_size, name)\r\n  File \"/home/shuai/distributed_vgg19_part/vgg19.py\", line 143, in get_fc_var\r\n    biases = self.get_var(initial_value, name, 1, name + \"_biases\")\r\n  File \"/home/shuai/distributed_vgg19_part/vgg19.py\", line 171, in get_var\r\n    var_rep = tf.Variable(var_concat, name=var_name, collections=[tf.GraphKeys.LOCAL_VARIABLES], trainable=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 229, in __init__\r\n    constraint=constraint)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 378, in _init_from_args\r\n    self._build_initializer_expr(self._initial_value),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 811, in _build_initializer_expr\r\n    new_op = self._build_initializer_expr(initial_value.op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 830, in _build_initializer_expr\r\n    new_tensor = self._build_initializer_expr(tensor)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 811, in _build_initializer_expr\r\n    new_op = self._build_initializer_expr(initial_value.op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 847, in _build_initializer_expr\r\n    attrs=initial_value.node_def.attr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3042, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1521, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot colocate nodes 'fc8/fc8_biases_1/fc8/fc8_biases/part_1/read_fc8/fc8_biases_1_0' and 'fc8/fc8_biases/part_1: Cannot merge devices with incompatible jobs: '/job:ps/task:0' and '/job:worker/task:1'\r\n         [[Node: fc8/fc8_biases_1/fc8/fc8_biases/part_1/read_fc8/fc8_biases_1_0 = Identity[T=DT_FLOAT, _class=[\"loc:@fc8/fc8_biases/part_1\"], _device=\"/job:worker/task:1\"](fc8/fc8_biases_1/cond_1/Merge)]]\r\n```\r\n\r\nthere are some relevant code:\r\n```\r\n            slice_list = []\r\n            for dim_index in range(value.get_shape().ndims):\r\n                if dim_index == 0:\r\n                    slice_list.append(self.num_of_ps)\r\n                else:\r\n                    slice_list.append(1)\r\n            var_list = tf.create_partitioned_variables(shape=value.get_shape(), \r\n                                                       slicing=slice_list, \r\n                                                       initializer=value,\r\n                                                       name=var_name)\r\n            var_concat = var_list[0]\r\n            for ps_index in range(self.num_of_ps - 1):\r\n                var_concat = tf.concat([var_concat, var_list[ps_index + 1]], 0)\r\n            with tf.device('/job:worker/task:%d' % self.task_index):\r\n                var = tf.Variable(var_concat, name=var_name, collections=[tf.GraphKeys.LOCAL_VARIABLES], trainable=False)\r\n```\r\n\r\nAnybody knows why this error happened? Is it caused by an bug? If not, what i can do to assign the value of a variable lying in ps to another variable lying in worker?\r\nThanks a lot!", "comments": ["BTW, the result of concatenating, var_concat, is a tensor lying in worker, so I think this assign op has nothing to do with  '/job:ps/task:0', is right?", "@alextp does that ring a bell?", "I'm confused, why are you creating a variable to store the result of the concatenation of other variables? Why not just use partitioned_variable.as_tensor() which returns a tensor with the concatenated value of all the partitions?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @alextp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @alextp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been solved. So it can be closed. Thank you!\n\n\u57282018-04-16 20:37:08\uff0cShuai Wang13211134@bjtu.edu.cn\u5199\u9053\uff1a\n\nIt has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.", "@wangshuaizs  Could you share your solution?", "@fengrussell \r\nI followed instructions here to add allow_soft_placement=True. Not sure what is the side effect.\r\nhttps://github.com/tensorflow/tensorflow/issues/2285", "Nagging Assignee @alextp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @alextp: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16541, "title": "Looking for a way to compile TF without SSE.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Slackware 14.2 (32bit)\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n\r\n- **Python version**: \r\n3.6.4\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.5.4\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.3.0\r\n\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n\r\n- **GPU model and memory**:\r\nN/A\r\n\r\n- **Exact command to reproduce**:\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n#### Problem\r\n Pardon me first to bother you. This is neither about a bug, nor a feature request. This is about the 32-bit architecture, and I know that TF do not officially support 32-bit machines.\r\n\r\nI also use decent NVIDIA workstations at my lab. Thank you for your work. However, sometimes, I open my old laptop to write some code, and it is a 32-bit machine.\r\n\r\nSo far, (up to TF 1.4.1), I could somehow manage to compile the source code to get the things right. But, in TF 1.5, I receive next error messages here and there:\r\n\r\n\"The TensorFlow library was compiled to use \" << SSE\r\n\" instructions, but these aren't available on your machine.\"\r\n\r\n\r\nI think that if `__SSE__` flag is disabled, then TF 1.5 can be available to the 32-bit architecuture, so I append -march=i686 to the configure script and bazel argument, but it's no worth. I'm trying to figure out what turns on SSE flag, but still I got no effort.\r\n\r\nCurrently I add to the bazel command this:\r\n\r\nbazel build --config=opt --copt=-march=\"i686\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["/CC @tfboyd any comments?", "@daisylab   I would have tried what you did with i686 or pentium2.  Adding Amit because if SSE is hard coded on that is beyond what I can figure out fast.  ", "@av8ramit   User used what I would have tried.  Maybe SSE is hardcoded or the check is wrong.  Random guesses.", "@gunan might have a better answer than me.", "By the way, I think the check routine might not work on 32-bit architecture. It uses rbx and rdi registers, which I believe 64-bit registers, to investigate cpu features;\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4ab2d8531c461169cd6a33bc0fef1129b419e9df/tensorflow/core/platform/cpu_info.cc#L40\r\n\r\nso checking cpu feature itself breaks the compilation process on 32-bit machine.\r\n\r\nHowever, it does not matter because if `__SSE__` is correctly disabled, then it is not called;\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4ab2d8531c461169cd6a33bc0fef1129b419e9df/tensorflow/core/platform/cpu_feature_guard.cc#L61\r\n\r\n", "All I can think of is playing with these flags:\r\nhttps://gcc.gnu.org/onlinedocs/gcc/x86-Options.html\r\n\r\nI wont be able to look into this, as it is pretty difficult to get old CPUs for me these days, but I am happy to review contributions, if the code needs any modifications.\r\nBazel team also may have some ideas @lberki to check, or redirect within bazel team.", "So I tried x86 options of gcc as @gunan suggested. Thank you very much.\r\n\r\nBut I still get trouble to compile TF on 32-bit machine. Sorry about that. Maybe it's time for me to let the old laptop go. However, I tracked down the matter just a little bit more to make things clear.\r\n\r\nBelow is the error message emitted from bazel:\r\n\r\n```\r\nERROR: /home/sungjin/.usr/src/tensorflow/tensorflow/contrib/factorization/BUILD:\r\n115:1: Executing genrule //tensorflow/contrib/factorization:gen_clustering_ops_p\r\nygenrule failed (Aborted): bash failed: error executing command /bin/bash -c ...\r\n (remaining 1 argument(s) skipped).\r\n2018-02-02 01:05:41.993519: F tensorflow/core/platform/cpu_feature_guard.cc:36] \r\nThe TensorFlow library was compiled to use SSE instructions, but these aren't av\r\nailable on your machine.\r\n/bin/bash: line 1: 29481 Aborted                 bazel-out/host/bin/tensorflow/c\r\nontrib/factorization/gen_gen_clustering_ops_py_wrappers_cc , '' 0 0 > bazel-out/\r\nlocal-py3-opt/genfiles/tensorflow/contrib/factorization/python/ops/gen_clusterin\r\ng_ops.py\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nand the command I gave is:\r\n\r\n```\r\nbazel build --config=opt --linkopt='-lrt' --copt=-march=i686 --cxxopt=-march=i68\r\n6 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nAs far as I know, `-march=i686` do not turn on `__SSE__` flag, and I'm curious about what turns on  SSE, said as above.\r\n\r\nThank you for your kind help.\r\n", "@daisylab   I think your process is correct.  i686 by the document would not have SSE,  just like if you used 'pentium2'.  Nothing extra to add, just wanted to say I think your command-line is correct and there is likely some other reason SSE is added.  I do not know enough in this area to debug, just adding moral support.  :-)", "I think I found a workaround. \r\n\r\nAlthough I could not find the exact location where SSE is added, I guessed that if the configure script sets machine architecture correctly, then there must be a point during the build process where SSE check is irrelevantly added, so I did a wild experiment, that is:\r\n\r\n1. changed the log level from `FATAL` to `WARNING` in the `cpu_feature_guard.cc`, to bypass CPU feature check routine.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bdea071e68fa3f5dfcffdd26523d5825f21620fe/tensorflow/core/platform/cpu_feature_guard.cc#L36\r\n\r\n2. ran `./configure` script as usual, and set `-march=i686` near the end of the script (not using explicit `--copt=-march=i686` in the bazel command line).\r\n\r\nThis time I could get the compiled wheel package. I installed the package, and ran a sample code (https://www.tensorflow.org/get_started/premade_estimators), which worked flawlessly. Also now I get auto-completion in my emacs session with elpy to review TF codes (which was the first reason to use this ancient 32-bit laptop, thinkpad x60).\r\n\r\nI thought if somethings are broken in the compilation process, so wrong binary code is generated, then calling TF would crash hopelessly, but so far I get no errors.\r\n\r\nDefinitely I know that this is not a good solution. Changing log level can be dangerous, just saying that this workaround helped my situation.\r\n\r\nThank you for your concern.\r\n\r\nSungjin.\r\n", "@daisylab   I like your trick.  This is likely still a bug until we are sure what is happening.  I am the wrong owner as this is not an area I understand very well.  Sadly, it may be a think that is a low priority due to the older hardware.  I am glad you found a workaround even though it required a code hack.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 16540, "title": "//tensorflow/python:nn_test fails with AssertionError", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 s390x\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: bazel test -c opt //tensorflow/python:nn_test \r\n\r\n### Describe the problem\r\nWhile running `nn_test`, 2 sub tests (`testOutput2DInput01`,`testOutput4DInput123`), fail on s390x with : \r\n```\r\nAssertionError:\r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nNone\r\n(mismatch 100.0%)\r\n x: array([[ 0.00111]], dtype=float32)\r\n y: array([[ 0.000837]])\r\n```\r\n\r\nThe test passes if the [tolerance](https://github.com/tensorflow/tensorflow/blob/v1.4.1/tensorflow/python/ops/nn_test.py#L865) is slightly increased:\r\n```\r\n-  def doOutputTest(self, input_shape, moments_axes, tol=1e-4,\r\n+  def doOutputTest(self, input_shape, moments_axes, tol=4e-4,\r\n```\r\n\r\nIs it ok to create a PR with this change? Could you please share your thoughts on this.\r\n\r\n### Source code / logs\r\n======================================================================\r\n```\r\nFAIL: testOutput2DInput01 (__main__.MomentsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py\", line 911, in testOutput2DInput01\r\n    self.doOutputTest((10, 300), (0, 1))\r\n  File \"/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py\", line 896, in doOutputTest\r\n    self.assertAllClose(variance, expected_var, rtol=tol, atol=tol)\r\n  File \"/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 972, in assertAllClose\r\n    self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol)\r\n  File \"/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 942, in _assertArrayLikeAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol, err_msg=msg)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 1395, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 778, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nNone\r\n(mismatch 100.0%)\r\n x: array([[ 0.00111]], dtype=float32)\r\n y: array([[ 0.000837]])\r\n\r\n```\r\n", "comments": ["@ebrevdo for context I think that s390x is the big endian system.\r\n\r\nThat seems like a large change, are you sure that this is working correctly?", "The problem is either with `reduce_mean` or `squared_difference`.  Could you identify within which op you get the precision loss?  My money's on reduce_mean but who knows if there's some issue with vectorisation primitives in Eigen's scalar_square or scalar_difference implementations.", "@drpngx , @ebrevdo , Thank you for your feedback.\r\nAfter doing some more analysis, found that the test fails, only for `mu` = `1e3` [here](https://github.com/tensorflow/tensorflow/blob/v1.4.1/tensorflow/python/ops/nn_test.py#L867) and passes for other cases.\r\nAfter printing actual and expected mean and variance values, found below for failing case:\r\n```\r\nmean :  [[ 1000.03216553]]\r\nexpected_mean:  [[ 1000.04957838]]\r\nvariance :  [[ 0.00113162]]\r\nexpected_var :  [[ 0.0008284]]\r\n```\r\nSo seems like some issue with variance calculation.\r\nFurther printed [squared_difference](https://github.com/tensorflow/tensorflow/blob/v1.4.1/tensorflow/python/ops/nn_impl.py#L621) as `print(\" squared diff: \", math_ops.squared_difference(y, array_ops.stop_gradient(mean)).eval())` and found following:\r\n\r\n```\r\ns390x:\r\n squared diff:  [[  9.41790640e-05   6.66562468e-04   1.11056864e-03 ...,   2.74242461e-03\r\n    2.63482705e-03   1.27498060e-04]\r\n [  2.23173201e-03   7.84847885e-04   4.25592065e-04 ...,   9.77665186e-05\r\n    1.73807144e-04   2.11015344e-04]\r\n [  2.20872462e-05   2.21446529e-03   6.47787005e-04 ...,   1.97067857e-04\r\n    7.57727772e-04   1.22044235e-04]\r\n ...,\r\n [  6.39297068e-05   3.52684036e-03   1.90446153e-03 ...,   1.26184523e-03\r\n    1.90272927e-04   2.27228180e-03]\r\n [  8.02040100e-04   5.96046448e-08   4.36128676e-03 ...,   4.40835953e-04\r\n    4.43402678e-04   2.33084336e-03]\r\n [  6.29279763e-04   7.04918057e-04   9.35051590e-04 ...,   4.23325598e-03\r\n    1.49011612e-06   6.29574060e-05]]\r\nmean :  [[ 1000.03216553]]\r\nexpected_mean:  [[ 1000.04957838]]\r\nvariance :  [[ 0.00113162]]\r\nexpected_var :  [[ 0.0008284]]\r\n\r\nwhile on x86:\r\n squared diff:  [[  4.50760126e-05   8.83489847e-05   2.85837799e-04 ...,   1.29238144e-03\r\n    1.21885538e-03   2.62856483e-05]\r\n [  9.50042158e-04   1.97434425e-03   1.37257949e-03 ...,   4.26508486e-05\r\n    1.04643404e-05   9.57582146e-04]\r\n [  4.45976853e-04   9.38788056e-04   1.75310671e-03 ...,   9.27601010e-04\r\n    1.93119049e-03   7.54371285e-04]\r\n ...,\r\n [  5.96046448e-04   1.84631348e-03   7.41019845e-04 ...,   3.64962965e-04\r\n    9.12789255e-04   9.76562500e-04]\r\n [  2.00155750e-03   2.77642161e-04   2.46230140e-03 ...,   2.09547579e-05\r\n    1.40441954e-03   1.01508200e-03]\r\n [  1.72257423e-03   1.02654099e-04   2.00510025e-04 ...,   2.36633793e-03\r\n    3.11139971e-04   7.19763339e-05]]\r\nmean :  [[ 1000.04858398]]\r\nexpected_mean:  [[ 1000.04957838]]\r\nvariance :  [[ 0.00082939]]\r\nexpected_var :  [[ 0.0008284]]\r\n\r\n```\r\n\r\nDo you think this variation can cause precision loss?", "@drpngx , @ebrevdo , Found the cause of failure of this test.\r\nLooks like the precision is lost [here](https://github.com/tensorflow/tensorflow/blob/v1.4.1/tensorflow/python/ops/nn_test.py#L878).\r\n\r\nWhen I changed the dtype from float32 to float64 as below, the test passed.\r\n```\r\n               inputs = constant_op.constant(\r\n-                  input_values, shape=input_shape, dtype=dtypes.float32)\r\n+                  input_values, shape=input_shape, dtype=dtypes.float64)\r\n```\r\n\r\nI also tried the same on x86, and it's still passing.\r\n\r\nIs it OK to create a PR with this change? ", "@drpngx , @ebrevdo, Could you please have a look?", "Printing few data types as below:\r\n```\r\n(Pdb) p input_values.dtype\r\ndtype('float64')\r\n```\r\nwhereas,\r\n```\r\n(Pdb) p inputs.dtype\r\ntf.float32\r\n```\r\nThis behavior is similar to earlier issues observed on s390x.\r\n@mrry, Could you please provide your feedback?", "Can the problem be solved by setting an explicit `dtype` for `np.random.rand()` on this line?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8ee732dae6202eac9c9ef1f14a94ba96fde677ce/tensorflow/python/ops/nn_test.py#L951\r\n\r\nI suspect we'd like to keep test coverage for the moments-with-single-precision case, since AFAIK that's the one that currently gets more use. However, looping over `[np.float32, np.float64]` might make sense for these tests (assuming the tolerance can be set appropriately).", "@mrry, it seems like `np.random.rand()` doesn't allow setting dtype yet, as seen [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.rand.html). \r\n\r\nJust tried below diff for looping over dtypes and it works fine:\r\n`//tensorflow/python:nn_test  -------                     PASSED in 22.6s`:\r\n```\r\n@@ -867,6 +866,7 @@ class MomentsTest(test_lib.TestCase):\r\n     for mu in [0.0, 1.0, 1e3]:\r\n       for sigma in [1.0, 0.1]:\r\n         for keep_dims in [True, False]:\r\n+         for dtype in [dtypes.float32, dtypes.float64]:\r\n             input_values = np.random.rand(*input_shape) * sigma + mu\r\n             expected_mean = np.mean(\r\n                 input_values, axis=moments_axes, keepdims=keep_dims)\r\n@@ -875,7 +875,7 @@ class MomentsTest(test_lib.TestCase):\r\n             with ops.Graph().as_default() as g:\r\n               with self.test_session(graph=g) as sess:\r\n                 inputs = constant_op.constant(\r\n-                  input_values, shape=input_shape, dtype=dtypes.float32)\r\n+                    input_values, shape=input_shape, dtype=dtype)\r\n                 mean, variance = nn_impl.moments(\r\n                     inputs, moments_axes, keep_dims=keep_dims)\r\n\r\n@@ -892,8 +892,12 @@ class MomentsTest(test_lib.TestCase):\r\n               # Make sure that there are no NaNs\r\n                 self.assertFalse(np.isnan(mean).any())\r\n                 self.assertFalse(np.isnan(variance).any())\r\n+               if dtype == 'dtypes.float64':\r\n                  self.assertAllClose(mean, expected_mean, rtol=tol, atol=tol)\r\n                   self.assertAllClose(variance, expected_var, rtol=tol, atol=tol)\r\n+                else:\r\n+                  self.assertAllClose(mean, expected_mean, rtol=4e-4, atol=4e-4)\r\n+                  self.assertAllClose(variance, expected_var, rtol=4e-4, atol=4e-4)\r\n```\r\nWhat would be the right approach?", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 30 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 46 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 61 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 76 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 91 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 106 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 121 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Is that still an issue with the latest code?", "@drpngx the issue still exists on v1.10.0.", "Following up on @mrry's idea, could we try maybe `np.random.rand(..).astype(np.float32)`?", "I tried the below:\r\n```\r\n--- a/tensorflow/python/ops/nn_test.py\r\n+++ b/tensorflow/python/ops/nn_test.py\r\n@@ -1009,7 +1009,7 @@ class MomentsTest(test_lib.TestCase):\r\n     for mu in [0.0, 1.0, 1e3]:\r\n       for sigma in [1.0, 0.1]:\r\n         for keep_dims in [True, False]:\r\n-          input_values = np.random.rand(*input_shape) * sigma + mu\r\n+          input_values = np.random.rand(*input_shape).astype(np.float32) * sigma + mu\r\n           expected_mean = np.mean(\r\n               input_values, axis=moments_axes, keepdims=keep_dims)\r\n           expected_var = np.var(\r\n```\r\nThe same issue still exists.", "@drpngx  @mrry  - Hi, could you please look into this issue.\r\n\r\n@namrata-ibm  @duane-ibm  -  Hi, are you still having this issue in the latest tensorflow versions ? Feel free to close if this doesn't persist. Also, appreciate if you create a new issue with the latest details(if the issue persists). Thanks !", "@harshini-gadige I don't have access to an s390x machine to reproduce this problem, which is why this issue was marked \"community support\".", "@hgadig Hi, yes we are still facing this issue for version 1.12.0 with same details. ", "This test passes in version 1.13.1. We may close this issue. Thanks for the support."]}, {"number": 16539, "title": "How to use tensorflow library in c programs??", "body": "I build shared libraries in tensorflow-serving with bazel.\r\nHowever, I can not build C programs with the shared libraries.\r\n\r\nI build C programs like this:\r\ng++ main.cc -L../../bazel-bin/tensorflow_serving/rnnlm -lrnnlm_client_run -ldata_generator\r\n\r\nDetails is shown below:\r\n\r\n../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictionService::Stub::Predict(grpc::Cli\r\n    entContext*, tensorflow::serving::PredictRequest const&, tensorflow::serving::PredictResponse*)'\r\n 12 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n 13 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType)\r\n     const'\r\n 14 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::_slow_mutable_model_spec()\r\n    '\r\n 15 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::FromProto(tensorflow::TensorProto const&)'\r\n 16 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictResponse::PredictResponse()'\r\n 17 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'\r\n 18 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::Tensor()'\r\n 19 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictionService::NewStub(std::shared_ptr\r\n    <grpc::ChannelInterface> const&, grpc::StubOptions const&)'\r\n 20 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeProto_Dim::TensorShapeProto_Dim(google::\r\n    protobuf::Arena*)'\r\n 21 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::TensorProto(google::protobuf::Arena*)'\r\n 22 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAligned(unsigned\r\n     long)'\r\n 23 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(\r\n    int) const'\r\n 24 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::~PredictRequest()'\r\n 25 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)'\r\n 26 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeProto_Dim::TensorShapeProto_Dim()'\r\n 27 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::PredictRequest()'\r\n 28 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const\r\n    *, unsigned long) const'\r\n 29 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::~TensorProto()'\r\n 30 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::_slow_mutable_tensor_shape()'", "comments": ["/CC @nfiedel.\r\n\r\nAlso, the TensorFlow Serving repo is [here](https://github.com/tensorflow/serving), so I think you should file issues there (@nfiedel can you confirm?)", "Feel free to file as a bug/request at the TensorFlow Serving link from @reedwm. Note that this looks like more general compiler / bazel issue that the team will likely not have time to investigate. If you do find a resolution, please share it along with any PRs for documentation or build changes. Thanks!"]}, {"number": 16538, "title": "Keras ProgBar is not working properly anymore with TF 1.5.0", "body": "If you run the following code with TF 1.4.*, this code works fine. However, if you run it with the newly released TF 1.5.0, the output is abnormal:\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\nprogress_bar = tf.contrib.keras.utils.Progbar(target=600)\r\n\r\nfor _ in range(600):\r\n    progress_bar.update(_)\r\n    time.sleep(0.01)\r\n```\r\n\r\n#### Screenshot Effect:\r\n\r\n##### In Jupyter NB:\r\n![image](https://user-images.githubusercontent.com/10923599/35502790-65112c36-04de-11e8-980f-59f1bc375d60.png)", "comments": ["Thank you for the bug report!\r\n\r\n@fchollet", "@fchollet I have submitted a PR to fix this issue: https://github.com/tensorflow/tensorflow/pull/16634", "I will try as soon as I can @frankgh \r\nThanks for the help", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Just for the record, this issue is still not fixed with TF 1.6.0", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed. The force argument is deprecated, unfortunately no warning is raised: f176a61", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16537, "title": "added audio_ops.cc to tf_op_files.txt to fix the Op type not registered DecodeWav error \u2026", "body": "\u2026- https://github.com/tensorflow/tensorflow/issues/15921", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Is there a reason this was closed and not merged? I'm seeing the same issue with the makefile build.", "@andrewharp sorry I closed it too fast.. please merge it first. \r\n\r\nBtw, I noticed there're other .cc files in the tensorflow/core/ops directory that are not included in the tf_op_files.txt file. Should all the ops/*.cc be included in tf_op_files.txt, as suggested by https://www.tensorflow.org/mobile/prepare_models: \"Operations are broken into two parts. The first is the op definition, which declares the signature of the operation, which inputs, outputs, and attributes it has. These take up very little space, and so all are included by default. \" \r\n", "@andrewharp I also had the same question as @jeffxtang about operations that are not included in `tf_op_files.txt` (or even in [core/kernels/BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD) for that matter), such as `cwise_op_cos.cc`. Is there any particular reason why some operations are not included there?\r\n\r\nIf I'm understanding the building process correctly, as a result at the moment `android_extended_ops` group includes complex operations such as `decode_wav_op.cc`, but not something as basic as cosine operation `cwise_op_cos.cc`."]}, {"number": 16536, "title": "Feature Request: global average pooling layer in tf.layers", "body": "Hello,\r\n\r\nCan we add an implementation of global_average_pooling to tf.layers or tf.contrib.layers? It can look much like the Keras implementation here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool3D\r\nand essentially just requires calling tf.reduce_mean.\r\n\r\nI suspect lots of people have written functions called global_pooling that just call reduce mean, and it would be nice to have a tf.layers function that just does this for consistency/readability.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: AWS Deep Learning AMI (Conda)\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: K80\r\n- **Exact command to reproduce**: N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@martinwicke WDYT? Contributions welcome?", "@fchollet This is a classic example of where having either a viable automatic LambdaLayer, or an alternative automated way of making all functions into Layers would help.\r\n\r\nI'm not a fan of adding every TF function as a layer. As insofar as this impacts Keras workflows (incomplete layer graph), we need to find a solution to this.", "Sorry, how would this impact Keras workflows? I believe this is a function that already exists in tf.keras as a layer that I'm hoping to add to tf.layers.", "@martinwicke auto-wrapping non-layer ops as layer should not be an issue (once we figure out serialization). \r\n\r\nBut in this specific case, a global pooling layer has valuable benefits: because it includes a `data_format` argument which can have a global default, it enables you to build convnets where you can switch from NCHW to NHWC in one click.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@fchollet in light of your comment, should we work on a PR for this? I can probably give it a shot.", "@ankitvgupta no, I recommend you use `tf.keras.layers.GlobalMaxPooling3D()` instead.", "Got it, thanks."]}]