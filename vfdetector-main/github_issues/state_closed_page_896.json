[{"number": 26598, "title": "Getting error - \"tf.gfile.Stat()\" gets result that \"file does not exist\", but it does exists", "body": "There are files in \"neg\" directory:\r\nIn [81]: tf.gfile.ListDirectory(\"s3://lookalike-sample/novel_user/ds=20190310/neg\")\r\nOut[81]:\r\n['_SUCCESS',\r\n 'part-00002.gz',\r\n 'part-00003.gz',\r\n 'part-00004.gz']\r\n\r\nBut when I input:\r\ntf.gfile.Stat(\"s3://lookalike-sample/novel_user/ds=20190310/neg/part-00002.gz\")\r\n\r\ngetting error:\r\nNotFoundError: Object s3://lookalike-sample/novel_user/ds=20190310/neg/part-00002.gz does not exist\r\n\r\n- OS Platform and Distribution ( Linux Ubuntu 16.04):\r\n- TensorFlow version 1.8.0\r\n- Python version:2.7.4\r\n\r\n\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26597, "title": "Writing to S3 fails occasionally", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nubuntu 16.04 (docker)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\npip installed into docker image\r\n- TensorFlow version (use command below):\r\n1.12.0-gpu\r\n- Python version:\r\n3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nOccasionally when I write to an S3 file like so:\r\n```python\r\ndef s3_print(*args,**kwargs):\r\n    with tf.gfile.GFile('s3://bucket/log.txt','a') as f:\r\n        print(*args,**kwargs, file=f)\r\n```\r\nI get errors like this\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/train.py\", line 602, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/root/train.py\", line 289, in main\r\n    s3_print('Distinct elements: {}'.format(elements))\r\n  File \"/root/train.py\", line 113, in tee_print\r\n    print(*args,**kwargs, file=f)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 208, in __exit__\r\n    self.close()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 240, in close\r\n    pywrap_tensorflow.Set_TF_Status_from_Status(status, ret_status)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: XAmzContentSHA256Mismatch: Unable to parse ExceptionName: XAmzContentSHA256Mismatch Message:\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Other info / logs**\r\nGoogling the keyword `XAmzContentSHA256Mismatch` led me to a bunch of s3 related issues, so I'm guessing that's related.\r\n\r\nI'm not sure how to work around this problem so any guidance would be appreciated.", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26596, "title": "Getting error - Expects arg[0] to be float but uint8 is provided while using bytebuffer for image data in tensorflow inference.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but have only modified the model name in tensorflow/examples/android and tried passing bytebuffer of bytes for the imagedata.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Yes, android phones - one plus 6\r\n- TensorFlow installed from (source or binary): github/tensorflow\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI have modified the tensorflow/examples/android/demo to use my custom tf model(created from mobilenet_v1 trained for posenet tfjs). Instead of float values, am passing the image as a byte buffer using the below code:-\r\n\r\nprivate String[] outputNames;\r\nByteBuffer imgData = null;\r\n\r\n        imgData =\r\n                ByteBuffer.allocateDirect(\r\n                          DIM_BATCH_SIZE  //1\r\n                * getImageSizeX()    //513\r\n                * getImageSizeY()    //513\r\n                * DIM_PIXEL_SIZE   //3\r\n                * getNumBytesPerChannel());    //4\r\n        imgData.order(ByteOrder.nativeOrder());\r\n\r\n        int pixel = 0;\r\n\r\n        for (int i = 0; i < 513; ++i) {\r\n            for (int j = 0; j < 513; ++j) {\r\n                final int val = intValues[pixel++];\r\n                imgData.putFloat( ((val >> 16) & 0xFF));\r\n                imgData.putFloat( ((val >> 8) & 0xFF));\r\n                imgData.putFloat( (val & 0xFF));\r\n            }\r\n        }\r\n\r\nImage size is 513 * 513.\r\n\r\n1. The interfaceinference.feed is running without any issues.\r\n\r\ninferenceInterface.feed(inputName, imgData, 1, inputSize, inputSize, 3, 4);\r\n\r\n2. It fails when I try to run interfaceinference.run.\r\n\r\ninferenceInterface.run(outputNames, logStats);\r\n\r\nHere outputNames is a String[]. Although in the tensorflow interface.java code, it seems to handle the feed for Byte Buffers, the tensor created is a byte buffer - uint8. Then how is the run failing?\r\n\r\n**Describe the expected behavior**\r\n\r\nThe inferenceInterface.run should be able to handle bytebuffer inputs as it is being handled in feed. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nCode is provided above. Can reuse the existing model as in tensorflow/examples/android/demo and modify the TensorflowImageClassifier.java to pass the input image as a byteBuffer.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n    \r\n    --------- beginning of crash\r\n2019-03-12 11:46:54.161 3453-3469/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.demo, PID: 3453\r\n    java.lang.IllegalArgumentException: Expects arg[0] to be float but uint8 is provided\r\n        at org.tensorflow.Session.run(Native Method)\r\n        at org.tensorflow.Session.access$100(Session.java:48)\r\n        at org.tensorflow.Session$Runner.runHelper(Session.java:314)\r\n        at org.tensorflow.Session$Runner.run(Session.java:264)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n", "comments": ["No answer ?", "[#34186 ](https://github.com/tensorflow/tensorflow/issues/34186)\r\ni have the same problem.", "i sloved", "@atlantiswqq How?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26596\">No</a>\n"]}, {"number": 26595, "title": "Issues with running compiled versions of tensorflow for different machines and CFLAGS", "body": "This is a bug  related to https://bugs.gentoo.org/show_bug.cgi?id=679714.\r\n\r\nHere is the system information:\r\n**### System information**\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**2019-03-12 05:23:07.606918: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.**\r\n\r\nActual system is an IvyBridge non AVX2 CPU. GCC version 8.3.0. No LTO. Simple use flags. \r\n\r\nHow did this happen? I compiled tensor flow on my build machine. The build machine has loads of RAM big fans and is suited for compling stuff. I wanted to run tensorflow on my notebook, which was a bit older and more limited. I tried building tensor flow from the gentoo ebuild directly, with specific CPU flags for the CPU that I was going to run tensorflow on. I then ran into the gentoo bug 679714. This is where the libtensorflow_framework.so is missing the symbol __cpu_model (the flags I used on the build machine were **-O2 -march=ivybridge -mcx16 -msahf -mno-movbe -maes -mpclmul -mpopcnt -mno-abm -mno-lwp -mno-fma -mno-fma4 -mno-xop -mno-bmi -mno-bmi2 -mno-tbm -mavx -mno-avx2 -msse4.1 -msse4.2 -mno-lzcnt -mno-rtm -mno-hle -mrdrnd -mf16c -mfsgsbase -mno-rdseed -mno-prfchw -mno-adx -mfxsr -mxsave -mxsaveopt -mtune=ivybridge -minline-all-stringops -finline-functions -fmerge-all-constants**). I changed the CFLAGS to \"-### O2\" only and tensorflow ran fine on the build machine. I tried to run a python script (python versions 3.6 and 2.7) found was told that the CPU didn't support AVX2.\r\n\r\n\r\n\r\n", "comments": ["@perfinion may have an idea.\r\nAccording to this, `-mno-avx2` should disable avx2, but I did not go through all flags.\r\nhttps://gcc.gnu.org/onlinedocs/gcc-8.3.0/gcc/x86-Options.html#x86-Options\r\n\r\nSo it is possible, within so many flags one of the enabled things is using avx2, and gcc is not handling all flag combinations correctly (which is not unheard of).\r\n", "No those flags are from gcc-march=native for IvyBridge. To my knowledge and that of wikipedia there are no know IvyBridge CPUs that have support AVX2. https://en.wikipedia.org/wiki/Advanced_Vector_Extensions\r\n\r\nThere are many many things that it could possibly be, but given the fact that these flags have remained constant for a few gcc versions and when I compile and AVX2 program with explicit AVX2 code I get a compiler error, I think that the odds of it being a GCC error are small.\r\n\r\nAnyhow, its almost the end of like for IvyBridge, so I would not worry about it, unless others are whining. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26595\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26595\">No</a>\n"]}, {"number": 26594, "title": "How to input a list of string values to Estimator", "body": "# I want to use tfhub'emlo to build a text classifier, but the elmo need a string values placeholder. in order to input feature to estimator, I need to use the tf.feature_column building the input feature columns.But how can i input a list of string values to estimator?\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26593, "title": "Cannot get tensorflow to work. DLL load failed and DLL initialization routine failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows OS 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI cant seem to import tensorflow even though I have supposedly installed CUDA and followed an online tutorial. \r\nThis is my error when I type import tensorflow:\r\n\r\nimport tensorflow Traceback (most recent call last): File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in _pywrap_tensorflow_internal = swig_import_helper() File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\imp.py\", line 243, in load_module return load_dynamic(name, filename, file) File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\imp.py\", line 343, in load_dynamic return _load(spec)\r\n\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last): File \"\", line 1, in File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow__init__.py\", line 24, in from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python__init__.py\", line 49, in from tensorflow.python import pywrap_tensorflow File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in raise ImportError(msg) ImportError: Traceback (most recent call last): File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in _pywrap_tensorflow_internal = swig_import_helper() File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\imp.py\", line 243, in load_module return load_dynamic(name, filename, file) File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\imp.py\", line 343, in load_dynamic return _load(spec)\r\n\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime.\r\n\r\nThis is what I get when I run tensorflow_self_check.py: ERROR: Failed to import the TensorFlow module.\r\n\r\nPython version is 3.6.\r\nTensorFlow is installed at: C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\r\nCould not load 'cudart64_80.dll'. The GPU version of TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 8.0 from this URL: https://developer.nvidia.com/cuda-toolkit\r\nCould not load 'nvcuda.dll'. The GPU version of TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Typically it is installed in 'C:\\Windows\\System32'. If it is not present, ensure that you have a CUDA-capable GPU with the correct driver installed.\r\n", "comments": ["Apparently you have installed TF-GPU version and it looks like you haven't added cuda path to your environment variable.\r\nPlease refer this [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup) to know more or else you can always try to install TF cpu version instead. Note that you need to install cuda 9.0 for TF 1.12 in order to use pre built TF-gpu binaries. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I am having the same issue is anybody able to solve this?"]}, {"number": 26592, "title": "Retrieve input/output lengths from Eager C API", "body": "Adds the equivalent of `TF_OperationInputListLength` and `TF_OperationOutputListLength` in the eager C API.\r\n\r\nNote: I based the solution on the same algorithm found in these methods, which is not optimal in case you are having more than one input/output of interest and you need to call the API more than once for the same operation (e.g. attributes are collected and name ranges are computed on each call).\r\n\r\nI presumed that optimizations could be applied later, maybe for both graph and eager execution mode.\r\n\r\nCC: @alextp ", "comments": ["Are the build failures still related to the PR?", "@karllessard they look unrelated at first glance"]}, {"number": 26591, "title": "Cannot create a SavedModel with a DenseFeatures layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION='2.0.0-dev20190311'\r\nGIT_VERSION='v1.12.0-9917-gf988edacf4'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nCannot create a `SavedModel` with a `DenseFeatures` layer, I get an exception: `AttributeError: 'str' object has no attribute 'shape'` (see full stacktrace below).\r\n\r\n**Describe the expected behavior**\r\nI except to be able to save any tf.keras model (except those with dynamic layers) that uses only standard layers.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.train import Example, Features, Feature, FloatList\r\nimport numpy as np\r\n\r\n# Create a simple TFRecord file\r\nages = np.random.rand(100)*50 + 20\r\nheights = np.random.rand(100)*40 + 150\r\nweights = 0.75 * heights - 75. + ages * 0.1 + np.random.rand(100) * 20\r\nwith tf.io.TFRecordWriter(\"weights.tfrecord\") as f:\r\n    for ex_age, ex_height, ex_weight in zip(ages, heights, weights):\r\n        example = Example(features=Features(feature={\r\n            \"age\": Feature(float_list=FloatList(value=[ex_age])),\r\n            \"height\": Feature(float_list=FloatList(value=[ex_height])),\r\n            \"weight\": Feature(float_list=FloatList(value=[ex_weight]))\r\n        }))\r\n        f.write(example.SerializeToString())\r\n\r\n# Create a TFRecordDataset to read the data\r\nage = tf.feature_column.numeric_column(\"age\")\r\nheight = tf.feature_column.numeric_column(\"height\")\r\nweight = tf.feature_column.numeric_column(\"weight\")\r\ncolumns = [age, height, weight]\r\nfeature_descriptions = tf.feature_column.make_parse_example_spec(columns)\r\ndef parse_examples(serialized_examples):\r\n    features = tf.io.parse_example(serialized_examples,\r\n                                   feature_descriptions)\r\n    targets = features.pop(\"weight\")\r\n    return features, targets\r\ndataset = tf.data.TFRecordDataset([\"weights.tfrecord\"])\r\ndataset = dataset.shuffle(100).batch(32).map(parse_examples)\r\n\r\n# Create, train and use the model with a DenseFeatures layer\r\nmodel = keras.models.Sequential([\r\n    keras.layers.DenseFeatures(columns[:-1]),\r\n    keras.layers.Dense(1)\r\n])\r\nmodel.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-5))\r\nhistory = model.fit(dataset, epochs=5)\r\ny_pred = model.predict({\"age\": tf.constant([25.]),\r\n                        \"height\": tf.constant([180.])})\r\n\r\n# Saving using the save() method works fine\r\nmodel.save(\"my_weight_model.h5\")\r\n\r\n# Saving to a SavedModel fails\r\ntf.saved_model.save(model, \"my_weight_model.savedmodel\") # AttributeError!\r\n```\r\n\r\n**Other info / logs**\r\nHere is the stacktrace:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-116-5ddc1d5acd67> in <module>\r\n     39 \r\n     40 model.save(\"my_weight_model.h5\")\r\n---> 41 tf.saved_model.save(model, \"my_weight_model.savedmodel\")\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures)\r\n    773   if signatures is None:\r\n    774     signatures = signature_serialization.find_function_to_export(\r\n--> 775         checkpoint_graph_view)\r\n    776 \r\n    777   signatures = signature_serialization.canonicalize_signatures(signatures)\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)\r\n     63   # If the user did not specify signatures, check the root object for a function\r\n     64   # that can be made into a signature.\r\n---> 65   functions = saveable_view.list_functions(saveable_view.root)\r\n     66   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)\r\n     67   if signature is not None:\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in list_functions(self, obj)\r\n    109     obj_functions = self._functions.get(obj, None)\r\n    110     if obj_functions is None:\r\n--> 111       obj_functions = obj._list_functions_for_serialization()  # pylint: disable=protected-access\r\n    112       self._functions[obj] = obj_functions\r\n    113     return obj_functions\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _list_functions_for_serialization(self)\r\n   1793   def _list_functions_for_serialization(self):\r\n   1794     return {\r\n-> 1795         '_default_save_signature': saving_utils.trace_model_call(self)\r\n   1796     }\r\n   1797 \r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py in trace_model_call(model, input_signature)\r\n     76     for input_tensor, input_name in zip(inputs, input_names):\r\n     77       input_specs.append(tensor_spec.TensorSpec(\r\n---> 78           shape=input_tensor.shape, dtype=input_tensor.dtype,\r\n     79           name=input_name))\r\n     80     # The input signature of the call function is a list with one element, since\r\n\r\nAttributeError: 'str' object has no attribute 'shape'\r\n```", "comments": ["Awesome, thanks for the quick fix.", "Thanks, I was having the same issue, but it seemed to reveal another issue:\r\nTypeError: input_signature must be either a tuple or a list, received <class 'dict'>\r\n\r\nUsing: https://files.pythonhosted.org/packages/57/08/8e39ef54e3aeaf35f87a5703df01cf1e5d11f1ac4ff0e7939191f22945de/tf_nightly_gpu_2.0_preview-2.0.0.dev20190315-cp37-cp37m-manylinux1_x86_64.whl\r\n\r\nimport os\r\nfrom os.path import expanduser\r\n\r\nfrom absl import app\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nMODEL_DIR = '/tmp/tensorflow'\r\nNUM_TRAIN_EPOCHS = 1\r\n\r\ndef mkmapfn(specs):\r\n  def mapfn(example):\r\n    x = tf.io.parse_single_example(example, specs)\r\n    y = x.pop('label')\r\n    x['image'] = tf.reshape(tf.image.decode_image(x['image']), [28, 28, 1])\r\n    return x, y\r\n\r\n  return mapfn\r\n\r\n\r\ndef get_tfds_pattern(name, split):\r\n  _, info = tfds.load(name, with_info=True)\r\n  file_pattern = os.path.join(\r\n      expanduser(tfds.core.utils.constants.DATA_DIR), info.full_name,\r\n      '*{}*'.format(split))\r\n  return file_pattern\r\n\r\n\r\ndef get_tfds_filenames(name, split):\r\n  pattern = get_tfds_pattern(name, split)\r\n  return tf.io.gfile.glob(pattern)\r\n\r\ndef main(args):\r\n  filenames = get_tfds_filenames('mnist', 'train')\r\n\r\n  image = tf.feature_column.numeric_column('image', shape=[28, 28, 1])\r\n\r\n  columns = [image]\r\n\r\n  specs = tf.feature_column.make_parse_example_spec(columns)\r\n  specs['label'] = tf.io.FixedLenFeature((), dtype=tf.int64, default_value=0)\r\n  specs['image'] = tf.io.FixedLenFeature((), dtype=tf.string, default_value='')\r\n\r\n  ds = tf.data.TFRecordDataset(filenames)\r\n  ds = ds.map(mkmapfn(specs))\r\n  ds = ds.take(1000)\r\n  ds = ds.batch(128, drop_remainder=True)\r\n\r\n  model = tf.keras.models.Sequential([\r\n    tf.keras.layers.DenseFeatures(columns),\r\n    tf.keras.layers.Dense(512, activation='relu'),\r\n    tf.keras.layers.Dense(512, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n\r\n  model.compile(\r\n      optimizer='rmsprop',\r\n      loss='categorical_crossentropy',\r\n      metrics=['accuracy'])\r\n  model.fit(ds)\r\n  tf.saved_model.save(model, MODEL_DIR)\r\n\r\n\r\nif __name__ == '__main__':\r\n  app.run(main)\r\n\r\n\r\nraceback (most recent call last):\r\n  File \"/home/joetoth/projects/psycho/psy/automl/auto_layer.py\", line 65, in <module>\r\n    app.run(main)\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/joetoth/projects/psycho/psy/automl/auto_layer.py\", line 61, in main\r\n    tf.saved_model.save(model, MODEL_DIR)\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 775, in save\r\n    checkpoint_graph_view)\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_serialization.py\", line 65, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 111, in list_functions\r\n    obj_functions = obj._list_functions_for_serialization()  # pylint: disable=protected-access\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1804, in _list_functions_for_serialization\r\n    '_default_save_signature': saving_utils.trace_model_call(self)\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py\", line 89, in trace_model_call\r\n    @def_function.function(input_signature=input_signature, autograph=False)\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 980, in decorated\r\n    experimental_autograph_options=experimental_autograph_options))\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 286, in __init__\r\n    python_function, input_signature)\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 962, in from_function_and_signature\r\n    kwargs_to_include, input_signature)\r\n  File \"/home/joetoth/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1001, in __init__\r\n    \"list, received \" + str(type(input_signature)))\r\nTypeError: input_signature must be either a tuple or a list, received <class 'dict'>", "@joetoth huh, yeah I can reproduce. Surprised the unit test didn't cover that too. Will take a look.", "There was a special case for single-element sequences that was broken. Should be fixed now. Thank you for the reports!", "I am not able to create a model by joining Dense on top of DenseFeatures. When I do that it is complaining that Dense Features doesnt have shape attribute.\r\n\r\n` for i in range(hidden_layers):\r\n---> 77         l= tf.keras.layers.Dense(hidden_units, activation='relu')(l)\r\n     78         # l=layers.Dropout(dropout_rate)(l)\r\n     79     pred= tf.keras.layers.Dense(horizon, name='out')(l)\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-ffd3b642-1ea5-4fc2-92b2-4a9b2d684b8b/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    674         # Eager execution on data tensors.\r\n    675         with backend.name_scope(self._name_scope()):\r\n--> 676           self._maybe_build(inputs)\r\n    677           with base_layer_utils.autocast_context_manager(\r\n    678               input_list, self._mixed_precision_policy.should_cast_variables):\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-ffd3b642-1ea5-4fc2-92b2-4a9b2d684b8b/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n   1863 \r\n   1864     input_spec.assert_input_compatibility(\r\n-> 1865         self.input_spec, inputs, self.name)\r\n   1866     input_list = nest.flatten(inputs)\r\n   1867     if input_list and self._dtype is None:\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-ffd3b642-1ea5-4fc2-92b2-4a9b2d684b8b/lib/python3.5/site-packages/tensorflow/python/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)\r\n    107         spec.min_ndim is not None or\r\n    108         spec.max_ndim is not None):\r\n--> 109       if x.shape.ndims is None:\r\n    110         raise ValueError('Input ' + str(input_index) + ' of layer ' +\r\n    111                          layer_name + ' is incompatible with the layer: '\r\n\r\nAttributeError: 'DenseFeatures' object has no attribute 'shape'`\r\n\r\nCan Someone help", "> I am not able to create a model by joining Dense on top of DenseFeatures. When I do that it is complaining that Dense Features doesnt have shape attribute.\r\n> \r\n> ` for i in range(hidden_layers):\r\n> ---> 77 l= tf.keras.layers.Dense(hidden_units, activation='relu')(l)\r\n> 78 # l=layers.Dropout(dropout_rate)(l)\r\n> 79 pred= tf.keras.layers.Dense(horizon, name='out')(l)\r\n> \r\n> /local_disk0/pythonVirtualEnvDirs/virtualEnv-ffd3b642-1ea5-4fc2-92b2-4a9b2d684b8b/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in **call**(self, inputs, *args, **kwargs)\r\n> 674 # Eager execution on data tensors.\r\n> 675 with backend.name_scope(self._name_scope()):\r\n> --> 676 self._maybe_build(inputs)\r\n> 677 with base_layer_utils.autocast_context_manager(\r\n> 678 input_list, self._mixed_precision_policy.should_cast_variables):\r\n> \r\n> /local_disk0/pythonVirtualEnvDirs/virtualEnv-ffd3b642-1ea5-4fc2-92b2-4a9b2d684b8b/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n> 1863\r\n> 1864 input_spec.assert_input_compatibility(\r\n> -> 1865 self.input_spec, inputs, self.name)\r\n> 1866 input_list = nest.flatten(inputs)\r\n> 1867 if input_list and self._dtype is None:\r\n> \r\n> /local_disk0/pythonVirtualEnvDirs/virtualEnv-ffd3b642-1ea5-4fc2-92b2-4a9b2d684b8b/lib/python3.5/site-packages/tensorflow/python/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)\r\n> 107 spec.min_ndim is not None or\r\n> 108 spec.max_ndim is not None):\r\n> --> 109 if x.shape.ndims is None:\r\n> 110 raise ValueError('Input ' + str(input_index) + ' of layer ' +\r\n> 111 layer_name + ' is incompatible with the layer: '\r\n> \r\n> AttributeError: 'DenseFeatures' object has no attribute 'shape'`\r\n> \r\n> Can Someone help\r\n\r\nI encountered the same problem, did you solve it?I  can't solve it without trying for a long time"]}, {"number": 26590, "title": "[tf.keras.layers.LSTM] Initializer fails with input_length parameter", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n```\r\n4.15.0-46-generic #49~16.04.1-Ubuntu SMP Tue Feb 12 17:45:24 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n- TensorFlow installed from (source or binary): `conda` as binary.\r\n- TensorFlow version (use command below): `1.10`, `1.12`, and `1.13` confirmed\r\n- Python version: `Python 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16)`\r\n\r\n(The following are irrelevant since I'm not even running with a session or in eager mode)\r\n- CUDA/cuDNN version: CUDA 9.0\r\n- GPU model and memory: `GeForce GTX TITAN X`\r\n\r\n**MWE**\r\n```\r\nimport tensorflow as tf\r\nlstm = tf.keras.layers.LSTM(512, input_length=32)\r\n```\r\n**Current behavior**\r\nHere's `python` error message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2230, in __init__\r\n    **kwargs)\r\n  File \"~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 462, in __init__\r\n    super(RNN, self).__init__(**kwargs)\r\n  File \"~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\", line 474, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 138, in __init__\r\n    raise TypeError('Keyword argument not understood:', kwarg)\r\nTypeError: ('Keyword argument not understood:', 'input_length')\r\n```\r\n\r\n**Expected behavior**\r\n`LSTM` class inherits from `RNN`, which has `input_length` as a parameter as described [here](https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/keras/layers/RNN). Therefore the constructor of `RNN` should use this parameter but it does not as you can see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L391-L399) in the code.", "comments": ["@qlzh727 Qianli, are you a good person to look at this?", "Yes, I will take a look within this week.", "Ah, seems that it is an error in the documentation, the input_length is no longer needed and is inferred from the input tensor shape. I will fix the documentation soon.", "This should now be fixed."]}, {"number": 26589, "title": "Undefined python symbols building TensorFlow with Python 3.7 on Mac", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: Git: df3a3375941b9e920667acfe72fb4c33a8f45503\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: Source\r\n- Bazel version (if compiling from source): 0.23.1\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nBuilding TensorFlow produces errors about missing Python symbols:\r\n\r\n```\r\n% bazel build --color=yes --config=opt --cxxopt=-std=c++1z --cxxopt=-stdlib=libc++ //tensorflow/tools/pip_package:build_pip_package\r\n...\r\nERROR: /private/var/tmp/_bazel_irving/d4bad52af7e8ccc6c91f211db4181990/external/protobuf_archive/BUILD:626:1: Linking of rule '@protobuf_archive//:python/google/protobuf/internal/_api_implementation.so' failed (Exit 1)\r\nUndefined symbols for architecture x86_64:\r\n  \"_PyModule_AddIntConstant\", referenced from:\r\n      _PyInit__api_implementation in api_implementation.o\r\n  \"_PyModule_Create2\", referenced from:\r\n      _PyInit__api_implementation in api_implementation.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 143.602s, Critical Path: 33.56s\r\nINFO: 427 processes: 427 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI don't seem to be the only person with this problem: https://stackoverflow.com/questions/55089427/installing-tensorflow-on-osx-clang-error-linker-command-failed-with-exit-cod\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n./configure\r\nbazel build --color=yes --config=opt --cxxopt=-std=c++1z --cxxopt=-stdlib=libc++ //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can't repeat (same config).", "Here's a more targeted failing command, using `-s` to get subcommand details:\r\n\r\n```\r\n% bazel build -s @protobuf_archive//:python/google/protobuf/internal/_api_implementation.so\r\nINFO: Analysed target @protobuf_archive//:python/google/protobuf/internal/_api_implementation.so (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nSUBCOMMAND: # @protobuf_archive//:python/google/protobuf/internal/_api_implementation.so [action 'Linking external/protobuf_archive/python/google/protobuf/internal/_api_implementation.so']\r\n(cd /private/var/tmp/_bazel_irving/d4bad52af7e8ccc6c91f211db4181990/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=MacOSX \\\r\n    APPLE_SDK_VERSION_OVERRIDE=10.14 \\\r\n    PATH=... \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n    XCODE_VERSION_OVERRIDE=10.1.0 \\\r\n  external/local_config_cc/cc_wrapper.sh -lc++ -fobjc-link-runtime -shared -o bazel-out/darwin-opt/bin/external/protobuf_archive/python/google/protobuf/internal/_api_implementation.so -Wl,-force_load,bazel-out/darwin-opt/bin/external/protobuf_archive/_objs/python/google/protobuf/internal/_api_implementation.so/api_implementation.o -headerpad_max_install_names -no-canonical-prefixes '-mmacosx-version-min=10.14')\r\nERROR: /private/var/tmp/_bazel_irving/d4bad52af7e8ccc6c91f211db4181990/external/protobuf_archive/BUILD:626:1: Linking of rule '@protobuf_archive//:python/google/protobuf/internal/_api_implementation.so' failed (Exit 1)\r\nUndefined symbols for architecture x86_64:\r\n  \"_PyModule_AddIntConstant\", referenced from:\r\n      _PyInit__api_implementation in api_implementation.o\r\n  \"_PyModule_Create2\", referenced from:\r\n      _PyInit__api_implementation in api_implementation.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget @protobuf_archive//:python/google/protobuf/internal/_api_implementation.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.202s, Critical Path: 0.07s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nThe obvious conclusion would be that it fails to find Python symbols since the link command doesn't mention Python, but this feels suspiciously simple.", "Appears to be a bazel bug: https://github.com/bazelbuild/bazel/issues/7607", "The bazel folk fixed this on their end."]}, {"number": 26588, "title": "module 'tensorflow' has no attribute 'zeroes' in Graph guide", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12.0\r\n- Doc Link: https://www.tensorflow.org/guide/graphs\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nOn the documentation an example was given using tf.zeroes().\r\n\r\nHowever, the module 'tensorflow' has no attribute 'zeroes'. \r\n\r\n```\r\nwith tf.device(\"/job:ps/task:0\"):\r\n  weights_1 = tf.Variable(tf.truncated_normal([784, 100]))\r\n  biases_1 = tf.Variable(tf.zeroes([100]))\r\n\r\nwith tf.device(\"/job:ps/task:1\"):\r\n  weights_2 = tf.Variable(tf.truncated_normal([100, 10]))\r\n  biases_2 = tf.Variable(tf.zeroes([10]))\r\n\r\nwith tf.device(\"/job:worker\"):\r\n  layer_1 = tf.matmul(train_batch, weights_1) + biases_1\r\n  layer_2 = tf.matmul(train_batch, weights_2) + biases_2\r\n```\r\nThe documentation should use tf.zeros() instead\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nyes", "comments": ["https://github.com/tensorflow/docs/pull/383 "]}, {"number": 26587, "title": "Tensorflow 2.0a tf.function variable batch size", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave\r\n- TensorFlow installed from (source or binary): pip installed \r\n- TensorFlow version (use command below): 2.0a\r\n- Python version: 3.7.2\r\n\r\nHi,\r\n\r\nI'm experimenting with irregular batch size and I ran accross this strange performance behavior.\r\nConsider the following code:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nnetwork = tf.keras.Sequential([\r\n    tf.keras.layers.LSTM(32),\r\n    tf.keras.layers.Dense(1)\r\n])\r\n\r\ndef f(x):\r\n    return tf.squeeze(network(x))\r\n\r\n\r\ntf_f = tf.function(f)\r\n\r\ndef g(use_tf=False, variable_batch_size=False):\r\n    np.random.seed(0)\r\n    \r\n    l = lambda: 90\r\n    if variable_batch_size:\r\n        l = lambda: np.random.randint(80, 101)\r\n        \r\n    fun = tf_f if use_tf else f\r\n    for _ in range(1000):\r\n        x = np.random.randn(l(), 3, 2).astype('float32')\r\n        fun(x)\r\n```\r\n\r\nNot using the wrapped tf_function performs equally fine whether variable_batch_size is on/off:\r\n\r\n```python\r\n%time g()  # 3.92s\r\n%time g(variable_batch_size=True)  # 3.85s\r\n```\r\n\r\nHowever for the tf_function version I get (after warm up)\r\n\r\n```python\r\n%time g(use_tf=True)   # 451ms\r\n%time g(use_tf=True, variable_batch_size=True)  # 4.34s !!!\r\n```\r\n\r\nI'm a bit surprise to see arrayprint at the top of the list when looking at the profiler\r\n\r\n```python\r\n%prun -s cumulative g(use_tf=True, variable_batch_size=True)\r\n\r\n       13112280 function calls (12256090 primitive calls) in 6.265 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.000    0.000    6.265    6.265 {built-in method builtins.exec}\r\n        1    0.000    0.000    6.265    6.265 <string>:1(<module>)\r\n        1    0.006    0.006    6.265    6.265 <ipython-input-34-9a908934c266>:1(g)\r\n     1000    0.005    0.000    6.227    0.006 def_function.py:407(__call__)\r\n     1000    0.002    0.000    6.222    0.006 function.py:1285(__call__)\r\n     1000    0.018    0.000    5.650    0.006 function.py:1526(_maybe_define_function)\r\n      950    0.006    0.000    5.512    0.006 ops.py:829(__repr__)\r\n      950    0.004    0.000    5.482    0.006 ops.py:215(numpy_text)\r\n      950    0.003    0.000    5.464    0.006 {built-in method builtins.repr}\r\n      950    0.008    0.000    5.462    0.006 arrayprint.py:1397(_array_repr_implementation)\r\n      950    0.004    0.000    5.433    0.006 arrayprint.py:518(array2string)\r\n      950    0.003    0.000    5.422    0.006 arrayprint.py:463(wrapper)\r\n      950    0.007    0.000    5.418    0.006 arrayprint.py:480(_array2string)\r\n      950    0.002    0.000    3.703    0.004 arrayprint.py:707(_formatArray)\r\n857140/950    1.477    0.000    3.701    0.004 arrayprint.py:716(recurser)\r\n      950    0.004    0.000    1.707    0.002 arrayprint.py:411(_get_format_function)\r\n      950    0.002    0.000    1.699    0.002 arrayprint.py:367(<lambda>)\r\n      950    0.020    0.000    1.697    0.002 arrayprint.py:834(__init__)\r\n...\r\n```", "comments": ["Actually, using\r\n\r\n```python\r\ntf_f = tf.function(f, input_signature=[tf.TensorSpec(shape=(None, 3, 2), dtype=tf.float32)])\r\n```\r\n\r\nsolves the performance issue. As stated in the doc for tf.function, \"`function` instantiates a separate graph for every unique set of **input shapes** and datatypes.\"\r\n\r\nClosing."]}, {"number": 26586, "title": "tensorflow lite: error while converting frozen model to lite format", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary):  Using Anaconda navigator\r\n- TensorFlow version (use command below): v1.12.0-9901-gf380d8b8e5 1.14.1-dev20190309\r\n- Python version:  Python 3.6.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: using cpu version\r\n- GPU model and memory: na\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI am trying to quantize a model that is made using tfslim library. I froze it using graph_util.convert_variables_to_constants function and tried to convert it to tflite formate for quantization. I am using win 10 and tf-nighlty. the following error appears:\r\nTOCO failed. See console for info.\r\n2019-03-12 00:20:27.142300: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 221 operators, 366 arrays (0 quantized)\r\n2019-03-12 00:20:27.147170: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 221 operators, 366 arrays (0 quantized)\r\n2019-03-12 00:20:27.243872: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:722] Check failed: start_array.data_type == ArrayDataType::kInt32 Range op inputs must be int32.\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00001f44 (most recent call first):\r\n  File \"c:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33 in execute\r\n  File \"c:\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 251 in _run_main\r\n  File \"c:\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 300 in run\r\n  File \"c:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40 in run\r\n  File \"c:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59 in main\r\n  File \"C:\\anaconda3\\Scripts\\toco_from_protos.exe\\__main__.py\", line 9 in <module>\r\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85 in _run_code\r\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193 in _run_module_as_main\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nShould convert frozen pb file to tflite with int8 quantization\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n# pb file in the below link \r\nhttps://drive.google.com/open?id=1Ba6aEuv2VTGD_bct24UBiMh7MFe_jkOr\r\n\r\n\r\ngraph_def_file = \"frozen_model_yolov3-tiny.pb\"\r\ninput_arrays = [\"input_to_model\"]\r\noutput_arrays = [\"concat_1\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file, input_arrays, output_arrays)\r\ntflite_model = converter.convert()\r\nopen(\"converted_tiny_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. ", "> \r\n> \r\n> Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue.\r\n\r\nIve updated the post. please have a look", "> \r\n> \r\n> Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue.\r\n\r\nIve updated the post. please have a look", "@haozha111 Could you help to take a look at the model? \r\nI think this requires broadening supported types of Range, in both converter and runtime", "I downloaded your model, but I can't reproduce your issue. I used virtualenv and tf-nightly. \r\n\r\n(tensorflow_venv) haoliang@haoliang0:~/tensorflow_venv/bin$ python test.py \r\n2019-03-19 16:37:17.875562: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-19 16:37:17.905232: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3492135000 Hz\r\n2019-03-19 16:37:17.906497: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a21cc6c070 executing computations on platform Host. Devices:\r\n2019-03-19 16:37:17.906550: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-19 16:37:18.050999: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-03-19 16:37:18.051125: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-03-19 16:37:18.283008: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:666] Optimization results for grappler item: graph_to_optimize\r\n2019-03-19 16:37:18.283043: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   model_pruner: Graph size after: 236 nodes (-63), 253 edges (-63), time = 1.355ms.\r\n2019-03-19 16:37:18.283054: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   implementation_selector: Graph size after: 236 nodes (0), 253 edges (0), time = 0.397ms.\r\n2019-03-19 16:37:18.283071: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   function_optimizer: Graph size after: 236 nodes (0), 253 edges (0), time = 0.306ms.\r\n2019-03-19 16:37:18.283078: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   constant folding: Graph size after: 158 nodes (-78), 173 edges (-80), time = 25.045ms.\r\n2019-03-19 16:37:18.283084: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   shape_optimizer: Graph size after: 158 nodes (0), 173 edges (0), time = 0.505ms.\r\n2019-03-19 16:37:18.283090: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   arithmetic_optimizer: Graph size after: 152 nodes (-6), 175 edges (2), time = 100.996ms.\r\n2019-03-19 16:37:18.283096: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   loop_optimizer: Graph size after: 152 nodes (0), 175 edges (0), time = 0.661ms.\r\n2019-03-19 16:37:18.283102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   dependency_optimizer: Graph size after: 151 nodes (-1), 173 edges (-2), time = 1.329ms.\r\n2019-03-19 16:37:18.283110: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   layout: Graph size after: 151 nodes (0), 173 edges (0), time = 0.286ms.\r\n2019-03-19 16:37:18.283119: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   memory_optimizer: Graph size after: 151 nodes (0), 173 edges (0), time = 9.367ms.\r\n2019-03-19 16:37:18.283125: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   model_pruner: Graph size after: 151 nodes (0), 173 edges (0), time = 0.983ms.\r\n2019-03-19 16:37:18.283130: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   implementation_selector: Graph size after: 151 nodes (0), 173 edges (0), time = 0.257ms.\r\n2019-03-19 16:37:18.283135: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   function_optimizer: Graph size after: 151 nodes (0), 173 edges (0), time = 0.254ms.\r\n2019-03-19 16:37:18.283140: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   constant folding: Graph size after: 149 nodes (-2), 171 edges (-2), time = 12.494ms.\r\n2019-03-19 16:37:18.283146: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   shape_optimizer: Graph size after: 149 nodes (0), 171 edges (0), time = 0.503ms.\r\n2019-03-19 16:37:18.283152: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   arithmetic_optimizer: Graph size after: 149 nodes (0), 171 edges (0), time = 37.109ms.\r\n2019-03-19 16:37:18.283160: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   dependency_optimizer: Graph size after: 149 nodes (0), 171 edges (0), time = 1.423ms.\r\n\r\n\r\nCould you check what version of Tensorflow you are using?", "@haozha111 \r\nwill this snippet trigger the error?\r\n\r\n```python3\r\ngraph_def_file = \"frozen_model_yolov3-tiny.pb\"\r\ninput_arrays = [\"input_to_model\"]\r\noutput_arrays = [\"concat_1\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {input_arrays[0] : (0., 1.)}  # mean, std_dev\r\ntflite_model = converter.convert()\r\nopen(\"converted_tiny_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n", "I tried the same, unfortunately got exactly the same error.\r\n\r\nI'm using virtualenv and tf-nightly ('1.14.1-dev20190410')", "Thanks Mark. Now I can reproduce that exact same error. I think this will only fail when quantization is enabled.\r\n\r\nSee the comments here[1], it looks like range op currently doesn't support quantization. So the check will fail when the input is not int32 or float32. I will transfer this issue to Pulkit since he is planning to support quantization for range op.\r\n\r\n[1]https://github.com/tensorflow/tensorflow/blob/c238fc4a25c71c71e1bfdbb6eac750c64aed52ad/tensorflow/lite/kernels/range.cc#L89", "@gjraza did you solve this ?", "@nutsiepully @haozha111 , When will you support quantization for range op?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26586\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26586\">No</a>\n"]}, {"number": 26585, "title": "[Intel MKL]: Quantized Concat", "body": "This PR adds support for the case where inputs to Quantized Concat don't have same range. We use Eigen to handle this case. ", "comments": ["@penpornk Thank you for reviewing this. I did code refactoring as you suggested. Sorry I had to push a single new commit due to major changes. The PR now is more concise and hopefully easier to review. Thanks again for the review and the suggestions. ", "@penpornk Thanks again for reviewing this and sorry for the things I missed. I have addressed the remaining comments."]}, {"number": 26584, "title": "Tensorflow Lite Java Interpreter methods throw Exceptions, but this is not in the method declaration", "body": "Methods such as `Interpreter.run` throw Java exceptions originating from native, but the method declaration does not include `throws Exception`. It would be helpful to beginners to include this. Thanks!", "comments": ["I see that it this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\nPlease provide following info:\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "**System information**\r\n\r\nTensorFlow version (you are using): 1.13.1\r\nAre you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe current Tensorflow Lite Java API does not declare that `Interpreter.run` can throw exceptions. However, the underlying code does throw exceptions, often containing useful debug information. If a call to `Interpreter.run` is not wrapped in a try/catch block, any exception thrown will cause the thread to break silently (seen on Android), making this difficult to debug.\r\n\r\nThis feature request asks for `Interpreter.run` to correctly declare which exceptions it can possibly throw. This likely involves similar fixes in the `NativeInterpreterWrapper` class.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes, as mentioned above.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThis will affect code where calls to `Interpreter.run` have not been wrapped in a try/catch block. This will result in a compiler error, notifying the programmer that it is a good idea to add a try/catch block and log any error message. This can help them find future errors more easily.\r\n\r\nThe Android example in the Tensorflow Github repository does not use a try/catch block (and I consider that a *bad* idea).\r\n\r\nAny Other info.", "@lu-wang-g and @xunkai55 , would one of you mind taking a pass at improving the documentation here? Thanks!", "No problem. I'll take a look at that.", "@joepmoritz The public APIs in Interpreter.java have been documented with exceptions in a recent code change. None of those exceptions are checked exceptions, therefore they are not declared in the method declaration. Feel free to let us know if you have any comments. Thanks!"]}, {"number": 26583, "title": "[ROCm] Enable ROCm support for \"softsign_op\"", "body": "This PR enables ROCm support for the \"softsign_op\".\r\n\r\nPR #26457 is a pre-req for this PR, and hence this PR includes commits from that PR. \r\nOnly the last commit in this PR  should be reviewed here (as all others will be reviewed as part of PR #26457 )\r\n\r\nThe change for this PR is rather trivial :)\r\n\r\n------------------------\r\n@tatianashp @whchung : just FYI.\r\n", "comments": ["Hi @rthadur, I'm not familiar with this, would you help to find someone else?\r\nThanks. ", "> Hi @rthadur, I'm not familiar with this, would you help to find someone else?\r\n> Thanks.\r\nSure \r\n", "@tatianashp please advise the next step ASAP. I don\u2019t think I have permission to approve this PR as I\u2019m part of the contributor, but it seems no one at google would like to review this PR?", "Also @chsigg ", "@chsigg \r\n\r\nrebased this PR, since PR #26457 has been merged...now this PR is truly trivial :)", "rebased to remove merge conflicts", "@chsigg , this PR should be good to go...please approve.  thanks."]}, {"number": 26582, "title": "Free builtin_data when state_ == kStateInvokableAndImmutable", "body": "free builtin_data when state_ == kStateInvokableAndImmutable, memleak", "comments": []}, {"number": 26581, "title": "Wrapped tf.nn.RNNCell* layers are incompatible with tf.keras.layers.Bidirectional", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed: binary\r\n- TensorFlow version: `2.0.0.dev20190311`\r\n- Python version: 3.6.6\r\n\r\n**Describe the current behavior**\r\n\r\nIn the TensorFlow 2.0 preview, the `tf.nn.RNNCellDropoutWrapper` and `tf.nn.RNNCellResidualWrapper` wrappers are incompatible with `tf.keras.layers.Bidirectional`. It raises an \"Unknown layer\" exception.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe RNN classes in `tf.nn` should be compatible with all Keras RNN layers.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ncell = tf.keras.layers.LSTMCell(10)\r\ncell = tf.nn.RNNCellResidualWrapper(cell)\r\n\r\nrnn = tf.keras.layers.RNN(cell)\r\nrnn = tf.keras.layers.Bidirectional(rnn)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"test/residual_wrapper.py\", line 30, in <module>\r\n    rnn = tf.keras.layers.Bidirectional(rnn)\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 393, in __init__\r\n    self.backward_layer = layer.__class__.from_config(config)\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 920, in from_config\r\n    cell = deserialize_layer(config.pop('cell'), custom_objects=custom_objects)\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 95, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 181, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 166, in class_and_config_for_serialized_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\nValueError: Unknown layer: ResidualWrapperV2\r\n```\r\n\r\n@qlzh727 ", "comments": ["Thanks for reporting the issue, I will send some fix within this week.", "This is weird. Why `tf.nn.rnn_cell.*` is moved to `tf.keras`, but ResidualWrapper and DeviceWrapper are not (and they are inheriting new base classes such like `ResidualWrapperV2`)?", "Sorry for the very late reply. The issue was caused by the serial/deserialization for a non-keras object, and the nn.RNNCell wrappers are not keras object. The commit https://github.com/tensorflow/tensorflow/commit/0667725ba4e010c06fcf5e2a388eb8c2f3f6efe1 is trying to fix the custom object serialization issue, which allows you to do something like https://gist.github.com/fchollet/9e361d1fe9046b512a2f3e0d31382a96.\r\n\r\nWe are still discussing the issue internally and see how we will fix this issue.", "Sorry for the long wait, this should now by fixed by e62dc433fcce833313b4174e20fc24c418593d27", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26581\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26581\">No</a>\n"]}, {"number": 26580, "title": "custom layer ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing this issue due to lack of information. Please provide all the information asked by the template. Thanks!"]}, {"number": 26579, "title": "[1.13] Incomplete information in the warning message on using tf.contrib.predictor", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.13.1\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/predictor/from_saved_model https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/saved_model/load https://www.tensorflow.org/alpha/guide/keras/saving_and_serializing\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nOn using `tf.contrib.predictor.from_saved_model` to get predictions from a saved_model, the warning message says that there _will_ be a new function to load saved models in TensorFlow 2.0, and nothing else.\r\n\r\nSpecifically, consider the following snippet of code (from https://colab.research.google.com/github/suyash/transformer/blob/master/imdb_sentiment_demo.ipynb)\r\n\r\n```py\r\np = predictor.from_saved_model(\r\n    export_dir, \r\n    input_names={\"input_text\": \"input_text:0\"}, \r\n    output_names={\r\n        \"prediction\": \"dense/Softmax:0\",\r\n        \"attention_0\": \"attention/attention_weights:0\",\r\n        \"attention_1\": \"attention_1/attention_weights:0\",\r\n    },\r\n)\r\noutputs = p({\"input_text\": inputs})\r\nprint(outputs[\"prediction\"])\r\nprint(outputs[\"attention_0\"])\r\nprint(outputs[\"attention_1\"])\r\n```\r\n\r\nThis prints the following warning message\r\n\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\n```\r\n\r\nI have trying to figure out extracting multiple outputs from the same saved model. The current guide for saved models at alpha (https://www.tensorflow.org/alpha/guide/keras/saving_and_serializing) does not provide information on loading, while the equivalent for stable (https://www.tensorflow.org/guide/saved_model) does.\r\n\r\nI am trying to use the new `tf.saved_model.load` function. I have prepared a demo for my use-case at https://colab.research.google.com/gist/suyash/2c7e5b77ea4d94d5c3b8c3e178d06878. Specifically, I create a model with dense layers, convert it to an estimator, trained it while defining an `Exporter` for evaluation checkpoints.\r\n\r\nNow, when I load using the new function, I am able to obtain values for the `dense_2` tensor, however in an equivalent scenario, how do I get the values of the `dense_1` tensor in the current approach. As I show in my predictor example, the output of the predictor will have 3 keys, \"prediction\", \"attention_0\" and \"attention_1\". How do I get the output of the \"dense_1\" tensor in my 2.0 example?\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@allenlavoie Can you PTAL? Thanks!", "So basically you want to fetch an intermediate Tensor? The loaded signatures do have a .prune method which takes input and output Tensors from the function graph and gives you a new function (`signature_function.graph` will get to the Tensor objects).\r\n\r\nThe documentation for prune() should be improved; it needs a docstring and should probably take Tensor names instead of just Tensors. And we should add a pointer to the 2.x guide to the contrib predictor depreciation warning.", "@allenlavoie I am not seeing anything about a prune method [on the documentation page for tf.saved_model.load](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/saved_model/load), where can I find that. Is there also some place I can find equivalent 2.x code for fetching intermediate values from saved_models.", "@suyash I think this was resolved already. I see the prune method mentioned almost in the end of [this page](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/saved_model/load). \r\n\r\nI am closing this issue. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 26578, "title": "Pip No matching distribution found for tensorflow-gpu==2.0.0-alpha0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["pip install -U --pre tensorflow-gpu==2.0.0-alpha0\r\n\r\nThen I got these:\r\n\r\nLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\nCollecting tensorflow-gpu==2.0.0-alpha0\r\n  Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0-alpha0 (from versions: 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.12.0, 1.13.1)\r\nNo matching distribution found for tensorflow-gpu==2.0.0-alpha0", "When I encountered a similar sounding error, I realized my problem was that I had Python 3.7.2 (32-bit) installed.\r\n\r\nYou need to have installed Python 3.7.2 **(64-bit)** in order to execute `pip install tensorflow==2.0.0-alpha0`\r\n\r\nHope this helps.", "Thanks, I think I was running 64-bit version python37.\r\nHowever, I solved it by directly download and install .whl file from https://pypi.org/project/tensorflow-gpu/2.0.0a0/#files", "Closing this issue since its resolved. Feel free to reopen if have any further problems. Thanks!", "After\r\n```\r\npip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade tensorflow-gpu==2.0.0\r\n```\r\nI got\r\n```\r\nNo matching distribution found for tensorflow-gpu==2.0.0\r\n```\r\nand I can see `2.0.0a0, 2.0.0b0, 2.0.0b1` is available.\r\nMy Python3 version is 3.6.8.\r\n\r\nBecause some days ago, I've already install tensorflow(cpu) in my windows PC, and tf.__version__ is 2.0.0, and I want to install tensorflow-gpu in my linux server, if I choose to install tensorflow-gpu==2.0.0a0, it will show that tf.__version__ is 2.0.0-alpha0.\r\n\r\nI want to know if my pip3 instruction was wrong.\r\nThanks a lot.", "`python -m pip install --upgrade pip`.", "> `python -m pip install --upgrade pip`.\r\n\r\nThis did it for me; pip later was able to see tf 2.0.0. (FTR using python 3.6.8 on ubuntu 18.04.)", "Thanks a lot. Finally, I uninstalled my python 2.0, and use the similar instruction above(aims to upgrade my pip) to solve my problem.", "ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow-gpu==2.0.0\r\n\r\n\r\n\"any thing could be done for this error?\"", "> `python -m pip install --upgrade pip`.\r\n\r\n(https://github.com/tensorflow/tensorflow/issues/26578#issuecomment-543286872)\r\n\r\nPlease read previous replies. Please note that we are not yet releasing python 3.8 pip. Please note that we don't release pip for python interpreters running on 32 bits. Please note that `pip debug --verbose` gives you a list of tags of pips that can be installed and you can check on pypi if those tags are present for the package.\r\n\r\nIf none of the above help, please open a new issue, filling in the template", "#Win Python 3.6.0\r\npython -m pip install --upgrade tensorflow-gpu  ", "python -m pip install tensorflow==2.0", "Some people might have issue related to having a **mac** and using old version of `tensorflow` or `tensorflow-gpu` as found here https://github.com/tensorflow/tensorflow/issues/8251#issuecomment-617080152", "There is no GPU for mac"]}, {"number": 26577, "title": "Accuracy Metric automatically selected, fails in certain cases", "body": "**System information**\r\n- Uses a basic CNN MNIST Keras example\r\n- CentOS7\r\n- TensorFlow installed from: Anaconda\r\n- TensorFlow version: Both 1.12-gpu and 2.0.0-alpha0-cpu\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n* When using a \"built in\" loss function, the \"accuracy\" metric is **automatically** resolved to the \"correct one\".\r\n* When using a custom loss function that \"touches\" `y_pred` and `y_true` in any way (even trivially as seen below), \"accuracy\" is no longer automatically resolved.\r\n\r\n**Preamble:**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras import datasets, layers, models\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\r\n\r\ntrain_images = train_images.reshape((60000, 28, 28, 1))\r\ntest_images = test_images.reshape((10000, 28, 28, 1))\r\n\r\n# Normalize pixel values to be between 0 and 1\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n```\r\n\r\n**\"Working\" Example:**\r\n```\r\ndef customLoss1():\r\n    return tf.keras.losses.sparse_categorical_crossentropy\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\n\r\nmodel.compile(loss=customLoss1(), optimizer='adam', metrics=['accuracy',])\r\nmodel.fit(train_images, train_labels, epochs=1)\r\n```\r\n\r\n**\"Broken\" Example:**\r\n```\r\ndef customLoss2(y_true, y_pred):\r\n    return K.sparse_categorical_crossentropy(y_true, y_pred)\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\n\r\nmodel.compile(loss=customLoss2(), optimizer='adam', metrics=['accuracy',])\r\nmodel.fit(train_images, train_labels, epochs=1)\r\n```\r\n\r\n**Describe the expected behavior**\r\nI am not sure if this is intended, a bug, or a reasonable failure. \r\n\r\n**Broken Example Now Works:**\r\nIn order to get the \"Broken\" exmaple to \"work\" correctly, you have to specify the correct accuracy metric directly:\r\n\r\n```\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\n\r\nmodel.compile(loss=customLossWorks(), optimizer='adam', metrics=['sparse_categorical_accuracy'])\r\nmodel.fit(train_images, train_labels, epochs=1)\r\n```\r\n\r\n**Other info / logs**\r\n* This was originally posted under #26490 but @timudk helped clarify my problem so I reposted as a separate issue.", "comments": ["@diego898 Thank you for reporting the issue.\r\n\r\nWithout a built-in loss function, we cannot tell whether categorical or sparse categorical accuracy should be used in this case as the outputs will have the same shape for both the use cases and it is the labels that determine the correct function that needs to be used. \r\n\r\nAs at the time of `compile` we do not have enough information we cannot determine the correct metric accuracy function to be used in this case."]}, {"number": 26576, "title": "SparseTensor takes ages to initialize and can not be saved independently", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): sorta\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory: Titan \r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n\r\nIt takes a while for me to generate a `SparseTensor`\r\n\r\n\r\n```python\r\n# dense is an n x m matrix\r\n\r\nsparse = coo_matrix(dense) # almost instantaneous \r\n\r\n# for legibility\r\nsparse_indicies = list(zip(\r\n    sparse.row.astype(np.int64).tolist(), \r\n    sparse.col.astype(np.int64).tolist()\r\n)) # almost instantaneous\r\n\r\ntype_casted = (sparse.data).astype(np.float32) # almost instantaneous\r\n\r\n# takes ages (ok it takes several minutes...)\r\ninput_tensor = tf.SparseTensor(\r\n    indices     = sparse_indicies,\r\n    values      = type_casted,\r\n    dense_shape = sparse.shape\r\n) \r\n\r\n# save to file so I can load it to memory locally if it exists.\r\n\r\n```\r\n\r\nHow can I save it just by itself? I have tried pickle and npy without success.\r\n\r\n```python\r\nimport pickle, numpy as np\r\n\r\nfilename = os.path.expanduser('~/tmp/test.tmp')\r\n\r\nwith open(fn, 'wb') as f:\r\n    pickle.dump(input_tensor, f)\r\n    # throws \"TypeError: can't pickle _thread.RLock objects\"\r\n\r\n\r\nnp.save(fn, input_tensor)\r\n# throws \"TypeError: can't pickle _thread.RLock objects\"\r\n\r\n\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\neither SparseTensor should be more performant in initialization or should be saveable as a pickle object so I don't always have to re-initialize it\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n(see above)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["sparse.shape is modest in size (200,000 x 50,000)", "This is expensive probably because it's creating a protobuf representation of the constant sparse tensor values and storing that constant in the graph.\r\n\r\nIt depends on what you're trying to do.  for examle, you could create a tf.sparse_placeholder instead of a SparseTensor, serialize your indices values etc in pure python/numpy, and feed them to a tensorflow graph via the `session.run(..., feed_dict={my_sparse_placeholder: tf.SparseTensorValue(numpy_arrays)})`", "Hi @SumNeuron ! we are  checking to see if you are still looking for assistance in this issue.\r\nCould you please try on latest stable version of  TF 2.6  and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26576\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26576\">No</a>\n"]}, {"number": 26575, "title": "Lite: Split_V test cases improvised", "body": "1:> Test cases improvised to support 2 more datatypes.\r\n2:> Error message improvised.", "comments": []}, {"number": 26573, "title": "Tensorboard is not showing the last checkpoint's evaluation result", "body": "I trained some object detection models with custom data for 4K steps using TensorFlow Object Detection API, and evaluated them during the training. Evaluation is done for all checkpoints, I watched the results on the console.\r\n\r\nHowever, I can't see the last two checkpoints' evaluation results on Tensorboard for some raeson. It shows the evaluation result of 3K steps, and nothing after that. I can see that evaluation is completed on the console, and also in the folder.\r\n\r\nThere are no error messages on the console when I start Tensorboard. I can see that training results are uploaded completely to Tensorboard, the only missing thing is the last evaluation results.\r\n\r\nI tried evaluating the latest checkpoints again, but nothing changed. At the end of the evaluation I receive a message saying that metrics are recorded to summary...\r\n\r\nThe training checkpoints are saved in every 10 minutes, and evaluation takes 12 minutes. But even in this case, I expect the latest checkpoint's evaluation results to be there.\r\n\r\nWhen I try to download the csv file from Tensorboard I also can't see the last two checkpoints' evaluations.\r\n\r\nWhat could be the reason?\r\n\r\n```\r\nI0311 16:57:21.281645 MainThread program.py:165] Not bringing up TensorBoard, but inspecting event files.\r\nI0311 16:57:21.281645 140028330256128 program.py:165] Not bringing up TensorBoard, but inspecting event files.\r\n======================================================================\r\nProcessing event files... (this can take a few minutes)\r\n======================================================================\r\n\r\nFound event files in:\r\n./CN_flow1_95/eval\r\n./CN_flow1_95/train\r\n\r\nThese tags are in ./CN_flow1_95/eval:\r\naudio -\r\nhistograms -\r\nimages\r\n   image-0\r\n   image-1\r\n   image-2\r\n   image-3\r\n   image-4\r\n   image-5\r\n   image-6\r\n   image-7\r\n   image-8\r\n   image-9\r\nscalars\r\n   Losses/Loss/BoxClassifierLoss/classification_loss\r\n   Losses/Loss/BoxClassifierLoss/localization_loss\r\n   Losses/Loss/RPNLoss/localization_loss\r\n   Losses/Loss/RPNLoss/objectness_loss\r\n   PascalBoxes_PerformanceByCategory/AP@0.5IOU/b'cyclist'\r\n   PascalBoxes_PerformanceByCategory/AP@0.5IOU/b'motorcyclist'\r\n   PascalBoxes_PerformanceByCategory/AP@0.5IOU/b'pedestrian'\r\n   PascalBoxes_Precision/mAP@0.5IOU\r\ntensor -\r\n======================================================================\r\n\r\nEvent statistics for ./CN_flow1_95/eval:\r\naudio -\r\ngraph\r\n   first_step           0\r\n   last_step            0\r\n   max_step             0\r\n   min_step             0\r\n   num_steps            1\r\n   outoforder_steps     []\r\nhistograms -\r\nimages\r\n   first_step           0\r\n   last_step            4112\r\n   max_step             4112\r\n   min_step             0\r\n   num_steps            7\r\n   outoforder_steps     []\r\nscalars\r\n   first_step           0\r\n   last_step            4112\r\n   max_step             4112\r\n   min_step             0\r\n   num_steps            7\r\n   outoforder_steps     []\r\nsessionlog:checkpoint -\r\nsessionlog:start -\r\nsessionlog:stop -\r\ntensor -\r\n======================================================================\r\n\r\nThese tags are in ./CN_flow1_95/train:\r\naudio -\r\nhistograms\r\n   ModelVars/...\r\nimages -\r\nscalars\r\n   Losses/TotalLoss\r\n   Losses/clone_0/Loss/BoxClassifierLoss/classification_loss\r\n   Losses/clone_0/Loss/BoxClassifierLoss/localization_loss\r\n   Losses/clone_0/Loss/RPNLoss/localization_loss\r\n   Losses/clone_0/Loss/RPNLoss/objectness_loss\r\n   Losses/clone_1/Loss/BoxClassifierLoss/classification_loss\r\n   Losses/clone_1/Loss/BoxClassifierLoss/localization_loss\r\n   Losses/clone_1/Loss/RPNLoss/localization_loss\r\n   Losses/clone_1/Loss/RPNLoss/objectness_loss\r\n   Losses/clone_2/Loss/BoxClassifierLoss/classification_loss\r\n   Losses/clone_2/Loss/BoxClassifierLoss/localization_loss\r\n   Losses/clone_2/Loss/RPNLoss/localization_loss\r\n   Losses/clone_2/Loss/RPNLoss/objectness_loss\r\n   batch/fraction_of_150_full\r\n   clone_0/Losses/clone_0//clone_loss\r\n   global_step/sec\r\n   queue/prefetch_queue/fraction_of_5_full\r\ntensor -\r\n======================================================================\r\n\r\nEvent statistics for ./CN_flow1_95/train:\r\naudio -\r\ngraph\r\n   first_step           0\r\n   last_step            0\r\n   max_step             0\r\n   min_step             0\r\n   num_steps            1\r\n   outoforder_steps     []\r\nhistograms\r\n   first_step           0\r\n   last_step            4110\r\n   max_step             4110\r\n   min_step             0\r\n   num_steps            28\r\n   outoforder_steps     []\r\nimages -\r\nscalars\r\n   first_step           0\r\n   last_step            4110\r\n   max_step             4110\r\n   min_step             0\r\n   num_steps            54\r\n   outoforder_steps     []\r\nsessionlog:checkpoint\r\n   first_step           1\r\n   last_step            4111\r\n   max_step             4111\r\n   min_step             1\r\n   num_steps            7\r\n   outoforder_steps     []\r\nsessionlog:start\r\n   outoforder_steps     []\r\n   steps                [0, 4110]\r\nsessionlog:stop\r\n   outoforder_steps     []\r\n   steps                [0, 0]\r\ntensor -\r\n======================================================================\r\n\r\n\r\n```", "comments": ["This issue is more suitable on tensorboard repo. The folks maintaining tensorboard repo can address your query better. Please post this issue on [tensorboard repo](https://github.com/tensorflow/tensorboard/issues). Thanks!"]}, {"number": 26572, "title": "Cuda 10: Failed to get device properties, error code: 3", "body": "I am using [keras-retinanet](https://github.com/fizyr/keras-retinanet) (0.5.0) to perform object detection on images using its TensorFlow backend. When I `load_model` their pre-trained COCO model, the GPU is initialized as expected:\r\n```\r\nUsing TensorFlow backend.\r\nWARNING:tensorflow:From .../virtualenv/lib64/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-03-11 11:26:25.063507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-03-11 11:26:25.064403: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c2e09b7bb0 executing computations on platform CUDA. Devices:\r\n2019-03-11 11:26:25.064432: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-03-11 11:26:25.066831: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2993245000 Hz\r\n2019-03-11 11:26:25.067138: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c2e06ce580 executing computations on platform Host. Devices:\r\n2019-03-11 11:26:25.067163: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-11 11:26:25.068038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.77GiB\r\n2019-03-11 11:26:25.068063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-11 11:26:25.069446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-11 11:26:25.069467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0\r\n2019-03-11 11:26:25.069477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N\r\n2019-03-11 11:26:25.070214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n.../virtualenv/lib64/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\n  warnings.warn('No training configuration found in save file: '\r\n```\r\n\r\nHowever, when calling `predict_on_batch` on this model afterwards, the GPU cannot be initialized any more:\r\n```\r\n2019-03-11 11:26:52.699096: E tensorflow/core/grappler/clusters/utils.cc:83] Failed to get device properties, error code: 3\r\n2019-03-11 11:26:52.947157: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_2. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 \r\n2019-03-11 11:26:52.947225: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_3. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 \r\n2019-03-11 11:26:52.947245: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_4. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 \r\n2019-03-11 11:26:52.947271: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_5. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 \r\n2019-03-11 11:26:53.215964: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_2. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 \r\n2019-03-11 11:26:53.216024: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_3. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 \r\n2019-03-11 11:26:53.216042: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_4. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 \r\n2019-03-11 11:26:53.216068: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_5. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2 \r\n2019-03-11 11:26:53.354507: E tensorflow/stream_executor/cuda/cuda_driver.cc:1131] failed to enqueue async memcpy from host to device: CUDA_ERROR_NOT_INITIALIZED: initialization error; GPU dst: 0x7fc4f4b62900; host src: 0x7fc77b201a00; size: 4=0x4\r\n```\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no custom code, but TensorFlow is called through keras-retinanet\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo (Base System release 2.6)\r\n- TensorFlow installed from (source or binary): binary from PyPi\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: 10.0.130 / 7.4.2.24\r\n- GPU model and memory: Nvidia GeForce GTX 1080 Ti (11 GB memory)\r\n- Nvidia driver version: 418.43\r\n\r\n**Describe the current behavior**\r\nsee above\r\n\r\n**Describe the expected behavior**\r\nThere should be no error initializing the GPU when using `predict_on_batch`. On other machines I have used older versions (TF 1.10.0 with CUDA 9.0) and it worked fine.\r\n\r\n**Code to reproduce the issue**\r\n* Install CUDA 10, cuDNN 7.4.2\r\n* Install tensorflow-gpu 1.13.1 (or 1.13.0rc1/rc2) and keras-retinanet 0.5.0 into a virtualenv from PyPi\r\n* Download pre-trained COCO model from [here](https://github.com/fizyr/keras-retinanet/releases/download/0.5.0/resnet50_coco_best_v2.1.0.h5)\r\n* Load the model using `model = keras_retinanet.models.load_model('path/to/model.h5')` (will generate first blob of output above)\r\n* Predict bounding boxes on an image via `model.predict_on_batch(np.expand_dims(my_image, axis=0))` (will generate second blob of output above)\r\n\r\n**Other info / logs**\r\nI am reporting this here and not at the keras-retinanet project because it seems to be a TensorFlow internal issue. Furthermore, it has worked fine with the same version of keras-retinanet when using older versions of TF and CUDA (see above).\r\n\r\nThank you!", "comments": ["I'm experiencing similar issue with object detection model training: Failed to get device properties. Unable to determine the device handle for GPU 0000:01:00.0: GPU is lost.  Reboot the system to recover this GPU\r\nOS: notebook with win10\r\nTensorFlow version: tf-gpu 1.13.1\r\nPython version: 3.7.1\r\nCUDA/cuDNN version: 10.0.130 / 7.5.0\r\nGPU model and memory: Nvidia GeForce GTX 1060 (6 GB memory)", "I have the same issue while running  keras-retinanet after upgraded to cuda 10 t.f. 1.13.1 python 3.6.8", "Same here running my Tensorflow multiprocess code with cuda 10, TF 1.13.1, python 3.6.8", "I am facing the similar issues when loading model for object detection and using multiprocessing ", "i have the same issue, have you deal with it ?", "This is a stale issue. If this is still an issue, can you please share a simple standalone code to reproduce the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26572\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26572\">No</a>\n"]}, {"number": 26571, "title": "CPU int8 inference", "body": "Ubuntu 16.04\r\n- TensorFlow version (you are using):1.12\r\n- Are you willing to contribute it (Yes/No):No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** When will conv layer support **int8** in **cpu**\r\n\r\n**Will this change the current api? How?** Add int8 datatype\r\n\r\n**Who will benefit with this feature?** Inference time will be reduced in cnn models\r\n\r\n**Any Other info.**\r\n", "comments": ["I think the TF build wheel with mkldnn on the TF main page already support INT8 inference on CPU ", "According to the above comment seems it's fixed."]}, {"number": 26570, "title": "Added quantized division for uint8", "body": "Added simple, non-optimized implementation of quantized division for unsigned 8-bit integers to reference ops. Implemented a few tests together with templates that can be used for testing quantized Div for other data types as well.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26570) for more info**.\n\n<!-- need_sender_cla -->", "@mwtarnowski please sign CLA", "@rthadur Micha\u0142 and @w-adamski are members of https://github.com/TCLResearchEurope\r\nand this organization has already signed CLA. Does @bbiskupski (our CEO who has signed CLA) needs to do something so that they can send PRs?\r\n", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26570) for more info**.\n\n<!-- ok -->", "@bjacob, thanks for your feedback and a comprehensive explanation of the problem. In fact, I've already implemented integer-only version of this operation with gemmlowp library. However, during several benchmarks, I found that, even on mobile devices with limited support of floating-point operations, an unoptimized (no LUT, no NEON) integer-only implementation performs poorly in comparison to the approach I proposed here. For my purposes, this implementation provided demanded functionality with fair performance and being pretty simple at the same time, which made it a good candidate for a reference.\r\n\r\nBut indeed, when it comes to ensure maximum portability, it makes perfect sense to avoid using floats. So I am updating this PR eliminating all floating point operations from the code. Any further suggestions are welcome.", "@bjacob, @suharshs, do you know when this code will be reviewed? Is there anything I should fix/change?", "Sorry, I had forgotten.  This looks good to me.", "@bjacob what is the further procedure? PR has been accepted and what are further steps needed to merge it?", "@herbakamil The normal procedure is that this PR has been converted to a Google-internal \"change\" that has been entered into integration tests. Unfortunately, there are some real test failures, so this has been blocked at that stage there.  The test failures are in this test target:\r\n\r\n//tensorflow/lite/kernels:div_test\r\n\r\nYou should be able to reproduce this test failure by this command:\r\n\r\nbazel test //tensorflow/lite/kernels:div_test\r\n\r\nPlease update this PR so as to make this test pass, then we can let this go through integration again.\r\n\r\nThe other failures reported above here (\"Windows Bazel\") look like spurious/flaky failures not actually caused by your PR, at least from a cursory look.", "@bjacob, can you provide more details? I am not able to reproduce this test failure on Ubuntu 18.04 with Bazel 0.24.1 and GCC 7.4.", "(Edit:  this is on a Linux setup fairly similar to yours, debian-based.  the compiler is some recent clang).\r\n\r\nIt seems to be failing in the new tests which this PR adds:\r\n\r\n```\r\n[==========] Running 12 tests from 3 test suites.\r\n[----------] Global test environment set-up.\r\n[----------] 4 tests from FloatDivOpTest\r\n[ RUN      ] FloatDivOpTest.NoActivation\r\nINFO: Initialized TensorFlow Lite runtime.\r\n[       OK ] FloatDivOpTest.NoActivation (7 ms)\r\n[ RUN      ] FloatDivOpTest.ActivationRELU_N1_TO_1\r\n[       OK ] FloatDivOpTest.ActivationRELU_N1_TO_1 (2 ms)\r\n[ RUN      ] FloatDivOpTest.VariousInputShapes\r\n[       OK ] FloatDivOpTest.VariousInputShapes (2 ms)\r\n[ RUN      ] FloatDivOpTest.WithBroadcast\r\n[       OK ] FloatDivOpTest.WithBroadcast (2 ms)\r\n[----------] 4 tests from FloatDivOpTest (13 ms total)\r\n\r\n[----------] 4 tests from IntegerDivOpTest\r\n[ RUN      ] IntegerDivOpTest.NoActivation\r\n[       OK ] IntegerDivOpTest.NoActivation (2 ms)\r\n[ RUN      ] IntegerDivOpTest.ActivationRELU_N1_TO_1\r\n[       OK ] IntegerDivOpTest.ActivationRELU_N1_TO_1 (2 ms)\r\n[ RUN      ] IntegerDivOpTest.VariousInputShapes\r\n[       OK ] IntegerDivOpTest.VariousInputShapes (2 ms)\r\n[ RUN      ] IntegerDivOpTest.WithBroadcast\r\n[       OK ] IntegerDivOpTest.WithBroadcast (2 ms)\r\n[----------] 4 tests from IntegerDivOpTest (8 ms total)\r\n\r\n[----------] 4 tests from QuantizedDivOpTest\r\n[ RUN      ] QuantizedDivOpTest.QuantizedNoActivationUInt8\r\nthird_party/tensorflow/lite/kernels/div_test.cc:199: Failure\r\nValue of: m.GetDequantizedOutput<integer_dtype>()\r\nExpected: has 4 elements where\r\nelement #0 is approximately 1 (absolute error <= 0.015747791),\r\nelement #1 is approximately -0.5 (absolute error <= 0.015747791),\r\nelement #2 is approximately 0.375 (absolute error <= 0.015747791),\r\nelement #3 is approximately 0.69999999 (absolute error <= 0.015747791)\r\n  Actual: { -0.25098, 0.133333, 0.368627, 0.698039 }, whose element #0 doesn't match, which is -1.25098 from 1\r\nStack trace:\r\n0x7f7778d3b915: tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedNoActivationUInt8_Test::TestBody() @ ??:??\r\n0x7f7777ae4ead: testing::Test::Run() @ ??:??\r\n0x7f7777ae6794: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...\r\n\r\n[  FAILED  ] QuantizedDivOpTest.QuantizedNoActivationUInt8 (34 ms)\r\n[ RUN      ] QuantizedDivOpTest.QuantizedActivationRELU_N1_TO_1UInt8\r\nthird_party/tensorflow/lite/kernels/div_test.cc:225: Failure\r\nValue of: m.GetDequantizedOutput<integer_dtype>()\r\nExpected: has 4 elements where\r\nelement #0 is approximately -1 (absolute error <= 0.015747791),\r\nelement #1 is approximately 0.5 (absolute error <= 0.015747791),\r\nelement #2 is approximately 1 (absolute error <= 0.015747791),\r\nelement #3 is approximately -0.875 (absolute error <= 0.015747791)\r\n  Actual: { 0.337255, 0.509804, 0.996078, -0.870588 }, whose element #0 doesn't match, which is 1.33725 from -1\r\nWith test number 0\r\nStack trace:\r\n0x7f7778d3c766: tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedActivationRELU_N1_TO_1UInt8_Test::TestBody() @ ??:??\r\n0x7f7777ae4ead: testing::Test::Run() @ ??:??\r\n0x7f7777ae6794: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...\r\n\r\n[  FAILED  ] QuantizedDivOpTest.QuantizedActivationRELU_N1_TO_1UInt8 (3 ms)\r\n[ RUN      ] QuantizedDivOpTest.QuantizedVariousInputShapesUInt8\r\nthird_party/tensorflow/lite/kernels/div_test.cc:252: Failure\r\nValue of: m.GetDequantizedOutput<integer_dtype>()\r\nExpected: has 6 elements where\r\nelement #0 is approximately -1.538 (absolute error <= 0.047612458),\r\nelement #1 is approximately 0.667 (absolute error <= 0.047612458),\r\nelement #2 is approximately 1.545 (absolute error <= 0.047612458),\r\nelement #3 is approximately 2.25 (absolute error <= 0.047612458),\r\nelement #4 is approximately -0.36399999 (absolute error <= 0.047612458),\r\nelement #5 is approximately 1.053 (absolute error <= 0.047612458)\r\n  Actual: { 0.776471, 0.682353, 1.52941, 2.23529, -0.352941, 1.05882 }, whose element #0 doesn't match, which is 2.31447 from -1.538\r\nWith shape number 0\r\nStack trace:\r\n0x7f7778d3d34d: tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedVariousInputShapesUInt8_Test::TestBody() @ ??:??\r\n0x7f7777ae4ead: testing::Test::Run() @ ??:??\r\n0x7f7777ae6794: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...\r\n\r\nthird_party/tensorflow/lite/kernels/div_test.cc:252: Failure\r\nValue of: m.GetDequantizedOutput<integer_dtype>()\r\nExpected: has 6 elements where\r\nelement #0 is approximately -1.538 (absolute error <= 0.047612458),\r\nelement #1 is approximately 0.667 (absolute error <= 0.047612458),\r\nelement #2 is approximately 1.545 (absolute error <= 0.047612458),\r\nelement #3 is approximately 2.25 (absolute error <= 0.047612458),\r\nelement #4 is approximately -0.36399999 (absolute error <= 0.047612458),\r\nelement #5 is approximately 1.053 (absolute error <= 0.047612458)\r\n  Actual: { 0.776471, 0.682353, 1.52941, 2.23529, -0.352941, 1.05882 }, whose element #0 doesn't match, which is 2.31447 from -1.538\r\nWith shape number 1\r\nStack trace:\r\n0x7f7778d3d34d: tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedVariousInputShapesUInt8_Test::TestBody() @ ??:??\r\n0x7f7777ae4ead: testing::Test::Run() @ ??:??\r\n0x7f7777ae6794: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...\r\n\r\nthird_party/tensorflow/lite/kernels/div_test.cc:252: Failure\r\nValue of: m.GetDequantizedOutput<integer_dtype>()\r\nExpected: has 6 elements where\r\nelement #0 is approximately -1.538 (absolute error <= 0.047612458),\r\nelement #1 is approximately 0.667 (absolute error <= 0.047612458),\r\nelement #2 is approximately 1.545 (absolute error <= 0.047612458),\r\nelement #3 is approximately 2.25 (absolute error <= 0.047612458),\r\nelement #4 is approximately -0.36399999 (absolute error <= 0.047612458),\r\nelement #5 is approximately 1.053 (absolute error <= 0.047612458)\r\n  Actual: { 0.776471, 0.682353, 1.52941, 2.23529, -0.352941, 1.05882 }, whose element #0 doesn't match, which is 2.31447 from -1.538\r\nWith shape number 2\r\nStack trace:\r\n0x7f7778d3d34d: tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedVariousInputShapesUInt8_Test::TestBody() @ ??:??\r\n0x7f7777ae4ead: testing::Test::Run() @ ??:??\r\n0x7f7777ae6794: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...\r\n\r\nthird_party/tensorflow/lite/kernels/div_test.cc:252: Failure\r\nValue of: m.GetDequantizedOutput<integer_dtype>()\r\nExpected: has 6 elements where\r\nelement #0 is approximately -1.538 (absolute error <= 0.047612458),\r\nelement #1 is approximately 0.667 (absolute error <= 0.047612458),\r\nelement #2 is approximately 1.545 (absolute error <= 0.047612458),\r\nelement #3 is approximately 2.25 (absolute error <= 0.047612458),\r\nelement #4 is approximately -0.36399999 (absolute error <= 0.047612458),\r\nelement #5 is approximately 1.053 (absolute error <= 0.047612458)\r\n  Actual: { 0.776471, 0.682353, 1.52941, 2.23529, -0.352941, 1.05882 }, whose element #0 doesn't match, which is 2.31447 from -1.538\r\nWith shape number 3\r\nStack trace:\r\n0x7f7778d3d34d: tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedVariousInputShapesUInt8_Test::TestBody() @ ??:??\r\n0x7f7777ae4ead: testing::Test::Run() @ ??:??\r\n0x7f7777ae6794: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...\r\n\r\n[  FAILED  ] QuantizedDivOpTest.QuantizedVariousInputShapesUInt8 (3 ms)\r\n[ RUN      ] QuantizedDivOpTest.QuantizedWithBroadcastUInt8\r\nthird_party/tensorflow/lite/kernels/div_test.cc:277: Failure\r\nValue of: m.GetDequantizedOutput<integer_dtype>()\r\nExpected: has 6 elements where\r\nelement #0 is approximately -2.8570001 (absolute error <= 0.047612458),\r\nelement #1 is approximately 0.28600001 (absolute error <= 0.047612458),\r\nelement #2 is approximately 1 (absolute error <= 0.047612458),\r\nelement #3 is approximately 1.143 (absolute error <= 0.047612458),\r\nelement #4 is approximately -0.71399999 (absolute error <= 0.047612458),\r\nelement #5 is approximately 1.571 (absolute error <= 0.047612458)\r\n  Actual: { 1.43529, 0.305882, 0.988235, 1.12941, 0.376471, 1.57647 }, whose element #0 doesn't match, which is 4.29229 from -2.857\r\nWith shape number 0\r\nStack trace:\r\n0x7f7778d3dd8d: tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedWithBroadcastUInt8_Test::TestBody() @ ??:??\r\n0x7f7777ae4ead: testing::Test::Run() @ ??:??\r\n0x7f7777ae6794: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...\r\n\r\nthird_party/tensorflow/lite/kernels/div_test.cc:277: Failure\r\nValue of: m.GetDequantizedOutput<integer_dtype>()\r\nExpected: has 6 elements where\r\nelement #0 is approximately -2.8570001 (absolute error <= 0.047612458),\r\nelement #1 is approximately 0.28600001 (absolute error <= 0.047612458),\r\nelement #2 is approximately 1 (absolute error <= 0.047612458),\r\nelement #3 is approximately 1.143 (absolute error <= 0.047612458),\r\nelement #4 is approximately -0.71399999 (absolute error <= 0.047612458),\r\nelement #5 is approximately 1.571 (absolute error <= 0.047612458)\r\n  Actual: { 1.43529, 0.305882, 0.988235, 1.12941, 0.376471, 1.57647 }, whose element #0 doesn't match, which is 4.29229 from -2.857\r\nWith shape number 1\r\nStack trace:\r\n0x7f7778d3dd8d: tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedWithBroadcastUInt8_Test::TestBody() @ ??:??\r\n0x7f7777ae4ead: testing::Test::Run() @ ??:??\r\n0x7f7777ae6794: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...\r\n\r\nthird_party/tensorflow/lite/kernels/div_test.cc:277: Failure\r\nValue of: m.GetDequantizedOutput<integer_dtype>()\r\nExpected: has 6 elements where\r\nelement #0 is approximately -2.8570001 (absolute error <= 0.047612458),\r\nelement #1 is approximately 0.28600001 (absolute error <= 0.047612458),\r\nelement #2 is approximately 1 (absolute error <= 0.047612458),\r\nelement #3 is approximately 1.143 (absolute error <= 0.047612458),\r\nelement #4 is approximately -0.71399999 (absolute error <= 0.047612458),\r\nelement #5 is approximately 1.571 (absolute error <= 0.047612458)\r\n  Actual: { 1.43529, 0.305882, 0.988235, 1.12941, 0.376471, 1.57647 }, whose element #0 doesn't match, which is 4.29229 from -2.857\r\nWith shape number 2\r\nStack trace:\r\n0x7f7778d3dd8d: tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedWithBroadcastUInt8_Test::TestBody() @ ??:??\r\n0x7f7777ae4ead: testing::Test::Run() @ ??:??\r\n0x7f7777ae6794: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...\r\n\r\nthird_party/tensorflow/lite/kernels/div_test.cc:277: Failure\r\nValue of: m.GetDequantizedOutput<integer_dtype>()\r\nExpected: has 6 elements where\r\nelement #0 is approximately -2.8570001 (absolute error <= 0.047612458),\r\nelement #1 is approximately 0.28600001 (absolute error <= 0.047612458),\r\nelement #2 is approximately 1 (absolute error <= 0.047612458),\r\nelement #3 is approximately 1.143 (absolute error <= 0.047612458),\r\nelement #4 is approximately -0.71399999 (absolute error <= 0.047612458),\r\nelement #5 is approximately 1.571 (absolute error <= 0.047612458)\r\n  Actual: { 1.43529, 0.305882, 0.988235, 1.12941, 0.376471, 1.57647 }, whose element #0 doesn't match, which is 4.29229 from -2.857\r\nWith shape number 3\r\nStack trace:\r\n0x7f7778d3dd8d: tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedWithBroadcastUInt8_Test::TestBody() @ ??:??\r\n0x7f7777ae4ead: testing::Test::Run() @ ??:??\r\n0x7f7777ae6794: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...\r\n\r\n[  FAILED  ] QuantizedDivOpTest.QuantizedWithBroadcastUInt8 (3 ms)\r\n[----------] 4 tests from QuantizedDivOpTest (43 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 12 tests from 3 test suites ran. (64 ms total)\r\n[  PASSED  ] 8 tests.\r\n[  FAILED  ] 4 tests, listed below:\r\n[  FAILED  ] QuantizedDivOpTest.QuantizedNoActivationUInt8\r\n[  FAILED  ] QuantizedDivOpTest.QuantizedActivationRELU_N1_TO_1UInt8\r\n[  FAILED  ] QuantizedDivOpTest.QuantizedVariousInputShapesUInt8\r\n[  FAILED  ] QuantizedDivOpTest.QuantizedWithBroadcastUInt8\r\n\r\n 4 FAILED TESTS\r\n```", "In the first of these failures,\r\n\r\n```\r\nelement #0 is approximately 1 (absolute error <= 0.015747791),\r\nelement #1 is approximately -0.5 (absolute error <= 0.015747791),\r\nelement #2 is approximately 0.375 (absolute error <= 0.015747791),\r\nelement #3 is approximately 0.69999999 (absolute error <= 0.015747791)\r\n  Actual: { -0.25098, 0.133333, 0.368627, 0.698039 }\r\n```\r\n\r\nNotice how it's the 2 first out of these 4 values that are off, while the 2 last values are OK (within the stated tolerance).", "@bjacob, is now everything ok?", "@mwtarnowski I just inquired again.  It's OK as far as I can see, someone needs to submit it into integration again."]}, {"number": 26569, "title": "Can you please add opportunity to use \"MobileNet\" model with more labels, pls", "body": "private List<Map<String, Object>> detectObjectOnBinary(HashMap args) throws IOException {\r\n    byte[] binary = (byte[])args.get(\"binary\");\r\n    String model = args.get(\"model\").toString();\r\n    double threshold = (double)args.get(\"threshold\");\r\n    float THRESHOLD = (float)threshold;\r\n    List<Double> ANCHORS = (ArrayList)args.get(\"anchors\");\r\n    int BLOCK_SIZE = (int)args.get(\"blockSize\");\r\n    int NUM_BOXES_PER_BLOCK = (int)args.get(\"numBoxesPerBlock\");\r\n    int NUM_RESULTS_PER_CLASS = (int)args.get(\"numResultsPerClass\");\r\n\r\n    ByteBuffer imgData = ByteBuffer.wrap(binary);\r\n\r\n    if (model.equals(\"SSDMobileNet\")) {\r\n      return parseSSDMobileNet(imgData, NUM_RESULTS_PER_CLASS, THRESHOLD);\r\n    } else {\r\n      return parseYOLO(imgData, BLOCK_SIZE, NUM_BOXES_PER_BLOCK, ANCHORS, THRESHOLD, NUM_RESULTS_PER_CLASS);\r\n    }\r\n  }\r\n\r\nsorry, no link, old browser", "comments": []}, {"number": 26568, "title": "added noisy relu activation function", "body": "I added the noisy relu activation function as described over here:\r\nhttps://en.wikipedia.org/wiki/Rectifier_(neural_networks)\r\n\r\nPlease review. @rthadur ", "comments": ["@tanzhenyu please review.", "Please add to addon. This is our new place to consolidate APIs before moving to tf core.", "Let me know if you would like to have code review for add-ons.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]