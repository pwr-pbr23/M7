[{"number": 21516, "title": "Build failes '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1): gcc failed: error executing command", "body": "I have an error when I try to build tensorflow with Bazel.\r\npython3 -V\r\nPython 3.7.0\r\n\r\ngcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper\r\n\r\n c++ -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=c++\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper\r\n\r\nbazel version\r\nBuild label: 0.16.0- (@non-git)\r\n\r\ncat /etc/redhat-release\r\nRed Hat Enterprise Linux Server release 7.5 (Maipo)\r\n\r\nComand\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures \r\n\r\n\r\nERROR: /root/.cache/bazel/_bazel_root/2fbd50cf126faad144af4acc96cb7595/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1): gcc failed: error executing command\r\n\r\n\r\nCan you help me please?\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform and Distribution\r\n**Red Hat Enterprise Linux Server release 7.5 (Maipo)**\r\n\r\nTensorFlow installed from\r\n**Source**\r\nTensorFlow version\r\n**tensorflow-1.9.0**\r\nCUDA/cuDNN version\r\nN/A\r\nGPU model and memory\r\nN/A\r\nExact command to reproduce\r\n**bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures**\r\nMobile device\r\nN/A", "Nagging Assignee @shivaniag: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I have fixed the issue using the below tolls versions:\r\n![image](https://user-images.githubusercontent.com/7471085/45383803-39adf580-b5da-11e8-9615-35ecc28ab936.png)\r\n\r\nregards", "I had same question,Do you have good sulotion?"]}, {"number": 21515, "title": "BUG: optimizer.compute_gradients() produces inconsistent gradient with the same training instance and label ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu Server & Window 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 & v1.9.0-0-g25c197e023\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: V9.0.178\r\n- **GPU model and memory**: NVIDIA Tesla V100-SXM2-16GB & NVIDIA GeForce GTX 1080 Ti 11GB\r\n- **Exact command to reproduce**:\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n#########################################################################\r\n## Define Tensorflow Wrapper\r\nprint('Define Tensorflow Wrapper......')\r\ndef weight_variable(shape):\r\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n    initial = tf.constant(0.2, shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n    initial = tf.constant(0.1, shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef initialize_variable(shape, name):\r\n    with tf.name_scope(name + '_layer'):\r\n        with tf.name_scope('weights'):\r\n            weights = weight_variable(shape)\r\n        with tf.name_scope('biases'):\r\n            biases = bias_variable([1,shape[3],1,1])\r\n        return weights, biases\r\n\r\ndef conv2dLayer(inputs, weights, biases, name, act=tf.nn.relu):\r\n    preactivate = tf.nn.conv2d(inputs, weights, strides=[1,1,1,1], padding='VALID', data_format=\"NCHW\", dilations=[1,1,1,1], name='conv2d') + biases\r\n    return act(preactivate, name='activation')\r\n\r\ndef max_pool_3x3(x, name):\r\n    return tf.nn.max_pool(x, ksize=[1, 1, 3, 3], strides=[1, 1, 3, 3], padding='VALID', data_format=\"NCHW\", name=name)\r\n    \r\n#########################################################################\r\n## Create Neural Network\r\nprint('Create Neural Network......')\r\n# Initial Trainable Parameters\r\nConv1W,Conv1b = initialize_variable([3,3,1,2],'Conv1')\r\nConv2W,Conv2b = initialize_variable([3,3,2,2],'Conv2')\r\nConv3W,Conv3b = initialize_variable([3,3,2,2],'Conv3')\r\nConv4W,Conv4b = initialize_variable([3,3,2,2],'Conv4')\r\nfc1W,fc1b = initialize_variable([39,14,2,372],'fc1')\r\nfc2W,fc2b = initialize_variable([1,1,372,372],'fc2')\r\nfc3W,fc3b = initialize_variable([1,1,372,1],'fc3')\r\n# Placeholder for input and output\r\nx = tf.placeholder(tf.float32, [None,1,372,None], name='xInput')\r\ny_ = tf.placeholder(tf.float32, [None,1,1,None], name='yInput')\r\n# Architecture\r\nConv1 = conv2dLayer(x,Conv1W,Conv1b,'Conv1')\r\nConv2 = conv2dLayer(Conv1,Conv2W,Conv2b,'Conv2')\r\nMaxPool1 = max_pool_3x3(Conv2,'MaxPool1')\r\nConv3 = conv2dLayer(MaxPool1,Conv3W,Conv3b,'Conv3')\r\nConv4 = conv2dLayer(Conv3,Conv4W,Conv4b,'Conv4')\r\nMaxPool2 = max_pool_3x3(Conv4,'MaxPool2')\r\nfc1 = conv2dLayer(MaxPool2,fc1W,fc1b,'fc1')\r\nfc2 = conv2dLayer(fc1,fc2W,fc2b,'fc2')\r\ny = conv2dLayer(fc2,fc3W,fc3b,'fc3',act=tf.identity)\r\n# Define loss\r\ncrossEntropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y, name='crossEntropy')\r\nlossValue = tf.reduce_mean(crossEntropy, name='lossValue')\r\n# Define optimizer\r\n#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.001,rho=0.95,epsilon=1e-08,use_locking=False,name='Adadelta')\r\n#optimizer = tf.train.AdagradOptimizer(learning_rate=0.01,initial_accumulator_value=0.1,use_locking=False,name='Adagrad')\r\n#optimizer = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\r\n#optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.95, name='nestrov', use_nesterov=True)\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01, name='GradientDescent')\r\ngvs = optimizer.compute_gradients(lossValue)\r\ntrain_step = optimizer.apply_gradients(gvs)\r\n\r\n#########################################################################\r\n## Illustrate compute_gradients() random issue\r\nprint('Load x and y input......')\r\ndata = np.load('testGrad.npz')\r\nxInput = data['x']\r\nyInput = data['y']\r\n\r\nprint(\"Session 1\")\r\nwith tf.Session() as sess1:\r\n    sess1.run(tf.global_variables_initializer())\r\n    lossValue_eval1, gvs_eval1 = sess1.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\r\n    sess1.run(train_step, feed_dict={x:xInput,y_:yInput})\r\n    lossValue1_eval1, gvs1_eval1, = sess1.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\r\n    sess1.close()\r\n\r\nprint(\"Session 2\")\r\nwith tf.Session() as sess2:\r\n    sess2.run(tf.global_variables_initializer())\r\n    lossValue_eval2, gvs_eval2 = sess2.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\r\n    sess2.run(train_step, feed_dict={x:xInput,y_:yInput})\r\n    lossValue1_eval2, gvs1_eval2, = sess2.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\r\n    sess2.close()\r\n\r\nprint('--------------------------------------------------')\r\nprint('Check whether they have the same values in each session')\r\nprint('Before Training: ')\r\nprint('lossValue? %r' % all(lossValue_eval1==lossValue_eval2))\r\ncheck_gvs_BT = np.zeros((14,2))\r\nfor i in range(14):\r\n    for j in range(2):\r\n        check_gvs_BT[i][j] = all(gvs_eval1[i][j] == gvs_eval2[i][j])\r\nprint('gvs? %r' % all(check_gvs_BT))\r\nprint('After Training: ')\r\nprint('lossValue? %r' % all(lossValue1_eval1==lossValue1_eval2))\r\ncheck_gvs_AT = np.zeros((14,2))\r\nfor i in range(14):\r\n    for j in range(2):\r\n        check_gvs_AT[i][j] = all(gvs1_eval1[i][j] == gvs1_eval2[i][j])\r\nprint('gvs? %r' % all(check_gvs_AT))\r\n\r\ntf.reset_default_graph()\r\n```\r\n\r\n### Describe the problem\r\nBy executing the above code with the fixed training instance and label (which you can download from https://www.dropbox.com/s/828v71z4tm399z2/testGrad.npz?dl=0 ), we can see that the gradient value computed by compute_gradients(),  gvs_eval1[0][0][0,0,0,0] & gvs_eval2[0][0][0,0,0,0] are not always the same in each session. This happens regardless which optimizer you use.\r\n\r\nCorrect me if I have made some stupid mistake in this code. As I have tried simple network architecture (for example, training y = weight * x), and compute_gradients() seems to be working fine. \r\n\r\n", "comments": ["How different are they? GPU computations are nondeterministic and so floating point errors can be different between runs.", "Opps... my fault, as I never experience nondeterministic before, and I thought randomness is different from nondeterministic.....\r\nI have checked the CPU computation, there is no different(error) between each run. \r\n\r\nNevertheless, I have changed the print code as follows, so that we know how severe this nondeterministic issue is. \r\n\r\n```\r\nprint('--------------------------------------------------')\r\nprint('Check whether they have the same values in each session')\r\nprint('Number of variables equal in each session? %r' % (len(vars1) == len(vars2)))\r\nprint('Number of variables? %d' % len(vars1))\r\nprint('Before Training: ')\r\nprint('lossValue? %r' % (lossValue_eval1==lossValue_eval2))\r\ncheck_gvs_BT = np.zeros((len(vars1),2))\r\nfor i in range(len(vars1)):\r\n    for j in range(2):\r\n        check_gvs_BT[i][j] = (np.sum(np.abs(gvs_eval1[i][j] - gvs_eval2[i][j])))/(gvs_eval1[i][j].shape[0]*gvs_eval1[i][j].shape[1]*gvs_eval1[i][j].shape[2]*gvs_eval1[i][j].shape[3])\r\nprint('gvs? %r' % (np.sum(check_gvs_BT)==0))\r\nprint('gvs error sum? %.16f' % np.sum(check_gvs_BT))\r\nprint('gvs error average? %.16f' % (np.sum(check_gvs_BT)/(len(vars1)*2)))\r\nprint('After Training: ')\r\nprint('lossValue? %r' % (lossValue1_eval1==lossValue1_eval2))\r\ncheck_gvs_AT = np.zeros((14,2))\r\nfor i in range(len(vars2)):\r\n    for j in range(2):\r\n        check_gvs_AT[i][j] = (np.sum(np.abs(gvs1_eval1[i][j] - gvs1_eval2[i][j])))/(gvs1_eval1[i][j].shape[0]*gvs1_eval1[i][j].shape[1]*gvs1_eval1[i][j].shape[2]*gvs1_eval1[i][j].shape[3])\r\nprint('gvs? %r' % (np.sum(check_gvs_AT)==0))\r\nprint('gvs error sum? %.16f' % np.sum(check_gvs_AT))\r\nprint('gvs error average? %.16f' % (np.sum(check_gvs_AT)/(len(vars1)*2)))\r\n```\r\nYou may experience the following result with GPU\r\nNVIDIA Tesla V100-SXM2-16GB\r\nBefore Training\r\n(1st run)                                         (2nd run)                           (3rd run)\r\nError Sum: 0.5033042114695341\t0.5967776937724014\t0.3815979222670251\r\nError Ave: 0.0179751504096262\t0.0213134890633001\t0.0136284972238223\r\nAfter Training\r\n(1st run)                                         (2nd run)                           (3rd run)\r\nError Sum: 0.0042671012194780\t0.0052051065843201 \t0.0050405413446461\r\nError Sum: 0.0001523964721242\t0.0001858966630114 \t0.0001800193337374\r\n\r\nNVIDIA GeForce GTX 1080 Ti 11GB\r\nBefore Training\r\n(1st run)                                         (2nd run)                           (3rd run)\r\nError Sum: 0.2072482638888889\t0.2625868055555556 \t 0.2703993055555556\t\r\nError Ave: 0.0074017237103175\t0.0093781001984127\t 0.0096571180555556\r\nAfter Training\r\n(1st run)                                         (2nd run)                           (3rd run)\r\nError Sum: 0.0030042860243056\t0.0021870930989583 \t 0.0028618706597222\r\nError Sum: 0.0001072959294395\t0.0000781104678199 \t 0.0001022096664187", "OK I hadn't realized you are reinitializing the weights to new random values for each run correct? I think this is probably the expected behavior.", "? FYI, based on my code, I did NOT randomizes the initial value of the weights. I make them all constant with tf.constant().\r\n\r\nBecause of the non-deterministic nature of GPU ops, now I know it is the tf.nn.conv2d() cause optimizer.compute_gradients() to compute random gradients. \r\n\r\nBy the way, is it possible that, in the tensorflow doc, to list out which tensorflow API/ops are\r\n1. non-deterministic/deterministic\r\n2. going to have an option for choosing non-deterministic/deterministic cuDNN algo\r\n(it seems you guys are working on that :)   really appreciate your hard work :)\r\nhttps://github.com/tensorflow/tensorflow/issues/12871\r\nhttps://github.com/tensorflow/tensorflow/pull/10636 )", "Yes sorry I didn't read your code properly.\r\n\r\n@rmlarsen do you think it's feasible to maintain a list of deterministic ops in TF?", "@EdwardLin2014,\r\nI guess **`Determinism`** in **`Tensorflow GPU`** is possible now using [tensorflow-determinism](https://pypi.org/project/tensorflow-determinism/). Can you PTAL. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21515\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21515\">No</a>\n"]}, {"number": 21514, "title": "Fix linking _dataset_ops with VS 2017", "body": "When building TensorFlow with VS 2017, I'm getting the following linking error:\r\n```\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.13.26128/bin/HostX64/x64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/contrib/data/_dataset_ops.so-2.params /DEF:bazel-out/x64_windows-opt/bin/tensorflow/contrib/data/_dataset_ops.so.def /ignore:4070\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/contrib/data/_dataset_ops.so.if.lib and object bazel-out/x64_windows-opt/bin/tensorflow/contrib/data/_dataset_ops.so.if.exp\r\nprefetching_kernels.lo.lib(prefetching_kernels.obj) : error LNK2019: unresolved external symbol \"public: __cdecl google::protobuf::internal::LogMessage::~LogMessage(void)\" (??1LogMessage@internal@protobuf@google@@QEAA@XZ) referenced in function \"public: void __cdecl google::protobuf::Map<class std::basic_string<char,struct std::char_traits<char\r\n>,class std::allocator<char> >,class tensorflow::AttrValue>::insert<class google::protobuf::Map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::AttrValue>::const_iterator>(class google::protobuf::Map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,\r\nclass tensorflow::AttrValue>::const_iterator,class google::protobuf::Map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::AttrValue>::const_iterator)\" (??$insert@Vconst_iterator@?$Map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VAttrValue@tensorflow@@@protobuf@google@@@?\r\n$Map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VAttrValue@tensorflow@@@protobuf@google@@QEAAXVconst_iterator@012@0@Z)\r\nbazel-out/x64_windows-opt/bin/tensorflow/contrib/data/_dataset_ops.so : fatal error LNK1120: 1 unresolved externals\r\n```\r\n\r\nThis CL fixes it by exporting the necessary symbol. It's strange it didn't fail with VS 2015.\r\n@gunan ", "comments": []}, {"number": 21513, "title": "Keras callbacks: TensorBoard class does not have \"embeddings\" parameters.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: Pip (PyPI) \r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nHi, I am using keras for my training and I wish to incorporate tensorboard's embedding visualization features. However, when I am trying to set the parameters on the TensorBoard's callback, it report that:\r\nTypeError: __init__() got an unexpected keyword argument 'embeddings_freq'\r\n\r\nI have then decided to take a look at the github source code for callbacks.py (/tensorflow/tensorflow/python/keras/callbacks.py)\r\n\r\nFrom there I can see that the initialization of \"Tensorboard\" class does have the embeddings parameters. \r\n\r\nI then go through the \"callbacks.py\" in my site-packages and what I found out is that the callbacks.py is different from the one on github (I am currently running on tensorflow 1.10). \r\n\r\nAs a way to double confirm my finding is true, I then proceed to PyPI to download the wheel file. I then proceed to extract them and check the \"callbacks.py\" and it seems like there aren't any embeddings parameters for \"TensorBoard\" class as well!\r\n![image](https://user-images.githubusercontent.com/40532196/43889045-beeff878-9bf5-11e8-9031-74e0752dfefa.png)\r\n\r\nAs shown in the capture above, the embeddings parameters do exist on the comments; however it is nowhere to be found in the rest of the code file (as evident in the bottom window where \"embeddings_freq\" only have 1 match in the comment)\r\n\r\nIf this isn't bug feel free to enlighten me and delete this issue. Thank you!\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I believe this feature has been added in TensorBoard now.\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\r\nThanks!"]}, {"number": 21512, "title": "Update custom_training with tf.contrib.eager.Variable", "body": "Replacing tf.Variable with tf.contrib.eager.Variable to avoid error: RuntimeError: tf.Variable not supported when eager execution is enabled. Please use tf.contrib.eager.Variable instead. As a result, the notebook can run without error.\r\n\r\n \"tf.contrib.eager.Variable\" is needed under eager execution is enabled.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the PR, but this change should not be needed. The notebooks in the master branch are meant to be compatible with the code on theaster branch, and `tf.Variable` works just fine there. \r\n\r\nThe notebooks in the release branches are in sync with the release libraries.", "(For reference, `tf.Variable` was made to work in https://github.com/tensorflow/tensorflow/commit/9cc29a75ce8131db67b48e92dac3c16a255b92ed , which isn't included in the 1.10 release, and hence the notebook in the 1.10 release branch continues to use `tfe.Variable` - https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/eager/python/examples/notebooks/custom_training.ipynb)", "@asimshankar \r\nThanks for the reference. Didn't notice that."]}, {"number": 21511, "title": "Hi i  run the flowing code", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi @muneebullahkhan please reopen another issue with the template filled out. Thanks."]}, {"number": 21510, "title": "tf.image.hsv_to_rgb cannot seem to compute gradient", "body": "### Describe the problem \r\nI found tf.image.hsv_to_rgb cannot seem to compute gradient. However, In principle, the hsv2rgb operation just a linear transformation. Ummm, I am not sure that whether I misunderstand the tf.image.hsv_to_rgb op, otherwise, this is a feature request. \r\n\r\n### Source code / logs \r\n\r\n    new_image_hsv = tf.random_normal(shape=[32,32,3]) \r\n    new_image_rgb = tf.image.hsv_to_rgb(new_image_hsv, name='hsv2rgb') \r\n    hsv2rgb = tf.get_default_graph().get_operation_by_name('hsv2rgb') \r\n    print('hsv2rgb: ', get_gradient_function(hsv2rgb)) #hsv2rgb: None", "comments": ["I got it!  the transformation from HSV to RGB color space is not differentiable."]}, {"number": 21509, "title": "Calculate feature_importances for BoostedTreesRegressor and BoostedTreesClassifier", "body": "Fix #21204.\r\n\r\n~~The PR has not been done yet~~, and we create it early to collect feedback.\r\n\r\nI have some questions about design:\r\n1. The PR is to add a method `compute_feature_importances` for estimator. I think it's convenient for user, however we have to construct graph and restore metadata from checkpoint. Does the solution sound reasonable?\r\n2. If yes, for `_read_tree_ensemble_from_checkpoint` method in the PR, is it necessary to replace `Session` by `MonitoredSession` with `ChiefSessionCreator`?\r\n3. Is it good to add feature_num attribute to TreeEnsemble protobuf file?\r\n4. As for unit test, because it is easier to construct TreeEnsemble than estimator.  How about only adding test cases for function \r\n `def compute_feature_importances(tree_ensemble, num_features, normalize=True): ` next? I mean, like `ModelFnTests`.\r\n\r\ncc @nataliaponomareva @martinwicke @dvdbisong", "comments": ["1) I am wondering if a better way would be just output a file with feature importances when the model is exported (this is the same way it is done in contrib). So you could define an export strategy and have a flag on its method that allows you to export feature importances too.  Mustafa - what do you think\r\n2) Why do u need feature_num in the proto? In estimator (boosted_trees.py), you have  _calculate_num_features that can return to you the number", "+@ispirmustafa", "feature importance seems very similar to predict. It's better to have an op which we can run.\r\nalso from ux perspective the return dictionary should be explainable based on inputs. For example how does this implementation handle if user have numeric vector feature, indicator_column, ...? Is the connection clear between returned feature importance dictionary and given feature-columns?", "@nataliaponomareva Thanks for your comments.\r\n\r\n1. I think there are at least three solutions:  \r\n   + add `calculate_feature_importances` method as shown in the PR. We need to construct graph and session by ourselves (yes, like `predict` method, thank @ispirmustafa)\r\n   + add `with_feature_importances` argument for `train` method, and print feature importances when training is finished. I think we can create a subclass of SessionHook and implement its `end` method.\r\n   + write a file when exporting (the same way it is done in contrib). I have no idea of how to implement it? Hack `export_savedmodel` of Estimator?\r\n  \r\n   @dvdbisong as user, which one of them is convenient for you? \r\n   @ispirmustafa How do you think?\r\n\r\n@ispirmustafa \r\n\r\n> feature importance seems very similar to predict.\r\n\r\nYes, the only difference is that feature importance cannot invoke `_call_model_fn` due to lacking of input_fn.\r\n\r\n> dictionary should be explainable based on inputs.\r\n\r\nGood question. It sounds great if we can connect feature importance dictionary with given feature-columns directly. I will look into it and submit proposal later. Thanks.", "Global Feature importances are **not** similar to predict - they don't depend on the data you feed at this point, they dependend only on already trained model\r\n\r\nAmong your proposed solutions, I like the option 2 the least (printing the feature importances). It might be hard to go through the logs and look where it printed, and imagine you didn't save the log but have an already a trained model.\r\nHowever the hook might work - we might have a param given to an estimator, that when set, adds a hook that writes the file with feature importances somewhere in model dir. This is akin to writing importances in export strategy, but without having to create a custom strategy\r\nCalculate feature importances method might work too, but i am slightly leaning towards a hook that will write a file for later examination\r\nAlso, regarding feature importances vs feature columns - we can use the the method akin _calculate_num_features to get back the name of the feature based on feature id. For categorical columns, we will need to output info about feature column and feature value, but it is all there", "+@crawles", "IMO, the calculate feature importances method would be best option from a user's point of view. That's how scikit, xgboost do it and what would familiar to user. Then they can plot them within a notebook. Other option of writing to file in model_dir would also work, just requires user to have model_dir!=None and to read in the saved file", "> @nataliaponomareva  Also, regarding feature importances vs feature columns - we can use the the method akin _calculate_num_features to get back the name of the feature based on feature id. For categorical columns, we will need to output info about feature column and feature value, but it is all there\r\n\r\nGood idea. I add `_generate_feature_name_for_index` method in 10af761 to get back the name, could you take a look?\r\n\r\n> @crawles the calculate feature importances method would be best option from a user's point of view. That's how scikit, xgboost do it and what would familiar to user.\r\n\r\nThanks for your explanation. I also prefer to choose option 1.  How do you think, @nataliaponomareva  @ispirmustafa, @dvdbisong?  ", "@nataliaponomareva Hi, I think all comments have been resolved. Could you take a look again? Thanks.", "Thanks for your help, @nataliaponomareva . Could you take a look again?", "Very close!", "@nataliaponomareva Thanks. I have removed those unnecessary tests :-)", "I actually like it. This way we keep around sorted features, num of\nfeatures etc. It will help later too.\n\nOn Wed, Aug 22, 2018 at 3:17 PM ispirmustafa <notifications@github.com>\nwrote:\n\n> *@ispirmustafa* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/estimator/canned/boosted_trees.py\n> <https://github.com/tensorflow/tensorflow/pull/21509#discussion_r212075219>\n> :\n>\n> >  @estimator_export('estimator.BoostedTreesClassifier')\n> -class BoostedTreesClassifier(estimator.Estimator):\n> +class BoostedTreesClassifier(_BoostedTrees):\n>\n> can we dissolve the inheritance here? You can convert the\n> experimental_feature_importances to a helper function and call it from all\n> other boosted tree estimators.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21509#pullrequestreview-148630550>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEHQFShMiVMyG5NvLogUzwW4oj4Vck19ks5uTa5lgaJpZM4V1KmZ>\n> .\n>\n", "I am fine with the test, it is part of the importances method output and it\nis tested there. Mustafa, it is LGTM from me\n\nOn Thu, Aug 23, 2018 at 6:34 AM Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> *@facaiy* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/estimator/canned/boosted_trees_test.py\n> <https://github.com/tensorflow/tensorflow/pull/21509#discussion_r212259352>\n> :\n>\n> > +        tree_weights: 1.0\n> +        \"\"\"\n> +    self._create_fake_checkpoint_with_tree_ensemble_proto(\n> +        est, tree_ensemble_text)\n> +\n> +    feature_names_expected = ['categorical_indicator:ok',\n> +                              'continuous_bucketized_indicator:(-2.0, 0.5)',\n> +                              'continuous_bucketized_indicator:(-inf, -2.0)',\n> +                              'categorical_indicator:bad',\n> +                              # Reverse order because feature importances\n> +                              # are sorted by np.argsort(f)[::-1]\n> +                              'continuous_bucketized_indicator:(12.0, inf)',\n> +                              'continuous_bucketized_indicator:(0.5, 12.0)',\n> +                              'continuous_bucketized',\n> +                              'categorical_indicator:good']\n> +\n>\n> @ispirmustafa <https://github.com/ispirmustafa> @nataliaponomareva\n> <https://github.com/nataliaponomareva> This test case is created for\n> _generate_feature_name_mapping\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21509#pullrequestreview-148849502>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEHQFen5t2fTtz6AkkCDJc7kfTZ_ZVt-ks5uToUjgaJpZM4V1KmZ>\n> .\n>\n", "@ispirmustafa Thanks for your correction. `testFeatureImportancesNamesForUnsupportedColumn` is added for unsupported feature column. Could you take a look?", "Hi, I have resolved the merge conflict with latest upstream master.", "Thank you @nataliaponomareva @ispirmustafa ", "Hi, any update for me?", "I thought it was merged already, let me check what's going on", "It does not pass the golden api test\r\nCan you run \r\nbazel run //tensorflow/tools/api/tests/api_compatibility_test \\\r\n        --update_goldens True\r\nit should update a file (u will need to add it to PR)", "@nataliaponomareva Sure, thanks for your feedback.\r\n\r\n@yifeif Hi, yifei. It seems that `api_compatibility_test` only updates v1 golden file automatically. We have to update v2 files by ourselves, right?", "Those failures are unrelated?", "@nataliaponomareva Hi, softmax is good idea to me. However, I cannot find a numerical stable softmax implementation. The original `e^(x_i)/sum_over_j e^(x_j)` is easy to overflow, and I don't want to make it very complex.\r\n\r\nSo I just move the assertion line into `if normalize` block, and leave others unchanged. Namely, we require feature_importance must be non-negative only if using normalization. How do you think?", "That's fine by me", "Last time, sanity check reported an `unused import` error when merging with latest master branch. I have fixed them, thanks for reminding me. ", "Could you please update/merge?", "Sure, I have resolved the conflict with upstream. And I think those test failures are unrelated.", "Hi, any update? ", "Working on it :)", "Thank you for contributing facaiy!!!", "many thanks, @nataliaponomareva! "]}, {"number": 21508, "title": "Try to find an allocator when the engine is not assigned a device.", "body": "This PR will fix #21487.\r\n+cc @pooyadavoodi.", "comments": ["Friendly ping @pooyadavoodi.", "The PR looks good to me. Thanks @aaroey "]}, {"number": 21507, "title": "[Intel MKL] Tweaking a URL that didn't work with command line utilities.", "body": "Also fixed the URL attached to the stable build badge.", "comments": []}, {"number": 21506, "title": "export_tflite_ssd_graph_lib.py didn't work with 1.10.0-rc1 ?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux  4.17.11-arch1\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None of them\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:b'v1.9.0-rc2-1981-gb674f63ab1' 1.10.0-rc1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.15.2- (@non-git)\r\n- **GCC/Compiler version (if compiling from source)**: gcc-7\r\n- **CUDA/cuDNN version**: cuda-9.2 cudnn-7.1\r\n- **GPU model and memory**: 24G\r\n- **Exact command to reproduce**: \r\n\r\n**Everything works fine before I updated the newest source code ,compiled and installed it**.\r\nWith the installed version(1.10.0-rc1) the **export_tflite_ssd_graph_lib.py**  said \r\n\r\n`\r\n2018-08-09 03:18:34.547212: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying strip_unused_nodes\r\n2018-08-09 03:18:34.565931: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying fold_constants\r\nTraceback (most recent call last):\r\n  File \"export_tflite_ssd_graph.py\", line 137, in <module>\r\n    tf.app.run(main)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"export_tflite_ssd_graph.py\", line 133, in main\r\n    FLAGS.max_classes_per_detection)\r\n  File \"/home/mae/tf-newest-nightly/tensorflow/tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py\", line 280, in export_tflite_graph\r\n    nms_score_threshold, nms_iou_threshold, num_classes, scale_values)\r\n  File \"/home/mae/tf-newest-nightly/tensorflow/tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py\", line 137, in append_postprocessing_op\r\n    output_names, transforms)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/tools/graph_transforms/__init__.py\", line 51, in TransformGraph\r\n    transforms_string, status)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TFLite_Detection_PostProcess' in binary running on Mae. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n`\r\n\r\nOther scripts such as train and eval can work except the exporter.", "comments": ["I found If i remove the 'fold_constants' in **TransformGraph** processing,it would be OK. Thanks.", "hello, how did you remove the 'fold_constants'?     I face the same problem and can not find  \"fold_constants\". "]}, {"number": 21505, "title": "[TFLite] Is there any plan to support \"ExtractImagePatches\" and \"Elu\" ?", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Samsung Galaxy)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.10\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: 1080ti\r\n- **Exact command to reproduce**:\r\n\r\n\r\n\r\n### Describe the problem\r\nIs there any plan to support \"ExtractImagePatches\" and \"Elu\" in TFLite ?\r\n\r\n### Source code / logs\r\nN/A\r\n", "comments": ["/CC @petewarden", "There are no immediate plans to support either of those. \"Elu\" is one of the \r\n [select TF ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md) supported in tflite_convert, but \"ExtractImagePatches\" is not in the list.", "Since we don't have plans to support these, closing for now."]}, {"number": 21504, "title": "Linking of rule '//tensorflow/core/grappler/costs:analytical_cost_estimator_test' failed", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10.0-rc1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.16.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: bazel build --tool_tag=ijwb:CLion --compilation_mode=dbg --copt=-O0 --copt=-g --strip=never --dynamic_mode=off --curses=no --color=yes --experimental_ui=no --progress_in_terminal_title=no --test_filter=AnalyticalCostEstimatorTest.SimpleTest:AnalyticalCostEstimatorTest/*.SimpleTest:*/AnalyticalCostEstimatorTest.SimpleTest/*:*/AnalyticalCostEstimatorTest/*.SimpleTest --config=monolithic --build_event_binary_file=/tmp/intellij-bep-991f7840-b7e9-4fc1-8f46-b72fcee6be72 --nobuild_event_binary_file_path_conversion -- //tensorflow/core/grappler/costs:analytical_cost_estimator_test\r\n\r\n\r\nI'm trying to run [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/analytical_cost_estimator_test.cc) tensorflow test in debug mode using Bazel on CLion IDE. And, I get this error message:\r\n\r\nERROR: /home/name/tensorflow/tensorflow/core/grappler/costs/BUILD:317:1: Linking of rule '//tensorflow/core/grappler/costs:analytical_cost_estimator_test' failed (Exit 1).\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/core/grappler/costs:analytical_cost_estimator_test failed to build\r\n\r\nThe command that is used is:\r\n\r\nbazel build --tool_tag=ijwb:CLion --compilation_mode=dbg --copt=-O0 --copt=-g --strip=never --dynamic_mode=off --curses=no --color=yes --experimental_ui=no --progress_in_terminal_title=no --test_filter=AnalyticalCostEstimatorTest.SimpleTest:AnalyticalCostEstimatorTest/*.SimpleTest:*/AnalyticalCostEstimatorTest.SimpleTest/*:*/AnalyticalCostEstimatorTest/*.SimpleTest --config=monolithic --build_event_binary_file=/tmp/intellij-bep-991f7840-b7e9-4fc1-8f46-b72fcee6be72 --nobuild_event_binary_file_path_conversion -- //tensorflow/core/grappler/costs:analytical_cost_estimator_test\r\n\r\nSame command with --verbose_failures added:\r\n\r\n (cd /home/name/.cache/bazel/_bazel_name/0dcafb4e3b790050f9e05bb5277a4ba3/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/name/bin:/home/name/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda-8.0/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /usr/bin/gcc -o bazel-out/k8-dbg/bin/tensorflow/core/grappler/costs/analytical_cost_estimator_test '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..,-rpath,$ORIGIN/../../..' -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,@bazel-out/k8-dbg/bin/tensorflow/core/grappler/costs/analytical_cost_estimator_test-2.params).", "comments": ["@pvn25 \r\nwhich version of tensorflow and clion you used, can teach me the configuration process import tensorflow source code .    After installing the plugin, import the source directly? Still need to run other steps?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21504\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21504\">No</a>\n"]}, {"number": 21503, "title": "Installing tensorflow-serving-api 1.9.1 makes the package of tensorflow unuseable", "body": "### System information\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.1.0 (clang-902.0.39.1)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.2\r\nprotobuf                           3.5.2.post1\r\ntensorflow                         1.8.0\r\ntensorflow-hub                     0.1.0\r\ntensorflow-model-analysis          0.6.0\r\ntensorflow-serving-api             1.8.0\r\ntensorflow-tensorboard             1.5.0\r\ntensorflow-transform               0.6.0\r\ntensorflowjs                       0.1.0\r\ntensorflowonspark                  1.0.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 106: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.1.0 (clang-902.0.39.1)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.2\r\nprotobuf                           3.5.2.post1\r\ntensorflow                         1.8.0\r\ntensorflow-hub                     0.1.0\r\ntensorflow-model-analysis          0.6.0\r\ntensorflow-serving-api             1.8.0\r\ntensorflow-tensorboard             1.5.0\r\ntensorflow-transform               0.6.0\r\ntensorflowjs                       0.1.0\r\ntensorflowonspark                  1.0.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 106: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\n\r\nAfter installing the package `tensorflow-serving-api`, the package `tensorflow` works abnormally.\r\n\r\nFirstly, we can run the script when TensorFlow installed to print the version.\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n```\r\n\r\nThen we install `tensorflow-serving-api` with `pip` and try to run the same script. It throws the exception like this.\r\n\r\n```\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n```\r\n\r\nIf we uninstall `tensorflow-serving-api` and reinstall `tensorflow`, everything works normally.\r\n\r\n\r\n### Source code / logs\r\n\r\n<img width=\"1280\" alt=\"screen shot 2018-08-09 at 11 41 10 am\" src=\"https://user-images.githubusercontent.com/2715000/43878299-e97209e4-9bd0-11e8-877b-949b85c80fa1.png\">\r\n", "comments": ["I have tried and it works with `pip install tensorflow-serving-api==1.9.0` and doesn't work with `pip install tensorflow-serving-api==1.9.1`.\r\n\r\nIt seems that the package breaks from `1.9.1`.", "Nagging Assignee @rohan100jain: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "hmm looks like tensorflow-serving-api comes with TF 1.6. When i just download tensorflow-serving-api and do import tensorflow, it shows version 1.6. I'm guessing thats maybe causing the confusion. \r\n\r\nI think most of the tensorflow-serving-api use cases include cases where you don't really want whole of tensorflow. So the idea is that you just install the serving-api package and then just run models / serve it.", "So what is the solution of this issue? Why it was closed? I have met the same issue here   \r\nhttps://github.com/tensorflow/tensorflow/issues/31576"]}, {"number": 21502, "title": "tf.train.Saver() will save both the checkpoint and meta-graph", "body": "I found that, `tf.train.Saver()` will save both the `checkpoint` and `meta-graph`.\r\nHowever in many tensorflow tutorial, it adopt the following usage:\r\n```py\r\nsaver = tf.train.Saver()\r\nsaver.save(sess, 'my-save-dir/my-model-10000')    # will generate my-model-10000.meta\r\nsaver.export_meta_graph('my-save-dir/my-model-10000.meta')  # not need\r\n```\r\n\r\n\r\n# the following case\r\n\r\nhttps://www.tensorflow.org/api_guides/python/meta_graph#Import_a_MetaGraph\r\n\r\n```py\r\nimport tensorflow as tf\r\nimport math\r\n# Creates an inference graph.\r\n# Hidden 1\r\nimages = tf.constant(1.2, tf.float32, shape=[100, 28])\r\nwith tf.name_scope(\"hidden1\"):\r\n  weights = tf.Variable(\r\n      tf.truncated_normal([28, 128],\r\n                          stddev=1.0 / math.sqrt(float(28))),\r\n      name=\"weights\")\r\n  biases = tf.Variable(tf.zeros([128]),\r\n                       name=\"biases\")\r\n  hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\r\n# Hidden 2\r\nwith tf.name_scope(\"hidden2\"):\r\n  weights = tf.Variable(\r\n      tf.truncated_normal([128, 32],\r\n                          stddev=1.0 / math.sqrt(float(128))),\r\n      name=\"weights\")\r\n  biases = tf.Variable(tf.zeros([32]),\r\n                       name=\"biases\")\r\n  hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\r\n# Linear\r\nwith tf.name_scope(\"softmax_linear\"):\r\n  weights = tf.Variable(\r\n      tf.truncated_normal([32, 10],\r\n                          stddev=1.0 / math.sqrt(float(32))),\r\n      name=\"weights\")\r\n  biases = tf.Variable(tf.zeros([10]),\r\n                       name=\"biases\")\r\n  logits = tf.matmul(hidden2, weights) + biases\r\n  tf.add_to_collection(\"logits\", logits)\r\n\r\ninit_all_op = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n  # Initializes all the variables.\r\n  sess.run(init_all_op)\r\n  # Runs to logit.\r\n  sess.run(logits)\r\n  # Creates a saver.\r\n  saver0 = tf.train.Saver()\r\n  saver0.save(sess, 'my-save-dir/my-model-10000')            # it save both checkpoints and meta-graph\r\n  # Generates MetaGraphDef.\r\n  saver0.export_meta_graph('my-save-dir/my-model-10000.meta')  # you can comment this line\r\n```\r\n\r\n# env\r\n```yml\r\ntensorflow=1.8.0\r\n```\r\n\r\n# background\r\n\r\nI read the tensorflow docs, and search in stackoverflow.\r\nI just find similar question, but no proper answer, like this one\r\n\r\nhttps://stackoverflow.com/questions/45208587/relationship-between-tensorflow-saver-exporter-and-", "comments": ["You're right. For save method, its argument `write_meta_graph` is True by default, see API [ tf.train.Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver#save):\r\n\r\n```python\r\nsave(\r\n    sess,\r\n    save_path,\r\n    global_step=None,\r\n    latest_filename=None,\r\n    meta_graph_suffix='meta',\r\n    write_meta_graph=True,\r\n    write_state=True,\r\n    strip_default_attrs=False\r\n)\r\n```", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "what is the difference between `my-model-10000.meta` and `graph.pb`?\r\nI export both `my-model-10000.meta` and `graph.pb` in txt format, I found they are similar.\r\nBoth contains `MetaInfoDef`\u3001`GraphDef`\u3001`SaverDef`\u3001`CollectionDef`", "I think this question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf I understand correctly, [GraphDef](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/core/framework/graph.proto) is a member of [MetaGraphDef](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/core/protobuf/meta_graph.proto).", "You see, there are a lot of questions about tensorflow in stackoverflow, **not solved**.\r\n\r\nYou are right, GraphDef is a member of MetaGraphDef. what I am concern is:\r\n-  the difference between `my-model-10000.meta` and `graph.pb`?\r\n- the difference between `export_meta_graph` and `write_graph`?  \r\n    - their content is different slightly\r\n    - freeze graph not work in `export_meta_graph`\r\n\r\n All the above are MetaGraphDef.\r\n"]}, {"number": 21501, "title": "How to encrypt the tflite file\uff1f", "body": "Now  I have my own tflite file\uff0cand I am worried that others will copy my file. I want to encrypt my tflite file\uff0cso even if others have my file\uff0cthey cannot use it. \r\nIs there a way to encrypt the file?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@aselle @andrehentz @petewarden this seems like a feature request, unless it's something that Lite already does, or it's something that's naturally handled elsewhere. ", "This is something that is available in most host operating systems. Look for AES encryption. Maybe https://stackoverflow.com/questions/6788018/android-encryption-decryption-with-aes\r\nOf course encryption is not a panacea as you need to store your key somewhere. You might look at say https://developer.android.com/training/articles/security-key-attestation\r\n\r\n", "@aselle  Thank you for the answer. I'll colse it", "But when I try to open the tflite model in bytes array the tflite interpreter can not read it.", "@mabdullahrafique I am facing the same problem. Did you find any solution for the same? Please help me with that.\r\n\r\nThanks in advance.", "The tflite file in the assets folder is visible to any third party and anybody can extract the proprietary tflite model, this creates a problem in creating a commercial application esp. when the option to host the model on a server is not available."]}, {"number": 21500, "title": "Memory error with tensorflow GPU", "body": "I have NVIDIA GPU Titan V installed on my remote machine and I am trying to do simple multi class lassification using CNN\r\n\r\n\r\n`train_file = './data/csv_data.csv.zip'`\r\n `x_raw, y_raw, df, labels = data_helper.load_data_and_labels(train_file)`\r\n\r\n\r\nhere my csv_data zip file is 65MB .\r\n\r\n`max_document_length = max([len(x.split(' ')) for x in x_raw])`\r\n`logging.info('The maximum length of all sentences: {}'.format(max_document_length))`\r\n`vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)`\r\n`x = np.array(list(vocab_processor.fit_transform(x_raw)))`\r\n`y = np.array(y_raw)`\r\n\r\n\r\nI am getting memory error at \r\n`vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)`\r\n\r\nI think it is trying to load all the data in memory which is obviously a bad practice. How can I optimize this code to load it in the memory efficiently.\r\n\r\nCan anyone please point me to a detailed example or tutorial? \r\n\r\nThanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code  **Yes**\r\nOS Platform and Distribution  **Ubuntu 16.04**\r\nTensorFlow installed from   **https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl**\r\n\r\nTensorFlow version  **1.9**\r\nBazel version \r\nCUDA/cuDNN version   **i think 7**\r\nGPU model and memory **NVIDIA Titan V 12288MB**\r\nExact command to reproduce **https://github.com/pathakrohit08/EatItRepository/blob/master/tf.py**\r\nMobile device  **No**", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21499, "title": "CUDA and CUDNN paths for debian", "body": "This pull request addresses issues encountered when trying to build tensorflow with cuda support on debian based systems (stretch and buster)\r\n\r\nThe current tensorflow build process does not work with the debian maintained cuda packages. This is because there are some differences in the paths where the libraries are installed.\r\n\r\nSummary of changes:\r\n- Update to configure.py to search `<cuda_root>/lib/x86_64-gnu-linux`  for libcudart in addition to `<cuda_root>/lib64`\r\n- Update to bazel configure script to find cupti header and library\r\n- Update to bazel configure script to find NVVM device files", "comments": ["Unrelated failure:\r\n\r\n```\r\nout/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/applications/__init__.py\", line 21, in <module>\r\n    import keras_applications\r\nImportError: No module named 'keras_applications'\r\n```", "Been long waiting for this fix so I could avoid manually linking the CUDA libraries into the place where TF tries to find them. Hooray! "]}, {"number": 21498, "title": "bayesflow", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please add more information and reopen a new issue with all the above fields filled out."]}, {"number": 21497, "title": "[Intel MKL] Adding support for python 3.6 dev containers ", "body": "@angersson I'll move this to the new Dockerfile architecture, but need it here as well until the new architecture gets merged.", "comments": []}, {"number": 21496, "title": "Updating the bazel version in install from sources.", "body": "@MarkDaoust please update this to the docs for 1.10 whenever you can. Thanks!", "comments": []}, {"number": 21495, "title": "Merge r1.10 back to master.", "body": "", "comments": ["Unrelated failures."]}, {"number": 21494, "title": "support IndexedSlices in `add_n`", "body": "Fixes #15943", "comments": ["(deleted to remove clutter -- see @ebrevdo comment)", "The relevant lines:\n\nFAIL: Found 2 non-whitelited pylint errors:\ntensorflow/python/ops/math_ops.py:2120: [C0301(line-too-long), ] Line\ntoo long (81/80)\ntensorflow/python/ops/math_ops.py:2124: [C0301(line-too-long), ] Line\ntoo long (81/80)\n\n\nOn Thu, Aug 9, 2018, 6:31 PM drpngx <notifications@github.com> wrote:\n\n> ound a whitelisted error:\n>   tensorflow/contrib/eager/python/evaluator.py:209: [E0202(method-hidden), Evaluator.call] An attribute defined in evaluator line 61 hides this method\n> Found a whitelisted error:\n>   tensorflow/contrib/eager/python/evaluator.py:361: [E0202(method-hidden), SparseSoftmaxEvaluator.call] An attribute defined in evaluator line 61 hides this method\n> Found a whitelisted error:\n>   tensorflow/contrib/eager/python/metrics_impl.py:179: [E0202(method-hidden), Metric.build] An attribute defined in metrics_impl line 121 hides this method\n> Found a whitelisted error:\n>   tensorflow/contrib/eager/python/metrics_impl.py:192: [E0202(method-hidden), Metric.call] An attribute defined in metrics_impl line 131 hides this method\n> Found a whitelisted error:\n>   tensorflow/contrib/eager/python/metrics_impl.py:301: [E0202(method-hidden), Mean.build] An attribute defined in metrics_impl line 121 hides this method\n> Found a whitelisted error:\n>   tensorflow/contrib/eager/python/metrics_impl.py:312: [E0202(method-hidden), Mean.call] An attribute defined in metrics_impl line 131 hides this method\n> Found a whitelisted error:\n>   tensorflow/contrib/eager/python/metrics_impl.py:356: [E0202(method-hidden), Accuracy.call] An attribute defined in metrics_impl line 131 hides this method\n> Found a whitelisted error:\n>   tensorflow/contrib/eager/python/metrics_impl.py:401: [E0202(method-hidden), CategoricalAccuracy.call] An attribute defined in metrics_impl line 131 hides this method\n> Found a whitelisted error:\n>   tensorflow/contrib/eager/python/metrics_impl.py:454: [E0202(method-hidden), BinaryAccuracy.call] An attribute defined in metrics_impl line 131 hides this method\n> Found a whitelisted error:\n>   tensorflow/contrib/eager/python/metrics_impl.py:498: [E0202(method-hidden), SparseAccuracy.call] An attribute defined in metrics_impl line 131 hides this method\n> Found a whitelisted error:\n>   tensorflow/python/kernel_tests/constant_op_eager_test.py:250: [E0303(invalid-length-returned), ConstantTest.testInvalidLength.BadList.__len__] __len__ does not return non-negative integer\n> Found a whitelisted error:\n>   tensorflow/python/feature_column/feature_column_test.py:3125: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_invalid_type] Abstract class '_TestFeatureColumn' with abstract methods instantiated\n> Found a whitelisted error:\n>   tensorflow/python/feature_column/feature_column_test.py:3132: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_one_feature_column] Abstract class '_TestFeatureColumn' with abstract methods instantiated\n> Found a whitelisted error:\n>   tensorflow/python/feature_column/feature_column_test.py:3142: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_two_feature_columns] Abstract class '_TestFeatureColumn' with abstract methods instantiated\n> Found a whitelisted error:\n>   tensorflow/python/feature_column/feature_column_test.py:3143: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_two_feature_columns] Abstract class '_TestFeatureColumn' with abstract methods instantiated\n> Found a whitelisted error:\n>   tensorflow/python/feature_column/feature_column_test.py:3155: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_different_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\n> Found a whitelisted error:\n>   tensorflow/python/feature_column/feature_column_test.py:3156: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_different_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\n> Found a whitelisted error:\n>   tensorflow/python/feature_column/feature_column_test.py:3163: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_equal_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\n> Found a whitelisted error:\n>   tensorflow/python/feature_column/feature_column_test.py:3164: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_equal_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\n> Found a whitelisted error:\n>   tensorflow/python/feature_column/feature_column_test.py:3177: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_multiple_features_dict] Abstract class '_TestFeatureColumn' with abstract methods instantiated\n> Found a whitelisted error:\n>   tensorflow/python/feature_column/feature_column_test.py:3178: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_multiple_features_dict] Abstract class '_TestFeatureColumn' with abstract methods instantiated\n> FAIL: Found 2 non-whitelited pylint errors:\n> tensorflow/python/ops/math_ops.py:2120: [C0301(line-too-long), ] Line too long (81/80)\n> tensorflow/python/ops/math_ops.py:2124: [C0301(line-too-long), ] Line too long (81/80)\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21494#issuecomment-411947880>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8Ir36TqUhaugNNH5Er8fdenBXIHks5uPOJ7gaJpZM4V0uZh>\n> .\n>\n", "Hi @drpngx, just curious what release this will make it into? I saw version 1.10.1 was released 5 days ago but this commit isn't in there.", "CC @av8ramit I guess the next one `1.11`.", "1.10.1 was a patch release and by definition will not necessarily contain all latest commits. @drpngx is correct it will be in 1.11 which will be cut tomorrow at 5pm."]}, {"number": 21493, "title": "Update RELEASE.md", "body": "Fixing a formatting error.", "comments": []}, {"number": 21492, "title": "Public API to preempt tf.train.Server", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\n`tf.train_and_evaluate` uses `time.sleep` to (optimistically) synchronize the startup of chief/worker nodes in the distributed mode (see [estimator/training.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/training.py#L753)). The implicit assumption in this logic is that by the time the worker is spawned, the chief has already started its `tf.train.Server`, i.e. the scheduler should be aware of the assumption and should schedule and initialize the chief first. This might not be easily achievable on general purpose systems like YARN.\r\n\r\nOne possible solution to this is to synchronize the chief/worker tasks on a barrier, and then preemptively start the server right after the barrier, but prior to calling `train_and_evaluate`. This does not eliminate the race condition entirely but makes it much less likely in practice. The only problem here is that `train_and_evaluate` does not provide a documented way to account for preempted servers. The undocumented way is:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b4fd1c2b7a37367e61bbae3d27d194a894cb7bb/tensorflow/python/estimator/training.py#L747-L748\r\n\r\nI was wondering if it would be possible to make this part of the `train_and_evaluate` contract public or, alternatively, address the issue in another way?", "comments": ["@xiejw @ispirmustafa -- what is the recommended way to handle pre-empted Servers for systems like YARN?", "This is a wrong argument to understand the sleep logic in the code. \r\n\r\n1. std server starts before the sleeping. All workers (including chief) will try to find each other first.\r\n2. The time.sleep after that is \"delay starting\". It is part of the machine learning, not related to distributed cluster management. It is the best practice for between-graph execution. For multiple worker training scenario, it is recommended to have few workers sending the gradients to update the initial variables. More workers should join later. This produces better model quality. ", "@xiejw thank you for the clarification, and apologies for poor wording on my side. The code fragment I've linked to gives the \"delay starting\" rationale in the comment. \r\n\r\n> All workers (including chief) will try to find each other first \r\n\r\nWould it be right to say that in order for a task to start, it should successfully connect to all other tasks; and that if the connection is unsuccessful, the task will retry indefinitely/\"long enough\"? I would very much appreciate it if you could reference relevant code in the reply.", "Assume device_filter is not set. The precise sequence can be described as follows:\r\n\r\n1. start_std_server. This opens the port and listens the grpc in background thread. All workers must do this first. Otherwise it might delay other workers to train the model.\r\n\r\n2. (optional) time.sleep for delay starting. Note that at this time the grpc port is open already.\r\n\r\n3. Estimator.train. This creates the graph and launches a new tf.Session to train the model. The session will try to talk to each other device (in this case other workers) and then run the TF graph. This is the part it will try to block until all devices are found. The code is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master.cc#L215).\r\n\r\nSay there is a worker in the time.sleep for 100000 secs (basically forever). As the port has been opened in step 1, so other workers can start train (step 3) without waiting. \r\n\r\nWith device filter, only step 3 is slightly different where each worker only tries to find the devices in the list rather than all devices. ", "Thank you very much for a detailed reply. Let me do some experiments and get back to you. ", "This is indeed a non-issue, thanks again @xiejw! ", "@xiejw I thought about this more over a weekend and came up with another use-case for preemption: port reservation. \r\n\r\nYARN does not manage the network on the containers, so in order to run TF on YARN one has to manually reserve a port on each of the allocated containers, communicate it to all other tasks to assemble a `ClusterSpec` and only then start the server. IIUC with the current API there is no race-free way of doing this is in TF. Could you advise?\r\n", "@karmel do we have any expert for YARN? I do not understand that environment, so it is very hard for me to give best suggestions here. \r\n\r\n@superbobry As I mentioned above, I do not understand YARN. So, my suggestions could be wrong or sub-optimal. The minimal knowledge each training job should know is \r\n\r\n1. The port itself uses\r\n2. The ports used by PS\r\n\r\nThe ports of other workers are optional. You still need to fill the TF_CONFIG, but the values of other workers do not matter. \r\n\r\nIf the conditions above are met in YARN, then you can construct a device_filter (see [example code](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/estimator/run_config.py#L518)). The session will try to find the device in that list only even other workers are online (or even the other worker's addresses are fake). ", "@xiejw the problem with YARN is that it does not manage the network, i.e. there is no way to reserve a port prior to submitting an application. One possible solution to this is to\r\n\r\n1. Spawn a Python process in each YARN container.\r\n2. Bind to port 0.\r\n3. Communicate the port assigned by the OS to all other containers and aggregate a cluster spec\r\n4. Export the spec in `TF_CONFIG` and start training.\r\n\r\nwhich roughly translates to\r\n\r\n```python\r\nwith socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\r\n    sock.bind((\"\", 0))\r\n    ipaddr, port = sock.getsockname()\r\n    cluster_spec = broadcast_addr_and_aggregate_spec(f\"{ipaddr}:{port}\")\r\n\r\n# export TF_CONFIG\r\ntf.estimator.train_and_evaluate(...)\r\n```\r\n\r\nNote that this implementation has a race condition between closing `sock` and binding a `tf.train.Server` to the reserved port. This means that a task from another distributed TF training running on the same YARN node could hijack the port, and effectively talk to a completely unrelated task. The window of opportunity for the race condition to occur is undefined and depends on the implementation of `train_and_evaluate`. \r\n\r\nI see multiple potential ways of getting rid of the race condition by slightly modifying the existing Python/C++ API of `tf.train.Server`:\r\n\r\n* Add an API to spawn a server using an existing FD. The gRPC backend seems to already support this (see grpc/grpc#6610). \r\n* Allow `SO_REUSEPORT` when creating a server. Currently the option is [explicitly disabled](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc#L191).", "@mrry could you comment on the flakiness caused by SO_REUSEPORT? ", "@jhseu -- I hear you might be able to advise on YARN?", "I don't have much personal experience with running on YARN, but the LinkedIn folks do:\r\nhttps://github.com/linkedin/TonY\r\nhttps://engineering.linkedin.com/blog/2018/09/open-sourcing-tony--native-support-of-tensorflow-on-hadoop", "Thanks @jhseu, I had a quick look at the code, and I think they have exactly the same issue. \r\n\r\nI'd like to point out that nothing above is specific to YARN, nor does it require any familiarity with the Hadoop ecosystem. The gist of the issue is that `tf.train.Server` does not provide a way to bind to an existing socket, either by reusing the fd or via SO_REUSEPORT.\r\n\r\n", "Nagging Assignee @karmel: It has been 47 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "[DistributionStrategies](https://github.com/tensorflow/community/blob/master/rfcs/20181016-replicator.md) is the new API for handling distribution and synchronization, and that API will be preferred in the coming months to the tf.train.Server API. That said, we have now started a Special Interest Group (SIG) for networking issues like these, so while I am closing this issue as it pertains to tf.train.Server, I would encourage you to join and participate in the [Networking SIG](https://groups.google.com/a/tensorflow.org/forum/#!forum/networking) to ensure these questions are addressed for YARN and other systems.", "> @xiejw the problem with YARN is that it does not manage the network, i.e. there is no way to reserve a port prior to submitting an application. One possible solution to this is to\r\n> \r\n> 1. Spawn a Python process in each YARN container.\r\n> 2. Bind to port 0.\r\n> 3. Communicate the port assigned by the OS to all other containers and aggregate a cluster spec\r\n> 4. Export the spec in `TF_CONFIG` and start training.\r\n> \r\n> which roughly translates to\r\n> \r\n> ```python\r\n> with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\r\n>     sock.bind((\"\", 0))\r\n>     ipaddr, port = sock.getsockname()\r\n>     cluster_spec = broadcast_addr_and_aggregate_spec(f\"{ipaddr}:{port}\")\r\n> \r\n> # export TF_CONFIG\r\n> tf.estimator.train_and_evaluate(...)\r\n> ```\r\n> \r\n> Note that this implementation has a race condition between closing `sock` and binding a `tf.train.Server` to the reserved port. This means that a task from another distributed TF training running on the same YARN node could hijack the port, and effectively talk to a completely unrelated task. The window of opportunity for the race condition to occur is undefined and depends on the implementation of `train_and_evaluate`.\r\n> \r\n> I see multiple potential ways of getting rid of the race condition by slightly modifying the existing Python/C++ API of `tf.train.Server`:\r\n> \r\n> * Add an API to spawn a server using an existing FD. The gRPC backend seems to already support this (see [grpc/grpc#6610](https://github.com/grpc/grpc/pull/6610)).\r\n> * Allow `SO_REUSEPORT` when creating a server. Currently the option is [explicitly disabled](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc#L191).\r\n\r\nHI. We have faced the exactly same problem as yours when training tensorflow on yarn. Do you have progress on it?", "@karmel as far as I can tell the old distributed TF corresponds to `ParameterServerStrategy` which is  configured via `TF_CONFIG` environment variable. This is exactly the setup described in the issue, meaning that the issue of port reservation is just as relevant in the context of distribution strategies.\r\n\r\n"]}, {"number": 21490, "title": "Add deprecation warning to tf.gfile.FastGFile.", "body": "Fixes #12663.", "comments": ["Hi @rasmi, it's probably better to keep this interface until we implement it properly in open source. I would rather we didn't submit this. WDYT?", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21489, "title": "Add deprecation warning to tf.gfile.FastGFile.", "body": "Fixes #12663.", "comments": []}, {"number": 21488, "title": "Tensorflow PIP Package fails to build - AttributeError: module 'pandas' has no attribute 'core'", "body": "This may be a bug in the Python PIP build script(s), or something askew in my environment.  I am trying to build Tensorflow from source, to include AVX/AVX2 support.  It always fails with the error log pasted below.  I have tried the latest git pull over the last week or so (i.e. multiple git pulls, followed by \"bazel clean ; bazel build <options>\".  I have also tried the same with the r1.10 branch.\r\n\r\nThere is [this somewhat similar question on StackOverflow](https://stackoverflow.com/questions/36521691/importing-pandas-gives-error-attributeerror-module-pandas-has-no-attribute-c).  That is more about using Pandas, rather than building Tensorflow.  At any rate I tried all the suggestions to no avail.  And my Pandas is working:\r\n\r\n```\r\n$ python -c \"import pandas as pd ; import pandas.core.ops as ops ; print(pd.__version__)\" && echo $?\r\n0.23.4\r\n0\r\n```\r\n\r\nSystem Details:\r\n-----------------\r\n\r\n  - Have I written custom code: NO\r\n  - OS Platform and Distribution: CentOS Linux 7.4.1708 with Anaconda Python3 distribution\r\n  - Mobile device: N/A\r\n  - TensorFlow installed from: not yet installed, trying to build from source (no build-halting errors during the C/C++ portion of the build)\r\n  - TensorFlow version: latest git pull over the last several days, both master and r1.10 branch\r\n  - Python version: Python 3.6.6 :: Anaconda custom (64-bit)\r\n  - Bazel version (if compiling from source):\r\n```\r\n$ bazel version\r\nBuild label: 0.16.0- (@non-git)\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jul 31 18:27:17 2018 (1533061637)\r\nBuild timestamp: 1533061637\r\nBuild timestamp as int: 1533061637\r\n```\r\n  - GCC/Compiler version: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\n  - CUDA/cuDNN version: N/A\r\n  - GPU model and memory: N/A\r\n  - Exact command to reproduce:\r\n```\r\n  $ git clone https://github.com/tensorflow/tensorflow.git\r\n  $ cd tensorflow\r\n  $ ./configure # accept all defaults\r\n  $ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --verbose_failures -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nOutput of tf_env_collect.sh:\r\n------------------------------\r\n\r\n== cat /etc/issue ===============================================\r\nLinux septictank 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x\r\n86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux septictank 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.15.0\r\nnumpydoc                           0.8.0\r\nprotobuf                           3.5.2\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"/home/matt/tensorflow/tensorflow/python/platform/self_check.py\", line 2$, in <module>\r\n    from tensorflow.python.platform import build_info\r\nImportError: cannot import name 'build_info'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/matt/tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-i$port\r\n  File \"/home/matt/tensorflow/tensorflow/python/__init__.py\", line 49, in <modul\r\ne>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/matt/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 25,\r\nin <module>\r\n    from tensorflow.python.platform import self_check\r\n  File \"/home/matt/tensorflow/tensorflow/python/platform/self_check.py\", line 27\r\n, in <module>\r\n    raise ImportError(\"Could not import tensorflow. Do not import tensorflow \"\r\nImportError: Could not import tensorflow. Do not import tensorflow from its sour\r\nce directory; change directory to outside the TensorFlow source tree, and relaun\r\nch your Python interpreter from there.\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\r\nError message from bazel:\r\n-----------------------------\r\n```\r\nERROR: /home/matt/tensorflow/tensorflow/python/estimator/api/BUILD:12:1: Couldn't build file tensorflow/python/estimator/api/__init__.py: Executing genrule //tensorflow/python/estimator/api:estimator_python_api_gen failed (Exit 1): bash failed: error executing command\r\n  (cd /home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/opt/anaconda3/bin:/opt/anaconda3/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/local/bin:/sbin:/usr/sbin:/usr/local/bin:/sbin:/usr/sbin \\\r\n    PYTHON_BIN_PATH=/opt/anaconda3/bin/python \\\r\n    PYTHON_LIB_PATH=/opt/anaconda3/lib/python3.6/site-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api  --apidir=bazel-out/k8-opt/genfiles/tensorflow/python/estimator/api --apiname=estimator --apiversion=2 --package=tensorflow.python.estimator --output_package=tensorflow.python.estimator.api bazel-out/k8-opt/genfiles/tensorflow/python/estimator/api/__init__.py bazel-out/k8-opt/genfiles/tensorflow/python/estimator/api/estimator/__init__.py bazel-out/k8-opt/genfiles/tensorflow/python/estimator/api/estimator/export/__init__.py bazel-out/k8-opt/genfiles/tensorflow/python/estimator/api/estimator/inputs/__init__.py')\r\ntf.estimator package not installed.\r\nTraceback (most recent call last):\r\n  File \"/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 511, in <module>\r\n    main()\r\n  File \"/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 504, in main\r\n    importlib.import_module(args.package)\r\n  File \"/opt/anaconda3/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow.python.estimator.estimator_lib\r\n  File \"/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/estimator/estimator_lib.py\", line 41, in <module>\r\n    from tensorflow.python.estimator.inputs import inputs\r\n  File \"/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/estimator/inputs/inputs.py\", line 23, in <module>\r\n    from tensorflow.python.estimator.inputs.pandas_io import pandas_input_fn\r\n  File \"/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/estimator/inputs/pandas_io.py\", line 31, in <module>\r\n    import pandas as pd\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/__init__.py\", line 42, in <module>\r\n    from pandas.core.api import *\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/core/api.py\", line 10, in <module>\r\n    from pandas.core.groupby.groupby import Grouper\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/__init__.py\", line 2, in <module>\r\n    from pandas.core.groupby.groupby import (\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\", line 49, in <module>\r\n    from pandas.core.frame import DataFrame\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\", line 74, in <module>\r\n    from pandas.core.series import Series\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\", line 67, in <module>\r\n    import pandas.core.ops as ops\r\nAttributeError: module 'pandas' has no attribute 'core'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n", "comments": ["@yifeif, could you ptal.", "Does re-install the package help?\r\n", "To which package are you referring?\n\nOn Thu, Aug 16, 2018 at 7:20 PM Yifei Feng <notifications@github.com> wrote:\n\n> Does re-install the package help?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21488#issuecomment-413721464>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADUj2Uvprwygv_tEr_77Fu7tcG22F6nDks5uRgxngaJpZM4V0q0M>\n> .\n>\n", "Sorry for not being clear @matt-garman. I meant pandas.", "I just un-installed, then re-installed pandas v0.23.4: same result as\nposted above.\n\n\n\nOn Fri, Aug 17, 2018 at 1:00 PM Yifei Feng <notifications@github.com> wrote:\n\n> Sorry for not being clear @matt-garman <https://github.com/matt-garman>.\n> I meant pandas.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21488#issuecomment-413942949>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADUj2ToAB36hQ6IfnSm_fyDovS558Nvhks5uRwS2gaJpZM4V0q0M>\n> .\n>\n", "i have the same issue.  how come pandas is not listed in the dependencies of the pip setup.py?\r\n\r\nit is listed in the build environment dependencies in ci_build\r\n\r\n```\r\ninstall/install_python3.5_pip_packages.sh:# pandas required by `inflow`\r\ninstall/install_python3.5_pip_packages.sh:pip3 install pandas==0.19.2\r\ninstall/install_python3.6_pip_packages.sh:# pandas required by `inflow`\r\ninstall/install_python3.6_pip_packages.sh:pip3 install pandas==0.19.2\r\ninstall/install_pip_packages.sh:# pandas required by `inflow`\r\ninstall/install_pip_packages.sh:pip2 install pandas==0.19.2\r\ninstall/install_pip_packages.sh:pip3 install pandas==0.19.2\r\n```\r\n", "perhaps it was transitively pulled in by something else which TF depends upon, which has removed recently.\r\n", "Here is my backtrace.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/buildslave/workdir/buildslave/framework_comparison_ubuntu_18_04_python2/workspace/external/framework-comparison/tensorflow/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/buildslave/workdir/buildslave/framework_comparison_ubuntu_18_04_python2/workspace/external/framework-comparison/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 82, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/buildslave/workdir/buildslave/framework_comparison_ubuntu_18_04_python2/workspace/external/framework-comparison/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/keras/__init__.py\", line 29, in <module>\r\n    from tensorflow.python.keras import datasets\r\n  File \"/home/buildslave/workdir/buildslave/framework_comparison_ubuntu_18_04_python2/workspace/external/framework-comparison/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/keras/datasets/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras.datasets import imdb\r\n  File \"/home/buildslave/workdir/buildslave/framework_comparison_ubuntu_18_04_python2/workspace/external/framework-comparison/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/keras/datasets/imdb.py\", line 25, in <module>\r\n    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n  File \"/home/buildslave/workdir/buildslave/framework_comparison_ubuntu_18_04_python2/workspace/external/framework-comparison/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/keras/preprocessing/__init__.py\", line 30, in <module>\r\n    from tensorflow.python.keras.preprocessing import image\r\n  File \"/home/buildslave/workdir/buildslave/framework_comparison_ubuntu_18_04_python2/workspace/external/framework-comparison/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/keras/preprocessing/image.py\", line 23, in <module>\r\n    from keras_preprocessing import image\r\n  File \"/home/buildslave/workdir/buildslave/framework_comparison_ubuntu_18_04_python2/workspace/external/framework-comparison/tensorflow/local/lib/python2.7/site-packages/keras_preprocessing/image/__init__.py\", line 8, in <module>\r\n    from .dataframe_iterator import DataFrameIterator\r\n  File \"/home/buildslave/workdir/buildslave/framework_comparison_ubuntu_18_04_python2/workspace/external/framework-comparison/tensorflow/local/lib/python2.7/site-packages/keras_preprocessing/image/dataframe_iterator.py\", line 11, in <module>\r\n    from pandas.api.types import is_numeric_dtype\r\nImportError: No module named pandas.api.types\r\n```\r\n\r\nthis TF installation has been installed from the pip wheel generated by `bazel build --config=opt tensorflow/tools/pip_package:build_pip_package`\r\n", "ah ha - https://github.com/keras-team/keras-preprocessing/issues/154\r\n", "Hi @matt-garman! We are checking to see if you still need help in this issue , Have you tried latest stable version TF 2.6  yet? Please create a new issue if the issue is replicating in newer version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21488\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21488\">No</a>\n"]}, {"number": 21487, "title": "TensorRT Can't identify the cuda device", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n('v1.9.0-rc2-1924-g054b046', '1.10.0-rc1'). Current master branch\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: CUDA 9.0 cuDNN 7.1.4\r\n- **GPU model and memory**: Titan V\r\n- **Exact command to reproduce**:\r\nFirst train the example small model use [mnist.py](https://gist.github.com/qinyao-he/60171806c40b7b46354234b896c7c2d5).\r\nThen use tensorflow built-in tools to freeze the graph:\r\n```\r\npython -m tensorflow.python.tools.freeze_graph --input_graph log/graph.pbtxt --input_checkpoint log/model.ckpt-20000 --output_node_names softmax_tensor --output_graph log/freeze_graph.pb\r\n```\r\nFinally use [tensorrt.py](https://gist.github.com/qinyao-he/28ddedb7f561bb3cb4ba880833f14a89) to optimize the graph use TensorRT engine.\r\n\r\n### Describe the problem\r\nThe log shows TensorRT could not find cuda devices. And the graph remains unchanged after the conversion.\r\n\r\n### Source code / logs\r\n> 2018-08-08 13:47:27.322236: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1                                                                                 \r\n> 2018-08-08 13:47:27.322317: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session                                                                                        \r\n> 2018-08-08 13:47:27.322928: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA                      \r\n> 2018-08-08 13:47:27.327805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties:                                                                               \r\n> name: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455\r\n> pciBusID: 0000:03:00.0\r\n> totalMemory: 11.78GiB freeMemory: 11.36GiB\r\n> 2018-08-08 13:47:27.327827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0                                                                                 \r\n> 2018-08-08 13:47:27.694717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:                                                \r\n> 2018-08-08 13:47:27.694749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n> 2018-08-08 13:47:27.694754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n> 2018-08-08 13:47:27.694984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10938 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:03:00.0, compute capability: 7.0)\r\n> 2018-08-08 13:47:27.935823: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2923] Segment @scope '', converted to graph                                                                     \r\n> 2018-08-08 13:47:27.935851: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:415] Can't find a device placement for the op!                                                                  \r\n> 2018-08-08 13:47:27.946155: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:799] Cluster is set but device '' is not found in the cluster                                                   \r\n> 2018-08-08 13:47:27.946194: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:916] Can't identify the cuda device. Running on device 0                                                        \r\n> 2018-08-08 13:47:28.915936: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger runtime.cpp (16) - Cuda Error in allocate: 2                                                         \r\n> 2018-08-08 13:47:28.916287: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger runtime.cpp (16) - Cuda Error in allocate: 2                                                         \r\n> 2018-08-08 13:47:28.916332: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:933] Engine my_trt_op_0 creation for segment 0, composed of 22 nodes failed: Internal: Failed to build TensorRT engine. Skipping...\r\n> 2018-08-08 13:47:28.967359: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2923] Segment @scope '', converted to graph                                                                     \r\n> 2018-08-08 13:47:28.967390: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:415] Can't find a device placement for the op!                                                                  \r\n> 2018-08-08 13:47:28.968228: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:799] Cluster is set but device '' is not found in the cluster                                                   \r\n> 2018-08-08 13:47:28.968242: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:916] Can't identify the cuda device. Running on device 0                                                        \r\n> 2018-08-08 13:47:28.974512: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger runtime.cpp (16) - Cuda Error in allocate: 2                                                         \r\n> 2018-08-08 13:47:28.974935: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger runtime.cpp (16) - Cuda Error in allocate: 2                                                         \r\n> 2018-08-08 13:47:28.974968: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:933] Engine my_trt_op_0 creation for segment 0, composed of 22 nodes failed: Internal: Failed to build TensorRT engine. Skipping...\r\n> 2018-08-08 13:47:28.979975: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:198] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n> 2018-08-08 13:47:28.981037: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:198] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n> 2018-08-08 13:47:28.982158: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:403] Optimization results for grappler item: tf_graph                                                          \r\n> 2018-08-08 13:47:28.982172: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 52 nodes (-11), 51 edges (-13), time = 72.462ms.                    \r\n> 2018-08-08 13:47:28.982176: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   layout: Graph size after: 52 nodes (0), 51 edges (0), time = 9.414ms.                                   \r\n> 2018-08-08 13:47:28.982179: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 52 nodes (0), 51 edges (0), time = 1003.37ms.                      \r\n> 2018-08-08 13:47:28.982183: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 52 nodes (0), 51 edges (0), time = 34.038ms.                        \r\n> 2018-08-08 13:47:28.982186: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 52 nodes (0), 51 edges (0), time = 20.332ms.                       \r\n> 2018-08-08 13:47:28.982193: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:403] Optimization results for grappler item: my_trt_op_0_native_segment                                        \r\n> 2018-08-08 13:47:28.982198: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 23 nodes (0), 22 edges (0), time = 1.062ms.                         \r\n> 2018-08-08 13:47:28.982204: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   layout: Graph size after: 23 nodes (0), 22 edges (0), time = 0.657ms.                                   \r\n> 2018-08-08 13:47:28.982240: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 23 nodes (0), 22 edges (0), time = 0.16ms.                         \r\n> 2018-08-08 13:47:28.982246: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 23 nodes (0), 22 edges (0), time = 0.898ms.                         \r\n> 2018-08-08 13:47:28.982253: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 23 nodes (0), 22 edges (0), time = 0.14ms.                         \r\n> 2018-08-08 13:47:29.038047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0                                                                                 \r\n> 2018-08-08 13:47:29.038105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:                                                \r\n> 2018-08-08 13:47:29.038120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n> 2018-08-08 13:47:29.038131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n> 2018-08-08 13:47:29.038327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10938 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:03:00.0, compute capability: 7.0)", "comments": ["@aaroey ", "@qinyao-he are you using tensorrt 4? The 1.9rc and 1.10 was built agains trt 3.0, so if you run with trt 4.0 there could be problem like this.", "It should print the loaded trt version before the first log message `Number of eligible GPUs (core count >= 8): 1` as shown above. If you want to use trt 4.0 would you please build from master? We plan to make trt 4.0 as the default in r1.11 but it'll take a while.", "@aaroey I actually use TensorRT 4, and I indeed build from master myself.", "Well I just managed to reproduce the problem using your script. It turns out the error was caused by another problem: the device is not set in the engine and the converter will use default cuda malloc for memory allocation during the conversion. I'll fix it.\r\n\r\nAs a work around, it should work if you add `with graph.device('gpu:0')` when building your model for training. Or you may read the `./log/freeze_graph.pb`, import it inside a `with graph.device('gpu:0')` context, write it out as a new `./log/freeze_graph.pb`, and use the new one to do the conversion.", "I use tensorrt 3.0.4, and came out with the error:\r\n\r\n`Can't determine the device, constructing an allocator at device 0`\r\n\r\nI have try to add `with graph.device('gpu:0')` when building model, then error came up as:\r\n\r\n`Non-OK-status: GpuIdManager::TfToCudaGpuId(tf_gpu_id, &cuda_gpu_id) status: Not found: TensorFlow device GPU:0 was not registered`\r\n\r\nhopelessness ...\r\n\r\n@aaroey ", "> I have try to add `with graph.device('gpu:0')` when building model, then error came up as:\r\n> \r\n> `Non-OK-status: GpuIdManager::TfToCudaGpuId(tf_gpu_id, &cuda_gpu_id) status: Not found: TensorFlow device GPU:0 was not registered`\r\n\r\nI think this is because the device is not initialized when you call `trt.create_inference_graph()`. Are you running with TF r1.10? Could you retry by initializing a session before calling `trt.create_inference_graph()`?\r\n\r\nActually I think this should be fixed in master by #20318. Could you also help to tried with master?\r\n\r\nThanks.", "maybe you should upgrade your cuda driver version.\r\nYou need to ensure that your driver version matches or exceeds your CUDA Toolkit version."]}, {"number": 21486, "title": "elastic averaging SGD update: support partitioner & more optimizers", "body": "The old implementation of elastic average optimizer does not partitioned variables, support optimizer like AdamOptimizer which will create global variables like beta_1, beta2.\r\n\r\nMain changes:\r\n1. supports partitioned variables\r\n2. support more optimizers, e.g., Adam/RMSProp\r\n3. add a saver to save the values in the global_center_variable\r\n    As it turns out the global model is better than the local worker. To allow chief worker save the global values, a mapping logic is added.\r\n\r\nVerifications I have done:  Cifar10 on AlexNet/MobileNetV2 and other internal models.\r\nAll Results shows better performance and precision.\r\n\r\nRelated Item\r\n#12472: Feature request: EASGD\r\n@ali01, @jinxin0924, @tmulc18 Could you kindly help on review the changes?\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed CLA just now, thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I signed it! Added the email address in commit log to CLA email list.", "CLAs look good, thanks!\n\n<!-- ok -->", "@drpngx , I can't see the 'Details' of feedback/copybara failure, anything I should handle on this?", "Somehow the test is creating a directory somewhere.\r\n```\r\nPermissionDeniedError (see above for traceback): Localfile::CreateDir mkdir failed on model: Permission denied\r\n````\r\nWe should be using [`self.get_temp_dir`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L851).", "Thanks. that's for checkpoint UT, will change it.", "@drpngx \r\nFind these two errors, was passed in previous run. I don't think they related to my [UT fixing commit](https://github.com/tensorflow/tensorflow/pull/21486/commits/7d9a839a26b7b801ffc53eff59688672021d6a43).\r\n\r\nUbuntu contrib:\r\n2018-08-16 19:22:30.637876: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n*** Error in `/opt/python3.6/bin/python3': corrupted double-linked list: 0x00007fcc60016a60 ***\r\n\r\nWindows Bazel GPU:\r\nERROR: T:/src/github/tensorflow/tensorflow/core/BUILD:2911:1: C++ compilation of rule '//tensorflow/core:device_tracer' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command", "Hi @drpngx , feedback/copybara test is passed now.\r\nAnd I checked the error information of 'Windows Bazel GPU' test as following is not introduced by last change. What should I do next?\r\n\r\nERROR: T:/src/github/tensorflow/tensorflow/core/BUILD:2911:1: C++ compilation of rule '//tensorflow/core:device_tracer' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n.\\tensorflow/core/platform/default/gpu/cupti_wrapper.h(24): fatal error C1083: Cannot open include file: 'extras/CUPTI/include/cupti.h': No such file or directory\r\n ", "We're testing internally. Let's see what happens.", "Great! Thanks!", "OK, it was pushed internally, should appear soon."]}]