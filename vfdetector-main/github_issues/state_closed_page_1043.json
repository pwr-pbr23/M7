[{"number": 22013, "title": "tf.scatter_nd_update - Segmentation fault (core dumped)", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n\r\nTF checkpoint I have built\r\n```\r\n/tmp/tensorflow# git log   \r\ncommit 09792df012c22622324f085f46edde33006c7355\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Sun Aug 26 02:07:11 2018 -0700\r\n\r\n    compat: Update forward compatibility horizon to 2018-08-26\r\n    \r\n    PiperOrigin-RevId: 210266798\r\n```\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.5)\r\nprotobuf (3.6.1)\r\ntensorflow (1.10.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Aug 29 19:57:14 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n```\r\n**Bazel** version\r\n```\r\n$ bazel version\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nBuild label: 0.16.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jul 31 17:01:24 2018 (1533056484)\r\nBuild timestamp: 1533056484\r\nBuild timestamp as int: 1533056484\r\n```\r\n\r\n**CUDNN** version:\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\r\nCuda compilation tools, release 9.2, V9.2.148\r\n```\r\n\r\n**GPU**: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS\r\n\r\n- **Exact command to reproduce**:\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\ndef scope_1():\r\n    print(\"DS1 SCOPE =============\")\r\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        x = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\r\n                               , trainable=False, use_resource=True)             \r\n        print(\"graph: {}\".format(x.graph))\r\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\" name: {}\".format(x.name))\r\n        print(\"  var: {}\".format(str(x)))\r\n        current_scope = tf.get_variable_scope()       \r\n        assign_one = tf.assign(x, 1.0, name=\"x_is_one\")\r\n    \r\n    def scope_2(inputs, label):        \r\n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\"DS1 SCOPE =============\")\r\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\r\n            y = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\r\n                                   , trainable=False, use_resource=True)         \r\n            print(\"graph: {}\".format(y.graph))\r\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n            print(\" name: {}\".format(y.name))\r\n            print(\"  var: {}\".format(str(y)))\r\n            print(\"=============\")\r\n            print(y)\r\n            print(inputs)\r\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\r\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0)))\r\n            with tf.control_dependencies([assign_two]):\r\n                with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\r\n                    return y.read_value(), label\r\n            #return x,label\r\n    \r\n    # test that original x is mutable\r\n    with tf.control_dependencies([assign_one]):\r\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\r\n                    .map(scope_2)\r\n                    .batch(1)\r\n                    .repeat(1)        \r\n                    )\r\n    return dataset\r\n    \r\n                \r\nwith tf.variable_scope(\"scope_0\"):\r\n        dataset_fn = scope_1()\r\n\r\nwith tf.variable_scope(\"iterator\"):\r\n    # Define iterator from_string_handle. In general it is useful to have\r\n    # this kind of iterator if one wants to switch between train and validation\r\n    # within the training loop.        \r\n    iterator_t = dataset_fn.make_initializable_iterator()\r\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\r\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \r\n                                                iterator_t.output_types,\r\n                                                iterator_t.output_shapes)\r\n    \r\n    def get_next_item():\r\n        next_elem = iterator.get_next(name=\"next_element\")\r\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\r\n        return x, y    \r\nwith tf.Session() as sess:\r\n\r\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\r\n    handle_t = sess.run(iterator_t.string_handle())\r\n    # Run data iterator initialisation\r\n    sess.run(iterator_t.initializer)\r\n    print(sess.graph.get_operations()) \r\n    while True:\r\n        try:\r\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\r\n        except tf.errors.OutOfRangeError:\r\n                        print(\"End of training dataset.\")\r\n                        break        \r\n    print()\r\n    print(\"global vars: {}\".format(tf.global_variables()))\r\n    print(\"local vars: {}\".format(tf.local_variables()))\r\n    print(tf.get_default_graph().get_name_scope())\r\n\r\n```\r\n\r\n### Describe the problem\r\nI am trying to create a function that would modify a Tensor within a pipeline of Dataset API.\r\nThe scoping may seem weird, but that is minimal example that shows the problem that I created from my project. After adding `tf.scatter_nd_update(y, [[0]], [0.22])` I started to get segmentation fault.\r\nA minimal example with `tf.add` instead of `tf.scatter_nd_update` worked, see https://github.com/tensorflow/tensorflow/issues/22009\r\n\r\nI tried disabling GPU with `config=tf.ConfigProto(device_count={'GPU': 0})` and `CUDA_VISIBLE_DEVICES=\"\"` but the result was the same.\r\n\r\nI will recompile TF overnight (from the current master) with `--copt=-g` and try to provide a stacktrace \r\n\r\n```\r\nTF_BUILD_INFO = {container_type: \"gpu\", command: \"bazel build --config=opt --config=cuda --copt=-march=native --copt=-mfpmath=both --copt=-mtune=native --copt=-g --verbose_failures --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --jobs=8 --config=mkl --action_env=LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib tensorflow/tools/pip_package:build_pip_package\", source_HEAD: \"201be3d514d7239aa19496dba4dd0c85303b03f1\", source_remote_origin: \"https://github.com/tensorflow/tensorflow\", OS: \"Linux\", kernel: \"4.13.0-38-generic\", architecture: \"x86_64\", processor: \"Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz\", processor_count: \"8\", memory_total: \"32877820 kB\", swap_total: \"69444596 kB\", Bazel_version: \"Build label: 0.16.0\", Java_version: \"1.8.0_181\", Python_version: \"3.6.2\", gpp_version: \"g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\", swig_version: \"\", NVIDIA_driver_version: \"396.26\", CUDA_device_count: \"1\", CUDA_device_names: \"GeForce GTX 1080 Ti   (*PrimaryCard),\", CUDA_toolkit_version: \"V9.2.148\"}\r\n```\r\n\r\n### Source code / logs\r\n\r\n```\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f0940f582b0>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f0940f582b0>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\n=============\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\nTensor(\"arg0:0\", shape=(), dtype=int32)\r\n2018-09-02 20:52:39.456276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-02 20:52:39.456710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.23GiB\r\n2018-09-02 20:52:39.456728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-09-02 20:52:39.665323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-02 20:52:39.665362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-09-02 20:52:39.665369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-09-02 20:52:39.665731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9887 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-09-02 20:52:39.756397: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nlog of run on CPU\r\n```\r\nCUDA_VISIBLE_DEVICES=\"\" python3 bug.py \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7ffaa61de390>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7ffaa61de390>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\n=============\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\nTensor(\"arg0:0\", shape=(), dtype=int32)\r\n2018-09-02 20:54:41.271567: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2018-09-02 20:54:41.271604: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: 3bed2f328777\r\n2018-09-02 20:54:41.271614: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: 3bed2f328777\r\n2018-09-02 20:54:41.271655: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.26.0\r\n2018-09-02 20:54:41.271686: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.26.0\r\n2018-09-02 20:54:41.271694: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.26.0\r\n2018-09-02 20:54:41.271962: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nSegmentation fault (core dumped)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version\nGPU model and memory", "When executing line by line everything works fine until it starts the session.\r\n\r\nI've tried running it on different environment, on kaggle.com, and the it also fails. ", "I tried it on one more environment. Where I have just installed TF from https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl\r\n\r\nI had to modify a bit lambda initializer in get_variable and specify the shape, but otherwise I got the same segmentation fault. This env does not have GPU, so it was run on a CPU.\r\n\r\n```\r\n# python3 tf_bug.py \r\n/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7fa7eb105a20>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7fa7eb105a20>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\n=============\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\nTensor(\"arg0:0\", shape=(), dtype=int32)\r\n2018-09-03 08:12:16.809170: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nSegmentation fault (core dumped)\r\n```\r\n", "I tried running the script in gdb but it says `no stack`. Any hints how to get it work?\r\n", "I've managed to get gdb working, looks like the issue is with inferring shape in `scatter_nd_update`\r\n```\r\n0x00007fffcd5f912f in tensorflow::shape_inference::ScatterNdUpdateShape(tensorflow::shape_inference::InferenceContext*) ()\r\n   from /opt/conda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n```\r\n\r\nFull log \r\n```\r\n# gdb python3\r\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1                                              Copyright (C) 2016 Free Software Foundation, Inc.                                         License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> py\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"gdb py\r\nand \"show warranty\" for details.\r\nThis GDB was configured as \"x86_64-linux-gnu\".\r\nType \"show configuration\" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n<http://www.gnu.org/software/gdb/documentation/>.\r\nFor help, type \"help\".\r\nType \"apropos word\" to search for commands related to \"word\"...\r\nReading symbols from python3...done.\r\n(gdb) run bug.py\r\nStarting program: /opt/conda/bin/python3 bug.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff31fa700 (LWP 606)]\r\n[New Thread 0x7ffff09f9700 (LWP 607)]\r\n[New Thread 0x7fffee1f8700 (LWP 608)]\r\n[New Thread 0x7fffeb9f7700 (LWP 609)]\r\n[New Thread 0x7fffe91f6700 (LWP 610)]\r\n[New Thread 0x7fffe69f5700 (LWP 611)]\r\n[New Thread 0x7fffe41f4700 (LWP 612)]\r\n[Thread 0x7fffe41f4700 (LWP 612) exited]\r\n[Thread 0x7fffe69f5700 (LWP 611) exited]\r\n[Thread 0x7fffe91f6700 (LWP 610) exited]\r\n[Thread 0x7fffeb9f7700 (LWP 609) exited]\r\n[Thread 0x7fffee1f8700 (LWP 608) exited]\r\n[Thread 0x7ffff09f9700 (LWP 607) exited]\r\n[Thread 0x7ffff31fa700 (LWP 606) exited]\r\n/opt/conda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/opt/conda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n[New Thread 0x7fffe41f4700 (LWP 617)]\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7ffff6789160>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\n[New Thread 0x7fffe69f5700 (LWP 618)]\r\ninitial scope:\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7ffff6789160>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\n=============\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\nTensor(\"arg0:0\", shape=(), dtype=int32)\r\n[New Thread 0x7fffe91f6700 (LWP 619)]\r\n[New Thread 0x7fffeb9f7700 (LWP 620)]\r\n[New Thread 0x7fff871ff700 (LWP 621)]\r\n[New Thread 0x7fff869fe700 (LWP 622)]\r\n[New Thread 0x7fff861fd700 (LWP 623)]\r\n[New Thread 0x7fff859fc700 (LWP 624)]\r\n[New Thread 0x7fff851fb700 (LWP 625)]\r\n[New Thread 0x7fff849fa700 (LWP 626)]\r\n[New Thread 0x7fff5fdff700 (LWP 627)]\r\n[New Thread 0x7fff5f5fe700 (LWP 628)]\r\n[New Thread 0x7fff5edfd700 (LWP 629)]\r\n[New Thread 0x7fff5e5fc700 (LWP 630)]\r\n2018-09-04 08:28:02.088839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-04 08:28:02.089267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.20GiB\r\n2018-09-04 08:28:02.089287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-04 08:28:02.089297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-04 08:28:02.089303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\r\n2018-09-04 08:28:02.089307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\r\n2018-09-04 08:28:02.089464: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n[New Thread 0x7fff5ddfb700 (LWP 631)]\r\n[New Thread 0x7fff5d5fa700 (LWP 632)]\r\n[New Thread 0x7fff5cdf9700 (LWP 633)]\r\n[Thread 0x7fff5cdf9700 (LWP 633) exited]\r\n[New Thread 0x7fff5cdf9700 (LWP 634)]\r\n[Thread 0x7fff5cdf9700 (LWP 634) exited]\r\n[New Thread 0x7fff5cdf9700 (LWP 635)]\r\n[Thread 0x7fff5cdf9700 (LWP 635) exited]\r\n[New Thread 0x7fff5cdf9700 (LWP 636)]\r\n[Thread 0x7fff5cdf9700 (LWP 636) exited]\r\n[New Thread 0x7fff5cdf9700 (LWP 637)]\r\n[Thread 0x7fff5cdf9700 (LWP 637) exited]\r\n[New Thread 0x7fff5cdf9700 (LWP 638)]\r\n[Thread 0x7fff5cdf9700 (LWP 638) exited]\r\n[New Thread 0x7fff5cdf9700 (LWP 639)]\r\n[Thread 0x7fff5cdf9700 (LWP 639) exited]\r\n[New Thread 0x7fff5cdf9700 (LWP 640)]\r\n[Thread 0x7fff5cdf9700 (LWP 640) exited]\r\n\r\nThread 1 \"python3\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fffcd5f912f in tensorflow::shape_inference::ScatterNdUpdateShape(tensorflow::shape_inference::InferenceContext*) ()\r\n   from /opt/conda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n(gdb)\r\n```", "I've got the code running by explicitly specifying shape in `tf.get_variable()`, and then making sure that the corresponding tensors in assign ops had the same shape (so not assigning 1.0 but [1.0]). Still, I could not make it work with dynamic shapes, and segmentation fault should not take place but throw some error message.\r\n\r\n\r\nHere is the working code\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\ndef scope_1():\r\n    print(\"DS1 SCOPE =============\")\r\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        x = tf.get_variable(\"x\", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32\r\n                               , trainable=False, use_resource=True, shape=[1])             \r\n        print(\"graph: {}\".format(x.graph))\r\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\" name: {}\".format(x.name))\r\n        print(\"  var: {}\".format(str(x)))\r\n        current_scope = tf.get_variable_scope()       \r\n        assign_one = tf.assign(x, [1.0], name=\"x_is_one\")\r\n    \r\n    def scope_2(inputs, label):        \r\n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\"DS1 SCOPE =============\")\r\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\r\n            y = tf.get_variable(\"x\", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32\r\n                                   , trainable=False, use_resource=True, shape=[1])         \r\n            print(\"graph: {}\".format(y.graph))\r\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n            print(\" name: {}\".format(y.name))\r\n            print(\"  var: {}\".format(str(y)))\r\n            print(\"=============\")\r\n            print(y)\r\n            print(inputs)\r\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\r\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), [1.0])))\r\n            with tf.control_dependencies([assign_two]):\r\n                with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\r\n                    return y.read_value(), label\r\n\r\n            #return x,label\r\n    \r\n    # test that original x is mutable\r\n    with tf.control_dependencies([assign_one]):\r\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\r\n                    .map(scope_2)\r\n                    .batch(1)\r\n                    .repeat(1)        \r\n                    )\r\n    return dataset\r\n    \r\n                \r\nwith tf.variable_scope(\"scope_0\"):\r\n        dataset_fn = scope_1()\r\n\r\nwith tf.variable_scope(\"iterator\"):\r\n    # Define iterator from_string_handle. In general it is useful to have\r\n    # this kind of iterator if one wants to switch between train and validation\r\n    # within the training loop.        \r\n    iterator_t = dataset_fn.make_initializable_iterator()\r\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\r\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \r\n                                                iterator_t.output_types,\r\n                                                iterator_t.output_shapes)\r\n    \r\n    def get_next_item():\r\n        next_elem = iterator.get_next(name=\"next_element\")\r\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\r\n        return x, y    \r\n        \r\nwith tf.Session() as sess:\r\n\r\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\r\n    handle_t = sess.run(iterator_t.string_handle())\r\n    # Run data iterator initialisation\r\n    sess.run(iterator_t.initializer)\r\n    print(sess.graph.get_operations()) \r\n    while True:\r\n        try:\r\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\r\n        except tf.errors.OutOfRangeError:\r\n                        print(\"End of training dataset.\")\r\n                        break        \r\n    print()\r\n    print(\"global vars: {}\".format(tf.global_variables()))\r\n    print(\"local vars: {}\".format(tf.local_variables()))\r\n    print(tf.get_default_graph().get_name_scope())\r\n```\r\n\r\nand the output\r\n\r\n```\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>\r\n=============\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>\r\nTensor(\"arg0:0\", shape=(), dtype=int32)\r\n[<tf.Operation 'Placeholder' type=Placeholder>, <tf.Operation 'scope_0/scope_1/Placeholder' type=Placeholder>, <tf.Operation 'scope_0_1/scope_1/Placeholder' type=Placeholder>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros/shape_as_tensor' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Fill>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/scope_1/Const' type=Const>, <tf.Operation 'scope_0_1/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0_1/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/tensors/component_0' type=Const>, <tf.Operation 'scope_0_1/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/shape_as_tensor' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Fill>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/batch_size' type=Const>, <tf.Operation 'scope_0_1/count' type=Const>, <tf.Operation 'iterator/Iterator' type=Iterator>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDataset' type=BatchDataset>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandle' type=IteratorFromStringHandle>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]\r\n(array([[0.22]], dtype=float32), array([-1], dtype=int32))\r\n(array([[0.22]], dtype=float32), array([-2], dtype=int32))\r\n(array([[0.22]], dtype=float32), array([-3], dtype=int32))\r\n(array([[0.22]], dtype=float32), array([-4], dtype=int32))\r\n(array([[0.22]], dtype=float32), array([-5], dtype=int32))\r\nEnd of training dataset.\r\n\r\nglobal vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>]\r\nlocal vars: []\r\n```", "@rohan100jain I have identified the reason for the segmentation fault. Please have a look at my comments above. ", "Nagging Assignee @rohan100jain: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looking at the documentation for scatter_nd_update (https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update) seems like indexing into a zero rank tensor is sort of invalid? Adding Eugene for more info.", "Thanks for reporting.  I found the cause of the problem and will send a fix."]}, {"number": 22012, "title": "[Bug] Discrepancy in tf.keras and keras in setting model.trainable = False and then compiling", "body": "### System information\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.10.1-0-g4dcfddc5d1 1.10.1\r\n- Bazel version: N/A\r\n- CUDA/cuDNN version: CUDA9.1, cuDNN7.0\r\n- GPU model and memory: TITAN V\r\n- Exact command to reproduce: python3 compare.py\r\n- Mobile device: N/A\r\n\r\n### Describe the problem\r\nThis is a toy case for training GAN problem.\r\nWhen I run the code shown below in keras2.2.2, I get\r\n```\r\nWARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\r\n```\r\nonly at the beginning of the training once.\r\nHowever, I run this code in tensorflow1.10.1, the warning raises at every iteration.\r\nAlthough it seems that the model is appropriately learned (I can make sure the weight is freezed by the result of `.summary()`), raising too many warning is not torelable.\r\n\r\nHere is the complete log.\r\n[tf.keras ver](https://github.com/tensorflow/tensorflow/files/2342979/tf_keras.log)\r\n[keras ver](https://github.com/tensorflow/tensorflow/files/2342980/keras.log)\r\n\r\nI see the same problem is posted in Stackoverflow\r\nhttps://stackoverflow.com/questions/50468940/tensorflow-1-8-tf-keras-gives-different-result-in-dcgan-from-keras\r\n\r\n```\r\nimport numpy as np\r\n\r\n# use keras\r\nfrom keras.layers import Dense, Input\r\nfrom keras.models import Model\r\n\r\n# use tf.keras\r\n# from tensorflow.keras.layers import Dense, Input\r\n# from tensorflow.keras.models import Model\r\n\r\n# define input\r\nnoise = Input(shape=(10,))\r\nx = Input(shape=(100,))\r\n\r\n# define generator and discriminator\r\ngen = Dense(100)\r\ndis = Dense(1)\r\n\r\ny = dis(x)\r\ndis_model = Model(x, y)\r\ndis_model.compile(optimizer='rmsprop', loss='mse')\r\ndis_model.summary()\r\n\r\nz = dis_model(gen(noise))\r\ndis_model.trainable = False\r\ncombined_model = Model(noise, z)\r\ncombined_model.compile(optimizer='rmsprop', loss='mse')\r\ncombined_model.summary()\r\n\r\nfor i in range(3):\r\n    dis_model.train_on_batch(x=np.random.rand(10, 100),\r\n                             y=np.random.rand(10, 1))\r\n    combined_model.train_on_batch(x=np.random.rand(10, 10),\r\n                             y=np.random.rand(10, 1))\r\n\r\n```\r\n", "comments": ["@naoto0804 \r\nKeras model training will update params in ```model._collected_trainable_weights``` which is produced at ```model.compile```.\r\nWhen you call ```dis_model.trainable = False``` to freeze ```dis_model```, you don't compile ```dis_model```, so two things happens:\r\n* ```dis_model._collected_trainable_weights``` still includes old trainable params.\r\n* ```dis_model.trainable_weights``` will return null, as you set all params non-trainable.\r\n\r\nWhen you call ```dis_model.train_on_batch```, the params in ```dis_model._collected_trainable_weights``` will be updated according gradient descent, so it meets your requirements. But Keras found ```dis_model._collected_trainable_weights``` and ```dis_model.trainable_weights``` are inconsistent, so print warning in case this is not by design.\r\n\r\nWhen you call ```combined_model.compile```, ```combined_model._collected_trainable_weights``` will be generated, and it doesn't includes the params in ```dis_model``` as they are marked as non-trainable.\r\n\r\nTo sum up, this warning is to prevent misuse. In your case, you code makes sense. I think we just need to reduce log frequency in ```tf.keras``` to once only, to prevent it floods the screen. I sent #22296 to fix it. Thanks. ", "I'm bit new to tf/keras. Thank you so much for clear explanation and PR!", "Note that you can also suppress the warning by setting back `dis_model.trainable = True` once you've compiled `combined_model`.", "That's really cool and straightforward way, thank you for the suggestion!", "> Note that you can also suppress the warning by setting back `dis_model.trainable = True` once you've compiled `combined_model`.\r\n\r\nthis does seem to work "]}, {"number": 22011, "title": "bug in string_input_producer", "body": "https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/training/input.py\r\n\r\nline169:\r\nelement_shape = input_tensor.shape[1:].merge_with(element_shape)\r\n input_tensor.shape[1:] must be compatable with element_shape\r\n\r\nline252:\r\nelement_shape = []\r\n\r\nso, no matter what kind of shape you defined in input_tensor.shape[1:], it will raise an error!\r\n\r\nimport tensorflow as tf\r\n\r\ntest_list_1 = ['andy', 'jerry', 'lala', 'coco', 'mimi']\r\ntest_list_2 = ['andy', 'jerry', 'lala', 'coco', 'mimi']\r\ntest_tensor_1 = tf.convert_to_tensor(test_list_1)\r\ntest_tensor_2 = tf.convert_to_tensor(test_list_2)\r\n\r\nres = tf.train.string_input_producer([test_tensor_1, test_tensor_2])\r\n\r\nsess = tf.Session()\r\n\r\ntf.train.start_queue_runners(sess = sess)\r\n\r\nprint(sess.run(res))\r\n\r\n#ValueError: Shapes (5,) and () are not compatible", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@andyhujinzhao The docstring:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/training/input.py#L208\r\n\r\nspecifies that the input is `A 1-D string tensor`.", "As @yongtang pointed out, this is working as intended.\r\n\r\n(Note that this API is now deprecated, and will be removed in TensorFlow 2.0. We recommend that new code uses `tf.data`.)"]}, {"number": 22010, "title": "Updated `Dataset.shard` documentation", "body": "Updated the `Dataset.shard` documentation to warn of the potential implications of invoking `Dataset.repeat` before `Dataset.shuffle`.", "comments": ["I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 22009, "title": "tf.get_variable returns Tensor instead of Variable", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n\r\nTF checkpoint I have built\r\n```\r\n/tmp/tensorflow# git log   \r\ncommit 09792df012c22622324f085f46edde33006c7355\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Sun Aug 26 02:07:11 2018 -0700\r\n\r\n    compat: Update forward compatibility horizon to 2018-08-26\r\n    \r\n    PiperOrigin-RevId: 210266798\r\n```\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.5)\r\nprotobuf (3.6.1)\r\ntensorflow (1.10.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Aug 29 19:57:14 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n```\r\n**Bazel** version\r\n```\r\n$ bazel version\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nBuild label: 0.16.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jul 31 17:01:24 2018 (1533056484)\r\nBuild timestamp: 1533056484\r\nBuild timestamp as int: 1533056484\r\n```\r\n\r\n**CUDNN** version:\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\r\nCuda compilation tools, release 9.2, V9.2.148\r\n```\r\n\r\n**GPU**: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS\r\n\r\n- **Exact command to reproduce**:\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\ndef scope_1():\r\n    print(\"DS1 SCOPE =============\")\r\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        x = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\r\n                               , trainable=False, use_resource=True)             \r\n        print(\"graph: {}\".format(x.graph))\r\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\" name: {}\".format(x.name))\r\n        print(\"  var: {}\".format(str(x)))\r\n        current_scope = tf.get_variable_scope()       \r\n        assign_one = tf.assign(x, 1, name=\"x_is_one\")\r\n    \r\n    def scope_2(inputs, label):        \r\n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\"DS1 SCOPE =============\")\r\n        with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        #with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\r\n            y = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\r\n                                   , trainable=False, use_resource=True)         \r\n            print(\"graph: {}\".format(y.graph))\r\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n            print(\" name: {}\".format(y.name))\r\n            print(\"  var: {}\".format(str(y)))\r\n            print(\"=============\")\r\n            \r\n            assign_two = tf.assign(y, inputs+1, name=\"inputs_plus_1\")\r\n            with tf.control_dependencies([assign_two]):\r\n                return y, label\r\n            #return x,label\r\n    \r\n    # test that original x is mutable\r\n    with tf.control_dependencies([assign_one]):\r\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [0,0,0,0,0]))\r\n                    .map(scope_2)\r\n                    .batch(2)\r\n                    .repeat(10)        \r\n                    )\r\n    \r\n                \r\nwith tf.variable_scope(\"scope_0\"):\r\n        dataset_fn = scope_1\r\n\r\nwith tf.Session() as sess:\r\n    dataset_fn()\r\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\r\n    print(sess.graph.get_operations())    \r\n    print()\r\n    print(\"global vars: {}\".format(tf.global_variables()))\r\n    print(\"local vars: {}\".format(tf.local_variables()))\r\n    print(tf.get_default_graph().get_name_scope())\r\n\r\n```\r\n\r\n### Describe the problem\r\nI am trying to create a function that would modify a Tensor within a pipeline of Dataset API.\r\nThe scoping may seem weird, but that is minimal example that shows the problem that I created from my project.but somehow it is using `ReadVariableOp` that only \r\n\r\nLooks like when I fetch a variable using `tf.get_variable` (within def scope_2) it returns immutable Tensor, whereas the original Variable is mutable. In `scope_1` I am able to run `x.assign(0)` but then within `scope_2` (that is invoked in dataset.map) it throws an `AttributeError: 'Tensor' object has no attribute 'assign'`\r\n\r\nAccording to the docs https://www.tensorflow.org/api_docs/python/tf/get_variable `tf.get_variable` should return a Variable not a Tensor.\r\n```\r\nReturns:\r\nThe created or existing Variable (or PartitionedVariable, if a partitioner was used).\r\n```\r\n\r\n### Source code / logs\r\n\r\n```\r\n2018-09-01 23:26:58.452631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-01 23:26:58.453216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 9.98GiB\r\n2018-09-01 23:26:58.453237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-09-01 23:26:58.653750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-01 23:26:58.653789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-09-01 23:26:58.653796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-09-01 23:26:58.654003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9644 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-09-01 23:26:58.758539: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f1ebf351400>\r\nscope: scope_1\r\n name: scope_1/x:0\r\n  var: <tf.Variable 'scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f1ebf351400>\r\nscope: scope_1\r\n name: ReadVariableOp:0\r\n  var: Tensor(\"ReadVariableOp:0\", shape=(), dtype=float32)\r\n=============\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 50, in <module>\r\n    dataset_fn()\r\n  File \"bug.py\", line 40, in scope_1\r\n    .map(scope_2)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1005, in map\r\n    return MapDataset(self, map_func)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2216, in __init__\r\n    map_func, \"Dataset.map()\", input_dataset)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1473, in __init__\r\n    self._function.add_to_graph(ops.get_default_graph())\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\r\n    self._capture_by_value, self._caller_device)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 865, in func_graph_from_py_func\r\n    outputs = func(*func_graph.inputs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1411, in tf_data_structured_function_wrapper\r\n    ret = func(*nested_args)\r\n  File \"bug.py\", line 32, in scope_2\r\n    assign_two = tf.assign(y, inputs+1, name=\"inputs_plus_1\")\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 222, in assign\r\n    return ref.assign(value, name=name)\r\nAttributeError: 'Tensor' object has no attribute 'assign'\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "I've updated the description with Bazel Version, CUDA/CuDNN version, and GPU model.", "I got it working with the piece of code I have commented out in the original issue. I do not know why it did not work for me before. Either way, the solution was to use the same scope again but with reference to it by variable not name, so use `current_scope` not `\"scope_1\"`.\r\n\r\nI find it strange that although I use `tf.get_variable` a new variable is created with the same name as original but followed by `_1`. \r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\ndef scope_1():\r\n    print(\"DS1 SCOPE =============\")\r\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        x = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\r\n                               , trainable=False, use_resource=True)             \r\n        print(\"graph: {}\".format(x.graph))\r\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\" name: {}\".format(x.name))\r\n        print(\"  var: {}\".format(str(x)))\r\n        current_scope = tf.get_variable_scope()       \r\n        assign_one = tf.assign(x, 1.0, name=\"x_is_one\")\r\n    \r\n    def scope_2(inputs, label):        \r\n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\"DS1 SCOPE =============\")\r\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\r\n            y = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\r\n                                   , trainable=False, use_resource=True)         \r\n            print(\"graph: {}\".format(y.graph))\r\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n            print(\" name: {}\".format(y.name))\r\n            print(\"  var: {}\".format(str(y)))\r\n            print(\"=============\")\r\n            \r\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\r\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0)))\r\n            with tf.control_dependencies([assign_two]):\r\n                return y.read_value(), label\r\n            #return x,label\r\n    \r\n    # test that original x is mutable\r\n    with tf.control_dependencies([assign_one]):\r\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\r\n                    .map(scope_2)\r\n                    .batch(1)\r\n                    .repeat(1)        \r\n                    )\r\n    return dataset\r\n    \r\n                \r\nwith tf.variable_scope(\"scope_0\"):\r\n        dataset_fn = scope_1()\r\n\r\nwith tf.variable_scope(\"iterator\"):\r\n    # Define iterator from_string_handle. In general it is useful to have\r\n    # this kind of iterator if one wants to switch between train and validation\r\n    # within the training loop.        \r\n    iterator_t = dataset_fn.make_initializable_iterator()\r\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\r\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \r\n                                                iterator_t.output_types,\r\n                                                iterator_t.output_shapes)\r\n    \r\n    def get_next_item():\r\n        next_elem = iterator.get_next(name=\"next_element\")\r\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\r\n        return x, y    \r\nwith tf.Session() as sess:\r\n    handle_t = sess.run(iterator_t.string_handle())\r\n    # Run data iterator initialisation\r\n    sess.run(iterator_t.initializer)\r\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\r\n    print(sess.graph.get_operations()) \r\n    while True:\r\n        try:\r\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\r\n        except tf.errors.OutOfRangeError:\r\n                        print(\"End of training dataset.\")\r\n                        break        \r\n    print()\r\n    print(\"global vars: {}\".format(tf.global_variables()))\r\n    print(\"local vars: {}\".format(tf.local_variables()))\r\n    print(tf.get_default_graph().get_name_scope())\r\n```\r\nLOG\r\n```\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f56a3a98470>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f56a3a98470>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\n=============\r\n2018-09-02 19:37:43.456845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-02 19:37:43.457271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.29GiB\r\n2018-09-02 19:37:43.457291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-09-02 19:37:43.662214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-02 19:37:43.662252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-09-02 19:37:43.662260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-09-02 19:37:43.662470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9951 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-09-02 19:37:43.759025: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n[<tf.Operation 'Const' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/initial_value' type=Const>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/scope_1/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/tensors/component_0' type=Const>, <tf.Operation 'scope_0/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/initial_value' type=Const>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/batch_size' type=Const>, <tf.Operation 'scope_0/drop_remainder' type=Const>, <tf.Operation 'scope_0/count' type=Const>, <tf.Operation 'iterator/IteratorV2' type=IteratorV2>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDatasetV2' type=BatchDatasetV2>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandleV2' type=IteratorFromStringHandleV2>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]\r\n(array([2.], dtype=float32), array([-1], dtype=int32))\r\n(array([3.], dtype=float32), array([-2], dtype=int32))\r\n(array([4.], dtype=float32), array([-3], dtype=int32))\r\n(array([5.], dtype=float32), array([-4], dtype=int32))\r\n(array([6.], dtype=float32), array([-5], dtype=int32))\r\nEnd of training dataset.\r\n\r\nglobal vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>]\r\nlocal vars: []\r\n```"]}, {"number": 22008, "title": "After install 1.10.1, it doesn't work now", "body": "1. Use \r\n\r\npip3 install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.10.1-py3-none-any.whl \r\n\r\nto install\r\n\r\n2. just run a hello-world:\r\n\r\n```\r\nFile \"tf_beginner.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"/Users/ /Desktop/dev/AI/TF_ENV/lib/python3.7/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/Users/ /Desktop/dev/AI/TF_ENV/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/ /Desktop/dev/AI/TF_ENV/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/ /Desktop/dev/AI/TF_ENV/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 114\r\n    def TFE_ContextOptionsSetAsync(arg1, async):\r\n                                             ^\r\nSyntaxError: invalid syntax\r\n\r\n``` ", "comments": ["#20517", "Yup, this seems to be an issue with Python 3.7 compatibility. Python 3.6 should work. \r\n\r\nClosing as a duplicate of #20517", "Specific issue is that `async` became a keyword in 3.7, but was being used as a variable by tensorflow interface. Fixed by https://github.com/tensorflow/tensorflow/pull/21202 (renames `async` to `enable`)", "any solution?", "Use python 3.6 or build from source for 3.7 for now"]}, {"number": 22007, "title": "Newly included absl headers are missing from the include path", "body": "[Newly included](https://github.com/tensorflow/tensorflow/commit/3cb3a450ed845c4602080f43d7bb6cfade298a22) `absl` headers are present in the `tf-nightly` Python package but do not seem to be part of the include path, and now custom op plugin compilation fails.\r\n\r\nThis has been broken between `tf_nightly-1.11.0.dev20180830` and `tf_nightly-1.11.0.dev20180831`.\r\n\r\n```\r\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -DHAVE_CUDA=1 -DHAVE_NCCL=1 -DHOROVOD_GPU_ALLREDUCE='N' -I/usr/local/cuda/include -I/usr/include/python2.7 -c horovod/tensorflow/mpi_ops.cc -o build/temp.linux-x86_64-2.7/horovod/tensorflow/mpi_ops.o -std=c++11 -fPIC -O2 -I/usr/local/include -pthread -Wl,-rpath -Wl,/usr/local/lib -Wl,--enable-new-dtags -L/usr/local/lib -lmpi -I/usr/local/lib/python2.7/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n    cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++\r\n    In file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/lib/core/status.h:24:0,\r\n                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def_builder.h:25,\r\n                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\r\n                     from horovod/tensorflow/mpi_ops.cc:22:\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/lib/core/stringpiece.h:34:38: fatal error: absl/strings/string_view.h: No such file or directory\r\n    compilation terminated.\r\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\r\n```\r\n\r\ncc @yunxing ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I saw same issue for the horovod installation.\r\nWorkaround was to clone abseil-cpp from git. And making soft link of absl directory to your include path.", "cc @gunan ", "Hello @mhorikos \r\ndoes \r\n\r\n> abseil-cpp from git\r\n\r\n mean this one\r\nhttps://github.com/abseil/abseil-cpp\r\n\r\nThank you.", "This is being investigated internally.\r\nWE are currently trying to roll back the change to fix custom ops, and then we will try moving forward with a fix.", "@willSapgreen ", "Hello  @willSapgreen \r\nYes, it is.", "Hello @mhorikos , Thanks!!!", "@gunan, thanks!", "Trying to build tensorflow as a standalone project for C++ and getting a similar error:\r\n\r\ng++ -std=c++11 -Wl,-rpath='$ORIGIN/lib' -Iinclude -Llib main.cpp -ltensorflow_cc -o exec\r\nIn file included from include/tensorflow/core/platform/tensor_coding.h:22:0,\r\n                 from include/tensorflow/core/framework/resource_handle.h:19,\r\n                 from include/tensorflow/core/framework/allocator.h:24,\r\n                 from include/tensorflow/core/framework/tensor.h:20,\r\n                 from include/tensorflow/cc/framework/ops.h:21,\r\n                 from include/tensorflow/cc/client/client_session.h:24,\r\n                 from main.cpp:9:\r\ninclude/tensorflow/core/lib/core/stringpiece.h:34:38: fatal error: absl/strings/string_view.h: No such file or directory\r\n #include \"absl/strings/string_view.h\"\r\n\r\nAny workaround for this? Thanks!", "I don't think we have support for that usage. All of our code has a lot of dependencies, and we recommend using bazel to build our code.\r\n\r\nFor the initial issue reported, I think @angersson submitted a fix. @alsrgv could you try the latest nightly package?", "@gunan, the issue is fixed, thanks!\r\n\r\nI do see a different error in Travis CI, very similar to #19375:\r\n```\r\n/usr/bin/python: Relink `/usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so' with `/lib/x86_64-linux-gnu/librt.so.1' for IFUNC symbol `clock_gettime'\r\n```\r\n\r\nI will look into it separately next week, after I'm back from my vacation.\r\n\r\n**Update:** The integration test issue is resolved.", "hello @mhorikos, can you describe more detailed?  I have the same error, thanks.", "Hello @dingevin \r\n\r\nIn your tensorflow installed top directory (that has Eigen, external, tensorflow, third_party, unsupported and etc.),\r\n\r\n$ git clone https://github.com/abseil/abseil-cpp.git\r\n$ ln -s abseil-cpp/absl ./absl\r\n\r\nThen you can install horovod\r\n", "> Hello @dingevin\r\n> \r\n> In your tensorflow installed top directory (that has Eigen, external, tensorflow, third_party, unsupported and etc.),\r\n> \r\n> $ git clone https://github.com/abseil/abseil-cpp.git\r\n> $ ln -s abseil-cpp/absl ./absl\r\n> \r\n> Then you can install horovod\r\n\r\nhi \uff0c @mhorikos\uff0c thank you very much.\r\n\r\n\r\n", "hello @tensorflowbutler , the same issue was disappear when I compiled older versions(1.8.0). At the beginning i was just executed the command: git clone https://github.com/tensorflow/tensorflow.git,  i thought it git the latest version. \r\n", "After updating to MacOS 10.14 and thus Xcode 10, I'm hitting a possibly related linker error when trying to load custom ops:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./safety/debate/test_azero.py\", line 4, in <module>\r\n    from safety.debate import azero\r\n  File \"/Users/irving/openai/safety/safety/debate/azero.py\", line 5, in <module>\r\n    from safety.debate import gomoku\r\n  File \"/Users/irving/openai/safety/safety/debate/gomoku.py\", line 9, in <module>\r\n    gomoku_value = utils.load_debate_ops().gomoku_value\r\n  File \"/Users/irving/openai/safety/safety/core/utils.py\", line 212, in load_debate_ops\r\n    return tf.load_op_library(path)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/irving/openai/safety/debate_ops.cpython-36m-darwin.so, 6): Symbol not found: __ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceENSt3__117basic_string_viewIcNS3_11char_traitsIcEEEEPNS3_12basic_stringIcS6_NS3_9allocatorIcEEEE\r\n  Referenced from: /Users/irving/openai/safety/debate_ops.cpython-36m-darwin.so\r\n  Expected in: flat namespace\r\n in /Users/irving/openai/safety/debate_ops.cpython-36m-darwin.so\r\n```\r\n\r\nThe custom op thinks the symbol is \r\n\r\n```\r\ntensorflow::GetNodeAttr(tensorflow::AttrSlice const&, std::__1::basic_string_view<char, std::__1::char_traits<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*)\r\n```\r\n\r\nbut the actual symbol is\r\n\r\n```\r\ntensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::string_view, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*)\r\n```\r\n\r\nI.e., `absl::string_view` vs. `basic_string_view`.", "Nagging Assignee @yunxing: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@girving I'm facing the same issue, did you find a solution?", "@liamuk No solution, but I haven't tried in a while.  Unfortunately I can't build at all now due to https://github.com/tensorflow/tensorflow/issues/22902, so I don't know if it's still an issue.", "@girving Ah. If you hit it again, here's an issue I opened for it (with a workaround): https://github.com/tensorflow/tensorflow/issues/23561", "> Hello @dingevin\r\n> \r\n> In your tensorflow installed top directory (that has Eigen, external, tensorflow, third_party, unsupported and etc.),\r\n> \r\n> $ git clone https://github.com/abseil/abseil-cpp.git\r\n> $ ln -s abseil-cpp/absl ./absl\r\n> \r\n> Then you can install horovod\r\n\r\nHello, I am working on windows10 visual studio 2015, how can I build the links?\r\nI think there must be a better way than just adding the include directory one by one in those vs projects.", "@LJXLJXLJX The file struct should looks like below:\r\n![image](https://user-images.githubusercontent.com/1705364/51222977-9c572b00-1905-11e9-9983-8eb340f70896.png)\r\n", "This probably helps https://github.com/tensorflow/tensorflow/commit/914f68bfc0d7629496cd5ef6a6104efc94b6eecc", "Hi @alsrgv! \r\nWe are checking to see if you still need help in this issue , Have you tried latest stable version TF 2.6  yet? Please create a new issue if the issue is replicating in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22007\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22007\">No</a>\n"]}, {"number": 22006, "title": "Append error msg to exception for test.assert_* method", "body": "Fix #21988.", "comments": ["@yifeif Hi, yifei. Could you take a look? Thanks.", "Gentle ping @yifeif ", "Thank you, yifei. I fixed it :-)"]}, {"number": 22005, "title": "Running inference with TensorRT optimized protobuf using C++ API", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.15.1\r\n- **GCC/Compiler version (if compiling from source)**: 5\r\n- **CUDA/cuDNN version**: 9.0 / 7.0\r\n- **GPU model and memory**:\r\n\r\n### Describe the problem\r\nI successfully build from source TF 1.10.1 with TensorRT 3.04 support. Using the Python `tensorflow.contrib.tensorrt` API, I can create a protobuf that contains some TRTEngineOp operations, giving me a substantial inference speed-up.\r\nHowever, now I need to run this optimized protobuf via a C++ API. I build the required .so libraries for the C++ inference API (such as libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so). I can successfully load and run inference on the native TF protobuf, but not on the TRT optimized protobuf. \r\nIt gives me an error saying that `TRTEngineOp not available.` Looking through Stackoverflow, I found some suggestions to use `TF_LoadLibrary` in the C API to load the `python/ops/_trt_engine_op.so` available in `tensorflow/contrib/tensorrt`. When I do this, the `TRTEngineOp` not available error disappears, but now it complains that it has no registered kernels. I noticed that the `python/ops/_trt_engine_op.so` Bazel target does not include any kernel definition, which explains why the TF_LoadLibrary doesn't find any kernels I guess. I am wondering how the Python API loads this. Can anyone help? Is it possible to run TensorRT optimized protobufs in C++ ?\r\n\r\n### Steps to reproduce\r\n1. Create an optimized protobuf in python, following for example this blog from [Google](https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html)\r\n\r\n```\r\n# Reserve memory for TensorRT inference engine\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = number_between_0_and_1)\r\n...  \r\ntrt_graph = trt.create_inference_graph(\r\n                 input_graph_def = frozen_graph_def, \r\n                 outputs = output_node_name,\r\n                 max_batch_size=batch_size,\r\n                 max_workspace_size_bytes=workspace_size,\r\n                 precision_mode=precision)  # Get optimized graph\r\n```\r\nthen save as protobuf file.\r\n\r\n2. Load optimized protobuf in C++, where we have access to the libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so precompiled.\r\n```\r\n#include <tensorflow/core/graph/graph.h>\r\n#include <tensorflow/core/graph/default_device.h>\r\n#include <cuda_runtime.h>\r\n#include tensorflow/c/c_api.h\r\n\r\nauto status = TF_NewStatus();\r\nTF_LoadLibrary(getPathToTrtEngineOpSo(), status);\r\n\r\ntensorflow::SessionOptions sessionOptions;\r\nsessionOptions.config.set_allow_soft_placement(getAllowSoftPlacement());\r\nsessionOptions.config.set_log_device_placement(getLogDevicePlacement());\r\nauto gpu_options = sessionOptions.config.mutable_gpu_options();\r\ngpu_options->set_allow_growth(getAllowMemoryGrowth());\r\ngpu_options->set_per_process_gpu_memory_fraction(getMemoryFraction());\r\n_session.reset(tensorflow::NewSession(sessionOptions));\r\n\r\ntensorflow::GraphDef graphDef;\r\ncheckStatusTF(tensorflow::ReadBinaryProto(tensorflow::Env::Default(),\r\n                                                  getModel(),\r\n                                                  &graphDef),\r\n                      _logger);\r\ntensorflow::graph::SetDefaultDevice(getDevice(), &graphDef);\r\ncheckStatusTF(_session->Create(graphDef), _logger);\r\n```\r\nfollowed by the appropriate run call.\r\n\r\nThe libtensorflow_cc.so, libtensorflow_framework.so and libtensorflow.so are unmodified targets from Tensorflow repostory r1.10.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "I would like to add, I recompiled libtensorflow_cc.so with a modified BUILD file with added TensorRT dependency targets, as suggested [here](https://stackoverflow.com/questions/50125889/c-tensorflow-api-with-tensorrt/52111461#52111461). However, when I try to load this target, it complains about the following when running the application:\r\n`\r\n2018-09-02 20:31:55.750531: F tensorflow/core/common_runtime/device_factory.cc:77] Duplicate registration of device factory for type GPU with the same priority 210\r\n`\r\nSo, I am still unable to run a TensorRT optimized graph using the C++ API. It would be great if someone could elaborate whether this is a current limitation, or a bug etc.", "@protoget could you perhaps redirect?", "`+1`", "@fferroni Have you tried all 4 of the workarounds suggested on the stack overflow page?", "@dhingratul in the end, it worked for me by adding required dependencies to the libtensorflow_cc.so target (as per Stackoverflow) but _not_ compiling monolithically. I also do not need to use the TF_LoadLibrary. Then, I do not get this Duplicate Registration thing and can run the graph. Still, feels like an ugly hack since TensorRT should really be included as an op/kernel if you set TensorRT support to true.", "@fferroni I was able to make the TF_LoadLibrary workaround work with r1.11, but my inference times are very slow(3x slower), did you observe this in your case?", "> @dhingratul in the end, it worked for me by adding required dependencies to the libtensorflow_cc.so target (as per Stackoverflow) but _not_ compiling monolithically. I also do not need to use the TF_LoadLibrary. Then, I do not get this Duplicate Registration thing and can run the graph. Still, feels like an ugly hack since TensorRT should really be included as an op/kernel if you set TensorRT support to true.\r\n\r\n@fferroni I tried this, it doesn't work with r1.11\r\n", "> > @dhingratul in the end, it worked for me by adding required dependencies to the libtensorflow_cc.so target (as per Stackoverflow) but _not_ compiling monolithically. I also do not need to use the TF_LoadLibrary. Then, I do not get this Duplicate Registration thing and can run the graph. Still, feels like an ugly hack since TensorRT should really be included as an op/kernel if you set TensorRT support to true.\r\n> \r\n> @fferroni I tried this, it doesn't work with r1.11\r\n\r\nI used TF 1.10.1, I have not tried with 1.11 yet, but these are the diffs that worked for me:\r\n```\r\ndiff --git a/tensorflow/BUILD b/tensorflow/BUILD\r\nindex 518c2b0..154cfc3 100644\r\n--- a/tensorflow/BUILD\r\n+++ b/tensorflow/BUILD\r\n@@ -568,6 +568,8 @@ tf_cc_shared_object(\r\n         \"//tensorflow/cc:scope\",\r\n         \"//tensorflow/cc/profiler\",\r\n         \"//tensorflow/core:tensorflow\",\r\n+       \"//tensorflow/contrib/tensorrt:trt_conversion\",\r\n+        \"//tensorflow/contrib/tensorrt:trt_engine_op_kernel\",\r\n     ],\r\n )\r\n \r\ndiff --git a/tensorflow/contrib/tensorrt/BUILD b/tensorflow/contrib/tensorrt/BUILD\r\nindex 70ce4a4..14ce991 100644\r\n--- a/tensorflow/contrib/tensorrt/BUILD\r\n+++ b/tensorflow/contrib/tensorrt/BUILD\r\n@@ -73,6 +73,7 @@ cc_library(\r\n     name = \"trt_engine_op_kernel\",\r\n     srcs = [\r\n         \"kernels/trt_engine_op.cc\",\r\n+       \"ops/trt_engine_op.cc\",\r\n     ],\r\n     hdrs = [\r\n         \"kernels/trt_engine_op.h\",\r\n```\r\nI used Bazel 0.15.1. Also, do not add the monolithic argument to the build command as you will get the double registration problem.\r\nIn terms of inference time, I observe a significant speedup.", "> > > @dhingratul in the end, it worked for me by adding required dependencies to the libtensorflow_cc.so target (as per Stackoverflow) but _not_ compiling monolithically. I also do not need to use the TF_LoadLibrary. Then, I do not get this Duplicate Registration thing and can run the graph. Still, feels like an ugly hack since TensorRT should really be included as an op/kernel if you set TensorRT support to true.\r\n> > \r\n> > \r\n> > @fferroni I tried this, it doesn't work with r1.11\r\n> \r\n> I used TF 1.10.1, I have not tried with 1.11 yet, but these are the diffs that worked for me:\r\n> \r\n> ```\r\n> diff --git a/tensorflow/BUILD b/tensorflow/BUILD\r\n> index 518c2b0..154cfc3 100644\r\n> --- a/tensorflow/BUILD\r\n> +++ b/tensorflow/BUILD\r\n> @@ -568,6 +568,8 @@ tf_cc_shared_object(\r\n>          \"//tensorflow/cc:scope\",\r\n>          \"//tensorflow/cc/profiler\",\r\n>          \"//tensorflow/core:tensorflow\",\r\n> +       \"//tensorflow/contrib/tensorrt:trt_conversion\",\r\n> +        \"//tensorflow/contrib/tensorrt:trt_engine_op_kernel\",\r\n>      ],\r\n>  )\r\n>  \r\n> diff --git a/tensorflow/contrib/tensorrt/BUILD b/tensorflow/contrib/tensorrt/BUILD\r\n> index 70ce4a4..14ce991 100644\r\n> --- a/tensorflow/contrib/tensorrt/BUILD\r\n> +++ b/tensorflow/contrib/tensorrt/BUILD\r\n> @@ -73,6 +73,7 @@ cc_library(\r\n>      name = \"trt_engine_op_kernel\",\r\n>      srcs = [\r\n>          \"kernels/trt_engine_op.cc\",\r\n> +       \"ops/trt_engine_op.cc\",\r\n>      ],\r\n>      hdrs = [\r\n>          \"kernels/trt_engine_op.h\",\r\n> ```\r\n> \r\n> I used Bazel 0.15.1. Also, do not add the monolithic argument to the build command as you will get the double registration problem.\r\n> In terms of inference time, I observe a significant speedup.\r\n\r\nI used Bazel 0.16.1,TensorRT 4.0.1.6,tensorflow 1.10.1. I modify tensorflow/BUILD and /tensorflow/contrib/tensorrt/BUILD.  \r\nI used bazel command:\r\nbazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so\r\n\r\nTrying to run the graph in c++ fails with the following error\uff1a\r\n the error \"Not found: Op type not registered 'TRTEngineOp'\"\r\n\r\ncan you help me ?\r\nThanks.", "@fferroni Hi, did you finally figure this out? Did you compile both libtensorflow_cc.so and libtensorflow_framework.so? "]}, {"number": 22003, "title": "Fix tensorflow master build failure with verbs", "body": "This fix tries to address the issue in #21999 where\r\ntensorflow master build failed with verbs. The issue was caused\r\nby StringPiece replaced with `absl::string_view`\r\n\r\nThis fix fixes #21999\r\n\r\nThis fix fixes #22113\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 22002, "title": "Feature Request: [tf.data] Access to the size of prefetch buffer", "body": "**(TL;DR)** Add a new API that can tell the size of the prefetch buffer, similar to [FIFOQueue.size()](https://www.tensorflow.org/api_docs/python/tf/FIFOQueue#size).\r\n\r\n### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ALL\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.10.1 (as of current master, it seems unimplemented yet)\r\n- **Python version**: ALL\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\n`tf.data.Dataset.prefetch()` allows to have an efficient input pipeline by making input processing operations runnable in parallel to downstream GPU operations. It has a similar semantic to [FIFO Queues](https://www.tensorflow.org/api_docs/python/tf/FIFOQueue).\r\n\r\nCurrently, there is no way to access the number of elements in the queue or prefetch buffers. We may want to add some methods or operations for this in the [PrefetchDataset](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/data/ops/dataset_ops.py#L2348) class. Traditional TF queue objects have [`size()`](https://www.tensorflow.org/api_docs/python/tf/FIFOQueue#size) method that returns a *scalar tensor* for the number of elements in the queue. It is very helpful to debug whether there is any performance issue on the input pipeline or preprocessing.\r\n\r\nFor example:\r\n\r\n```python\r\n  dataset = tf.data.Dataset.from_generator(...)\r\n  dataset = dataset.shuffle(buffer_size=10000).repeat()\r\n\r\n  dataset = dataset.prefetch(1024)             # returns an instance of PrefetchDataset\r\n  prefetch_size_op = dataset.size()            # [!] Proposal. Needs to be named properly\r\n  prefetch_capacity_op = dataset._buffer_size         # this is already available, but needs to be exposed public, e.g. dataset.capacity()\r\n\r\n  dataset = dataset.batch(32)\r\n  # ...\r\n```\r\n\r\nThanks!", "comments": ["@mrry How do you think?", "@shivaniag is working on a feature that will expose the buffer size as a summary. It probably won't have identical syntax to the example (especially because there may be multiple iterators and hence buffers associated with a single `Dataset.prefetch()` call), but it will enable the same kind of debugging.", "@facaiy You can now use `tf.data.experimental.StatsAggregator()` to gather the statistics about prefetch (i.e., prefetch buffer size and buffer capacity). Your workflow to attach a `stats_aggregator` with dataset pipeline would be: \r\n\r\n```\r\naggregator = tf.data.experimental.StatsAggregator()\r\ndataset = `your dataset pipeline`\r\n\r\noptions = tf.data.Options()\r\noptions.experimental_stats.aggregator = aggregator\r\ndataset = dataset.with_options(options)\r\n```\r\n\r\nFurther, you can collect data statistics ( prefetch buffer size and buffer capacity) in form of tensor-board summaries from stats_aggregator, and thus the required stats could be visualized in tensorboard. \r\n```\r\nstats_summary = stats_aggregator.get_summary()\r\ntf.add_to_collection(tf.GraphKeys.SUMMARIES, stats_summary)\r\n```", "[StatAggregator](https://www.tensorflow.org/api_docs/python/tf/data/experimental/StatsAggregator) (TF 1.13.0+) is great! Thank for your hard work on this.", "@wookayin , @shivaniag , @facaiy ,\r\nguys, could you please provide simple working example of using StatAggregator?\r\n\r\n", "Every TF API has test cases for it. See:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/data/experimental/kernel_tests/stats_dataset_ops_test.py", "@wookayin Nope!  I tried logging summaries as done in the tests / docs using an Estimator-based workflow and I saw no summaries recorded in Tensorboard.  Could you please provide a less specific working sample?", "@shivaniag @wookayin @facaiy   Looks like the unit tests might just be insufficient, and that the docs are broken. I was finally able to get some histogram / distribution summaries using:\r\n```\r\nds = ds.apply(tf.data.experimental.bytes_produced_stats('bytes_produced'))\r\nds = ds.apply(tf.data.experimental.latency_stats(\"record_latency\"))\r\n```\r\n\r\nThe docs say to use `tf.data.experimental.latency_stats(\"total_bytes\")`, but that was broken.  It's also unclear exactly what the summaries measure.. does one need to apply before/after applying other ops like `batch()`?  ", "FWIW the above actually breaks when I try using in concert with `tf.contrib.distribute.MirroredStrategy`", "Is this still a working feature in TF 2.2+? I've tried using the examples mentioned here, but now `tf.data.experimental.StatsAggregator` (a `StatsAggregatorV2` object) does not seem to have a `get_summary()` method.\r\n\r\nNot only that, depending where I add the aggregator in my pipeline I get the following error.\r\n> tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_0_StatsAggregatorHandleV2/N10tensorflow4data23StatsAggregatorResourceE does not exist. [Op:SetStatsAggregatorDataset]\r\n\r\nIn particular, it seems to fail if I add the aggregator after interleaving."]}, {"number": 22001, "title": "dataset from generator generate mess data with tf.device('cpu:0')", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi There;\r\n\r\nI experienced serious bug out of tf.dataset.fromgenerator call.\r\n\r\nif the callable function applies tf.device('cpu:0'):\r\n\r\nthe receiving side of the data record is very unstable. it differs from time to time and machine from machine. \r\n\r\na lot of times I got exactly same output for each batch from receiving side. If I take out the tf.device('cpu:0'), it seems to work fine. Seems to me the data/memory during the transform were messed up.\r\n\r\nfollowing is sample code for reference;\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nNEPOCHES = 5\r\nBATCHSIZE = 10\r\nNUMFEATURES = 2\r\nPROCCOLS = ['item1', 'item2']\r\nFILENAME = 'test.csv'\r\nGENLOG = 'gen.log'\r\nRECLOG = 'rec.log'\r\n\r\ndef getNextBatch():\r\n\r\n    with tf.device('cpu:0'):\r\n        f = open(GENLOG, mode='w')\r\n        x = np.empty(shape=[BATCHSIZE, NUMFEATURES], dtype=np.float32)\r\n        y = np.empty(shape=[BATCHSIZE], dtype=np.float32)\r\n        z = np.int64(0)\r\n        r = np.empty(shape=[BATCHSIZE], dtype=np.int64)\r\n\r\n        x.fill(0)\r\n        y.fill(0)\r\n        z = 0\r\n        r.fill(0)\r\n        # print('start yield data')\r\n        f.write('start the data send from generator z: {} \\n x - \\n {}; \\n y - {}; \\n  r - {}\\n'.format(z, x, y, r))\r\n        yield (x, y, z, r)\r\n\r\n\r\n        df = pd.read_csv(filepath_or_buffer=FILENAME)\r\n\r\n\r\n        nrows = len(df)\r\n\r\n        ie = -1\r\n        ibatch = 0\r\n\r\n        while (ie + 1) < nrows:\r\n            ie += 1\r\n\r\n            if not ((df.loc[ie, 'item1'] > 3.68888) and (df.loc[ie, 'item2'] == 8)):\r\n                continue\r\n\r\n            x[ibatch] = df.loc[ie, ['item1', 'item2']].values\r\n            y[ibatch] = df.loc[ie, 'item3']\r\n            r[ibatch] = ie\r\n\r\n            ibatch += 1\r\n\r\n            if ibatch == BATCHSIZE:\r\n                ibatch = 0\r\n                z += 1\r\n\r\n                #print('{} th the data send from generator: x - \\n {}; \\n y - {}; \\n r - {}'.format(z, x, y, r))\r\n                f.write('{} th the data send from generator: x - \\n {}; \\n y - {}; \\n r - {}\\n'.format(z, x, y, r))\r\n                yield (x, y, z, r)\r\n        f.close()\r\n\r\n\r\ndef getDataSet():\r\n    ds = tf.data.Dataset.from_generator(generator=getNextBatch, output_types=(tf.float32, tf.float32, tf.int64, tf.int64),\r\n                                        output_shapes=(tf.TensorShape(dims=[BATCHSIZE, NUMFEATURES]), tf.TensorShape(dims=[BATCHSIZE]),\r\n                                                       tf.TensorShape(dims=[]), tf.TensorShape(dims=[BATCHSIZE])))\r\n\r\n\r\n    return ds\r\n\r\n\r\ndef recDataSet():\r\n    curSess = tf.Session()\r\n    dsn = getDataSet()\r\n    dsn = dsn.prefetch(10)\r\n    dsn = dsn.cache()\r\n    dsn = dsn.repeat(NEPOCHES)\r\n    dsn_it = dsn.make_initializable_iterator()\r\n    curSess.run(dsn_it.initializer)\r\n    dsn_it_nt = dsn_it.get_next()\r\n    f = open(RECLOG, mode='w')\r\n\r\n    while True:\r\n        try:\r\n            ndt = curSess.run(dsn_it_nt)\r\n\r\n            f.write('receive data from generator, z -- {}\\n'.format(ndt[2]))\r\n            f.write('x: {}\\n'.format(ndt[0]))\r\n            f.write('y: {}\\n'.format(ndt[1]))\r\n            f.write('r: {}\\n'.format(ndt[3]))\r\n        except:\r\n            break\r\n\r\n    f.close()\r\nif __name__ == '__main__':\r\n    recDataSet()", "Hi All;\r\n\r\nin addition, I found the use of prefetch also cause the problem. comment out the prefetch the program performs normally.\r\n"]}, {"number": 22000, "title": "Map twice on a shuffled dataset got different result", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source (self built)\r\n- **TensorFlow version (use command below)**: v1.9.0-rc2-1175-g12f51c2 1.9.0\r\n- **Python version**: 3.x\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI do a shuffle on a dataset and then map it twice before zip the result to a pair. I expect the pair has the same value, e.g. (0,0) or (30,30) but the running result shows differently, e.g. (78, 31). Is it a normal behavior of shuffle and map? Why?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\ndataset = tf.data.Dataset.range(100)\r\ndataset = dataset.shuffle(buffer_size=100)\r\nfeature = dataset.map(lambda x: x)\r\nlabel = dataset.map(lambda x: x)\r\ndataset = tf.data.Dataset.zip((feature, label))\r\nnext_item = dataset.make_one_shot_iterator().get_next()\r\nwith tf.Session() as sess:\r\n    print(sess.run(next_item))\r\n```", "comments": ["I guess you need to set seed for shuffle.", "@mpekalski Thanks. I understood that setting a seed would work but I found the current result is anti-intuitive since the computation graph is expected to create two map operations based one a single shuffled dataset and the mapped sequence should be the same for both map operations. Is my understanding correct?", "@jgong5 that is not how tf.data works. The definition in your example will in fact create two independent instances of the range and shuffle datasets. Informally, you can think of the dataset construction as having \"by value\" semantics.", "Hi @jgong5 ! Sorry for the late response .Output will be different each time as you have 100 different values in both features and labels . Attaching [gist ](https://colab.sandbox.google.com/gist/mohantym/ddad1225de4d96dd27b0370fc747e7ab/github_22000.ipynb#scrollTo=RH_zjirrB0-1)for reference. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 21999, "title": "tensorflow master build fail  (with VERBS)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 and Centos 7.5\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source/ master branch\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**:2.7 and 3.5\r\n- **Bazel version (if compiling from source)**:0.15\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.0 and 9.2\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nBuilt (with VERBS) successfully several days ago, but today after new pull from master with same setting as before, build failed.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nERROR: /mnt/nfs/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1)\r\nIn file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/common_runtime/device.h:43,\r\n                 from ./tensorflow/core/common_runtime/device_mgr.h:24,\r\n                 from ./tensorflow/core/distributed_runtime/worker_session.h:21,\r\n                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:24,\r\n                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:22,\r\n                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:\r\n./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow\r\n::gtl::ArraySlice<long long int>, tensorflow::int64)':\r\n./tensorflow/core/util/tensor_format.h:501:65: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {\r\n                                                                 ^\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In member function 'virtual void tensorflow::RdmaRemoteRendezvous::RecvFromRemoteAsync(const tensorflow::Rendez\r\nvous::ParsedKey&, const tensorflow::Rendezvous::Args&, tensorflow::Rendezvous::DoneCallback)':\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:66:41: error: 'using StringPiece = class absl::string_view' has no member named 'ToString'\r\n   string key(std::move(parsed.FullKey().ToString()));\r\n                                         ^\r\n```", "comments": ["Added PR #22003 for the fix.", "There was already tensorflow 1.11rc1 release, and this issue is still not fixed (build failed if the VERBS support is enabled).", "Just tried to compile tensorflow 1.11.0 and received the `error: 'class absl::string_view' has no member named 'ToString'`. Is there a solution for building the stable point release?", "u may change by hand to:\r\n.toString()"]}, {"number": 21998, "title": "TensorRT INT8 assertion failed when performing calibration", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n('v1.9.0-rc2-3472-g61038f8', '1.10.0')\r\ngithub master commit: 61038f89e98fc430c56a9aa056692413c82eaafa\r\n\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n0.16.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0 cuDNN 7.1.4\r\n- **GPU model and memory**:\r\nTitan V\r\n- **Exact command to reproduce**:\r\nI build TensorFlow from source with TensorRT 4.\r\n\r\nUse official TensorFlow-TensorRT example:\r\nhttps://github.com/tensorflow/models/tree/master/research/tensorrt\r\nand do INT8 test with command:\r\n\r\n> python tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb --image_file=image.jpg --int8 --output_dir=output\r\n\r\nNote that you should download ImageNet pretrained model http://download.tensorflow.org/models/official/resnetv2_imagenet_frozen_graph.pb\r\n\r\n### Describe the problem\r\nProgram core dumped when doing calibration step\r\n\r\n> INFO:tensorflow:Starting Warmup cycle\r\n> 2018-08-31 14:53:15.227087: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:577] Starting calibration thread on device 0, Calibration Resource @ 0x7f76f0001860                             \r\n> 2018-08-31 14:53:31.166463: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:577] Starting calibration thread on device 0, Calibration Resource @ 0x7f76f1709b00                             \r\n> python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.                                                                               \r\n> [1]    4880 abort (core dumped)  python tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb  --int8\r\n\r\nThis problem doesn't exist with tf 1.7 and 1.8.\r\n", "comments": ["@aaroey ", "have you solved this iusse? @qinyao-he ", "Sorry for late reply. The assertion error is caused by a known bug in TRT 4.0 library, which should be fixed by TRT 5.0. But TRT 5.0 is still in RC and we do not release TF with any libraries that are RC.", "Hi @qinyao-he! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported anymore. Have you checked the above issue in latest version TF 2.6 yet?. Thanks!", "Should have been solved.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 21997, "title": "Fixes the formatting issue pointed out at #21762", "body": "This fixes #21762\r\n`TypeError: not all arguments converted during string formatting`\r\n\r\n> I believe there's a mistake in the BasicLSTMCell.build method:\r\n> \r\n> ```\r\n> raise ValueError(\"Expected inputs.shape[-1] to be known, saw shape: %s\"\r\n>                        % inputs_shape)\r\n> ```\r\n> will be called with inputs_shape being a tuple and % formatting will interpret this tuple as multiple formatting arguments. Can we change it to\r\n> \r\n> ```\r\n> raise ValueError(\"Expected inputs.shape[-1] to be known, saw shape: %s\"\r\n>                        % (inputs_shape,))\r\n> ```\r\n> ?", "comments": ["@aaroey Requested Changes have been committed to the PR originating branch. Could you please review it again."]}, {"number": 21996, "title": "Added axis parameter to tf.losses.softmax_cross_entropy. Fixes #20866", "body": "The internal implementation of tf.losses.softmax_cross_entropy, uses nn.softmax_cross_entropy_with_logits_v2 which has a dim argument with a default value of -1. The new argument axis=-1 wraps the dim argument of nn.softmax_cross_entropy_with_logits_v2. Documentations also updated.\r\nThis will fix #20866", "comments": ["I had to force push because I did a rebase to upstream. Hope that doesn't cause any issues. I am running the `bazel build tensorflow/tools/compatibility/update:generate_v2_reorders_map` once done i will push the changes.\r\n\r\n**EDIT**\r\n\r\nI am having some issues in running the command. It fails for some reason with following trace\r\n```\r\n ERROR: /home/ashar786khan/tensorflow/tensorflow/BUILD:713:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/ashar786khan/.cache/bazel/_bazel_ashar786khan/07fd05535f7feb3d974366e977ffd283/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 534, in <module>\r\n    main()\r\n  File \"/home/ashar786khan/.cache/bazel/_bazel_ashar786khan/07fd05535f7feb3d974366e977ffd283/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 530, in main\r\n    args.compat_apiversions, args.compat_init_templates)\r\n  File \"/home/ashar786khan/.cache/bazel/_bazel_ashar786khan/07fd05535f7feb3d974366e977ffd283/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 468, in create_api_files\r\n    % ',\\n'.join(sorted(missing_output_files)))\r\nValueError: Missing outputs for genrule:\r\n\"compat/v2/losses/__init__.py\". Be sure to add these targets to\r\ntensorflow/python/tools/api/generator/api_init_files_v1.bzl and\r\ntensorflow/python/tools/api/generator/api_init_files.bzl (tensorflow repo), or\r\ntensorflow_estimator/python/estimator/api/api_gen.bzl (estimator repo)\r\nTarget //tensorflow/tools/compatibility/update:generate_v2_reorders_map failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2.316s, Critical Path: 1.85s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nIs this related to code changes or some issues in my build environment?\r\n\r\n**EDIT 2**\r\nIt is related to code changes. I am missing something after `@tf_export(\"losses.softmax_cross_entropy\", v1=[])` due to which API generation is not able to complete itself.\r\n\r\n**EDIT 3**\r\n\r\nThis issue has been resolved in the commit which follows this section. \r\n", "Now I have an issue running `bazel-bin/tensorflow/tools/compatibility/update/generate_v2_reorders_map` \r\n\r\nIt not only fails on my PR branch but even the tensorflow/master fails (I tried out of curiosity) with the following error :\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ashar786khan/tf2/tensorflow/bazel-bin/tensorfl\r\now/tools/compatibility/update/generate_v2_reorders_map.runfi\r\nles/org_tensorflow/tensorflow/tools/compatibility/update/gen\r\nerate_v2_reorders_map.py\", line 141, in <module>\r\n    app.run(main=main)\r\n  File \"/home/ashar786khan/tf2/tensorflow/bazel-bin/tensorfl\r\now/tools/compatibility/update/generate_v2_reorders_map.runfi\r\nles/org_tensorflow/tensorflow/python/platform/app.py\", line\r\n40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tol\r\nerate_undef)\r\n  File \"/home/ashar786khan/tf2/tensorflow/bazel-bin/tensorfl\r\now/tools/compatibility/update/generate_v2_reorders_map.runfi\r\nles/absl_py/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/ashar786khan/tf2/tensorflow/bazel-bin/tensorfl\r\now/tools/compatibility/update/generate_v2_reorders_map.runfi\r\nles/absl_py/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ashar786khan/tf2/tensorflow/bazel-bin/tensorfl\r\now/tools/compatibility/update/generate_v2_reorders_map.runfi\r\nles/org_tensorflow/tensorflow/tools/compatibility/update/gen\r\nerate_v2_reorders_map.py\", line 137, in main\r\n    update_reorders_v2(_OUTPUT_FILE_PATH)\r\n  File \"/home/ashar786khan/tf2/tensorflow/bazel-bin/tensorfl\r\now/tools/compatibility/update/generate_v2_reorders_map.runfi\r\nles/org_tensorflow/tensorflow/tools/compatibility/update/gen\r\nerate_v2_reorders_map.py\", line 133, in update_reorders_v2\r\n    file_io.write_string_to_file(output_file_path, renames_f\r\nile_text)\r\n  File \"/home/ashar786khan/tf2/tensorflow/bazel-bin/tensorfl\r\now/tools/compatibility/update/generate_v2_reorders_map.runfi\r\nles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", lin\r\ne 351, in write_string_to_file\r\n    f.write(file_content)\r\n  File \"/home/ashar786khan/tf2/tensorflow/bazel-bin/tensorfl\r\now/tools/compatibility/update/generate_v2_reorders_map.runfi\r\nles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", lin\r\ne 108, in write\r\n    self._prewrite_check()\r\nFile \"/home/ashar786khan/tf2/tensorflow/bazel-bin/tensorfl\r\now/tools/compatibility/update/generate_v2_reorders_map.runfi\r\nles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", lin\r\ne 94, in _prewrite_check\r\n    compat.as_bytes(self.__name), compat.as_bytes(self.__mod\r\ne), status)\r\n  File \"/home/ashar786khan/tf2/tensorflow/bazel-bin/tensorfl\r\now/tools/compatibility/update/generate_v2_reorders_map.runfi\r\nles/org_tensorflow/tensorflow/python/framework/errors_impl.p\r\ny\", line 548, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: third\r\n_party/tensorflow/tools/compatibility/reorders_v2.py; No suc\r\nh file or directory\r\n\r\n```", "@coder3101 can you please resolve conflicts", "@rthadur Fixed", "This change looks fine to me. But my understanding, it might need to be cherrypicked into 1.14 at this point since it changes API (adds an argument in the middle of function signature).\r\n@josh11b @martinwicke Should we accept and cherrypick this change or instead ask author to add `axis` argument at the end?", "My reply above was incorrect.\r\nActually, all the current losses under `tf.losses` are deprecated in 2.0. Instead, They are replaced by losses from `tf.keras.losses`.\r\n\r\n`axis` would need to be added here instead: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L405"]}, {"number": 21995, "title": "[solved] Android Static Library in Visual Studio with Tensorflow C++ API", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Any Android device\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.7\r\n- **NDK version**: 14b\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```sh\r\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a\r\n```\r\n\r\n### Describe the problem\r\n\r\nHi,\r\ncurrently, I have been trying to use Tensorflow C++ API to create an Android static library using Visual Studio. There are some Stack Overflow Questions related to this ([1](https://stackoverflow.com/questions/45066790/tensorflow-c-example-on-android), [2](https://stackoverflow.com/questions/48647592/compile-tensorflow-c-api-for-arm64-v8a), [3](https://stackoverflow.com/questions/50964234/using-tensorflow-c-in-android-with-cmake), [4](https://stackoverflow.com/questions/48181092/tensorflow-c-input-output-tensor-reshape)), but all with no answers. I was unable to find issues about it in Tensorflow's repo.\r\n\r\nI built Tensorflow for Android using Bazel in a Linux Ubuntu 18.04.1 LTS machine following [this instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android). Thus, I have a **libtensorflow_inference.so** and a java binding (**libandroid_tensorflow_inference_java.jar**). For my Android static library project in Visual Studio, I'm using the _libtensorflow_inference.so_ as \"_Additional Dependency_\" and the following \"_Additional Include Directories_\":\r\n\r\n```\r\nC:\\tensorflow\r\nC:\\tensorflow\\build\r\nC:\\tensorflow\\build\\external\\eigen_archive\r\nC:\\tensorflow\\third_party\\eigen3\r\nC:\\tensorflow\\build\\protobuf\\src\\protobuf\\src\r\nC:\\tensorflow\\build\\nsync\\src\\nsync\\public\r\nC:\\tensorflow\\build\\snappy\\src\\snappy\r\n```\r\n\r\nHowever, I'm getting the following errors:\r\n\r\n```\r\nundefined reference to 'tensorflow::SessionOptions::SessionOptions()'\r\nundefined reference to 'tensorflow::ConfigProto::~ConfigProto()'\r\nundefined reference to 'tensorflow::ConfigProto::~ConfigProto()'\r\nundefined reference to 'tensorflow::GraphDef::GraphDef(tensorflow::GraphDef const&)'\r\nundefined reference to 'tensorflow::ConfigProto::ConfigProto(tensorflow::ConfigProto const&)'\r\nundefined reference to 'tensorflow::ConfigProto::~ConfigProto()'\r\nundefined reference to 'tensorflow::GraphDef::~GraphDef()'\r\nundefined reference to 'tensorflow::GraphDef::~GraphDef()'\r\nundefined reference to 'tensorflow::ConfigProto::~ConfigProto()'\r\nundefined reference to 'tensorflow::GraphDef::~GraphDef()'\r\nundefined reference to 'tensorflow::GraphDef::~GraphDef()'\r\nundefined reference to 'tensorflow::GraphDef::GraphDef()'\r\nundefined reference to 'tensorflow::Env::Default()'\r\nundefined reference to 'tensorflow::ReadBinaryProto(tensorflow::Env*, std::string const&, google::protobuf::MessageLite*)'\r\nundefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'\r\nundefined reference to 'tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\nundefined reference to 'tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\nundefined reference to 'tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\nundefined reference to 'tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**)'\r\nundefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'\r\nundefined reference to 'tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\nundefined reference to 'tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\nundefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'\r\nundefined reference to 'tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\nundefined reference to 'tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n```\r\n\r\nTherefore, my questions are:\r\n- It is possible to use the __libtensorflow_inference.so__ for a C++ Android static library?\r\n- Am I missing addiitional dependencies?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version\nGPU model and memory", "After a while, I was able to solve the problem. I wrote a Medium post about it for those facing the same problem as mine:\r\n[https://medium.com/@arnaldog12/how-to-build-tensorflow-as-a-static-library-for-android-5c762dbdd5d4](https://medium.com/@arnaldog12/how-to-build-tensorflow-as-a-static-library-for-android-5c762dbdd5d4)"]}, {"number": 21994, "title": "Tensorflow install should setup up GPU support automatically", "body": "My only real experience with TensorFlow is using it with Keras and sometimes writing some probabilistic objective functions in TensorFlow. Anytime I require GPU support I switch to PyTorch because when I install it, it is automatically set up to run on GPUs. Is there any plan to implement this for TensorFlow?", "comments": ["I do not understand your use cases.\r\n\r\nAre there any of them not covered by using the `tensorflow-gpu` Python package?", "@arose13 There are actually two different packages `tensorflow` and `tensorflow-gpu`. However, PyTorch doesn't have a different package for GPU.\r\nBut I agree it can be/is difficult to set up `tensorflow-gpu` as compared to PyTorch. But here is a thing, Why not get docker Image of `tensorflow-gpu` and use it instead. Here are the [docs](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dockerfiles) on how to get it.\r\n", "Nagging Assignee @gunan: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@arose13  - It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 21993, "title": "third_party: update libjpeg-turbo to 2.0.0", "body": "libjpeg-turbo-2.0.0 fixes CVE-2018-1152 and CVE-2018-11813\r\n\r\nThe build and source tree has been rearranged, the simd files are now in\r\nsubdirs.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>\r\n\r\nThis obsoletes https://github.com/tensorflow/tensorflow/pull/21654. I have only tested amd64. Lets see how badly the tests fail for the other arches.", "comments": ["Looks like windows does not HAVE_BUILTIN_CTZL. The rest all passed, XLA failure looked unrelated.\r\n@gunan @yifeif Does this PR look better than the patch one to fix libjpeg-turbo CVEs?", "Yay the windows tests pass now too", "This should be able to go in all automated. Sorry for the delay on the other one.\r\nI will retrigger the tests.", "@gunan I saw the r1.11 branch was just made. Can you make sure this gets in to that branch as well before the releases? We dont want any security risks in those."]}, {"number": 21992, "title": "[tflite] do not create unused idle thread when SetNumThreads(1)", "body": "\u2026avoid creating unused idle thread when user specifies `interpreter->SetNumThreads(1);`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @aaroey: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21991, "title": "Strange behavior of loss function in WALSMatrixFactorization TF 1.8", "body": "Our current implementation of recommendation system use TF 1.8 and WALS algorithm. The model was trained using self.fit(input_fn=input_fn) and ML Engine with run time version 1.8. Data set was formed following [example](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/10_recommend/wals.ipynb) using tensorflow.train.Example(...) Extraction from training logs shown below. \r\n\r\n[Extraction from training logs](https://drive.google.com/open?id=1HS4dPXC31Qv42YAWb_PT5y_C_uFD03GM) \r\n\r\nThe fit was performed with some default parameters. The loss value did decreased on second evaluation. However loss did not changed after that. The final Root weighted squared error (rwse) in this training became 0.126.\r\n\r\nHyperparameter tuning was performed later and the best parameter set was used in the following training. The result of that training is shown below.\r\n\r\n[Post hyperparameter tuning log extraction](https://drive.google.com/open?id=1zrt1jsfVdFAtuEegzquV-8sU8AJtlmKY)\r\n\r\nTree things to note here. First, the loss value at the beginning is lower than at later evaluation steps. Low value in the beginning most likely due to choice of parameters from the results of hyperparameter tuning. Increase of the loss value later on looks strange. Second, it\u2019s unchanged loss value after second evaluation. This pattern remains the same while self.fit(input_fn=input_fn) is used for model training. Third, the final rwse in this training became 0.487 while during hyperparameter tuning with the same parameter set rwse=0.015. \r\n\r\nThe question is if anyone observed something similar? Could the reason of unchanged loss value be due to fixed learning rate? If yes, is it possible to change it while using WALSMatrixFactorization class and self.fit(input_fn=input_fn)??\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n- GCP VM instance  created-with-datalab-version\t20180503\r\n\r\n- TensorFlow version 1.8.0\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, but my code tightly follows [examlple](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/10_recommend/wals.ipynb)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: TensorFlow version 1.8.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A", "I used fully managed by Google n1-highmem-16 (16 vCPUs, 104 GB memory) instance for code development. Further packaged model was trained and later hyperparameter tuned with Google cloud ML engine. ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Yes, you have misinterpreted a bug. I compile a code using **MovieLens 100k** public data set, which reproduce the bug, namely, **loss value does not update during the training**, on both Windows 10 and Ubuntu 16.04, as well as on TF 1.8 and TF 1.10.0.", "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes [the code to reproduce the bug](https://drive.google.com/open?id=1ahzI5vZQwqXB0TsjI8dSVCGGH7z-ut0f)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: both, Windows 10 and Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: A/N\r\n- **TensorFlow installed from (source or binary)**: no, I used miniconda environment and pip install on Windows 10\r\n- **TensorFlow version (use command below)**: both TensorFlow version 1.8.0 and TensorFlow version 1.8.0\r\n- **Python version**: both 2.7 and 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: [the code](https://drive.google.com/open?id=1ahzI5vZQwqXB0TsjI8dSVCGGH7z-ut0f)\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\n> Line 1: \tLine 24: 2018-09-11 14:16:17,930 - tensorflow - INFO - loss = 1258.4746, step = 1\r\n\tLine 2: \tLine 25: 2018-09-11 14:16:17,930 - tensorflow - INFO - loss = 1258.4746, step = 1\r\n\tLine 3: \tLine 232: 2018-09-11 14:16:22,078 - tensorflow - INFO - loss = 42.56178, step = 101 (4.148 sec)\r\n\tLine 4: \tLine 233: 2018-09-11 14:16:22,078 - tensorflow - INFO - loss = 42.56178, step = 101 (4.148 sec)\r\n\tLine 5: \tLine 436: 2018-09-11 14:16:25,565 - tensorflow - INFO - loss = 42.56178, step = 201 (3.487 sec)\r\n\tLine 6: \tLine 437: 2018-09-11 14:16:25,565 - tensorflow - INFO - loss = 42.56178, step = 201 (3.487 sec)\r\n\tLine 7: \tLine 640: 2018-09-11 14:16:29,490 - tensorflow - INFO - loss = 42.56178, step = 301 (3.925 sec)\r\n\tLine 8: \tLine 641: 2018-09-11 14:16:29,490 - tensorflow - INFO - loss = 42.56178, step = 301 (3.925 sec)\r\n\tLine 9: \tLine 844: 2018-09-11 14:16:33,250 - tensorflow - INFO - loss = 42.56178, step = 401 (3.760 sec)\r\n\tLine 10: \tLine 845: 2018-09-11 14:16:33,250 - tensorflow - INFO - loss = 42.56178, step = 401 (3.760 sec)\r\n\tLine 11: \tLine 1048: 2018-09-11 14:16:36,489 - tensorflow - INFO - loss = 42.56178, step = 501 (3.239 sec)\r\n\tLine 12: \tLine 1049: 2018-09-11 14:16:36,489 - tensorflow - INFO - loss = 42.56178, step = 501 (3.239 sec)\r\n\tLine 13: \tLine 1252: 2018-09-11 14:16:40,309 - tensorflow - INFO - loss = 42.56178, step = 601 (3.820 sec)\r\n\tLine 14: \tLine 1253: 2018-09-11 14:16:40,309 - tensorflow - INFO - loss = 42.56178, step = 601 (3.820 sec)\r\n\tLine 15: \tLine 1456: 2018-09-11 14:16:43,931 - tensorflow - INFO - loss = 42.56178, step = 701 (3.623 sec)\r\n\tLine 16: \tLine 1457: 2018-09-11 14:16:43,931 - tensorflow - INFO - loss = 42.56178, step = 701 (3.623 sec)\r\n\tLine 17: \tLine 1534: 2018-09-11 14:16:45,488 - tensorflow - INFO - loss = 42.56\r\n\tLine 18: \tLine 1535: 2018-09-11 14:16:45,488 - tensorflow - INFO - loss = 42.56\r\n\r\nAs can seen from the log file extraction contained lines \"loss =\", the loss value **does not get updated** after second evaluation. \r\n\r\n### Source code / logs\r\nComplete log could be found [here](https://drive.google.com/open?id=1vy0GDKK4HMdAirLGewM2vUa_61MGfc_x)\r\n", "In order to enforce the bug appearance one could used **tensorflow.contrib.learn** in TF 1.8 and Google Datalab as shown [here](https://drive.google.com/open?id=1yF1XwCN8J0tHPOB1IFYMDTXX1kITBE0Y)\r\n\r\nThe log from this training is [here](https://drive.google.com/open?id=1rZ72r5VQQJqbOFQcqOYRT7hEM09lL0sX)  \r\n\r\nselected rows with loss value:\r\n\r\n> Line 39: INFO:tensorflow:loss = 0.44986597, step = 1\r\n\tLine 140: INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 0.44986597\r\n\tLine 141: INFO:tensorflow:Validation (step 94): loss = 0.44986597, global_step = 1\r\n\tLine 152: INFO:tensorflow:loss = 67.273926, step = 101 (28.679 sec)\r\n\tLine 256: INFO:tensorflow:loss = 18.575909, step = 201 (24.346 sec)\r\n\tLine 362: INFO:tensorflow:loss = 1.3139261, step = 301 (30.483 sec)\r\n\tLine 468: INFO:tensorflow:loss = 0.608429, step = 401 (29.466 sec)\r\n\tLine 574: INFO:tensorflow:loss = 1.2948558, step = 501 (27.934 sec)\r\n\tLine 680: INFO:tensorflow:loss = 1.8674694, step = 601 (27.611 sec)\r\n\tLine 784: INFO:tensorflow:loss = 86.05044, step = 701 (23.794 sec)\r\n\tLine 888: INFO:tensorflow:loss = 63.37451, step = 801 (28.128 sec)\r\n\tLine 994: INFO:tensorflow:loss = 2.190352, step = 901 (34.924 sec)\r\n\tLine 1042: INFO:tensorflow:Saving dict for global step 937: global_step = 937, loss = 0.38724956\r\n\r\nBehavior of  the loss function is clearly different!", "Sorry for closing this issue. @walidk, can you take a look?", "@walidk Could you please look into this.", "Hi @NatLun091238 ! \r\nWe are checking to see whether you still need help in this issue . Please create a new issue if the issue is replicating in Latest version TF 2.6 . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21991\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21991\">No</a>\n"]}, {"number": 21990, "title": "R1.10", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "hey, thanks for the PR but I think something must have gone wrong. There are a lot of unrelated commits from another branch in here.\r\nWhat were you trying to push and lets see how we can get it fixed up?", "I've already merged back r1.10 into master"]}, {"number": 21989, "title": "CODEOWNERS: add myself for third_party/systemlibs/", "body": "Add myself so get assigned for PRs in third_party/systemlibs/", "comments": ["resolved conflicts"]}, {"number": 21988, "title": "Feature request for tf.test.TestCase.assertNotAllClose: do not log the differences", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen running this test, it logs the failure of `assertAllClose` which is not necessary.\r\n\r\n### Source code / logs\r\n\r\n    import tensorflow as tf\r\n    test_case = tf.test.TestCase()\r\n    test_case.assertNotAllClose([1, 2, 3], [2, 3, 4])\r\n\r\n\r\nThe logs are (I would expect no log):\r\n\r\n    not close where =  (array([0, 1, 2]),)\r\n    not close lhs =  [1 2 3]\r\n    not close rhs =  [2 3 4]\r\n    not close dif =  [1 1 1]\r\n    not close tol =  [3.e-06 4.e-06 5.e-06]\r\n    dtype = int64, shape = (3,)\r\n", "comments": ["Hi, I create PR #22006 to fix the issue.", "Thanks a lot @facaiy. Looks perfect ...\r\nWill check it out once it has been merged (which seems to take a while)."]}, {"number": 21987, "title": "problem of compiler libtensorflow_cc.so with debug info", "body": "I want to compiler libtensorflow_cc.so with debug info by bazel .\r\ntry with lines :bazel build -c dbg --strip=never //tensorflow:libtensorflow_cc.so.\r\nbut it's not work .when I gdb .so, (no debug info ).\r\nversion info: tf-r1.4 ,bazel-10.0.\r\nWhat's worng with my lines ?  thanks a lot .", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@yuwang89thu  Hi, can you provide the following info.\r\n\r\nHave I written custom code\r\nOS Platform and Distribution\r\nTensorFlow installed from\r\nCUDA/cuDNN version\r\nGPU model and memory\r\nExact command to reproduce\r\nMobile device", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21986, "title": "ValueError: None is only supported in the 1st dimension.", "body": "Using TensorFlow backend.\r\n2018-08-30 20:26:09.136414: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n(?, 32, 280, 1)\r\n(?, 35, 5990)\r\n(?, 32, 280, 1)\r\n(?, 35, 5990)\r\nWARNING:tensorflow:From /home/huangwei/.local/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:86: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`normal` is a deprecated alias for `truncated_normal`\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\nTraceback (most recent call last):\r\n  File \"1.py\", line 15, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/huangwei/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 394, in convert\r\n    \"invalid shape '{1}'.\".format(_tensor_name(tensor), shape))\r\nValueError: None is only supported in the 1st dimension. Tensor 'the_input' has invalid shape '[None, 32, None, 1]'.", "comments": ["tf.__verion\r\n'1.11.0-dev20180830'\r\npython3.5\r\nubuntu", "As the error message mentions, `None` is only supported in the first dimension. You can set the input shape using the `input_shapes` argument in whichever classmethod you are using.\r\n\r\nIf you believe the tensor `the_input` should not have a shape [None, 32, None, 1] based on what you built, please provide the following to help debug the issue:\r\n1. The model that you are converting\r\n2. The code that you are running to convert the model\r\n3. [optional] The code you ran to create the model", "now, i have sovled this problem,but when i quantized my model got a bug(no quantized no bug)\r\n\r\nb\"2018-09-04 17:43:07.631853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.632044: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.632181: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.632298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.632415: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.632566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.632683: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.632920: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.633044: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.633390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.633573: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.633686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.633807: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.633943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.634092: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.634256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.634413: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.634594: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.634825: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.635026: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.635212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.635350: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.635546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.635691: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.635836: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.636058: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.636231: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: SquaredDifference\\n2018-09-04 17:43:07.674613: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1157 operators, 1673 arrays (0 quantized)\\n2018-09-04 17:43:07.703190: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1099 operators, 1586 arrays (0 quantized)\\n2018-09-04 17:43:07.738731: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1099 operators, 1586 arrays (0 quantized)\\n2018-09-04 17:43:07.791890: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 822 operators, 1281 arrays (1 quantized)\\n2018-09-04 17:43:07.833745: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 820 operators, 1278 arrays (1 quantized)\\n2018-09-04 17:43:07.866853: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 820 operators, 1278 arrays (1 quantized)\\n2018-09-04 17:43:07.879104: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 820 operators, 1278 arrays (1 quantized)\\n2018-09-04 17:43:07.891322: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 820 operators, 1278 arrays (1 quantized)\\n2018-09-04 17:43:07.909161: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 820 operators, 1278 arrays (1 quantized)\\n2018-09-04 17:43:07.909290: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:100] Constant array conv2d_55/kernel lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.\\n2018-09-04 17:43:07.909526: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:474] Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: SquaredDifference) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\\nAborted (core dumped)\\n\"\r\n", "when i quantized  my model,got an Unsupported TensorFlow op: SquaredDifference, if no use quantized form, i can get a tflite form(use_custom_op)", "ValueError: Didn't find custom op for name 'Merge' with version 1\r\nDidn't find custom op for name 'Switch' with version 1\r\nDidn't find custom op for name 'SquaredDifference' with version 1\r\nRegistration failed.\r\n", "Hi @dlml, were you able to use the workaround you mentioned in #22022 ?", "The list of supported ops are available [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md). The two recommended solutions are:\r\n1. Add a custom op as described [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md).\r\n2. Only use the ops that are currently supported by TFLite in your model.\r\n\r\nQuantized `SquaredDifference` hasn't been prioritized yet, contributions are welcome, but we are also tracking these operations requests here: https://github.com/tensorflow/tensorflow/issues/21526 to help with prioritizing. I have added your request there.\r\n\r\nAdditionally, as @jdduke  mentioned, `Merge` and `Switch` are not prioritized and are not in our immediate roadmap due to complexity. For that reason, we will close this issue, please reopen if you have more info on the specific model you are trying to convert. Thanks!", "Solved it with extra parameters inside code:\r\n\r\n    import tensorflow as tf\r\n\r\n    graph_def_file = \"mask_rcnn_resnet50_atrous_coco_2018_01_28/frozen_inference_graph.pb\"\r\n    input_arrays = [\"image_tensor\"]\r\n    output_arrays = [\"detection_scores\",\"detection_boxes\",\"detection_classes\",\"detection_masks\"]\r\n\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n            graph_def_file, input_arrays, output_arrays,input_shapes={\"image_tensor\":[1,600,600,3]})\r\n    tflite_model = converter.convert()\r\n    open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\nAlthough it cannot produce .tflite because it raises kmerge error \"OperatorType::kMerge Found Sub as non-selected output from Switch, but only Merge supported\""]}, {"number": 21985, "title": "While loop no gradients provided for any variable", "body": "After i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.\r\n\r\n![default](https://user-images.githubusercontent.com/12975526/44889979-d9ca6b80-ad0a-11e8-967b-0f31a2467c68.PNG)\r\n\r\n# while_loop\r\ndef dynamic_pointing_decoder(self, U, mask):\r\n        def _HMN(ut, h, us, ue):\r\n            h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\r\n            WD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\r\n            r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\r\n            ut_r = tf.concat([ut, r], axis=1) #batch,3d\r\n            W1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\r\n            b1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\r\n            mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\r\n            mt1 = tf.reduce_max(mt1, axis=2) #batch * d\r\n            W2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\r\n            b2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\r\n            mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\r\n            mt2 = tf.reduce_max(mt2, axis=2) #batch * d\r\n            mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\r\n            W3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\r\n            b3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\r\n            hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\r\n            hmn = tf.reduce_max(hmn, axis=2) #batch 1\r\n            hmn = tf.reshape(hmn, [-1]) #batch\r\n            return hmn\r\n\r\n        def body(time_step, p1s, p2s, alphas, betas, us, ue, state):\r\n            us_ue = tf.concat([us, ue], axis=1)  # batch 4d\r\n            h, state = cell(inputs=us_ue, state=state)  # batch * d\r\n\r\n            with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):\r\n                alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\r\n                alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\r\n            i_start = tf.argmax(alpha, axis=1)  # batch\r\n            i_start = tf.cast(i_start, tf.int32)\r\n            s_idx = tf.stack([idx, i_start], axis=1)\r\n            us = tf.gather_nd(U, s_idx)  # batch 2d\r\n\r\n            with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):\r\n                beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\r\n                beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\r\n            i_end = tf.argmax(beta, axis=1)  # batch\r\n            i_end = tf.cast(i_end, tf.int32)\r\n            e_idx = tf.stack([idx, i_end], axis=1)\r\n            ue = tf.gather_nd(U, e_idx)  # batch 2d\r\n\r\n            p1s.write(time_step, i_start)\r\n            p2s.write(time_step, i_end)\r\n            alphas.write(time_step, alpha)\r\n            betas.write(time_step, beta)\r\n            return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)\r\n\r\n        def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):\r\n            return tf.less(time_step, 4)\r\n\r\n\r\n        with tf.variable_scope('dynamic_pointing_decoder'):\r\n            cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\r\n            i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            idx = tf.range(0, tf.shape(U)[0], 1)\r\n            s_idx = tf.stack([idx, i_start], axis=1)\r\n            e_idx = tf.stack([idx, i_end], axis=1)\r\n            us = tf.gather_nd(U, s_idx) #batch 2d\r\n            ue = tf.gather_nd(U, e_idx) #batch 2d\r\n            p1s = tf.TensorArray(tf.int32, size=4)\r\n            p2s = tf.TensorArray(tf.int32, size=4)\r\n            alphas = tf.TensorArray(tf.float32, size=4)\r\n            betas = tf.TensorArray(tf.float32, size=4)\r\n            state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\r\n                                              tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\r\n            U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\r\n            #time_step, p1s, p2s, us, ue, state\r\n            time_step = 0\r\n            time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,\r\n                                                                              loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),\r\n                                                                              maximum_iterations=4)\r\n            self.p1s = tf.transpose(p1s.stack()) #batch*4\r\n            self.p2s = tf.transpose(p2s.stack())\r\n            print(\r\n                \"p1s  shape : {0}\".format(np.shape(self.p1s))\r\n            )\r\n            self.p1 = tf.unstack(self.p1s,axis=0)[-1]\r\n            print(\"p1 shape : {0}\".format(np.shape(self.p1)))\r\n            self.p2 = tf.unstack(self.p2s, axis=0)[-1]\r\n            alphas = tf.unstack(alphas.stack(), axis=0)\r\n            betas = tf.unstack(betas.stack(), axis=0)\r\n            print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))\r\n            return alphas, betas\r\n\r\n#no while loop\r\ndef dynamic_pointing_decoder(self, U, mask):\r\n        def _HMN(ut, h, us, ue):\r\n            h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\r\n            WD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\r\n            r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\r\n            ut_r = tf.concat([ut, r], axis=1) #batch,3d\r\n            W1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\r\n            b1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\r\n            mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\r\n            mt1 = tf.reduce_max(mt1, axis=2) #batch * d\r\n            W2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\r\n            b2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\r\n            mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\r\n            mt2 = tf.reduce_max(mt2, axis=2) #batch * d\r\n            mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\r\n            W3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\r\n            b3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\r\n            hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\r\n            hmn = tf.reduce_max(hmn, axis=2) #batch 1\r\n            hmn = tf.reshape(hmn, [-1]) #batch\r\n            return hmn\r\n        with tf.variable_scope('dynamic_pointing_decoder'):\r\n            #single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)\r\n            #cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])\r\n            #cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)\r\n            cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\r\n            i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            #pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            #pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            idx = tf.range(0, tf.shape(U)[0], 1)\r\n            s_idx = tf.stack([idx, i_start], axis=1)\r\n            e_idx = tf.stack([idx, i_end], axis=1)\r\n            us = tf.gather_nd(U, s_idx) #batch 2d\r\n            ue = tf.gather_nd(U, e_idx) #batch 2d\r\n            alphas, betas = [], []\r\n            state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\r\n                                              tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\r\n            U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\r\n            for time_step in range(4):\r\n                if time_step >= 1:\r\n                    tf.get_variable_scope().reuse_variables()\r\n                us_ue = tf.concat([us,ue], axis=1) #batch 4d\r\n                h, state = cell(inputs=us_ue, state=state) #batch * d\r\n\r\n                with tf.variable_scope('alpha_HMN'):\r\n                    if time_step >= 1:\r\n                        tf.get_variable_scope().reuse_variables()\r\n                    alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\r\n                    alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\r\n                i_start = tf.argmax(alpha, axis=1) #batch\r\n                i_start = tf.cast(i_start, tf.int32)\r\n                s_idx = tf.stack([idx, i_start], axis=1)\r\n                us = tf.gather_nd(U, s_idx) #batch 2d\r\n\r\n                with tf.variable_scope('betas_HMN'):\r\n                    if time_step >= 1:\r\n                        tf.get_variable_scope().reuse_variables()\r\n                    beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\r\n                    beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\r\n                i_end = tf.argmax(alpha, axis=1) #batch\r\n                i_end = tf.cast(i_end, tf.int32)\r\n                e_idx = tf.stack([idx, i_end], axis=1)\r\n                ue = tf.gather_nd(U, e_idx) #batch 2d\r\n\r\n                alphas.append(alpha)\r\n                betas.append(beta)\r\n                #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):\r\n                #    break\r\n                #else:\r\n                #    pre_start = i_start\r\n                #    pre_end = i_end\r\n            return alpha, beta\r\n\r\nanyone can help me? Thank you very much\r\n", "comments": ["i found the gradients are None, why and how to solve it. anyone give me some advices?", "In my program. i found a map_fn operation in body,  Is it right?", "i have solved it, but i found the speed of while_loop is slower than for, Is it reasonable? what are the tips to speed up training when using while_loop?", "Glad you're up and running. #9527 has tips about the work in the body of the loop versus the control overheads. "]}, {"number": 21984, "title": "Build from source failed on MSBuild absl/strings/str_format.h': No such file or directory", "body": "\r\n### System informationOS \r\nPlatform and Distribution: Windows 2016 DataCenter\r\nTensorFlow installed from: #21690\r\nTensorFlow version: 1.10.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: Cuda v 9.2 / cuDNN 10-x64-v7.2.1.38\r\nGPU model and memory: Nvidia K80 11GB\r\nExact command to reproduce: cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=C:/Users/neoxz/Downloads/swigwin-3.0.10/swigwin-3.0.10/swig.exe ^\r\n-DPYTHON_EXECUTABLE=C:/Anaconda/python.exe ^\r\n-DPYTHON_LIBRARIES=C:/Anaconda/libs/python35.lib ^\r\n-Dtensorflow_ENABLE_GPU=ON ^\r\n-DCUDNN_HOME=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2\" ^\r\n-Dtensorflow_ENABLE_MKL_SUPPORT=ON ^\r\n-DMKL_HOME=\"C:/Program Files (x86)/IntelSWTools/compilers_and_libraries\" ^\r\n-Dtensorflow_CUDA_VERSION=9.2 ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 ^\r\n-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF ^\r\n-Dtensorflow_BUILD_PYTHON_TESTS=OFF ^\r\n-Dtensorflow_BUILD_CC_EXAMPLE=OFF\r\nMobile device:N/A\r\n[cmakeresult.txt](https://github.com/tensorflow/tensorflow/files/2338523/cmakeresult.txt)\r\n[cmakecommand.txt](https://github.com/tensorflow/tensorflow/files/2338524/cmakecommand.txt)\r\n[old-tf_env.txt](https://github.com/tensorflow/tensorflow/files/2338525/old-tf_env.txt)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2338526/tf_env.txt)\r\n[msbuild.txt](https://github.com/tensorflow/tensorflow/files/2338527/msbuild.txt)\r\n\r\n\r\n\r\n\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n Files Attached.\r\n\r\nb'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n\r\n### Describe the problem\r\nTensorflow build from source failed on MSBuild.\r\n\r\n### Source code / logs\r\nFiles attached.\r\n\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=C:/Users/neoxz/Downloads/swigwin-3.0.10/swigwin-3.0.10/swig.exe ^\r\n-DPYTHON_EXECUTABLE=C:/Anaconda/python.exe ^\r\n-DPYTHON_LIBRARIES=C:/Anaconda/libs/python35.lib ^\r\n-Dtensorflow_ENABLE_GPU=ON ^\r\n-DCUDNN_HOME=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2\" ^\r\n-Dtensorflow_ENABLE_MKL_SUPPORT=ON ^\r\n-DMKL_HOME=\"C:/Program Files (x86)/IntelSWTools/compilers_and_libraries\" ^\r\n-Dtensorflow_CUDA_VERSION=9.2 ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 ^\r\n-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF ^\r\n-Dtensorflow_BUILD_PYTHON_TESTS=OFF ^\r\n-Dtensorflow_BUILD_CC_EXAMPLE=OFF", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS: Windows 2016 DataCenter\r\n== cat /etc/issue ===============================================\r\nMINGW64_NT-10.0 neoxzai 2.10.0(0.325/5/3) 2018-03-15 14:12 x86_64 Msys\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nbash: c++: command not found\r\n\r\n== uname -a =====================================================\r\nMINGW64_NT-10.0 neoxzai 2.10.0(0.325/5/3) 2018-03-15 14:12 x86_64 Msys\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.3   \r\nnumpydoc                           0.8.0    \r\nprotobuf                           3.6.1    \r\ntensorflow                         1.10.0   \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = b'v1.10.0-rc1-19-g656e7a2b34'\r\ntf.COMPILER_VERSION = b'v1.10.0-rc1-19-g656e7a2b34'\r\nSanity check: array([1])\r\n\r\n== cat /etc/issue ===============================================\r\nMINGW64_NT-10.0 neoxzai 2.10.0(0.325/5/3) 2018-03-15 14:12 x86_64 Msys\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nbash: c++: command not found\r\n\r\n== uname -a =====================================================\r\nMINGW64_NT-10.0 neoxzai 2.10.0(0.325/5/3) 2018-03-15 14:12 x86_64 Msys\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.3   \r\nnumpydoc                           0.8.0    \r\nprotobuf                           3.6.1    \r\ntensorflow                         1.10.0   \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = b'v1.10.0-rc1-19-g656e7a2b34'\r\ntf.COMPILER_VERSION = b'v1.10.0-rc1-19-g656e7a2b34'\r\nSanity check: array([1])\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Aug 31 02:07:33 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 386.07                 Driver Version: 386.07                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           TCC  | 00000001:00:00.0 Off |                    0 |\r\n| N/A   39C    P8    26W / 149W |      0MiB / 11444MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n\r\n== cat /etc/issue ===============================================\r\nMINGW64_NT-10.0 neoxzai 2.10.0(0.325/5/3) 2018-03-15 14:12 x86_64 Msys\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nbash: c++: command not found\r\n\r\n== uname -a =====================================================\r\nMINGW64_NT-10.0 neoxzai 2.10.0(0.325/5/3) 2018-03-15 14:12 x86_64 Msys\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.3   \r\nnumpydoc                           0.8.0    \r\nprotobuf                           3.6.1    \r\ntensorflow                         1.10.0   \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = b'v1.10.0-rc1-19-g656e7a2b34'\r\ntf.COMPILER_VERSION = b'v1.10.0-rc1-19-g656e7a2b34'\r\nSanity check: array([1])\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Aug 31 02:07:45 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 386.07                 Driver Version: 386.07                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           TCC  | 00000001:00:00.0 Off |                    0 |\r\n| N/A   39C    P8    26W / 149W |      0MiB / 11444MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n Cuda v 9.2 / cuDNN 10-x64-v7.2.1.38\r\n", "**OS Platform and Distribution**: Windows 2016 DataCenter\r\n**TensorFlow installed from**: #21690 \r\n**TensorFlow version**: 1.10.0\r\n**Bazel version**: N/A\r\n**CUDA/cuDNN version**: Cuda v 9.2 / cuDNN 10-x64-v7.2.1.38\r\n**GPU model and memory**: Nvidia K80 11GB\r\n**Exact command to reproduce**: cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=C:/Users/neoxz/Downloads/swigwin-3.0.10/swigwin-3.0.10/swig.exe ^\r\n-DPYTHON_EXECUTABLE=C:/Anaconda/python.exe ^\r\n-DPYTHON_LIBRARIES=C:/Anaconda/libs/python35.lib ^\r\n-Dtensorflow_ENABLE_GPU=ON ^\r\n-DCUDNN_HOME=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2\" ^\r\n-Dtensorflow_ENABLE_MKL_SUPPORT=ON ^\r\n-DMKL_HOME=\"C:/Program Files (x86)/IntelSWTools/compilers_and_libraries\" ^\r\n-Dtensorflow_CUDA_VERSION=9.2 ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 ^\r\n-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF ^\r\n-Dtensorflow_BUILD_PYTHON_TESTS=OFF ^\r\n-Dtensorflow_BUILD_CC_EXAMPLE=OFF\r\n**Mobile device**:N/A", "Nagging Assignee @drpngx: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21983, "title": "Keras models converted to Estimators do not write summaries.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Jessie\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.9.0-0-g25c197e023', '1.9.0') \r\n- **Python version**: 2.7.9\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: See provided gist\r\n\r\n### Describe the problem\r\nI've been setting up Estimators by using Keras layers to define tensors and then feeding them into my own `model_fn`. This was a pain and prevented me from easily doing stuff like adding regularization, but was necessary until recently due to various bugs within `model_to_estimator`. Luckily those bugs have been fixed, and I can now use the much more canonical and proper `model_to_estimator`. Unfortunately there are still some holes...\r\n\r\nAnyways, when I did this I would, in defining the model, set various summaries to be collected simply by calling `tf.summary.scalar` etc. as documented. However, when I take the same tensors and put them through a Keras model and then through `model_to_estimator`, the summaries I've defined do not get \r\nwritten.\r\n\r\nI've tried a few things here, but largely haven't met with any success. One of the workarounds [I've seen suggested](https://groups.google.com/forum/#!topic/keras-users/rEJ1xYqD3AM) is to try and shove the result of `tf.summary.merge_all` into a `metric`, but this does not work since Keras wants those to be numeric and summaries are strings. This does not work if I put this in as `target_tensors` either, as commented out in the gist.\r\n\r\nNote: I recognize that this is filed against TF 1.9, but I don't see anything in the 1.10 changelog to indicate that this was noted or fixed. I am in a weird situation where it is best for me to use 1.9, unfortunately.\r\n\r\n### Source code / logs\r\nThe [gist](https://gist.github.com/zmjjmz/75ba919d5f2755738252b4d0b0032faa) here reproduces this issue by taking a common function to get the tensors for a model, and creating estimators in both scenarios. \r\n\r\nBelow is the first event with a summary from each case. It shows that in the 'plain' case, the summary tag `class_norm` shows up whereas in the `keras` case it does not. This is the crux of the issue I'm encountering.\r\n\r\nPlain case:\r\n`{'value': [{'simple_value': 1.0, 'tag': u'enqueue_input/queue/enqueue_input/random_shuffle_queuefraction_over_250_of_750_full'}, {'simple_value': 4.119956016540527, 'tag': u'class_norm'}, {'simple_value': 0.6801050901412964, 'tag': u'loss'}]}`\r\nKeras case:\r\n`{'value': [{'simple_value': 1.0, 'tag': u'enqueue_input/queue/enqueue_input/random_shuffle_queuefraction_over_250_of_750_full'}, {'simple_value': 0.8805878758430481, 'tag': u'loss_1'}]}`\r\n\r\n", "comments": ["@karmel @tanzhenyu : mind taking a look?", "See also https://github.com/tensorflow/tensorflow/issues/17571", "Ah I didn't find that when I was looking around for related issues. I'll try that workaround for now I guess, even if ti's pretty ugly.", "Seems that some use of that \"workaround\" now create problem with DistributionStrategy. I still hope as you and others in the old ticket that we could have a straight/pretty solution but I've not seen any proposal. ", "Oof, ok I'll see if I can reproduce that as well.", "I posted a response at #17571 -- referring to that response here, without duplication to avoid splitting the thread: @zmjjmz , I see that you had issues with the model_fn route previously; does the use of `head` as in that example help? Or is it still too cumbersome? Is there a code sample you can provide?\r\n\r\nCC @omalleyt12 , @ispirmustafa ", "I'd refer to the code sample provided in the gist I linked. Previously I was going the pure `model_fn` route, however the issue with the way I was doing it before was that by manually creating the loss function I was losing anything that might contribute to that loss function, e.g. an L2 penalty.\r\n\r\nIt's not clear to me that using the `multi_class_head` solves this, as it seems to create the loss function [here](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/estimator/canned/head.py#L602) which doesn't appear to have any knowledge of the Keras superstructure and thus regularizers that are meant to be applied, but I could be wrong. Would it be able to take those into account?", "The main advantage of the model_to_estimator is that we can skip the implementation of the model_fn. Writing the pure model_fn is not convenient for who uses the keras model and model_to_estimator to get the estimator. If adding just a summary or warm start an estimator will require an explicit model_fn manually defined the scope of model_to_estimator will be too narrow.", "I went down the `model_fn` route using `estimator.train_and_evaluate` with version `1.12`. However, no summaries get written in `train` mode, while they do get written in `eval` mode. I've tried adding an instance of `train.SummarySaverHook` to `estimator.TrainSpec.hooks`, but this also had no effect. I've also tried explicitly specifying `summary.scalar` in `model_fn`, but this didn't help either.", "After inspecting the event files using `train.summary_iterator`, I realized that the summaries got written, but were not displayed in tensorboard.\r\n\r\nSpecifying `training` as log directory name when launching tensorboard solved the problem for me:\r\n```\r\ntensorboard --logdir training:/path/to/model_dir --host localhost --port 6006\r\n```\r\n\r\nThe [comment](https://github.com/tensorflow/tensorflow/issues/9701#issuecomment-301114886) by @MMCMA on issue #9701 pointed me in the right direction.", "Hey @hackermd was that effective in getting all summaries written for the Keras case? I'm not able to use the `training:model_dir` workaround to get the `class_norm` summaries to show up (in TF 1.12), which is unsurprising since I'm still not seeing it written to the events file.", "@zmjjmz I've only tested `scalar` and `image` summaries. Both of them work.", "So @hackermd just to be clear are you able to put a `SummarySaverHook` into the `TrainSpec` that goes into `train_and_evaluate` along with an Estimator created by putting a Keras model thru `keras.model_to_estimator`? I've been attempting this with no luck -- the `merge_all` op seems to exist outside of the graph that is used during `train_and_evaluate`. \r\n\r\nDid you manage to work around this?", "@zmjjmz I didn't create an estimator via `keras.model_to_estimator`, but used the `model_fn` approach, i.e. I constructed the model within the scope of `model_fn` using `Layer` implementations provided by the `keras.layers` API (with `model_fn` returning an instance of `estimator.EstimatorSpec`). This approach worked without passing an instance of `train.SummarySaverHook` to `estimator.TrainSpec`.", "@hackermd how do I write an estimator directly without using `model_to_estimator`, can you clarify what to do in the marked lines in this minimal example?\r\n\r\n```py\r\ndef model_fn(features, labels, mode, params):   \r\n\r\n    # say for mnist, we have a simple keras sequential model\r\n    model = get_keras_model()    \r\n    model.compile(optimizer=Adam(lr=params.learning_rate), loss=tf.keras.losses.categorical_crossentropy, metrics=[tf.keras.metrics.categorical_accuracy])\r\n    predictions = model(features)\r\n    \r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions={'output': predictions})\r\n    \r\n    loss = model.get_losses_for(features)\r\n    accuracy = model.how_do_get_metrics()            <---THIS LINE-----\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        eval_metrics_ops = {\"accuracy\": accuracy}    <---THIS LINE-----\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metrics_ops)\r\n        \r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        train_op = how_do_get_train_op               <---THIS LINE-----\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n    \r\n    raise NotImplementedError(f\"Unknown mode {mode}\")\r\n```\r\n\r\nWill help those who want to use a workaround, until the bug is fixed :)", "I tried\r\n\r\n```py\r\noptimizer = model.optimizer\r\ntrain_op = optimizer.minimize(lambda: model.loss(labels, logits), var_list=model.trainable_variables)\r\nreturn tf.estimator.EstimatorSpec(mode, loss=model.loss, train_op=train_op)\r\n```\r\n\r\nBut I got `No gradients provided` error,\r\n\r\n```\r\n<ipython-input-21-f0637616c1fb> in model_fn(features, labels, mode, params)\r\n     34         # train_op = model.train_function.updates_op\r\n     35         optimizer = model.optimizer\r\n---> 36         train_op = optimizer.minimize(lambda: model.loss(labels, logits), var_list=model.trainable_variables)\r\n     37         return tf.estimator.EstimatorSpec(mode, loss=model.loss, train_op=train_op)\r\n     38 \r\n\r\n~/.conda/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in minimize(self, loss, var_list, grad_loss, name)\r\n    317         loss, var_list=var_list, grad_loss=grad_loss)\r\n    318 \r\n--> 319     return self.apply_gradients(grads_and_vars, name=name)\r\n    320 \r\n    321   def _compute_gradients(self, loss, var_list, grad_loss=None):\r\n\r\n~/.conda/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)\r\n    425       ValueError: If none of the variables have gradients.\r\n    426     \"\"\"\r\n--> 427     grads_and_vars = _filter_grads(grads_and_vars)\r\n    428     var_list = [v for (_, v) in grads_and_vars]\r\n    429 \r\n\r\n~/.conda/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)\r\n   1023   if not filtered:\r\n   1024     raise ValueError(\"No gradients provided for any variable: %s.\" %\r\n-> 1025                      ([v.name for _, v in grads_and_vars],))\r\n   1026   if vars_with_empty_grads:\r\n   1027     logging.warning(\r\n\r\nValueError: No gradients provided for any variable: ['layer_1_conv_2d/kernel:0', 'layer_1_conv_2d/bias:0', 'layer_2_conv_2d/kernel:0', 'layer_2_conv_2d/bias:0', 'dense/kernel:0', 'dense/bias:0'].\r\n```", "I was able to resolve no gradients error using\r\n\r\n```\r\nlambda: model.loss(labels, model(features))\r\n```\r\n\r\ninstead of \r\n\r\n```\r\nlambda: model.loss(labels, logits))\r\n```\r\n\r\nThis happens probably because the variable `logits` is not tracked by Gradient Tape and hence results in No Gradient. If you used a batch_norm layer, you may get `Incompatible types: <dtype: 'variant'> vs. int64. Value is 1`, don't know how to resolve it, hence I removed the batch_norm layer.\r\n\r\n**I was finally able to build the train_op part successfully**, but noticed that the `global_step` was always zero. I verified that the train_op is being incremented running separately i.e. outside the model_fn.\r\n\r\nNow I'm don't know what to do if I want estimators in tf2. Compiled keras model with model_to_estimators is not logging summaries, custom made estimator using tf.keras.layers isn't logging summaries either. **How do we log from estimators then?**", "Is this still unresolved?  How do we log with estimators using model_to_estimator?  ", "Hi @zmjjmz !We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to TF 2.6 version and Python 3.6-3.8 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .   Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21983\">No</a>\n"]}]