[{"number": 37308, "title": "[XLA] Compare only the filename, not the full path.", "body": "A split from another PR: https://github.com/tensorflow/tensorflow/pull/37260/files/33945aa088d9159fae3d7356a9933b4c14d39b1e#r387799764\r\n\r\nThis allow to put the forced PTX in directories.\r\n\r\n@cheshire ", "comments": ["Could you give more context for this change? Is it possible to write a test?", "This is a debug features for XLA dev. Sometimes we need to test what would be the results if we change the generated PTX to something else without changing everything. Just to evaluate what would be the results if we did this change. The flag `XLA_FLAGS=--xla_gpu_ptx_file=FILENAME` take a filename that point to the PTX for a specific module.\r\n\r\nThe workflow, get the PTX for the module you want to modify. Modify this file to be what you want. Give the filename with the flag to XLA. Then XLA will pick up that PTX instead of what it normally generate.\r\n\r\nBefore this PR, the flag was forcing us to put the file in the current directly. Which isn't nice. Now with this PR, we can put the file in a directory. So we can organize them more easily.\r\n\r\nI didn't found how to test this dev only feature."]}, {"number": 37307, "title": "gradient of `einsum` is incorrect for complex numbers", "body": "**System information** \r\n- Attached is a small script reproducing the problem\r\n- OS Platform and Distribution: Ubuntu 18.04 LTS\r\n- TensorFlow installed from pip\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: Python 3.7.5\r\n- Observed both on CPU and GPU\r\n\r\n**Describe the current behavior**\r\n\r\nThe gradient of this `tf.matmul` expression with respect to `p` is computed correctly:\r\n\r\n```python\r\n# `h` is an NxN complex128 matrix, `p` is float64 number, and `zero` is a float64 zero\r\ndef f(p,h):\r\n    h1 = tf.complex(p,zero) * h\r\n    return tf.abs(tf.reduce_sum(tf.matmul(h,h)))\r\n```\r\n\r\nBut the value of the following `tf.einsum` expression is the same (and computed correctly), while the gradient with respect to `p` is wrong:\r\n\r\n```python\r\ndef f(p,h):\r\n    h1 = tf.complex(p,zero) * h\r\n    return tf.abs(tf.reduce_sum(tf.einsum('ab,bc->ac',h,h)))\r\n```\r\n\r\nThe problem happens only when the matrices are complex.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe two functions should produce the same value (which is working fine), and their gradients with respect to `p` should be the same (which is not happening).\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nhttps://colab.research.google.com/drive/1FdC-x5Q74NqoLiB6kR7Omtcb5npdrDtx", "comments": ["I'm also experiencing this issue (even with tf-nightly '2.2.0-dev20200406').\r\n\r\nAny ETA on a fix?", "I'm also experiencing this issue, and would love to see it resolved:)", "Same issue for me.", "@gowthamkpr @alextp  this issue has been blocking several research projects for weeks now, do you know if anyone is looking into a fix?", "@ziofil, FYI `einsum` calls can be replaced by calls to `matmul`/`tensordot` and `transpose`. It seems you use tensorflow in the same type of research work as me, and while it is a bit more clunky to write the code with `matmul` and `transpose` it ended working fine and unblocked my work (and this bug does not exist there).\r\n\r\nOf course, this is no reason not to try to fix this bug.", "Would another solution be to do the following?\r\n```python\r\nfrom tensorflow.python.ops.special_math_ops import _einsum_v1 as einsum\r\n```\r\n\r\nIt seems that TensorFlow 2.1 [introduced a significant refactor of `tf.einsum`](https://github.com/tensorflow/tensorflow/commit/4eba4d514e24b6c1fdae69b40c2b958cc2a1cb42) to use the optimized einsum library, and that might be where the bug was introduced. `_einsum_v1` seems to be the original implementation.", "For example:\r\n```python\r\nfrom tensorflow.python.ops.special_math_ops import _einsum_v1\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef f1(h):\r\n    return tf.abs(tf.reduce_sum(tf.matmul(h,h)))\r\n\r\ndef f2(h):\r\n    return tf.abs(tf.reduce_sum(tf.einsum('ab,bc->ac',h,h)))\r\n\r\ndef f3(h):\r\n    return tf.abs(tf.reduce_sum(_einsum_v1('ab,bc->ac',h,h)))\r\n\r\ntf.random.set_seed(1)\r\nA = tf.Variable(tf.random.uniform(shape=[2, 2]))\r\nB = tf.random.uniform(shape=[2, 2])\r\n\r\nfor f in (f1, f2, f3):\r\n    with tf.GradientTape() as tape:\r\n        C = tf.cast(A, dtype=tf.complex64) + tf.cast(B, dtype=tf.complex64)*1j\r\n        loss = f(C)\r\n\r\n    grad = tape.gradient(loss, A)\r\n    print(grad)\r\n```\r\n\r\nThis gives the output\r\n\r\n```python\r\ntf.Tensor(\r\n[[1.6376667 2.0820131]\r\n [2.087226  2.5315723]], shape=(2, 2), dtype=float32)\r\ntf.Tensor(\r\n[[-2.0803595 -2.5244465]\r\n [-2.6582363 -3.1023233]], shape=(2, 2), dtype=float32)\r\ntf.Tensor(\r\n[[1.6376667 2.0820131]\r\n [2.087226  2.5315723]], shape=(2, 2), dtype=float32)\r\n```", "@bloops are you aware of this issue?", "Seems like a number of additional cases fail for complex numbers, which weren't noticed since gradient tests are performed only for `float64`.", "I wasn't aware of this. I'll take a look...", "I've tried adding complex tests, many are failing for complex: https://github.com/Randl/tensorflow/commit/f6865e57c93a1d92df30aee4b3756b2979b9125c", "> Seems like a number of additional cases fail for complex numbers, which weren't noticed since gradient tests are performed only for `float64`.\r\n\r\n@Randl do you mean for einsum or in general?", "for einsum. I haven't checked other things, but maybe it worth adding complex number to tests in more systematic way", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37307\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37307\">No</a>\n"]}, {"number": 37306, "title": "Bug with callbacks in tf.keras model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 19.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): pip install / tensorflow==2.0\r\n- Python version: - Bazel\r\nversion (if compiling from source): 3.7.5\r\n- GCC/Compiler version (if compiling from\r\nsource): N/A\r\n- CUDA/cuDNN version: - GPU model and memory: n/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen force stopping a model on training, all the callbacks(on_epoch_end, on_train_end) are called. This does not happen with a keras model.\r\n**Describe the expected behavior**\r\nThe other callbacks, namely on_epoch_end and on_train_end aren't supposed to be called.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/1Uu_LPSgdyXAyWubGAQDpbMq8BcUcXqUd\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@CleanPegasus,\r\nI was able to reproduce the issue with [TF2.1](https://colab.sandbox.google.com/gist/amahendrakar/ae4921f2d23cd2dfdf60524888168574/37306.ipynb), however the issue seems to be fixed in [TF-nightly](https://colab.sandbox.google.com/gist/amahendrakar/da264a2d14b1b6242ee67f663e5059fe/37306-tf-nightly.ipynb). With TF-nightly, I see that the functions `on_epoch_end` and `on_train_end` are not being called.\r\nPlease find the attached gist. Thanks!  ", "Thanks for the help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37306\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37306\">No</a>\n"]}, {"number": 37305, "title": "[r2.2:Cherrypick] Fix windows GPU build - revert absl update", "body": "Also:\n* look for xla sources under org_tensorflow\n* don't include absl strings:cord since the older version of absl lacks it.\nPiperOrigin-RevId: 298638795\nChange-Id: Ic85d55240aaa353db60bebea0f0919cbfc5208f1", "comments": []}, {"number": 37304, "title": "Implement manip gradient in cc.", "body": "Create the Roll op gradient function in `manip_grad.cc` corresponding to the `manip_grad.py`. ", "comments": ["@siyavash can you please add some description to the PR", "Sure, updated.", "Approved, once those  minor comments are addressed, thanks!", "@suharshs \r\nHow long does it usually take for status to be reported ?!"]}, {"number": 37303, "title": "tf.data unique leads to negative cardinality", "body": "**System information** \r\n- **OS** Reproduced on Win 10, not checked on other OS.\r\n- **TensorFlow**: Version 2.1, installed with pip\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version:** 10.1\r\n- **GPU model**:  GeForce GTX 1050\r\n\r\n**Describe the current behavior**\r\nCardinalities (evaluated using `tf.data.experimental.cardinality`) become negative after using `ds.apply(tf.data.experimental.unique())`\r\n\r\n**Describe the expected behavior**\r\nCardinalities should not become negative after using `ds.apply(tf.data.experimental.unique())`\r\n\r\n**Standalone code to reproduce the issue** \r\nRunning the following\r\n```\r\nds = tf.data.Dataset.from_tensor_slices([0,1])\r\nunique_ds = ds.apply(tf.data.experimental.unique())\r\nprint(tf.data.experimental.cardinality(ds).numpy())\r\nprint(tf.data.experimental.cardinality(unique_ds).numpy())\r\n```\r\nPrints these lines\r\n```\r\n2\r\n-2\r\n```\r\nWhile I would expect it to print\r\n```\r\n2\r\n2\r\n```\r\n", "comments": ["I just learned that `-2` is a constant indicating that the cardinality could not be computed in constant time. Thus, this is **not a bug**. \r\n\r\nThe calculation, while quite slow, can be done as follows\r\n`\r\nunique_ds.reduce(np.int64(0), lambda x,_ : x + 1)\r\n`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37303\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37303\">No</a>\n"]}, {"number": 37302, "title": "Add Earlystopping function to train.py of the speech commands API", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nAs you know, the amount of training epochs is hard coded in the train.py file of the API. the 18000 epochs are perfect if we train the model with the official dataset. \r\n\r\n**Will this change the current api? How?**\r\n\r\nAdding a Earlystop function will prevent under or overfitting regardless which dataset will be trained. \r\n\r\n**Who will benefit with this feature?**\r\n\r\nI would really apreciate if one of your could emplement a version of train.py with a Early stop flag. \r\nThis would be a plus for the API and of big help for my use case.\r\n\r\n**Any Other info.**\r\n\r\nIf you guys could give me a hint about how to add this feature to the existing API, I'd be happy to share my work.\r\n\r\nThank you very much \r\n\r\nKind regards\r\n", "comments": ["We're porting this example to TF2.0 completely and will ensure this is factored in. I have personally wanted this feature as well, and it's one of our requirements for porting it. Is this a blocker for you right now?  ", "Hi Megna,\n\nthese are really great news. This feature is a great plus if we want to add a smooth workflow that consists on adding new labels and train.\n\nCurrently I have to adjust the number of epochs manuals depending on my training results and the EarlyStopping function would save me a lot of time. This is kind of a Blocker because my workflow is not completely automated :-).\n\nI a really looking forward to the 2.0 version of this API. When do you think that EarlyStopping will be implemented?\n\nBest thanks \n\n> Am 06.03.2020 um 01:36 schrieb Meghna Natraj <notifications@github.com>:\n> \n> \ufeff\n> We're porting this example to TF2.0 completely and will ensure this is factored in. I have personally wanted this feature as well, and it's one of our requirements for porting it. Is this a blocker for you right now?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "@alex83803 we will not be updating the TF1 code, however you can follow the updates in #41310 for the TF2 version. Closing this issue.", "Hey Meghna,\n\nNo Problem this is what we had discussed.\n\nWhen do you think the tensorflow 2.0 version of the speech recognition framework will be ready?\n\nKind regards\n\nAlex \n\n> Am 12.07.2020 um 03:48 schrieb Meghna Natraj <notifications@github.com>:\n> \n> \ufeff\n> @alex83803 we will not be updating the TF1 code, however you can follow the updates in #41310 for the TF2 version. Closing this issue.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "This is an additional feature, so it's not high priority - could take weeks/few months! Subscribe to #41310 for updates."]}, {"number": 37301, "title": "CombinedNonMaxSuppression is not whitelisted operator", "body": "Hi! CombinedNonMaxSuppression is not whitelisted operator. Is it possible to add it as a whitelisted operator?\r\n\r\n**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed from Docker Image\r\n- TensorFlow version 2.1.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2020-03-04 15:50:57.748145: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-03-04 15:50:57.748972: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-03-04 15:50:57.771173: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-03-04 15:50:57.771259: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.018ms.\r\n2020-03-04 15:50:57.771283: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.011ms.\r\n2020-03-04 15:50:59.782146: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-03-04 15:50:59.782403: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-03-04 15:51:00.470079: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-03-04 15:51:00.470148: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 360 nodes (-59), 639 edges (-59), time = 377.824ms.\r\n2020-03-04 15:51:00.470167: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 360 nodes (0), 639 edges (0), time = 39.896ms.\r\nTraceback (most recent call last):\r\n  File \"./to_tf_lite.py\", line 54, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"./to_tf_lite.py\", line 44, in main\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py\", line 457, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py\", line 203, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-03-04 15:51:02.143649: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-03-04 15:51:02.146593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n2020-03-04 15:51:03.291126: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-03-04 15:51:03.291210: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-03-04 15:51:03.291322: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-03-04 15:51:03.291357: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-03-04 15:51:03.291468: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: CombinedNonMaxSuppression\r\n2020-03-04 15:51:03.297057: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 223 operators, 426 arrays (0 quantized)\r\n2020-03-04 15:51:03.299828: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 223 operators, 426 arrays (0 quantized)\r\n2020-03-04 15:51:03.400832: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 105 operators, 198 arrays (0 quantized)\r\n2020-03-04 15:51:03.402525: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 105 operators, 198 arrays (0 quantized)\r\n2020-03-04 15:51:03.404185: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 105 operators, 198 arrays (0 quantized)\r\n2020-03-04 15:51:03.405338: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 105 operators, 198 arrays (0 quantized)\r\n2020-03-04 15:51:03.406273: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 105 operators, 198 arrays (0 quantized)\r\n2020-03-04 15:51:03.407942: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 22151168 bytes, theoretical optimal value: 22151168 bytes.\r\n2020-03-04 15:51:03.408239: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 8849370\r\n2020-03-04 15:51:03.408888: W tensorflow/lite/toco/tflite/operator.cc:2024] Op CombinedNonMaxSuppression is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-03-04 15:51:03.409165: W tensorflow/lite/toco/tflite/operator.cc:2024] Op CombinedNonMaxSuppression is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-03-04 15:51:03.409257: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MAX_POOL_2D, MUL, PACK, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MAX_POOL_2D, MUL, PACK, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n--\r\n\r\n**Any other info / logs**\r\n\r\n--", "comments": ["@timzatko \r\nCould you please share simple standalone code that you executed before facing this issue.\r\n\r\n Did you try adding converter.allow_custom_ops = True to enable custom ops in the tf.lite? \r\nAlso please refer to [this issue](https://github.com/tensorflow/tensorflow/issues/35449#issuecomment-570318723) and let us know if it helps. Thanks!", "I did not try to add converter.allow_custom_ops, if I add it, I have to implement that operator in c, right? Since it is tensorflow operator, I would like to avoid that.\r\n\r\nSteps to reproduce:\r\n1. Clone this repository:\r\nhttps://github.com/zzh8829/yolov3-tf2\r\n2. Download model and copy it to `checkpoints` path https://drive.google.com/open?id=1gl4JY4sbyjFXOFiz9MZsd84PL4vuhy3Q\r\n3. Create this script in the root of the repository and run it\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom absl import app, flags, logging\r\nfrom absl.flags import FLAGS\r\n\r\nfrom yolov3_tf2.models import YoloV3, YoloV3Tiny\r\n\r\nflags.DEFINE_string('weights', './checkpoints/yolov3-tiny.tf', 'path to weights file')\r\nflags.DEFINE_boolean('tiny', True, 'yolov3 or yolov3-tiny')\r\nflags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\r\nflags.DEFINE_integer('input_size', 416, 'image size')\r\n\r\n\r\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/flex/whitelisted_flex_ops.cc\r\n# https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression\r\n# https://www.tensorflow.org/s/results/?q=combined_non_max_suppression\r\n\r\ndef main(_argv):\r\n    if FLAGS.tiny:\r\n        yolo = YoloV3Tiny(FLAGS.input_size, classes=FLAGS.num_classes)\r\n    else:\r\n        yolo = YoloV3(FLAGS.input_size, classes=FLAGS.num_classes)\r\n\r\n    yolo.load_weights(FLAGS.weights).expect_partial()\r\n    logging.info('weights loaded')\r\n\r\n    path = 'tmp/yolov3.h5'\r\n    out_path = 'tmp/yolov3.tflite'\r\n\r\n    if FLAGS.tiny:\r\n        path = 'tmp/yolov3-tiny.h5'\r\n        out_path = 'tmp/yolov3-tiny.tflite'\r\n\r\n    yolo.save(path)\r\n\r\n    logging.info(f'keras model saved to location {path}')\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(yolo)\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS\r\n    ]\r\n    # converter.experimental_new_converter = True\r\n    # converter.experimental_new_quantizer = True\r\n    # converter.allow_custom_ops = True\r\n\r\n    tflite_model = converter.convert()\r\n    logging.info(f'model converted')\r\n\r\n    open(out_path, 'wb').write(tflite_model)\r\n\r\n    logging.info(f'tflite model saved to location {out_path}')\r\n\r\n\r\nif __name__ == '__main__':\r\n    try:\r\n        app.run(main)\r\n    except SystemExit:\r\n        pass\r\n```\r\n\r\nI tried setting `converter.experimental_new_converter = True` and here is the result.\r\n\r\n```\r\n2020-03-05 07:00:50.823406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-03-05 07:00:50.834836: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n2020-03-05 07:00:51.577175: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-03-05 07:00:51.577237: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (-1)\r\n2020-03-05 07:00:51.577259: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c807e06a611a): /proc/driver/nvidia/version does not exist\r\n2020-03-05 07:00:51.578259: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-05 07:00:51.589959: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3096000000 Hz\r\n2020-03-05 07:00:51.591151: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x58cdd60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-05 07:00:51.591490: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nI0305 07:00:52.946302 139749900048192 to_tf_lite.py:25] weights loaded\r\nI0305 07:00:53.313143 139749900048192 to_tf_lite.py:36] keras model saved to location tmp/yolov3-tiny.h5\r\n2020-03-05 07:00:53.590270: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-03-05 07:00:53.590894: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-03-05 07:00:53.602172: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-03-05 07:00:53.602322: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.005ms.\r\n2020-03-05 07:00:53.602341: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-03-05 07:00:54.439903: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-03-05 07:00:54.440240: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-03-05 07:00:54.700659: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-03-05 07:00:54.700721: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 360 nodes (-59), 639 edges (-59), time = 132.996ms.\r\n2020-03-05 07:00:54.700740: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 360 nodes (0), 639 edges (0), time = 36.936ms.\r\nTraceback (most recent call last):\r\n  File \"to_tf_lite.py\", line 56, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"to_tf_lite.py\", line 46, in main\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py\", line 457, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py\", line 203, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-03-05 07:00:55.882389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-03-05 07:00:55.883827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n2020-03-05 07:00:56.553882: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:89] Ignored output_format.\r\n2020-03-05 07:00:56.553939: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:95] Ignored drop_control_dependency.\r\n2020-03-05 07:00:56.863763: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-05 07:00:56.868908: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3096000000 Hz\r\n2020-03-05 07:00:56.869444: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x605b620 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-05 07:00:56.869491: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-03-05 07:00:56.870769: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-03-05 07:00:56.870813: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (-1)\r\n2020-03-05 07:00:56.870833: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c807e06a611a): /proc/driver/nvidia/version does not exist\r\nloc(callsite(\"yolov3_tiny/yolo_nms/combined_non_max_suppression/CombinedNonMaxSuppression\"(\"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_image_ops.py\":378:0) at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/image_ops_impl.py\":3958:0 at callsite(\"/home/src/yolov3_tf2/models.py\":198:0 at callsite(\"/home/src/yolov3_tf2/models.py\":254:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/core.py\":846:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\":778:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\":891:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\":717:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\":778:0 at \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saving_utils.py\":150:0)))))))))): error: 'tf.CombinedNonMaxSuppression' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main'\r\nOps that need custom implementation (enabled via setting the -emit-custom-ops flag): CombinedNonMaxSuppression\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_image_ops.py:378:38: error: 'tf.CombinedNonMaxSuppression' op is neither a custom op nor a flex op\r\n                                     clip_boxes=clip_boxes, name=name)\r\n                                     ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/image_ops_impl.py:3958:9: note: called from\r\n        score_threshold, pad_per_class, clip_boxes)\r\n        ^\r\n/home/src/yolov3_tf2/models.py:198:9: note: called from\r\n        score_threshold=FLAGS.yolo_score_threshold\r\n        ^\r\n/home/src/yolov3_tf2/models.py:254:5: note: called from\r\n    outputs = Lambda(lambda x: yolo_nms(x, anchors, masks, classes),\r\n    ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/core.py:846:7: note: called from\r\n      result = self.function(inputs, **kwargs)\r\n      ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py:778:19: note: called from\r\n                  outputs = call_fn(cast_inputs, *args, **kwargs)\r\n                  ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py:891:11: note: called from\r\n          output_tensors = layer(computed_tensors, **kwargs)\r\n          ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py:717:9: note: called from\r\n        convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n        ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py:778:19: note: called from\r\n                  outputs = call_fn(cast_inputs, *args, **kwargs)\r\n                  ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saving_utils.py:150:7: note: called from\r\n      outputs_list = nest.flatten(model(inputs=inputs, training=False))\r\n      ^\r\n<unknown>:0: error: failed while converting: 'main'\r\nOps that need custom implementation (enabled via setting the -emit-custom-ops flag): CombinedNonMaxSuppression\r\n```\r\n\r\n", "I am also getting similar error [notebook link](https://github.com/anspire/Notebooks/blob/master/yolov3_tiny_keras.ipynb)\r\n`RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 41 (FlexSize) failed to prepare.\r\n`\r\n\r\nlog test file of converted model while running on android native c++\r\n[android_native_log.txt](https://github.com/tensorflow/tensorflow/files/4292321/android_native_log.txt)\r\n", "@timzatko,\r\nCould you please un-comment these three lines from your code and then try to convert the model.\r\n```\r\n# converter.experimental_new_converter = True\r\n# converter.experimental_new_quantizer = True\r\n# converter.allow_custom_ops = True\r\n```\r\n\r\nI was able to create the .tflite file after removing the comments. Please check the attached screenshot for the output.\r\n![37301](https://user-images.githubusercontent.com/57165142/76396663-9e006f80-639f-11ea-8b45-ded650c65085.png)\r\n\r\n[37301_complete_logs.txt](https://github.com/tensorflow/tensorflow/files/4316878/37301_complete_logs.txt)\r\n", "Hi! When I uncomment this line\r\n\r\n`# converter.allow_custom_ops = True`\r\n\r\nI'm able to convert the model.\r\n\r\nHowever, I need to provide an implementation of that operator to the TensorFlow Lite.\r\n\r\n> allow_custom_ops: Boolean indicating whether to allow custom operations. When false any unknown operation is an error. When true, custom ops are created for any op that is unknown. **The developer will need to provide these to the TensorFlow Lite runtime with a custom resolver**. (default False)\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter\r\n\r\nSince the `CombinedNonMaxSuppression` operator is a Tensorflow operator I believe it can be whitelisted in `lite/delegates/flex/whitelisted_flex_ops.cc` (https://www.tensorflow.org/lite/guide/ops_select) so will not need to provide that implementation of that operator.\r\n\r\nI forked the repository and added it there by myself, I was able to convert the model then. https://github.com/timzatko/tensorflow (https://hub.docker.com/layers/timzatko/tensorflow/2.1.0-gpu-py3/images/sha256-f1838218fc5d333153a4594c0c76925aab334ee83e7de97700bc91b36b613d30?context=repo) However, I did not try to run inference on it yet since I need to build interpreter for my use-case - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/examples/unity/TensorFlowLitePlugin.", "@timzatko any luck with converting the model?", "> @timzatko any luck with converting the model?\r\n\r\nHi, I was able to convert the model by whitelisting that operator. There's a docker image of TensorFlow with my fix above, you can use it to convert a model.\r\n\r\nI also tried to do inference in my app (it's a Unity app based on this example https://github.com/asus4/tf-lite-unity-sample/tree/master/Assets/Samples/SSD), however, it does not work for some reason (I don't remember why). Maybe I did a conversion with the wrong settings. Then I decided to use another model and don't waste my time with this.", "@timzatko Thx, I'm heading similar direction. Can I ask which model are you using?", "> @timzatko Thx, I'm heading similar direction. Can I ask which model are you using?\r\n\r\n**ssd_mobilenet_v1_coco** (http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz)\r\nfrom tensorflow model zoo https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\r\n\r\nHere is the tutorial how to train it on GCP https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md", "@timzatko seems like adding the operator to the whitelisted operators solves the issue, can you make a PR from your repo, please", "Hi, seems like we have a solution here. Can we close this issue?", "The PR was never done, this was never committed or at least there is no reference in this issue that this was fixed, so until the issue is fixed I don't think we can close it. And @timzatko fork seems to not exist anymore.", "Actually the `CombinedNonMaxSuppression` is already in the allowlist of the flex delegate for a while:\r\nhttps://github.com/tensorflow/tensorflow/blob/b333031345c13c3d9b7a0f06d8a5871e76d89571/tensorflow/lite/delegates/flex/allowlisted_flex_ops.cc#L119\r\n\r\nSince this is already added, I would recommend the user to try it with the most recent tf release, or we just close this issue since the main concern is addressed.", "Yeah, the issue has been addressed, I believe that it can be closed now :) "]}, {"number": 37300, "title": "Crashing introduced when call kernel.matrix with latest nightly build of TF 2.2", "body": "Cross Post from Tensorflow Probability (at request of admins)\r\n\r\nhttps://github.com/tensorflow/probability/issues/829\r\n\r\n**System information** \r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from : - Binary\r\nTensorFlow version: 2.2 Nightly\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: - 10.1\r\n- GPU model and memory:  Nvidia 2070 Max_Q\r\n\r\nUsing TFP 0.10.0-dev20200304, I have a standard call to create a positive semi-definite kernel e.g.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\nimport numpy as np\r\n\r\namplitude = 1.0\r\nlength_scale = 1.0\r\nkernel = tfp.math.psd_kernels.ExponentiatedQuadratic(amplitude, length_scale)\r\nt_x = tf.Variable(np.array([[0.0, 0.25]]), dtype=tf.float32)\r\nt_y = tf.Variable(np.array([[0.5, 0.5]]), dtype=tf.float32)\r\nkernel.matrix(t_x, t_y)\r\n```\r\n\r\nThis works apparently fine with all versions of TF-GPU upto and including GPU 2.2.0-dev20200302.\r\n\r\nHowever, if I upgraded to TF-GPU 2.2.0-dev20200303 and now it crashes the following message :\r\nProcess finished with exit code -1073740940 (0xC0000374)\r\n\r\nSeems like some incompatibility has been introduced with last nights build.\r\n", "comments": ["@oracle3001 \r\nCan you try running the code in latest -tf-nightly 2.2.0-dev20200304version? Issue seemed to be fixed, kindly find the [gist of colab ](https://colab.sandbox.google.com/gist/ravikyram/2fc4071083b91e0991695f16d4b8d11d/untitled704.ipynb)for the same.Thanks!", "Yes, that has fixed the issue. Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37300\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37300\">No</a>\n"]}, {"number": 37299, "title": "Which NVIDIA GPU to buy for TF with Windows?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1 & 10 dual-boot\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: the =last one\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?:pip conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10\r\n- GPU model and memory: That's the question!\r\n\r\n\r\n\r\n**Describe the problem**\r\nHello, I've tried since some days to use TF with my GPU GeForce GTX 760. It doesn't work because its computing capabilities are 3.0. \r\nI've even tried to recompile the 1.8 version unsuccessfully.\r\nWell, I've decided to change it. \r\nThe NVIDIA MSI GTX 1050 Ti 4GT OC - 4 Go seems to fit with the requisites but I can read it's not more available on their site.\r\nIs it recommended anywhere?\r\nI will mainly use TF for fractal art.\r\nBelow some infos of my PC.\r\nThanks for your help and patience.\r\nBest regards,\r\nFran\u00e7ois\r\nType de processeur\tQuadCore Intel Core i5-4670K, 4200 MHz (42 x 100)\r\nNom de la carte m\u00e8re\tAsus Z87-A  (2 PCI, 2 PCI-E x1, 3 PCI-E x16, 4 DDR3 DIMM, Audio, Video, Gigabit LAN)  3.0 (Gen 3) slots https://www.asus.com/fr/Motherboards/Z87A/\r\nDIMM1: G Skill RipjawsX F3-12800CL9-4GBXL\t4 Go DDR3-1600 DDR3 SDRAM  (11-11-11-28 @ 800 MHz)  (10-10-10-27 @ 761 MHz)  (9-9-9-24 @ 685 MHz)  (8-8-8-22 @ 609 MHz)  (7-7-7-19 @ 533 MHz)  (6-6-6-16 @ 457 MHz)\r\nMoniteur\tSamsung SyncMaster S24F350  [24\" LCD]  (H4ZM101320)\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Forgotten: alim= 650 w, total memory 16 Go.", "@FB43,\r\nAny GPU which satisfies the hardware and software requirements listed [here](https://www.tensorflow.org/install/gpu#hardware_requirements) can be used to build the TensorFlow GPU package. \r\n\r\nYou can also check out the tested build configurations for Windows from the official [documentation](https://www.tensorflow.org/install/source_windows#gpu). Thanks!", "OK. The new GPU is on the list! Thanks for your help. Fran\u00e7ois", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37299\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37299\">No</a>\n"]}, {"number": 37298, "title": "Cannot load SavedModel involving slice", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.2\r\n- Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: Cuda 10.1, cuDNN 7.6.3, \r\n- GPU model and memory: NVIDIA GeForce GTX 1080, 8 GB\r\n\r\n**Describe the current behavior**\r\nWhen importing a SavedModel that contains a slice operation with tf.keras.models.load_model, an exception is thrown:\r\n```\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * Tensor(\"inputs:0\", shape=(None, 5, 16), dtype=float32)\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 1 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (1 total):\r\n    * [TensorSpec(shape=(None, 5, 16), dtype=tf.float32, name='inputs/0')]\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe keras model should be importet.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\n\r\ninp = tf.keras.Input((5,16), dtype='float', name='bb')\r\nnet = inp[:,3:,:]\r\nnet = tf.keras.layers.Conv1D(2,1)(net)\r\n\r\nmodel = tf.keras.Model(inputs=inp, outputs=net)\r\n\r\noptimizer = tf.keras.optimizers.Adam()\r\nloss = tf.keras.losses.BinaryCrossentropy()\r\nmodel.compile(optimizer=optimizer, loss=loss)\r\n\r\nmodel.save('out')\r\n\r\nloaded = tf.keras.models.load_model('out')\r\n```\r\n\r\n\r\n", "comments": ["This also happens when using tf.slice() instead of numpy-style slicing.", "@thomaslundgaard, This issue is fixed in Tf-niglty version. Would you like to try with latest version.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/Saduf2019/578347ae4dc350907ffd1aee58d30f63/untitled417.ipynb). Thanks!", "This is indeed fixed in tf-nightly.\r\nUnfortunately, nightly has other bugs which means I cannot use that either. I will have to wait for those issues to get fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37298\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37298\">No</a>\n"]}, {"number": 37297, "title": "build ktrain model with tensorflow 2.1.0 kera version 2.3.1", "body": "I am trying to build a model with ktrain. Howver, I received the following errors:\r\n\r\n```\r\nimport ktrain\r\nfrom ktrain import text\r\n```\r\n\r\n> WARNING:tensorflow:From c:\\users\\chth\\anaconda3\\envs\\kg\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> non-resource variables are not supported in the long term\r\n> Using DISABLE_V2_BEHAVIOR with TensorFlow\r\n> using Keras version: 2.2.4-tf\r\n\r\n`model = text.sequence_tagger('bilstm-crf', preproc)\r\n`\r\n> pretrained word2vec word embeddings will be used with bilstm-crf\r\n> Loading pretrained word vectors...this may take a few moments...\r\n> Done.\r\n> WARNING:tensorflow:From c:\\users\\chth\\anaconda3\\envs\\kg\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Call initializer instance with the dtype argument instead of passing it to the constructor\r\n> WARNING:tensorflow:From c:\\users\\chth\\anaconda3\\envs\\kg\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> If using Keras pass *_constraint arguments to layers.\r\n> WARNING:tensorflow:From c:\\users\\chth\\anaconda3\\envs\\kg\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Call initializer instance with the dtype argument instead of passing it to the constructor\r\n> WARNING:tensorflow:From c:\\users\\chth\\anaconda3\\envs\\kg\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Call initializer instance with the dtype argument instead of passing it to the constructor\r\n> WARNING:tensorflow:From c:\\users\\chth\\anaconda3\\envs\\kg\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Call initializer instance with the dtype argument instead of passing it to the constructor\r\n> WARNING:tensorflow:AutoGraph could not transform <bound method CRF.viterbi_decoding of <ktrain.text.ner.anago.layers.CRF object at 0x000002E483BAF508>> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: LIVE_VARS_IN\r\n> WARNING: AutoGraph could not transform <bound method CRF.viterbi_decoding of <ktrain.text.ner.anago.layers.CRF object at 0x000002E483BAF508>> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: LIVE_VARS_IN\r\n\r\n`learner.fit(1e-3, 1)`\r\n\r\n> WARNING:tensorflow:From c:\\users\\chth\\anaconda3\\envs\\kg\\lib\\site-packages\\ktrain\\core.py:1185: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Please use Model.fit, which supports generators.", "comments": ["@CH0102NG Can you please share colab link or simple standalone code to reproduce the issue in our environment,it helps us localizing the issue faster.Thanks!", "Hi, sorry I forgot to update. I have managed to resolve the issue by upgrading my ktrain to version 0.10.0 and shutting down another jupyter notebook that was also using cuda. Thanks so much for response:)", "@CH0102NG  Closing this issue as it was resolved.Thanks!"]}, {"number": 37296, "title": "fix document typo in tf.Module", "body": "to close issue #37293", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37296) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37296) for more info**.\n\n<!-- ok -->", "I'm not sure the new argument names are valid either. Here's is the relevant part of the `Dense` constructor: https://github.com/tensorflow/tensorflow/blob/3820a4ac5d017b5c8ea179b2c5b99d7b89004f02/tensorflow/python/keras/layers/core.py#L1116-L1132\r\n\r\nand here is the relevant part of the `Layer` (the superclass of `Dense`) constructor: https://github.com/tensorflow/tensorflow/blob/3820a4ac5d017b5c8ea179b2c5b99d7b89004f02/tensorflow/python/keras/engine/base_layer.py#L278-L293\r\n\r\nSo, `in_features` and `out_features` are neither arguments of either of the two `__init__`s nor are they members of the `allowed_kwargs`", "@JiaRu2016 Can you please check @mihaimaruseac's comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 37295, "title": "Low efficiency to do the Dequantize", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Centos\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nRefer to this code\r\nhttps://github.com/tensorflow/tensorflow/blob/c653d96483ef94ff51dbcae752e6377411e7761f/tensorflow/core/kernels/dequantize_op.cc#L131-L135\r\nEven the target output is fp32, the code still doing the cast operation in a single thread for loop.\r\nWhen the input tensor is large, the efficiency is quite lower.\r\n\r\n**Describe the expected behavior**\r\n1. The output is fp32, we needn't to do the cast again.\r\n2. The output is bfloat16, use multi-thread to do the cast.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Leslie-Fang \r\nplease share the tensorflow version for us to replicate the issue faced by you, could you fill in the template it helps us resolve the issue faster, we would require a simple stand alone code with indentation and all dependencies for us to help you resolve.", "will submit a PR to improve the performance."]}, {"number": 37294, "title": "Fix TF-TRT control edges between TRTEngineOp nodes", "body": "This PR fixes https://github.com/tensorflow/tensorrt/issues/181. \r\n\r\nThere is a bug in handling control edges between two TRTEngineOp nodes. Tagging @aaroey for review since he was the last one changing the [input edge handling](https://github.com/tensorflow/tensorflow/commit/93bf7372515057204fdadc4db367cfb7136f8e85), and this PR is about doing the same for the output edges. \r\n\r\n**Problem description**\r\nAfter the graph is segmented, an EngineInfo struct is created for each segment, which list the connections with nodes outside of the segment.\r\n\r\n1. For some reason input control edges from const nodes are omitted here: https://github.com/tensorflow/tensorflow/blob/c1915ebc2ad8115636f88b69884194da2f111fa8/tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc#L174-L180\r\n2. At the same time output control edges from const nodes are not omitted: https://github.com/tensorflow/tensorflow/blob/c1915ebc2ad8115636f88b69884194da2f111fa8/tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc#L222-L227\r\n\r\nThis leads fatal error in the following case:\r\n- there is a control edge from TRTEngineOp_0 --> TRTEngineOp_1 and\r\n- the original node int TRTEngineOp_0 was const.\r\n\r\nSo let's consider such a case:\r\n- const node A in TRTEgineOp_0\r\n- node B in TRTEngineOp_1, and \r\n- a control edge A->B in between. \r\n\r\nThe error occurs the following way: \r\nBecause of (2.), the A->B edge is listed for TRTEngineOp_0, and we try to rewire this edge for TRTEngineOp_0 [here](https://github.com/tensorflow/tensorflow/blob/c1915ebc2ad8115636f88b69884194da2f111fa8/tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc#L515). `UpdateToEngineNode` tries to look up edge A->B in the EnigineInfo->connections list for TRTEngineOp_1. This was not not added to the list (1.), and UpdateToEngineNode returns with a fatal error.\r\n\r\n**Fix**\r\nThe problem can be fixed by omitting control edges from const nodes. Alternatively one could consider keeping the control input edges from const nodes. \r\n\r\n**Note**\r\nIt seems the way the control edges are wired for the same high level (Keras) network changes between different TensorFlow versions. It can also change during Grappler transformations. The TensorFlow version that is used for NVIDIA's NGC 20.02 docker image happens to wire problematic control edges, and therefore that causes TF-TRT to fail. \r\n\r\nThe current upstream master version has changed control edge handling, and the problem does not show up if you compile that.\r\n\r\n**Code to reproduce teh issue**\r\nTo reproduce the problem, run the following code in the nvcr.io/nvidia/tensorflow:20.02-tf2-py3 docker container (or see https://github.com/tensorflow/tensorrt/issues/181). \r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\ninputs = tf.keras.Input(shape=(512, 512, 1))\r\nx = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),\r\n                           activation=None, padding='same', use_bias=False)(inputs)\r\noutput = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=(3, 3),\r\n                                         strides=(2, 2), padding='same',\r\n                                         activation=tf.nn.relu)(x)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=output)\r\n\r\nmodel.save_weights(\"results/checkpoint\")\r\ntf.keras.models.save_model(model, \"results/SavedModel\", save_format=\"tf\",\r\n                           overwrite=True, include_optimizer=False)\r\nprint(\"Model saved to SavedModel\")\r\n\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=\"results/SavedModel\")\r\nconverter.convert()\r\nconverter.save(\"results/trt_model\")\r\nprint(\"Model exported to TRT\")\r\n```\r\n", "comments": []}, {"number": 37293, "title": "TF2.x API Docs:  example codes in `tf.Module` raise error, and fix it", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/Module\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI extract the example codes, run it and got Error. It seems caused by inconsistant kwargs name:\r\n\r\n```python\r\n class Dense(tf.Module):\r\n   def __init__(self, in_features, output_features, name=None):\r\n     super(Dense, self).__init__(name=name)\r\n     self.w = tf.Variable(\r\n         tf.random.normal([input_features, output_features]), name='w')\r\n     self.b = tf.Variable(tf.zeros([output_features]), name='b')\r\n\r\n   def __call__(self, x):\r\n     y = tf.matmul(x, self.w) + self.b\r\n     return tf.nn.relu(y)\r\n\r\n\r\nclass MLP(tf.Module):\r\n  def __init__(self, input_size, sizes, name=None):\r\n    super(MLP, self).__init__(name=name)\r\n    self.layers = []\r\n    with self.name_scope:\r\n      for size in sizes:\r\n        self.layers.append(Dense(input_size=input_size, output_size=size))\r\n        input_size = size\r\n\r\n  @tf.Module.with_name_scope\r\n  def __call__(self, x):\r\n    for layer in self.layers:\r\n      x = layer(x)\r\n    return x\r\n\r\nmlp = MLP(input_size=100, sizes=[30, 30])\r\n```\r\n\r\noutput:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-137-269f8996957a> in <module>()\r\n----> 1 mlp = MLP(input_size=100, sizes=[30, 30])\r\n\r\n<ipython-input-135-066c337c5b7a> in __init__(self, input_size, sizes, name)\r\n     17    with self.name_scope:\r\n     18      for size in sizes:\r\n---> 19        self.layers.append(Dense(input_size=input_size, output_size=size))\r\n     20        input_size = size\r\n     21 \r\n\r\nTypeError: __init__() got an unexpected keyword argument 'input_size'\r\n```\r\n\r\n\r\n### Submit a pull request?\r\n\r\n I think just need to modify several lines to fix it:\r\n\r\n```python\r\n class Dense(tf.Module):\r\n   def __init__(self, input_size, output_size, name=None):\r\n     super(Dense, self).__init__(name=name)\r\n     self.w = tf.Variable(\r\n         tf.random.normal([input_size, output_size]), name='w')\r\n     self.b = tf.Variable(tf.zeros([output_size]), name='b')\r\n\r\n   def __call__(self, x):\r\n     y = tf.matmul(x, self.w) + self.b\r\n     return tf.nn.relu(y)\r\n\r\n\r\nclass MLP(tf.Module):\r\n  def __init__(self, input_size, sizes, name=None):\r\n    super(MLP, self).__init__(name=name)\r\n    self.layers = []\r\n    with self.name_scope:\r\n      for size in sizes:\r\n        self.layers.append(Dense(input_size=input_size, output_size=size))\r\n        input_size = size\r\n\r\n  @tf.Module.with_name_scope\r\n  def __call__(self, x):\r\n    for layer in self.layers:\r\n      x = layer(x)\r\n    return x\r\n```\r\n\r\nShould I submit a PR to fix this?\r\n", "comments": ["Looking at the thread under #37296 it seems that there seems to be an issue regarding the superclass constructor. Specifically its args/kwargs. Now the obvious solution would be to either add a new argument to their constructors, or to add new allowed kwarg. \r\n\r\nThat does seem like an overkill, considering that the bug is only present in toy docs example. \r\nTherefore it might be more prudent to try changing the example instead. For example:\r\n\r\n\r\n```\r\nclass Dense(tf.Module):\r\n   def __init__(self, input_dim, output_size, name=None):\r\n     super(Dense, self).__init__(name=name)\r\n     self.w = tf.Variable(\r\n       tf.random.normal([input_dim, output_size]), name='w')\r\n     self.b = tf.Variable(tf.zeros([output_size]), name='b')\r\n   def __call__(self, x):\r\n     y = tf.matmul(x, self.w) + self.b\r\n     return tf.nn.relu(y)\r\n\r\nclass MLP(tf.Module):\r\n  def __init__(self, input_size, sizes, name=None):\r\n    super(MLP, self).__init__(name=name)\r\n    self.layers = []\r\n    with self.name_scope:\r\n      for size in sizes:\r\n        self.layers.append(Dense(input_dim=input_size, output_size=size))\r\n        input_size = size\r\n\r\n  @tf.Module.with_name_scope\r\n  def __call__(self, x):\r\n    for layer in self.layers:\r\n      x = layer(x)\r\n    return x\r\n\r\nmlp = MLP(100, sizes=[30, 30])\r\n\r\n```\r\n\r\nOptionally the docs could also use a note about args in super classes and how they work with modules, but that's something developers should already know.\r\n\r\nIf the proposal looks fine I'll go forward with PR.", "You can try copy-paste the example code in the [nightly version](https://www.tensorflow.org/api_docs/python/tf/Module?version=nightly).\r\nClosing this since the associated PR has been merged and it is fixed. Thanks all!"]}, {"number": 37292, "title": "Update license year", "body": "Updated license year in LICENSE file.", "comments": ["No. License years should not be updated. They should point to the first time the file was created and not changed. If you have a file written in 2010 but license year is 2020 then any change to the file between 2010 and 2020 could be argued to not be covered by license (I am not a lawyer, that's my understanding of the law).\r\n\r\nNext, the text of the license is copied verbatim in all files as comment preamble. Why not change the year in all of these files?\r\n\r\nFurthermore, we don't accept PRs which only change one word.\r\n\r\nHence, closing the PR."]}, {"number": 37291, "title": "My model has tow inputs. when I load SavedModel, how to use Concretefunction?", "body": "my model has tow inputs. when I load SavedModel, how to use Concretefunction?\r\nmodel = tf.saved_model.load('./model_rec')\r\ninference = model.signatures['serving_default']\r\nI tried many ways, but all failed\r\ne.g\r\noutputs  = inference(tf.constant(image_batch,dtype=tf.float32),tf.constant(anchors_batch,dtype=tf.float32))\r\nTypeError: Expected at most 0 positional arguments (and the rest keywords, of ['input_1', 'input_4']),\r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['input_1'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 512, 512, 3)\r\n        name: serving_default_input_1:0\r\n    inputs['input_4'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, -1, 4)\r\n        name: serving_default_input_4:0", "comments": ["@Stealers,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "@amahendrakar ,\r\nmy tensorflow version is 2.1,\r\nmodel = tf.saved_model.load('./model_rec')\r\ninference = model.signatures['serving_default']\r\n\r\nmy model has tow inputs, for one input, I can easily use the following code to inference. \r\ncode:\r\nimg = processing_inp(\"img_test/11.jpg\")\r\ny_pred = inference(tf.constant(img,dtype=tf.float32))\r\n\r\nfor tow inputs, I don't know how to inference.\r\n\r\ny_pred = inference(tf.constant(input_1,dtype=tf.float32), tf.constant(anchors_batch,dtype=tf.float32))#failed\r\ny_pred = inference([tf.constant(input_2,dtype=tf.float32), tf.constant(anchors_batch,dtype=tf.float32)])#failed\r\n.....", "y_pred = inference(input_1=tf.constant(image_batch,dtype=tf.float32), input_4=tf.constant(anchors_batch,dtype=tf.float32))#, tf.constant(anchors_batch,dtype=tf.float32)\r\n\r\nI made it!"]}, {"number": 37290, "title": "Wrong NVIDIA driver and CUDA libs versions installed when running GPU setup instructions.", "body": "On Ubuntu 18.04, when running the instructions given [here](https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101) the driver version installed is 440 et the CUDA version is 10.2, while the recommended versions are, respectively, 430 and 10.1.", "comments": ["@julj, As per the [doc](https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101), it downloads `cuda-repo-ubuntu1804_10.1.243-1_amd64.deb` CUDA 10.1 and nvidia-driver version as `sudo apt-get install --no-install-recommends nvidia-driver-430` driver version 430. Can you try once and let us know. Thanks!", "I followed the instructions and got the wrong version installed in the end. I had to purge cuda* and nvidia and install everything manually (downloading the packages from NVIDIA).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37290\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37290\">No</a>\n"]}, {"number": 37289, "title": "Unable to remove model and release GPU memory", "body": "**System information** \r\n- Custom code\r\n- Ubuntu 16.04\r\n- TensorFlow installed from source (with pip)\r\n- TensorFlow version v2.0.0\r\n- Python version: 3.6\r\n- CUDA 10.0\r\n- TITAN XP\r\n\r\nAt first, I tried to load the pre-trained transformer-base model (A) and then add additional word embeddings (dynamic vocabulary) to it. However, it doesn't work (i.e., can't add values to `tf.Variable`)\r\nInstead, I created another model (B) with the size of dynamic vocabulary and set its weights to _existing_ transformer model (A).\r\nThen, I do not need _initial_ model (A), so I want to remove it and clear GPU memory.\r\nI tried such commands but they didn't work as well.\r\n```\r\n# delete not model (B) but only model (A)\r\ntf.keras.backend.clear_session()\r\ndel model\r\ngc.collect()\r\n```\r\nAccording to #36465, I can either use multiprocessing functions... but then I have to remove both model (A) and model (B). So I just want more clear solution.\r\nThanks.", "comments": ["I recently looked at this issue of making tensorflow free GPU memory, from what I understood I does not seem like it's gonna be available anytime soon. So for now when tf allocates some GPU memory, it will not free it until the cuda session is closed, regardless of it being used to store objects or not.\r\n\r\nYou can close the whole cuda session using numba (e.g. https://github.com/tensorflow/tensorflow/issues/19731#issuecomment-423426372) but this is very hacky: it releases all the memory of the current process but no longer allows you to use the GPU with tf. \r\n\r\nThe features you're looking for, i.e. garbage collection + ram freezing, is not available on tensorflow and I'm not sure you can do it in any other way. I don't have any \"side\" in the tf/pytorch war, I use both. If this feature is crutial to you and if it's possible to consider switching in your context, I just wanted to let you know that [this feature of accessing cuda garbage collection is available in pytorch](https://pytorch.org/docs/stable/cuda.html#torch.cuda.empty_cache) so it might be an option.", "Thanks for suggestion. (especially numba)\r\nI decided to execute it as a sub-process with another python code.\r\nI hope this problem to be resolved in TF society someday :) \r\n\r\nThanks \ud83d\udc4d", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37289\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37289\">No</a>\n"]}, {"number": 37288, "title": "TypeError: Saw an object that is an instance of a strict subclass of EagerTensor", "body": "I noticed that importing `from tensorflow_core.python.framework.ops import EagerTensor` is a problem when debugging Tensorflow in eager mode.\r\n\r\n**System information** \r\n$ pip freeze | grep tensor\r\ntensorboard==2.1.0\r\ntensorflow==2.1.0\r\ntensorflow-data-validation==0.21.2\r\ntensorflow-datasets==2.0.0\r\ntensorflow-estimator==2.1.0\r\ntensorflow-metadata==0.21.1\r\ntensorflow-serving-api==2.1.0\r\ntensorflow-transform==0.21.0\r\n\r\n**Describe the current behavior**\r\nImporting `from tensorflow_core.python.framework.ops import EagerTensor` will throw\r\n\r\n> TypeError: Saw an object that is an instance of a strict subclass of EagerTensor, which is not supported.  Item 0 is type: tensorflow.python.framework.ops.EagerTensor\r\n\r\nwhen running the code example below.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo exception when importing `from tensorflow_core.python.framework.ops import EagerTensor` or at least an exception/error message which allows one to track down the issue more easily.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```python\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n# noinspection PyUnresolvedReferences\r\nfrom tensorflow_core.python.framework.ops import EagerTensor\r\n\r\n\r\ndef main():\r\n    model = keras.Sequential()\r\n    model.add(layers.Embedding(1000, 64, input_length=10))\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nPlease let me know if you need additional information.", "comments": ["@stefan-falk \r\nI have replicated your code on mentioned tensorflow version 2.1 and do not face any issues, please refer to [this gist ](https://colab.sandbox.google.com/gist/Saduf2019/6bacad64a1e239009ed007e38377d794/37288.ipynb)for the same.", "Okay this is weird I am not seeing this error anymore either O_o.\r\n\r\nI am using are remote debugger and maybe there was/is an issue with my dependencies. I am closing this issue. Thank you @Saduf2019 for taking a look at this!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37288\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37288\">No</a>\n", "@Saduf2019 This is weird. I have this issue again. This time I cannot even reproduce it in a separate project. I must have to do something with the way I am importing Tensorflow components. I can't see what's causing this. Last time, I remember, I uninstalled Tensorflow via pip and reinstalled everything. \r\n\r\nBut this time I am certain this comes from importing something I shouldn't ..", "@Saduf2019 Okay, it took me a while but I finally was able to track the error down by commenting out line by line and made a recursive search through my entire code and found the problem where I wasn't even thinking about.\r\n\r\nI don't know if this is something that's intended.\r\n\r\nFor writing data shards, I have this helper function:\r\n\r\n```python\r\nfrom tensorflow_core.python.framework.ops import EagerTensor\r\n# ...\r\ndef write_records(shard_filepath, records):\r\n    with tf.io.TFRecordWriter(shard_filepath) as writer:\r\n        for record in records:\r\n            if isinstance(record, EagerTensor):\r\n                record = record.numpy()\r\n            writer.write(record)\r\n````\r\n\r\nApparently it's not possible to import `from tensorflow_core.python.framework.ops import EagerTensor` without breaking things. My code from above should look like this:\r\n\r\n```python\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n# noinspection PyUnresolvedReferences\r\nfrom tensorflow_core.python.framework.ops import EagerTensor\r\n\r\n\r\ndef main():\r\n    model = keras.Sequential()\r\n    model.add(layers.Embedding(1000, 64, input_length=10))\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nI don't know if this intended or unavoidable but damn it was no fun to find the problem.\r\n\r\nThere's probably a reason behind this comment @agrawalnishant: https://github.com/tensorflow/tensorflow/blob/v2.2.0-rc1/tensorflow/python/framework/ops.py#L862", "@stefan-falk  Following import method should work,\r\n```python\r\nfrom tensorflow.python.framework.ops import EagerTensor\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 37287, "title": "Merge upstream", "body": "", "comments": ["sorry, wrong repo", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37287) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 37286, "title": "ModuleNotFoundError: No module named 'tensorflow_datasets'", "body": "I try to run\r\n\r\n    import tensorflow_datasets as tfds\r\n\r\nbut got the following error\r\n\r\n    ---------------------------------------------------------------------------\r\n    ModuleNotFoundError                       Traceback (most recent call last)\r\n    <ipython-input-1-46a8a2031c9c> in <module>\r\n    ----> 1 import tensorflow_datasets as tfds\r\n    \r\n    ModuleNotFoundError: No module named 'tensorflow_datasets'\r\n\r\nI have the tensorflow_datasets installed, below is a snippets of `pip list`\r\n\r\n    tensorboard                        2.1.1              \r\n    tensorflow                         2.1.0              \r\n    tensorflow-datasets                2.1.0              \r\n    tensorflow-estimator               2.1.0              \r\n    tensorflow-hub                     0.7.0              \r\n    tensorflow-metadata                0.21.1   \r\n\r\nI am running using Ubuntu 16.04 with Anaconda virtual environment. How should I resolve this issue? ", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37286\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37286\">No</a>\n", "Not an issue as jupyterlab does not pickup the virtual environment. We need to set according to the following: https://janakiev.com/blog/jupyter-virtual-envs/"]}, {"number": 37285, "title": "modify input shape from [?,?,?,3] to [1,640,640,3]", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\n\r\nhello, everyone.\r\n\r\nI am using tensorflow.contrib.graph_editor to modify my SSD model. In fact, my model has input shape [?,?,?,3], so many nodes in the graph may not have exact shapes, such as [?,?,?,32] and [?,?,?,64]. So i want to use 'swap_inputs' to swap a new node whose shape is [1,640,640,3] with the input node whose shape is [?,?,?,3]. So how could the change affect all the nodes in the graph whose shape are not exact?", "comments": ["@hnuhchen,\r\nIf you are looking to change the input layer for your model you can check out [this](https://stackoverflow.com/a/49554465) StackOverflow comment on a similar issue.\r\n\r\nAlso, please fill in the issue template along with a minimal reproducible code in order to expedite the trouble-shooting process. \r\nThanks!", "Any updates regarding this issue? Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37283, "title": "How to register a custom op in Python API\uff1f", "body": "I just try to implement a zero_out op in tflite, and minimal project load the converted tflite model succesfully, but in python, it failed? How to register a custom op in Python API\uff1f\r\nThanks for your help!", "comments": ["@lizhen2017 \r\nplease refer to [this link](https://www.tensorflow.org/lite/guide/ops_custom) as example to modify as per your requirement.\r\nIn specific zero_out is available as custom op [here](https://www.tensorflow.org/guide/create_op#use_the_op_in_python)\r\nCould you please let us know what tensorflow version are you using.", "> @lizhen2017\r\n> please refer to [this link](https://www.tensorflow.org/lite/guide/ops_custom) as example to modify as per your requirement.\r\n> In specific zero_out is available as custom op [here](https://www.tensorflow.org/guide/create_op#use_the_op_in_python)\r\n> Could you please let us know what tensorflow version are you using.\r\n\r\n1.15", "\r\n\r\n\r\n> @lizhen2017\r\n> please refer to [this link](https://www.tensorflow.org/lite/guide/ops_custom) as example to modify as per your requirement.\r\n> In specific zero_out is available as custom op [here](https://www.tensorflow.org/guide/create_op#use_the_op_in_python)\r\n> Could you please let us know what tensorflow version are you using.\r\n\r\nI have implemented the zero_op both in tflite and tf, and I have built the project located at tensorflow/lite/example/minimal, I loaded the tfllite model with zero_out op successfully. \r\nAnd my question is how to register the zero_out op in tflite python API, because, I can't load the tflite model with zero_out op with python.", "@lizhen2017 Please take a look at this [article](https://medium.com/@bsramasubramanian/running-a-tensorflow-lite-model-in-python-with-custom-ops-9b2b46efd355) about Loading a TensorFlow-Lite model in Python with Custom Operators. ", "@lizhen2017 Please respond to the above comment so that we can help you further. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue as it has been inactive for more than 3 weeks. Please add additional comments for us to open this issue again. Thanks!", "> Closing this issue as it has been inactive for more than 3 weeks. Please add additional comments for us to open this issue again. Thanks!\r\n\r\nHello gowthamkpr! \r\nI have been following the same article you suggested and now that the compilation finished I'm getting this error when trying to use it:\r\n`ImportError: /home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite3ops6custom27Register_POSENET_DECODER_OPEv`\r\n\r\nI have opened an issue at #45944  but I found this and it would be really helpful if you can take a look at it, greetings!"]}, {"number": 37282, "title": "Android demo application is crushed when I import my .pb file.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 / Android Studio 3.5.3\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (or github SHA if from source): Tensorflow 1.14.0\r\n\r\n\r\nhttp://seangtkelley.me/blog/2017/12/23/using-custom-yolov2-model-on-android\r\n\r\nI tried this link to use YOLO v2 model about android application.\r\n\r\nSo I trained my network with YOLO v2 with darkflow, and I converted the weight file to .pb file. I imported my .pb file to android demo application that Tensorflow provides about github; tensorflow/tensorflow/examples/android/.\r\n\r\nBut when I generate apk file to that application with my .pb file, the app is crushed and don't work.\r\n\r\nIn first I think this problem because I use YOLO v2 that Tensorflow doesn't support. So I try my dataset with SSD_Mobilenetv1 that Tensorflow supports like this link;\r\n\r\nhttps://towardsdatascience.com/detecting-pikachu-on-android-using-tensorflow-object-detection-15464c7a60cd\r\n\r\nBut there was same error. The application doesn't work and be crushed. I don't know what's error. There's no problem to buile and generate apk in android studio.\r\n\r\nHow can I solve this problem?", "comments": ["ps.) My android version & android studio sdk version is 9.0 (pie)", "I solved this problem. The solution is to downgrade the version of tensorflow to 1.13.1.\r\n\r\nThe android application of tensorflow provided by this github uses tensorflow-android module, not tflite. However, tensorflow-android is not compatible with version 1.14.0 or higher since it was deprecated last.\r\n\r\nDowngrade the version of tensorflow to 1.13.1 and then install darkflow again. The demo application can read the .pb file converted to version 1.13.1 darkflow.\r\n\r\nThanks a lot."]}, {"number": 37281, "title": "docs: add examples for random_uniform, random_normal and random_binomial in tf.keras.backend", "body": "closes #31277", "comments": ["Hi @qlzh727 I updated the PR as you suggested in the comments. I've also updated the examples for `random_uniform_variable` and `random_normal_variable`, because previously I was trying to follow the convention, and they had the same problems.", "Thanks for the update.", "@Ir1d Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned I've solved the pylint issue, it should be ready now.", "CI build returns the following error message. Posting it here as a friendly reference!\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"/usr/lib/python3.6/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \"/usr/lib/python3.6/doctest.py\", line 2199, in runTest\r\n    raise self.failureException(self.format_failure(new.getvalue()))\r\nAssertionError: Failed doctest test for tensorflow.python.keras.backend.random_binomial\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/keras/backend.py\", line 316, in random_binomial\r\n\r\n----------------------------------------------------------------------\r\nFile \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/keras/backend.py\", line 340, in tensorflow.python.keras.backend.random_binomial\r\nFailed example:\r\n    random_binomial_tensor = tf.keras.backend.random_binomial(shape=(2,3),\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"/usr/lib/python3.6/doctest.py\", line 1330, in __run\r\n        compileflags, 1), test.globs)\r\n      File \"<doctest tensorflow.python.keras.backend.random_binomial[0]>\", line 1\r\n        random_binomial_tensor = tf.keras.backend.random_binomial(shape=(2,3),\r\n                                                                             ^\r\n    SyntaxError: unexpected EOF while parsing\r\n----------------------------------------------------------------------\r\nFile \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/keras/backend.py\", line 342, in tensorflow.python.keras.backend.random_binomial\r\nFailed example:\r\n    random_binomial_tensor\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"/usr/lib/python3.6/doctest.py\", line 1330, in __run\r\n        compileflags, 1), test.globs)\r\n      File \"<doctest tensorflow.python.keras.backend.random_binomial[1]>\", line 1, in <module>\r\n        random_binomial_tensor\r\n    NameError: name 'random_binomial_tensor' is not defined\r\n```"]}, {"number": 37280, "title": "How to update saved model", "body": " I would like to update my pretrained mode, which means the data of model should exist and new data to add.", "comments": ["@PonraJS-21 \r\nCan you please go through the [link ](https://www.tensorflow.org/tutorials/images/transfer_learning)and see if it helps you.\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37279, "title": "[bugfix] Fix Quantized Leaky ReLU TFLite inference Ops.", "body": "This PR addresses TFLite Leaky ReLU inference failure when the input/output type is INT8. See https://github.com/tensorflow/tensorflow/issues/33397. \r\n\r\nFirstly, BuiltIn LeakyReLU OP v2 was included in the register list.\r\n\r\nSecondly, the INT8 version of Leaky ReLU inference was implemented. Also, the initially implementation (UINT8) was only correct when the input and output scale are the same, which was fixed. A few test cases was added to test UINT8 and INT8 version.", "comments": ["Hi @renjie-liu ,\r\n\r\nThank you for the approval. I just wonder when this PR can be merged?\r\n\r\nBest,\r\n\r\nHaoyu", "@gbaned "]}, {"number": 37278, "title": "How can I share the weights between two different dilations cnn layer in tensorflow2.0", "body": "How can I share the weights between two different dilations cnn layer in tensorflow2.0\r\nIn tensorflow1.x, I can just use the tf.variable_scope with the tf.AUTO_REUSE\r\n ", "comments": ["Hi,\r\nOne of the easiest ways to share weights is to actually call the same `Layer` instance twice, _i.e._ running `cnn_layer(cnn_layer(inputs))` is strictly similar to using two similarly-specified layers with shared weights.\r\nOf course, this may be more or less easy to do depending on your code and context. If you want better advice, please share some details and example code - although to be honest this would probably be better asked on StackOverflow as this is more a use-case question than an issue _per se_ :)", "Hello, thank you for your reply!\r\n```\r\natro_cnn1 = Conv1D(dilation_rate=1,...)\r\natro_cnn2 = Conv1D(dilation_rate=3,...)\r\n\r\nx = atro_cnn1(x)\r\nx = atro_cnn2(x)\r\n```\r\n\r\n`atro_cnn1`  and `atro_cnn2`  are the same except for `dilation_rate`. In dilated convolution, `dilation_rate` does not affect the parameters of the layer. So I want `atro_cnn1`  and `atro_cnn2` to be able to use the same parameter. But in tf2, it seems that I can't get past reuse and change the `dilation_rate` of that object.", "Thank you for clarifying!\r\nThere probably could be a simpler way, but I guess one way is to implement a custom `Layer` that takes a parent one and re-uses its weights while allowing to change some of the convolution operation's parameters. Here is a home-made implementation of the sort (I did not fully test it, let me know if it works). I tried to generalize it to both Conv1D, Conv2D and Conv3d; the idea is that any of the `__init__` parameter set to None re-uses the value of the parent layer (in your case, `atro_cnn1`).\r\n```python\r\nclass SharedWeightsConv(tf.keras.layers.Layer):\r\n    \r\n    def __init__(\r\n            self,\r\n            parent,\r\n            strides=None,\r\n            padding=None,\r\n            dilation_rate=None,\r\n            activation=None,\r\n            **kwargs\r\n        ):\r\n        conv_classes = (\r\n            tf.keras.layers.Conv1D,\r\n            tf.keras.layers.Conv2D,\r\n            tf.keras.layers.Conv3D\r\n        )\r\n        if not any(isinstance(parent, cls) for cls in conv_classes):\r\n            raise TypeError(\"'parent' should be a keras convolution layer.\")\r\n        super().__init__(**kwargs)\r\n        self.parent = parent\r\n        self.rank = parent.rank\r\n        self.activation = (\r\n            parent.activation if activation is None \r\n            else tf.keras.activations.get(activation)\r\n        )\r\n        cnn_kwargs = {\r\n            'strides': strides,\r\n            'padding': padding,\r\n            'data_format': None,\r\n            'diltion_rate': dilation_rate,\r\n        }\r\n        self.cnn_kwargs = {\r\n            key: getattr(parent, key) if value is None else value\r\n            for key, value in cnn_kwargs\r\n        }\r\n        self.built = self.parent.built\r\n        self.cnn_op = {\r\n            1: tf.keras.backend.conv1d,\r\n            2: tf.keras.backend.conv2d,\r\n            3: tf.keras.backend.conv3d\r\n        }.get(self.rank)\r\n\r\n    def build(self, input_shape):\r\n        if not self.built:\r\n            self.parent.build(input_shape)\r\n        self.built = True\r\n\r\n    def call(self, inputs):  # adapted from Conv parent layer\r\n        if self.cnn_kwargs['padding'] == 'causal' and self.rank == 1:\r\n            inputs = tf.pad(inputs, self._compute_causal_padding())\r\n        outputs = self.cnn_op(inputs, self.parent.kernel, **self.cnn_kwargs)\r\n        if self.parent.use_bias:\r\n            if self.cnn_kwargs['data_format'] == 'channels_first':\r\n                if self.rank == 1:\r\n                    shape = (1, self.parent.filters, 1)\r\n                    outputs += tf.reshape(self.parent.bias, shape)\r\n                else:\r\n                    outputs = tf.nn.bias_add(\r\n                        outputs, self.parent.bias, data_format='NCHW'\r\n                    )\r\n            else:\r\n                outputs = tf.nn.bias_add(\r\n                    outputs, self.parent.bias, data_format='NHWC'\r\n                )\r\n        if self.activation is not None:\r\n            return self.activation(outputs)\r\n        return outputs\r\n\r\n    def _compute_causal_padding(self):  # adapted from Conv parent layer\r\n        left_pad = self.cnn_kwargs['dilation_rate'][0]\r\n        left_pad *= (self.parent.kernel_size[0] - 1)\r\n        if self.cnn_kwargs['data_format'] == 'channels_last':\r\n          causal_padding = [[0, 0], [left_pad, 0], [0, 0]]\r\n        else:\r\n          causal_padding = [[0, 0], [0, 0], [left_pad, 0]]\r\n        return causal_padding\r\n```\r\n\r\nFor your example:\r\n```python\r\natro_cnn1 = Conv1D(dilation_rate=1, ...)\r\natro_cnn2 = SharedWeightsConv(atro_cnn1, dilation_rate=1)\r\n\r\nx = atro_cnn1(x)\r\nx = atro_cnn2(x)\r\n```\r\n\r\nI hope this helps, let me know if bugs arise!", "Thank you very much for your answer. I think I've got your point, and it should be a workable approach, although it seems a bit cumbersome. \r\nI will try to really test the feasibility of this method in my later experiments, in my scenario. \r\nFor some more complex layers (customizations), such a shared approach does not seem to be easy to implement. \r\nAs you might expect, I also think there should be some more intuitive approach here (or will be implemented). \r\n", "I totally agree; I had to write some similarly cumbersome custom layers for some personal projects, and I really hope a better solution could be exposed or implemented by a tensorflower soon...", "A simpler way to share weights using a custom layer would be to use `tf.nn.conv2d`:\r\n\r\n    class SharedDilatedConv(tf.keras.layers.Layer):\r\n        def __init__(\r\n            self,\r\n            filters,\r\n            kernel_size,\r\n            strides=None,\r\n            padding=None,\r\n            dilation_rates=None,\r\n            activation=None,\r\n            use_bias=True,\r\n            **kwargs\r\n        ):\r\n            self.filters = filters\r\n            self.kernel_size = kernel_size\r\n            self.strides = strides\r\n            self.padding = padding\r\n            self.dilation_rates = dilation_rates\r\n            self.activation = activation\r\n            self.use_bias = use_bias\r\n            super().__init__(*args, **kwargs)\r\n\r\n       def build(self, input_shape):\r\n           self.conv = Conv2D(\r\n               self.filters,\r\n               self.kernel_size,\r\n               padding=self.padding,\r\n               dilation_rate=self.dilation_rates[0]\r\n           )\r\n           self.act1 = Activation(self.activation)\r\n           self.act2 = Activation(self.activation)\r\n\r\n       def call(self, inputs, **kwargs):\r\n           x1 = self.conv(inputs)\r\n           x1 = self.act1(x1)\r\n           x2 = tf.nn.conv2d(\r\n                    inputs,\r\n                    self.conv.weights[0], \r\n                    padding=self.padding,\r\n                    strides=self.strides,\r\n                    dilations=self.dilation_rates[1]\r\n                  )\r\n            if self.use_bias:\r\n                x2 = x2 + self.conv.weights[1]\r\n            x2 = self.act2(x2)\r\n            return x1, x2\r\n\r\nOf course, you can also assign a variable and bias directly using `self.add_weights` inside `build`,\r\nand only use `tf.nn.conv2d` in `call`", "@HuaYZhao,\r\nAny updates regarding this, is this still an issue? Thanks!", "@amahendrakar As established per the previous discussion thread, both @Susmit-A and I were able to provide with _ad hoc_ custom layers that can be used in this specific context, however the largest question, _i.e._ \"how to share weights (in a simple manner) between layers\", is still open.", "@amahendrakar Okay, there's no more updates to this question right now! Thank you", "What do you mean by sharing weights?\r\nDo you intend to:\r\n1) initialize weights of cnn_2 from weights of cnn_1, or\r\n2) let both cnn_1 and cnn_2 use the same set of weights during both forward and backward path? How do you want to update the weights from gradients?", "@tanzhenyu I believe the aim is option 2: letting both layers use the same set of weights (otherwise the framework already allows that, _e.g._ using `get_weights` / `set_weights`), as is done in some modern architectures (_e.g._ ALBERT with transformer layers). I suppose weights update would rely on averaging the gradients computed at each layer's level? From my experience, this is already workable when using similarly-specified layers, simply by iteratively calling a single layer instance, but the question here is to do so with layers that have slightly different specifications (but same number of weights).", "> @tanzhenyu I believe the aim is option 2: letting both layers use the same set of weights (otherwise the framework already allows that, _e.g._ using `get_weights` / `set_weights`), as is done in some modern architectures (_e.g._ ALBERT with transformer layers). I suppose weights update would rely on averaging the gradients computed at each layer's level? From my experience, this is already workable when using similarly-specified layers, simply by iteratively calling a single layer instance, but the question here is to do so with layers that have slightly different specifications (but same number of weights).\r\n\r\nI see. In that case, what we would recommend is the same way you would share any other python objects, and yes 1) either re-calling a single layer instance, or 2) create a customized layer that does both things, for example [SubwordEncoder](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder) would use the table for both encode and decode.", "Closing it. Let us know if you have further questions!", "@Susmit-A How would you deal with `get_config` in your example with respect to the `filters` argument?"]}]