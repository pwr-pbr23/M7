[{"number": 6145, "title": "[CMake] Add missing Python packages to build scripts", "body": "Fixes Windows PIP tests after #6137.", "comments": ["@tensorflow-jenkins test this please.", "The only failures are in non-CMake code, so this should be good to fix the Windows build now."]}, {"number": 6144, "title": "hitting 2GB limit when importing large frozen graphs onto multiple gpus", "body": "When I tried to import a big graph onto multiple gpus (multiple times), the graph def usually goes beyond 2GB. Is there a solution to this in tf?", "comments": ["6 months ago there was no plan on the horizon to get rid of the 2GB limit (it would require deep changes to protobuf library) .... but perhaps you can make your graph smaller? IE, you can use distributed tensorflow locally where each worker holds only part of graph relevant to it. Alternatively, you can use `function.Defun` to factor out large repeating chunks of your computation.", "Trying to import a pre-trained vgg model, little of the graph that i can control ...", "I just tried importing vgg example from [Toronto site](https://www.cs.toronto.edu/~frossard/post/vgg16/) and GraphDef is 85k, so most of your graphdef is coming from somewhere else. You may get better community support in refactoring your graph creation on stack overflow.\r\n\r\n2GB may seem like an arbitrary limit, but it's actually quite large and hitting it usually means user error, like node creation \"leak\". For instance VGG example is 85k graphdef and it has 266 nodes. So it's an average of 320 bytes per node. So to exceed 2GB with similar kind of network, you'd need 6.2 million nodes. There's a 5usec delay on schedule a GPU op, so you will be spending 31 seconds just on op schedule overhead to run these 6 million operations\r\n\r\nAnother way to hit the limit is having large graph-inlined constants (`tf.constant`). Storing large data inlined in Graph is inefficient because of locks, so you would be better off moving them into variables anyway.", "If the graphdef only contains the nodes, 85k makes more sense to me actually ... I was referring to something like this: https://github.com/pavelgonchar/vgg-face-tensorflow/blob/master/vggface16.tfmodel, which is way larger. I think this might be a frozen model in which all variables are converted into constants, and so it falls into the \"graph-inlined constants\" problem that you mentioned?", "Ah, yes, it seems that [tf_run.py](https://github.com/pavelgonchar/face-transfer-tensorflow/blob/master/tf_run.py) in that example is loading 500MB graphdef, so weights are inlined, maybe using script like [freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/5a5a25ea3ebef623e07fb9a46419a9df377a37a5/tensorflow/python/tools/freeze_graph.py). In such case I'm not aware of any solution -- I think the issue should be called \"hitting 2GB limit when dealing with large frozen graphs\"", "@petewarden can you comment on whether there's a best practice around this for freeze_graph?", "It's a bit of a hail mary, but you could try using the memory-mapping approach to get around the 2GB limit, as described here:\r\nhttps://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/\r\n\r\nI've no idea if that will actually work in this case though.", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 6143, "title": "Feature request: second derivatives for pooling operations", "body": "Currently, neither `tf.nn.avg_pool` or `tf.nn.max_pool` support automatically taking second derivatives, making it difficult to experiment with second-order methods on neural networks that use any sort of pooling (which unfortunately includes most modern networks). Trying to take a second derivative gives a `LookupError`, e.g., `LookupError: No gradient defined for operation 'gradients_4...AvgPoolGrad' (op type: AvgPoolGrad)`.  \r\n\r\nDo you have any plans for implementing second derivatives for pooling operations, or any suggested workarounds like rewriting the pooling operations in terms of elementary functions that have second derivatives defined? Thank you!\r\n", "comments": ["@vrv I see your name on discussions around this but can't tell who if anyone is working on it: would you reassign?", "Well, nobody's job is to write ops for every feature request we get, we prioritize our work based on importance.  I'll mark this as contributions welcome in case somebody in the public wants to implement this, but perhaps somebody internally will find this important and do the work.", "I will work on this! :)", "@AnishShah good luck!", "As per https://github.com/tensorflow/tensorflow/pull/6299 I'm not totally sure of the status of this, @aam-at have you picked this up in https://github.com/aam-at/tensorflow/tree/max_pool_grad_grad?", "Currently, I am finishing tests for new kernels. You can check up max_pool_grad_grad branch in my fork. As soon as I finish, I will make pull request.", "Please, checkout #6664 ", "@kohpangwei the functionality has been merged to master in #6664, if this covers everything can this issue be closed?", "@ahundt It looks great. Thank you @aam-at!"]}, {"number": 6142, "title": "NewCheckpointReader does not work for the V2 checkpoint format", "body": "Unable to read the new checkpoints (V2 format) that became default from r0.12 release using `NewCheckpointReader`. This breaks the TF slim API, specifically:\r\nhttps://github.com/tensorflow/tensorflow/blob/c2d14aa0d80897b7164e32e16951d03a36342370/tensorflow/contrib/framework/python/ops/variables.py#L487\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/3AalcBJ-6z8\r\nI started that thread and was suggested to post an issue here\r\n\r\n### Environment info\r\nOperating System: CentOS 6.6\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): CUDA 8, cuDNN 5\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 0.12.0-rc0\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nw1 = tf.Variable(tf.truncated_normal(shape=[10]), name='w1')\r\nsaver = tf.train.Saver()\r\nsess = tf.Session()\r\nsess.run(tf.initialize_all_variables())\r\nsaver.save(sess, 'my-model')\r\n\r\nreader = tf.train.NewCheckpointReader('my-model.index')  ## error\r\nprint reader.get_variable_to_shape_map()\r\n```\r\n\r\nThe error is:\r\n```txt\r\nTraceback (most recent call last):\r\n  File \"save_and_restore.py\", line 9, in <module>\r\n    reader = tf.train.NewCheckpointReader('my-model.index')\r\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 110, in NewCheckpointReader\r\n    return CheckpointReader(compat.as_bytes(filepattern), status)\r\n  File \"/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for my-model.index\r\n```\r\n", "comments": ["I also notice that the recommanded script [inspect_checkpoint.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/inspect_checkpoint.py) is not working anymore since the r0.12 release of Tensorflow.", "@sherrym would you reassign as necessary?", "You can use `tf.train.NewCheckpointReader('./my-model')` instead.\r\nWith V2 saver format, the reader is not expecting a file name, but only the prefix of the filename. (Frankly I don't like this feature very much..because filename auto-completion doesn't help anymore\r\nThe `./` is necessary because of a bug #4921.", "As @ppwwyyxx said, you just need to use \"my-model\" without \".index\". Please let me know if that fixes the problem for you. Thanks.", "Yes, that seems to fix the problem. Thanks @ppwwyyxx !", "Dot-slash workaround does not work for checkpoints that re not in the current directory. Neither `/path/to/./my-model`  nor `./path/to/my-model`.\r\nIs there a workaround for that one? ;)\r\n\r\nP.S. tested with the latest as of now commit fe70bd9c22357741e69751bdf189726c440db02e at r0.12 branch\r\n ", "@ivan-aksamentov I thought you don't need this workaround for checkpoints that are not in `./`.\r\nAre you suggesting that simply using `/path/to/my-model` doesn't work? What is the error?", "@ppwwyyxx Indeed. Sorry, I screwed up the path. False alarm. Full path seems to be working now."]}, {"number": 6141, "title": "Feature request: directional distributions (like von Mises-Fisher)", "body": "Are there plans to implement directional statistical distributions? In particular, I'm looking to get a von Mises-Fisher distribution into `contrib/distributions/python`.\r\n\r\nI'm beginning work on a VMF distribution myself, but as a new TF user my progress is likely to be slow.\r\n\r\n(Note that `scipy.stats` implements a basic von Mises distribution.)\r\n\r\nReferences:\r\n[Wikipedia: Von Mises\u2013Fisher distribution](https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution)\r\n[Arxiv: Directional Statistics in Machine Learning: a Brief Review]( https://arxiv.org/pdf/1605.00316.pdf\r\n)", "comments": ["@rw I've just recently needed to implement this distribution and have some code that is based on the contrib.distributions interface (now in the process of being moved into the core). I haven't implemented the sampling algorithm yet and I only handle the p=3 and p=4 cases, but if you haven't already solved this problem I'm willing to share my code. Once I've got things in good shape I was planning on contributing it to TF.", "@xodus7 Good to hear! I didn't get around to implementing this.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@xodus7 I am interested in this feature as well. If your code is still available, can you please contact me using alexander.pashevich@inria.fr?", "@alexpashevich I sent you and email with the code. I hope it's useful!", "We are tracking this in TF probability. Would love a contribution to get the ball rolling, even if you don't have a sampler.", "What would be helpful would be an implementation of the modified bessel function with a custom gradient function. This repo https://github.com/nicola-decao/s-vae/blob/master/hyperspherical_vae/ops/ive.py wraps scipy methods which isn't the right approach for a contribution here, but is in many ways better than my approach approximating using power series expansions for small and large concentration parameters. If someone has some guidance for the best method to get an implementation of the bessel function then I'd be willing to make a go at a generalized implementation that supports dimensions P > 4.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 6140, "title": "test PR", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6139, "title": "[CMake] Fix support for custom kernels in `tf.contrib.metrics`.", "body": "Fixes #6115. Cherrypick into release.", "comments": ["@tensorflow-jenkins test this please."]}, {"number": 6138, "title": "[Windows/CMake] Run ctest in parallel", "body": "Using 32 parallel jobs, as we're using that degree of parallelism for the build too.", "comments": ["21 minutes to run the Windows tests!!"]}, {"number": 6137, "title": "Branch 141241224", "body": "Pushing internal commits.", "comments": ["@tensorflow-jenkins Test this, please.", "@mrry could you look at the below cmake issue?\r\n\r\n```\r\nError evaluating generator expression:\r\n\r\n    $<TARGET_OBJECTS:tf_contrib_metrics_set_ops>\r\n\r\n  Objects of target \"tf_contrib_metrics_set_ops\" referenced but no such\r\n  target exists.\r\nCall Stack (most recent call first):\r\n  tf_python.cmake:497 (GENERATE_PYTHON_OP_LIB)\r\n  CMakeLists.txt:214 (include)\r\n```", "For posterity, the following changes were squashed into 6de612e7767c4114d65cb06fc6606436e3081dc3 due to a temporary export problem:\r\n\r\n141201358\tForce fix documentation causing open source blocks.\r\n141198275\tAvoid making horrible docstrings when KeywordRequired() is used\r\n141196439\tTensorFlow: Update generated OSS Python Op docs.\r\n141195963\tMoved cuda_libdevice_path.cc to make the functionality available accross all platforms\r\n141194861\tChange head_test size from small to medium.\r\n141189800\tTensorFlow: Update generated OSS Python Op docs.\r\n141188581\tPart 2c of renaming SparseTensor.shape -> SparseTensor.dense_shape\r\n141188238\t[XLA:CPU] Mac portability fixes.\r\n141187979\tPart 2d of renaming SparseTensor.shape -> SparseTensor.dense_shape\r\n141187865\tadd name arguments to tf.split and standardize names between tf.split and\r\n141187490\tPart 2d of renaming SparseTensor.shape -> SparseTensor.dense_shape\r\n141186655\tFixed name collision in saver test.\r\n141186135\tPart 2b of renaming SparseTensor.shape -> SparseTensor.dense_shape\r\n141185169\tTensorFlow: Update generated OSS Python Op docs.\r\n141180811\tTensorFlow: Update generated OSS Python Op docs.\r\n141176394\tTensorFlow: Update generated OSS Python Op docs.\r\n141174390\tRemove resources from the public API for now.\r\n141172667\tTensorFlow: Update generated OSS Python Op docs.\r\n141170685\tPart 2c of renaming SparseTensor.shape -> SparseTensor.dense_shape\r\n141169934\tTensorFlow: Update generated OSS Python Op docs.\r\n141167099\tTensorFlow: Update generated OSS Python Op docs.\r\n141163858\tTensorFlow: Update generated OSS Python Op docs.\r\n141161010\tTensorFlow: Update generated OSS Python Op docs.\r\n141158743\tTensorFlow: Update generated OSS Python Op docs.\r\n141155653\tTensorFlow: Update generated OSS Python Op docs.\r\n141152716\tTensorFlow: Update generated OSS Python Op docs.\r\n141152197\tClarify wording in RNN tutorial.\r\n141149578\tTensorFlow: Update generated OSS Python Op docs.\r\n141147227\tTensorFlow: Update generated OSS Python Op docs.\r\n141145195\tTensorFlow: Update generated OSS Python Op docs.\r\n141142798\tTensorFlow: Update generated OSS Python Op docs.\r\n141140181\tTensorFlow: Update generated OSS Python Op docs.\r\n141139340\tPart 2c of renaming SparseTensor.shape -> SparseTensor.dense_shape\r\n141137926\tTensorFlow: Update generated OSS Python Op docs.\r\n141135381\tTensorFlow: Update generated OSS Python Op docs.\r\n141132823\tTensorFlow: Update generated OSS Python Op docs.\r\n141129991\tTensorFlow: Update generated OSS Python Op docs.\r\n141127103\tTensorFlow: Update generated OSS Python Op docs.", "Merging to unblock pulls, test failures will need to be dealt with in followups."]}, {"number": 6136, "title": "ImportError: No module named 'tensorflow' on windows + anaconda", "body": "I followed the instructions on downloading and setting up tensorflow on windows. they result in one red line on the pip installation and the no-module-found error message in python interactive.\r\n\r\nsteps:\r\n- install anaconda for windows 64bit for python 3.5 as per given link in the tensorflow install page\r\n- issue a pip for tensorflow (either one of these would result in the samething):\r\nC:\\> pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\nor\r\nC:\\> pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\n\r\nthe pip installation ends with a line printed in red:\r\n\"Cannot remove entries from nonexistent file c:\\users\\jesaremi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\easy-install.pth\"\r\n\r\n- open up python and type \"import tensorflow as tf\":\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named 'tensorflow'", "comments": ["I also checked the c++ runtime as mentioned in the doc page. I have the exact same thing already installed", "here's the entire pip install log:\r\npip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\nCollecting tensorflow==0.12.0rc0 from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\n  Downloading https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl (12.2MB)\r\n    100% |################################| 12.2MB 112kB/s\r\nCollecting protobuf==3.1.0 (from tensorflow==0.12.0rc0)\r\n  Downloading protobuf-3.1.0-py2.py3-none-any.whl (339kB)\r\n    100% |################################| 348kB 1.6MB/s\r\nCollecting numpy>=1.11.0 (from tensorflow==0.12.0rc0)\r\n  Downloading numpy-1.11.2-cp35-none-win_amd64.whl (7.6MB)\r\n    100% |################################| 7.6MB 183kB/s\r\nRequirement already up-to-date: six>=1.10.0 in c:\\users\\jesaremi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow==0.12.0rc0)\r\nRequirement already up-to-date: wheel>=0.26 in c:\\users\\jesaremi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow==0.12.0rc0)\r\nCollecting setuptools (from protobuf==3.1.0->tensorflow==0.12.0rc0)\r\n  Downloading setuptools-30.2.0-py2.py3-none-any.whl (472kB)\r\n    100% |################################| 481kB 2.1MB/s\r\nInstalling collected packages: setuptools, protobuf, numpy, tensorflow\r\n  Found existing installation: setuptools 27.2.0\r\nCannot remove entries from nonexistent file c:\\users\\jesaremi\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\easy-install.pth", "This seems an Anaconda issue. Could you please:\r\n1. Upgrade setuptools:\r\n`pip install --upgrade -I setuptools` \r\n\r\n2. Install TensorFlow again with `--ignore-installed` flag:\r\n`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl `\r\n\r\nand see if it solves the problem?\r\n\r\n**EDIT**: At the time of this answer TensorFlow last release was still 0.12 so I updated the wheel version to 1.0.", "@mrry should I submit a PR to get_started docs with a note about this anaconda issue when pip is trying to remove `easy-install.pth` and it does not exist in a new Anaconda installation as conda packages can't include it?", "Adriano, the setuptools upgrade fixed everything. thanks very much", "@Carmezim That would be a great addition to the \"Common Problems\" section, thanks!", "@mrry Cool. Will be doing it ASAP.", "@Carmezim - the setuptools upgrade fixed my issue too - awesome!", "This did not fix it for me. I'm still having the same problem. ", "@tristanbrown could you please elaborate giving more specifics of what you did and used? Thanks! \r\nedit: You can reply on #6548 ", "I had the same issue. Thanks very much. Sound lad!", "the setuptools upgrade fixed everything. Thanks", "Setuptools upgrade fixed thanks a lot.", "Upgrading the Setuptools fixed everything, Thank you @Carmezim ", "Great - upgrading the set-up tools worked !", "Will this solve the issue on windows too\r\n", "Yes I did the same in Windows. Worked fine\n\nOn Mar 9, 2017 10:40 AM, \"Indrajeet Gour\" <notifications@github.com> wrote:\n\n> Will this solve the issue on windows too\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6136#issuecomment-285256185>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHevRjv_ZDtUhuJR3ZGWcNKrx12Jgot5ks5rj4nhgaJpZM4LGD27>\n> .\n>\n", "@igour Do you mean upgrading `setuptools`? If it is, the answer was to address a problem with Anaconda distribution in this case on Windows, so at least on it the solution is supposed to work.", "Hi, I have laid down step-by-step instructions to successfully install Tensorflow on Windows.\r\n\r\nhttps://github.com/bhavsarpratik/install_Tensorflow_GPU_windows", "Hi Guys!\r\nhere is the way how i fixed the problem of installing tensorflow on Windows. I will start \ud83d\udc4dfrom the begining \ud83d\udc4d \r\n1. I downloaded the Anaconda 4.3.1 For Windows with Python 3.6 version. \r\n2. Create a conda environment named tensorflow by invoking the following command:\r\nC:> conda create -n tensorflow \r\n\r\n3. Activate the conda environment by issuing the following command:\r\nC:> activate tensorflow\r\n (tensorflow)C:>  # Your prompt should change \r\n\r\n3. Issue the appropriate command to install TensorFlow inside your conda environment. To install the CPU-only version of TensorFlow, enter the following command:\r\n\r\n(tensorflow)C:> pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl \r\n\r\nMessage appear : can not install this wheel ......... [ i forgot the message, it's just couldn't find the point]\r\nTo install the GPU version of TensorFlow, enter the following command (on a single line):\r\n(tensorflow)C:> pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl  \r\n\r\nsame message appear couldn't install \r\n\r\nso after that i wanted to write my first code \ud83d\udc4d \r\nc>python\r\npython version 3.6 .... (anaconda).............\r\n>>> import tensorflow as tf\r\n[it appears this Error : ]\r\n\r\nTraceback <most recent call last>:\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n-----------------------------------------------------------------------------\r\nFix The Problem \r\nhttps://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl\r\n\r\ntensorflow-1.0.1 -cp35-cp35m-win_amd64.whl\r\ntensorflow version 1.0.1 \r\n-cp 35 : python version needed\r\nwin_amd64 : windows x64 \r\n\r\nso the Anaconda 4.3.1 For Windows with Python 3.6 version so we need python 3.5 i downloaded it from other this web site cause anaconda has only the version 4.3.1 &  for python 2.7 : \r\nso this anaconda for python 3.5 : http://www.gurobi.com/downloads/get-anaconda \r\nthen i followed the other steps & everything worked fine :), i hope everything was worked with you \r\n", "@EMCL  read my comment above, install anaconda that may help you", "Hello , i am using tensorflow in windows. I tried every thing still i am getting no module name tensorflow\r\nI tried following ways\r\nC:> conda create -n tensorflow python=3.5 \r\nC:> activate tensorflow\r\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.2.0-cp35-cp35m-win_amd64.whl \r\n\r\nStill i am getting two errors\r\n1 -tensorflow-1.2.0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.\r\n2- No module name tensorflow\r\n", "@vap31 Are you using the 64-bit version of Python?", "Yea, i do use 64 - bit of python", "@vap31 as the most recent version of anaconda is incorporated with python 3.6. you may change the commands to \r\nconda create -n tensorflow python=3.6\r\n activate tensorflow\r\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.2.0-cp36-cp36m-win_amd64.whl \r\n", "![ok](https://user-images.githubusercontent.com/24687663/28496262-d592a20c-6f2c-11e7-87fb-d32e4eccb2b6.png)\r\n\r\nThanks!\r\n\r\nFor Windows 7 x64, **> conda update numpy --no-pin**\r\n\u00a1resolved!", "The best way is to install anaconda and then install tensorflow with pip instead of creating a conda environment, activating tensorflow.  Thats the only thing that worked for me as of now.  ", "@joellerena @xgwei \r\n\r\nI recently downloaded Anaconda 4.4.0 with python 3.6 version for 64bit windows 10 OS and the suggested code did not work even with python=3.6 but I did the following steps to get it installed.\r\n\r\n1) After Installing New Anaconda 4.4.0 with python 3.6 version, Open Anaconda Prompt and type in the following :\r\n\r\nC:\\Users\\Mahesh>conda create -n tensorflow python=3.5\r\nC:\\Users\\Mahesh>activate tensorflow\r\n(tensorflow) C:\\Users\\Mahesh>conda install -c conda-forge tensorflow\r\n\r\nCome back to Anaconda Navigator which got installed with Anaconda 4.4.0. Here, you have an option of \"Application on\" on your home page displaying 2 answers: \r\n1) root\r\n2)tensorflow\r\n\r\nSelect tensorflow and you can see all the applications which are under it. if they are not installed then install them once again by clicking on install button and launch an application to run the code(Spyder/Jupyter notebook)\r\n\r\nThen Validate with the following code:\r\n\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n\r\nChange the application to root/tensorflow depending on your workflow. ", "Its working perfect.\r\nBut should we install again all packages like cv2, matplotlib etc.\r\nBecause spyder(tensorflow) shows the below error.\r\n\r\nrunfile('C:/Anaconda3/Scripts/programs/maskplot.py', wdir='C:/Anaconda3/Scripts/programs')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-fa20f58c72ab>\", line 1, in <module>\r\n    runfile('C:/Anaconda3/Scripts/programs/maskplot.py', wdir='C:/Anaconda3/Scripts/programs')\r\n\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 688, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 101, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Anaconda3/Scripts/programs/maskplot.py\", line 1, in <module>\r\n    import cv2\r\n\r\nModuleNotFoundError: No module named 'cv2'", "you can try \"pip install opencv-python\" @pksnkl  ", "ImportError: No module named 'tensorflowvisu' ", "Thank you, Carmezim, you solution worked perfectly well. It saved my two days work.", "i had the same issue.  and how i fixed it in my issue : is set path of Environment go to Anaconda.\r\neg. ('C:\\ProgramData\\Anaconda3') in path value ", "isee", "up", "Hello, I this problem, too.  and I installed the [tensorflow] package through the anaconda with the default python is python 3.5.2, and [activate tnsorflow] through the cmd with administrator privilege, and there is no any problem, and the [Enveriments] of anaconda is still [root].", "Brilliant, this is how I fixed mine with setupTools.\r\n\r\nUpgrade setuptools:\r\npip install --upgrade -I setuptools\r\n\r\nInstall tensorflow\r\npip install tensorflow --ignore-installed --upgrade", "Hi Carmezim\r\nI installed tensorflow via anaconda prompt. was getting error 'No module named tensorflow'.\r\nTrying to upgrade pip but it's not happening.\r\n![image](https://user-images.githubusercontent.com/25078915/40268097-22aaa4fa-5b85-11e8-810a-1e9ed0abe27a.png)\r\n", "if not already installed, download and install Anaconda. Then open the command prompt in Anaconda, now run your commands, it will work.", "Installing with pip works for me: \r\n\r\npython -m pip install --upgrade tensorflow\r\n\r\nhttps://stackoverflow.com/questions/42559222/installation-of-tensorflow-on-windows-7-pip3-is-not-recognized-as-an-interna", "If you are using Anaconda Navigator,  selection Applications on as \"tensorflow\" before launching the visual studio code\r\n\r\nif you are using cmd prompt, activate tensorflow first. ", "![image](https://user-images.githubusercontent.com/43405947/45753810-c1919200-bbce-11e8-89f6-ee70dfc15062.png)why this is happening frequently . plz help me in installing tensorflow\r\n\r\n", "thanks Carmezim, you fix my trouble perfectly!", "thx", "Thanks man, I was looking for so much for that, I solved it.", "still getting error module not found python 3.6\r\n", "Using Anaconda prompt, was upgrading my Tensorflow in tf env using the command below. \r\n(tf) C:\\Users\\User>pip install tensorflow --ignore-installed --upgrade --use-feature=2020-resolver\r\n\r\nHit error below.\r\nInstalling collected packages: urllib3, pyasn1, idna, chardet, certifi, zipp, six, setuptools, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, astunparse, tensorflow\r\n**ERROR: yarl 1.6.2 requires typing-extensions>=3.7.4; python_version < \"3.8\", which is not installed.\r\naiohttp 3.6.3 requires yarl<1.6.0,>=1.0, but you'll have yarl 1.6.2 which is incompatible.**\r\nSuccessfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 certifi-2020.11.8 chardet-3.0.4 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.33.2 h5py-2.10.0 idna-2.10 importlib-metadata-2.0.0 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.2 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.0 requests-oauthlib-1.3.0 rsa-4.6 setuptools-50.3.2 six-1.15.0 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 urllib3-1.26.2 werkzeug-1.0.1 wheel-0.35.1 wrapt-1.12.1 zipp-3.4.0\r\n\r\n(tf) C:\\Users\\User>\r\nPls help to advise how to fix this error. Thanks for your help."]}, {"number": 6135, "title": "fix relaxed-constexpr warning", "body": "This patch should remove the warning for nvcc 7.5, 8.0 and still work on nvcc 7.0:\r\n```\r\nINFO: From Compiling *.cu.cc:\r\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\r\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\r\n```\r\n\r\nThis warning has been out for a lot of time see:\r\n\r\n#5799 \r\n#5833 \r\n#5256\r\n#3980\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "seems like the failures are not related to the patch... ? ", "Jenkins, test this please.\r\n"]}, {"number": 6134, "title": "make `tensorflow.python.tools` importable (freeze_graph, and others)", "body": "for mysterious reasons, these functions aren't importable\r\n\r\ncan we add an `__init__.py` to this directory so that we can import them? thoughts?", "comments": ["Most directories don't have `__init__.py` because they are pulled up to be importable as `tf.<name>`. IE, you can see the list here  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/__init__.py\r\n\r\nBut also, things in python/tools directory are meant to be run as stand-alone scripts, not clear that it makes sense to have something like `tf.freeze_graph`", "fair enough re the first point\r\n\r\nre the second, i actually don't require this function anymore, so i don't really mind. closing."]}, {"number": 6133, "title": "Branch 141209775", "body": "Pushing internal commits.", "comments": ["since the other push is complete, can we close this one?"]}, {"number": 6132, "title": "contrib.learn.Estimator does not work with multiple GPU", "body": "Attempting to assign ops to a GPU within model_fn passed to an Estimator produces the following error:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'save/ShardedFilename_2': Could not satisfy explicit device specification '/device:GPU:1' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nIdentity: CPU \r\nShardedFilename: CPU \r\n\t [[Node: save/ShardedFilename_2 = ShardedFilename[_device=\"/device:GPU:1\"](save/Const, save/ShardedFilename_2/shard, save/num_shards)]]\r\n```\r\n\r\nThis can be reproduced by running the example in examples/learn/multiple_gpu.py", "comments": ["A quick bump on this - am I making fundamentally incorrect assumptions on how this should work?\r\n\r\nI'm not eager to replicate the Estimator's functionality, which is great overall, but it's important for me to get my models working in TF w/ multiple GPUs. I'd be happy to help w/ a PR if some work is needed to get this functioning.\r\n\r\nI had also posted on SO [here](http://stackoverflow.com/questions/41003989/tensorflow-contrib-learn-estimator-multi-gpu).\r\n\r\nThank you - wonderful library.", "Thanks for filing this issue. We know about the problem and are in the process of fixing it. Specifically, and as a stopgap, we will allow you do (optionally) create a Saver yourself, so you can control where its ops land. We'll update this thread once that has landed (@ispirmustafa FYI)", "Thank you, @martinwicke. I'll keep an eye out.\r\n\r\nIn the meanwhile, were I to dig into the source and add allow_soft_placement=True on the session creation, is that likely to work-around the problem or is there another blocking issue (recognizing the can of worms I'm opening w/ that option setting)?", "I believe if you did that, you should be fine. ", "Hi,\r\n\r\nI get the same errors when using contrib.learn package (Regressors) where only the GPU memory is allocated but the GPU processing is at zero. I tried allow_soft_placement=True but still get the same errors.\r\n\r\nIs this fixed yet?", "We're making \"allow_soft_placement\" default in Estimator implementation.\r\nThat should fix this issue.", "@vvpreetham it's surprising that allow_soft_placement would not have fixed this. How did you set it?", "Thanks for the quick response Wicke.\r\n\r\nI have done the following: tensorflow (v0.12)\r\n\r\nIn\r\nthe tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/run_config.py\r\n\r\nLine 241: I have changed the code to:\r\n    self.tf_config = ConfigProto(allow_soft_placement=True,....\r\n\r\nThen in my files: \r\nI am using a LinearRegressor as follows:\r\n       config_ = run_config.RunConfig()\r\n       m = tf.contrib.learn.LinearRegressor(model_dir=model_dir,\r\n                                          config=config_,\r\n                                          feature_columns=wide_columns,\r\n                                          optimizer=tf.train.FtrlOptimizer(\r\n            learning_rate=0.2,\r\n            l1_regularization_strength=1.0,\r\n            l2_regularization_strength=1.0))\r\n\r\nWhich has a feed function:\r\ndef feed_fn(df):\r\n    with tf.device('/gpu:2'):\r\n        categorical_cols = {k: tf.SparseTensor(\r\n         indices=[[i, 0] for i in range(df[k].size)],\r\n         values=df[k].values,\r\n         shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}\r\n        label = tf.constant(df[REGRESSOR_LABEL_COLUMN].values)\r\n    return categorical_cols, label\r\n\r\nI get the error:\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to\r\nnode 'SparseTensor_55/values': Could not satisfy explicit device\r\nspecification '/device:GPU:2' because no supported kernel for GPU devices\r\nis available.\r\n         [[Node: SparseTensor_55/values = Const[dtype=DT_STRING,\r\nvalue=Tensor<type: string shape: [149999] values:\r\n6b76ea8a9a6046649c021c2c26f9e2e7 bf9319d59ac94dd396de82601d3ccba0\r\n4028cbe0384137e1013846732b4e009d...>, _device=\"/device:GPU:2\"]()]]\r\n\r\nShould I use the scope for the variable (Given that SparseTensor is a\r\nvariable and not a constant?)", "Also, I am using the estimator as follows:\r\n\r\n        estimator.fit(input_fn=lambda: feed_fn(batch_df_train), steps=train_steps)", "Also as an update, the reason I am trying out the specific device allocation is that, when I normally use the program without tf.device then, I am seeing a strange behavior where only the Titan-X GPU memory is allocated but the GPU processor is 0% used. (I have 3 Titan-X GPU on the same box) ", "@vvpreetham  I think what you're seeing is expected behavior, and unrelated to Estimator. Tensorflow will automatically claim all available memory for all of the GPUs that it sees, unless you tell it otherwise. It will do this whether you are using them for your model or not. Take a look at CUDA_VISIBLE_DEVICES environment setting to specify GPUs to use, or you can change the memory settings (see using GPUs in the docs)", "@ashern I am guessing your comment is specific to my last comment (memory allocation). How do I ensure that tensorflow is actually using all the GPU processors I have? \r\nDoes CUDA_VISIBLE_DEVICE also ensure the GPU processor usage?", "@vvpreetham That's a pretty big topic. Take a look here: https://www.tensorflow.org/how_tos/using_gpu/ - and I think you'll find more help if you post your questions to StackOverflow for a wider audience to answer.", "@ashern Thanks. I shall post on StackOverflow, meanwhile setting CUDA_VISIBLE_DEVICES still does not enable my processor usage.\r\n\r\n(I have tried everything on the link you have specified and still no luck. Hence posted here)", "@martinwicke bump on my original question (so that this thread is not lost) :)", "The problem seems to be the SparseTensor. Note that the following piece of code works:\r\n\r\n    categorical_cols = {k: tf.SparseTensor(\r\n            indices=[[i, 0] for i in range(df[k].size)],\r\n            values=df[k].values,\r\n            shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}\r\n    for d in ['/gpu:0','/gpu:1','/gpu:2']:\r\n        with tf.device(d):\r\n            # Converts the label column into a constant Tensor.\r\n            label = tf.constant(df[REGRESSOR_LABEL_COLUMN].values)\r\n            # Returns the feature columns and the label.\r\n\r\n\r\nBUT if I do the following (that-is, assign the SparseTensor to the devices, it fails)\r\n\r\n    for d in ['/gpu:0','/gpu:1','/gpu:2']:\r\n        with tf.device(d):\r\n            # Creates a dictionary mapping from each categorical feature column name (k)\r\n            # to the values of that column stored in a tf.SparseTensor.\r\n            categorical_cols = {k: tf.SparseTensor(\r\n                    indices=[[i, 0] for i in range(df[k].size)],\r\n                    values=df[k].values,\r\n                    shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}\r\n            # Converts the label column into a constant Tensor.\r\n            label = tf.constant(df[REGRESSOR_LABEL_COLUMN].values)\r\n            # Returns the feature columns and the label.\r\n", "If you're still getting the same error message (cannot put string tensor on GPU), then you still haven't enabled soft placement (or soft placement does not consider colocation constraints? @vrv do you know?). You cannot put string tensors on GPUs, so that particular tensor has to live on the CPU. ", "I am still getting the error. I am super certain that soft placement is enabled as I have dry run the code with log.info and also breakpoints. I also add the gpu_fraction and soft placement directly in the session (The GPU memory fraction works, and soft placement works as I have stated for constants). My modified code for session is as follows:\r\n\r\n```\r\ndef get_session(gpu_fraction=DEFAULT_GPU_FRACTION):\r\n    num_threads = os.environ.get('OMP_NUM_THREADS')\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\r\n    if num_threads:\r\n        return tf.Session(config=tf.ConfigProto(\r\n                    allow_soft_placement=True,\r\n                    log_device_placement=True,\r\n                    gpu_options=gpu_options, \r\n                    intra_op_parallelism_threads=num_threads))\r\n    else:\r\n        return tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\r\n                                                log_device_placement=True,\r\n                                                gpu_options=gpu_options))\r\n```\r\n\r\nI setup the session as follows:\r\n\r\n```\r\ndef main(_):\r\n    with get_session() as sess:\r\n        m, df_test = wide_batch_train()\r\n```\r\n\r\nThe problem seems to be the tf.SparseTensor", "I see in the repo that Estimator has been promoted to core from contrib for 1.1 - great news.\r\n\r\nBriefly checking in on this issue - will the deployed implementation handle multiple GPU device assignment & soft placement? \r\n\r\nMany thanks for the hard work!\r\n", "\u200bYes, the core estimator uses soft placement, and multi-GPU training should\nwork.\n", "Excellent. Time to put my homespun solution to rest...\r\n\r\nMany thanks, Martin. Appreciated your overview at the summit!", "@martinwicke: any examples of multi-gpu for tf.learn now that it will be on core besides the one provided in the example section? The one from example section only does parallel-model as opposed to data-parallel https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/multiple_gpu.py\r\n\r\nThanks.", "I don't think creating sparse string Tensors on the GPU will work since the GPU does not support strings.\r\n\r\nIt's best to structure your model so the string to int conversion happens on the CPU, and the GPU just processes the dense part of your model.\r\n\r\nThat said, the linear regression canned estimator will probably see no benefit from running on the GPU (not enough matrix multiplications to offset the data transfer cost)."]}, {"number": 6131, "title": "tf.get_session_tensor awkwardness", "body": "`tf.get_session_tensor()` op constructor is unique in that it needs to be passed output of a session.run call. IE, when you are setting up your graph structure and don't have a relevant handle, you need to do something like this\r\n\r\n```\r\ndummy_handle = sess.run(tf.get_session_handle(tf.constant(1, dtype=dt)))\r\ntensor_holder, tensor = tf.get_session_tensor(dummy_handle, dtype=dt)\r\n\r\n```\r\nThis means that it's incompatible with workflow of \"create graph -> finalize graph -> run ops\" (like when using Supervisor which finalizes graph at the end of `init`). It seems the reason was to figure out which device the tensor would live on. Would it make sense to have it take device from the current graph context instead of a live tensor handle?\r\n\r\n@yuanbyu @keveman ", "comments": ["BTW, if this makes it into TF 1.0 it'll be forever the lone exception to \"define-then-run\" convention of TensorFlow. (ie, session ops need result of `session.run` to be constructed)", "Ultimate sadness.", "@yaroslavvb Is this possible to fix in a backwards compatible way by making it take a session directly?", "Unassigning Yuan since he's fled.", "@girving it could be fixed in backward compatible way by adding back single argument version of `tf.get_session_tensor`, which was there before it was removed in 0.10   . I'll close this issue because there's a work-around by using dummy_handle, and fixing it now that it made into 1.0 API is probably too much work for what it's worth"]}, {"number": 6130, "title": "Adding extra options to Kubernetes object generator for distributed example.", "body": "This enables multiple training clusters to be deployed in the same namespace.", "comments": ["Can one of the admins verify this patch?", "No opinion about whether to merge, but I think these scripts should be used purely for testing and not a demonstration of how to run distributed TF :)", "@jhseu Agreed. These scripts should be treated as they are, i.e., tests (they live in the folder called dist_test), not examples, even though some users may find bits and pieces of them useful.", "Closing for now as these changes aren't needed for testing. @caisq feel free to reopen if you disagree.", "@caisq @jhseu The hardcoding of the objects into the generator script is definitely a shortcoming, and the data is baked into the containers, but aside from that what are the other shortcomings of this test?\r\n\r\nI ask because I'm totally new to tensorflow, and was trying to prototype a kubernetes third party resource to help developers quickly create training clusters: https://github.com/elsonrodriguez/tensorsets\r\n\r\nWhat's a better script/test to base this type of automation on?"]}, {"number": 6129, "title": "Branch 141211730", "body": "Pushing internal commits.", "comments": []}, {"number": 6128, "title": "Branch 141125443", "body": "Pushing internal commits.", "comments": ["@tensorflow-jenkins Test this, please."]}, {"number": 6127, "title": "Clarify about 'SAME' padding in docs", "body": "Fix #5203. Also use `//` instead of `/` to be consistent with Python2/3.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 6126, "title": "oops...please ignore this -- I don't know what I'm doing", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6125, "title": "[Windows] Compute 3.0 ", "body": "I'm trying to run tensorflow 0.12.0rc0 on Windows 10 with a Compute 3.0 Device (gtx 660 Ti) but I get the following message:\r\n```\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:948] Ignoring visible gpu device (device: 0, name: GeForce GTX 660 Ti, pci bus id: 0000:01:00.0) with Cuda c\r\nompute capability 3.0. The minimum required Cuda capability is 3.5.\r\n```\r\nThe installation manual seems to suggest that it should work with 3.0. (And it works just fine on Linux)\r\n```\r\nTensorFlow GPU support requires having a GPU card with NVidia Compute Capability (>= 3.0). Supported cards include but are not limited to:\r\n\r\nNVidia Titan\r\nNVidia Titan X\r\nNVidia K20\r\nNVidia K40\r\n```", "comments": ["We didn't build the binary package for the 0.12rc0 release with support for Compute Capability 3.0 devices.\r\n\r\nSince we merged #6092 and #6099, the next release candidate and [nightly builds](http://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=gpu,OS=windows/) for Windows will have this support."]}, {"number": 6124, "title": "Using Boolean Placeholder for is_training Causes Problems when Loading on iOS", "body": "Currently I am training a model that uses Batch Normalization and Dropout. Because of this I build the model to take a placeholder, `is_training`, that I can feed_dict true or false depending on whether the graph is being used for training or validation.\r\n\r\nI take the trained model and export using `tensorflow.contrib.session_bundle.exporter` however when I try to load this graph on iOS, I run into problems (see [SO](http://stackoverflow.com/questions/40855271/no-opkernel-was-registered-to-support-op-switch-with-these-attrs-on-ios)):\r\n```\r\nNo OpKernel was registered to support Op 'Switch' with these attrs.\r\n```\r\nI have tried removing the placeholder and injecting the actual value of `False` for `is_training` (see [SO](http://stackoverflow.com/questions/40852729/permanently-inject-constant-into-tensorflow-graph-for-inference) and [GH](https://github.com/tensorflow/tensorflow/issues/5919)), however the model still does not load (see [SO](http://stackoverflow.com/questions/40852729/permanently-inject-constant-into-tensorflow-graph-for-inference) post):\r\n```\r\nInput 0 of node dropout6/cond/dropout/random_uniform/max was passed bool from dropout6/cond/Switch:1 incompatible with expected INVALID.\r\n```\r\n\r\nIt seems like something is missing to clean up / recompile the graph, stripping out the training part of the graph because the [inject](https://github.com/tensorflow/tensorflow/issues/5919) does not seem like enough. This was one idea to solve the problem: https://github.com/tensorflow/tensorflow/issues/5919.\r\n\r\nRight now, I can instead of using placeholder for `is_training` use constants and create the model twice, sharing weights, and then export just the validation subgraph and then load this into iOS, but the downside is that I need to instantiate the model twice into the graph.\r\n\r\nEDIT: Would be nice to not have to resort to compiling iOS with switch: https://github.com/tensorflow/tensorflow/issues/2680#issuecomment-237483554 since that node shouldn't even be needed in the final graph. It would be nice to fully remove it from the saved graph.", "comments": ["I struggled a lot with this until I gave up and ended up freezing the model without is_training.", "Sorry we weren't able to give you a better solution for this one! We do generally recommend having separate graphs for training and inference, though that isn't well documented. Closing this bug since it looks like you have a workaround? Let me know if we should reopen it.", "Hey guys! I've been having the same problem trying to make this work on android and I have used some of the solutions from above but none seem to work...here are some of the errors I get back from the various methods of trying to solve this problem...Any Help would be appreciated!\r\n\r\nThe models work when I run it on python but doesn't when I run them on android.\r\n\r\nIm using the SSD_Mobilenet model from tensorflow object detection api.\r\n\r\nThis is from the optimize for inference function:\r\nNodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: Preprocessor/map/while/add/y = Constdtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 1>\r\n\r\nThis is from renaming Switch to Identify:\r\nNot a valid TensorFlow Graph serialization: Node 'Preprocessor/map/while/add/y': Connecting to invalid output 1 of source node Preprocessor/map/while/Switch which has 1 outputs\r\n\r\nThis last one loads the model but crashes when processing the inputs:\r\njava.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs. Registered devices: [CPU], Registered kernels:\r\ndevice='GPU'; T in [DT_STRING]\r\ndevice='GPU'; T in [DT_BOOL]\r\ndevice='GPU'; T in [DT_INT32]\r\ndevice='GPU'; T in [DT_FLOAT]\r\ndevice='CPU'; T in [DT_FLOAT]\r\ndevice='CPU'; T in [DT_INT32]\r\n\r\n                                                              \t [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/cond/Switch = Switch[T=DT_BOOL](Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/Greater, Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/Greater)]]\r\nAny ideas guys? I've also tried to recompile tensorflow with the changes from [https://stackoverflow.com/questions/40855271/no-opkernel-was-registered-to-support-op-switch-with-these-attrs-on-ios/43627334#43627334](url) but bazel crashes while compiling :(", "@ronny3050 could you share how to freeze the model without is_training ? \r\nthanks.", "[This gist](https://gist.github.com/ronny3050/d3dc669601a5604b347365ed652645dd) can be used for freezing models. Do not include `is_training` in the list of whitelisted nodes. \r\nYou may have to change a couple of hardcorded node-names to fit your needs.\r\n\r\nHope it helps. :) \r\n", "@ronny3050, thanks for your reply.\r\nIt seems \"phase_train\" node is removed according to below log, it only has one input parameter.\r\n\" E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[input], outputs:[embeddings]\"\r\nBut it still need feed a value for \"phase_train\" when call Session.run.\r\nIs there any other code change for this issue ? \r\n\"java.lang.IllegalArgumentException: You must feed a value for placeholder tensor 'phase_train' with dtype bool  [[Node: phase_train = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\"\r\n\r\nthanks.", "Just FYI, we can simply feed phase_train and keep_prob etc. along with input in Android and it fixes the issue.\r\n\r\n``tensorflow.feed(\"phase_train\", new boolean[]{false});  \r\n``\r\n``tensorflow.feed(\"keep_prob\", new float[]{1f});\r\n``", "@ronny3050 If you know can you please tell me, how to make phase_train option false if I have .pb(frozen model).", "I wrote an example below to convert a tensorflow(v1.3) model with batchnorm to tflite. Hope it's helpful.\r\nhttps://gist.github.com/iamyb/779a14c2da79c7d7fe5d7256e4d71ace"]}, {"number": 6123, "title": "Disable contrib/integrate/python/ops/odes_test due to timeout", "body": "The test hangs during PIP tests on GPU (but not CPU).", "comments": ["Do we have a corresponding bug opened about this?", "@gunan bug opened internally for @shoyer ", "DO NOT MERGE this for now. I'm seeing some indication that this test doesn't hang when the tests are run serially, instead of concurrently.", "nvm. the test times out even when run exclusively."]}, {"number": 6122, "title": "Avoid const char-related warning internally", "body": "", "comments": ["https://github.com/tensorflow/tensorflow/commit/c84d0b1c2abef256aba64a99b9c2fec03b3989a5 caused warnings that were treated as errors internally, hence blocking pull. This CL addresses that. Corresponding change has been tested internally. @mrry @gunan ", "@tensorflow-jenkins test this please", "@mrry Changed to COMPILE_MSVC."]}, {"number": 6121, "title": "[CMake] Fix support for custom kernels in `tf.contrib.metrics`.", "body": "Fixes #6115.", "comments": ["Looks like a flake on the Linux GPU tests.\r\n\r\n@yifeif I'm not sure if the window has closed for r0.12 cherry picks, but it'd be nice to get this bug fix in as well....", "E tensorflow/stream_executor/cuda/cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\nW tensorflow/stream_executor/stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n\r\nThis one is all new. We can cherrypick, but I would feel safer rerunning the tests once more.\r\nJenkins, test this please.", "Jenkins, test this please.", "Oh for crying out loud, I wanted gpu tests to rerun and they failed to download dependencies.", "Jenkins, test this please.", "different GPU tests flaked this time, going forward with the merge."]}, {"number": 6120, "title": "Error needed when loading a GraphDef created in a newer version of TensorFlow", "body": "There are a lot of issues (e.g. #5628) that are caused by users loading GraphDefs that were created by a newer version of TensorFlow (e.g. 0.12 that introduces SaverV2) into older code bases. This currently results in cryptic errors late in the loading process, but instead we should raise an error (or at least a prominent warning) when a newer GraphDef version than is supported is loaded.", "comments": ["I'm surprised load_graph_def doesn't check the version information, which is already present in the GraphDef proto. It seems like a simple check to add. I'm assigning to @girving since he was the one who added the version field to the proto.", "@petewarden I'm not sure I understand.  An old version of TensorFlow can't know which future GraphDef versions it will be compatible with, since those new versions are created in the future.  What kind of error message would you like?", "It would be \"This GraphDef was created with a newer version of TensorFlow, and can't be loaded\". Since we don't support this, we should fail hard, clearly, and early, rather than ignoring the mismatch and hitting cryptic errors later in the process.", "@petewarden That's a good idea, but I don't see how one would check for that.  We do check if a GraphDef's `min_consumer` version is too high, but this usually isn't the case.  We absolutely want TensorFlow to be able to load a GraphDef with a future version if it doesn't use any of the features introduced by that future version.  Since we have no idea what those future features are, the only kinds of errors we can produce are about unimplemented features.", "> We absolutely want TensorFlow to be able to load a GraphDef with a future version if it doesn't use any of the features introduced by that future version.\r\n\r\nI'm surprised by this. I'm either used to the case where we guarantee forwards compatibility (so GraphDefs created by future versions will load and run correctly in older versions) or we don't. What we seem to have is no guarantee of forwards compatibility, but also no error that we're in that \"no guarantee this will load\" situation. Anyway, I'll come and talk in person to understand more.", "@petewarden Is it still worth having a chat about this?  I forget whether we already did.", "@girving thanks, let's see if we can have a quick chat today!", "Proposal: we add a `string tf_version` to [`VersionDef`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/versions.proto#L22), and use it to hint when op kernels are missing.  It could be either just `TF_VERSION_STRING` or the concatenation of that and `tf_git_version`.\r\n\r\n@josh11b Thoughts or objections?", "I chatted with @girving offline, and we came to the conclusion that it would be useful to have an advisory producer source code version (for example 1.2) stored in the graphdef. This would only be used for creating better error messages in the case that there's an unrecognized op found, or one of the other similar errors.", "Do we still plan to this, @petewarden @girving?", "@yifeif I'm unlikely to get to this before I vanish.", "I see. I'm closing this for now. @petewarden feel free to re-open if we plan to precede on this.", "According to [this](https://www.tensorflow.org/programmers_guide/version_compat) TF document, graphs produced on later version of TF can be consumed on earlier version of TF if min_consumer_version and min_producer_version params set correctly. I found how to set min_consumer_version and producer_version using GraphDef.versions field, but could not figure out how to set min_producer_version or consumer_version ([Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/versions.proto) says these fields should be defined somewhere else, but where?). And without that I get \"Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary\" error", "Please reopen this issue. There are new ops in newer versions of tensorflow that make it impossible to load graphs generated in newer versions using older ones. For example the `NonMaxSuppressionV3` op got added some time between 1.8 and 1.10. At the very least it should be possible to check before getting a cryptic error message."]}, {"number": 6119, "title": "Different prediction result for tf.learn QuickStart?", "body": "Hello,\r\n\r\nToday I upgrade tensorflow package and read the [tutorials from the beginning](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/tflearn/index.md).And find the result of classifier prediction is different, it is `[1, 1]`. I remember that the old version tf run out `[1, 2]`. So what's the problem?\r\n\r\nThe OS is OSX EI Capitan, and tensoflow is 0.12.0-rc0\r\n\r\n``` \r\n$ `python -c \"import tensorflow; print(tensorflow.__version__)\"`\r\n-bash: 0.12.0-rc0: command not found\r\n```\r\n\r\nIf I turn on the INFO log level, the log lists below:\r\n```\r\n$ python iris_classifier.py\r\nWARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:315 in fit.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:315 in fit.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:315 in fit.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:*******************************************************\r\nWARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\r\nWARNING:tensorflow:Consider switching to the more efficient V2 format:\r\nWARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\r\nWARNING:tensorflow:now on by default.\r\nWARNING:tensorflow:*******************************************************\r\nWARNING:tensorflow:*******************************************************\r\nWARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\r\nWARNING:tensorflow:Consider switching to the more efficient V2 format:\r\nWARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\r\nWARNING:tensorflow:now on by default.\r\nWARNING:tensorflow:*******************************************************\r\nWARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nAccuracy: 0.966667\r\nWARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:348 in predict.: calling predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:348 in predict.: calling predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:348 in predict.: calling predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nPredictions: [1, 1]\r\n```\r\n", "comments": ["same issue with Linux server ( CentOS Linux release 7.2.1511 (Core) ) \r\ntensorflow version 0.12.0\r\n\r\nI got the correct predictions [1,2] of my first run, but my following runs were all [1,1]...\r\n", "same problem .. still no solution ?", "Make sure you delete any pre-existing models. Assuming this is the iris tutorial take a look at \r\n\r\n``classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\r\n                                            hidden_units=[10, 20, 10],\r\n                                            n_classes=3,\r\n                                            model_dir=\"/tmp/iris_model\")\r\n``\r\n\r\nIf you don't delete the files in ``model_dir=\"/tmp/iris_model\"`` you will be modifying the old model rather than creating a new one.", "Update to tensorflow 0.12.1. It seems fixed.", "Also, does anyone have a way to get rid of the error messages? The API is deprecated and tf.contrib.learn keeps changing so it's a bit hard to keep track of API updates.", "I made a fix version of this here. Using the custom pipe lines.\r\nhttps://github.com/XRayCheng/tensorflow_iris_fix", "Looks like @jacktang said this was fixed in 0.12.1, and @XRayCheng has a fix.\r\n\r\nI'm closing this out.  Feel free to file issues for new problems you may run in to!", "With v1.1.0, this issue still occurs. The first output is `[1,2]` and all subsequent runs are `[1,1]` until I destroy the `tmp` model.\r\n\r\n@XRayCheng's \"fix\" is not actually a fix -- README still forewarns that all non-fresh runs will produce `[1,1]`.\r\n\r\nSome kind of clarification would be nice. \ud83d\ude1e Picking up TF for the first time, and it's pretty rough when the main documentation examples aren't replicated locally.", "@lukeed You've already found the solution. After you run the model once, it will be trained and stored in the tmp folder. If you do not delete the tmp folder you are retraining an existing model rather than doing one from scratch. It makes sense that it would have different results", "@agriasgg Thanks! But if that's truly the case, then that should be included as a note, imo. I was under the assumption that the model was _only_ looking at the same CSV data every time, which means it should have arrived to `[1,2]` every time. \r\n\r\nNowhere in the example documentation did it say that every run was going to affect the subsequent outputs."]}, {"number": 6118, "title": "Export Model For Serving But Tensor Type Dismatch", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI am trying to export the model for serving , but it's report type error about *inputs* tensor.\r\nbut in the export and predict part , the **inputs** are the same type.\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nhere is a sample code for my exporting\r\n```\r\n            named_graph_signature = {\r\n                'inputs': exporter.generic_signature({\r\n                              'sparse_index': tf.placeholder(tf.int64, name=\"feature_index\")\r\n                              'sparse_ids': tf.placeholder(tf.int64,name = \"feature_ids\"),\r\n                              'sparse_values':tf.placeholder(tf.int64, name =\"feature_values\"),\r\n                              'sparse_shape':tf.placeholder(tf.int64, name=\"feature_shape\")\r\n                }),\r\n                'outputs': exporter.generic_signature({\r\n                    'prob': inference_softmax\r\n                })}\r\n            model_exporter.init(\r\n                sess.graph.as_graph_def(),\r\n                #default_graph_signature=named_graph_signature,\r\n                named_graph_signatures=named_graph_signature,\r\n                init_op=init_op)\r\n            model_exporter.export(export_path, tf.constant(export_version), sess)\r\n            print('Done exporting!')\r\n```\r\nhere is my code for predicting\r\n```\r\n  ins = \"0 142635:1 250810:1 335229:1 375278:1 392970:1 506983:1 554566:1 631968:1 647823:1 658803:1 733446:1 856305:1 868202:1\"\r\n  FEATURE_SIZE = 1000000\r\n  tokens = ins.split(\" \")\r\n  feature_num = 0\r\n  feature_ids = []\r\n  feature_values = []\r\n  feature_index = []\r\n\r\n  for feature in tokens[1:]:\r\n      feature_id, feature_value = feature.split(\":\")\r\n      feature_ids.append(int(feature_id))\r\n      feature_values.append(float(feature_value))\r\n      feature_index.append([1, feature_num])\r\n      feature_num += 1\r\n\r\n  feature_shape = [1, FEATURE_SIZE]\r\n\r\n  sparse_index = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_index), dtype=tf.int64)\r\n  sparse_ids = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_ids), dtype=tf.int64)\r\n  sparse_values = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_values), dtype=tf.float32)\r\n  sparse_shape= tf.contrib.util.make_tensor_proto(numpy.asarray(feature_shape), dtype=tf.int64)\r\n\r\n  channel = implementations.insecure_channel(host, port)\r\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n  request = predict_pb2.PredictRequest()\r\n  request.model_spec.name = model_name\r\n  request.model_spec.version.value = model_version\r\n  print model_name,model_version\r\n\r\n  request.inputs['sparse_index'].CopyFrom(sparse_index)\r\n  request.inputs['sparse_ids'].CopyFrom(sparse_ids)\r\n  request.inputs['sparse_values'].CopyFrom(sparse_values)\r\n  request.inputs['sparse_shape'].CopyFrom(sparse_shape)\r\n  # Send request\r\n\r\n  result = stub.Predict(request, request_timeout)\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 63, in <module>\r\n    main()\r\n  File \"run.py\", line 59, in main\r\n    result = stub.Predict(request, request_timeout)\r\n  File \"/home/serving/.local/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 305, in __call__\r\n    self._request_serializer, self._response_deserializer)\r\n  File \"/home/serving/.local/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 203, in _blocking_unary_unary\r\n    raise _abortion_error(rpc_error_call)\r\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input size does not match signature\")\r\n```", "comments": ["This question may be better asked on (stackoverflow)[http://stackoverflow.com/questions/tagged/tensorflow] since it seems to be a usage issue not a bug report."]}, {"number": 6117, "title": "distributed tensorflow failed to save variable larger than 2G", "body": "  when use distributed tensorflow saver to save larger variables(more than 2G),  it get stuck and won't save successfully.\r\n   tensorflow version: rc0.11\r\n   bazel version: 0.3.\r\n  minimal reproducible example:\r\n\r\nfile1: test_saver.py\r\n------------------------------------------\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport time\r\nfrom datetime import datetime\r\n\r\nimport numpy\r\nimport tensorflow as tf\r\n\r\nimport sys\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\ntf.app.flags.DEFINE_string('job_name', '', 'One of \"ps\", \"worker\"')\r\n\r\ndef run_training(target, cluster_spec):\r\n  with tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d/%s\" % (0, 'cpu:0'),\r\n        cluster=cluster_spec)):\r\n    with tf.name_scope('test'):\r\n        test_weight1 = tf.get_variable(\"test_weight1\", [24500000, 22],  # bigger then 2G\r\n                    initializer=tf.random_normal_initializer())\r\n        test_weight2 = tf.get_variable(\"test_weight2\", [24500, 22], # smaller then 2G\r\n                    initializer=tf.random_normal_initializer())\r\n\r\n    saver1 = tf.train.Saver([test_weight1 ], write_version=tf.train.SaverDef.V2)\r\n#    saver2 = tf.train.Saver([test_weight2], write_version=tf.train.SaverDef.V2)\r\n\r\n    # The op for initializing the variables.\r\n    init_op = tf.group(tf.initialize_all_variables(),\r\n                       tf.initialize_local_variables())\r\n    sv = tf.train.Supervisor(is_chief=True,\r\n                             logdir=\"log\",\r\n                             init_op=init_op,\r\n                             summary_op=None,\r\n                             saver=saver1,\r\n#                             saver=saver2,\r\n                             )\r\n    # Get a session.\r\n    sess = sv.prepare_or_wait_for_session(target)\r\n\r\n    # Start the queue runners.\r\n    queue_runners = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)\r\n    sv.start_queue_runners(sess, queue_runners)\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n    step = 1\r\n    print('start to train.')\r\n    sys.stdout.flush()\r\n    while not coord.should_stop():\r\n      w1 = sess.run([test_weight2])\r\n      print('start to save checkpoint, time=%s'%datetime.now())\r\n      sys.stdout.flush()\r\n      checkpoint_path = 'model.ckpt'\r\n      saver1.save(sess, checkpoint_path, global_step=1)    # saver1 get stuck with high cpu, please refer to line 29 and 39\r\n      #saver2.save(sess, checkpoint_path, global_step=1)   # saver2 can save smoothly, please refer to line 30 and 40\r\n      print('after save checkpoint, time=%s'%datetime.now())\r\n      sys.stdout.flush()\r\n      step += 1\r\n    sess.close()\r\n\r\ndef main(_):\r\n  assert FLAGS.job_name in ['ps', 'worker'], 'job_name must be ps or worker'\r\n\r\n  ps_hosts = ['127.0.0.1:3721',]\r\n  worker_hosts = ['127.0.0.1:3722',]\r\n  cluster_spec = tf.train.ClusterSpec({'ps': ps_hosts, 'worker': worker_hosts})\r\n  server = tf.train.Server(\r\n      {'ps': ps_hosts,\r\n       'worker': worker_hosts},\r\n      job_name=FLAGS.job_name,\r\n      task_index=0)   # test for one ps and one worker\r\n\r\n  if FLAGS.job_name == 'ps':\r\n    # `ps` jobs wait for incoming connections from the workers.\r\n    server.join()\r\n  else:\r\n    run_training(server.target, cluster_spec)\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n-------------------------------------------------------------------------------------\r\nfile2: run_test_saver.sh\r\n-------------------------------------------------------------------------------------\r\n```\r\n#!/bin/bash\r\n\r\nCUDA_VISIBLE_DEVICES='' nohup python test_saver.py --job_name=ps > ps.out&\r\n\r\nCUDA_VISIBLE_DEVICES='' nohup python test_saver.py --job_name=worker > worker.out &\r\n```\r\n-------------------------------------------------------------------------------------\r\n\r\n  \r\nPS:  both saver1 and saver2 can work correctly when use standalone tensorflow(i.e  without parameter server)", "comments": ["Individual tensors larger than 2GB are not supported by protobuf, which the distributed runtime uses as the format for tensors exchanged over the network. Generally speaking, if you have to transfer >2GB over the network in a single step, you are probably not going to get very good performance. There are two main workarounds for dealing with this problem when checkpointing:\r\n\r\n1. Create the [`tf.train.Saver`](https://www.tensorflow.org/versions/r0.12/api_docs/python/state_ops.html#Saver.__init__) with the `sharded=True` optional argument. This causes each parameter server to write its data directly to the filesystem, avoiding the protobuf limit.\r\n2. Use a [partitioner](https://www.tensorflow.org/versions/r0.12/api_docs/python/state_ops.html#variable-partitioners-for-sharding) when creating the variable (i.e. with `tf.get_variable()`) so that the individual variables are smaller. You can still use these partitioned variables directly with `tf.nn.embedding_lookup()`, and the lookup will be distributed across the parameter servers.", "thanks very much."]}, {"number": 6116, "title": "grpc RecvTensor is slow", "body": "I made benchmark tests for distributed setup with loopback network, profiling it and found there is excessive memory copying in the client side of RecvTensor call, which is actually one of the bottleneck.\r\n\r\nHere is the code, which mainly stolen from @yaroslavvb [here](https://gist.github.com/yaroslavvb/1124bb02a9fd4abce3d86caf2f950cb2),\r\n```python\r\n  with tf.device(device1):                                                      \r\n    params = tf.get_variable(\"params\", shape=[params_size], dtype=dtype,        \r\n                             initializer=tf.zeros_initializer)                  \r\n  with tf.device(device2):                                                      \r\n    # constant node gets placed on device1 because of simple_placer             \r\n    #    update = tf.constant(1, shape=[params_size], dtype=dtype)              \r\n    update = tf.get_variable(\"update\", shape=[params_size], dtype=dtype,        \r\n                             initializer=tf.ones_initializer())                 \r\n    add_op = params.assign(update)\r\n```\r\n\r\nHere is the profiling result (google perftools) with tensor size 100MB (one fact is, the throughput will degrade with the increasing of tensor size):\r\n* [device1 worker profiling report](https://github.com/tensorflow/tensorflow/files/633183/device1-prof.pdf)\r\n* [device2 worker profiling report](https://github.com/tensorflow/tensorflow/files/633184/device2-prof.pdf)\r\n\r\nFrom the result, the sending side (device2) look fine, but the receiving side (device1, the grpc client) consumes too many CPU cycles for the data transfer.\r\n\r\nBy the way, I made rough stats for this [memmove](https://github.com/grpc/grpc/blob/d7ff4ff40071d2b486a052183e3e9f9382afb745/src/core/lib/support/slice_buffer.c#L278) call. For one round of 100MB tensor assignment, there are roughly 2GB data moved (actually, including the copy inside memmove, it should be 4GB copied with a naive memmove), which is 20+ times RAM bandwidth amplification (the result is an average of 100 round run, which may not precise but the scale should be ok).", "comments": ["To make things more clear, I collected more detailed data for [memmove](https://github.com/grpc/grpc/blob/d7ff4ff40071d2b486a052183e3e9f9382afb745/src/core/lib/support/slice_buffer.c#L278):\r\n```c\r\n  // int move_size = (sb->count - 1) * sizeof(gpr_slice);\r\n  memmove(&sb->slices[0], &sb->slices[1], (sb->count - 1) * sizeof(gpr_slice));\r\n  // int data_size = GPR_SLICE_LENGTH(slice);\r\n```\r\nA typical `move_size`, `slice_size` sequence,\r\n* move_size: 6096, slice_size: 608\r\n* move_size: 6072, slice_size: 8192\r\n* move_size: 6048, slice_size: 7583\r\n* move_size: 6024, slice_size: 600\r\n* move_size: 6000, slice_size: 8192\r\n* move_size: 5976, slice_size: 7591\r\n* move_size: 5952, slice_size: 592\r\n* move_size: 5928, slice_size: 8192\r\n* move_size: 5904, slice_size: 7599\r\n* move_size: 5880, slice_size: 584\r\n* move_size: 5856, slice_size: 8192\r\n* move_size: 5832, slice_size: 7607\r\n* move_size: 5808, slice_size: 576\r\n* move_size: 5784, slice_size: 8192\r\n* move_size: 5760, slice_size: 7615\r\n* move_size: 5736, slice_size: 568\r\n* move_size: 5712, slice_size: 8192\r\n* move_size: 5688, slice_size: 7623\r\n* move_size: 5664, slice_size: 560\r\n* move_size: 5640, slice_size: 8192\r\n* move_size: 5616, slice_size: 7631\r\n* move_size: 5592, slice_size: 552\r\n* move_size: 5568, slice_size: 8192\r\n* move_size: 5544, slice_size: 7639\r\n* move_size: 5520, slice_size: 544\r\n* move_size: 5496, slice_size: 8192\r\n* move_size: 5472, slice_size: 7647\r\n* move_size: 5448, slice_size: 536\r\n* move_size: 5424, slice_size: 8192\r\n* move_size: 5400, slice_size: 7655\r\n* move_size: 5376, slice_size: 528\r\n* move_size: 5352, slice_size: 8192\r\n* move_size: 5328, slice_size: 7663\r\n* move_size: 5304, slice_size: 520\r\n\r\nSo the problem is obvious (the slice_size will sum to 100MB per run). The root cause should be the grpc buffer management does not work well for large message. This also explains why the throughput will decrease with the increase of the tensor size.\r\n\r\nNot quite familiar with the grpc code, adding an grpc option to change 'gpr_slice_buffer_take_first' to 'gpr_slice_buffer_take_all' can remove the unnecessary memory copy? Tuning the slice size can also help reducing the overhead but can't eliminate it.", "Interesting!\r\n\r\nThe correct fix is probably to have `grpc_chttp2_incoming_byte_stream` become a ring-like buffer, so instead of doing a move down the slice array, we just increment an index. When we reach the end of `slices`, we can reset the counter to zero.\r\n\r\nI'll make sure someone takes a look soon.", "@llhe amazing investigation work!", "I wonder if there's extra ineffiency in that benchmark in that repeated fields are used (`Tensor::AsProtoField`) rather than tensor_data (`Tensor::AsProtoTensorContent`), I see 11% of the time being spent in ` RepeatedField::Reserve`", "@yaroslavvb Do you mean the serialization in sending side? I haven't identified that issue. I use float32 with size 100MB, and looks like it goes in to [this branch](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_tensor_coding.cc#L220) as expected. And in the receiving side, it also goes to [this branch](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/tensor_coding.cc#L199) and in [protobuf](https://github.com/google/protobuf/blob/master/src/google/protobuf/io/coded_stream_inl.h#L70).\r\n\r\nHowever, the 18.2% time consumption looks unexpected high to me for the bare `memcpy`, compared to AssignOp's `memcpy`. Maybe just caused by poor alignment?", "As discussed off-channel, we were looking at slightly different benchmarks. My original [ local_distributed_benchmark.py](https://gist.github.com/yaroslavvb/1124bb02a9fd4abce3d86caf2f950cb2) does `sess.run(add_op)` which fetches the buffer back into Python runtime, while timings in this issue is for `sess.run(add_op.op)` which doesn't.\r\n\r\nThe \"fetching into Python\" version is ridiculously slow for gRPC runtime (0.05 GB/sec grpc vs 3.4 GB/sec in-process), whereas the non-fetching is just slow (0.9 GB/sec grpc vs 20.2 GB/sec in-process) for `Xeon(R) CPU E5-2630 v3 @ 2.40GHz`\r\n", "BTW, you can partially mitigate this problem by running multiple ps processes and sharding your variables over\u00a0them.\r\nOn a 32 core Xeon, I can get transfer rate to go from 0.9 GB/s to 2.6 GB/s\r\n[sharded_ps_benchmark.py](https://gist.github.com/yaroslavvb/ea1b1bae0a75c4aae593df7eca72d9ca)\r\n\r\nthis makes the logic more complicated unfortunately\r\n```\r\n./sharded_ps_benchmark.py --ps=8\r\n...\r\nworker 0 done: 2555.34 MB per second\r\n\r\n```", "@yaroslavvb Good suggestion! Make more shardings should help.", "This is being fixed here: https://github.com/grpc/grpc/issues/8975.\r\nI made an integration with the temp fix: https://github.com/llhe/tensorflow/tree/grpc-fix. \r\n\r\nThe issue with grpc polling buffer is resolved:\r\n[device1-prof-grpc-fix.pdf](https://github.com/tensorflow/tensorflow/files/653818/device1-prof-grpc-fix.pdf)\r\n\r\nThe current remaining unnecessary memory copy is a known issue marked as TODO by Jeff and Sanjay: [tensor raw content decoding](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/tensor_coding.cc#L199) which actually [buffer memcpy](https://github.com/google/protobuf/blob/master/src/google/protobuf/io/coded_stream_inl.h#L70).\r\n\r\n", "With that patch, I'm seeing 10x improvement for transfer speed of 1GB buffer\r\n\r\n./[sharded_ps_benchmark.py](https://gist.github.com/yaroslavvb/ea1b1bae0a75c4aae593df7eca72d9ca) --ps=1 --iters=1 --data_mb=1024\r\n`worker 0 done: 104.52 MB per second\r\n`\r\n\r\nafter patch\r\n`worker 0 done: 1088.05 MB per second\r\n`", "@jhseu, thought you might want to take a look at this issue. Thanks.", "Is there any news on this being ported over to TensorFlow? Seems to be a very straightforward patch.", "Looks like it hasn't been merged into gRPC yet. I'll make sure it gets into the TensorFlow 1.0 release.", "Anyone working on this? The grpc patch is merged. I can have a try.", "Yeah, that'd be great! Note that it didn't make it into the gRPC 1.1 release, so you'd have to update gRPC to a more recent git version.\r\n\r\nWe also use a custom BUILD file that's in third_party/grpc.BUILD. You can diff it from the version it's derived from to see its changes. Ideally, we'd just use gRPC's build file and not have our own, but I'm not sure how feasible that is. I'm not sure how many of those changes are still required, or whether any more might be needed. We haven't updated gRPC for a few months.", "Also, if you update, it'd be useful to run this benchmark to compare before and after:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpcbench_test.cc\r\n\r\nRun it with: `bazel run -c opt rpcbench_test -- --benchmarks=all`", "I'd be super interested in hearing the result.\n\nOn Thu, Feb 9, 2017 at 7:56 PM Jonathan Hseu <notifications@github.com>\nwrote:\n\n> Also, if you update, it'd be useful to run this benchmark to compare\n> before and after:\n>\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpcbench_test.cc\n>\n> Run it with: bazel run -c opt rpcbench_test -- --benchmarks=all\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6116#issuecomment-278851372>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJpudblAbqS0zHHFttY5W99lwFot0MXkks5ra9_7gaJpZM4LFH9p>\n> .\n>\n", "@ctiller Is bazel still supported in gRPC trunk? Looks like there are somethings broken, like unreferenced header, incorrect header path, redefined symbols etc. I'm trying to solve them to make it build with tensorflow.", "We are in the process of switching to Bazel as our source of truth. I'd be\nsurprised if anything is broken, but if so it's something we'd want to\ntrack down immediately.\n\nCan I trouble you for more details?\n\nOn Sun, Feb 12, 2017, 9:38 AM Liangliang He <notifications@github.com>\nwrote:\n\n> @ctiller <https://github.com/ctiller> Is bazel still supported in gRPC\n> trunk? Looks like there are somethings broken, like unreferenced header,\n> incorrect header path, redefined symbols etc. I'm trying to solve them to\n> make it build with tensorflow.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6116#issuecomment-279234376>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJpudbyHQLiZLObsOhH5Ntxydr_6asDiks5rb0OwgaJpZM4LFH9p>\n> .\n>\n", "@ctiller I encountered some issues when I try to include grpc as a bazel submodule:\r\n1. duplicated symbols, for example, `grpc_register_built_in_plugins` is defined both in `grpc_unsecure_plugin_registry.c` and `grpc_plugin_registry.c` which are both included in `grpc++_unsecure` target, there more than one such cases, see [all the hacking fix here](https://github.com/llhe/tensorflow/blob/19bf520f381b50086aadb2eb41eabfeca55d0e3e/third_party/grpc.BUILD#L58)\r\n2. incorrect header path -- This is a actually a false alarm, I finally resolved it by deleting nanopb package from tensorflow workspace, the issue is resolved.", "The benchmark result posted here https://github.com/tensorflow/tensorflow/pull/7466", "Nice! More than 3x faster for large Tensors.", "Looks like it also may have some regressions (but perhaps within noise). I'll run it on our benchmark machines.", "@tfboyd ", "This PR is still being worked on.  The PR was updated recently.", "Shall we close this issue as gRPC upgrade has been completed by @jhseu? I re-ran the test [benchmark_grpc_recv.py](https://gist.github.com/yaroslavvb/e196107b5e0afc834652bd3153030c42) and found it has been improved from 100-200 MB/s to 800 MB/s. It now fully utilizes a 10Gbps link, and it is actually much better than we'd expected.", "@jhseu @byronyi woohoo! progress!", "Wow @yaroslavvb I suppose you are a committer to TF now? That is awesome.", "@byronyi , I re-ran the test using tf built from the latest master branch, and I still get\r\n```\r\nLocal rate:       1766.32 MB per second\r\nDistributed rate: 287.89 MB per second\r\n```\r\nnot much improvement.\r\nI ran the script using 3 different machines, and `grpc+gdr` is about 3400MB/s, `grpc` is about `500MB/s`.", "For your reference:\r\n```\r\n$ md5sum benchmark_grpc_recv.py\r\ncebe408e4063bb9db817a2f75d5cd792  benchmark_grpc_recv.py\r\n$ python benchmark_grpc_recv.py\r\n2017-08-14 14:00:54.045878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Found device 0 with properties:\r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.17GiB freeMemory: 456.25MiB\r\n2017-08-14 14:00:54.248079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Found device 1 with properties:\r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 11.17GiB freeMemory: 456.25MiB\r\n2017-08-14 14:00:54.463848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Found device 2 with properties:\r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:82:00.0\r\ntotalMemory: 11.17GiB freeMemory: 456.25MiB\r\n2017-08-14 14:00:54.690266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Found device 3 with properties:\r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 11.17GiB freeMemory: 456.25MiB\r\n2017-08-14 14:00:54.691945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Device peer to peer matrix\r\n2017-08-14 14:00:54.692043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] DMA: 0 1 2 3\r\n2017-08-14 14:00:54.692057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] 0:   Y Y N N\r\n2017-08-14 14:00:54.692067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] 1:   Y Y N N\r\n2017-08-14 14:00:54.692075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] 2:   N N Y Y\r\n2017-08-14 14:00:54.692083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] 3:   N N Y Y\r\n2017-08-14 14:00:54.692099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1052] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2017-08-14 14:00:54.692111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1052] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5)\r\n2017-08-14 14:00:54.692121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1052] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40m, pci bus id: 0000:82:00.0, compute capability: 3.5)\r\n2017-08-14 14:00:54.692130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1052] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40m, pci bus id: 0000:83:00.0, compute capability: 3.5)\r\nE0814 14:00:55.341150222    5197 ev_epoll1_linux.c:1051]     grpc epoll fd: 49\r\n2017-08-14 14:00:56.331284: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nE0814 14:00:56.331737994    5285 ev_epoll1_linux.c:1051]     grpc epoll fd: 3\r\n2017-08-14 14:00:56.353470: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nE0814 14:00:56.353890934    5283 ev_epoll1_linux.c:1051]     grpc epoll fd: 3\r\nLocal rate:       10118.01 MB/s\r\nDistributed rate: 726.68 MB/s\r\n```", "I got\r\n```\r\nLocal rate:       10841.73 MB/s\r\nDistributed rate: 971.94 MB/s\r\n```\r\n, but have you run workers on two different machines, how is the speed?", "I use [this script](https://gist.github.com/suiyuan2009/3022d9aecd05eb5ea68a606299810928) to test tensor transmission on 3 different machines, note that task 2 is a client which is responsible for submitting job. `grpc` is about 500MB/s, `grpc+gdr` is about 3400MB/s.\r\n```\r\npython3 tensor_transmission.py --host=xxxx1 --port1=xx1 --host_2=xxxx2 --port2=xx2 --task=0\r\npython3 tensor_transmission.py --host=xxxx1 --port1=xx1 --host_2=xxxx2 --port2=xx2 --task=1\r\npython3 tensor_transmission.py --host=xxxx1 --port1=xx1 --host_2=xxxx2 --port2=xx2 --task=2\r\n```", "So I just tested this again on latest version, and the speed is roughly the same. However, this 971 MB/second is too slow. AWS now has 25 Gbps ethernet cards, so using this capacity requires being able to send 100MB tensor in <=32ms. Currently RecvTensor of that size takes >100ms locally\r\n\r\n```\r\nwget -N https://gist.githubusercontent.com/yaroslavvb/e196107b5e0afc834652bd3153030c42/raw/5ff6df416933232fc2d3f09416e9bee50b221367/benchmark_grpc_recv.py\r\npython benchmark_grpc_recv.py --data_mb=100 --iters=100\r\n\r\nLocal rate:       15320.74 MB/s\r\nDistributed rate: 872.92 MB/s\r\n```\r\n\r\n![screenshot 2018-03-05 20 08 06](https://user-images.githubusercontent.com/23068/37013849-59cf28d4-20b1-11e8-9d90-5b75c5d061ed.png)\r\n\r\n", "@yaroslavvb I guess you could give the latest [accelerated networking](https://docs.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-cli#confirm-that-accelerated-networking-is-enabled) feature a shot, which provides Mellanox ConnectX-3/ConnectX-3 Pro Virtual Function (and supposedly supports RDMA). ", "@byronyi well, this slowness within a single machine, so the bottleneck must software related. My suspicion is that RecvTensor involves a single-threaded memcpy somewhere, this would explain 75% of the slowness -- I see 105ms, while 100 MB at 1.25 GBps memcpy speed = 80ms", "@yaroslavvb Well I do agree more optimisations could be done with gRPC to eliminate memcpy even with current TCP/IP stack. The loopback performance in my DigitalOcean box is around 20 Gbps:\r\n\r\n```shell\r\n$ sudo perf record netperf -t TCP_STREAM -H 127.0.0.1\r\nMIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 127.0.0.1 () port 0 AF_INET : demo\r\nRecv   Send    Send\r\nSocket Socket  Message  Elapsed\r\nSize   Size    Size     Time     Throughput\r\nbytes  bytes   bytes    secs.    10^6bits/sec\r\n\r\n 87380  16384  16384    10.00    18905.90\r\n[ perf record: Woken up 6 times to write data ]\r\n[ perf record: Captured and wrote 1.338 MB perf.data (30386 samples) ]\r\n```\r\n\r\nBut I don't think it is really about memcpy, as the kernel also memcpy and it seems doing it pretty fast:\r\n```shell\r\n$ sudo perf report --header\r\n# ========\r\n# captured on: Thu Mar  8 05:09:45 2018\r\n# hostname : localhost\r\n# os release : 4.15.0-1-amd64\r\n# perf version : 4.15.4\r\n# arch : x86_64\r\n# nrcpus online : 4\r\n# nrcpus avail : 4\r\n# cpudesc : Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz\r\n# cpuid : GenuineIntel,6,79,1\r\n# total memory : 8172116 kB\r\n# cmdline : /usr/bin/perf_4.15 record netperf -t TCP_STREAM -H 127.0.0.1\r\n# event : name = cycles, , size = 112, { sample_period, sample_freq } = 1500, sample_type = IP|TID|TIME|PERIOD, disabled = 1, inherit = 1, mmap = 1, comm = 1,\r\n# CPU_TOPOLOGY info available, use -I to display\r\n# NUMA_TOPOLOGY info available, use -I to display\r\n# pmu mappings: breakpoint = 5, cpu = 4, software = 1, tracepoint = 2, msr = 6\r\n# CACHE info available, use -I to display\r\n# missing features: TRACING_DATA BRANCH_STACK GROUP_DESC AUXTRACE STAT\r\n# ========\r\n#\r\n#\r\n# Total Lost Samples: 0\r\n#\r\n# Samples: 13K of event 'cycles'\r\n# Event count (approx.): 19061735130\r\n#\r\n# Overhead  Command  Shared Object      Symbol\r\n# ........  .......  .................  ..............................................\r\n#\r\n    34.79%  netperf  [kernel.kallsyms]  [k] copy_user_enhanced_fast_string\r\n     4.40%  netperf  [kernel.kallsyms]  [k] tcp_sendmsg_locked\r\n     3.08%  netperf  [kernel.kallsyms]  [k] __tcp_ack_snd_check\r\n     2.55%  netperf  [kernel.kallsyms]  [k] __pv_queued_spin_lock_slowpath\r\n     2.29%  netperf  [kernel.kallsyms]  [k] syscall_return_via_sysret\r\n     1.77%  netperf  [kernel.kallsyms]  [k] pvclock_clocksource_read\r\n     1.49%  netperf  [unknown]          [k] 0xfffffe000003201e\r\n     1.38%  netperf  [kernel.kallsyms]  [k] __raw_callee_save___pv_queued_spin_unlock\r\n     1.23%  netperf  [kernel.kallsyms]  [k] get_page_from_freelist\r\n```", "It sounds like your loopback performance is also bottlenecked by single-threaded memcpy, 20 Gbps loopback seems low given that AWS can do 25 Gbps over ethernet (ps, my earlier numbers are GBps rather than Gbps)", "@yaroslavvb If you are using in kernel TCP/IP stack then it\u2019s rather unlikely to avoid memory copy at receiving side even if you use the new feature [MSG_ZEROCOPY](https://lwn.net/Articles/726917/) available since Linux 4.9 :)", "Just a quote from the article:\r\n\r\n> Readers might be wondering why the patch does not support zero-copy reception; while the patch set itself does not address this question, it is possible to make an educated guess. Reading is inherently harder because it is not generally known where a packet is headed when the network interface receives it. In particular, the interface itself, which must place the packet somewhere, is probably not in a position to know that a specific buffer should be used. So incoming packets end up in a pile and the kernel sorts them out afterward. Fancier interfaces have a fair amount of programmability, to the point that zero-copy reception is not entirely infeasible, but it remains a more complex problem. For many common use cases (web servers, for example), transmission is the more important problem anyway.", "So I think the original problem with RecvTensor is the **single-threaded** memcpy. You can do memory copy fast if you use multiple threads. For instance putting 100MB object into [Ray](https://github.com/ray-project/ray) storage takes 17ms,  that involves a memory copy and translates to about 50 Gbps. I can add a constant to 100MB worth of 1s and put result into new memory in 3.5ms, that's about 250 Gbps and is probably the upper limit of how fast you can copy memory on XeonV4", "Would a possible workaround be splitting your tensor into partitions and load balance your RecvTensor calls? I think that\u2019s what TF currently does to load balance PS tasks.", "Plus for a truly distributed case, as TCP connection is stream oriented, it would involve a fair amount of locking if you\u2019d perform multithreading with the same socket and effectively make its performance equivalent to the single-thread case at best. If we restrict it to the intranode case, why not just use immutable shared memory? You could avoid bulk memcpy all together."]}]