[{"number": 48075, "title": "pip install tensorflow==1.15 with Python 2.7 issue", "body": "**System information**\r\n \r\n On `CentOS Linux 7 (Core)` I'm trying to install `tensorflow==1.15` in a Python `2.7.3` venv (in order to run [this repo](https://github.com/ShafeenTejani/fast-style-transfer)).\r\n \r\n However, I get the error that there is `No matching distribution found for tensorflow==1.15`?\r\n \r\n \r\n ```\r\n $ python --version\r\nPython 2.7.13\r\n$ virtualenv tensorflow\r\n\r\n$ virtualenv venv\r\n$ source venv/bin/activate\r\n\r\n\r\n\r\n(venv) $ pip install --upgrade pip\r\nLooking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\r\nRequirement already up-to-date: pip<=18.1 in ./venv/lib/python2.7/site-packages (from -c /cvmfs/soft.computecanada.ca/config/python/constraints.txt (line 1)) (18.1)\r\n\r\n(venv) $ pip install tensorflow==1.15\r\nLooking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\r\nCollecting tensorflow==1.15\r\n  Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: )\r\nNo matching distribution found for tensorflow==1.15\r\n```\r\n\r\n## `$ pip freeze`\r\n\r\n```\r\n(venv) $ pip freeze\r\n-f /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/avx2\r\n-f /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/generic\r\n-f /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\r\n```", "comments": ["@moldach \r\nCan you please upgrade to tensorflow 2.x and refer to this guide and simlilar issues as there is no support for 1.x.\r\n#42367, #47824,#45642,#34302, See https://www.tensorflow.org/install/pip#system-requirements\r\n\r\npip install --upgrade tensorflow\r\n\r\nThanks!", "No.\r\n\r\nhttps://github.com/ShafeenTejani/fast-style-transfer\r\n\r\nSay's you need **Tensorflow 1.n** so **2.x** is not going to work.\r\n\r\nHow can you get a legacy version working in order to use older code-bases?", "Please post output of:\r\n\r\n```\r\npip install -vvv tensorflow==1.15\r\n```\r\n\r\nand\r\n\r\n```\r\npip debug --verbose\r\n```\r\n\r\nNote that we no longer support 1.x so we won't be able to fix anything. This is most likely an issue wth the pip version on your system anyway.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48075\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48075\">No</a>\n"]}, {"number": 48074, "title": "Pow layer is not available when fallback quantized conversion to tflite", "body": "I used tfliteConverter to convert DPED network to tflite and it has the Pow layer inside.\r\nWhen I tried tflite quantization conversion with default optimization with fallback,\r\npow layer is not available to run because the input of Pow layer is dequantized but the parameter of pow layer is not dequantized and tflite return error about the two of them have different data type (float != uint).\r\nThank you.", "comments": ["@fredrec could you take a look?", "@janoslim,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, a minimal code snippet and the dataset you are using. Thanks!", "I am using tensorflow 2.4.1 in google colab.\r\nand my file link is [here.](url\r\n[frozen_DPED-r1.13.zip](https://github.com/tensorflow/tensorflow/files/6216867/frozen_DPED-r1.13.zip)\r\n\r\n)\r\nMy source codes are,\r\n```\r\nimport cv2\r\nimport os\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\n\r\nEXPORT_DIR = \"./DPED_quant_fallback.tflite\"\r\n\r\nimg_path = './0.jpg'\r\n\r\ndef representative_data_gen():\r\n    image = tf.io.read_file(img_path)\r\n    image = tf.io.decode_jpeg(image, channels=3)\r\n    image = tf.image.resize(image, [1, 9437184])\r\n    image = tf.image.convert_image_dtype(image, tf.float32)\r\n    yield [image]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(input_arrays=[\"Placeholder\"],\r\n                                                    graph_def_file=\"./frozen_DPED-r1.13.pb\",\r\n                                                    output_arrays=[\"generator/add_32\"],\r\n                                                    input_shapes={\"Placeholder\":[1,9437184]})\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\n# converter.inference_input_type = tf.uint8\r\n# converter.inference_output_type = tf.uint8\r\ntflite_model = converter.convert()\r\nopen(EXPORT_DIR, \"wb\").write(tflite_model)\r\n```\r\n", "Hi, could you please try using `converter.experimental_new_quantizer = True` before `convert()`, as I believe that error is fixed in the new quantizer.\r\n\r\n`!pip install tf-nightly` to use the new flag.", "@janoslim,\r\nAny updates regarding this?\r\n\r\nI was able to run the code without any issues on the latest TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/7611451e11d1630196065645cd1c733d/48074.ipynb). Thanks!", "Hello @amahendrakar ,\r\nI'm sorry for the late reply. \r\nI think it works well. \r\nThanks a lot!!.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48074\">No</a>\n"]}, {"number": 48073, "title": "No code size difference between MicroInterpreter and AllOpsResolver ", "body": "@tensorflow/micro\r\nHi,\r\n\r\nI do not see any code size reduction while using microInterpreter with a selection of usefull operators versus the full ops list.\r\nFor a quite simple network the code size for TFlite runtime is 160KBytes. Which is quite huge compared to what it claims (16KB for CM3!).\r\nCan you explain this behavior?\r\nThank you,\r\nOC\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Unbuntu 1804\r\n- TensorFlow installed from (source or binary): Anaconda\r\n- Tensorflow version (commit SHA if source): 2.4.1\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): tflite micro\r\n\r\n**Description of the problem**\r\n\r\nMy TFLITE model is a 12 layers network of 52000 bytes obtained with full integer (int8) quantization.\r\nAll application code, except TFLite stuff is around 8KBytes. TFLite code is around 160KB.\r\n\r\n- \"Full\" operators version is done with:\r\n_static tflite::AllOpsResolver resolver;\r\nstatic tflite::MicroInterpreter static_interpreter( model, resolver, tensor_arena, kTensorArenaSize, error_reporter);_\r\n\r\n=> Code size:\r\n.text                **164818**   \r\n.rodata              124764    \r\n\r\n- \"Reduced\" version\r\n_static tflite::MicroMutableOpResolver<5> micro_op_resolver;\r\n  micro_op_resolver.AddConv2D();\r\n  micro_op_resolver.AddDepthwiseConv2D();\r\n  micro_op_resolver.AddAveragePool2D();\r\n  micro_op_resolver.AddReshape();\r\n  micro_op_resolver.AddFullyConnected();\r\n  static tflite::MicroInterpreter static_interpreter(model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);_\r\n\r\n=> Code size:\r\n.text                **165442**     (Even bigger!)\r\n.rodata              124816    \r\n\r\n** Compilation **\r\nCompilation of code is done with linux gcc\r\nFlags: \r\n-Os \r\n-DTF_LITE_STATIC_MEMORY \r\n-DNDEBUG \r\n-DTF_LITE_DISABLE_X86_NEON \r\n\r\n", "comments": ["I have a smiliar problem. I want to implement it to arm, so I am using ARM compiler. When I tried to compile the helloworld example from tflite micro, it gives me a hex file for over 300KB...", "@ocaff1 \r\nPlease share simple stand alone code or a colab gist with the issue reported.", "Hi, \r\n\r\nThe issue is solved when using a **static lib.** \r\nAll unused objects are then removed at link for the code with MicroMutableOpResolver.\r\n=> Code size\r\n.text                 **51874**      \r\n.rodata             61320     \r\n\r\nMy mistake was to compile all the tensorflow sources with application ones. I though compiler/linker was able to remove the unused part automatically. It seems not.\r\n\r\nOC.\r\n\r\n", "@ocaff1 \r\nPlease move this issue to closed status as its resolved."]}, {"number": 48072, "title": "Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.2\r\n- Are you willing to contribute it (Yes/No):Wish I was smart enough\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nResize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nEasy switch and tf wll become as competant as pytorch\r\n**Any Other info.**\r\nNope, just that", "comments": ["You may want to post this issue now [tf-onnx](https://github.com/onnx/onnx-tensorflow/issues) repo as a feature request since the `runtime error` hits from tf-onnx codebase.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48071, "title": "Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):conda\r\n- TensorFlow version (use command below):2.2\r\n- Python version:3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nResize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@tensorflowbutler Nice", "@Adhithya-tech,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "@amahendrakar I think \r\n\r\n> This is a known limitation, as seen in notes section of https://github.com/onnx/onnx-tensorflow/blob/master/doc/support_status_v1_7_0.md.\r\n\r\nThis should solve it", "Anyways, it's basically converting a faster RCNN model to tensorfow using onnx in between", "> Anyways, it's basically converting a faster RCNN model to tensorfow using onnx in between\r\n\r\nWithout a sample code it would be difficult for us to debug the issue. Hence, could you please provide a minimal code snippet to reproduce the issue or share the Python script/notebook you are running. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48071\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48071\">No</a>\n", "@amahendrakar  \r\n\r\nHere is one text boundary detection model I'm using. Let me know if this helps - \r\n\r\nInstall ONNX-Tensorflow\r\n\r\n`git clone git@github.com:onnx/onnx-tensorflow.git && cd onnx-tensorflow`\r\n`pip install -e .`\r\n\r\nCode snippet: \r\nModel - [Link](https://1drv.ms/u/s!Astr8XJs2VCYge02Is9jjtWzpec3aA?e=88EZTP)\r\n\r\n```python\r\nimport cv2\r\nimport torch\r\nimport numpy as np\r\nfrom skimage import io\r\nimport sys\r\nsys.path.append(\"path/to/cloned/folder/onnx-tensorflow\")\r\nimport onnx\r\nfrom onnx_tf.backend import prepare\r\n\r\n\r\n# Load the ONNX file\r\nmodel = onnx.load('onnx_export/detect2.onnx')  #Model File is provided in this above link\r\n\r\n# Prepare the Image Input\r\n\r\nimg = io.imread(img_path)      # Any google image with text on it will do.\r\nimg = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\r\nimg = np.array(img)\r\nimg = img.copy().astype(np.float32)\r\nimg -= np.array([0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0], dtype=np.float32)\r\nimg /= np.array([ 0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0], dtype=np.float32)\r\n\r\nx = torch.from_numpy(img).permute(2, 0, 1)  # Converts [h, w, c] to [c, h, w]\r\nx = Variable(x.unsqueeze(0))                           # Converts [c, h, w] to [b, c, h, w]\r\n\r\n# Import the ONNX model to Tensorflow\r\ntf_rep = prepare(model)\r\n\r\n# Run the forward pass\r\ndetections, features = tf_rep.run(x)   # This function give error as stated in title\r\n\r\n```\r\n\r\nThanks !", "@Adhithya-tech, have you resolved this issue with converting a Torch FasterRCNN model to TF through ONNX? Through some digging, the error is caused by the Resize op that the FasterRCNN model does regardless of the backbone. Buried within the `GeneralizedRCNNTransform` class is this `interpolate` method call:\r\n\r\n```\r\nimage = torch.nn.functional.interpolate(image[None], size=size, scale_factor=scale_factor, mode='bilinear',\r\n                                            recompute_scale_factor=recompute_scale_factor, align_corners=False)[0]\r\n```\r\n\r\nThe key is to set `align_corners` to `True` and retrain the model.", "I am also experiencing the same error. I am using [mmsegmentation](https://github.com/open-mmlab/mmsegmentation) pytorch framework. And as @alexispascual pointed interpolate function is also the problem:\r\n\r\nhttps://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/ops/wrappers.py#L26\r\n\r\nI fixed the operation to use those values:\r\n```python\r\nmode='bilinear'\r\nalign_corners=True\r\nreturn F.interpolate(input, size=size, scale_factor=scale_factor, mode=mode, align_corners=align_corners)\r\n```\r\n\r\nbut I am still getting the same error:\r\n\r\n```\r\nRuntimeError: in user code:\r\n\r\n    /opt/miniconda3/envs/tf_exporter/lib/python3.8/site-packages/onnx_tf/backend_tf_module.py:98 __call__  *\r\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\r\n    /opt/miniconda3/envs/tf_exporter/lib/python3.8/site-packages/onnx_tf/backend.py:328 _onnx_node_to_tensorflow_op  *\r\n        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)\r\n    /opt/miniconda3/envs/tf_exporter/lib/python3.8/site-packages/onnx_tf/handlers/handler.py:58 handle  *\r\n        cls.args_check(node, **kwargs)\r\n    /opt/miniconda3/envs/tf_exporter/lib/python3.8/site-packages/onnx_tf/handlers/backend/resize.py:125 args_check  *\r\n        exception.OP_UNSUPPORTED_EXCEPT(\r\n    /opt/miniconda3/envs/tf_exporter/lib/python3.8/site-packages/onnx_tf/common/exception.py:50 __call__  *\r\n        raise self._func(self.get_message(op, framework))\r\n\r\n    RuntimeError: Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow.\r\n\r\n```"]}, {"number": 48070, "title": "3D Non Max Suppression", "body": "**System information**\r\n\r\n- TensorFlow version (you are using): 2.0.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n- Tensorflow incorporates a native Non Max Suppression algorithm for 2D bounding boxes.\r\n\r\n**Will this change the current api? How?**\r\n\r\n- The feature would add a Non Max Suppression algorithm to TF for 3D bounding boxes on the model of the 2D case algorithm.\r\n\r\n**Who will benefit with this feature?**\r\n\r\n- This addon will greatly benefit to the increasing community of 3D object detection or instance segmentation.\r\n\r\n**Any Other info.**\r\n", "comments": ["@gdavid57 \r\nPlease provide with more information in support of your request.", "**Describe the feature and the current behavior/state.**\r\n\r\nTensorFlow already incorporates a native Non Max Suppression algorithm for 2D bounding boxes. It is used for instance for objet detection tasks, usually after a anchor box generation step as it is observed in YOLO. Another remarkable use exists within the Mask R-CNN algorithm: many anchor boxes are generated (~200,000-300,000 boxes). After a step to reduce this number to 6,000 using some classification results, tf.image.non_max_suppression is called to sort these 6,0000 best anchors and finally obtain 2,000 regions of interest.\r\n\r\nThe feature would consist of a Non Max Suppression algorithm for three dimensional bounding boxes. This 3D Non Max Suppression would almost have the same parameters than the existing 2D implementation, called with\r\n```\r\ntf.image.non_max_suppression(\r\n    boxes, scores, max_output_size, iou_threshold=0.5,\r\n    score_threshold=float('-inf'), name=None\r\n)\r\n```\r\nwith _boxes_ containing no more the 4 coordinates of the two points that define a 2D bounding box, but the 6 coordinates of the two points defining a 3D bounding box. The other parameters would stay the same. The heart of this addition would be to adapt the IOU (Intersection Over Union) function within the Non Max Suppression C file to a three dimensional IOU evaluation.\r\n\r\n**Will this change the current api? How?**\r\n\r\nThe feature would add a Non Max Suppression algorithm _tf.image.non_max_suppression_3d_ to TensorFlow for 3D bounding boxes following the structure of the existing 2D implementation. It will add three files to tensorflow/tensorflow/core/kernels/image/ : _non_max_suppression_3d_op.cc_, _non_max_suppression_3d_op.cu.cc_ and _non_max_suppression_3d_op.h_.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThis addon will greatly benefit to the increasing community of 3D object detection or 3D instance segmentation. The demand is increasing. I give two examples: this stackoverflow [thread](https://stackoverflow.com/questions/66311026/how-to-do-3d-non-max-suppression) (alas removed by its author now), and this [implementation](https://github.com/MIC-DKFZ/medicaldetectiontoolkit) in PyTorch that possesses a not working 3D implementation of the NMS (the implementation author have been asked many times to correct it).\r\n\r\nIt actually exists some implementations in Python language but the speed performance given by a C implementation would be crucial when handling 3D data.", "@gdavid57 As this is a new feature, I think this is more suited for `tf-addons` repo [here](https://github.com/tensorflow/addons/issues). Please raise the feature request with that repo as close it here. Thanks!", "Hi, I want to contribute to 3D NMS. ", "@jvishnuvardhan Alright, thank you for your response.\r\n@therc01 Hi! follow the [way](https://github.com/tensorflow/addons/issues/2434)"]}, {"number": 48069, "title": "Slow startup and model loading time", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Linux Ubuntu 18.04, 5.4.0-66-generic\r\n- Device: MPG B550I GAMING EDGE MAX WIFI (MS-7C92). AMD Ryzen 9 5950X 16-Core Processor\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce RTX 3090, 24265MiB\r\n- NVIDIA Driver: 460.39\r\n\r\n**Describe the current behavior**\r\nI was experiencing very slow speed when loading models into the GPU memory (about 1MiB/s when watching with nvidia-smi) and investigated this by using tensorflow/tensorflow:2.1.0-gpu-py3 docker image to run this command:\r\n`time python -c \"import tensorflow as tf; tf.test.is_gpu_available()\"`\r\n\r\nand it prints the following logs:\r\n```\r\n2021-03-25 07:39:11.387959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2021-03-25 07:39:11.388827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\nWARNING:tensorflow:From <string>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2021-03-25 07:39:11.747442: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2021-03-25 07:39:11.771337: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3399945000 Hz\r\n2021-03-25 07:39:11.772645: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47819f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-03-25 07:39:11.772667: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-03-25 07:39:11.774848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2021-03-25 07:39:11.863115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-25 07:39:11.863658: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4783c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-03-25 07:39:11.863670: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6\r\n2021-03-25 07:39:11.863767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-25 07:39:11.864237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:2b:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.785GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-03-25 07:39:11.864264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-25 07:39:11.864281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-03-25 07:39:11.865203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-03-25 07:39:11.865342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-03-25 07:39:11.866162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-25 07:39:11.866588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-25 07:39:11.866606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-25 07:39:11.866644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-25 07:39:11.867128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-25 07:39:11.867588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2021-03-25 07:39:11.867606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n```\r\n\r\nafter which it doesn't print anything and after about 2.5 minutes prints this:\r\n```\r\n2021-03-25 07:41:58.768624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-25 07:41:58.768648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2021-03-25 07:41:58.768653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2021-03-25 07:41:58.768811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-25 07:41:58.769347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-25 07:41:58.769954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 22243 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:2b:00.0, compute capability: 8.6)\r\n\r\nreal\t2m48.205s\r\nuser\t2m47.844s\r\nsys\t0m2.368s\r\n```\r\n\r\nRunning the same command again finishes execution in 2 seconds. \r\n\r\n**Describe the expected behavior**\r\nThere should not be such a long startup and model loading time. I am not entirely sure if this issue lies with tensorflow, the driver or the hardware itself. I would greatly appreciate any help I can get.\r\n\r\n**Standalone code to reproduce the issue**\r\nWithin tensorflow/tensorflow:2.1.0-gpu-py3 docker container, run\r\n`time python -c \"import tensorflow as tf; tf.test.is_gpu_available()\"`", "comments": ["@julng \r\ncan you please upgrade your tf version to 2.4 and let us know if you face the issue.", "Thanks for the help. Using tensorflow/tensorflow:2.4.1-gpu fixed the performance issue. This other log message kept appearing, though I am not too concerned about it.\r\n`W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'`\r\n\r\nOn a separate note, I have other older models using tf version 1.15.0 that experience this same issue as well on the same machine. Is there a workaround for these models?\r\n\r\n**System Information**\r\n* TensorFlow installed from (source or binary): source\r\n* TensorFlow version: v1.15.0\r\n* Python version: NA, using C++\r\n* Bazel version (if compiling from source): 0.26.1\r\n* GCC/Compiler version (if compiling from source): 7.5.0\r\n* CUDA/cuDNN version: 10.2.89", "@julng \r\nPlease  move this issue to closed status as the performance issue is resolved, for any other query please create a new issue and it will be addressed.\r\nAS there is no more support for 1.x, users have to upgrade to 2.x only.", "Hi. I believe I am facing the same issue with a similar hardware configuration: Ryzen Zen 3 CPU with RTX 3090. \r\nWould this be an issue with the Linux Kernel or Nvidia Driver? Anything we can do to change these for compatibility? \r\nWe do have production software working on Tensorflow 1.14 and 1.15 so it would be hard for us to move to 2.4 in the immediate future. ", "As a workaround, following this [install tf1.15 for Nvidia 3090](https://www.pugetsystems.com/labs/hpc/How-To-Install-TensorFlow-1-15-for-NVIDIA-RTX30-GPUs-without-docker-or-CUDA-install-2005/#UPDATE!QuickSetup--Wed15Sep2021) blog, might allow you to successfully run the tf1.x code.\r\n\r\n![image](https://user-images.githubusercontent.com/50506397/145914353-2dfa6707-1103-4b3d-9fb3-6dc3b56cb6aa.png)\r\n"]}, {"number": 48068, "title": "how to reduce libtensorflowlite.so size", "body": "\r\n**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 18.04):\r\n- TensorFlow (source)\r\n- TensorFlow version (master)\r\n- Bazel version (3.7.2):\r\n- ndk (r21)\r\n- android sdk(android-30)\r\n\r\n**Describe the problem**\r\nwhen I use bazel to build libtensorflowlite.so, I got library size about 3.0M\r\nwhen I use bazel to build tensorflow-lite.aar, I got aar size about 1.1M\r\nquestion: How to reduce libtensorflowltie.so size, it's too big for me when i use c++ to inference model\r\n\r\nso build command:\r\nbazel build -c opt --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\" --fat_apk_cpu=arm64-v8a --config=android_arm64 //tensorflow/lite/java:tensorflow-lite\r\n\r\naar build command:\r\nbazel build -c opt --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\" --fat_apk_cpu=arm64-v8a --config=android_arm64 //tensorflow/lite/java:tensorflow-lite", "comments": ["Please refer to https://www.tensorflow.org/lite/guide/reduce_binary_size. You can reduce your TFLite binary based on selective building. Basically, selective building will include the necessary operators for your models.", "> Please refer to https://www.tensorflow.org/lite/guide/reduce_binary_size. You can reduce your TFLite binary based on selective building. Basically, selective building will include the necessary operators for your models.\r\n\r\nI don\u2019t need the aar package. I need a library that can be inferred in C++. Is this method suitable for generating C++ library? And how to make selective building\uff0c are there executable scripts or input parameters that can be set?", "You can build your own selective built C++ TensorFlow Lite library only under the bazel development environment.\r\n\r\nFor example,\r\n\r\n```\r\nload(\r\n    \"//third_party/tensorflow/lite:build_def.bzl\",\r\n    \"tflite_custom_cc_library\",\r\n)\r\n\r\n# A selective built tflite for testing.\r\ntflite_custom_cc_library(\r\n    name = \"custom_tflite_lib\",\r\n    models = [\r\n        \"first_model_tflite_path_under_bazel\",\r\n        \"second_model_tflite_path_under_bazel\",\r\n    ],\r\n)\r\n```", "I imitated the build_aar.sh file and created a bash file. In bash, I added a function that can compile the C++ library correctly. The library is very small, only 162k, but when I use this library to inference, there are many API functions that can\u2019t be found. \r\n\r\n**Added function:**\r\nfunction generate_tflite_cc {\r\n  pushd ${TMP_DIR} > /dev/null\r\n  message=(\r\n    'load(\"//tensorflow/lite:build_def.bzl\", \"tflite_custom_cc_library\")'\r\n    ''\r\n    'tflite_custom_cc_library('\r\n    '    name = \"custom_tflite_lib\",'\r\n  )\r\n  message+=('    '$(generate_list_field \"models\" $MODEL_NAMES))\r\n  message+=(\r\n    ')'\r\n    ''\r\n  )\r\n  printf '%s\\n' \"${message[@]}\" >> BUILD\r\n\r\n  popd > /dev/null\r\n  bazel  build -c opt --cxxopt='--std=c++11' \\\r\n    --config=${CONFIGS}\\\r\n    --fat_apk_cpu=${TARGET_ARCHS} \\\r\n    --crosstool_top=//external:android/crosstool \\\r\n    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n    //tmp:custom_tflite_lib\r\n}\r\n\r\n**NDK build error:**\r\n*.cpp:537: undefined reference to `tflite::Interpreter::SetNumThreads(int)'\r\n*.cpp:543: undefined reference to `tflite::Interpreter::Invoke()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_PAD()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_DEQUANTIZE()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_AVERAGE_POOL_2D()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int)'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_PACK()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_TRANSPOSE_CONV()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_DEPTHWISE_CONV_2D()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_ADD()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_RELU6()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_CONCATENATION()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_RESIZE_BILINEAR()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_RELU()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_SPACE_TO_BATCH_ND()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_CONV_2D()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_SHAPE()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_BATCH_TO_SPACE_ND()'\r\nlibcustom_tflite_lib.so: undefined reference to `tflite::ops::builtin::Register_STRIDED_SLICE()'", "Looks like the above custom built script strips out all the operators from the generated library file. I would recommend using the bazel based solution for this case, which is verified working.", "I put\r\n\"tflite_custom_cc_library(\r\n    name = \"custom_tflite_lib\",\r\n    models = [\r\n        \"model1.tflite\",\r\n        \"model2.tflite\",\r\n    ],\r\n)\" into tensorflow/lite/BUILD, and build again, I got the same library, and inference with same error.\r\n\r\nI am not familiar with bazel build, could you provide a script that can be directly executed? TKS", "Please refer to the following document for the bazel build environment:\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_android#set_up_build_environment_without_docker\r\n\r\nAnd then run the following command and you can find the generated artifact file paths in the screen:\r\n\r\n```\r\nbazel build -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  //tensorflow/lite:custom_tflite_lib\r\n```", "I can generate so, but this so cannot be used normally (missing operators).\r\nHow can i fix this problem? \r\n", "Use the code below ...This will convert frozen graph into tflite and reduce the size of your tflite model... just provide the path of the frozen graph and run the following code:\r\n\r\nimport logging\r\nlogging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nimport pathlib\r\n\r\n#conversion of faster_rcnn_inception_v2_coco_model frozen_inference_graph.pbin to tflite model\r\n\r\nimport tensorflow as tf\r\nimport os \r\n\r\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\r\n\r\n\r\n#In place of frozen_inference_graph.pb you provide the path of your frozen graph... don't change anything else and run the code.\r\n#you will get reduced size tflite model which will be equal to 1/4 size of your model.\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\"frozen_inference_graph.pb\", [\"image_tensor\"], [\"detection_classes\", \"detection_scores\", \"detection_boxes\", \"num_detections\"])\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nmodel = converter.convert()\r\n\r\ntflite_models_dir = pathlib.Path(\"model/\")\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\ntflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\r\ntflite_model_file.write_bytes(model)\r\n\r\n\r\n#To Quantize the model on Export, Set the optimizations flag to optimize for size\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_quant_model = converter.convert()\r\ntflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\r\ntflite_model_quant_file.write_bytes(tflite_quant_model)\r\n", "Did you correctly put the models arguments in your BUILD rule? Could you share how to declare your custom library in the BUILD file?\r\n\r\n@thaink", "+1 We need more explanation about how you build it. Where did you put your models? If you modified the  tensorflow/lite/BUILD, the models should better be copied there.", "Modified in the tensorflow/lite/BUILD:\r\n```\r\nload(\"//tensorflow/lite:build_def.bzl\", \"tflite_cc_shared_object\", \"tflite_copts\", \"tflite_copts_warnings\", \"tflite_custom_cc_library\")\r\ntflite_custom_cc_library(\r\n    name = \"custom_tflite_lib\",\r\n    models = [\r\n        \"model1.tflite\",\r\n        \"model2.tflite\",\r\n    ],\r\n)\r\n```\r\nI put model1.tflite and model2.tflite into the path \"tensorflow/lite\".\r\nExecution: \r\n`bazel build -c opt --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\" --fat_apk_cpu=arm64-v8a --config=android_arm64 //tensorflow/lite:custom_tflite_lib`\r\n\r\nI got:\r\n`-r-xr-xr-x 1 xxxx  xxxx  162K 3\u6708  29 09:17 bazel-bin/tensorflow/lite/libcustom_tflite_lib.so`", "@thaink any thoughts on this?", "Seems like tflite_custom_cc_library does not meant to generate a standalone .so library but to generate a dependency for using in bazel build only.\r\n\r\n@herbiezhao At the end of tensorflow/lite/BUILD file, in the \"deps\" file for the rule \"tensorflowlite\", could you:\r\n- remove \":framework\",\r\n- replace \"//tensorflow/lite/kernels:builtin_ops_all_linked\", with \": custom_tflite_lib\",\r\nand build the //tensorflow/lite:tensorflowlite instead?", "@thaink  I can use your method to compile the library, but the size of the library has not decreased. The size is the same as the normal compilation\r\n`-r-xr-xr-x 1 xxxx xxxx 3.0M  3.29 13:51 bazel-bin/tensorflow/lite/libtensorflowlite.so`", "The problem turn to be the list in -exported_symbols_list is too inclusive. It include all builtin and custom ops definition.\r\n@herbiezhao will the C api work for you?", "I will consider using the C api, but is C++ compilation still possible? Because building the aar package can greatly reduce the size of the library.", "Do you mean the size of the aar file or the so file inside it? the aar file might have been compressed so it should be smaller.", "The size of the aar file is 635KB, and the size of so file in aar is 1384KB. So I think there should be a way to achieve compression.", "Hi, I am trying out the custom C Lib using `tflite_custom_c_library`. Is there an example that might guide me to troubleshoot the custom c_api lib? Here's what I am trying on 2.5.0-rc0:\r\n\r\n1.  Added the following to `tensorflow/lite/c/BUILD`\r\n```\r\ntflite_custom_c_library(\r\n    name = \"custom_tensorflowlite_c\",\r\n    models = [\r\n        \"model.tflite\"\r\n    ]\r\n)\r\n``` \r\n2. Ran `bazel build -c opt //tensorflow/lite/c:custom_tensorflowlite_c `\r\n```logs\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=178\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nINFO: Analyzed target //tensorflow/lite/c:custom_tensorflowlite_c (1 packages loaded, 86 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/c:custom_tensorflowlite_c up-to-date:\r\n  bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.a\r\n  bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.pic.a\r\n  bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so\r\nINFO: Elapsed time: 0.316s, Critical Path: 0.01s\r\nINFO: 1 process: 1 internal.\r\nINFO: Build completed successfully, 1 total action\r\n```  \r\n3. However, when I try to link `libcustom_tensorflowlite_c.so` I get the following error: \r\n``` \r\ncc main.c -I/tensorflow_src/tensorflow/lite/c -I/tensorflow_src \\\r\n-Wl,-rpath=/tensorflow_src/bazel-bin/tensorflow/lite/c \\\r\n-L/tensorflow_src/bazel-bin/tensorflow/lite/c -lcustom_tensorflowlite_c -o test-custom-model\r\n\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::DefaultErrorReporter()'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::Interpreter::AllocateTensors()'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `vtable for tflite::MutableOpResolver'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::MutableOpResolver::AddAll(tflite::MutableOpResolver const&)'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::FlatBufferModel::VerifyAndBuildFromBuffer(char const*, unsigned long, tflite::TfLiteVerifier*, tflite::ErrorReporter*)'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::Interpreter::Invoke()'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::Interpreter::~Interpreter()'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::FlatBufferModel::VerifyAndBuildFromFile(char const*, tflite::TfLiteVerifier*, tflite::ErrorReporter*)'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::Interpreter::SetNumThreads(int)'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::NnApiDelegate()'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::InterpreterBuilder::InterpreterBuilder(tflite::Model const*, tflite::OpResolver const&, tflite::ErrorReporter*)'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::CreateOpResolver()'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::Interpreter::ResizeInputTensor(int, std::vector<int, std::allocator<int> > const&)'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::delegates::InterpreterUtils::InvokeWithCPUFallback(tflite::Interpreter*)'\r\n/tensorflow_src/bazel-bin/tensorflow/lite/c/libcustom_tensorflowlite_c.so: undefined reference to `tflite::Interpreter::ModifyGraphWithDelegate(TfLiteDelegate*)'\r\ncollect2: error: ld returned 1 exit status \r\n```  \r\n\r\nI'll really appreciate some direction! Thanks!", "@avroshk Can you try in `deps` field of `tensorflowlite_c` in tensorflow/lite/c/BUILD:\r\nreplace:\r\n\":c_api\",\r\n\":c_api_experimental\",\r\nwith \r\n`: custom_tensorflowlite_c` , \r\nand build the tensorflowlite_c instead. Please remember to add build config for android if you are doing so.", "Thank you! That did it. I am currently testing on linux. When built using my test model, libtensorflowlite_c.so file size dropped from 2.9M to to 351K", "@thaink One of my models uses one of select TF ops (FlexSelu). Is there a way to build the c_api lib with flex delegate but only include the required ops? I found a C++ example: https://www.tensorflow.org/lite/guide/ops_select#c", "You can follow https://www.tensorflow.org/lite/guide/reduce_binary_size and extract the so file for flex delegate in tensorflow-lite-select-tf-ops.aar", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48068\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48068\">No</a>\n"]}, {"number": 48067, "title": "Tensorflow 2.0: where is tensorflow.contrib.layers.apply_regularization and tensorflow.contrib.layers.l2_regularizer?", "body": "<from tensorflow.contrib.layers import apply_regularization, l2_regularizer>\r\nI am trying to run the example of VAE which uses above code. Need help how to update this for latest tensorflow version.\r\n\r\nbelow is snippet of code where it is getting used\r\n\r\n<def build_graph(self):\r\n\r\n        self.construct_weights()\r\n\r\n        saver, logits = self.forward_pass()\r\n        log_softmax_var = tf.nn.log_softmax(logits)\r\n\r\n        # per-user average negative log-likelihood\r\n        neg_ll = -tf.reduce_mean(tf.reduce_sum(\r\n            log_softmax_var * self.input_ph, axis=1))\r\n        **# apply regularization to weights\r\n        reg = l2_regularizer(self.lam)\r\n        reg_var = apply_regularization(reg, self.weights)**\r\n        # tensorflow l2 regularization multiply 0.5 to the l2 norm\r\n        # multiply 2 so that it is back in the same scale\r\n        loss = neg_ll + 2 * reg_var\r\n        \r\n        train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\r\n\r\n        # add summary statistics\r\n        tf.summary.scalar('negative_multi_ll', neg_ll)\r\n        tf.summary.scalar('loss', loss)\r\n        merged = tf.summary.merge_all()\r\n        return saver, logits, loss, train_op, merged>\r\n\r\n\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1040-gcp x86_64)\r\n- TensorFlow installed from (source or binary): using pip\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n", "comments": ["@anandece4u \r\nKindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48067\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48067\">No</a>\n"]}, {"number": 48066, "title": "Evaluate mobilenet v3 keras model performance on ImageNet", "body": "Hi, I've tested the keras mobilenetv3_large_1.0 model accuracy, the model is created by calling the official API and using the official h5 model:\r\n`model = tf.keras.applications.MobileNetV3Large(alpha=1.0, minimalistic=False, weights='imagenet')`\r\n\r\nI do the following preprocess on ImageNet:\r\n```\r\n1. cropp the image of size [0.875*original height, 0.875*original width] from the center of original image, \r\n2. resize the image to 224x224\r\n3. each channel subtract 127.5, multiply 2 (as the .h5 model has a rescaling layer, *1/255)\r\n4. convert BGR to RGB format\r\n```\r\nFinally, I call model.evaluate() to do evaluation. But I can only get the accuracy:\r\n>  sparse_categorical_accuracy: 0.7208 - sparse_top_k_categorical_accuracy: 0.9122\r\n\r\nwhich is 3% less than reported 75%. Could you help me some advice on how the get the right accuracy? \r\n", "comments": ["@Kewenjing1020 \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n", "@Saduf2019 Here's my validation code:\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.keras.optimizers import RMSprop\r\nfrom ipdb import set_trace\r\nimport numpy as np\r\nimport cv2\r\n\r\nmodel = tf.keras.applications.MobileNetV3Large(alpha=1.0, minimalistic=False, weights='imagenet')\r\ni = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)\r\nx = tf.cast(i, tf.float32)\r\nx = model(x)\r\nmy_model = tf.keras.Model(inputs=[i], outputs=[x])\r\n\r\neval_image_list='/group/modelzoo/test_dataset/Imagenet/val.txt'\r\neval_image_path='/group/modelzoo/test_dataset/Imagenet/val_dataset'\r\n\r\nwith open(eval_image_list, 'r') as fr:\r\n    lines = fr.readlines()\r\n\r\ndef inception_preprocess(img_path, central_fraction=0.875,\r\n                                     central_crop=True):\r\n    image = tf.io.read_file(img_path)\r\n    image = tf.image.decode_jpeg(image, channels=3)\r\n    image = tf.cast(image,tf.float32)\r\n    if central_crop and central_fraction:\r\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\r\n\r\n    image = tf.expand_dims(image, 0)\r\n    image = tf.compat.v1.image.resize_bilinear(image,[224,224])\r\n    image = tf.subtract(image, 127.5)\r\n    image = tf.multiply(image, 2.0)\r\n    return image\r\n\r\n\r\nopt = RMSprop(learning_rate=1e-5)\r\nloss = tf.losses.SparseCategoricalCrossentropy()\r\nmetric_top_5 = keras.metrics.SparseTopKCategoricalAccuracy()\r\naccuracy = keras.metrics.SparseCategoricalAccuracy()\r\nmy_model.compile(optimizer=opt, loss=loss, metrics=[accuracy, metric_top_5])\r\n\r\nbatch = 1000\r\ntop1_accs = []\r\ntop5_accs = []\r\nfor n in range(len(lines)//batch+1):\r\n    imagelist = []\r\n    labels = []\r\n    for k,line in enumerate(lines[n*batch:min((n+1)*batch,len(lines))]):\r\n        img_path, label = lines[k].strip().split(\" \")\r\n        img_path = os.path.join(eval_image_path, img_path)\r\n        label = np.array([label], dtype=np.int64)\r\n        image = inception_preprocess(img_path)\r\n        imagelist.append(image)\r\n        labels.append(label)\r\n\r\n    _,top1,top5= my_model.evaluate(keras.layers.Concatenate(0)(imagelist),np.array(labels))\r\n    top1_accs.append(top1*len(labels))\r\n    top5_accs.append(top5*len(labels))\r\n\r\nprint('top1:',np.array(top1_accs).sum()/len(lines))\r\nprint('top5:',np.array(top5_accs).sum()/len(lines))\r\n\r\n```\r\nMy environment is : tensorflow==2.4.0, python=3.8.2, cuda-version 11.2.\r\nWith this code, I can only achieve top5 ~50%. ", "i ran the above code and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/926bf6ea52dcf1cd8287e3e018655c86/untitled579.ipynb), please share code such that we can replicate the error reported or share a colab gist with the error.", "@Saduf2019 The error happens because you don't install ipdb. Just install ipdb by 'pip install ipdb'. Or, remove 'from ipdb import set_trace'.\r\nBesides, in this code, it calls the Imagenet dataset by giving the datapath and vallist.  You need to change the 'eval_image_list' and 'eval_image_path'. The eval_image_list file is defined as (image name+blank space+label id):\r\n```\r\nILSVRC2012_val_00000001.JPEG 65\r\nILSVRC2012_val_00000002.JPEG 970\r\nILSVRC2012_val_00000003.JPEG 230\r\nILSVRC2012_val_00000004.JPEG 809\r\nILSVRC2012_val_00000005.JPEG 516\r\nILSVRC2012_val_00000006.JPEG 57\r\n...\r\n```\r\n", "@Kewenjing1020 \r\nPlease share all dependencies for us to replicate the issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5c7073a8b1586100ef96a27ebf36a82e/untitled586.ipynb) or if possible share a colab gist with the issue reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thanks for reporting the issue. I wasn't able to access the data file you are using in the colab.\r\n\r\nHaving said that, the mobilenet v3 include the preprocessing logic in the model and was expecting the input data to be ranged from [0, 255]. Since I can't access your data file, I can't tell whether you logic in inception_preprocess is correct (it seems to make the data ranged from [-255, 255], assuming the original data is from [0, 255]). Could u update that and test again? \r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/python/keras/applications/mobilenet_v3.py#L143", "Thanks for reporting the issue. Please https://github.com/tensorflow/tensorflow/pull/48542#issuecomment-824258134 for more updates. #48542 should fix the issue."]}, {"number": 48065, "title": "[ClusterSpec] Allow sparse representation of TF_CONFIG.", "body": "For asynchronous parallel training with PS and Worker, `tf.train.ClusterSpec` propagation is usaully enabled for better fault-tolerance and worker-scalability. In this case:\r\n- Each PS server is started without others' addresses like:\r\n```\r\n{\r\n  \"cluster\": {\r\n    \"local\": {\"0\": \"localhost:2222\"}\r\n  },\r\n  \"task\": {\r\n    \"type\": \"local\",\r\n    \"index\": 0\r\n  }\r\n}\r\n```\r\n- Each Worker session is started with a sparse `TF_CONFIG` containing all PS IP-port pairs and its own address like:\r\n```json\r\n{\r\n  \"cluster\": {\r\n    \"ps\": [\"ps0:2222\", \"ps1:2222\"],\r\n    \"worker\": {\"1\": \"worker1:2222\"}\r\n  },\r\n  \"task\": {\r\n    \"type\": \"worker\",\r\n    \"index\": 1\r\n  }\r\n}\r\n```\r\n\r\nHowever, in Json, the type of dictionary key is always `string`. Thus, task index should be casted to integer for compatibility with `tf.train.ClusterSpec`. Otherwise, an error comes out:\r\n```text\r\n>>> cluster_resolver.cluster_spec()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/data00/home/shishaochen/.venvs/dev-py2/local/lib/python2.7/site-packages/tensorflow_core/python/distribute/cluster_resolver/tfconfig_cluster_resolver.py\", line 138, in cluster_spec\r\n    return ClusterSpec(tf_config['cluster'])\r\n  File \"/data00/home/shishaochen/.venvs/dev-py2/local/lib/python2.7/site-packages/tensorflow_core/python/training/server_lib.py\", line 295, in __init__\r\n    self._make_cluster_def()\r\n  File \"/data00/home/shishaochen/.venvs/dev-py2/local/lib/python2.7/site-packages/tensorflow_core/python/training/server_lib.py\", line 490, in _make_cluster_def\r\n    job_def.tasks[i] = task_address\r\nTypeError: u'1' has type unicode, but expected one of: int, long\r\n```\r\n\r\nTo keep compatibilty with both `tf.distribute.cluster_resolver.TFConfigClusterResolver` and `tf.estimator.RunConfig`, we'd better push down type casting to constructor of `tf.train.ClusterSpec`.\r\n\r\nThe following code snippets tell how they directly consume the parsed `TF_CONFIG` of Json.\r\n- https://github.com/tensorflow/estimator/blob/v2.4.0/tensorflow_estimator/python/estimator/run_config.py#L647\r\n```python\r\nself._cluster_spec = tf.train.ClusterSpec(tf_config.get(_CLUSTER_KEY, {}))\r\n```\r\n- https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/distribute/cluster_resolver/tfconfig_cluster_resolver.py#L162\r\n```python\r\nreturn ClusterSpec(tf_config['cluster'])\r\n```\r\n\r\nCollaborated with @wangcaihua.", "comments": ["@frankchn What's the error in \"import/copybara\" check? There is no hint or log for me to fix the error. Thanks a lot.", "@gbaned for more info - I don't have access to the copybara panel, but it usually means something isn't conforming to the Google python style guide."]}, {"number": 48064, "title": "Build failure: Undefined symbol error. ", "body": "**System information**\r\n- Linux Ubuntu 18.04:\r\n- TensorFlow installed from source from the main\r\n- TensorFlow version: 2.3, 2.4, 2.5\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version: 3.7.2 (installed using bazelisk release)\r\n- GCC/Compiler version: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n\r\n**Describe the problem**\r\nI'm trying to build a CPU optimized Tensorflow version. I've been trying different flags in the building process to test performance for weeks. I was able to build yesterday, but today it started to give an undefined symbol error and failed in build. I tried tensorflow with branches r2.3, r2.4 and the main branch, they all failed. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n./configure\r\nbazel build --config=mkl --config=noaws --config=nogcp --config=nohdfs //tensorflow/tools/lib_package:libtensorflow --verbose_failures\r\n\r\nNote that I also tried removing the options from the last bazel command such as, \r\nbazel build //tensorflow/tools/lib_package:libtensorflow --verbose_failures\r\nand\r\nbazel build --config=opt //tensorflow/tools/lib_package:libtensorflow --verbose_failures\r\nThey all failed. \r\n\r\n**configure output**\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /home/dogac/anaconda3/envs/MnM/bin/python3]: \r\n\r\nFound possible Python library paths:\r\n  /home/dogac/anaconda3/envs/MnM/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/dogac/anaconda3/envs/MnM/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: \r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: \r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=mkl_aarch64 \t# Build with oneDNN support for Aarch64.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\n\r\n**bazel output and error**\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=179\r\nINFO: Reading rc options for 'build' from /home/dogac/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/dogac/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/dogac/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/dogac/anaconda3/envs/MnM/bin/python3 --action_env PYTHON_LIB_PATH=/home/dogac/anaconda3/envs/MnM/lib/python3.6/site-packages --python_path=/home/dogac/anaconda3/envs/MnM/bin/python3\r\nINFO: Found applicable config definition build:short_logs in file /home/dogac/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/dogac/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:mkl in file /home/dogac/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt\r\nINFO: Found applicable config definition build:noaws in file /home/dogac/tensorflow/.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file /home/dogac/tensorflow/.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file /home/dogac/tensorflow/.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:linux in file /home/dogac/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/dogac/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/tools/lib_package:libtensorflow (218 packages loaded, 20353 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/dogac/tensorflow/tensorflow/compiler/tf2xla/cc/BUILD:31:21: Executing genrule //tensorflow/compiler/tf2xla/cc:xla_jit_op_gen_genrule failed (Exit 127): bash failed: error executing command \r\n  (cd /home/dogac/.cache/bazel/_bazel_dogac/e57be43ab57df3f949d2ecc82ad15737/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/lib \\\r\n    PATH=/home/dogac/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/home/dogac/bin:/home/dogac/.local/bin:/home/dogac/anaconda3/envs/MnM/bin:/home/dogac/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/bin:/home/dogac/.go/bin \\\r\n    PYTHON_BIN_PATH=/home/dogac/anaconda3/envs/MnM/bin/python3 \\\r\n    PYTHON_LIB_PATH=/home/dogac/anaconda3/envs/MnM/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops_gen_cc bazel-out/k8-opt/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops.h bazel-out/k8-opt/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops.cc 1 ,')\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/host/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops_gen_cc: symbol lookup error: bazel-out/host/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops_gen_cc: undefined symbol: _ZN4absl14lts_2020_09_239ByAnyCharC1ENS0_11string_viewE\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nINFO: Elapsed time: 688.756s, Critical Path: 228.18s\r\nINFO: 5362 processes: 180 internal, 5182 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@dogacbasaran,\r\nAs per the [tested build configurations](https://www.tensorflow.org/install/source#cpu), could you please check if you are facing the same issue with **GCC 7.3.1** as well?\r\n\r\nVersion | Python version | Compiler | Build tools\r\n-- | -- | -- | --\r\ntensorflow-2.4.0 | 3.6-3.8 | GCC 7.3.1 | Bazel 3.1.0\r\ntensorflow-2.3.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 3.1.0\r\ntensorflow-2.2.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 2.0.0\r\n\r\nAlso, please follow the official [build from source](https://www.tensorflow.org/install/source#linux) guide while building TensorFlow. Thanks!", "@amahendrakar I'm having trouble installing GCC 7.3.1 specifically. I downgraded my Bazel to 3.1.0, tried with GCC 7.5.0 it still failed the same way. Could you please direct me on how to install that specific version please? Thanks.", "@amahendrakar I was able to build TF from the source. I copied my previous built /lib and /include directories into /usr/local before I failed in building. When I deleted them from the /usr/local, I was able to build again. I don't know why it works like that but that is ok for my case anyway. Thanks for your help. Cheers.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48064\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48064\">No</a>\n"]}, {"number": 48063, "title": "Add _FusedBatchNormGrad to support side_input and activation", "body": "This PR adds CUDNN support of fused_batch_norm_grad + <side_input_backprop> + activation_backprop.\r\n\r\nThere are two parts of the PR:\r\n\r\n1. Add a new op _FusedBatchNormGrad\r\n2. Enable the fusion via remapper\r\n\r\n@nluehr ", "comments": ["@sanjoy Can you help review this PR or assign it appropriately? Thanks.", "@sanjoy Any update on this PR?", "@ezhulenev Do the grappler changes look good to you?", "The stream_executor and XLA changes look good.", "@sanjoy Do we know why this PR and https://github.com/tensorflow/tensorflow/pull/48893 are reverted?"]}, {"number": 48062, "title": "Tensorflow probability +tensorflow 2.4 and tf.distribute.MirroredStrategy() | error: Not JSON Serializable:', MirroredVariable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 11.0 ( Cuda compilation tools, release 11.2, V11.2.152, Build cuda_11.2.r11.2/compiler.29618528_0)\r\n- GPU model and memory: , 12Gb\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nPoint 1:\r\nWARNING:tensorflow:Model failed to serialize as JSON. Ignoring... ('Not JSON Serializable:', MirroredVariable:{\r\n  0: <tf.Variable 'conv2d_flipout/kl_loss_weight:0' shape=() dtype=float32, numpy=0.0>\r\n\r\nPoint 2:\r\nError details shared below. \r\n\r\n\r\n**Describe the expected behavior**\r\nTensorflow probability +tensorflow 2.4 and tf.distribute.MirroredStrategy() should work without any problem\r\n\r\n**Standalone code to reproduce the issue**\r\n_Please confirm if the combination of Tensorflow probability +tensorflow 2.4 and tf.distribute.MirroredStrategy() and share the key points to be taken care of for this case_. Since the code is big, If I share the code, the question will be to share a small code to reproduce!\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n`File \"............_vevn/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1145, in fit\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"............_vevn/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 432, in on_epoch_end\r\n    callback.on_epoch_end(epoch, numpy_logs)\r\n  File \"/home/rr/Sensor_fusion_ws/RR_SF_net_tf2_ws_Good_CRL_vgg_21Mar2021_res_trial/rrsfnet/../rrsfnet/callbacks/common.py\", line 31, in on_epoch_end\r\n    self.callback.on_epoch_end(epoch, logs=logs)\r\n  File \"...................._vevn/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1344, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"......................._vevn/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1396, in _save_model\r\n    self.model.save(filepath, overwrite=True, options=self._options)\r\n  File \"......................._vevn/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2002, in save\r\n    signatures, options, save_traces)\r\n  File \"........................_vevn/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 154, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"............................._vevn/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 119, in save_model_to_hdf5\r\n    v, default=json_utils.get_json_type).encode('utf8')\r\n  File \"/usr/lib/python3.6/json/__init__.py\", line 238, in dumps\r\n    **kw).encode(obj)\r\n  File \"/usr/lib/python3.6/json/encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python3.6/json/encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"..............................._vevn/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/json_utils.py\", line 134, in get_json_type\r\n    raise TypeError('Not JSON Serializable:', obj)\r\nTypeError: ('Not JSON Serializable:', MirroredVariable:{\r\n  0: <tf.Variable 'conv2d_flipout/kl_loss_weight:0' shape=() dtype=float32, numpy=0.0>\r\n})`\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48062\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48062\">No</a>\n", "."]}, {"number": 48061, "title": "TF-TRT Add Erf unary op converter", "body": "Add conversion for Erf op.\r\n\r\nTracker: #45481 \r\nTagging @bixia1 for review and @DEKHTIARJonathan ", "comments": ["I am able to fix the test by replacing erf with 1-erfc, instead of replacing erf with erfc."]}, {"number": 48060, "title": "TF-TRT Prefer static shapes", "body": "This PR improves TF-TRT conversion when the \"Optimal\" profile generation strategy is used with only a single input shape. \r\n\r\nIn that case the TRT engine would have a single profile with min=opt=max. This means that the engine should only handle the concrete input dimension defined by the single profile. Therefore, we can define the TRT network with static shapes. This increases conversion rate, because some of our converters are not able to convert dynamic shape input.\r\n\r\nThis PR updates `TRTEngineOp` constructor to retrieve the `profile_strategy` graph attribute, and pass it to `TrtShapeOptimizationProfiles` during the `InitProfiles` call. Additionally an `IsStaticCompatible` method is introduced to `TrtShapeOptimizationProfiles`, and is used in `TRTEngineOp::BuildEngine` to decide whether to build a dynamic or a static engine. \r\n\r\nTracker #45481\r\nTagging @bixia1 for review and @DEKHTIARJonathan", "comments": ["I have updated the lowercase name handling and fixed one error in trt_engine_op_test."]}, {"number": 48059, "title": "incorrect number of nodes in graph - TFLite C++", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5\r\n- TensorFlow installed from (source or binary): Source. Using Bazel and with [libcoral C++](https://github.com/google-coral/libcoral)\r\n- Tensorflow version (commit SHA if source): installed from Bazel\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Coral edge TPU (USB accelerator)\r\n\r\n**Describe the problem**\r\nI'm using TF lite C++ working with coral edge TPU. I want to output intermediate results of the graph, so I use `lldb` debugger to examine the `interpreter.cc` and `subgraph.cc` file during `Invoke()`.\r\n\r\nAlthough I'm running a model `mobilenet_v1_1.0_224_quant_edgetpu.tflite`, the command `primary_subgraph().nodes_size()` only gives a value of 2, meaning that there are only two nodes in the graph. Also, input and output sizes (and `tensors_size`) show the same. From my understanding, the number of nodes should be similar to the number of operations in the model (i.e. the mobilenet). Is there anything I missed or misunderstood? Could you please help to point it to me?\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n1- set `COMPILATION_MODE ?= dbg`, and compile [libcoral C++](https://github.com/google-coral/libcoral) with bazel. `make CPU=k8 examples`\r\n2- debug classify_image example with `lldb .out/k8-debug/classify_image`\r\n3- set breakpoint `b interpreter.cc:180`\r\n4- run `p primary_subgraph().nodes_size()` which then gives a `2`\r\n", "comments": ["It might be an intended behavior since operators, that are delegated to Coral Edge TPU, will be replaced into few partitions. Each partition consumes only one node.", "Thanks. Seems you're right. \r\n\r\nI see the coral team mentioned it on their [compiling page](https://coral.ai/docs/edgetpu/models-intro/#compiling). \r\n`The compiler creates a single custom op for all Edge TPU compatible ops`\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48059\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48059\">No</a>\n"]}, {"number": 48055, "title": "Merge pull request #1 from tensorflow/master", "body": "merge 20210319", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48055) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 48054, "title": "Merge pull request #1 from tensorflow/master", "body": "merge 20210325", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48054) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 48053, "title": "[TFLM] Platform specific system_setup.cc & micro_time.cc for CEVA-DSP cores", "body": "Continuation of https://github.com/tensorflow/tensorflow/pull/47960\r\nMoved all functionality out of micro_time.cc into system_setup.cc\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> Can you resolve the conflicts?\r\n\r\nShould be ok now I think :)"]}, {"number": 48052, "title": "[TFLM] TANH kernel refactoring", "body": "In preparation for adding support for  an optimized version of TANH for CEVA-DSP cores, refactoring and separation of tanh into tanh.cc and tanh_common.cc to allow for reference fallback from the optimized kernel.\r\n\r\n@advaitjain I haven't removed uint8 support yet since that's pretty much all the tests are for, so I'm assuming its in use here? Let me know what you think.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@yair-ehrenwald  Can you please check @advaitjain's comments and keep us posted ? Thanks!", "@yair-ehrenwald Any update on this PR? Please. Thanks!", "With TFLM moving to its own GitHub repository, we are not going to be merging any TFLM specific pull requests in the TensorFlow repository starting today.\r\n\r\nI am closing the current PR but please feel free to open a new PR in https://github.com/tensorflow/tflite-micro/.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/micro/c/W4DACgjPmOE\r\n"]}, {"number": 48050, "title": "Temporarily delete Resize lowering ops for TOSA", "body": "Newer TOSA op in LLVM combined with updated legalizations to follow.\r\n\r\nSigned-off-by: Suraj Sudhir <suraj.sudhir@arm.com>", "comments": ["@stellaraccident and @rsuderman , while creating the LLVM side update, I realized I'd need to temporarily suppress the resize legalizations too. I already have patches ready for both LLVM and TF to follow up once this and https://github.com/tensorflow/tensorflow/pull/47984 are accepted. "]}, {"number": 48049, "title": "[TFTRT] Add Dynamic Shape Testing and fix Explicit Batch Mode for ConvertUnpack", "body": "Fixed ConvertSplitHelper to make ConvertUnpack work for explicit batch mode.\r\nAdd dynamic shape testing for ConvertUnpack.\r\n\r\n@bixia1 @tfeher for review\r\n\r\nFeature Tracker: #45481", "comments": ["> I fixed the test.\r\n\r\nThanks Bixia"]}, {"number": 48048, "title": "[tf.data] graduate experimental `get_single_element` API to tf.data", "body": "UPDATED: This PR graduates the `tf.data.experimental.get_single_element` API into `tf.data.get_single_element` by making the following changes:\r\n\r\n - [x] Adds the deprecation decorator for the experimental API.\r\n - [x] Adds the necessary `group_by_window` method to `DatasetV2` class.\r\n - [x] Updates example in documentation with new API.\r\n - [x] Regenerate golden API's.\r\n - [x] Moved and updated the `get_single_element_test` test target from experimental/kernel_tests to kernel_tests\r\n\r\nTEST LOG\r\n```\r\nINFO: Build completed successfully, 343 total actions\r\n//tensorflow/python/data/kernel_tests:get_single_element_test            PASSED in 3.7s\r\n\r\n```\r\ncc: @jsimsa ", "comments": ["Thanks @kvignesh1420. I think we will have to decouple this PR into multiple separate steps because we cannot really remove the `tf.data.experimental.get_single_element` method without breaking a lot of users, which should be avoided.\r\n\r\nMy suggestion would be for this PR do the following:\r\n1) Add \"data.get_single_element\" into the `tf_export` decorator of the current implementation and deprecate the experimental endpoint ([example](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/data/ops/optional_ops.py#L35-L36))\r\n2) Update the documentation.\r\n\r\nIn particular, you should not try to remove the \"tf.data.experimental.get_single_element\" endpoint or move the current source / test code location. Because is strictly speaking not necessary to graduate the API to non-experimental and could cause churn and breakages.", "@jsimsa thanks for the info. I have made the changes to deprecate the experimental API and expose it in `tf.data`.\r\nAlso, please let me know your opinion on the documentation. I tried to keep the code sample consistent between examples so that it's easy to understand. Thanks!", "@jsimsa thanks for the references, I was able to document a concise and fully working `tf.keras` based example. I kept the estimator based example as well just in case someone wants to use it (however, I would like to remove the example as it is not straightforward). Also, the lack of proper documentation around `tf.estimator.export.ServingInputReceiver` made it a bit difficult to document the details. Please check and let me know.", "@jsimsa done!", "@jsimsa the .pbtxt files have been updated!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48048) for more info**.\n\n<!-- need_author_cla -->", "@jsimsa the `get_single_element` API is now a part of `tf.data.Dataset`. Also, the docstrings have been modified as per the latest and deprecated APIs. If the docstring of `get_single_element` API  `Dataset` needs to be simplified, I can do that. Please take a look.", "Could we also move `tensorflow/python/data/experimental/kernel_tests/get_single_element_test.py` to `tensorflow/python/data/kernel_tests/get_single_element_test.py` and update it so that it uses the non-experimental API as opposed to the now deprecated experimental API? Thanks.", "@jsimsa done! Updated the PR description as well.", "One last change. Please update https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md adding a \"tf.data\" subsection under the \"Bug Fixes and Other Changes\" section of TF 2.6.0 (next to the existing \"TF Core\" section) and add the following item there: \"promoting `tf.data.experimental.get_single_element` API to `tf.data.Dataset.get_single_element` and deprecating the experimental endpoint\"", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48048) for more info**.\n\n<!-- need_author_cla -->", "@jsimsa the `RELEASE.md` file has been updated. Can we merge this before https://github.com/tensorflow/tensorflow/pull/48217 so that I can rebase and make necessary changes?", "@jsimsa any pending changes to be addressed?", "no, I am waiting for your change to RELEASE.md to be propagated to the internal copy of your PR (it has not been as of this morning) ... if it is still stuck this PM, I will fix things manually", "ok, thanks!", "@kvignesh1420 Can you please resolve conflicts? Thanks!", "@gbaned done!\r\ncc: @jsimsa ", "@kvignesh1420 Can you please resolve conflicts? Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48048) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48048) for more info**.\n\n<!-- need_author_cla -->", "@jsimsa can we get this merged so that https://github.com/tensorflow/tensorflow/pull/48217 can be updated with the release notes. Please let me know.", "FYI, I have manually fixed outstanding issues and merged your change, so this PR can be closed."]}, {"number": 48047, "title": "only two nodes in graph - TFLite C++", "body": "Hi Team,\r\n\r\nI'm using TF lite C++ working with coral edge TPU. I want to output intermediate results of the graph, so I use `lldb` debugger to examine the `interpreter.cc` and `subgraph.cc` file during `Invoke()`.\r\n\r\nAlthough I'm running a model `mobilenet_v1_1.0_224_quant_edgetpu.tflite`, the command `primary_subgraph().nodes_size()` only gives a value of 2, meaning that there are only two nodes in the graph. Also, input and output sizes show the same. From my understanding, the number of nodes should be similar to the number of operations in the model. Is there anything I missed or misunderstood? Could you please help to point it to me?\r\n\r\nThank you in advance!\r\nBest,\r\nFan", "comments": []}, {"number": 48046, "title": "issue: tflite size same as frozen_inference graph and not working on android app.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution =linux\r\n- TensorFlow installation : TF 2.4.1\r\n\r\n\r\n### 2. Code\r\n\r\nimport tensorflow as tf\r\nimport os \r\n\r\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\r\n\r\n\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\"frozen_inference_graph.pb\", [\"image_tensor\"], [\"detection_classes\", \"detection_scores\", \"detection_boxes\", \"num_detections\"])\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nmodel = converter.convert()\r\n\r\ntflite_model_name=\"TF.tflite\"\r\nopen(tflite_model_name,\"wb\").write(model)\r\n\r\n### 3. Failure after conversion\r\nI am getting the tflite model size same as my inference graph of faster rcnn inception v2.pet model which is 54MB ...and when I am trying to use the same tflite model on my android app ,it doesn't run....", "comments": []}, {"number": 48045, "title": "Remove the redundant #include \"tensorflow/c/tf_tensor.h\" in tf_tstring.cc", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48045) for more info**.\n\n<!-- need_sender_cla -->", " @googlebot I signed it!\n\nOn Wed, Mar 24, 2021 at 10:57 AM google-cla[bot] ***@***.***>\nwrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here with @googlebot\n> I signed it! and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48045>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/48045#issuecomment-805889567>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABISKGFMIHIIYBPSO62LAELTFH4T3ANCNFSM4ZXN3IZQ>\n> .\n>\n"]}, {"number": 48044, "title": "[Tensorflow Lite] GPU delegate problem, SSD-mobilenetV2 converted from object detection API", "body": "\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: i used object detection demo app, some change for gpu delegate\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 64 bit\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: emulator : Google Pixel 4  (android 10.0 / 11.0)\r\n    and real device : Samsung Galuxy Note 10+ / Android 11.0\r\n-   **TensorFlow installed from (source or binary)**: Source\r\n-   **TensorFlow version (use command below)**: 2.4.1\r\n-   **Python version**: 3.6\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:11.0 / 8.0\r\n-   **GPU model and memory**: Nvidia 1070 8gb, 32gb\r\n-   **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI use custom model from Object Detection API Model Zoo, its SSD-Moblienet-v2-fpnlite-640x640.\r\nmy trained model run Object detection demo app successfully, but i want to run demo app on GPU with my trained model.\r\n\r\nSo, I change some demo app code, and run it on real device.\r\nwith some error message, the application is shutdown.\r\n\r\nI Just wonder PostProcessing operation, in my model is not supported?\r\nor something other problem?\r\n\r\n### Source code / logs\r\ndemo app : https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\n\r\nsome code change, lib_interpreter/TFLiteObjectDetecionAPIModel.java\r\nlike this\r\n```\r\n    CompatibilityList compatList = new CompatibilityList();\r\n    try {\r\n      Interpreter.Options options = new Interpreter.Options();\r\n      if (compatList.isDelegateSupportedOnThisDevice()){\r\n        System.out.println(\"This walking on GPU!\");\r\n        GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();\r\n        GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);\r\n        options.addDelegate(gpuDelegate);\r\n      } else {\r\n        System.out.println(\"This walking on CPU!\");\r\n      }\r\n\r\n      options.setNumThreads(NUM_THREADS);\r\n      options.setUseXNNPACK(true);\r\n      d.tfLite = new Interpreter(modelFile, options);\r\n      d.tfLiteModel = modelFile;\r\n      d.tfLiteOptions = options;\r\n    } catch (Exception e) {\r\n      throw new RuntimeException(e);\r\n    }\r\n```\r\n\r\nerror logs\r\n```\r\nE/libEGL: call to OpenGL ES API with no current context (logged once per thread)\r\nI/System.out: This walking on GPU!\r\nI/tflite: Created TensorFlow Lite delegate for GPU.\r\nI/tflite: Initialized TensorFlow Lite runtime.\r\nI/tflite: Created 0 GPU delegate kernels.\r\nD/AndroidRuntime: Shutting down VM\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.tensorflow.lite.examples.detection, PID: 12798\r\n    java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Following operations are not supported by GPU delegate:\r\n    CUSTOM TFLite_Detection_PostProcess: TFLite_Detection_PostProcess\r\n    154 operations will run on the GPU, and the remaining 1 operations will run on the CPU.\r\n    TfLiteGpuDelegate Init: PACK: Tensor \"tfl.pack\" has bad input dims size: 5.\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 155 (TfLiteGpuDelegateV2) failed to prepare.\r\n    \r\n    Restored original execution plan after delegate application failure.\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:162)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:168)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity$7.onPreviewSizeChosen(CameraActivity.java:453)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:357)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:362)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.access$300(CameraConnectionFragment.java:66)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment$3.onSurfaceTextureAvailable(CameraConnectionFragment.java:171)\r\n        at android.view.TextureView.getTextureLayer(TextureView.java:402)\r\nand more...\r\n```\r\n\r\nSearching very hard, but I can't found solution.", "comments": ["@impjdi could you take a look?", "never knew we support `PACK`.  the reason why it failed is stated:\r\n\r\n> TfLiteGpuDelegate Init: PACK: Tensor \"tfl.pack\" has bad input dims size: 5.\r\n\r\nIIRC we don't support 5D tensors in OpenGL.  We might support 5D tensors in OpenCL for some operations.", "thank you for answering,\r\nso I can change OpenGL to OpenCL manually?\r\n\r\n\r\n", "Is there any solution? I am facing the same issue using `org.tensorflow:tensorflow-lite:2.4.0` or `org.tensorflow:tensorflow-lite:2.5.0`, 2.3.0 is working fine.", "@anywhere133 Sorry for the belated reply, must have missed the email.  There are a couple of prerequisites: (a) your phone needs to have OpenCL installed, and (b) you want to set the experimental flag that enforce OpenCL unconditionally.  I am lazy and just comment out the InitializeOpenGlApi (or whatever it's called) In the delegate.cc.\r\n\r\n@pautho The download of the model fails: http://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip  so I'm not sure what model you are working with.  If the tflite file is the same, but only works in 2.3 but not in 2.4, there's gotta be some regression which must be identified and fixed.  If it's the converter that broke and produces a tflite file that is no more compatible with the GPU delegate, we might have to assign it to the tflite converter people.  Can you identify why it stopped working?", "@anywhere133,\r\n\r\nCan you take a look at the above comment by @impjdi and let us know if it helps in resolving your issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48044\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48044\">No</a>\n"]}, {"number": 48042, "title": "Update README.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48042) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48042) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\r\n\r\n"]}, {"number": 48041, "title": "Failed to create cuSolverDN instance on GPU with 4GB", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 11.0 / 8.0.4.30-1+cuda11.0\r\n- GPU model and memory: RTX 1650 / T2000 with 4GB\r\n\r\n**Describe the current behavior**\r\n\r\nThe following linear algebra operations:\r\n- tf.linalg.eigh\r\n- tf.linalg.eigvalsh\r\n- tf.linalg.expm\r\n\r\nwhen applied to a tensor with tf.complex* type in a RTX1650/T2000 GPUs with 4GB produce:\r\n```bash\r\nF tensorflow/core/util/cuda_solvers.cc:115] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.\r\nAborted (core dumped)\r\n```\r\nThis issue is not visible when setting a GPU memory limit or `set_memory_growth`.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo crash.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nrho = tf.constant([[ 9.14909492+1.14491749e-16j,  0.27933987-1.27970271e-01j, \r\n    0.25616772+1.12442776e-01j, -0.05876706-3.76165554e-01j],\r\n    [ 0.27933987+1.27970271e-01j,  9.62506895+2.77555756e-17j,\r\n    -0.19640032-1.72149754e-01j,  0.28515435+2.58716412e-01j],\r\n    [ 0.25616772-1.12442776e-01j, -0.19640032+1.72149754e-01j,\r\n    9.13383471-1.38777878e-17j,  0.18600413-2.68212068e-01j],\r\n    [-0.05876706+3.76165554e-01j,  0.28515435-2.58716412e-01j,\r\n    0.18600413+2.68212068e-01j,  9.58168325-5.20417043e-17j]])\r\ntf.linalg.eigvalsh(rho)\r\n```", "comments": ["@scarrazza,\r\nI was able to run the code on Google Colab without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3c77d7241788a2ad7753b3015386abc7/48041.ipynb).\r\n\r\n> This issue is not visible when setting a GPU memory limit or `set_memory_growth`.\r\n\r\nBy default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process. You can read more about it [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth). \r\n\r\nIn this case, looks like the error was caused due to the GPU running out of memory. And as you've already mentioned the workaround would be to set a limit on the GPU memory growth. Also check similar issue [#22105](https://github.com/tensorflow/tensorflow/issues/22105) for reference.\r\n\r\nThanks!", "@amahendrakar thanks for testing. \r\n\r\nHowever the GPUs on colab have 15gb of memory (they are usually T4). The issue I report is reproducible with low specs cards such as RTX 1650 and T2000 with 4GB of ram. I can confirm that this issue happens even if the GPU memory has 3.5GB of free memory.", "@nluehr Does this mean `MinSystemMemory` needs some more adjustment, like in https://github.com/tensorflow/tensorflow/pull/44810?", "The required MinSystemMemory (GPU memory not claimed by TF allocator) can be controlled at runtime using the environment variable TF_DEVICE_MIN_SYS_MEMORY_IN_MB. @scarrazza can you check what minimum value of TF_DEVICE_MIN_SYS_MEMORY_IN_MB is needed to allow your model to run? As a starting point, I believe the default value TF selects for your GPU is 1050.", "@nluehr thanks for the feedback. I will try to reproduce this issue, however so far with the latest 2.5.0 with CUDA 11.2, I am not able to reproduce the crash anymore.", "I encountered a similar error when calling `tf.linalg.solve` after loading and executing a model. However, if I call `tf.linalg.solve` before loading and executing the model, the error does not occur when I call `tf.linalg.solve` again after loading and executing the model. `set_memory_growth` does not appear to fix the problem.\r\n\r\nIf I were to guess, it's probably that model inference used up all graphic memory but does not release the memory when inference is finished, as the error did not happen when I tried to reproduce it with a small model.", "@scarrazza,\r\n\r\nAs per this [comment](https://github.com/tensorflow/tensorflow/issues/48041#issuecomment-845419240), you have mentioned that its not reproducible with `2.5.0`.  We recommend that you can try with latest stable version i.e `2.6.0` along with CUDA 11.2 and other configurations as per this [link](https://www.tensorflow.org/install/source#gpu) and let us know if the issue persists. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48041\">No</a>\n"]}]