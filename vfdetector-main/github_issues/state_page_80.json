[{"number": 8584, "title": "TF for Xeon Phi", "body": "Hi,\r\n\r\nDoes tensorflow support xeon phi? \r\n\r\nThanks !\r\nAfshin", "comments": ["Not yet. But we would be happy to accept this contribution into tensorflow.", "Hello. I just finished watching a webinar by Intel called, \"TensorFlow* on Modern Intel\u00ae Architectures Webinar\". It was unclear if TensorFlow currently has support for Xeon phi cards. The webinar made me assume yes but now I'm under the impression that only Knights Landing is supported (which is not available?) and the Knights Corner is not supported. I am just trying to find clear instructions on how to setup my Knights Corner Xeon Phi card to use all of its core when training a model in TensorFlow. ", "I heard from Intel in January that they had their own fork of TensorFlow which was modified to support Xeon Phi and they were working on integrating it into the main repo, maybe someone familiar with the partnernship knows the status", "That's great news. Thank you.", "Hi, \r\nPlease check the below links, \r\n\r\nhttps://itpeernetwork.intel.com/tensorflow-intel-architecture-ai/\r\nhttps://devmesh.intel.com/projects/intel-optimized-tensorflow1-0\r\n\r\nRegards,\r\nPriya\r\n", " @priyaarul \r\n\r\nHi, so are you saying, \"Yes, TensorFlow currently supports Knights Corner Xeon Phi cards.\"?", "Hey,\r\nIntel has released an article:\r\nhttps://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture\r\nhttps://software.intel.com/en-us/articles/intel-optimized-tensorflow-wheel-now-available#\r\nwhich states that it does support tf on the xeon phi, but they haven't yet provided the documentation on how are they running the benchmarks. I have installed tf1.2 on the Xeon Phi Knight's Landing, but the models run too slow. @priyaarul @afshin67 @gunan @yaroslavvb ", "@cryptox31 Did you find a way to run your models a bit faster? \r\nI'm trying to find out if my implementation for Knight's corner is incorrect or if I'm juicing it for all it's got. I should really just benchmark it against a GPU. If the Knight is slower I have my answer. ", "Greetings @cryptox31 @WorksWellWithOthers \r\nDoes the Intel Xeon Phi x100 PCI-E [Knight's corner] support Tensorflow?  Has anyone successfully executed the system on the 1st generation Xeon Phi?\r\n\r\nThanks,\r\ncoast", "On TF side we still have not tested with avx512. So nothing from our end.\r\n", "greetings @gunan \r\nPlease excuse the cross-link, but as a k1om vet, I strongly suggest that we consider implementation using https://github.com/edanor/umesimd .  This lib handles avx512 and the eol 512-bit instruction set of knights corner.  I would be more than willing to contribute, but require direction/instruction.\r\n\r\nThanks,\r\ncoast", "@WorksWellWithOthers No, I haven't found a way yet but you can follow [this](https://software.intel.com/en-us/comment/1912006#comment-1912006) post on intel.", "Hi All,please chk this-  https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture- examples shown running inference on Intel\u00ae Xeon Phi\u2122 processor 7250 (KNL)", "It's nice to have a TensorFlow wheel provided by Intel for their Xeon Phi cards. I should have tried it on a different machine. Unfortunately, after the wheel installation, my old nemesis is back with a warning that the binary was not compiled to use: SSE4.1, SSE4.2, AVX, AVX2, FMA. \r\n\r\nSource:\r\nhttps://software.intel.com/en-us/articles/intel-optimized-tensorflow-wheel-now-available#\r\n\r\n# Python 2.7\r\npip install https://anaconda.org/intel/tensorflow/1.4.0/download/tensorflow-1.4.0-cp27-cp27mu-linux_x86_64.whl\r\n \r\n# Python 3.5\r\npip install https://anaconda.org/intel/tensorflow/1.3.0/download/tensorflow-1.3.0-cp35-cp35m-linux_x86_64.whl\r\n \r\n# Python 3.6\r\npip install https://anaconda.org/intel/tensorflow/1.4.0/download/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n\r\nWarning:\r\nPython 2.7.5 (default, May  3 2017, 07:55:04) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-14)] on linux2                                                                            \r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf \r\n>>> sess = tf.Session()                                                                                                                                      \r\n2018-01-03 12:53:16.067051: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA                                                                                           \r\n\r\nThere is also a claim by someone stating that CPU with MKL is slower than just CPU support:\r\nhttps://github.com/mind/wheels/issues/12\r\n\r\nAre my times optimal or is there something I can do to increase performance?\r\n\r\nSample:\r\nsudo git clone https://github.com/tensorflow/models.git \r\npython models/tutorials/image/mnist/convolutional.py --batch_size=32\r\n\r\nStep 0 (epoch 0.00), 3.2 ms     <br />                                                                                                                                 Minibatch loss: 8.334, learning rate: 0.010000                 <br />                                                                                                  Minibatch error: 85.9%                    <br />                                                                                                                       Validation error: 84.6%                <br />                                                                                                                          Step 100 (epoch 0.12), 26.2 ms           <br />                                                                                                                        Minibatch loss: 3.250, learning rate: 0.010000      <br />                                                                                                             Minibatch error: 6.2%              <br />                                                                                                                              Validation error: 7.7%          <br />                                                                                                                                 Step 200 (epoch 0.23), 24.6 ms     <br />                                                                                                                              Minibatch loss: 3.359, learning rate: 0.010000       <br />                                                                                                            Minibatch error: 10.9%           <br />                                                                                                                                Validation error: 4.2%         <br />                                                                                                                                 Step 300 (epoch 0.35), 24.4 ms       <br />                                                                                                                            Minibatch loss: 3.139, learning rate: 0.010000             <br />                                                                                                     Minibatch error: 3.1%    <br />                                                                                                                                        Validation error: 3.0%     <br />                                                                                                                                      Step 400 (epoch 0.47), 23.8 ms   <br />                                                                                                                               Minibatch loss: 3.197, learning rate: 0.010000         <br />                                                                                                          Minibatch error: 4.7%    <br />                                                                                                                                        Validation error: 2.6%      <br />                                                                                                                                     Step 500 (epoch 0.58), 24.0 ms    <br />                                                                                                                               Minibatch loss: 3.166, learning rate: 0.010000      <br />                                                                                                             Minibatch error: 6.2%        <br />                                                                                                                                    Validation error: 2.3%     <br />\r\n  \r\n  ", "Doing the following gave me some improvement:\r\n\r\nKMP_SETTINGS=1 KMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 OMP_NUM_THREADS=72 python models/tutorials/image/mnist/convolutional.py\r\nStep 100 (epoch 0.12), 20.9 ms                   <br />                                                                                                            Minibatch loss: 3.250, learning rate: 0.010000                     <br />                                                                                                             Minibatch error: 6.2%                          <br />                                                                                                                                 Validation error: 7.7%                            <br />                                                                                                                              Step 200 (epoch 0.23), 18.7 ms                        <br />                                                                                                                          Minibatch loss: 3.359, learning rate: 0.010000                           <br />                                                                                                       Minibatch error: 10.9%                              <br />                                                                                                                            Validation error: 4.2%                             <br />                                                                                                                             Step 300 (epoch 0.35), 18.7 ms                          <br />                                                                                                                        Minibatch loss: 3.139, learning rate: 0.010000                      <br />                                                                                                            Minibatch error: 3.1%                                           <br />                                                                                                                Validation error: 3.0%                                    <br />                                                                                                                      Step 400 (epoch 0.47), 18.6 ms                          <br />                                                                                                                        Minibatch loss: 3.197, learning rate: 0.010000                        <br />                                                                                                          Minibatch error: 4.7%                                               <br />                                                                                                            Validation error: 2.6%                                           <br />                                                                                                               Step 500 (epoch 0.58), 18.8 ms                             <br />                                                                                                                     Minibatch loss: 3.166, learning rate: 0.010000                       <br />                                                                                                           Minibatch error: 6.2%                                                <br />                                                                                                           Validation error: 2.3%                                           <br />                                                                                                               Step 600 (epoch 0.70), 18.8 ms                            <br />                                                                                                                      Minibatch loss: 3.121, learning rate: 0.010000                     <br />                                                                                                             Minibatch error: 3.1%                                           <br />                                                                                                                Validation error: 2.2%                                        <br />                                                                                                                  Step 700 (epoch 0.81), 18.7 ms                         <br />                                                                                                                         Minibatch loss: 2.970, learning rate: 0.010000                   <br />                                                                                                               Minibatch error: 1.6%                           <br />                                                                                                                                Validation error: 2.1%                                 <br />                                                                                                                         Step 800 (epoch 0.93), 18.6 ms                           <br />                                                                                                                       Minibatch loss: 3.040, learning rate: 0.010000                       <br />                                                                                                           Minibatch error: 4.7%                             <br />                                                                                                                              Validation error: 2.1%                            <br />                                                                                                                              Step 900 (epoch 1.05), 19.2 ms                           <br />                                                                                                                       Minibatch loss: 2.906, learning rate: 0.009500                 <br />                                                                                                                 Minibatch error: 1.6%                             <br />                                                                                                                              Validation error: 1.8%                         <br />                                                                                                                                 Step 1000 (epoch 1.16), 19.2 ms                  <br />                                                                                                                               Minibatch loss: 2.872, learning rate: 0.009500                     <br />                                                                                                             Minibatch error: 1.6%                                     <br />                                                                                                                      Validation error: 1.8%                                      <br />    \r\n  ", "GPU benchmark claims (5ms?):\r\nhttps://stackoverflow.com/questions/35703201/speed-benchmark-for-testing-tensorflow-install\r\n\r\nCPU benchmark (my)\r\n27-29ms: TensorFlow 1.2rc \r\n18-20ms: TensorFlow 1.4 (Intel wheel)\r\n\r\nBoth have warning message involving: SSE4.1 SSE4.2 AVX AVX2 FMA ", "9ms: Geforce GTX 1060\r\n![1060](https://user-images.githubusercontent.com/13571190/34545914-2c7b2ffe-f0a5-11e7-99a3-0232a1e63a9e.png)\r\n\r\n  ", "It seems that I need MPSS to manage the Xeon and I see that Intel just released the RedHat and Suse versions. I have Debian, can I install it? \r\nI found some instructions for converting the rpm packages into deb files. Do they work? \r\nIs there any suggestion?", "@WorksWellWithOthers I use tensorflow-mkl 1.8 on KNL, but my best result is about 65ms. Do you think this is caused by TF version?", "@coldlionfanjin \r\nHi, it's been a while since I did these benchmarks and I no longer have access to the machine.\r\n\r\nFrom my previous comments it seems this might help:\r\n  KMP_SETTINGS=1 KMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 \r\n  OMP_NUM_THREADS=72 python models/tutorials/image/mnist/convolutional.py\r\n\r\nBest of luck!\r\n\r\nP.S.\r\nMost of my ML focus is now on game bots on consumer GPU cards. "]}, {"number": 8465, "title": "non_max_suppression should support batches", "body": "\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nMight be related/nice to do when overworking non_max_suppression for #7511\r\n\r\nRelated SO: http://stackoverflow.com/questions/39456554/tensorflow-tensor-with-inconsistent-dimension-size\r\n\r\n\r\n### Description\r\n\r\nMost image ops work on batches of images, so it'd make sense if tf.image.non_max_suppression worked on batches of bounding-boxes , returning batches of selected indices.\r\n\r\nAlthough I'm not completely sure how it'd work with the variable result lengths that are likely to happen in this case, perhaps masking or padding them? There's already the max_output_size parameter.\r\n\r\nAnd I'm not entirely sure how well it'd work with gather (gather_nd?), since if it doesn't work well with those, this change wouldn't be as useful\r\n", "comments": ["+1", "+1", "+1", "+1\r\nAnd could return a sparse tensor ...", "There is a new-ish op which does that: \r\ntf.image.combined_non_max_suppression", "> There is a new-ish op which does that:\r\n> tf.image.combined_non_max_suppression\r\n\r\nIt does not return indices..."]}, {"number": 8451, "title": "XLA segfaults with large graphs", "body": "TensorFlow allows graphs to be larger than 2GB, however those graphs can't be serialized.\r\n\r\nI suspect this is the cause of segfaults we've been seeing because of following line in dump_graph.cc\r\n\r\n`65    TF_CHECK_OK(WriteTextProto(Env::Default(), path, graph_def));\r\n`\r\n\r\nIt would be useful to provide an informative error message because troubleshooting this requires looking at core file:\r\n\r\n```\r\nulimit -Sc unlimited\r\ngdb python\r\ncore core\r\nbt\r\n```\r\n\r\nHere's the backtrace\r\n\r\n```\r\n#0  0x00007fc6dcdae428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007fc6dcdb002a in __GI_abort () at abort.c:89\r\n#2  0x00007fc6c04b8417 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#3  0x00007fc6bd05cb1e in tensorflow::dump_graph::DumpGraphDefToFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::GraphDef const&) ()\r\n   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#4  0x00007fc6bd05ce88 in tensorflow::dump_graph::DumpGraphToFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::Graph const&, tensorflow::FunctionLibraryDefinition const*) () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#5  0x00007fc6bcffd6c2 in tensorflow::EncapsulateSubgraphsPass::Run(tensorflow::GraphOptimizationPassOptions const&) ()\r\n   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#6  0x00007fc6bec68e00 in tensorflow::OptimizationPassRegistry::RunGrouping(tensorflow::OptimizationPassRegistry::Grouping, tensorflow::GraphOptimizationPassOptions const&) ()\r\n   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#7  0x00007fc6bec768a3 in tensorflow::SimpleGraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::SimpleClientGraph, std::default_delete<tensorflow::SimpleClientGraph> >*) () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#8  0x00007fc6be43d293 in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*) () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#9  0x00007fc6be43f4cf in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::thread::ThreadPool*, tensorflow::gtl::ArraySlice<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, tensorflow::gtl::ArraySlice<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, tensorflow::gtl::ArraySlice<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()\r\n   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#10 0x00007fc6be44085e in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#11 0x00007fc6bcfd4641 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Tensor**, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Buffer*, TF_Status*) [clone .constprop.498] ()\r\n   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#12 0x00007fc6bcfd4e58 in TF_Run () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#13 0x00007fc6bcf0b15d in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\r\n   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#14 0x00007fc6bcf0b2a3 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#15 0x00007fc6bceed225 in _wrap_TF_Run () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so\r\n#16 0x00007fc6ddd335e9 in PyCFunction_Call (func=0x7fc6c5120828, args=0x7fc68c1765f8, kwds=<optimized out>) at Objects/methodobject.c:109\r\n#17 0x00007fc6dddbabd5 in call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0dfa78) at Python/ceval.c:4705\r\n#18 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236\r\n#19 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=6, kws=0x0, kwcount=0, defs=0x0, defcount=0, \r\n    kwdefs=0x0, closure=0x7fc6d70ac780, name=0x0, qualname=0x0) at Python/ceval.c:4018\r\n#20 0x00007fc6dddbbcd8 in PyEval_EvalCodeEx (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, \r\n    defcount=0, kwdefs=0x0, closure=0x7fc6d70ac780) at Python/ceval.c:4039\r\n#21 0x00007fc6ddd11542 in function_call (func=0x7fc68feb3268, arg=0x7fc68c3903a8, kw=0x0) at Objects/funcobject.c:627\r\n#22 0x00007fc6ddcde236 in PyObject_Call (func=0x7fc68feb3268, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2165\r\n#23 0x00007fc6dddb8234 in ext_do_call (nk=-1942420568, na=0, flags=<optimized out>, pp_stack=0x7ffc0d0dfdc8, func=0x7fc68feb3268) at Python/ceval.c:5034\r\n#24 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3275\r\n#25 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=8, kws=0x7fd5098, kwcount=0, defs=0x0, defcount=0, \r\n    kwdefs=0x0, closure=0x0, name=0x7fc6d74eafb0, qualname=0x7fc6d74f07c8) at Python/ceval.c:4018\r\n#26 0x00007fc6dddbadf5 in fast_function (nk=<optimized out>, na=8, n=<optimized out>, pp_stack=0x7ffc0d0dffe8, func=0x7fc6d74add90) at Python/ceval.c:4813\r\n#27 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0dffe8) at Python/ceval.c:4730\r\n#28 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236\r\n#29 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=7, kws=0x7fd7640, kwcount=0, defs=0x0, defcount=0, \r\n    kwdefs=0x0, closure=0x0, name=0x7fc6d74f13e8, qualname=0x7fc6d74f0738) at Python/ceval.c:4018\r\n#30 0x00007fc6dddbadf5 in fast_function (nk=<optimized out>, na=7, n=<optimized out>, pp_stack=0x7ffc0d0e0208, func=0x7fc6d74add08) at Python/ceval.c:4813\r\n#31 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0e0208) at Python/ceval.c:4730\r\n#32 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236\r\n#33 0x00007fc6dddbb166 in fast_function (nk=<optimized out>, na=6, n=<optimized out>, pp_stack=0x7ffc0d0e0388, func=0x7fc6d74adc80) at Python/ceval.c:4803\r\n#34 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0e0388) at Python/ceval.c:4730\r\n#35 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236\r\n#36 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=3, kws=0x40e5488, kwcount=0, defs=0x7fc6d74a7060, \r\n    defcount=3, kwdefs=0x0, closure=0x0, name=0x7fc6dc762458, qualname=0x7fc6d74ead30) at Python/ceval.c:4018\r\n#37 0x00007fc6dddbadf5 in fast_function (nk=<optimized out>, na=3, n=<optimized out>, pp_stack=0x7ffc0d0e05a8, func=0x7fc6d74adae8) at Python/ceval.c:4813\r\n#38 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0e05a8) at Python/ceval.c:4730\r\n#39 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236\r\n#40 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=4, kws=0x41f8440, kwcount=0, defs=0x7fc6d7761df0, \r\n    defcount=1, kwdefs=0x0, closure=0x0, name=0x7fc6d77a1ee0, qualname=0x7fc6d77a1ee0) at Python/ceval.c:4018\r\n#41 0x00007fc6dddbadf5 in fast_function (nk=<optimized out>, na=4, n=<optimized out>, pp_stack=0x7ffc0d0e07c8, func=0x7fc6d76ecd90) at Python/ceval.c:4813\r\n#42 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0e07c8) at Python/ceval.c:4730\r\n#43 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236\r\n#44 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=1, kws=0x1c06428, kwcount=0, defs=0x7fc6d76f1320, \r\n    defcount=2, kwdefs=0x0, closure=0x0, name=0x7fc6dc762458, qualname=0x7fc6d772a170) at Python/ceval.c:4018\r\n#45 0x00007fc6dddbadf5 in fast_function (nk=<optimized out>, na=1, n=<optimized out>, pp_stack=0x7ffc0d0e09e8, func=0x7fc6d76e7730) at Python/ceval.c:4813\r\n#46 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0e09e8) at Python/ceval.c:4730\r\n#47 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236\r\n```", "comments": ["@tatatodd: Todd, do you know who's the right person on XLA to assign to?\r\n\r\nWe won't fix the 2GB limit, of course, but we can emit a warning at call site (using `ByteSize()`, or something like that).", "@yaroslavvb Ehh.. Same old story :) https://github.com/BVLC/caffe/issues/2006", "I'll take this issue myself.", "@tatatodd any update on this? If you're not gonna get to it, please unassign yourself and mark contribution welcome if appropriate.", "Indeed, contributions welcome!", "Does it mean we should add size warning at C++ side? Because python side already has the error message that GraphDef cannot be larger than 2GB. Is there any complexity I can't see?\r\nPing @tatatodd for the contributions welcome label."]}, {"number": 8227, "title": "[feature] Smarter Handling of Image Data Format", "body": "Right now the responsibility of choosing image data format (i.e. the representation of batches of image) is that of the data scientist (ie model writer). I suggest there should a solution in TF to move this to the optimizer (XLA perhaps?) or worst cast Op writer.\r\n\r\nFor some background:\r\n\r\nCurrently Tensorflow supports `NCHW` and `NHWC` (though other formats like [CHWN](https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155944875) might be possible down the road). Many of the Ops support both formats. That being said, the docs say:\r\n>The best practice is to build models that work with both NCHW and NHWC as it is common to train using NCHW on GPU, and then do inference with NHWC on CPU.\r\n\r\nThis requires the user to have to do some \"wrangling\" (e.g. loading the checkpoint of weights and re-buidling graph in Python) to map from one image format to another. Further this must be done with some knowledge of the platform on which the graph will be executed (ie which ops are defined, and if both, which is faster)?\r\n\r\nRight now model builder must [build the model to take in channel order and pass that around](https://github.com/tensorflow/tensorflow/issues/8137). Ideally the model could be written once with enough meta information attached to the graph to allow optimizers after the fact (ie at inference time on other platforms) to choose the best representation. Further even at training time, it would be great if the data scientist didn't need to be concerned with image data format (ie dimension ordering) and could use abstractions for accessing results that took care of data access.\r\n\r\nI don't have a clear proposal of how to clean this up, but this seems like a potential pain point, or a the very least results in people leaving performance on the table both when training and at inference. \r\n\r\n**TL; DR** Many data scientists just want to write CNNs without thinking about tensor layouts.", "comments": ["I agree.  \r\n\r\nIt is actually the case that internally XLA is free to permute the physical layout order of Tensor dimensions to improve speed.  However, when tensors flow in and out of XLA code from regular TensorFlow ops the data needs to be in row-major layout.\r\n\r\nThe visible `data_layout` fields of Conv ops, etc. are an unfortunate artifact of the NVidia cuDNN library interface, and the way it is supported in TensorFlow.  Theoretically it ought to be possible to write a TensorFlow `GraphOptimizationPass` to analyze the user model (written with canonical layouts) and make the appropriate transformations.  ", "@cancan101 @prb12 Hi! Is there someone working on this already? If not, I would like to take part in implementation of this feature (yeah, I know this is the serious issue, not like my first one #7948). Could you push me in right direction?\r\n\r\nAs I understood, the point is this:\r\n\u2022 We know that `NCHW` is better for training on GPU, but `NHWC` is better for inference on CPU\r\n\u2022 But we don't want to force user to think about these details.\r\n\u2022 User wants to build one model with one `data_format` and use it both for inference and training.\r\n\u2022 What we need to do is still run Cudnn ops with `NCHW` format and cpu ops with `NHWC`, so we want to do appropriate reshapes.\r\n\r\n@prb12 Could you please elaborate more on your idea with `GraphOptimizationPass`?", "In an ideal world, I think we'd like to not specify `data_format` for the model at all. I'd like to just tag my input batch with some data format, and have the model internally run transpose operations as needed to use the fastest data format for the given platform... but this is going to be a tradeoff between the cost of transposing and the benefit of using a more optimal data format, if certain ops only support certain data formats.", "I also had a thought about that, but I think that's the lesser issue than bothering about creating model twice for training and inference. In most cases, data is in one of these two formats, or can be reshaped in proper format on the fly outside tensorflow.\r\n\r\np.s. Maybe I am wrong and most experienced computer vision researchers/developers have other opinion.", "Inference-time models need to be different from training-time models regardless, no? You want to build them with `training` or `is_training` set to `False` to drop unnecessary stuff for batch norm, dropout, &c.\r\n\r\nI guess these can be handled in optimization passes, but at least that's not how we're handling it right now. We rebuild our graphs for inference with `training=False` before passing them through optimization.", "Hi @cancan101 ,\r\n\r\nI'm facing similar problem as you mentioned in your initial post in this thread. I've a trained checkpoint model trained using NCHW format. But I want to convert into a checkpoint with weights so that the model works for NHWC format input images. May I know how to do it. You've mentioned\r\n'`\r\nThis requires the user to have to do some \"wrangling\" (e.g. loading the checkpoint of weights and re-buidling graph in Python) to map from one image format to another.'`\r\n\r\nI know how to load a checkpoint file but how do I rebuild the graph for different image input and save a new checkpoint model for the new graph ? It would be great if you can provide some help or any sample/pseudo code.\r\n\r\nThanks in advance !!!"]}, {"number": 8137, "title": "Support consistent data_format between tf.layers and everything else", "body": "The functions in `tf.layers` take a `data_format` parameter. However, this parameter has different semantics from the identically named `data_format` parameter everywhere else in TensorFlow. It's expected to be `channels_first` or `channels_last`, versus `NHWC`, `NCHW`, or `NDHWC` everywhere else. As such, it's inconvenient from a DX perspective to intersperse `tf.layers` code with other TensorFlow code, as it requires passing different values for the identically-named `data_format` parameter.\r\n\r\nIdeally, the functions in `tf.layers` should support the more explicit `data_format` strings. While `channels_first` and `channels_last` are easier to understand, they're less explicit, as there do exist cases outside of TensorFlow where the tensor layout is CHWN, given which `channels_first` meaning `NCHW` is not optimally clear.\r\n\r\nOn the same note, it's a bit inconvenient that `tf.layers.batch_normalization` takes `axis` instead of `data_format`; while this is more correct, it makes it annoying to switch back and forth, especially that the fused batch norm implementation only supports NHWC and NCHW anyway, rather than batch norm on an arbitrary axis.", "comments": ["Drive-by comments from my recent experience switching NWHC to NCWH:\r\n- `tf.layers.conv2d` doesn't support NCHW on CPU because of BiasOp. Seems weird that op is still being used, a+b should be sufficient\r\n- Tensorflow performance guide [recommends](https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/docs_src/performance/performance_guide.md) recommends using `tf.contrib.layers.batch_norm(x, fused=True, ...)` which accepts `data_format='NCWH'`\r\n- When it doubt for notation, I think PyTorch should be followed, since API is super-clean", "@yaroslavvb I haven't checked, but per https://github.com/tensorflow/tensorflow/issues/7551, it might be slower to use the broadcasted add instead of `bias_add`.", "Yup, `tf.nn.bias_add` seems to be faster than doing a broadcasted addition: https://github.com/tensorflow/tensorflow/issues/7551#issuecomment-284519625", "We could add support for the relevant strings to layers easily. The question is what to do for the modes we don't support. @fchollet FYI.", "It's planned to add support for `data_format` argument values such as `NHWC`, etc. It will be easy, too, and we'll get to it soon. @martinwicke: what do you mean by \"modes we don't support\", is it e.g. `C*` formats? Or is it the fact that a lot of TF ops support some formats but not others? (e.g. `conv3d`, `bias_add`, etc...", "Exactly, what to we do when we encounter a partially unsupported format. I guess we could let the inside ops error out if they wish. When we start allowing HW and WH modes we also have to be careful about what should happen when they are mixed (can they be?)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "There isn't any new information, but this issue is still relevant. It would be really nice to not have to convert back and forth between `NCHW` and `channels_first` in switching between API levels.", "@tfboyd @tatianashp if I understand correctly, this will become more pertinent with MKL, for which the default (or only supported) data format is different from GPU?\r\n\r\n@rmlarsen data layout optimization is a grappler pass, right? \r\n\r\nWhat is the way forward to relieve actual users from these problems?", "The right answer there is something like https://github.com/tensorflow/tensorflow/issues/8227.\r\n\r\nThis is more of an smaller ergonomics thing. I just want to be able to do `data_format='NCHW'` for e.g. `tf.layers.conv2d`, as is the case for most other things in TF, rather than having to do `data_format='channels_first'`, which is the only supported scheme in `tf.layers`, and is not supported anywhere else in TF.", "@taion This is a solved issue. Consider using `tf.keras` to build your models. In all layers from `tf.keras.layers` that take a `data_format` argument, the default will be `keras.backend.image_data_format()` (which you can set via `keras.backend.set_image_data_format('channels_...')` or via your Keras settings files). This allows you to build models that work for either data format, without changing anything.\r\n\r\nAgreed that we should extend support to `BatchNormalization`.", "We should probably leave discussion there on #8227.\r\n\r\nThe specific issue here is just that it's awkward to write:\r\n\r\n```python\r\nnet = tf.layers.conv2d(net, ..., data_format='channels_first')\r\nnet = tf.whatever(net, ..., data_format='NCHW')\r\n```", "@martinwicke, could you please help shepherd this issue towards resolution? It would be nice to have a consistent set of strings for `data_format` across all of TF.", "@yaroslavvb: I'm wondering if you have any inputs on this. Currently, is it possible to convert a Pytorch model that inputs NCHW (3, 224, 224) to a TFLite model that inputs NHWC (224, 224, 3)? I was able to convert Pytorch NCHW to TFLite NCHW but the generated model is complicated (10 times increase in memory and inference speed)."]}, {"number": 7958, "title": "Support MPSCNN (MetalPerformanceShaders) on iOS", "body": "Related to: https://github.com/tensorflow/tensorflow/issues/3001\r\n\r\nTake advantage of the [MPSCNN (Metal Performance Shaders) framework from Apple](https://developer.apple.com/reference/metalperformanceshaders).\r\n\r\nSee [blog post](http://machinethink.net/blog/apple-deep-learning-bnns-versus-metal-cnn/) for a comparison of BNSS to MPSCNN (and [associated code](https://github.com/hollance/BNNS-vs-MPSCNN)).\r\n\r\n**TL; DR** BNNS is faster for smaller networks but slower for bigger networks.\r\n\r\nRelated: https://github.com/tensorflow/tensorflow/issues/4846", "comments": ["Thanks for filing this issue @cancan101 !  I think the commentary described on #4846 and #3001 describes the current situation well.  I'm marking this as \"contributions welcome\".", "@tatatodd I might be interested in looking at this. Any suggestions for how to tackle this? Is there a concept of GPU device when running on iOS? The BNNS is simpler to think about as it uses the accelerate framework which runs on CPU.", "@petewarden knows the space pretty well, and might have some suggestions.", "CC-ing @keveman who's interested in this area.", "I would like to contribute for this as well. Is there any movement or at least slack for interested people?", "I am putting together a skeletal framework for calling Metal from TF. I would hold off until that is upstreamed. Once that is done, there will room for lot of contributions in building the repertoire of TF operations that can be run using Metal. @s1ddok and @cancan101 , does it sound like a plan?", "One should keep in mind that there are a lot of things lacking in MPS framework. So we will have to provide custom layer-implementation for absent layers. \r\n\r\nAlso, you gain maximum performance when using `MPSTemporaryImage`s, so everything should be encoded in one stage. ", "Yes. I am counting on contributors like you to add the missing pieces :) \r\nAlso, yes, I am aware of the need to use `MPSTemporaryImage` and set its `readCount` to `0` as soon as possible. My framework would handle those.", "You may take a look on this, it is a C++ wrapper around Metal. Not around MPS, but still. https://github.com/naleksiev/mtlpp", "> set its readCount to 0\r\n\r\nYou don't really do it. MPS decreases it everytime that image is being encoded, but what we need to do is to set that `readCount` to the amount of \"output\" layers. Should be fairly simple.\r\n\r\nP.S. We will of course need to decrease that number in custom layers, but those are details.", "@s1ddok thanks for the pointers. The C++ wrapper around Metal is great. We do really want to call the convolution kernels in MPS, though.", "We started with MPS integration into Tensorflow ![here](https://github.com/ButterflyNetwork/tensorflow/commit/86b3137d50f4096c1ff996eb05dcce2d0993e936). We focused on the 2D convolution operation. This is a summary of what we did:\r\n\r\n- created a generic iOS test-bench that generates test data using Python/Tensorflow on the host, ships the data to a phone app, runs the MPS equivalent operation  and compares the result (note that MPS runs on mobile devices exclusively) (![readme](https://github.com/ButterflyNetwork/tensorflow/blob/mps-conv2d-prototype/tensorflow/contrib/ios_examples/mps/README.md), ![test-data script](https://github.com/ButterflyNetwork/tensorflow/blob/mps-conv2d-prototype/tensorflow/contrib/ios_examples/mps/mps-test/prepare_data.py), ![unit test](https://github.com/ButterflyNetwork/tensorflow/blob/mps-conv2d-prototype/tensorflow/contrib/ios_examples/mps/mps-test/mps-testTests/mps_testTests.mm))\r\n- implemented a MPS conv2d prototype (![code](https://github.com/ButterflyNetwork/tensorflow/blob/mps-conv2d-prototype/tensorflow/core/user_ops/mps_conv.mm)) - due to MPS esoteric data format it is somewhat tricky to implement a generic conv2d operation (data shuffling is necessary); with temporary images and chaining multiple conv2d operations we expect that this can be avoided but this prototype is not capable of doing that\r\n\r\nWe show that MPS can be used for conv2D but as mentioned here we need some infrastructure for calling Metal, sending Metal GPU memory (and temporary images) between nodes to get full performance. This is why at this time we are not ready to upstream these changes. \r\n\r\nThe proposed MTLPP library looks great - we would love to see something like this integrated. We are working on a similar wrapper library that wraps CUDA, OpenCL, Metal called ![Aura](https://github.com/sschaetz/aura) - if TF allowed us to switch the GPU backend under the hood we would be all set.", "@sschaetz good to see this. The goal is to of course shuffle the data one time only and then encode verything in one pass. Apple has a good example on how to implement Inception_v3, so basically all the ideas of the best performance can be gained from there.\r\n\r\nThere is also a rendering library called `bgfx`, it is an abstract API for GPU backends, I built a game engine on that one back in the day. Abstraction layer like that one could help to switch backends seamlessly.", "@keveman any updates on your framework for calling metal ? ", "@cancan101 No significant updates, but coming soon.", "http://caffe2.ai/docs/mobile-integration.html#null__performance-considerations makes the following claim:\r\n>ARM CPUs outperform the on-board GPUs (our NNPACK ARM CPU implementation outperforms Apple\u2019s MPSCNNConvolution for all devices except the iPhone 7). ", "And here is the pr adding the MPSCNN functionality to caffe2 : https://github.com/caffe2/caffe2/pull/215", "@cancan101 small correction per @ajtulloch: Metal is faster for devices that are iPhone 6s and above, and NNPack faster on the rest of devices.", "Hey @keveman, I wrote the C2 mobile stuff so really interested in the TF approach. How are you thinking of structuring the TF integration? It'd be cool if we can reuse kernel sources and stuff.\r\n\r\nSome decisions/hacks/notes I made that I was kind of unsure about, so I wondered how you'd approach them:\r\n\r\n- Using textures (MPSImage, MPSTemporaryImage) vs buffers (MTLBuffer) as the tensor representation. There are a few issues here - standard textures vs buffer stuff, but also some semi-surprising limitations of the implementation (e.g. a maximum of 2800 textures in a `texture2d_array`, which introduces issues with representing a batch-size 50, 224-channel image. You need the `MPSTemporaryImage` for the MPSCNN kernels obviously, but these kernels seem to be operating well below peak so I imagine there's space to improve there with a custom implementation.\r\n- Handling of the `MTLCommandBuffer` object. Instead of the CUDA approach with a reusable stream that you can schedule kernels/events on, this transient object is single use, so there needs to be some initialization and passing it around along with the MPSTemporaryImage in the [`MPSImageWrapper`](https://github.com/caffe2/caffe2/blob/d0ce49/caffe2/contrib/mpscnn-fb/mpscnn.mm#L113-L116) struct.\r\n- Memory management - solving it with `MPSTemporaryImage`'s ([and having a SSA pass to compute the `readCount` field](https://github.com/caffe2/caffe2/blob/d0ce49/caffe2/contrib/mpscnn-fb/mpscnn.mm#L1778-L1814)) vs using a custom allocator with `MTLBuffer`.\r\n- General object ownership issues, integrating the ARC refcounting model with the DFG style ownership. In the C2 impl, the op maintains a ref on the `MPSTemporaryImage` which solves the common case but has some ugly corner cases, so it's probably worth fixing it to do proper ARC on the `MPSImageWrapper` itself.\r\n- Passing compile-time specialization arguments to kernels helped a lot (https://github.com/caffe2/caffe2/blob/d0ce49/caffe2/contrib/mpscnn-fb/MPSCNN.metal#L7-L13), but ideally that would be a bit nicer from a kernel authors perspective without cluttering it up with a lot of single-use structs/syntax.\r\n- Speaking of kernels, the kind of ugly switching/duplication between `texture2d`/`texture2d_array` is something that might be good to revisit via some macro sugar or something?\r\n- In general, there were a bunch of good snippets in https://developer.apple.com/videos/play/wwdc2016/606/. \r\n- Getting the GPU profiler working (via a call to [`MTLCommandQueue:insertDebugCaptureBoundary`](https://developer.apple.com/reference/metal/mtlcommandqueue/1508692-insertdebugcaptureboundary) after `Net::Run`/`Session::Run` was pretty useful for some stuff.\r\n- For some use-cases, the cost of command construction/encoding was nontrivial compared to execution time, so I was thinking of doing some double-buffering wrapper (at a higher level) for latency-sensitive applications.\r\n", "@Yangqing That's what I noticed too. Using Metal on iPhone 6s and above was significantly faster.", "@ajtulloch Thanks for your detailed comments. It looks like the C2 implementation is further along than what I have, but it would be great to share code if possible. I haven't looked at the C2 implementation in detail yet, but I am thinking about all the points you bring up here. Especially, I am super frustrated by the `texture2d/texture2d_array` issue too. Here are some thoughts I have on some of your points.\r\n\r\n- Using textures: I want to be using `MPS[Temporary]Image` to represent tensors. One of the biggest use cases that is going to benefit from Metal is a model that works on individual frames of a video (batch size of 1). I am inclined towards supporting that use case well. So the limitation on the number of textures won't be too bad.\r\n- Memory management: Since the Metal framework manages the underlying memory of a `MPSTemporaryImage` pretty efficiently, I didn't think I needed any analysis (I may be wrong). Effectively, I declare one `MPSTemporaryImage` per tensor in the graph and set its `readCount` to `0` as soon as its last consumer is enqueued.\r\n- Double buffering: Yes, the host side cost of spinning up a pipeline and enqueing kernels is pretty significant. I don't have a great answer for this yet.", "- Re. textures - makes sense. When would you use `MPSImage` in that case? In the C2 approach, all the intermediate states are transient/unobservable, and are only observable via a copy (which is also the synchronization point), so there's no need for textures that persist outside the lifetime of the command buffer (which is what MPSImage would give you?).\r\n- Re. memory management: yes, it seems pretty solid. I experimented with the [`MPSTemporaryImage:prefetchStorage`](https://developer.apple.com/reference/metalperformanceshaders/mpstemporaryimage/2097544-prefetchstorage) call but didn't get a perf win on any of the models I was using, which seemed surprising and I wonder if I was misusing the API.  Have you got a win out of this API at all?\r\n- Re. double buffering - I was thinking of having an async API that takes a complete metal compute graph, then preallocates a pool of buffers and net/session instances, and spins up a background thread that repeatedly constructs all the commands with respect to the stable pointer and enqueues the resulting `MLTCommandBuffer/MTLBuffer` back to the caller thread.  Then, the client code just needs to do a memcpy from the caller data -> buffer + encode command buffer + wait, which I believe should be a win in the scenarios I was looking at. Does that make sense? I think a bunch of these applications are caveated on being able to execute the entire computation graph on the Metal device, which is what I was targeting for a bunch of our applications but might not be applicable for other applications.\r\n\r\n", "At @xmartlabs we have been working on a framework to use neural nets for iOS on top of metal that supports running TensorFlow models, maybe someone is interested in using it, at least until TF supports metal\r\n\r\nhttps://github.com/xmartlabs/Bender", "Would it make sense to take the same approach to adapt TF models to run on Caffe2?", "@bryant1410 Thanks for the note. The Bender project is awesome! Some of the code, especially the shaders can be shared between the implementations. I'll keep you posted.\r\n", "Core ML might be an interesting abstraction to both this and BNNS: https://github.com/tensorflow/tensorflow/issues/10468.", "@keveman @ajtulloch \r\nContinuing your discussion from June \ud83d\ude38 \r\n\r\n* [prefetchStorageWithCommandBuffer:imageDescriptorList:](https://developer.apple.com/documentation/metalperformanceshaders/mpstemporaryimage/2097544-prefetchstoragewithcommandbuffer?language=objc) Seems to give a small win when calling the same pipeline more than once. While counter intuitive being linked to a single `commandBuffer` calling the prefetch method on each new buffer with the same set of descriptors results in faster run times for me.\r\n* preencoding - @ajtulloch Did you actually manage to make this work? preencoding the whole buffer and then just copying the data into the **input** `MPSImage` and committing?\r\n* Double buffering - I did not try it yet, but I believe that for large enough graphs you can switch an `MPSTemporaryImage` in the middle for an `MPSImage` in order to encode a buffer up to that point, then while that buffer is commited, use the CPU to encode a new buffer to run from that point all the way to the output. Also, did not try it yet, but if any of you did, I would love to hear about it.\r\n* `texture2d` vs. `texture2d_array` - Since the code for C2 is already out there, I will open a PR for that in a couple of days. The essence is the fact that Metal is based on C++14 which fully supports templating, namespacing and other tools that allow you to create common code called by several kernels.\r\n\r\nLast but not least, while being Swift based, [Forge](https://github.com/hollance/Forge) has some nice ideas in it too for building a framework on top of MPS.", "@keveman Do you have your framework somewhere to try ? Even if it is WIP maybe easier to build on the same foundation ", "@ofirbb interesting. \r\n\r\n- Re: prefetching - how did you select the descriptors to prefetch?  Seems to trade off heap size vs time spent allocating right?\r\n- Re: preencoding/double buffering - we do something similar for models like Mask R-CNN, where we run parts of the graph on the CPU and parts on the GPU. Not very cleanly abstrated though.\r\n- Re: texture2d/2d_array - cool stuff. I was thinking of just a minor refactor to go to single kernel and keep using function_constants to select array/non-array, but if there's some nice template abstractions that's pretty neat.", "Is there any updates on supporting MPSCNN in TensorFlow or TensorFlow-Lite? I am new to machine learning so I probably can only describe the problem from a user's perspective. \r\n\r\nWe are currently comparing the accuracy / performance between TF-Lite and Metal (MPSCNN), and it seems Metal works better on iOS (Mobilenet_v1). As we have to support iOS 10 so Core ML is not an option.\r\n\r\nThough I don't like the Metal approach we used, which required us to extract the weights and biases from frozen TF graph and also we had to write Metal code by ourselves to build the inference. I just thought it would be nice if TF-Lite can provide comparable performance, then there will be no need for us to dig into the Apple's MPSCNN APIs :)\r\n\r\nThank you for all your efforts!"]}, {"number": 7956, "title": "Method log_prob_with_logits() for Dirichlet", "body": "It would be useful to have a log_prob_with_logits() method for the Dirichlet distribution.\r\n\r\nThe reason being is that it is often useful to model the discrete posterior distribution in log space, which doesn't let all probabilities to go exactly to zero. Then the Dirichlet prior can be applied to the data in log space. Note, that if the posterior is converted to the normalised distribution, then some of the discrete probabilities may actually go to zero due to the rounding errors. Then the Dirichlet prior cannot be applied to this distribution because it doesn't allow zero probabilities.\r\n\r\nThe log_prob_with_logits() calculation would be very simple to implement. The log(x_i) would need to be replaced with just x_i, and an additional term with LogSumExp(x) added.\r\n\r\nThis would actually enable the possibility to use the Dirichlet prior on the discrete viariables in log space (can also be part of a neural network).", "comments": ["Namely, the formula for log_prob_with_logits(x) would be as follows (where x are logits, and L is the log of the Dirichlet normalisation constant):\r\n\r\n![formula](https://cloud.githubusercontent.com/assets/2409854/23442260/5dc438cc-fe1f-11e6-813d-220f9be70c85.jpg)\r\n\r\nNote: formula contains a mistake of using alpha instead of alpha - 1\r\n", "@langmore Can you comment on this?  Thanks!", "Good point akuz.  Rather than adding a new method, I think it would be better to allow the Dirichlet to be initialized either with `concentration` (as it currently is) OR `log_concentration`.  See e.g. the `Categorical` distribution for an example.\r\n\r\nI'm choosing `log_concentration` rather than `logits`, since to me, `logits` are something that is intended to be fed through a softmax, and therefore they are only defined up to an additive constant.  Whereas here, the absolute value matters.\r\n\r\nDoes this sound like something you would want to do?", "Hi Ian, I guess this could also accept log_concentration in parameter (although I don't have a use case for that). This still doesn't solve the problem I was talking about. The Dirichlet distribution is defined *over* the categorical distributions. These are the concrete distributions that can be passed as x to log_prob(x). They have to be valid distributions, I.e. sum up to one, and all numbers must be > 0 and < 1. The Dirichlet doesn't allow any of the components to be exactly 0 or 1, and will output NAN in such case. Now imagine that the input to log_prob(x) is such that x = softmax(y). This can easily result in the output like x = [0, 0, 1, ..., 0, 0], for which the Dirichlet log_prob(x) will output NAN. So what I am suggesting is to pass logits directly to log_prob() without applying softmax first. We can do it like Dirichlet.log_prob_with_logits(), which would be similar to cross_entropy_with_logits(). With cross entropy they don't apply softmax before passing to the function for exactly the same reason of potential rounding of the output of softmax to x = [0, 0, 1, ..., 0, 0]. (Aternatively, instead of making a new method, we could add a new parameter to log_prob(x, logits) such that only one of x or logits can be specified. What do you think?", "You're right, I was mistaken.  The problem with zeros or ones in `x` needs some sort of `log_prob_with_logits` function.  I suppose there is another problem if `concentration` has any zeros in it, but that would require a new version of the Gamma function.\r\n\r\nSo the solution suggested by @tatatodd is to add a `log_prob_with_logits` function for distributions defined on the interior of the simplex, `{(x_1,...,x_K) :  x_i \\in (0, 1), \\sum_i x_i = 1}`.  This would be `Dirichlet`, and `Beta`.\r\n\r\n Note however that `Gamma.log_prob(x)` will fail for the exact same reason if `x = 0` (both compute `log(x)`).  In fact, all of our distributions defined on `(0, inf)` have this issue.  In my work, I've avoided this by careful use of priors, but before I had this down it was a pain.  It would be good if the solution worked for these distributions as well.  Or, @tatatodd do you think that `Dirichlet` is more prone to this issue?\r\n\r\n@ebrevdo What do you think?", "I agree about Dirichlet and Beta. Not sure about Gamma, maybe log/exp is not the best transform in that case.", "I'd prefer a new Dirichlet distribution defined over the logits space,\nrather than adding a new method to all distributions.  wdyt?\n\nOn Sat, Mar 4, 2017 at 11:06 AM, AK <notifications@github.com> wrote:\n\n> I agree about Dirichlet and Beta. Not sure about Gamma, maybe log is not\n> the best transform in that case.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7956#issuecomment-284173488>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim50BEpuZ8vjUzgbvJuk1ohHMweDOks5ribZBgaJpZM4MPGnM>\n> .\n>\n", "Actually, sounds good, abrevdo. I haven't worked with probabilistic inference much in TensorFlow yet, but I see there is a bunch of distributions already following your suggestion.", "@ebrevdo , If we simply use the formula above then we do not have the log of a probability density over `R^k`, so we can't call it `log_prob(x)`.  To see that the above formula is not a `log_prob`, note that it doesn't go to negative infinity as `|x| \\to \\inf`, so the corresponding `prob(x) := exp(log_prob(x))` will not go to zero.  You can see also that the above formula is constant if all the `x_i` are equal.  The reason being that whenever all the `x_i` are equal, `Softmax(x) = [1/k,...., 1/k]`.  So the entire line `{x : x_1 = x_2 = ... = x_k}` is mapped by Softmax to a single point.\r\n\r\nI believe @akuz wants the standard Dirichlet distribution over `k-simplex` of probabilities, but wants to evaluate it on an intermediate state of data (before the probabilities are probabilities).\r\n\r\nNote @akuz that since this doesn't give you a valid prior on logits, it may be better to create the distribution over logits that corresponds to a specified Dirichlet distribution over probabilities.  After all, your network produces logits.  This can be done as follows:\r\n\r\n```python\r\nds = tf.contrib.distributions\r\nbijectors = tf.contrib.distributions.bijectors  # you'll have to search for this...depending on version.\r\n\r\n# The dirichlet is a distribution over the k-1 dimensional k-simplex\r\ndirichlet = ds.Dirichlet(concentration=[1., 2., 3.])\r\nsoftmax_centered = bijectors.SoftmaxCentered(event_ndims=1)\r\n\r\n# dist_over_logits is a distribution over R^{k-1}\r\ndist_over_logits = ds.TransformedDistribution(\r\n    dirichlet,\r\n    bijector=bijectors.Invert(softmax_centered))\r\n\r\nlogits = [-1., 1.]\r\nprobs = softmax_centered.forward(logits)\r\n\r\n# These two are equal\r\nprobs\r\n==> [ 0.09003057  0.66524088  0.24472845]\r\ntf.nn.softmax(logits + [0.])  # Note implicit appending of [0]\r\n==> [ 0.09003057  0.66524088  0.24472845]\r\n\r\n# These two are not equal\r\ndirichlet.log_prob(probs)\r\n==> 0.871526\r\ndist_over_logits.log_prob(logits)\r\n==> -3.35129\r\n```\r\n\r\nThe last two lines above are not equal because the Dirichlet is a distribution over the (k-1 dimensional) k-simplex, which is very different (in particular, it is much smaller) than R^{k-1} in which logits live.  The probability density over R^{k-1} must be much smaller at the point `logits` that corresponds to `probs`\r\n", "....in the above example, `dirichlet` is the distribution for a random variable `Z = Dirichlet([1, 2, 3])`, and `dist_over_logits` is the distribution of the random variable `X = InverseSoftmaxCentered(Z)`, i.e. `Z = SoftmaxCentered(X)`.  In other words, `Z` is the correct distribution for your logits.", "@langmore you're right that the above formula doesn't define a proper density on the k-D space of logits, due to the reasons you mentioned, so it would not make sense to call it log_prob(x)... The transformed distribution trick you suggested looks great, but I just looked through the documentation, and it seems SoftmaxCentered only supports low dimensionality inputs. What I have is a 4D tensor, the last dimension of which would be the logits, so I'm afraid the transformed distribution might not work in this case? Also would that be efficient?\r\n\r\nOn the other side, if you ask yourself the following question: \"Does the above formula calculate log probability of softmax(x) under the specified Dirichlet prior?\" - the answer is yes. So, it does not provide the density in the space of logits, but rather in the space of discrete distributions resulting from softmax(x). So, all I'm trying to do is: 1) y = softmax(x), 2) Dirichlet(concentrations).log_prob(y). But because softmax has a tendency to collapse to [0, 0, ..., 1, 0] due to rounding, the Dirichlet log_prob fails to work properly. The above formula would provide a shortcut to compute steps 1) and 2) in one go, without actually computing softmax. Yes, it would not define a valid density on x, and therefore maybe should not be implemented as a Distribution. Maybe it would be better if implemented as some sort of function on tensors? dirichlet_log_prior_on_softmax(concentrations, logits)? Well, maybe a better name would be needed :)\r\n\r\nSo, it short, I'm not looking to define any new types of Distributions, but an efficient way to compute steps 1) and 2) above in one go, without actually computing the softmax in between. Similar concept to cross_entropy_with_logits().", "In fact, if we ignore the normalisation constant for now, the above formula can be implemented using the convolution (where concentrations would be used as weights for the last dimension) for the first part, and reduce_logsumexp() for the second part (you would also need to compute and multiply it by the negative of the sum of concentrations). But this implementation has two drawbacks 1) you need to configure the convolution in a special way, and it is only implemented for some specific shapes of tensors, whereas here we always need to convolve along the last dimension only, and 2) it produces two intermediate tensors, which are the summed up. Therefore, a specific implementation would be much more efficient. \r\n\r\nAlso, the dirichlet_prior_on_softmax() could just sum up the numbers along all dimensions above the last one, because all that is usually needed is the total log probability (user may need to specify the axis for the computation, and whether to reduce all the others axes). In case when everything is reduced to one number, a sum of log probabilities along all remaining dimensions, this function would not produce any intermediate tensors at all, and would just output a scalar.", "@akuz , I agree that your original suggestion does provide an alternative method to compute the log_prob on the space of probabilities by intercepting an intermediate step of computation of an input.\r\n\r\nSo.... we can't call this `log_prob` since it isn't one.  akuz's original suggestion of `log_prob_with_logits(x)` would work (although I think the `alpha` in the formula should be `alpha - 1`).  @ebrevdo I think the options are:\r\n\r\n1. Do nothing, let users build their own (`Dirichlet._log_unnormalized_prob` will help BTW)\r\n1. Write a helper function, e.g. `dirichlet_log_prob_with_logits(dirichlet_dist, logits)`.\r\n1. Create a `Dirichlet.log_prob_with_logits(logits)` method.\r\n\r\nBTW, the dimensionality restriction on SoftmaxCentered is that it works for batch scalars and vectors (event_ndims = 0 or 1), not batch matrices (event_ndims = 2).  So it would work well for a Tensor of shape [2, 3, 4, 5], representing a shape [2, 3, 4] batch of length [5] logits.  But, it seems this is not what you need.", "@langmore , I think that option 3 from your list of suggestions would be most convenient to use in practice. Sorry about the mistake missing -1 in the formula. The computation of the \"L\" part in the formula should be possible to do easily too (I've done it with lngamma function in C++ previously). I think I would not be the only one to need this, hopefully. \r\n\r\nP.S. Thanks for explaining about the SoftmaxCentered, I didn't understand before what was the event_ndim.", "@langmore I was looking more into prior distributions defined on R+, such as Gamma, InverseGamma, etc. I think these would benefit from something like log_prob_with_softplus(). I've made a simple model of clustering using expectation maximisation. In it, the latent variables (cluster assignments) and the parameters of the clusters (cluster probs, means, stdevs) are estimated using a single optimizer loop (all together). If I put the stdevs directly as the variables to optimize, this leads to problems, because optimizers don't allow specifying the bounds. So what I ended up doing is:\r\n\r\nstdevs_raw = tf.Variable(...)\r\n\r\nstdevs = tf.nn.softplus(stdevs_raw) - this makes the stdevs always positive\r\n\r\nstdevs_prior = tf.constrib.distributions.InverseGamma(100, 10).log_prob(tf.pow(stdevs, 2))\r\n\r\nHowever, this still has a problem if one of the stdevs collapses to zero (which is possible if stdevs_raw becomes a large negative number).\r\n\r\nTherefore, it would be useful to have something like InverseGamma.log_prob_with_softplus(x) which would compute the log probability for softplus(x) without actually computing the softplus.\r\n\r\nMaybe this can be done with some variable transformations? I am not sure\r\n", "Question:  Why did you feed `stdevs ** 2` to `log_prob` rather than `stdevs`?\r\n\r\nI see this as a good features request.  We have to figure out if there is a solution that will work across different distributions.  Building upon your previous suggestion, what about `log_prob_pre_X`, where `X` is `softmax`, `softplus`, `exp`, etc...  ?\r\n\r\nI don't think this will be resolved immediately.  So for the moment I recommend writing helper functions.  This will actually be better since you can see what works and give us feedback.  If you want, you could add a helper function to the same file that contains the class and submit a pull request.  E.g. add `log_prob_pre_softmax(dist, logits)` to `dirichlet.py`, along with some tests.  For the moment, keep it hidden, i.e. don't add it to the `__all__` list at the top of the file.  If this helper function works well, we can either un-hide it, or add it as a method.\r\n\r\nAlso, and I realize every use case is different, let me tell you how I've been able to solve this issue.  So far, I've been able to avoid the collapse to zero (with 32 bit tensors) by using `AdamOptimizer` with a small step size of around 0.01 and making sure that the prior's log-prob was included in my loss (you're already doing this I assume, but just in case...).  So in your case above I make sure my loss is `-1 * log_likelihood - 1 * stdevs_prior`.  In theory, the penalty from the prior should prevent collapse to zero, but in practice of course many crazy things can happen, especially when you have hidden layers.", "@langmore , I fed stdevs ** 2 to InverseGamma, because inverse gamma is a conjugate prior for the variance of Normal with known mean. The conjugacy doesn't play much role here, but there is another benefit. The conjugate prior hyperparameters have interpretation that is easy for me to use to set a specific prior, see here: https://en.m.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions (see \"Interpretation of hyperparameters\" column).\r\n\r\nI agree that would be a useful feature (partially because I've created this request :)), because of the collapse of the values (resulting from softmax or softplus) to zeroes, which renders many useful priors invalid, if not calculated in log space directly. As you say, it's not clear what would be the best way to implement it in conjunction with what already exists. When I have time, I will try to follow your suggestion, but I have to spend most of my time on my full time job. Hopefully I can find some time on lunch breaks / evenings soon.\r\n", "@ebrevdo - just saw the new paper that came out \"Deep Probabilistic Programming\" - how do you deal with the problem described in this ticket within the Edward lib?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'll bring this up to the team.  I think the way to think of this is that we would offer a version of `Dirichlet` (and others) that has been \"pre-transformed\" (in a stable manner) to unconstrained space using some `Bijector`.  E.g. `DiricheletUnconstrained` would be a stable equivalent to\r\n\r\n```python\r\ntfd = tf.contrib.distributions\r\ntfb = tfd.bijectors\r\ndist = tfd.Dirchlet([0.5, 0.5])\r\ndist_uc = tfd.TransformedDistribution(\r\n    dist,\r\n    bijector=tfb.Invert(tfb.SoftmaxCentered()))\r\n```", "I like your solution @langmore to use transformed distribution. Among other things, a canned implementation could be useful for Dirichlet priors on Categorical/Multinomial logits.  \r\n\r\nHow do you propose handling the fact that the desired function doesn't normalize? If I understand the discussion correctly, `dist_uc.log_prob` does _not_ return the same as \"`log_prob_pre_softmax`\"?\r\n\r\n> @akuz: @ebrevdo - just saw the new paper that came out \"Deep Probabilistic Programming\" - how do you deal with the problem described in this ticket within the Edward lib?\r\n\r\nThis is actually a problem in Edward's mixture models. We just work in the `probs` space and it can be unstable.\r\n\r\nExamples:\r\n+ [`examples/stochastic_block_model.py`](https://github.com/blei-lab/edward/blob/93b6e31dce17c2ad3dca46b80a825819f02da5fe/examples/stochastic_block_model.py)\r\n+ [`examples/mixture_gaussian_gibbs.py`](https://github.com/blei-lab/edward/blob/master/examples/mixture_gaussian_gibbs.py)", "Note that I'm not suggesting we use transformed distribution (since what I wrote above would be no more stable than `Dirichlet`.  I'm suggesting we pre-bake an equivalent distribution.\r\n\r\nI don't understand the normalization issue.  Perhaps tell me what the args are above to `log_prob` and `log_prob_pre_softmax`?", "Oh I see. I'm only for a canned distribution over logits space;  so I still agree :)\r\n\r\nI don't understand the normalization issue either. The above conversation is rather involved\u2014something about `log_prob_with_logits(x)` in the latex formula not defining a proper density? Maybe you can summarize how that relates to your \"implement a canned distribution\" proposal?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Given that [this endpoint](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Dirichlet) has moved to TensorFlow Probability, it would make sense to transfer the feature request. \ud83d\udc4d \r\n\r\ncc: @jvdillon "]}, {"number": 7848, "title": "[Feature Request] Predict posterior probability of data per each component in GMM ", "body": "It would be very helpful  if  method is provided to predict posterior probability of data per each component in GMM.\r\n\r\nA stackoverflow [question ](http://stackoverflow.com/questions/42357268/looking-for-a-method-to-replace-sklearn-mixture-gaussianmixture-predict-probax) has already been raised by longwoo.\r\n\r\n", "comments": ["@xavigonzalvo: is GMM still supported?  Would this be a welcome contribution?", "Hi, @poxvoculi @xavigonzalvo @agarwal-ashish \r\nAny update on this issue. I have started working on the nightly build of tensorflow 1.1.0-rc0 but I could method which will give me the posterior probability of data per each component in GMM. \r\nPlease look into it.", "It will be supported next quarter. It's not something difficult to do but I\nam on my paternity leave.\n\n2017-03-30 10:38 GMT-04:00 Sourav Mahato <notifications@github.com>:\n\n> Hi, @poxvoculi <https://github.com/poxvoculi> @xavigonzalvo\n> <https://github.com/xavigonzalvo> @agarwal-ashish\n> <https://github.com/agarwal-ashish>\n> Any update on this issue. I have started working on the nightly build of\n> tensorflow 1.1.0-rc0 but I could method which will give me the posterior\n> probability of data per each component in GMM.\n> Please look into it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7848#issuecomment-290430735>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEVsodmNVnXySFftRpmuiiOk-Mypv-9Bks5rq75IgaJpZM4MLM-u>\n> .\n>\n\n\n\n-- \nXavi.\n", "@xavigonzalvo Any updates?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Ping, @xavigonzalvo could you provide updates?\r\nIf this is obsolete, please close the issue.\r\nIf we cannot get to this feature, please apply the contributions welcome label.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Have you considered giving it a go and add this yourself?", "Looks like we are open to contributions on this one?"]}, {"number": 7822, "title": "Returning argmax with tf.nn.pool", "body": "Feature request for returning argmax for N-D pooling with `tf.nn.pool`, as in `tf.nn.max_pool_with_argmax`.", "comments": ["@fchollet : thoughts?", "I am interested to take up this issue. Can you please assign it to me ?\r\n"]}, {"number": 7662, "title": "Add a dynamic_partial_sum operator to tensorflow?", "body": "Hi,\r\n\r\nIn my application I need to do operations that dynamically sum some rows of a matrix to get a new matrix.  There will be an input tensor named \"index\" that guides which part of the tensor to be summed. \r\n\r\nAn example is, if the input matrix is\r\n```python\r\n[ [   1,   1,   1],\r\n  [  10,  10,  10],\r\n  [ 100, 100, 100] ]\r\n```\r\nAnd the index is\r\n```python\r\n[ [ 0, 2 ],\r\n  [ 2, 3 ]]\r\n```\r\nwhich simply says the output tensor will have two rows (because the \"index\" have two rows), the first row is the sum of rows with row number i that satisfies `0 <= i < 2` in the input, and the second row is the sum of rows with row number 2. So the result should be\r\n```python\r\n[ [  11,  11,  11],\r\n  [ 100, 100, 100] ]\r\n```\r\n\r\nI don't find any existing operation that does this job, so I implement it (support only 2d matrix and GPU) by myself.  Since I already have an implementation, I'm not requesting a new feature here. But I do want to know that if the tensorflow team is interested in adding this operation as part of tensorflow. If the answer is yes, I will add CPU support (maybe also xla? I have no idea on how to add xla support yet), and then create a pull request for that.", "comments": ["@ebrevdo, @rmlarsen, what do you two think? What would we call such an operation. It does seem useful. I've wanted to do this type of thing once or twice, even.\r\n", "Is there a numpy equivalent function or composition of functions that do\nthis?\n\nOn Sun, Feb 19, 2017 at 4:24 PM, Andrew Selle <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo>, @rmlarsen\n> <https://github.com/rmlarsen>, what do you two think? What would we call\n> such an operation. It does seem useful. I've wanted to do this type of\n> thing once or twice, even.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7662#issuecomment-280962290>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxNzOClfnr087AfR8NrJkxeOLF_7ks5reN02gaJpZM4MFYyd>\n> .\n>\n", "There is something similar:\nhttps://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.ufunc.reduceat.html\n\nOn Sun, Feb 19, 2017 at 10:50 PM ebrevdo <notifications@github.com> wrote:\n\n> Is there a numpy equivalent function or composition of functions that do\n> this?\n>\n> On Sun, Feb 19, 2017 at 4:24 PM, Andrew Selle <notifications@github.com>\n> wrote:\n>\n> > @ebrevdo <https://github.com/ebrevdo>, @rmlarsen\n> > <https://github.com/rmlarsen>, what do you two think? What would we call\n> > such an operation. It does seem useful. I've wanted to do this type of\n> > thing once or twice, even.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/7662#issuecomment-280962290\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/ABtimxNzOClfnr087AfR8NrJkxeOLF_7ks5reN02gaJpZM4MFYyd\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7662#issuecomment-280981873>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA_Auf9fI0NfkizBfh0iBx83-mzwRgtvks5reQ2IgaJpZM4MFYyd>\n> .\n>\n", "To be honest, numpy's `numpy.ufunc.reduceat` is so confusing and unnatural to me. I prefer the behavior I define here at this issue.", "This functionality seems useful to me. Marking as contributions welcome.", "Isn't this a scan, a gather, and a subtraction?", "@girving Yes, in the case of `sum` it is, but I don't think it is a good idea to implement it this way. Let's say you have a tensor containing 10000 rows, each row have value about `~1`, and you want the sum of row number 5000 to 5005, then computationally speaking, implementing using scan, gather and subtract, you are doing a `5005-5000`, which has less precision than `1+1+1+1+1`.\r\n\r\nIn the case of product, it is not, because `0 * something / 0` is `NaN`. Neither does max and min.  In my application, I only use sum, so it might be OK to use scan-gather-subtraction for me. But I'm not sure if other reduction operations like prod, min, max would be useful to others.\r\n\r\nBy the way, there are similar thing as the Op I'm implementing named `tf.segment_sum`, written in C++, see: \r\nhttps://www.tensorflow.org/api_docs/python/tf/segment_sum\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/segment_reduction_ops.cc", "> To be honest, numpy's numpy.ufunc.reduceat is so confusing and unnatural to me. I prefer the behavior I define here at this issue.\r\n\r\nOver in https://github.com/numpy/numpy/issues/834, we are discussing how to replace `reduceat`, which I think everyone agrees was poorly designed. We have not yet settled on a replacement, however."]}, {"number": 7641, "title": "'LookupError: No gradient defined for operation '...' (op type: ResizeBicubic)' is raised", "body": "I tried to use [tf.image.resize_images](https://www.tensorflow.org/api_docs/python/tf/image/resize_images) with ```method=ResizeMethod.BICUBIC```.\r\n\r\nThe shorten version of my code is the following:\r\n```python\r\n# module import is omitted\r\n# config setting is omitted\r\n# global variable declaration is omitted\r\n\r\ndef train():\r\n    # file queuing and reading are omitted.\r\n    magnification = tf.random_uniform([1], minval = 0, maxval = 4, dtype = tf.float32)\r\n\r\n    image_patches_blurred = tf.image.resize_images(image_patches, [tf.to_int32(64/magnification[0]), tf.to_int32(64/magnification[0])], \\\r\n                                                    method = tf.image.ResizeMethod.BICUBIC)\r\n    image_patches_blurred = tf.image.resize_images(image_patches_blurred, [64, 64], method = tf.image.ResizeMethod.BICUBIC)\r\n\r\n    with tf.variable_scope('generator'):\r\n        G = Stage_Unet_model.generator(image_patches_blurred, batch_size = 16, image_size = 64, input_channels = 3, gf_dim = 32)\r\n\r\n    # Model Loss definition is omitted\r\n\r\n    # Summary Operation is omitted\r\n\r\n    # tf variables loading\r\n    all_vars = tf.global_variables()\r\n    model_generator_vars = [k for k in all_vars if k.name.startswith(\"generator\")]\r\n\r\n    # Gradient Clipping\r\n    generator_grads = tf.gradients(G_loss, model_generator_vars)\r\n    generator_grads, _ = tf.clip_by_global_norm(generator_grads, clip_norm = 1.0)\r\n    generator_var_pairs = zip(generator_grads, model_generator_vars)\r\n\r\n    # tf.train.Saver and global_step declaration is omitted\r\n\r\n    # tf.train.AdamOptimizer\r\n    Adam = tf.train.AdamOptimizer(config.learning_rate)\r\n    G_optim = Adam.apply_gradients(generator_var_pairs, global_step = G_global_step)\r\n\r\n    # merge summary operation is omitted\r\n\r\n    with tf.Session() as sess:\r\n        #...\r\n        for epoch in range(config.epoch):\r\n            for idx in range(config.train_data_number):\r\n                # Note that graph operations related with model D are omitted\r\n                train_D_l, train_G_l, train_D_t_s, train_G_t_s, D_step, G_step, _, __ = sess.run([D_loss, G_loss, \\\r\n                                                                D_tot_summary, G_tot_summary, \\\r\n                                                                D_global_step, G_global_step, D_optim, G_optim])\r\n        #...\r\n\r\nif __name__ == '__main__':\r\n    if not tf.gfile.Exists(log_dir):\r\n        tf.gfile.MakeDirs(log_dir)\r\n    train()\r\n```\r\n\r\nRunning the above code will result in the following error:\r\n```\r\n$ python train_stage1.py \r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nTraceback (most recent call last):\r\n  File \"train_stage1.py\", line 216, in <module>\r\n    train()\r\n  File \"train_stage1.py\", line 150, in train\r\n    generator_grads = tf.gradients(G_loss, model_generator_vars)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 459, in gradients\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'ResizeBicubic_3' (op type: ResizeBicubic)\r\n```\r\n\r\nThank you for any help.\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n```\r\n$ ls -l /usr/local/cuda-8.0/lib64/libcud*\r\n-rw-r--r-- 1 root root   558720 11\uc6d4  4 05:18 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 11\uc6d4  4 05:18 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 11\uc6d4  4 05:18 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 11\uc6d4  4 05:18 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 11\uc6d4  4 05:18 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 11\uc6d4  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 11\uc6d4  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 11\uc6d4  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 11\uc6d4  4 05:20 /usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n```\r\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\n```", "comments": ["Not every operation in TensorFlow has a gradient defined. Often this is because these operations are not used during training loops and instead are used for preprocessing. In general, image operations are often done with bilinear for performance reasons, but also because bilinear defines the gradient. We likely will not add this any time soon, but you could easily add it yourself by looking at how image resize bilinear defines the gradient. Let me know if you need further direction. ", "@mikigom , @aselle  ,do you have fixed this problem?  I meeted the same problem when I want to calculate the gradients of ResizeNearestNeighbor.  I know image resize bilinear defines the gradient and know where it is , but I still don't know how to  solve this problem.\r\n\r\n` @ops.RegisterGradient(\"ResizeNearestNeighbor\")\r\ndef _ResizeNearestNeighborGrad(op, grad):\r\n  \"\"\"The derivatives for nearest neighbor resizing.\r\n\r\n  Args:\r\n    op: The ResizeNearestNeighbor op.\r\n    grad: The tensor representing the gradient w.r.t. the output.\r\n\r\n  Returns:\r\n    The gradients w.r.t. the input and the output.\r\n  \"\"\"\r\n  image = op.inputs[0]\r\n  if image.get_shape()[1:3].is_fully_defined():\r\n    image_shape = image.get_shape()[1:3]\r\n  else:\r\n    image_shape = array_ops.shape(image)[1:3]\r\n\r\n   grads = gen_image_ops._resize_nearest_neighbor_grad(\r\n      grad,\r\n      image_shape,\r\n      align_corners=op.get_attr(\"align_corners\"))\r\n  return [grads, None]`\r\n\r\nI copy this to nn_grad.py , but it is useless.\r\nThanks for any help \r\n", "@hlzlyc Did you resolve this issue? I am having the same problem. ", "I found this useful repo for this problem: https://github.com/iwyoo/bicubic_interp-tensorflow\r\nNot yet got it to work in my model.", "It seems that it has been fixed in 1.4.0.\r\nhttps://github.com/tensorflow/tensorflow/commit/3331c574bcfd85787d7a4f3d1b1b139239a6595b", "Any update on this?", "I think this must be updated especially for the sake of tf2. The tutorial does not provide enough detail for implementing custom gradients on tensor operators.", "@aselle I am also facing this problem of no defined gradient for the function tf.image.adjust_saturation. I am new to tensorflow so I would like to ask how should one proceed when you have to add your own gradient. Could you please give more details about that?"]}, {"number": 7515, "title": "[feature requests] DecodeCSVOP to parse only the first len(record_defaults) columns of a csv", "body": "Suppose that my data.csv is :\r\n_1,2,3_\r\n_2,4,6_\r\n\r\nand for some other purpose we add a new column to a new csv data2.csv\r\n_1,2,3,comment_\r\n\r\nthe code below fails if the input is data2.csv:\r\n`col1,col2,col3 = tf.decode_csv(line,record_defaults=[[1],[1],[1]]) `\r\n\r\nHope that tf.decode_csv() only decodes the first len(record_defaults) columns only, so that it works for both data1.csv as well as data2.csv.\r\nWhat's more, we can add  \"a column index list parameter\" to indicate which columns to be decoded.\r\n", "comments": ["Work on csv improvements is limited right now since it is usually only used for simple examples. We'd really appreciate any PR that would improve this. You could argue that this change might not be ideal, since you have a data file that doesn't match the schema you are declaring. I'd say that if you want to add this behavior, it should be optional, since another user might want to know if the schema of the csv matches exactly. For example, somebody might have inserted a new column, and it is good to know that the sizes don't match so you can check that.\r\n", "one line change in [decode_csv_op.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/decode_csv_op.cc)\r\nworks for me:\r\nchange line 172:\r\n`      while (static_cast<size_t>(current_idx) < input.size()) {`\r\nto \r\n` while (static_cast<size_t>(current_idx) < input.size() && result->size()<out_type_.size()) {\r\n`\r\n\r\nThe modified version (DecodeCSVFirstNOp) can be found here:  [DecodeCSVFirstNOp](https://github.com/cherishlc/tensorflow_ext/blob/master/CPPModules/decode_csv_first_n.cc)\r\n", "Cool! Could you prepare a pull request and contributing it back to core?", "@aselle  I have just developed [DecodeCSVSelectedColumnsOp](https://github.com/cherishlc/tensorflow_ext/blob/master/CPPModules/decode_csv_selected_columns.cc) \r\n\r\nI'd like to prepare a pull request,  but  not familiar with that. Learning to do pull request.", "Pull request is pending review.  ", "The PR was closed as it was adding the Op to core and not contrib.  It would be easy to move this to contrib using the code that was already written.  ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied."]}, {"number": 7511, "title": "non_max_suppression is very slow and doesn't appear to have a cuda or multi-threaded implementation", "body": "\r\nIt appears that tf.image.non_max_suppression currently takes about 200ms for about 8000 boxes, runs on a single CPU thread and doesn't have a GPU implementation.\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n8.0, 5.1.5\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.12.0-rc1\r\n\r\n", "comments": ["Is there any progress on this feature? ", "Also interested in this as well.", "Are there any good examples online on how to use tf.image.non_max_suppression ?? And does using this in-built nms function over our own function improves performance of the model ? ", "@harishannavajjala  The documentation is pretty clear IMHO, which part do you have questions?\r\nFor speed the answer is always the same: it depends on the speed of \"your own function\" (which only you know) plus the overhead to transfer data between TF and your own function.", "@ppwwyyxx also want GPU version of NMS? Does there any progress here?", "@ppwwyyxx when I use nms. I set the profile and find mns is in CPU.Is there any way to use NMS on GPU?", "This feature will be fantastic", "In [Work-efficient parallel non-maximum suppression for embedded GPU architectures](https://ieeexplore.ieee.org/document/7471831/) the authors describe how to bring NMS to the GPU.\r\n\r\nAlso does anyone know what the difference is between the implementation of NonMaxSuppressionV2 and NonMaxSuppression in Tensorflow?", "@jonla1 Someone has implemented a CUDA version based on the mentioned paper here:\r\nhttps://github.com/jeetkanjani7/Parallel_NMS\r\nAnd following is my PyCUDA version:\r\nhttps://github.com/keineahnung2345/Parallel_NMS/tree/pycuda/PyCUDA", "+1", "https://github.com/zengarden/light_head_rcnn/tree/master/lib/lib_kernel/lib_nms_dev\r\n\r\nHere is a GPU version which can be built for TF.", "There seems to be a related commit #28745"]}, {"number": 7480, "title": "preprocessor definition clash with glog", "body": "CHECK macros from `platform/logging.h` leak out into `core/public` headers which clash with users of glog.\r\n\r\nOne path is through `core/platform/allocator.h`:\r\n```\r\nIn file included from external/org_tensorflow/tensorflow/core/platform/logging.h:25:0,\r\n                 from external/org_tensorflow/tensorflow/core/framework/allocator.h:26,\r\n                 from external/org_tensorflow/tensorflow/core/framework/tensor.h:21,\r\n                 from external/org_tensorflow/tensorflow/core/public/session.h:23,\r\n(snip)\r\nexternal/org_tensorflow/tensorflow/core/platform/default/logging.h:224:0: note: this is the location of the previous definition\r\n #define CHECK_OP_LOG(name, op, val1, val2)                            \\\r\n ^\r\n```\r\nThis one is easy to fix by moving method implementation to allocator.cc.\r\n\r\nAnother is through `core/lib/core/status.h`.\r\n```\r\nIn file included from external/org_tensorflow/tensorflow/core/platform/logging.h:25:0,\r\n                 from external/org_tensorflow/tensorflow/core/lib/core/status.h:24,\r\n                 from external/org_tensorflow/tensorflow/core/lib/core/errors.h:19,\r\n                 from external/org_tensorflow/tensorflow/core/framework/tensor_shape.h:24,\r\n                 from external/org_tensorflow/tensorflow/core/framework/tensor.h:24,\r\n                 from external/org_tensorflow/tensorflow/core/public/session.h:23,\r\n```\r\n\r\nThis one is more work to fix because `TF_CHECK_OK` is used all over the code, but it does not seem to be necessary for `core/public`.\r\n", "comments": ["It looks like there's no need for `TF_CHECK_OK(...) << \"message\"`, so this could be replaced with\r\n```\r\n#define TF_CHECK_OK(val) val.CheckOk()\r\n```\r\n", "Seems like CHECK macros shouldn't be leaking into core/public headers, @drpngx -- do you know who is in charge of sealing core/public API?", "I see. Our C++ API is probably kind of porous right now. It will probably also include protobufs, `DISALLOW_COPY_AND_ASSIGN`, `RETURN_ON_ERROR`, and the rest of google3 paraphenelia, so this might not be the last of it.\r\n\r\n@asimshankar @jhseu @skye for the C++", "I assume `#undef` after the tensorflow include is a workaround?\r\n\r\nWe should write a tool to prefix these with TF_, probably with ClangMR.", "Echoing @drpngx, unfortunately our public headers #include many internals at the moment. Our current top priority for the C++ API is to straighten this out so only public symbols (and macros, etc.) are exposed.", "CC: @mrry for when we clean the linkage as well (`-fvisibility=hidden`). We'll need these `EXPORT_DLL` macros that set `__attribute__((visibility=\"default\"))`. LMK if you run into trouble with that.", "@szym is that still a problem?", "Yes, this is still a problem as of 1.4.1 release", "This will be solved as a part of the official C/C++ API. But until then should we keep it open, or dupe it against another issue?", "@yifeif has been making progress on cleaning up the headers. There's a lot to do.", "@szym  -  Hi, is this still an issue ? Feel free to close if it no longer exists with the latest tensorflow version.", "@yifeif  -  Any update on this ?", "Is this issue resolved? Please close If it was resolved already. Thanks!", "This is still an issue in 1.13. ", "We are still working on cleaning up headers, it is proving to be a real challenge as a lot of technical debt is accumulated.\r\n\r\nIn the meantime, if there are any proposals for a temporary solution, I am happy to discuss.", "Hello, I am working with glog 0.3.5 and TF 2.3.1 and I encountered this issue. Has it been solved anywhere yet or is it still relevant with 2.3.1 ? Thanks !\r\n![image](https://user-images.githubusercontent.com/19663407/97287292-b1841c00-1844-11eb-9837-85b0a404344c.png)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/7480\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/7480\">No</a>\n", "This issue is not resolved at all, as tensorflow is still defining its own log macros.\r\n\r\nHas anyone found a work-around? ", "I am working with TF 2.4.3\uff0c this issue is not resolved", "or-tools also have the same issue.", "> or-tools also have the same issue.\r\n\r\nHave you find a solution to use or-tools and Tensorflow in one bazel project?"]}, {"number": 7397, "title": "`tf.dynamic_stitch` gradient is incorrect", "body": "### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n``` python\r\nimport tensorflow as tf\r\n\r\nx = tf.zeros((1, 3))\r\ny = tf.dynamic_stitch([[0], [0]], [x, tf.ones((1, 3))])\r\n\r\nwith tf.Session() as sess:\r\n    print(\"y\")\r\n    print(sess.run(y))\r\n\r\n    analytic, numeric = tf.test.compute_gradient(x, (1, 3), y, (1, 3))\r\n    print(\"analytic\")\r\n    print(analytic)\r\n    print(\"numeric\")\r\n    print(numeric)\r\n```\r\n\r\ngives output\r\n```\r\ny\r\n[[ 1.  1.  1.]]\r\nanalytic\r\n[[ 1.  0.  0.]\r\n [ 0.  1.  0.]\r\n [ 0.  0.  1.]]\r\nnumeric\r\n[[ 0.  0.  0.]\r\n [ 0.  0.  0.]\r\n [ 0.  0.  0.]]\r\n```\r\n\r\nThe numeric gradient correctly shows that `x` has no impact on `y` (since the value of `x` is completely overwritten by a constant in the `dynamic_stitch`).  The analytic gradient is incorrect; it seems like the gradient calculation in `dynamic_stitch` does not handle the case where there are duplicate indices being merged.\r\n", "comments": ["Ug.  You're correct that the gradients are wrong, but I don't see how to fix it without a dramatic performance hit.  Do you have any suggestions?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "The bug still exists, meaning that the `tf.dynamic_stitch` gradients are incorrect.  Is there any other information I can provide that would be helpful?", "Let's leave this open.  Anyone interested should refer to the comments in https://github.com/tensorflow/tensorflow/pull/7487.  The next step would have been to add a new C++ kernel to speed up the bookkeeping required by accurate gradients.", "Can we close this?", "The gradient implementation is still incorrect, as of TF 2.5.0rc.  Here is an updated example showing the same error\r\n``` python\r\nimport tensorflow as tf\r\n\r\nx = tf.zeros((1, 3))\r\n\r\nanalytic, numeric = tf.test.compute_gradient(\r\n    lambda x: tf.dynamic_stitch([[0], [0]], [x, tf.ones((1, 3))]), [x]\r\n)\r\nprint(\"analytic\")\r\nprint(analytic)\r\nprint(\"numeric\")\r\nprint(numeric)\r\n```\r\ngives\r\n```\r\nanalytic\r\n(array([[1., 0., 0.],\r\n       [0., 1., 0.],\r\n       [0., 0., 1.]], dtype=float32),)\r\nnumeric\r\n(array([[0., 0., 0.],\r\n       [0., 0., 0.],\r\n       [0., 0., 0.]], dtype=float32),)\r\n```", "/cc @rthadur Can we update the label?"]}, {"number": 7004, "title": "Build with --define tensorflow_xsmm=1 fails on MacOS Sierra", "body": "Building current master after the recent commits to enable libxsmm.\r\n\r\n    bazel build --copt=-march=native -c opt --define tensorflow_xsmm=1 --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\nThe important error lines (I think) are:\r\n\r\n> tensorflow/core/kernels/sparse_matmul_op.cc:1408:3: error: unknown type name 'cpu_set_t'\r\n  cpu_set_t old_cpu_set;\r\n  \r\n> tensorflow/core/kernels/sparse_matmul_op.cc:1413:39: error: use of undeclared identifier 'cpu_set_t'\r\n    ret = sched_getaffinity(0, sizeof(cpu_set_t), &old_cpu_set);\r\n\r\nThe full result of --verbose_failures is attached.\r\n[tf_libxsmm_error.txt](https://github.com/tensorflow/tensorflow/files/721760/tf_libxsmm_error.txt)\r\n\r\nSearching for 'cpu_set_t' on OSX turned up [this code example under heading 2.2](https://github.com/Cibiv/NextGenMap/issues/6) and claims that 'cpu_set_t' is not defined on OSX and Apple's own multithreading code needs to be substituted. So maybe the code using 'cpu_set_t' is not meant to be reached on Macs and the libxsmm code is not ready to be used on my machine.\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@mikowals : Support for libxsmm is experimental (see description of commit https://github.com/tensorflow/tensorflow/commit/9a1e2d5d3d2c6420c410378c385b0c4665cedb9b), so we haven't gotten around to polishing it up.\r\n\r\nIf you'd like to contribute updates to make this more OS X friendly, do let us know and we can work it out.\r\n\r\nFYI @jewillco", "I think this is being worked on.\r\n@tfboyd has more context on this issue.", "This is still experimental and OS X support will be prioritized after Linux.  The contributions welcome label is intended to suggest that the core team is not likely to work on this feature.  We are hoping to get MKL running on OS X which will add a significant performance boost.  \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/10685", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@tfboyd Hi, are there any contributions for this till date or any implementations from the Tensorflow team? Is it still open for more contributions ?"]}, {"number": 6541, "title": "tf.signal CPU FFT implementation is slower than NumPy, PyTorch, etc.", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 16.04 LTS 64bit\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n`\r\n-rw-r--r-- 1 root root   558720 9\u6708  15 07:02 libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 9\u6708  15 07:05 libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 9\u6708  15 07:05 libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   415432 9\u6708  15 07:02 libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 9\u6708  15 07:02 libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 10\u6708 27 23:13 libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 10\u6708 27 23:13 libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 10\u6708 27 23:13 libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 10\u6708 27 23:13 libcudnn_static.a\r\n`\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n`0.12.head`\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\nwav = np.random.random_sample((1024,))\r\nspec = np.fft.fft(wav)[:513]\r\n\r\n\r\nx = tf.placeholder(dtype=tf.complex64, shape=[513])\r\nresult = tf.ifft(x) \r\nsess = tf.Session()\r\n\r\nstart = time.time()\r\nfor i in range(10000):\r\n    something = sess.run(result, feed_dict={x:spec})\r\nprint 'tensorflow:{}s'.format(time.time()-start)\r\n\r\nstart = time.time()\r\nfor i in range(10000):\r\n   \tsomething = np.fft.ifft(spec)\r\nprint 'numpy:{}s'.format(time.time() - start)\r\n\r\n```\r\n```\r\ntensorflow:25.7219519615s\r\nnumpy:0.391902923584s\r\n```\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Likely you are measuring the overhead of .run(), the copying of the feed variable spec which doesn't happen in numpy. In general, you will get good performance if each .run() does a significant portion of work. For example, I move your computation into a while loop inside tensorflow graph and get\r\n```\r\ntensorflow:0.0105199813843s\r\nnumpy:0.040412902832s\r\n```\r\n\r\nHere's the code:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\nniterations=1000\r\nN=1024\r\nwav = np.random.random_sample((N,))\r\nspec = np.fft.fft(wav)[:N/2+1]\r\n\r\nsess = tf.Session()\r\n\r\n# Create and initialize variables\r\ncnt = tf.Variable(tf.constant(niterations))\r\nspecVar = tf.Variable(spec, dtype=tf.complex64)\r\nsess.run(tf.variables_initializer([specVar,cnt]))\r\n\r\n# While loop that counts down to zero and computes reverse and forward fft's\r\ndef condition(x,cnt):\r\n  return cnt <= 0\r\n\r\ndef body(x,cnt):\r\n  xrev=tf.ifft(x)\r\n  xnew=tf.fft(xrev)\r\n  cntnew=cnt-1\r\n  return xnew, cntnew\r\n\r\nstart = time.time()\r\ntf.while_loop(condition, body, [specVar,cnt], parallel_iterations=1)\r\n\r\nprint 'tensorflow:{}s'.format(time.time()-start)\r\n\r\n# Equivalent numpy loop\r\nstart = time.time()\r\nx = spec\r\nfor i in range(niterations):\r\n   xrev = np.fft.ifft(x)\r\n   x= np.fft.fft(xrev)\r\n   \r\nprint 'numpy:{}s'.format(time.time() - start)\r\n\r\n```", "@aselle do you run the ops in your version or just construct them?", "Good catch @vrv, forgot the run() call. When putting in a run() call (see below code), I get \r\n```\r\ntensorflow:1.78562903404s\r\nnumpy:0.0487790107727s\r\n```\r\nwhich I suppose is comparible to your results (yours was numpy 66x faster, and mine was like numpy 33x faster). One explanation is that the GPU FFT implementation is really not tuned to smalls sizes, so that it can't achieve the same performance of the CPU FFT on a relatively small 513 element array. That's only 2 KBytes of data, which is not much for throughput optimized devices. As such I ran another one where I set `N=16384` which will basically use `8193` element array. Then I got\r\n```\r\ntensorflow:5.21\r\nnumpy:41.5\r\n```\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\nniterations=1000\r\nN=16384\r\nwav = np.random.random_sample((N,))\r\nspec = np.fft.fft(wav)[:N/2+1]\r\n\r\n\r\nwith tf.Session() as sess:\r\n  # Create and initialize variables\r\n  cnt = tf.Variable(tf.constant(niterations))\r\n  specVar = tf.Variable(spec, dtype=tf.complex64)\r\n  sess.run(tf.variables_initializer([specVar,cnt]))\r\n\r\n  # While loop that counts down to zero and computes reverse and forward fft's\r\n  def condition(x,cnt):\r\n    return cnt > 0\r\n\r\n  def body(x,cnt):\r\n    xrev=tf.ifft(x)\r\n    xnew=tf.fft(xrev)\r\n    cntnew=cnt-1\r\n    return xnew, cntnew\r\n\r\n  start = time.time()\r\n\r\n  final, cnt= tf.while_loop(condition, body, [specVar,cnt], parallel_iterations=1)\r\n  final, cnt =  sess.run([final,cnt])\r\n\r\n  print 'tensorflow:{}s'.format(time.time()-start)\r\n\r\n# Equivalent numpy loop\r\nstart = time.time()\r\nx = spec\r\nfor i in range(niterations):\r\n   xrev = np.fft.ifft(x)\r\n   x= np.fft.fft(xrev)\r\n   \r\nprint 'numpy:{}s'.format(time.time() - start)\r\n\r\n```", "Closing due to lack of recent activity. We will reopen when additional information becomes available. Thanks!", "I've done some simple profiling and it seems the CPU version of tf.spectrum.fft is slow in small `nfft` because of EIGEN fft function. The most time spent on the execution of CPU fft kernel is by EIGEN fft function.", "I have done some profiling of my own using the trace profiler and the results suggest that the `tensorflow` FFT2D is about 50 times slower than `numpy`'s.\r\n\r\nHere is what I have done (with IFFT2D but the results are the same for FFT2D):\r\n```python\r\nfrom keras.layers import Layer, Input, Lambda\r\nfrom keras.models import Model\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\nfrom tensorflow.signal import ifft2d\r\n\r\n\r\ndef tf_unmasked_adj_op(x):\r\n    return tf.expand_dims(ifft2d(x[..., 0]), axis=-1)\r\n\r\n# Model definition (we basically perform and inverse fft and then get the module)\r\ninput_size=(320, None, 1)\r\nkspace_input = Input(input_size, dtype='complex64', name='kspace_input')\r\nzero_filled = Lambda(tf_unmasked_adj_op, output_shape=input_size, name='ifft_simple')(kspace_input)\r\nzero_filled_abs = Lambda(tf.math.abs)(zero_filled)\r\nmodel = Model(inputs=kspace_input, outputs=zero_filled_abs)\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\n\r\nmodel.compile(\r\n    optimizer='adam',\r\n    loss='mse',\r\n    options=run_options,\r\n    run_metadata=run_metadata,\r\n)\r\n\r\n# fake data\r\ndata_x = np.random.rand(35, 320, 320, 1) + 1j * np.random.rand(35, 320, 320, 1)\r\ndata_y = np.random.rand(35, 320, 320, 1)\r\n\r\n\r\n# a single pass\r\nmodel.fit(\r\n    x=data_x, \r\n    y=data_y, \r\n    batch_size=35, \r\n    epochs=1,\r\n    verbose=2, \r\n    shuffle=False,\r\n)\r\n\r\n\r\n# profiling output\r\ntl = timeline.Timeline(run_metadata.step_stats)\r\nctf = tl.generate_chrome_trace_format()\r\nwith open('timeline_fft_test.json', 'w') as f:\r\n    f.write(ctf)\r\n```\r\n\r\nOn 8 CPUs (intel i7), ubuntu 16.04, tensorflow 1.14.0, keras 2.2.4, the fitting time is 5s. Looking into the timeline (chrome://tracing/), I find that the IFFT2D has a duration of **5000ms** (=5s).\r\n\r\nWith a GPU, on ubuntu 18.04, other versions similar, the IFFT2D takes **500ms**.\r\n\r\nAs a comparison, the `numpy` version is only **80ms**.\r\n\r\nHere is the code I used to perform the `numpy` profiling (in a jupyter notebook, with the same `data_x`):\r\n```python\r\ndef np_inv_fft(x):\r\n    return np.fft.ifft2(x)\r\n\r\n%%timeit\r\nfor i in range(len(data_x)):\r\n    np_inv_fft(np.squeeze(data_x[i]))\r\n```\r\n\r\nMaybe I am doing the profiling in a wrong way, but it does feel like the FFT2D is the bottleneck (and a very big one) in my actual models (this is just a minimal reproducible example).\r\n\r\nI haven't taken the time to see if it was due to the fourier transform 2D being in a `Lambda` layer in a `keras` model, but I guess it shouldn't. I will try to do this in pure `tensorflow` to see how it is.", "This is what I tried in pure `tensorflow` with the same results:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\nfrom tensorflow.signal import ifft2d\r\n\r\n\r\ndef tf_unmasked_adj_op(x):\r\n    return tf.expand_dims(ifft2d(x[..., 0]), axis=-1)\r\n\r\n# Model definition (we basically perform and inverse fft and then get the module)\r\nkspace_input = tf.placeholder(dtype='complex64', shape=(None, 320, None, 1))\r\nfake_output = tf.placeholder(dtype='float32', shape=(None, 320, None, 1))\r\nzero_filled = tf_unmasked_adj_op(kspace_input)\r\nzero_filled_abs = tf.math.abs(zero_filled)\r\n# this is just to have an adam optimizer similar to the keras training in order to keep things similar (we just compare the IFFT2D compute time in the timeline anyway)\r\nw = tf.Variable((1,), trainable=True, dtype=tf.float32) \r\nzero_filled_abs = tf.math.multiply(zero_filled_abs, w)\r\n\r\nloss = tf.losses.mean_squared_error(fake_output, zero_filled_abs)\r\n\r\nadam = tf.train.AdamOptimizer(learning_rate=0.3)\r\na = adam.minimize(loss, var_list=[w])\r\n\r\n\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\n\r\n# fake data\r\ndata_x = np.random.rand(35, 320, 320, 1) + 1j * np.random.rand(35, 320, 320, 1)\r\ndata_y = np.random.rand(35, 320, 320, 1)\r\n\r\n\r\n# a single pass\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(1):\r\n        sess.run(a, feed_dict={kspace_input: data_x, fake_output: data_y}, options=run_options, run_metadata=run_metadata)\r\n\r\n# profiling output\r\ntl = timeline.Timeline(run_metadata.step_stats)\r\nctf = tl.generate_chrome_trace_format()\r\nwith open('timeline_fft_test.json', 'w') as f:\r\n    f.write(ctf)\r\n```", "So I tried one further thing to see how big the problem was. \r\nI wrapped the `numpy` IFFT2D in a `tensorflow` layer using concepts from [`odl`](https://github.com/odlgroup/odl/blob/master/odl/contrib/tensorflow/layer.py) (itself inspired by gists shared in [#1095](https://github.com/tensorflow/tensorflow/issues/1095)).\r\n\r\nThis is the code I used to create the `tensorflow` layer:\r\n```python\r\nimport numpy as np\r\nimport uuid\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\n\r\n\r\n# Define custom py_func which takes also a grad op as argument:\r\ndef py_func(func, inp, Tout, name=None, grad=None):\r\n    if grad is None:\r\n        return tf.py_func(func, inp, Tout, stateful=False, name=name)\r\n    else:\r\n        override_name = 'PyFuncStateless'\r\n\r\n        # Need to generate a unique name to avoid duplicates:\r\n        rnd_name = override_name + 'Grad' + str(uuid.uuid4())\r\n\r\n        tf.RegisterGradient(rnd_name)(grad)\r\n        g = tf.get_default_graph()\r\n\r\n        with g.gradient_override_map({override_name: rnd_name}):\r\n            return tf.py_func(func, inp, Tout, stateful=False,\r\n                              name=name)\r\n\r\n\r\ndef fft_layer(mask, op_name='forward'):\r\n    def forward_op(imgs):\r\n        fft_coeffs = np.empty_like(imgs)\r\n        for i, img in enumerate(imgs):\r\n            fft_coeffs[i] = mask * np.fft.ifftshift(np.fft.fft2(np.fft.fftshift(img[..., 0]), norm='ortho'))[..., None]\r\n        return fft_coeffs\r\n\r\n    def adj_op(kspaces):\r\n        imgs = np.empty_like(kspaces)\r\n        for i, kspace in enumerate(kspaces):\r\n            masked_fft_coeffs = mask * kspace[..., 0]\r\n            imgs[i] = np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(masked_fft_coeffs), norm='ortho'))[..., None]\r\n        return imgs\r\n\r\n    if op_name == 'forward':\r\n        op = forward_op\r\n        grad_op = adj_op\r\n    else:\r\n        op = adj_op\r\n        grad_op = forward_op\r\n\r\n    def tf_grad_op(x, dy, name):\r\n        with tf.name_scope(name):\r\n            out_shape = x.get_shape()\r\n            with ops.name_scope(name + '_pyfunc', values=[x, dy]) as name_call:\r\n                result = py_func(\r\n                    grad_op,\r\n                    [dy],\r\n                    [tf.complex64],\r\n                    name=name_call,\r\n                )\r\n\r\n                # We must manually set the output shape since tensorflow cannot\r\n                # figure it out\r\n                result = result[0]\r\n                result.set_shape(out_shape)\r\n                return result\r\n\r\n    # Def custom square function using np.square instead of tf.square:\r\n    def tf_op(x, name=None):\r\n        with tf.name_scope(name, op_name, values=[x]) as name:\r\n            x_shape = x.get_shape()\r\n            def tensorflow_layer_grad(op, grad):\r\n                \"\"\"Thin wrapper for the gradient.\"\"\"\r\n                x = op.inputs[0]\r\n                return tf_grad_op(x, grad, name=name + '_grad')\r\n            with ops.name_scope(name + '_pyfunc', values=[x]) as name_call:\r\n                result = py_func(\r\n                    op,\r\n                    [x],\r\n                    [tf.complex64],\r\n                    name=name_call,\r\n                    grad=tensorflow_layer_grad,\r\n                )\r\n                # We must manually set the output shape since tensorflow cannot\r\n                # figure it out\r\n                result = result[0]\r\n                result.set_shape(x_shape)\r\n                return result\r\n\r\n    return tf_op\r\n```\r\n\r\n(a bit more complex than a simple inverse fourier transform but I need it for my application)\r\n\r\nEven with this slightly more complex function, in `numpy`, on CPU, the operation now takes **200ms**.\r\n\r\nI don't know if I am doing things incorrectly, if so please tell me. I don't think I could help solve this problem unfortunately as I don't know how to code in C (I guess if there is a problem it's related to the [eigen](https://github.com/OPM/eigen3/tree/master/Eigen) support of FFT2D).\r\n\r\n@aselle @vrv I see that you were involved in this discussion earlier, do you have any idea of what's going on? (sorry to ping you directly but this is really a bottleneck for me).\r\n\r\n@rryan I also know from my PR that you are involved with the fft stuff, do you have any idea of what's happening by any chance?", "The main problem with FFT ops in TensorFlow that makes them slow is that we compute the FFT plan on every execution instead of caching it for a given size. Due to the multi-threaded nature of op execution, nobody has done the work of implementing a plan cache that would be thread safe. Beyond this, Eigen's \"TensorFFT\" itself is not particularly fast when compared to other libraries like FFTW (which we can't use in TensorFlow due to lack legal approval). ", "But even for a batch size of 1 (i.e. the caching isn't involved), TensorFlow's FFT is still slower than numpy's wrapped (in a TensorFlow `py_func`) IFFT2D (130ms compared to 25ms on CPU).\r\nI also don't understand how the benchmark I did relates to @aselle 's one. Is it because I am considering 2D?", "> But even for a batch size of 1 (i.e. the caching isn't involved), TensorFlow's FFT is still slower than numpy's wrapped (in a TensorFlow `py_func`) IFFT2D (130ms compared to 25ms on CPU).\r\n> I also don't understand how the benchmark I did relates to @aselle 's one. Is it because I am considering 2D?\r\n\r\nEven at batch size 1, for every invocation of the op (i.e. sess.run call) we are re-computing a plan for the FFT, executing the 1 FFT, then returning. Thankfully when the batch size is greater than 1 we re-use the plan within that execution :), just not across executions. \r\n\r\nNumpy caches the FFT plan in a process-wide cache, so separate calls to ifft2d will re-use the plan made on the first call with that shape.", "It looks like you're producing trace timelines (great!) -- can you post them?\r\n\r\nI would like to see the actual measured time within the op instead of the overall time of calling sess.run (which as @aselle points out can be a source of measurement error)", "> It looks like you're producing trace timelines (great!) -- can you post them?\r\n\r\nSure. Attached you will find a zip [tf_fft_timelines.zip](https://github.com/tensorflow/tensorflow/files/3554383/tf_fft_timelines.zip) containing 4 files. They correspond to the experiments I showed above. Basically the network is doing an IFFT2D, masking the result, and taking the complex module. The batch size by default is 35. I am fitting the network, but since there is no parameters to fit, it's just doing a forward pass (results are roughly the same for only a batch prediction).\r\n- _timeline_fft_test_tf_bs1.json_ : the IFFT2D is done with `tf.signal.ifft2d` and the batch size is 1. Here the measured time for the op is 130ms.\r\n- _timeline_fft_test_tf.json_ : the IFFT2D is done with `tf.signal.ifft2d`. Here the measured time for the ops is 3900ms.\r\n- _timeline_fft_test_tf_numpy_bs1.json_ : the IFFT2D is done with the layer described in [this comment](https://github.com/tensorflow/tensorflow/issues/6541#issuecomment-524854368) (i.e. wrapping `np.fft.ifft2d` in a tensorFlow layer with some extra tweaks) and the batch size is 1. Here the measured time for the op is 28ms.\r\n- _timeline_fft_test_tf_numpy.json_ : the IFFT2D is done with the layer described in [this comment](https://github.com/tensorflow/tensorflow/issues/6541#issuecomment-524854368) (i.e. wrapping `np.fft.ifft2d` in a tensorFlow layer with some extra tweaks). Here the measured time for the op is 228ms.\r\n\r\nThis benchmark was done with Ubuntu 16.04, on CPU only.\r\n", "> * _timeline_fft_test_tf_bs1.json_ : the IFFT2D is done with `tf.signal.ifft2d` and the batch size is 1. Here the measured time for the op is 130ms.\r\n> * _timeline_fft_test_tf.json_ : the IFFT2D is done with `tf.signal.ifft2d`. Here the measured time for the ops is 3900ms.\r\n> * _timeline_fft_test_tf_numpy_bs1.json_ : the IFFT2D is done with the layer described in [this comment](https://github.com/tensorflow/tensorflow/issues/6541#issuecomment-524854368) (i.e. wrapping `np.fft.ifft2d` in a tensorFlow layer with some extra tweaks) and the batch size is 1. Here the measured time for the op is 28ms.\r\n> * _timeline_fft_test_tf_numpy.json_ : the IFFT2D is done with the layer described in [this comment](https://github.com/tensorflow/tensorflow/issues/6541#issuecomment-524854368) (i.e. wrapping `np.fft.ifft2d` in a tensorFlow layer with some extra tweaks). Here the measured time for the op is 228ms.\r\n\r\nThanks for that! What CPU are you using? Does it have AVX support and was TensorFlow built with AVX support? ", "> What CPU are you using?\r\n\r\nOutput of `lshw`:\r\n```\r\n*-cpu:0\r\n          product: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz\r\n          vendor: Intel Corp.\r\n          physical id: 6\r\n          bus info: cpu@0\r\n          size: 800MHz\r\n          capacity: 3GHz\r\n          width: 64 bits\r\n```\r\n\r\n> Does it have AVX support and was TensorFlow built with AVX support?\r\n\r\nI did not build TensorFlow from source so it was not built with AVX support. However, even when I use my GPU (Quadro P5000), the IFFT2D is still slower with `tf.signal.ifft2d` than with `np.fft.ifft2d` wrapped (don't know if it's supposed to make a big difference), but faster than on CPU.", "I just did another [simple benchmark](https://gist.github.com/zaccharieramzi/e1bec4fb51b7bf1703a7a7d8d52cf999), this time comparing TensorFlow (used with `keras`) to `pytorch` for the IFFT2D. I find that for prediction, `pytorch` is 40 times faster than `tensorflow` when using the IFFT2D.", "Thanks @zaccharieramzi!\r\n\r\nCould you please run a GPU benchmark as well? Based on [this](https://github.com/pytorch/pytorch/blob/71af7c46bb831330807b87776edfb770999d2aae/aten/src/ATen/native/SpectralOps.cpp#L21) I think PyTorch uses cuFFT and MKL, so on GPU TensorFlow and PyTorch should be using the same underlying FFT implementation. It would be nice to confirm whether or not there is a difference when both systems are using cuFFT because that would point to inefficiencies outside the particular FFT implementation that need to be addressed.", "I'm reopening because it is a known issue that TensorFlow's CPU FFT implementation is based on Eigen's TensorFFT, which is not fast compared to FFTW, MKL (what PyTorch uses), or FFTPACK (what NumPy uses). \r\n\r\nFor projects I work on at Google, we use TPUs and GPUs for FFTs in both training and serving, so I think we are not noticing the pain of the CPU FFTs being slow. Sorry about that :(. Unfortunately we are a little stuck in terms of legal approval for the above mentioned libraries. When we implemented tf.signal Eigen's TensorFFT was the best option in terms of features (non-power-of-2 FFTs via Bluestein's algorithm is a hard requirement) and legal compatibility.", "I am training a model on my main GPU right now, but as soon as I can I will run the same benchmarks. But as I recall (and wrote [here](https://github.com/tensorflow/tensorflow/issues/6541#issuecomment-524371619)), even with GPU, `tf.signal.ifft2d` is quite slow.\r\n\r\nI will come back with exact numbers coming from the trace timelines (and a zip with all of them).", "I ran the benchmarks on GPU. You can find a zip [tf_fft_timelines_gpu.zip](https://github.com/tensorflow/tensorflow/files/3559271/tf_fft_timelines_gpu.zip) attached with similar nomenclature [as before](https://github.com/tensorflow/tensorflow/issues/6541#issuecomment-526069901). (You will notice that only one run \"epoch\" appears in each trace, I did not find a way -did not look very hard as well- to have all the \"epochs\" in once trace)\r\n\r\nThe first \"epoch\" is way slower (that's why I did more than 1 \"epoch\" this time). Was the remark about not caching the plan only applicable to CPU?\r\n\r\nThis time the pure `tf.signal` op takes only **1.2 ms** on GPU after being cached (**500 ms** before being cached so still slower than `numpy` on CPU in that regard, to be considered in my case where all the images don't have the same shape) for a batch size of 35.\r\n\r\nI also enhanced my [`pytorch` vs `keras` benchmark](https://gist.github.com/zaccharieramzi/f0c4e8594727e87af8448449a8103c33), to feature trace timelines [keras_vs_pytorch_fft_timelines.zip](https://github.com/tensorflow/tensorflow/files/3559691/keras_vs_pytorch_fft_timelines.zip) for both for the forward passes, and taking into account caching. I think the IFFT2D times are equivalent when looking at the timelines (but there is some much stuff in there I am not sure how to read them, if you have a resource it would be welcome).\r\nHowever, when I look at the overall run time for, for example, prediction, `pytorch` is 4 times faster (7 times on fitting). I don't think it's only due to the fact that the tensors are already in GPU for `pytorch`. But maybe it's due to something else that I don't get.\r\n", "Thanks again @zaccharieramzi. While an end-to-end benchmark is useful, for the purpose of this bug I'm only concerned with the FFT ops themselves, so I think it could be simplified. \r\n\r\nSome issues I noticed:\r\n\r\n- The PyTorch benchmark starts with `x` already on the GPU, while the TensorFlow one has to transfer the NumPy array from host to device.\r\n- The PyTorch benchmark does not measure device to host time for the result tensor (the final `r` in the predict benchmark is a device = cuda object, while I believe Keras would implicitly fetch the tensor from device to host when running `predict_on_batch`.\r\n- The TensorFlow benchmark is running with tracing enabled -- tracing has some overhead, and so should not be enabled in a benchmark.\r\n- Instead of `%%time`, it could give a lower variance estimate if you use `%%timeit`. Both of these methods are subject to variance introduced by overheads in the Python interpreter, etc.\r\n\r\nHere's a more \"raw\" benchmark for TensorFlow that can give a better sense of what's going on.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.reset_default_graph()\r\n\r\nx_np = np.random.rand(35, 320, 320) + 1j * np.random.rand(35, 320, 320)\r\nwith tf.device('/gpu:0'):\r\n  x_tf = tf.constant(x_np)\r\n  y = tf.signal.ifft2d(x_tf)\r\n\r\nsess = tf.Session()\r\n```\r\nAnd then:\r\n\r\n```python\r\n%%timeit -n1000\r\nsess.run(y.op)  # Note the .op avoids fetching y from device to host.\r\n```\r\n\r\nOn a public Colab runtime with a K80 GPU, I'm observing a *very* high initial run time, followed by a steady result of:\r\n```\r\n1000 loops, best of 3: 3.47 ms per loop\r\n```\r\n\r\nCompared to PyTorch, which seems to be faster by about 2x:\r\n```python\r\nimport torch\r\ndtype = torch.float32\r\ndevice = torch.device(\"cuda\")\r\nx = torch.randn(35, 320, 320, 2, device=device, dtype=dtype)\r\n```\r\n\r\nAnd then:\r\n```python\r\n%%timeit -n1000\r\ny_pt = torch.ifft(x, 2)\r\n```\r\nI'm getting:\r\n```\r\n1000 loops, best of 3: 1.45 ms per loop\r\n```\r\n\r\nI believe this is apples-to-apples, since we're not measuring device/host transfer time, and the input shapes are the same.\r\n\r\nHere is a screenshot of a trace of the above TensorFlow graph executing on the same K80:\r\n\r\n![Screenshot 2019-08-30 at 9 48 06 AM](https://user-images.githubusercontent.com/26527/64038681-894c4080-cb0d-11e9-97b2-0388a0ee70f6.png)\r\n\r\nYou can see there are no memcpy D2H/H2Ds (good). You can see there is over 0.5 ms of time between the IFFT2D op execution (`GPU:0` on the bottom) and the CUDA stream beginning work (`stream:all`). Then the CUDA FFT kernel itself takes about 3ms total.\r\n\r\nI'll take a look at how PyTorch invokes cuFFT to see how they differ from TensorFlow. The performance should be identical on GPU.\r\n\r\nHere is the PyTorch trace of the above:\r\n\r\n![Screenshot 2019-08-30 at 10 16 45 AM](https://user-images.githubusercontent.com/26527/64039388-5b67fb80-cb0f-11e9-9a22-817cfcbc03de.png)\r\n\r\nI'm less familiar with how to interpret PyTorch traces, so I don't know how to figure out what the latency was between CPU scheduling the kernel and it starting, but it at least confirms the overall execution time took just over 2 ms, and I don't see anything that looks like a D2H/H2D copy.", "> While an end-to-end benchmark is useful, for the purpose of this bug I'm only concerned with the FFT ops themselves, so I think it could be simplified.\r\n\r\nTotally agree. Thanks for the remarks on the different aspects of the benchmark.\r\n\r\nHowever, the trace timelines I got (still relevant I think) are not exactly the same as yours for TensorFlow. For example, I only have 2 items in the CUDA stream.\r\nThe overall time taken by the IFFT (1.7ms) is also much closer to that of pytorch (1.4ms). Do you think it might be due to differences in the GPU we are using?\r\n\r\nAnyway, I do think your benchmark shows that it might be worth investigating.", "I slightly modified mentioned above script for pytorch:\r\n```python\r\nimport torch\r\ndtype = torch.float32\r\ndevice = torch.device(\"cuda\")\r\nfft_size = 1024 * 8\r\nbatch_size = 100 * 1024 * 1024 // fft_size # to fit into GPU memory\r\n```\r\nAnd then:\r\n```python\r\n%%timeit -n1 -r1\r\nx = torch.randn(batch_size,fft_size,2, device=device, dtype=dtype)\r\ny = torch.randn(batch_size,fft_size,2, device=device, dtype=dtype)\r\nfor i in range(100):\r\n    f = torch.fft(x, 1)   # 13*4=52 mults per complex sample\r\n    z = torch.mul(f, y)   #  4 mults per complex sample\r\n    x = torch.ifft(z, 1)  # 13*4=52 mults per complex sample\r\n```\r\nScript output after the first run after kernel restart in Jupyter notebook:\r\n`2.42 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)`\r\n\r\nScript output after the second and all other consecutive runs keeping all structures in GPU memory:\r\n`19.2 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)`\r\n\r\nBoth input arrays (x and y) have 100M complex FP32 samples each. If I calculated correct there are 52*2+4=106 FP32 mults per input sample. Taking into account 100 iterations in a loop I would estimate number of required operations as 100M * 100 * 106 = 1.06 TFLOP. \r\nI have GTX 1070 installed on my PC and by spec it can perform up to 6.5 TFLOPS for FP32 operations. Looks like I did some mistake in my calculations because script supposedly performs 1.06 TFLOP each 19.2 ms which can't be correct.\r\nWould anyone point out on my mistake, please?\r\n", "Removing the support label, as this describes a real issue with tensorflow that is not yet resolved. ", "It's still an issue -- bad bot, @tensorflowbutler! ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I think this is a very relevant issue since almost all speech applications need this . \r\nI developed some model for speech recognition and in branches 1.13.1 or 1.14, with the contrib spectrogram implementation, run like 10x or 20x faster than with tf 2.x ... with the signal.stft implementation. \r\n\r\n\r\n ", "Here is a simple benchmark comparing tensorflow and numpy's real-valued 1D FFT on CPU, showing that TF (version 2.1) is slower than numpy by a factor of 10:\r\nhttps://colab.research.google.com/gist/Andreas5739738/fc603468829ee0fc7e40a2e27d8a6661/fft.ipynb\r\n\r\n```\r\nimport timeit\r\n\r\nprint(timeit.timeit('X = tf.signal.rfft(x)', setup='import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))\r\nprint(timeit.timeit('X = numpy.fft.rfft(x)', setup='import numpy.fft; import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))\r\n```\r\n```\r\n24.331672433999984\r\n2.2700748900000463\r\n```", "Could we expect any update on it? CPU would be pretty popular for inference and improving it from 10+times speech difference would be great.", "@rryan Are there any updates on this ?\r\nI found myself confronted to this problem again while trying to optimize some of my input data pipelines, which are placed on CPU but still need to perform fft-like operations.\r\n\r\nBtw, a hack I hadn't thought about beforehand is to use [`map_fn`](https://www.tensorflow.org/api_docs/python/tf/map_fn) if you are using the Fast Fourier Transform operations in batches on CPU in an architecture with multiple cores. Depending on your batch size and architecture it could greatly reduce the computation time.", "Actually, using this idea, you can build a much faster FFT for dimension 2 or higher.\r\nHere is an example for 2D:\r\n\r\n```python\r\nimport multiprocessing\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.signal.fft_ops import fft2d, fft\r\n\r\n\r\n@tf.function\r\ndef parallel_fft2d(image):\r\n    partial_fourier_coefs = tf.map_fn(\r\n        fft,\r\n        image,\r\n        parallel_iterations=multiprocessing.cpu_count(),\r\n    )\r\n    partial_fourier_coefs_t = tf.transpose(partial_fourier_coefs)\r\n    fourier_coefs_t = tf.map_fn(\r\n        fft,\r\n        partial_fourier_coefs_t,\r\n        parallel_iterations=multiprocessing.cpu_count(),\r\n    )\r\n    fourier_coefs = tf.transpose(fourier_coefs_t)\r\n    return fourier_coefs\r\n\r\na = tf.cast(tf.random.normal([320, 320]), dtype=tf.complex64)\r\ntf.test.TestCase().assertAllClose(\r\n    fft2d(a),\r\n    parallel_fft2d(a),\r\n    rtol=1e-4,\r\n    atol=1e-3,\r\n)\r\n```\r\n\r\nThe speed-up is only visible on non-power of 2 shapes, and in this case, on my machine with 8 cores, you can go from `150ms` to `30ms`. My guess is the speed-up will be even more significant if you have more cores and higher dimensions.", "Any idea on how to use this [map_fn](https://www.tensorflow.org/api_docs/python/tf/map_fn) function to improve the stft instead of the fft2d, @zaccharieramzi? ", "Hi @JPery ,\r\n\r\nI seem to notice from your issue [here](https://github.com/keunwoochoi/kapre/issues/98), that you are interested in the stft for audio processing, so we are talking 1D stft.\r\nFor 1D applications, unfortunately, the only way to currently benefit from the `map_fn` is to compute the transform in batches and have each element in the batch be transformed separately in parallel (the more cores you have the more acceleration you will get basically up to your batch size).\r\n\r\nThe idea behind the acceleration for 2D transforms is that you can write the application of the 2D transform as successive applications of the 1D transform, on each dimension.\r\n\r\n\r\nAfter reading the [code of the stft](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/ops/signal/spectral_ops.py#L38-L96), there actually is a way to do it for the stft but it's a bit involved.\r\nSo my basic understanding of the stft, is that you will compute an fft (or rftt in this case) over multiple segments (or frames) of your input signal.\r\nYou could see these segments as multiple elements in a batch, and you can definitely try to transform those in parallel. \r\nSchematically, this would mean replacing [this line](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/ops/signal/spectral_ops.py#L96), by something like:\r\n\r\n```python\r\nfrom functools import partial\r\n\r\nreturn tf.map_fn(\r\n\tpartial(fft_ops.rfft, fft_length=[fft_length]),\r\n\tframed_signals,\r\n\tparallel_iterations=multiprocessing.cpu_count(),  # or how many parallel ops you see fit\r\n)\r\n``` \r\n\r\nSo you could just copy-paste the stft code and replace the final line, you should see a speed-up related to the number of frames you consider and the number of cpus you have at hand.", "Thank you for your comment, @zaccharieramzi. \r\n\r\nWith your implementation I was able to perform up to 10 times faster in my use case.\r\n\r\nIt works like a charm!", "Glad I could help! \r\nI hope this idea can help others to enhance their CPU-Fourier workflow, until this issue is resolved.\r\n\r\nIt could maybe be directly implemented in TF if the [legalities](https://github.com/tensorflow/tensorflow/issues/6541#issuecomment-526239900) are going to block any further improvement. ", "@diggerdu,\r\nSorry for the delayed response. I have executed your code in the latest version of **`Tensorflow (2.5)`** and observed that **`Tensorflow`** is much faster compared to **`Numpy`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/b43e5da93fe6f665800d13e65820cdd6/gh_6541.ipynb) of the working code. Thanks!", "@rmothukuru the gist you linked does not seem to be a proper benchmark since it only executes variable assignment in a loop, no computation.\r\n\r\nHere is an example demonstrating that TF 2.5 is still 10x slower than numpy:\r\nhttps://colab.research.google.com/gist/Andreas5739738/fc603468829ee0fc7e40a2e27d8a6661/fft.ipynb", "I added jax to the comparison, which is also much faster than tensorflow:\r\n\r\nseconds (lower is better):\r\nTensorflow 2.5.0: 23.75205922899977\r\nNumpy:  2.10841278099997\r\nJax:  1.2626468630001\r\n\r\njax uses pocketfft (https://jax.readthedocs.io/en/latest/_modules/jax/_src/lax/fft.html#fft), which seems to be much faster then Tensorflow's Eigen.\r\n\r\nUpdated colab: https://colab.research.google.com/gist/Andreas5739738/fc603468829ee0fc7e40a2e27d8a6661/fft.ipynb", "Yes ! Inline with @Andreas5739738 Gist , was able to replicate the issue in CPU environment  for TF 2.5,But in GPU environment it is approx 405 times faster than numpy , providing [Gist ](https://colab.research.google.com/gist/mohantym/fecf2c6acf4e78ba63bdf35789b58f83/fft.ipynb#scrollTo=xWKC7vEbKIQz)for reference . Thanks!", "The situation has not improved with TF 2.6:\r\n```\r\nseconds (lower is better):\r\n\r\nTensorflow 2.6.0 26.172108803000015\r\nNumpy:  1.4308665990000122\r\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\r\nJax:  1.412082421999969\r\n```", "This is still an issue in current tf-nightly (2.8+).\r\n\r\nAlso noticed something interesting: when using double precision, TF is more than twice as fast as with single precision (although still ~8x slower than Jax):\r\n\r\n```\r\nprint(\"seconds (lower is better):\")\r\nprint(f\"Tensorflow {tf.__version__}\", timeit.timeit('X = tf.signal.rfft(x)', setup='import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))\r\nprint(f\"Tensorflow {tf.__version__}, double precision\", timeit.timeit('X = tf.cast(tf.signal.rfft(tf.cast(x, tf.float64)), tf.complex64)', setup='import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))\r\nprint(\"Numpy: \", timeit.timeit('X = numpy.fft.rfft(x)', setup='import numpy.fft; import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))\r\nprint(\"Jax: \", timeit.timeit('jnp.fft.rfft(x).block_until_ready()', setup='import jax.numpy as jnp; import tensorflow as tf; x = tf.random.normal([50000, 512]).numpy()', number=10))\r\n```\r\n\r\n```\r\nseconds (lower is better):\r\nTensorflow 2.9.0-dev20220126 24.444629474999942\r\nTensorflow 2.9.0-dev20220126, double precision 11.099884848999977\r\nNumpy:  2.1127802319999773\r\nJax:  1.2639448529999981\r\n```\r\nhttps://colab.research.google.com/gist/Andreas5739738/fc603468829ee0fc7e40a2e27d8a6661/fft.ipynb", "Looks like Jax team found the same issue with XLA FFT slowness, and integrated PocketFFT as a workaround: https://github.com/google/jax/issues/2952\r\nWould be great if the pocketfft OP could also be integrated into XLA itself, so that both TF and Jax benefit from the speedup.", "Is there any workaround for GPU? Most of my computational cost is just going to compute FFT. PyTorch's implementation is also very fast."]}, {"number": 6504, "title": "Feature Request: Gradient for QR op", "body": "The QR op is currently implemented but has no gradient.  It would be very useful if the gradient were defined so that the op could be used in networks and cost functions.\r\n", "comments": ["I am not aware of anybody working on this ATM. Care to send us a PR @kstant0725 ?", "@kstant0725 Are there any cost functions/models that would benefit from QR decomposition? How else could this be used in training a network?", "@rmlarsen and @krishpop The need for a QR decomposition came up my research when we wanted to implement a network layer that acts to orthonormalize the layer below it.  This can be a useful constraint for  training networks to find eigenfunctions.  SVD could also accomplish this, but has a higher computational cost, and imposes an additional rotational constraint. ", "@kstant0725 Have you found a gradient implement for QR decomposition?", "I have checked the implement detail of QR decomposition in tensorflow. The QR decomposition in tensorflow simply calls the QR function in Eigen. But unfortunately there is no gradient implement for QR decomposition in Eigen, which indirectly results in the absence of gradient for QR decomposition in tensorflow. The QR decomposition is based on some basic matrix operations. Thus it's theoretically differentiable. The typical QR decomposition methods like Givens Rotations and Householder reflections include an iterative process, which makes the gradient very complex. There is no automatic gradient mechanism in Eigen. I think that's why there is no gradient implement for QR decomposition in Eigen. Based on the above observation, it's better to implement QR decomposition algorithm using available tensorflow operations and make use of the automatic gradient mechanism in tensorflow to obtain the gradient for QR decomposition. What do you think? @kstant0725 @rmlarsen ", "So far I have been using the following to orthogonalize:\r\n\r\n    AA =tf.matmul(tf.transpose(A),A)\r\n    L = tf.cholesky(AA)\r\n    U = K.transpose(tf.matrix_solve(L,tf.transpose(A)))\r\n\r\nThis works well most of the time unless the matrix A is ill conditioned or not full rank.  Do you have any suggestions for an alternative which will wont throw an exception when the matrix is not full rank?", "I tried a simple implementation of  the gradient expression based on an article and it seemed to match the numerical gradients for the QR decomposition of tall and square random matrices. For fat ones (width>height) the results weren't as good, which might be associated with some assumption in the reference, numerical difficulties (since it requires the computation of a pseudo inverse instead of an exact one) or a mistake in the code.\r\n\r\nAny suggestions?\r\n\r\n```python\r\nfrom tensorflow.python.framework import ops\r\n@ops.RegisterGradient(\"Qr\")\r\ndef _qr_grad(op, Qb,Rb):\r\n  \"\"\"Computes and returns the gradient of a QR decomposition\r\n\r\n  computes the expression (in Matlab terms)\r\n  Q*(Rb+( Pl.*(R*Rb'-Rb*R'+Q'*Qb-Qb'*Q))pinv(R)')\r\n  where Pl is 1 below the diagonal and 0 elsewhere\r\n\r\n  reference in http://dx.doi.org/10.1080/10556788.2011.610454 eq (13)\r\n\r\n  Args:\r\n    op: operation\r\n    Qb: gradient in Q\r\n    Rb: gradient in R\r\n\r\n  Returns:\r\n    Ab: gradient in Q*R\r\n  \"\"\"\r\n\r\n  Q = op.outputs[0]\r\n  R = op.outputs[1]\r\n  batch_shape = Q.shape[0:-2]\r\n  tr_ord = list(range(len(Q.shape)))\r\n  tr_ord[-1],tr_ord[-2] = tr_ord[-2],tr_ord[-1]\r\n\r\n  M1 =    tf.matmul(R,tf.transpose(Rb,tr_ord))\\\r\n        - tf.matmul(Rb,tf.transpose(R,tr_ord))\\\r\n        + tf.matmul(tf.transpose(Q,tr_ord),Qb)\\\r\n        - tf.matmul(tf.transpose(Qb,tr_ord),Q)\r\n\r\n  # lower triangular part without the diagonal\r\n  # equivalent to Haddamard product with Pl\r\n  M1_low = tf.matrix_band_part(M1,-1,0)-tf.matrix_band_part(M1,0,0)\r\n\r\n  # equivalent to explicitly computing the pseudo inverse\r\n  # ideally we should use the fact that R is triangular\r\n  M2_tr = tf.matrix_solve_ls(r,tf.transpose(M1_low,tr_ord),fast=False)\r\n\r\n\r\n  M2 = tf.transpose(M2_tr,tr_ord)\r\n  Ab = tf.matmul(Q,Rb+M2)\r\n  return Ab\r\n```", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", " @D-Roberts Many thanks for you implementation :pray: Works like a charm! Any chance that there will also be a gradient implementation for `nrows > ncols` and `full_matrices = True` soon? I am currently looking for a TF option to get the basis of the null space of a deep matrix, but the usual candidates (Q_2 in [Q_1 Q_2] [R_1^T R_2^T]^T in the QR decomposition or U in U Sigma V^T in the SVD decomposition) are not supported.", "@davidruegamer Glad to hear this is useful. As much as I enjoy developing tf open source, I don't have current plans for the full_matrices case. However, there are some theoretical considerations (unpublished yet) around it that I am happy to share if you'd like to email me at denisa.roberts[at]denisaroberts.me ."]}, {"number": 6230, "title": "Poor colors for embedding projector", "body": "The second and third colors available in \"color by\" are extremely similar when there is overlap: the second color (orangish) appears very similar to third (redish) one.   This prevents any pop-out effect.", "comments": ["Being able to customise the colours would be a massive improvement too.", "When there are multiple values, I wish it could display color by gradient scale.\r\nThe color now only works as categoy. I cannot represent continuous values.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "This would be a great external contribution - would be great to also have feedback from a color blind person, so we can make a color-blind friendly palette.", "Closing due to lack of recent activity , but please let me know if I'm mistaken. Thanks!", "Any leads on this feature?\r\n"]}, {"number": 6219, "title": "Allow using raw values for Projector?", "body": "It would be great if Projector would allow raw values for visualization (e.g. in case the data has PCA or t-SNE applied already )", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "This would be a great external contribution!", "Closing due to lack of recent activity, but please let me know if I'm mistaken."]}, {"number": 5965, "title": "Feature request: GPU ops for tf.unique; tf.where; and tf.dynamic_partition ", "body": "This is a feature request.  As far as I know, all three of them currently do not have GPU ops.\r\n\r\nIt seems that if we can at least get a GPU implementation of `tf.unique` for integers, then the user can make `tf.where` and `tf.dynamic_partition` manually.  For those of us who are trying to build models that want to mess around with indices rather frequently, this would be incredibly helpful. ", "comments": ["I marked this contribution-welcome. GPU is not particularly fast for those operations. But it is conceivable that it is faster than copying the data from GPU-to-CPU, operate on CPU, and then copy the data back. \r\n\r\nThe kernel itself may need some consideration. For tf.unique, if the range is limited, we can first convert it into a bit-mask, and then gather the active location, through either atomic operations, or a multiple-pass algorithm that figure out the size with preprocessing. \r\n\r\nAs for TensorFlow itself, it is a bit tricky to deal with variable-size output Tensor on GPU. I think the best way is to mark them as async-op and wait for the result to come back and then set the tensor size. I can elaborate it a bit more if someone is interested in taking this up.\r\n", "Hi @zheng-xq, I would like to work on this.\r\n\r\nI was trying to read the source code to understand how to add GPU ops for `tf.unique`. \r\nAfter reading different files like `argmax_op.cc` and `softmax_op.cc`, I understood that I need to add `Device` and `functor`. `Functor` will have the logic for the GPU op. Please correct me if I'm wrong.\r\n\r\nI read `unique_op.cc` and understood the current logic which is a multiple-pass algorithm. I wanted to know if the new `functor` will have the same logic.\r\nThen, I read `where_op.cc` and  found out that there's already a GPU logic for it. So, I think, we don't need any changes in that.\r\n\r\nThanks! :)\r\n", "Probably the best general purpose way (arbitrary data types, arbitrary range) to write unique on gpus is to do a stable sort (that also maintains the index) and then extract the first element from each grouping.\r\n\r\nthrust::sort and thrust::unique would do this.  Thrust is already a part of cuda.  The radix sort that would be used the vast majority of time would naturally take advantage of input that has a limited range by limiting the number of passes of the sort.\r\n\r\nDownside of thrust::sort is that it is out of place and would require additional memory.  I'm not aware of any high performance sort implementations on the GPU that are in-place.", "I feel that that the primary practical concern being addressed is the gpu-cpu-gpu latency; the actual time/memory costof `thrust::sort` itself is probably the lesser concern.", "I agree that running it on the GPU will likely be faster than the GPU->CPU->GPU pipeline even for non-optimal implementations.  But, the additional memory requirement could break code that works today which might upset some people...", "Note that you would also need to do this: https://github.com/thrust/thrust/blob/1.6.0/examples/cuda/custom_temporary_allocation.cu\r\n\r\nto get thrust to use the TF allocator for the temporary storage that it will allocate.", "Has this been implemented yet?", "tf.where is done.\r\n\r\ntf.unique probably cannot be done - it is already registered as a GPU op, but the calculation happens on the CPU - moving this to the GPU would probably slow down some tiny use cases.  It would be best to create a new op in contrib - unique_gpu or something.\r\n\r\ntf.dynamic_partition is not implemented yet.  The easiest way to do it would be to use CUB's radix sort, then scan, then gather.  You could also try to implement the calculation of the output indices as one scan if the total number of partitions was not too large with a custom fancy iterator.", "@ekelsen I've been looking for GPU implementation of tf.unique, and it seems one could implement non-stable variant with hash tables using tf operations like scatter_nd_update and gather_nd. However, I don't know if these operations run on GPU?\r\n\r\nFurthermore, a unique operation that is applied to each item of a minibatch separately complicates things, as scatter works with tf.Variable, but map_fn will produce tensor outputs.", "@dantkz I think the best bet for a GPU version of unique would still be to create a separate op in contrib so that it doesn't mess up the current registration on the host.\r\n\r\nscatter_nd and gather_nd do have GPU registrations.  You could try to write a unique using these.  However, I think that using the CUB techniques I outlined would probably be give the best performance across a wide range of input data distributions.", "Hey folks, is it the case that only work related to tf.dynamic_partition is left? ", "@ekelsen \r\nIt seems that the GPU kernel of tf.unique has memory leak. https://github.com/tensorflow/tensorflow/issues/17155", "@ekelsen @zheng-xq  I recently did some work add a GPU kernel for tf.unique op, I did local benchmark of original c++ hash table and GPU version, it's about 75-100x faster(not include memory copy time). since during training, tf.unique will take most the time of Opmimizer, I think it will help improve the training performance. \r\n\r\nalthough I noticed there is some comments in `unique_op.cc`:\r\n\r\n```\r\n// Fake integer GPU kernels so that the use of Unique in optimizers (to\r\n// de-duplicate sparse gradient indices) does not conflict with gradients being\r\n// located on a GPU. These kernels run on the CPU, their inputs and outputs\r\n// residing in host (not GPU) memory.\r\n```\r\n\r\nI'm not sure what this comment means, did it means this op cannot implement in GPU ? \r\nbtw, I did this work and test based on Unique op (not the recently added UniqueV2 Op).\r\n\r\n", "It means that we actually would really like GPU kernels for unique, but have to resort to running CPU implementations because there is no GPU option.  If you have a GPU implementation, then that would be great, please submit a CL.", "Hi! any progress here? tf.unique  is very time consuming on CPU. Just for reference, torch has such op on GPU [torch.unique](https://pytorch.org/docs/master/torch.html#torch.unique) which also supports dim for applying unique. ", "tf.unique on GPU  +1", "Hi @zheng-xq , I write tf.unique GPU code, but it can't run concurrently. For 55 independent unique tasks, it runs one by one on the GPU. It seems all tasks  on GPU run sequentially. The reason is tensorflow only creates one compute-stream as you mentioned at [stackoverflow](https://stackoverflow.com/a/36108627/5193007). Tensorflow can run GPU op concurrently?", "@fightbob as far as I know, tf.unique always runs on CPU, even if you force-place it on GPU.", "> @fightbob as far as I know, tf.unique always runs on CPU, even if you force-place it on GPU.\r\n\r\n@dantkz Yes,  always run on CPU, guessing it's associated with TF Op Register mechanism.  I remove all tf.Unique cpu codes, and only stay gpu code. It's tricky but works. If tf.Unique can run parallelly on mutiple gpu stream, I will take better ways instead of the tricky. Do you know how to call TF op concurrently on multiple stream?", "any news? This is the major performance hit our pipeline takes during training. Approximately 50 % of training time is spent in the unique_with_counts op. That it can be done efficiently has been demonstrated by pytorch.", "@sanjoy Can you comment on this?\r\n", "We (TensorFlow team) don't have enough cycles to work on this right now, but I'll happily review a PR."]}, {"number": 5443, "title": "einsum with ellipses \"...\" (indefinite number of axes)", "body": "Thank you very much for providing the (numpy) einsum feature in tensorflow, that is really great. The documentation for einsum says to look at the numpy documentation as it provides the same api. It is not exactly the same, one difference is the possibility of using \"...\" in numpy which seems not to be implemented in tensorflow. This would definitely be a nice feature to have in tensorflow also since that way one could build various function/transformations in tensorflow which are not dependent on the number of axis the tensor has (i.e. the specific use case). \r\n\r\nHere is the error I encountered when trying to do that:\r\n`d = tf.einsum(\"i...,ij->j...\",c,b)`\r\n\r\n> ---------------------------------------------------------------------------\r\n> AssertionError                            Traceback (most recent call last)\r\n> <ipython-input-12-d7c0123ec21a> in <module>()\r\n> ----> 1 d = tf.einsum(\"i...,ij->j...\",c,b)\r\n> \r\n> //anaconda/envs/chaos/lib/python3.5/site-packages/tensorflow/python/ops/special_math_ops.py in einsum(axes, *inputs)\r\n>     100   match = re.match('([a-z,]+)->([a-z]+)', axes)\r\n>     101   assert match, \\\r\n> --> 102     \"Indices have incorrect format: %s\" % axes\r\n>     103 \r\n>     104   inputs = list(inputs)\r\n> \r\n> AssertionError: Indices have incorrect format: i...,ij->j...", "comments": ["@nikhilmishra000 I believe you implemented it? Maybe you know of a work around in the meantime? \n", "einsum is known to be not fully implemented (see #4722). We welcome continued contributions to improve it (as it was a contribution in of itself). Thanks for bringing this up.\n", "@isabeaups, see the following from the docstring about the current implementation:\n\n```\nLike `numpy.einsum`, but does not support:\n* Ellipses (subscripts like `ij...,jk...->ik...`)\n* Subscripts where an axis appears more than once for a single input (e.g. `ijj,jk->ik`).\n```\n\nIt shouldn't be too hard to add some of this functionality, if you wanted to take a stab at it :D \n", "Thanks @nikhilmishra000. I don't really know C++, but I might take a look at it. \n", "@nikhilmishra000 I just looked at the code and it is in python so that is good. If you could maybe explain to me the code (once the indices and axes are defined, I get that from the code, how do you contract them) then I would be happy to add this functionality to it. \n", "Closing due to lack of recent activity, but please let me know if I'm mistaken. Thanks!", "@isabeaups Is this still an issue ?\r\nCould you please have a look at this [thread](https://stackoverflow.com/questions/55549264/ultimate-flexibility-with-einsum-ellipses) and refer to this [link](https://www.tensorflow.org/api_docs/python/tf/einsum) ,please let us know if it helps?\r\nThanks!", "@sushreebarsa  Thanks. I don't know if this is still an issue, I have not used TF for years now since I moved to PyTorch "]}, {"number": 5193, "title": "CuSPARSE support", "body": "For some machine learning /algorithm problems, we require to multiply matrices with vectors. In some cases - especially those associated with large graph analytics, the matrices are sparse. Given the major limitation with GPUs is the limited GPU memory, representing the matrix in sparse format is vital.\n\nTo this end, the CUSPARSE libraries (http://docs.nvidia.com/cuda/cusparse/) provide a suitable object model and various functions for sparse matrix/vector operations.\n\nI would therefore like to request a basic CUSPARSE implementation. \ne.g. - \n- CSR matrix representation - (see http://docs.nvidia.com/cuda/cusparse/#compressed-sparse-row-format-csr)\n- basic matrix-vector multiplication for CSR representations (see http://docs.nvidia.com/cuda/cusparse/#cusparse-lt-t-gt-csrmv)\n\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Yes, that would be great. Is that something you would be willing to contribute?\n", "@martinwicke @vincentvanhoucke for comments\n", "Maybe interested parties: @rmlarsen @aselle\n", "We have sparse matrix support here:\nhttps://www.tensorflow.org/versions/r0.11/api_docs/python/sparse_ops.html\nI don't know what the interop story with CSR representations in general and CUSPARSE in particular might look like.\n", "I'm definitely interested in more support for accelerated sparse matrix formats like CSR. It will likely not appear soon, and we still are thinking about design and impact of such a feature. In particular, we want to make sure there is some indication it wil be generally useful and impactful and enable apps that were hard to do otherwise.  Any further information on use case and your experience with our existing index based sparse matrix format would be very helpful. \n", "@mhobby, Did you manage to work around this in anyway, or do you still need this support?", "HI @aselle - we found a work around by using Julia which has direct implementation/wrapper of the CSR routines in CUDA.\r\nHas there been any further progress in tensorflow for this?", "There really has not been any application that has driven that forward for us as of yet. So, for now I am marking this contributions welcome, although there would likely need to be a pretty good design document to make a case for way to integrating this into TensorFlow elegantly to go with any effort to solve the feature.", "Hi @aselle , I am also looking forward to having the support from tf to accelerate sparse matrix computations. sparse_matmul op is heavily used in my case, but this sparse op cannot compete with the normal matmul op on CPU with SSE support, not to say on GPU. I also find sparse_matmul op cannot run on GPU. \r\n\r\nI can say my motivation to have sparse matrix support. For neural network, many papers have proven neuron weights can be pruned, thus the matmul turns to be sparse matrix multiply, which gives speedup space and saves energy consumption. ", "Hi @aselle, could you tell me more about how to contribute ? After searching TF, Mxnet and Caffe,  I find TF has done the most on sparse matrix support. Although TF support sparse tensors, but it still lacks optimization and acceleration for sparse matrix ops. So I hope to do something to improve sparse matrix computations, which will also benefit my daytime work. If someone has already started on this issue,   please let me known that I can join. \r\nLooking forward to your feedback. ", "I'm looking into this, adding cusparse support is nontrivial.  I wouldn't expect anything until August at the earliest.  For now we go have a GPU accelerated sparse_tensor_dense_matmul.", "(it's not trivial because the primitives don't have batched computation\nsupport)\n\nOn Jul 3, 2017 10:40 AM, \"ebrevdo\" <notifications@github.com> wrote:\n\n> I'm looking into this, adding cusparse support is nontrivial. I wouldn't\n> expect anything until August at the earliest. For now we go have a GPU\n> accelerated sparse_tensor_dense_matmul.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5193#issuecomment-312701554>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0j0DSIDbd7wtF5pbEbiHAViOQqFks5sKSd5gaJpZM4Kf5xR>\n> .\n>\n", "Hi, Qing @qingyangqing , in the density of pruned neural networks (about 10%-30%), sparse-dense matrix multiplication **cannot** outperform dense-dense matrix multiplication both in GPU and CPU. Although the sparse method reduces lots of computation, the utilization of CPU/GPU is much lower than that of dense-dense matrix multiplication. According to experiments, GEMM can achieve up to 90%+ peak flops and CSRMM can only achieve <5% peak flops in GPUs.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Closing due to lack of recent activity, but please let me know if I'm mistaken. Thanks!", "OpenAI has recently published a library for GPU-accelerated block-sparse operations: https://github.com/openai/blocksparse **Linux only**\r\n\r\nThe lowest allowed block size for their matrix multiplication is 8. Block size 1 would make it properly sparse, but I guess that was too inefficient, and not necessary to achieve benefits."]}, {"number": 4722, "title": "einsum not fully implemented", "body": "I am glad to see the newly added einsum function. The documentation claims that its usage is the same as numpy. However, it can do almost nothing as compared to numpy. For example, it only supports subscripts in the form of '_->_'. Unfortunately, even matrix transpose does not work, i.e., 'ij->ji'. \nNumpy works:\n\n```\n>>> A\narray([[ 0.3828997 , -0.39114848, -0.09727838, -0.20430113],\n       [ 0.48020577, -0.47122706,  0.42830791,  0.25665744],\n       [-0.30885863,  0.21669025,  0.31648793,  0.22417514],\n       [ 0.32505724,  0.30478035,  0.48655034,  0.20040547]])\n>>> einsum('ij->ji',A)\narray([[ 0.3828997 ,  0.48020577, -0.30885863,  0.32505724],\n       [-0.39114848, -0.47122706,  0.21669025,  0.30478035],\n       [-0.09727838,  0.42830791,  0.31648793,  0.48655034],\n       [-0.20430113,  0.25665744,  0.22417514,  0.20040547]])\n```\n\nTensorflow does not work:\n\n```\npseudo-code:\nM_ = tf.Variable(tf.random_normal([4,4]))\nN_ = tf.einsum('ij->ji',M_)              \nprint [M_, N_]\n\noutput:\n[array([[ 0.80474716, -1.38590837, -0.3379252 , -1.24965811],\n       [ 2.57852983,  0.05492432,  0.23039417, -0.74263287],\n       [-2.42627382,  1.70774114,  1.19503212,  0.43006262],\n       [-1.04652011, -0.32753903, -1.26430523,  0.8810069 ]], dtype=float32), \narray([[ 0.80474716, -1.38590837, -0.3379252 , -1.24965811],\n       [ 2.57852983,  0.05492432,  0.23039417, -0.74263287],\n       [-2.42627382,  1.70774114,  1.19503212,  0.43006262],\n       [-1.04652011, -0.32753903, -1.26430523,  0.8810069 ]], dtype=float32)]\n```\n\nI want to multiply a matrix with every frame vector in every batch. Or similar operations which can be done by a simple tensor product. It seems that I still have to duplicate the matrix so many times and perform a batch_matmul, which is very inconvenient and slow and memory consuming.\n\nI suggest tensorflow to implement either the tensordot or einsum function which can perform tensor product.\n\nIt is quite a shame that tensorflow cannot even perform basic tensor product so far :(\n", "comments": ["@xuancong84 you seem to feel strongly about the behavior of `einsum`.  We definitely welcome contributions to the codebase!\n\n@nikhilmishra000 contributed the current implementation of `einsum` (thanks for that!) in #4378, and might have comments on the current restrictions.\n", "Hi all -- the current einsum implementation expects the output indices to be in alphabetical order (that should be fairly straightforward to fix, though). Other than that, I think it should work correctly. \n\n@xuancong84 Have you noticed any other bugs? Would it be easier to talk offline about any other unimplemented functionality you'd like to see?\n", "@nikhilmishra000 Although tensor transpose does not work, one-to-many matrix multiplication is working correctly. However, I have tried and compared the following two alternatives:\n\n```\nif True:                                                                                                          \n    batch_Wy1 = tf.reshape(tf.tile(weights['Wy1'],[current_batch_size,1]),[-1, hiddenSize, hiddenSize], name=\"Wy1\")\n    WyY1 = tf.batch_matmul(outputs2,batch_Wy1, name=\"WyY1\")                                                        \nelse:                                                                                                              \n    WyY1 = tf.einsum('ijk,kl->ijl',outputs2,weights['Wy1'])                  \n```\n\nDuring training, the cost functions are slightly different and the results diverge gradually. Is there any justification for the numerical difference? Otherwise, there might be a potential bug. Thanks!\n", "I can verify that `tf.einsum` is working correctly as follows:\n\n```\nAv = np.random.random(a_shape)\nBv = np.random.random(b_shape)\nAt = tf.constant(Av)\nBt = tf.constant(Bv)\n\nCt = tf.einsum('ijk,kl->ijl', At, Bt)\nCv = np.einsum('ijk,kl->ijl', Av, Bv)\n\nprint np.abs(session.run(Ct) - Cv).max()\n```\n\nWhich gives me an output of 0.0, and suggests that think the discrepancy is coming from elsewhere in your code. Are you perhaps initializing your variables or choosing batches randomly?\n", "@nikhilmishra000 Yes, the forward is correct. You don't need to verify, I have verified by myself. The forward calculation of the following two gives exactly the same results up to all significant figures:\n\n```\nif True:                                                                                                          \n    batch_Wy1 = tf.reshape(tf.tile(weights['Wy1'],[current_batch_size,1]),[-1, hiddenSize, hiddenSize], name=\"Wy1\")\n    WyY1 = tf.batch_matmul(outputs2,batch_Wy1, name=\"WyY1\")                                                        \nelse:                                                                                                              \n    WyY1 = tf.einsum('ijk,kl->ijl',outputs2,weights['Wy1'])      \n```\n\nHowever, if I use Adam loss, the update gives slightly different results. I didn't try other loss functions, and this is not necessarily a bug.\n", "Strong opinions aside, I don't think the OP is being unreasonable. If TensorFlow is offering an op that's \"Like numpy.einsum\" then it should ideally be roughly equivalent in terms of behavior. I just left a comment in PR #4837 suggesting that this function might be more appropriate for the contrib package.\n", "We have a number of ops that do not fully replicate numpy behavior. That is not a problem per se, as long as the differences are clearly documented. I will encourage more complete documentation of the differences in #4837, but I don't think it needs to be in contrib -- its interface is fixed (by inheriting it from numpy). \n\nOf course, replicating the behavior completely is not unreasonable at all, and a good feature request. I've adjusted the tag accordingly.\n", "einsum does not support dynamic sized tensors either. If any of the matrices contains a dimension of 'None' einsum returns an error.", "@fps7806 I am currently using `einsum` with dynamic batch dimension like so:\r\n`input = tf.placeholder(dtype=tf.float32, shape=(None, n_step, n_in))`\r\n`weights = tf.Variable(tf.truncated_normal((n_in, 1), stddev=1.0/np.sqrt(n_in)))`\r\n`Y_predict = tf.einsum('ijk,kl->ijl', input, weights)`\r\nIt works without errors which contradicts your previous statement. Has this been patched in the meantime?\r\n\r\n@xuancong84  I am also possibly experiencing 'bad' gradients when using `einsum`. Since the `einsum`is syntactic sugar for `matmul` I assumed there should be no problems with back propagation. But when using `einsum` even with untrainable weights (same example as above) the networks fails to learn. Is `einsum` verified to work well in backprop?\r\nI can confirm that if in my example above, `einsum` is replaced by reshape matmul alternative:\r\n`input_ = tf.reshape(input, [-1, n_in])`\r\n`Y_predict_ = tf.matmul(input_ , weights)`\r\n`Y_predict = tf.reshape(Y_predict_, [-1, n_step])`\r\nthe network learns as expected. Is this a bug in `einsum`?", "I think there's no reason einsum (being implemented as a combination of existing ops) should not support gradients. I would take a look at its implementation for clues first, but if you have a simple repro case, you should file an issue.", "Is this still valid?", "@xuancong84 Is this still an issue ?\r\nCould you please have a look at this [link](https://www.tensorflow.org/api_docs/python/tf/einsum)  for tf.einsum and let us know if it helps?\r\nThanks!"]}, {"number": 2625, "title": "Numerically stable summation methods", "body": "It would be nice to be able to use numerically stable summation methods like\n- pairwise summation\n- Kahan summation\n\nfor applications where numerical accuracy is important.\nThis could work with an optional `stable` keyword argument to `tf.reduce_sum` or via a `tf.stable_reduce_sum` op.\n\nI think implementing Kahan summation in the Eigen tensor library wouldn't be difficult at all, we would simply have to add a stateful `KahanSumReducer`.\nIt should also be possible to provide a vectorized version.\nPairwise summation might be more difficult, as the reduction code would have to be touched.\n", "comments": ["dibs\n", "@suiyuan2009: Cool, but you should get some input from one of the tensorflow devs before starting. @benoitsteiner, would this be a reasonable contribution?\n", "Any news? I could make good use of this.\n", "@egpbos want to contribute?\n", "In principle, yes, but I'm still deciding whether to go with TensorFlow in our project in the first place, which most importantly depends on #5323. Might take a few more weeks before we decide.\n", "Looks like this fell through the cracks.  Good idea though, so I'll leave it open.", "I have started working on this issue by implementing a modified version of Kahan summation in **eigen**. I have created the following functor.\r\n\r\n```cpp\r\ntemplate <typename T> struct NeumaierSumReducer\r\n{\r\n  static const bool PacketAccess = false; //packet_traits<T>::HasAdd;\r\n  static const bool IsStateful = true;\r\n\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE\r\n  NeumaierSumReducer() : compensation_(0) { }\r\n\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reduce(const T t, T* accum) {\r\n    internal::scalar_sum_op<T> sum_op;\r\n    internal::scalar_difference_op<T> diff_op;\r\n    internal::scalar_abs_op<T> abs_op;\r\n    T tempSum_ = sum_op(*accum, t);\r\n    if (abs_op(*accum) >= abs_op(t)) {\r\n      compensation_ += sum_op(diff_op(*accum, tempSum_), t);\r\n    } else {\r\n      compensation_ += sum_op(diff_op(t, tempSum_), *accum);\r\n    }\r\n    *accum = tempSum_;\r\n  }\r\n  //template <typename Packet>\r\n  //EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reducePacket(const Packet& p, Packet* accum) {\r\n    //Packet tempSum_ = padd<Packet>(*accum, p);\r\n    //if (pabs<Packet>(*accum) >= pabs<Packet>(p)) {\r\n      //compensation_ += predux(padd<Packet>(psub<Packet>(*accum, tempSum_), p));\r\n    //} else {\r\n      //compensation_ += predux(padd<Packet>(psub<Packet>(p, tempSum_), *accum));\r\n    //}\r\n    //(*accum) = tempSum_;\r\n  //}\r\n\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T initialize() const {\r\n    internal::scalar_cast_op<int, T> conv;\r\n    return conv(0);\r\n  }\r\n  //template <typename Packet>\r\n  //EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet initializePacket() const {\r\n    //return pset1<Packet>(initialize());\r\n  //}\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T finalize(const T accum) const {\r\n    return accum + compensation_;\r\n  }\r\n  //template <typename Packet>\r\n  //EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet finalizePacket(const Packet& vaccum) const {\r\n    //return vaccum;\r\n  //}\r\n  //template <typename Packet>\r\n  //EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T finalizeBoth(const T saccum, const Packet& vaccum) const {\r\n    //internal::scalar_sum_op<T> sum_op;\r\n    //return sum_op(sum_op(saccum, predux(vaccum)), compensation_);\r\n  //}\r\n\r\n  protected:\r\n    T compensation_;\r\n};\r\n\r\n#define SumReducer NeumaierSumReducer\r\n\r\ntemplate <typename T, typename Device>\r\nstruct reducer_traits<NeumaierSumReducer<T>, Device> {\r\n  enum {\r\n    Cost = 4 * NumTraits<T>::AddCost,\r\n    PacketAccess = false//PacketType<T, Device>::HasAdd\r\n  };\r\n};\r\n\r\n```\r\nI commented out the code for `SumReducer` and used `#define` to replace all instances of `SumReducer` with `NeumaierSumReducer`. All the relevant tests seem to be running fine after this change.\r\n\r\nNow I am facing 2 issues:\r\n\r\n1. I am unable to get the functor working with `Packet` as I do not understand it completely. I tried creating a compensation variable of type `Packet` but it gave an error saying that `Packet` does not name a type.\r\n2. I am unable to see if the change actually made any improvement. I tried with a custom code and gave it a case which should cause erroneous results with the standard summation method but work well with the modified algorithm. But both the versions seem to be performing identically. I used a debugger to see if the `SumReducer` functor was being used but surprisingly, it was never used when computing the sum.\r\n\r\nHere is the sample code\r\n```cpp\r\n#include <iostream>\r\n#include <Eigen/Dense>\r\n#include <iomanip>\r\n\r\nusing namespace std;\r\nint main()\r\n{\r\n  Eigen::Matrix2f mat;\r\n  mat << 1000000000.0, 2.00002,\r\n         1.00003, -1000000000.0;\r\n  cout << \"Here is mat.sum():       \" << std::setprecision(10) << mat.sum()       << endl;\r\n}\r\n```\r\n\r\nAny hints will be helpful. The eigen mailing list doesn't seem to be very active.", "@girving I am trying to get this into the Eigen library but the mailing list/issue tracker seem very inactive and I think that getting it into Eigen is going to take a long time. Is there some way I could implement this within tensorflow?", "@rmlarsen Any suggestions for how to do Kahan summation without changing Eigen?  One problem is that an ideal implementation would probably stride the Kahan summation algorithm based on the architecture optimal packet size.", "TF members, I'm not sure I understand the tensorflowbutler comments, but it seems the issue creator is not responding to the request to drop requests for contributions. Can you fix this your end? It would suck for this issue to stall due to miscommunication/poor timing of interests. It is now 2 years old, but very important to those who care about it.", "Encounter the same problem here, the differences are huge for my task.  Any replacements? ", "Is there any update on this issue? I think it's especially important for policy gradient reinforcement learning, since the basin of attraction is less stable. I can run the same RL algorithm twice with all other sources of nondeterminism eliminated, and get significantly different results each time. I would like to publish reproducible results, even if the performance hit is significant.", "This snippet might be related:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndata = np.random.rand(int(1e4), int(1e4)) + np.random.rand(int(1e4), 1)\r\ndata = data.astype('float32')\r\nvec = tf.placeholder(tf.float32, data.shape)\r\ns = tf.reduce_sum(vec)\r\nwith tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1)) as sess:\r\n    np_sum = np.sum(data)\r\n    tf_sum = sess.run(s, feed_dict={vec: data})\r\n    diff = abs(tf_sum - np_sum)\r\n    print(diff, diff/1e8)   # difference can be as large as 200656, when running with CPU.\r\n\r\ndata = data.astype('float64')\r\nvec = tf.placeholder(tf.float64, data.shape)\r\ns = tf.reduce_sum(vec)\r\nwith tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1)) as sess:\r\n    np_sum = np.sum(data)\r\n    tf_sum = sess.run(s, feed_dict={vec: data})\r\n    diff = abs(tf_sum - np_sum)\r\n    print(diff, diff/1e8)\r\n```", "unassign", "Would it be sufficient to su recursively across the dimensions of a Tensor?\r\n\r\nThis would reduce the problem from being O(size) to O(size of largest dimension), which might just be enough for most use-cases, and should be a lot easier to implement than compensated summation."]}, {"number": 956, "title": "Feature request: other types of padding besides zero-padding.", "body": "I would like to have other options of padding for tf.pad and convolution ops.\n\nSome types that come to my mind right now:\n- reflect\n- constant value (other than zero)\n- maybe implement other options of numpy.pad: http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.pad.html\n", "comments": ["Note that, for experimental purposes, all of these can be implemented by doing the padding as a separate layer and using VALID padding.\n", "A work is in progress to add reflect and symmetric padding mode (as in numpy.pad).\n", "@sungjinhwang What about pad with constant value other than zero? This would be useful, because I want to set the constant value to the average values of the input. It would also be useful to choose a constant value for each channel.\n", "@cesarsalgado: You can pad with a constant other than zero by subtracting a constant from the input, padding with zero, and compensating in the output.\n", "@girving That is clever =) Thanks!\n", "Would it be possible to also support border replication? Something similar to https://github.com/torch/nn/blob/master/doc/convolution.md#nn.SpatialReplicationPadding\r\n\r\nI think repeatedly calling tf.pad with symmetric/pad size 1 would work, but perhaps there could be a streamlined version?", "Any progress in this?", "please add other padding modes, as in numpy.pad\r\n\r\nhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\r\n\r\n`\u2018constant\u2019`\r\nPads with a constant value (different than zero).\r\n\r\n`\u2018mean\u2019`\r\nPads with the mean value of all or part of the vector along each axis.\r\n\r\n`\u2018wrap\u2019`\r\nPads with the wrap of the vector along the axis. The first values are used to pad the end and the end values are used to pad the beginning.\r\n\r\nThe last one correspond to the periodic boundary conditions, especially useful during physics calculation and for Convolutional Neural Networks with full symmetry on the images (example: Ising model).\r\nNote that this is very different from `tf.pad(t, paddings, \"REFLECT\")`, as can be seen in the following code:\r\n```\r\nimport numpy\r\nmatrix = numpy.arange(25).reshape(5,5)\r\npaddings = [[1,1],[1,1]]\r\n# periodic boundary conditions\r\npadded_matrix = numpy.pad(matrix, pad_width=paddings, mode='wrap')\r\n\r\nimport tensorflow as tf\r\nsession = tf.InteractiveSession()\r\nM = tf.constant(matrix)\r\npadded_M = tf.pad(M, paddings, mode=\"reflect\")\r\n\r\npadded_matrix == padded_M.eval()\r\n# False\r\n```\r\n", "The 'CONSTANT' option in Tensorflow is quite confusing as it only supports 0 padding. If constant padding is not available, the option should be changed to 'ZERO'.", "Please add other padding modes, as \u2018wrap\u2019 in numpy.pad. (as also requested by @FedericoMuciaccia )\r\nBesides, could anyone give me a clue that how to implement 'wrap' padding using only the current tensorflow functions?  Thanks in advance. ", "To implement the wrap padding (only for the width and height with size), I wrote the code as (hope I am right)\r\n\r\n``` python\r\ndef wrap_pad(input, size):\r\n    M1 = tf.concat([input[:,:, -size:,:], input, input[:,:, 0:size,:]], 2)\r\n    M1 = tf.concat([M1[:,-size:, :,:], M1, M1[:,0:size, :,:]], 1)\r\n    return M1\r\n```\r\n", "@YuanmanLi here are two ways to possibly implement this. the first one is way simpler, faster and memory efficient:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef periodic_padding(image, padding=1):\r\n    '''\r\n    Create a periodic padding (wrap) around the image, to emulate periodic boundary conditions\r\n    '''\r\n    \r\n    upper_pad = image[-padding:,:]\r\n    lower_pad = image[:padding,:]\r\n    \r\n    partial_image = tf.concat([upper_pad, image, lower_pad], axis=0)\r\n    \r\n    left_pad = partial_image[:,-padding:]\r\n    right_pad = partial_image[:,:padding]\r\n    \r\n    padded_image = tf.concat([left_pad, partial_image, right_pad], axis=1)\r\n    \r\n    return padded_image\r\n\r\n# usage example:\r\nsession = tf.InteractiveSession()\r\nimage = tf.reshape(tf.range(30, dtype='float32'), shape=[5,6])\r\npadded_image = periodic_padding(image, padding=2)\r\nimage.eval()\r\npadded_image.eval()\r\n```\r\n\r\nthe second one is more complex, and slower, because it requires a lot of calculations. but here the padding is done via matrix multiplication (with fixed auxiliary matrices), so in principle I think that this implementation could be used to exploit end-to-end differentiation\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef periodic_padding(image, padding=1):\r\n    '''\r\n    Create a periodic padding (wrap) around the image, to emulate periodic boundary conditions\r\n    '''\r\n\r\n    rows, columns = image.shape\r\n    \r\n    # create left matrix\r\n    left_corner_diagonal = tf.eye(padding)\r\n    left_filled_zeros = tf.zeros([padding,rows.value-padding])\r\n    \r\n    left_upper = tf.concat([left_filled_zeros, left_corner_diagonal], axis=1)\r\n    left_center_diagonal = tf.eye(rows.value)\r\n    left_lower = tf.concat([left_corner_diagonal,left_filled_zeros], axis=1)\r\n    \r\n    left_matrix = tf.concat([left_upper, left_center_diagonal, left_lower], axis=0)\r\n    \r\n    # create right matrix\r\n    right_corner_diagonal = tf.eye(padding)\r\n    right_filled_zeros = tf.zeros([columns.value-padding,padding])\r\n    \r\n    right_left_side = tf.concat([right_filled_zeros, right_corner_diagonal], axis=0)\r\n    right_center_diagonal = tf.eye(columns.value)\r\n    right_right_side = tf.concat([right_corner_diagonal,right_filled_zeros], axis=0)\r\n    \r\n    right_matrix = tf.concat([right_left_side, right_center_diagonal, right_right_side], axis=1)\r\n    \r\n    # left and right matrices are immutable\r\n    padded_image = tf.matmul(left_matrix, tf.matmul(image, right_matrix))\r\n\r\n    return padded_image\r\n\r\n# usage example:\r\nsession = tf.InteractiveSession()\r\n# left and right matrices are immutable, given a fixed image shape\r\nimage = tf.reshape(tf.range(30, dtype='float32'), shape=[5,6])\r\npadded_image = periodic_padding(image, padding=2)\r\nimage.eval()\r\npadded_image.eval()\r\n```\r\n\r\nis there any tensorflower willing to expand this sketch of code to implement the wrap padding in the next official release?", "I am closing this issue now, as the original posters goals were to add more functionality, and we did. Specifically, we added the most commonly needed types (backed the op MirrorPad) several months ago. If the more exotic numpy.pad types of padding are needed, please file a new bug referring to this one. However, if somebody implements a new type they need and makes it perform well, we will accept the patch. ", "@aselle The original poster explicitly mentioned two types of padding: reflect and constant, and only MirrorPad is added, but not constant.\r\n\r\nAs I have mentioned, the current \"CONSTANT\" option is confusing, as it can only fill zeros rather than a selected constant. Although tricks like `tf.pad(X - c) + c` can be used, the code will be obscure.\r\n\r\nI don't think constant padding is \"exotic\" at all as it is a common practice in machine learning (appending a row of 1 to a matrix is very common).", "@aselle The original poster also asked that the padding be added to the convolution operation. Would padding inside the convolution be more efficient and avoid unnecessary copy compared to tf.pad followed by convolution?", "Constant padding with values other than 0 is already supported, so it looks like this issue can be closed.\r\n\r\n@lubomir1 your question is answered at https://github.com/tensorflow/tensorflow/issues/18213#issuecomment-396736457", "The original poster asked for the following choices of padding to be added, both to tf.pad and (!) convolution:\r\n\u2018constant\u2019, \u2018edge\u2019, \u2018linear_ramp\u2019, \u2018maximum\u2019, \u2018mean\u2019, \u2018median\u2019, \u2018minimum\u2019, \u2018reflect\u2019, \u2018symmetric\u2019, \u2018wrap\u2019, \"function\"\r\n\r\nThis functionality is essential to the usability of tensorflow for practical problems, e.g. solution of PDEs.\r\nThere should be a way of specifying a broad range of boundary condition using padding (Neuman, Dirichlet, periodic, mixed, etc.)", "Another upvote for 'wrap' padding from someone coming from a physics-simulation point of view.\r\n\r\nThe code snippet based on concatenation is pretty suboptimal for an application of this kind; which tends to work with simple filters, and everything is 100% memory bottlenecked. Any naive implementation that does not take into account the pecularities of gpu memory access is going to be far from optimal I suppose.\r\n\r\nIndeed having a 'wrap' option baked into the convolutional operators at a low level would be ideal; but I suppose that this isnt so much up to tensorflow, as it is up to CUDNN to implement; and AFAIK it does not?", "@FedericoMuciaccia \r\n> the second one is more complex, and slower, because it requires a lot of calculations. but here the padding is done via matrix multiplication (with fixed auxiliary matrices), so in principle I think that this implementation could be used to exploit end-to-end differentiation\r\n\r\nWhy would your first method using concatenation not work for differentiation? I just implemented this version and from what I can see, autodiff works fine.\r\n", "I need to left-pad and right-pad with different values. I have no idea how to convince keras to do this. Any ideas?", "Based on @YuanmanLi answer, here is a way to pad data periodically (which is a very common padding type in scientific computing) with a layer dedicated to it:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom keras import layers\r\nfrom tensorflow.keras.layers import InputSpec \r\nfrom tensorflow.python.keras.utils import conv_utils\r\n\r\nclass PeriodicPadding2D(layers.Layer):\r\n\r\n  def __init__(self, padding=1, **kwargs):\r\n    super(PeriodicPadding2D, self).__init__(**kwargs)\r\n    self.padding = conv_utils.normalize_tuple(padding, 1, 'padding')\r\n    self.input_spec = InputSpec(ndim=3)\r\n\r\n  def wrap_pad(self, input, size):\r\n    M1 = tf.concat([input[:,:, -size:], input, input[:,:, 0:size]], 2)\r\n    M1 = tf.concat([M1[:,-size:, :], M1, M1[:,0:size, :]], 1)\r\n    return M1\r\n\r\n  def compute_output_shape(self, input_shape):\r\n    shape = list(input_shape)\r\n    assert len(shape) == 3  \r\n    if shape[1] is not None:\r\n      length = shape[1] + 2*self.padding[0]\r\n    else:\r\n      length = None\r\n    return tuple([shape[0], length, length])\r\n\r\n  def call(self, inputs): \r\n    return self.wrap_pad(inputs, self.padding[0])\r\n\r\n  def get_config(self):\r\n    config = {'padding': self.padding}\r\n    base_config = super(PeriodicPadding2D, self).get_config()\r\n    return dict(list(base_config.items()) + list(config.items()))\r\n\r\n```\r\nPlease note that this code is for my particular problem and I didn't test its generality yet. Please verify if it works for you.", "I think that the scope of this ticket is too broad. Can we close this as some of the operations are now covered and open new issues for the missing ones?"]}, {"number": 252, "title": "Make TensorFlow compatible with PyPy", "body": "I know it's not a priority and will be a long way to get there; but making TF compatible with PyPy woud be super cool.\n\nThoughts?\n", "comments": ["As a team we don't use PyPy day-to-day, but we would welcome contributions if it's easy to do this without breaking CPython compatibility.\n\nMy guess is that the two stumbling blocks would be TensorFlow's reliance on NumPy in the Python front-end, and SWIG for interfacing with the C++ backend. Are you aware of any other issues that one would face?\n", "I'm not sure what the state of PyPy binding layers is these days, but there's a good chance this would require rewriting the whole swig interface.\n", "PyPy only good interface with native code seems to be [CFFI](https://cffi.readthedocs.io/en/latest/), but it is a C <-> Python interface library. Besides being slower than CPython native interface (when running on CPython, of course), it would be too painful to interface with C++ (Python <-> C <-> C++).\n", "The medium term plan for language bindings is to go through the C API.  We're in the process of moving functionality from Python to C++, at which point it will also be exposed through an extended C API.  Moving stuff out of Python to C++ isn't relevant for PyPy, but the improved C API might make it easy to write a separate PyPy interface.\n", "Great news from the side of PyPy: since version 5.6.0 (released in November) the devs included a compatibility layer for the CPython C-API (cpyext) (see also [here](https://morepypy.blogspot.de/2016/11/pypy27-v56-released-stdlib-2712-support.html)). With this, I was able to build numpy, scipy and sklearn out-of-the box, so finally machine learning with pypy gets interesting.\r\n\r\nOf course I wanted to then also check tensorflow, with pretty good results! The build completes successfully out-of-the box (!!), so no need to adapt any interfaces. However, `import tensorflow` then fails with the well-known \"ImportError: No module named _pywrap_tensorflow\" (of course not from the tensorflow directory). Looking into it, `ldd -r _pywrap_tensorflow.so` gave me some \"undefined symbol: PyPy....\" errors, indicating that just an additional link command during the build could fix the issue! Since I'm not so familiar with bazel, I didn't go further to resolve it...", "@classner Very cool!  Presumably you have to link against a `cpyext` library.  @martinwicke Who would be the right person to ask for assistance here?", "@classner can you try the cmake build? If you can make that work, then we know what needs to be added, and we can put it into the bazel build. Bazel really wants all its dependencies declared, so it would be good to have a clear picture of what exactly is missing.", "Just posted on the pypy dev mailing list to get a bit more input from that side! Will post further info here...", "One of the devs was incredibly quick to reply: apparently, all the symbols I mentioned are part of `libpypy-c.so`, which is in the bin folder of the pypy distribution. I'm currently super-short on time and can't look at it right now, but could do sometime during the next weeks.", "Alright, managed to build with bazel and cmake successfully (v1.0.0-rc0). The trick to make it work is to rename the generated `_pywrap_tensorflow.so` to `_pywrap_tensorflow.pypy-XX.so`, where XX is the pypy version code (in my case 41 for pypy 5.6.0). With this naming scheme, `import tensorflow` works nicely. So second base touched.\r\n\r\nThis part of the 'get started' code runs:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\r\nx_data = np.random.rand(100).astype(np.float32)\r\ny_data = x_data * 0.1 + 0.3\r\n\r\n# Try to find values for W and b that compute y_data = W * x_data + b\r\n# (We know that W should be 0.1 and b 0.3, but TensorFlow will\r\n# figure that out for us.)\r\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\r\nb = tf.Variable(tf.zeros([1]))\r\ny = W * x_data + b\r\n\r\n# Minimize the mean squared errors.\r\nloss = tf.reduce_mean(tf.square(y - y_data))\r\noptimizer = tf.train.GradientDescentOptimizer(0.5)\r\ntrain = optimizer.minimize(loss)\r\n\r\n# Before starting, initialize the variables.  We will 'run' this first.\r\ninit = tf.global_variables_initializer()\r\n\r\n# Launch the graph.\r\nsess = tf.Session()\r\n```\r\n\r\n`sess.run(init)` fails with a `TypeError`:\r\n\r\n`TypeError: in method 'TF_DeleteBuffer', argument 1 of type 'TF_Buffer *'`.\r\n\r\n\r\n", "Any feedback on this from the interface developers? Could this TypeError be resolved easily?\r\n", "Can you provide more information about the `TypeError`? From looking at `session.py`, it seems the most likely candidate for raising this exception is [this line](https://github.com/tensorflow/tensorflow/blob/ac352bc807410c8a863d0b95202cceeb62ed0f10/tensorflow/python/client/session.py#L772). What type does `run_metadata_ptr` have in your version of the code?", "The line in question is indeed causing the TypeError. This is what I get when adding an ipdb breakpoint just before:\r\n\r\n```\r\n    772       import ipdb; ipdb.set_trace()\r\n--> 773       tf_session.TF_DeleteBuffer(run_metadata_ptr)\r\n    774       if options:\r\n\r\nipdb> run_metadata_ptr\r\n<tensorflow.python.pywrap_tensorflow.TF_Buffer;  >\r\nipdb> type(run_metadata_ptr)\r\n<class 'tensorflow.python.pywrap_tensorflow.TF_Buffer'>\r\nipdb> n\r\nTypeError: \"in method 'TF_DeleteBuffer', argument 1 of type 'TF_Buffer *'\"\r\n```\r\n\r\n(`str` and `repr` don't give any more helpful information)", "I don't have anything constructive to say, but I can't resist chiming in to gripe about error messages in one of these two forms:\r\n\r\n1. \"You gave me an X.  I expected something else!\"\r\n2. \"I expected a Y.  You gave me something else!\"\r\n\r\nHead...desk.", "@girving Unfortunately that error message is in generated code. Maybe take it up with the authors of https://github.com/swig/swig? :)\r\n\r\nJudging by that generated code, we seem to be in case 2 (\"I expected a `TF_Buffer*`\"). SWIG converts this C type to and from a pointer-wrapper object. I don't know enough about PyPy or `cpyext` to know if this type could be getting swizzled somehow. Here are the relevant generated code fragments for the wrappers for `TF_NewBuffer()` and `TF_DeleteBuffer()`:\r\n\r\n```c++\r\nSWIGINTERN PyObject *_wrap_TF_NewBuffer(PyObject *SWIGUNUSEDPARM(self), PyObject *args) {\r\n  PyObject *resultobj = 0;\r\n  TF_Buffer *result = 0 ;\r\n  \r\n  if (!PyArg_ParseTuple(args,(char *)\":TF_NewBuffer\")) SWIG_fail;\r\n  {\r\n    Py_BEGIN_ALLOW_THREADS;\r\n    result = (TF_Buffer *)TF_NewBuffer();\r\n    Py_END_ALLOW_THREADS;\r\n  }\r\n  resultobj = SWIG_NewPointerObj(SWIG_as_voidptr(result), SWIGTYPE_p_TF_Buffer, 0 |  0 );\r\n  return resultobj;\r\nfail:\r\n  return NULL;\r\n}\r\n\r\nSWIGINTERN PyObject *_wrap_TF_DeleteBuffer(PyObject *SWIGUNUSEDPARM(self), PyObject *args) {\r\n  PyObject *resultobj = 0;\r\n  TF_Buffer *arg1 = (TF_Buffer *) 0 ;\r\n  void *argp1 = 0 ;\r\n  int res1 = 0 ;\r\n  PyObject * obj0 = 0 ;\r\n  \r\n  if (!PyArg_ParseTuple(args,(char *)\"O:TF_DeleteBuffer\",&obj0)) SWIG_fail;\r\n  res1 = SWIG_ConvertPtr(obj0, &argp1,SWIGTYPE_p_TF_Buffer, 0 |  0 );\r\n  if (!SWIG_IsOK(res1)) {\r\n    SWIG_exception_fail(SWIG_ArgError(res1), \"in method '\" \"TF_DeleteBuffer\" \"', argument \" \"1\"\" of type '\" \"TF_Buffer *\"\"'\"); \r\n  }\r\n  arg1 = reinterpret_cast< TF_Buffer * >(argp1);\r\n  {\r\n    Py_BEGIN_ALLOW_THREADS;\r\n    TF_DeleteBuffer(arg1);\r\n    Py_END_ALLOW_THREADS;\r\n  }\r\n  resultobj = SWIG_Py_Void();\r\n  return resultobj;\r\nfail:\r\n  return NULL;\r\n}\r\n```", "I was able to reproduce @classner steps for tensorflow 1.2 with CUDA for pypy3 version 5.8.0-beta0 (Python 3.5.3). In order to do this, `_pywrap_tensorflow_internal.so` library in `site-packages/tensorflow/python` needs to be renamed to `_pywrap_tensorflow_internal.pypy3-58-x86_64-linux-gnu.so`. I am getting a slightly different error during `sess.run`:\r\n\r\n```bash\r\n$ LD_LIBRARY_PATH=/usr/local/cuda/lib64 pypy3.5\r\n\r\nPython 3.5.3 (a37ecfe5f142bc971a86d17305cc5d1d70abec64, Jun 08 2017, 19:43:54)\r\n[PyPy 5.8.0-beta0 with GCC 6.3.0] on linux\r\n\r\n>>>> import tensorflow as tf\r\n>>>> x = tf.constant(1)\r\n>>>> y = tf.constant(2)\r\n>>>> z = x * y\r\n>>>> with tf.Session() as sess:\r\n....     sess.run(z)\r\n....     \r\n2017-07-20 14:43:47.795964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-07-20 14:43:47.796979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.936\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.67GiB\r\n2017-07-20 14:43:47.796992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-07-20 14:43:47.796995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-07-20 14:43:47.797003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/home/andrew/.virtualenvs/tensorflow_pypy/site-packages/tensorflow/python/client/session.py\", line 780, in run\r\n    run_metadata_ptr = tf_session.TF_NewBuffer()\r\nTypeError: argument after * must be an iterable, not NoneType\r\n```\r\n\r\nIt seems that [the first line](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/client/session.py#L780) in `session.run` fails to execute.", "I'm quite interested in this working under pypy. I didn't have any luck with CNTK either but I've discovered that Theano actually works under pypy (with some minor hacks), so for now, it seems I'll be using that instead.\r\n\r\nEdit: Patches are now upstream for anyone interested.", "I think the `TypeError: argument after * must be an iterable, not NoneType` error is due to using an old version of SWIG. Could you make sure you are using a version after 4.0.0?", "Any news regarding this matter?\r\n @classner @standy66 do you mind sharing instructions how you reached that point?\r\n", "I think this issue is closed by mistake. The #252 referenced in 913871c's message is actually https://github.com/tensorflow/mlir/pull/252, not this issue.", "I ran into issues compiling tensorflow 1.15.2 with pypy3 and pybind11 - \r\nThe error is not being able to find 'frame' \r\nhttps://github.com/tensorflow/tensorflow/blob/v1.15.2/tensorflow/python/util/tf_stack.cc#L63\r\n\r\nThis was about missing members something like\r\nhttps://github.com/pybind/pybind11/issues/982\r\n\r\nSOABI : pypy36-pp73\r\n\r\nUsing pypy (pypy-41 - 5.10.0), am stuck at the TypeError ( re-created @classner 's output ). \r\n\r\n\r\n", "> SOABI : pypy36-pp73. Using pypy (pypy-41 - 5.10.0)\r\n\r\nI am confused. Are you using the latest release of PyPy (v7.3.1)? What is pypy-41 - 5.10.0?", "@mattip , sorry for the confusion, when pypy3 failed, i tried to make it work with pypy.", "I am not sure it will help, but if it is not too much trouble could you try with pypy3.6-v7.3.1 and pybind11 using this PR pybind/pybind11#2146?", "pypy3: pypy3.6-v7.3.1\r\npybind11: 2.4.dev4 (after applying fixes: https://github.com/pybind/pybind11/pull/2146 )\r\n\r\nHere is the trace back.\r\n\r\ntensorflow/python/util/tf_stack.cc: In function 'std::vector<tensorflow::{anonymous}::StackFrame> tensorflow::{anonymous}::ExtractStack(ssize_t, const pybind11::list&, const pybind11::list&)':\r\ntensorflow/python/util/tf_stack.cc:56:36: error: 'const struct _ts' has no member named 'frame'\r\n   const PyFrameObject* f = tstate->frame->f_back;  // TODO(slebedev): INCREF?\r\n                                    ^~~~~\r\ntensorflow/python/util/tf_stack.cc:61:51: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (; f != nullptr && (limit < 0 || ret.size() < limit); f = f->f_back) {\r\n                                        ~~~~~~~~~~~^~~~~~~\r\ntensorflow/python/util/tf_stack.cc:61:68: error: 'const PyFrameObject {aka const struct _frame}' has no member named 'f_back'\r\n   for (; f != nullptr && (limit < 0 || ret.size() < limit); f = f->f_back) {\r\n                                                                    ^~~~~~\r\ntensorflow/python/util/tf_stack.cc:63:18: error: 'PyFrame_GetLineNumber' was not declared in this scope\r\n     int lineno = PyFrame_GetLineNumber(const_cast<PyFrameObject*>(f));\r\n                  ^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/util/tf_stack.cc:63:18: note: suggested alternative: 'PyFile_GetLine'\r\n     int lineno = PyFrame_GetLineNumber(const_cast<PyFrameObject*>(f));\r\n                  ^~~~~~~~~~~~~~~~~~~~~\r\n                  PyFile_GetLine\r\ntensorflow/python/util/tf_stack.cc:85:39: error: 'struct PyCodeObject' has no member named 'co_firstlineno'; did you mean 'co_filename'?\r\n     const int func_start_lineno = co->co_firstlineno;\r\n                                       ^~~~~~~~~~~~~~\r\n                                       co_filename\r\n", "@mattip , was able to get it working with tensorflow 1.14.0\r\n\r\nHere, i ran into this error:\r\nbuf = c_api.TF_NewBuffer()\r\nTypeError: argument after * must be an iterable, not NoneType\r\n\r\nDo you suggest any pending prs to fix this?\r\nThanks for the help.", "@arjunc77 that sounds familiar: looking higher in the thread I see a similar comment. Could you make sure you are using SWIG > 4.0.0 to generate the wrappers?", "Thanks @mattip , that helped.\r\n\r\nThere could be a better way to update swig, but here is what worked for me.\r\nhttps://github.com/tensorflow/tensorflow/commit/4b5840ed1bb21fa7250355753fab7fc50164b1a5", "I finally made my pypy compatible in tensorflow. It works good but tf.conv layer is still not working yet in my pypy", "@SungmanHong can you share the formula? What did you need to do?", "> I finally made my pypy compatible in tensorflow. It works good but tf.conv layer is still not working yet in my pypy\r\n\r\nCould you please help me on that? I also desperately want my tensorflow to be compatible with pypy <33 Thank you!", "What is the status of this issue ? Is there any progress ?", "@SungmanHong \r\n> \r\n> \r\n> I finally made my pypy compatible in tensorflow. It works good but tf.conv layer is still not working yet in my pypy\r\n\r\nCould you share receipt how you did this ? Or maybe article ?", "@mrry Do you have some plan to support PyPy ?", "We have removed SWIG, so there's a chance it works now? We do not\nofficially support it, but maybe we're lucky.\n", "> \r\n> We have removed SWIG, so there's a chance it works now? We do not officially support it, but maybe we're lucky.\r\n\r\n@martinwicke I have tried a few seconds ago to install tensorflow on PyPy and it still does not work ...\r\nWhat do you mean that it could work ?", "@redradist: did you try with the latest nightly from http://buildbot.pypy.org/nightly/py3.6  and latest HEAD of tensorflow? Was the error similar to the one above?", "@mattip I have tried and got the following error:\r\n```bash\r\nWARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nERROR: An error occurred during the fetch of repository 'aws':\r\n   java.io.IOException: Error extracting /home/redra/.cache/bazel/_bazel_redra/41d86d998710872df1eae1e2464f95b2/external/aws/1.7.336.tar.gz to /home/redra/.cache/bazel/_bazel_redra/41d86d998710872df1eae1e2464f95b2/external/aws: writing file failed\r\nERROR: /mnt/i/tensorflow/tensorflow/tools/pip_package/BUILD:171:1: //tensorflow/tools/pip_package:licenses depends on @aws//:LICENSE in repository @aws which failed to fetch. no such package '@aws//': java.io.IOException: Error extracting /home/redra/.cache/bazel/_bazel_redra/41d86d998710872df1eae1e2464f95b2/external/aws/1.7.336.tar.gz to /home/redra/.cache/bazel/_bazel_redra/41d86d998710872df1eae1e2464f95b2/external/aws: writing file failed\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@aws//': java.io.IOException: Error extracting /home/redra/.cache/bazel/_bazel_redra/41d86d998710872df1eae1e2464f95b2/external/aws/1.7.336.tar.gz to /home/redra/.cache/bazel/_bazel_redra/41d86d998710872df1eae1e2464f95b2/external/aws: writing file failed\r\n```", "Are you sure this error is pypy-specific?", "> \r\n> \r\n> Are you sure this error is pypy-specific?\r\n\r\n@mattip No, this error relates to TensorFlow build from master HEAD\r\npypy-latest works properly ...", "OK, my directions should have been \"latest pypy3.6 nightly and latest tensorflow that successfully builds with cpython3.6\"", "Does anyone have any specific instructions for making this work?  I tried both version 2.3.1 (my desired version) and trying to pip install from the git repo and got this message:\r\n\r\n```\r\n[user@ba cli]$ sudo /opt/pypy3/current/bin/pypy3 -m pip install git+https://github.com/tensorflow/tensorflow.git@master\r\nsudo: unable to resolve host ba\r\nCollecting git+https://github.com/tensorflow/tensorflow.git@master\r\n  Cloning https://github.com/tensorflow/tensorflow.git (to revision master) to /tmp/pip-req-build-dpufquz5\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /opt/pypy3/current/bin/pypy3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-dpufquz5/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-dpufquz5/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-ed0ud5zb\r\n         cwd: /tmp/pip-req-build-dpufquz5/\r\n    Complete output (5 lines):\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/opt/pypy3/current/lib-python/3/tokenize.py\", line 452, in open\r\n        buffer = _builtin_open(filename, 'rb')\r\n    FileNotFoundError: [Errno 2] No such file or directory: '/tmp/pip-req-build-dpufquz5/setup.py'\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\nWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\r\nYou should consider upgrading via the '/opt/pypy3/current/bin/pypy3 -m pip install --upgrade pip' command.\r\n```\r\n\r\nI'm using pypy3.6-v7.3.3-linux64.tar.bz2", "Hello everyone,\r\nI downloaded the latest pypy nightly build with python 3.6 using https://buildbot.pypy.org/nightly/py3.6/pypy-c-jit-latest-linux64.tar.bz2 as suggested (**pypy-c-jit-101029-118564033d0a-linux64**) previously as well as trying to get the latest head of tensorflow. I have been trying to get tensorflow 2 to work in a PyPy venv using \r\n```\r\nPython 3.6.12 (118564033d0a, Nov 26 2020, 02:00:26)\r\n[PyPy 7.3.4-alpha0 with GCC 7.3.1 20180303 (Red Hat 7.3.1-5)] on linux\r\npip 21.1.1 on pypy-venv\r\n```\r\nBut I have been having issues. I have followed this thread and some other issue posts, but I remain unable to advance.\r\n\r\nFirst off, running `pip install tensorflow` or `pip install tf-nightly` just returns the following error:\r\n\r\n```\r\nERROR: Could not find a version that satisfies the requirement tf-nightly (from versions: none)\r\nERROR: No matching distribution found for tf-nightly\r\n```\r\nI then tried to compile tensorflow from source in the PyPy venv and, after installing the requisites and configuring the environment, the following error log occurs during `bazel build //tensorflow/tools/pip_package:build_pip_package`:\r\n\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=83\r\nINFO: Reading rc options for 'build' from /root/prema-rpi/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /root/prema-rpi/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /root/prema-rpi/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/root/prema-rpi/venv-pypy3.6/bin/python3 --action_env PYTHON_LIB_PATH=/root/prema-rpi/venv-pypy3.6/site-packages --python_path=/root/prema-rpi/venv-pypy3.6/bin/python3\r\nINFO: Found applicable config definition build:short_logs in file /root/prema-rpi/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /root/prema-rpi/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /root/prema-rpi/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /root/prema-rpi/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (414 packages loaded, 33390 targets configured).\r\nINFO: Found 1 target...\r\n[10,060 / 15,182] 8 actions running\r\nINFO: Found 1 target...\r\nERROR: /tensorflow/tensorflow/python/util/BUILD:367:11: C++ compilation of rule '//tensorflow/python/util:stack_trace' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 74 argument(s) skipped)\r\nIn file included from tensorflow/python/util/stack_trace.cc:16:0:\r\n./tensorflow/python/util/stack_trace.h: In static member function 'static tensorflow::StackTrace tensorflow::StackTrace::Capture(int)':\r\n./tensorflow/python/util/stack_trace.h:64:55: error: 'PyThreadState {aka struct _ts}' has no member named 'frame'\r\n     const PyFrameObject* frame = PyThreadState_GET()->frame;\r\n                                                       ^~~~~\r\n./tensorflow/python/util/stack_trace.h:66:58: error: 'const PyFrameObject {aka const struct _frame}' has no member named 'f_back'\r\n     for (; i < limit && frame != nullptr; frame = frame->f_back, ++i) {\r\n                                                          ^~~~~~\r\n./tensorflow/python/util/stack_trace.h:71:67: error: 'const PyFrameObject {aka const struct _frame}' has no member named 'f_lasti'\r\n       result.code_objs_.push_back(std::make_pair(code_obj, frame->f_lasti));\r\n                                                                   ^~~~~~~\r\ntensorflow/python/util/stack_trace.cc: In member function 'std::vector<tensorflow::StackFrame> tensorflow::StackTrace::ToStackFrames(const StackTraceMap&, const StackTraceFilter&, bool, int) const':\r\ntensorflow/python/util/stack_trace.cc:58:77: error: 'PyCode_Addr2Line' was not declared in this scope\r\n     const int line_number = PyCode_Addr2Line(code_obj.first, code_obj.second);\r\n                                                                             ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /tensorflow/tensorflow/lite/python/BUILD:59:10 C++ compilation of rule '//tensorflow/python/util:stack_trace' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 74 argument(s) skipped)\r\nINFO: Elapsed time: 2992.692s, Critical Path: 109.58s\r\nINFO: 9434 processes: 124 internal, 9310 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\nAs @martinwicke mentioned, since SWIG was removed I did not apply any fix to pybind like mentioned in https://github.com/pybind/pybind11/pull/2146.\r\n\r\nI have also tried to install it by using the google storage links provided in the tensorflow installation docs:\r\n```\r\npython3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.5.0-cp36-cp36m-manylinux2010_x86_64.whl\r\n```\r\nbut I get the error:\r\n```\r\nERROR: tensorflow_cpu-2.5.0-cp36-cp36m-manylinux2010_x86_64.whl is not a supported wheel on this platform.\r\n```\r\nI have also tried changing from manylinux2010 to manylinux1 to solve this, but it also did not work.\r\n\r\nLooking for most of these issues online leads me to 64 and 32 bit conflicts, but I have made sure that I am running the proper 64bit version.\r\n\r\nI have also tried doing the pip install with the latest tensorflow git head as source. I dont get any output after cloning and tensorflow remains not installed.\r\n```\r\npypy -m pip install git+https://github.com/tensorflow/tensorflow.git@master\r\n\r\nCollecting git+https://github.com/tensorflow/tensorflow.git@master\r\n  Cloning https://github.com/tensorflow/tensorflow.git (to revision master) to /tmp/pip-req-build-y109k5h2\r\n  Running command git clone -q https://github.com/tensorflow/tensorflow.git /tmp/pip-req-build-y109k5h2\r\n```\r\n\r\nI have come across several installation issues with other libraries in pypy, but I have usually been able to resolve them either by compiling from source or falling back to a specific version. Tensorflow is being rather stuborn.\r\n\r\nSomething of note is that I am able to `import tensorflow` but unable to actually use any module like `from tensorflow import keras` or `tensorflow.zeroes[int]`.\r\n\r\nIf I freeze pip, tensorflow does not appear in the list.\r\nI have also tried with the latest releases of PyPy using python 3.7 and python 3.8, with varying degrees of success. The 3.6 version I am trying now seems to be having better results, considering compatibility with other libraries I am using. @mattip are there any other directions you could give?\r\n\r\n", "You will not find a binary package available for PyPy, so the attempts to do `pip install` will not succeed. As for python3.6/python3.7: python3.6 is in security-fix only support. Many package maintainers like numpy and scipy have dropped support for it, as has PyPy. Please use 3.7.\r\n\r\nSince there are no binary packages, you will have to build from source. The relevant information in your analysis is:\r\n- The code in `./tensorflow/python/util/stack_trace.h` `'static tensorflow::StackTrace tensorflow::StackTrace::Capture(int)'` will not compile on PyPy. The best strategy here is for tensorflow to adopt the [workaround from pybind11](https://github.com/pybind/pybind11/blob/e08a58111dbea38d667b209f7543864d51a3b185/include/pybind11/pybind11.h#L2127)\r\n- `PyCode_Addr2Line` is not implemented in PyPy. That API call is undocumented and not implemented in PyPy. I opened an issue for this in [the PyPy issue tracker](https://foss.heptapod.net/pypy/pypy/-/issues/3466).\r\n\r\n", "> I finally made my pypy compatible in tensorflow. It works good but tf.conv layer is still not working yet in my pypy\r\n\r\nHi, could you share your method?"]}]