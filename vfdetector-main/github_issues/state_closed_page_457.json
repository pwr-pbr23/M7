[{"number": 40116, "title": "Fix [SplitV CPU Kernel] condition error for parallelism output judgement.", "body": "The condition wants to make sure each intra-thread has enough work.\r\nThe detail requirement should be:\r\nif num_splits < num_threads, then we just use **num_splits** intra-threads to work, each intra-thread should handle at least 4096 elements.\r\nif num_splits >= num_threads, then we use **num_threads** intra-threads to work, each intra-thread should handle multi-splits, avg sigma( elements of num_spilits / num_threads) > 4096.\r\n\r\nSo the condition should be **min**(num_splits, num_threads), not **max**.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40116) for more info**.\n\n<!-- need_author_cla -->", "@kitstar Thank you for your contribution. Can you please sign CLA? Thanks!", "@googlebot I fixed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40116) for more info**.\n\n<!-- ok -->", "We may want to ask @jewillco to review the change to split_v_op.cc.", "@jewillco Can you please take a look on this PR ? Thanks!", "I am fine with the CL but I'm not allowed to merge it myself."]}, {"number": 42805, "title": "Tensorflow\u306e\u300coverfit and underfit\u300d\u306e\u4e0d\u5099", "body": "%tensorboard --logdir {logdir}/sizes\u3000\u3092\u5b9f\u884c\u3059\u308b\u3068\u3000UsageError: Line magic function `%tensorboard` not found.\u3000\u3068\u306a\u308b\u3002", "comments": ["Hi, this is the same issue described in https://github.com/tensorflow/docs/pull/1580 I believe this issue will be fixed by merging the PR.", "thank you for teaching me about it.", "https://github.com/tensorflow/docs/pull/1580 is merged. Thank you"]}, {"number": 40115, "title": "Attempted to use a closed filewriter", "body": "Hello, everyone, I am doing cosine_matrix_learning based on tensorflow_1.14.0. When I run the code, I got a warning like this:\r\n\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\r\n  warnings.warn(\"Attempting to use a closed FileWriter. \"\r\n\r\nHowever, I found there is the reopen function in the tensorflow_writer.py, but it seems not work.\r\n\r\n def reopen(self):\r\n    \"\"\"Reopens the EventFileWriter.\r\n\r\n    Can be called after `close()` to add more events in the same directory.\r\n    The events will go into a new events file.\r\n\r\n    Does nothing if the EventFileWriter was not closed.\r\n    \"\"\"\r\n    self.event_writer.reopen()\r\n    self._closed = False\r\n\r\n\r\nIt seems that the problem is not caused by my project-related codes. I am confused about this, do I need to make some configuration about tensorflow when I train my model? I hope someone can give me some advice, thank you~\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@duyao-art \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Also you are using a very old version of tensor flow is there any particular reason to do so, can you please upgrade to later versions and let us know if that helps.", "\r\nEncountered the same problem, did you solve it, how to solve it", "@Z-ZPX  Sorry, I did not solve it at this time. I emailed the author, but got no reply now. I just kept the problem and started to do other needed things. Hope to solve it soon for both of us.", "@duyao-art \r\nPlease update as per my comment on code snippet and version details of tensorflow.", "@Saduf2019  OK, thank you so much.\r\n\r\nThe below is the URL of the relevant project. You can clone it if it is possible. Then just run train_veri.py.\r\n\r\nhttps://github.com/duyao-art/cosine_metric_learning_customer.git\r\n\r\nThe tensorflow version I used for this project is 1.14.0. Because in this project, because when I used tensorflow>=2.0, it would have this problem: ModuleNotFoundError: No module named 'tensorflow.contrib'. By the way, my python version=3.7\r\n\r\n(cosine_metric_learning) duyao@duyao-desktop:~/PycharmProjects/cosine_metric_learning$ python train_veri.py --dataset_dir=./VeRi --loss_mode=cosine-softmax --log_dir=./output/veri --run_id=cosine-softmax\r\nTraceback (most recent call last):\r\n  File \"train_veri.py\", line 42, in <module>\r\n    import train_app\r\n  File \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", line 5, in <module>\r\n    import tensorflow.contrib.slim as slim\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\nWhile when I run the code based on tensorflow 1.14.0, another problem happens:\r\n\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\r\n\r\nSo I wonder if it is something about my configuration error of tensorflow? I hope you could give me some advice, thank you so much for your attention.\r\n\r\n\r\n", "@duyao-art tf.contrib module has been depricated in tensorflow 2.x. So, you cannot use it. \r\nAlso can you provide the full stacktrace of your error when running with tensorflow 1.14. Thanks!", "@duyao-art Can you please respond to the above comment so that we can take the discussion further. Thanks!", "@gowthamkpr  Thank you so much for your attention.\r\n I am doing another research work recently, so I did not spend too much time on this project. I plan to solve it when I finish my work now. Anyway, thank you so much for your interest, otherwise, I still need to struggle with it. \r\nBelow is the running result of my code. I did not know what was wrong with the code....\r\n\r\n\r\n################################################################################\r\n\r\nduyao@duyao-desktop:~$ cd ~/PycharmProjects/cosine_metric_learning/\r\nduyao@duyao-desktop:~/PycharmProjects/cosine_metric_learning$ conda activate cosine_metric_learning\r\n(cosine_metric_learning) duyao@duyao-desktop:~/PycharmProjects/cosine_metric_learning$ python train_veri.py --dataset_dir=./VeRi --loss_mode=cosine-softmax --log_dir=./output/veri --run_id=cosine-softmax\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nTrain set size: 34267 images, 519 identites\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:243: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:250: The name tf.read_file is deprecated. Please use tf.io.read_file instead.\r\n\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:252: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\r\n\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/queued_trainer.py:316: The name tf.FIFOQueue is deprecated. Please use tf.queue.FIFOQueue instead.\r\n\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:266: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\r\n\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd959bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd959bd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/nets/deep_sort/network_definition.py:19: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\r\n\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd903510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd903510>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/nets/deep_sort/network_definition.py:28: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\r\n\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd882b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd882b10>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd82db50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd82db50>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f44dd843750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f44dd843750>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd82d750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd82d750>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd8a8350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd8a8350>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dd971b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dd971b10>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd806450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd806450>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd7a5250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd7a5250>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd971b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd971b10>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd7a5f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd7a5f50>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dd8f1750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dd8f1750>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd8add50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd8add50>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd7da7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd7da7d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd7da290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd7da290>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd7f6710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd7f6710>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dca622d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dca622d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd7a5dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd7a5dd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd7a5450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd7a5450>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd8f11d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd8f11d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd7a5450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd7a5450>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dc96e810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dc96e810>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dca731d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dca731d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd8f6590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd8f6590>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dca0b410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dca0b410>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dc853a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dc853a50>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd911c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dd911c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dd8f13d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dd8f13d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dc9dd550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dc9dd550>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd805110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd805110>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dc7b7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dc7b7290>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd78fed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dd78fed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dc6d2850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f44dc6d2850>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dc743c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dc743c90>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dc7c58d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f44dc7c58d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nfeature dimensionality:  128\r\nWARNING:tensorflow:From /home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.flatten instead.\r\nWARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f44dc884990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f44dc884990>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dd882750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f44dd882750>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f44dc7e9d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f44dc7e9d90>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/nets/deep_sort/network_definition.py:84: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ndim is deprecated, use axis instead\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/nets/deep_sort/network_definition.py:94: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/nets/deep_sort/network_definition.py:97: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:615: sparse_softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\r\nInstructions for updating:\r\nUse tf.losses.sparse_softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\r\nWARNING:tensorflow:From /home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:409: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\r\nInstructions for updating:\r\nUse tf.losses.compute_weighted_loss instead.\r\nWARNING:tensorflow:From /home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:152: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nWARNING:tensorflow:From /home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:154: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nWARNING:tensorflow:From /home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:121: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\r\nInstructions for updating:\r\nUse tf.losses.add_loss instead.\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/losses.py:142: The name tf.log is deprecated. Please use tf.math.log instead.\r\n\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:280: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:282: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\r\n\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:284: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\r\n\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:290: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\r\n\r\n---------------------------------------\r\nRun ID:  cosine-softmax\r\nLog directory:  ./output/veri/cosine-softmax\r\n---------------------------------------\r\nWARNING:tensorflow:From /home/duyao/PycharmProjects/cosine_metric_learning/queued_trainer.py:404: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\nWARNING:tensorflow:From /home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\n2020-06-18 09:33:26.779723: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-18 09:33:26.999128: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3199980000 Hz\r\n2020-06-18 09:33:27.000557: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5628c5a85590 executing computations on platform Host. Devices:\r\n2020-06-18 09:33:27.000623: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING:tensorflow:From /home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\n2020-06-18 09:33:27.409714: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\nWARNING:tensorflow:From /home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file utilities to get mtimes.\r\n2020-06-18 09:33:29.475248: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at tensor_array_ops.cc:447 : Invalid argument: TensorArray map/TensorArray_1_1: Could not write to TensorArray index 1 because the value shape is [190,252,3] which is incompatible with the TensorArray's inferred element shape: [366,300,3] (consider setting infer_shape=False).\r\n2020-06-18 09:33:29.585824: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at tensor_array_ops.cc:447 : Invalid argument: TensorArray map/TensorArray_1_4: Could not write to TensorArray index 1 because the value shape is [441,654,3] which is incompatible with the TensorArray's inferred element shape: [331,316,3] (consider setting infer_shape=False).\r\n2020-06-18 09:33:29.624818: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at tensor_array_ops.cc:447 : Invalid argument: TensorArray map/TensorArray_1_3: Could not write to TensorArray index 1 because the value shape is [569,752,3] which is incompatible with the TensorArray's inferred element shape: [179,169,3] (consider setting infer_shape=False).\r\n2020-06-18 09:33:29.642372: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at tensor_array_ops.cc:447 : Invalid argument: TensorArray map/TensorArray_1_5: Could not write to TensorArray index 1 because the value shape is [184,238,3] which is incompatible with the TensorArray's inferred element shape: [210,298,3] (consider setting infer_shape=False).\r\nEnqueueError: EnqueueError: EnqueueError: EnqueueError: TensorArray map/TensorArray_1_1: Could not write to TensorArray index 1 because the value shape is [190,252,3] which is incompatible with the TensorArray's inferred element shape: [366,300,3] (consider setting infer_shape=False).\r\n\t [[node map/while/TensorArrayWrite/TensorArrayWriteV3 (defined at /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:251) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node map/while/TensorArrayWrite/TensorArrayWriteV3:\r\n map/while/DecodeJpeg (defined at /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:250)\r\n\r\nOriginal stack trace for 'map/while/TensorArrayWrite/TensorArrayWriteV3':\r\n  File \"train_veri.py\", line 133, in <module>\r\n    main()\r\n  File \"train_veri.py\", line 97, in main\r\n    **train_kwargs)\r\n  File \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", line 188, in train_loop\r\n    trainable_scopes=trainable_scopes)\r\n  File \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", line 251, in create_trainer\r\n    filename_var, back_prop=False, dtype=tf.uint8)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 268, in map_fn\r\n    maximum_iterations=n)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3501, in while_loop\r\n    return_same_structure)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3012, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2937, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3456, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 260, in compute\r\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 260, in <listcomp>\r\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1191, in write\r\n    return self._implementation.write(index, value, name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 293, in write\r\n    name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 8288, in tensor_array_write_v3\r\n    flow_in=flow_in, name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nTensorArray map/TensorArray_1_3: Could not write to TensorArray index 1 because the value shape is [569,752,3] which is incompatible with the TensorArray's inferred element shape: [179,169,3] (consider setting infer_shape=False).\r\n\t [[node map/while/TensorArrayWrite/TensorArrayWriteV3 (defined at /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:251) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node map/while/TensorArrayWrite/TensorArrayWriteV3:\r\n map/while/DecodeJpeg (defined at /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:250)\r\n\r\nOriginal stack trace for 'map/while/TensorArrayWrite/TensorArrayWriteV3':\r\n  File \"train_veri.py\", line 133, in <module>\r\n    main()\r\n  File \"train_veri.py\", line 97, in main\r\n    **train_kwargs)\r\n  File \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", line 188, in train_loop\r\n    trainable_scopes=trainable_scopes)\r\n  File \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", line 251, in create_trainer\r\n    filename_var, back_prop=False, dtype=tf.uint8)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 268, in map_fn\r\n    maximum_iterations=n)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3501, in while_loop\r\n    return_same_structure)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3012, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2937, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3456, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 260, in compute\r\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 260, in <listcomp>\r\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1191, in write\r\n    return self._implementation.write(index, value, name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 293, in write\r\n    name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 8288, in tensor_array_write_v3\r\n    flow_in=flow_in, name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nTensorArray map/TensorArray_1_5: Could not write to TensorArray index 1 because the value shape is [184,238,3] which is incompatible with the TensorArray's inferred element shape: [210,298,3] (consider setting infer_shape=False).\r\n\t [[node map/while/TensorArrayWrite/TensorArrayWriteV3 (defined at /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:251) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node map/while/TensorArrayWrite/TensorArrayWriteV3:\r\n map/while/DecodeJpeg (defined at /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:250)\r\n\r\nOriginal stack trace for 'map/while/TensorArrayWrite/TensorArrayWriteV3':\r\n  File \"train_veri.py\", line 133, in <module>\r\n    main()\r\n  File \"train_veri.py\", line 97, in main\r\n    **train_kwargs)\r\n  File \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", line 188, in train_loop\r\n    trainable_scopes=trainable_scopes)\r\n  File \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", line 251, in create_trainer\r\n    filename_var, back_prop=False, dtype=tf.uint8)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 268, in map_fn\r\n    maximum_iterations=n)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3501, in while_loop\r\n    return_same_structure)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3012, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2937, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3456, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 260, in compute\r\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 260, in <listcomp>\r\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1191, in write\r\n    return self._implementation.write(index, value, name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 293, in write\r\n    name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 8288, in tensor_array_write_v3\r\n    flow_in=flow_in, name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nTensorArray map/TensorArray_1_4: Could not write to TensorArray index 1 because the value shape is [441,654,3] which is incompatible with the TensorArray's inferred element shape: [331,316,3] (consider setting infer_shape=False).\r\n\t [[node map/while/TensorArrayWrite/TensorArrayWriteV3 (defined at /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:251) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node map/while/TensorArrayWrite/TensorArrayWriteV3:\r\n map/while/DecodeJpeg (defined at /home/duyao/PycharmProjects/cosine_metric_learning/train_app.py:250)\r\n\r\nOriginal stack trace for 'map/while/TensorArrayWrite/TensorArrayWriteV3':\r\n  File \"train_veri.py\", line 133, in <module>\r\n    main()\r\n  File \"train_veri.py\", line 97, in main\r\n    **train_kwargs)\r\n  File \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", line 188, in train_loop\r\n    trainable_scopes=trainable_scopes)\r\n  File \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", line 251, in create_trainer\r\n    filename_var, back_prop=False, dtype=tf.uint8)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 268, in map_fn\r\n    maximum_iterations=n)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3501, in while_loop\r\n    return_same_structure)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3012, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2937, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3456, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 260, in compute\r\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 260, in <listcomp>\r\n    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1191, in write\r\n    return self._implementation.write(index, value, name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 293, in write\r\n    name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 8288, in tensor_array_write_v3\r\n    flow_in=flow_in, name=name)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n/home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\r\n  warnings.warn(\"Attempting to use a closed FileWriter. \"\r\n(cosine_metric_learning) duyao@duyao-desktop:~/PycharmProjects/cosine_metric_learning$ \r\n\r\n##############################################################################\r\n\r\nThank you again for your kind help~\r\n", "@duyao-art I would recommend you to post this issue in stackoverflow as  this issue is not related to build/install, bug/performance or feature request related issues. Thanks!", "> @ Saduf2019 OK, muchas gracias.\r\n> \r\n> La siguiente es la URL del proyecto relevante. Puede clonarlo si es posible. Luego solo ejecuta train_veri.py.\r\n> \r\n> https://github.com/duyao-art/cosine_metric_learning_customer.git\r\n> \r\n> La versi\u00f3n de tensorflow que utilic\u00e9 para este proyecto es 1.14.0. Porque en este proyecto, porque cuando usaba tensorflow> = 2.0, tendr\u00eda este problema: ModuleNotFoundError: Ning\u00fan m\u00f3dulo llamado 'tensorflow.contrib'. Por cierto, mi versi\u00f3n de Python = 3.7\r\n> \r\n> (cosine_metric_learning) duyao @ duyao-desktop: ~ / PycharmProjects / cosine_metric_learning $ python train_veri.py --dataset_dir =. / VeRi --loss_mode = cosine-softmax --log_dir =. / output / veri --run_id = cosine-softmax\r\n> Traceback (\u00faltima llamada m\u00e1s reciente):\r\n> Archivo \"train_veri.py\", l\u00ednea 42, en el archivo\r\n> import train_app\r\n> \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", l\u00ednea 5, en\r\n> import tensorflow.contrib.slim como Slim\r\n> ModuleNotFoundError: Ning\u00fan m\u00f3dulo llamado 'tensorflow.contrib'\r\n> \r\n> Mientras que cuando ejecuto el c\u00f3digo basado en tensorflow 1.14.0, ocurre otro problema:\r\n> \r\n> /home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Intentando utilizar un FileWriter cerrado. La operaci\u00f3n ser\u00e1 un noop a menos que FileWriter se vuelva a abrir expl\u00edcitamente.\r\n> \r\n> Entonces, \u00bfme pregunto si se trata de un error de configuraci\u00f3n de tensorflow? Espero que me puedan dar algunos consejos, muchas gracias por su atenci\u00f3n.\r\n\r\n**hello, I also present the same error, could you solve it ????\r\nplease help me**", "> > @ Saduf2019 OK, muchas gracias.\r\n> > La siguiente es la URL del proyecto relevante. Puede clonarlo si es posible. Luego solo ejecuta train_veri.py.\r\n> > https://github.com/duyao-art/cosine_metric_learning_customer.git\r\n> > La versi\u00f3n de tensorflow que utilic\u00e9 para este proyecto es 1.14.0. Porque en este proyecto, porque cuando usaba tensorflow> = 2.0, tendr\u00eda este problema: ModuleNotFoundError: Ning\u00fan m\u00f3dulo llamado 'tensorflow.contrib'. Por cierto, mi versi\u00f3n de Python = 3.7\r\n> > (cosine_metric_learning) duyao @ duyao-desktop: ~ / PycharmProjects / cosine_metric_learning $ python train_veri.py --dataset_dir =. / VeRi --loss_mode = cosine-softmax --log_dir =. / output / veri --run_id = cosine-softmax\r\n> > Traceback (\u00faltima llamada m\u00e1s reciente):\r\n> > Archivo \"train_veri.py\", l\u00ednea 42, en el archivo\r\n> > import train_app\r\n> > \"/home/duyao/PycharmProjects/cosine_metric_learning/train_app.py\", l\u00ednea 5, en\r\n> > import tensorflow.contrib.slim como Slim\r\n> > ModuleNotFoundError: Ning\u00fan m\u00f3dulo llamado 'tensorflow.contrib'\r\n> > Mientras que cuando ejecuto el c\u00f3digo basado en tensorflow 1.14.0, ocurre otro problema:\r\n> > /home/duyao/anaconda3/envs/cosine_metric_learning/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Intentando utilizar un FileWriter cerrado. La operaci\u00f3n ser\u00e1 un noop a menos que FileWriter se vuelva a abrir expl\u00edcitamente.\r\n> > Entonces, \u00bfme pregunto si se trata de un error de configuraci\u00f3n de tensorflow? Espero que me puedan dar algunos consejos, muchas gracias por su atenci\u00f3n.\r\n> \r\n> **hello, I also present the same error, could you solve it ???? please help me**\r\n\r\nI also have the exact same error. Did you find any solution?\r\nAny help will be appreciated. Thanks."]}, {"number": 40113, "title": "ERROR: /tensorflow_src/tensorflow/compiler/aot/BUILD:277:1: C++ compilation of rule '//tensorflow/compiler/aot:embedded_protocol_buffers' failed (Exit 1)", "body": "\r\n\r\n**System information**\r\n- OS Platform: host: mac 14.10 \r\n- docker image:  tensorflow/tensorflow:latest-devel\r\n- TensorFlow version: 2.2\r\n- Python version: 3.6.9\r\n- building tensorflow from source using docker on mac with ubuntu in docker image\r\n- Bazel version (if compiling from source): 3.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NONE ( cpu only build)\r\n- GPU model and memory: NONE \r\n\r\n**the problem**\r\n```\r\nERROR: /tensorflow_src/tensorflow/compiler/aot/BUILD:277:1: C++ compilation of rule '//tensorflow/compiler/aot:embedded_protocol_buffers' failed (Exit 1)\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:41:0,\r\n                 from ./tensorflow/core/framework/numeric_types.h:24,\r\n                 from ./tensorflow/compiler/xla/types.h:23,\r\n                 from ./tensorflow/compiler/xla/array.h:34,\r\n                 from ./tensorflow/compiler/xla/array2d.h:29,\r\n                 from ./tensorflow/compiler/xla/literal.h:32,\r\n                 from ./tensorflow/compiler/xla/service/llvm_ir/llvm_util.h:33,\r\n                 from tensorflow/compiler/aot/embedded_protocol_buffers.cc:31:\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:30:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m256i, 20> Packet32q8i;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:31:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m256i, 21> Packet16q16i;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:32:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m256i, 22> Packet32q8u;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:33:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m128i, 23> Packet16q8i;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:34:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m128i, 25> Packet16q8u;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:35:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m128i, 26> Packet8q16i;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:36:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m256i, 27> Packet8q32i;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:37:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m128i, 28> Packet4q32i;\r\n                                         ^\r\nIn file included from ./tensorflow/compiler/xla/index_util.h:24:0,\r\n                 from ./tensorflow/compiler/xla/literal.h:35,\r\n                 from ./tensorflow/compiler/xla/service/llvm_ir/llvm_util.h:33,\r\n                 from tensorflow/compiler/aot/embedded_protocol_buffers.cc:31:\r\n./tensorflow/compiler/xla/shape.h: In member function 'void xla::Shape::clear_dynamic_dimensions()':\r\n./tensorflow/compiler/xla/shape.h:139:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       for (int64 i = 0; i < dynamic_dimensions_.size(); ++i) {\r\n                         ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/compiler/xla/service/llvm_ir/llvm_util.h:34:0,\r\n                 from tensorflow/compiler/aot/embedded_protocol_buffers.cc:31:\r\n./tensorflow/compiler/xla/service/hlo_instruction.h: In member function 'void xla::HloInstruction::ReplaceCalledComputations(std::function<xla::HloComputation*(xla::HloComputation*)>)':\r\n./tensorflow/compiler/xla/service/hlo_instruction.h:1431:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int64 i = 0; i < called_computations_.size(); ++i) {\r\n                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:29:0,\r\n                 from ./tensorflow/core/platform/logging.h:27,\r\n                 from ./tensorflow/core/platform/status.h:24,\r\n                 from ./tensorflow/core/lib/core/status.h:19,\r\n                 from ./tensorflow/compiler/xla/status.h:19,\r\n                 from ./tensorflow/compiler/xla/statusor.h:18,\r\n                 from ./tensorflow/compiler/aot/embedded_protocol_buffers.h:24,\r\n                 from tensorflow/compiler/aot/embedded_protocol_buffers.cc:16:\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_LEImpl(const T1&, const T2&, const char*) [with T1 = long long int; T2 = long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\n./tensorflow/compiler/xla/shape_util.h:129:5:   required from here\r\n./tensorflow/core/platform/default/logging.h:388:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\r\n./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))\r\n                                                 ^\r\n./tensorflow/core/platform/default/logging.h:388:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\r\n ^~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/compiler/xla/array2d.h:29:0,\r\n                 from ./tensorflow/compiler/xla/literal.h:32,\r\n                 from ./tensorflow/compiler/xla/service/llvm_ir/llvm_util.h:33,\r\n                 from tensorflow/compiler/aot/embedded_protocol_buffers.cc:31:\r\n./tensorflow/compiler/xla/array.h: In instantiation of 'bool xla::Array<T>::operator==(const xla::Array<T>&) const [with T = long long int]':\r\n./tensorflow/compiler/xla/service/hlo_sharding.h:190:38:   required from here\r\n./tensorflow/compiler/xla/array.h:430:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int64 i = 0; i < sizes_.size(); ++i) {\r\n                       ~~^~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:29:0,\r\n                 from ./tensorflow/core/platform/logging.h:27,\r\n                 from ./tensorflow/core/platform/status.h:24,\r\n                 from ./tensorflow/core/lib/core/status.h:19,\r\n                 from ./tensorflow/compiler/xla/status.h:19,\r\n                 from ./tensorflow/compiler/xla/statusor.h:18,\r\n                 from ./tensorflow/compiler/aot/embedded_protocol_buffers.h:24,\r\n                 from tensorflow/compiler/aot/embedded_protocol_buffers.cc:16:\r\n./tensorflow/compiler/xla/array.h: In instantiation of 'tensorflow::int64 xla::Array<T>::dim(tensorflow::int64) const [with T = int; tensorflow::int64 = long long int]':\r\n./tensorflow/compiler/xla/array2d.h:71:44:   required from 'tensorflow::int64 xla::Array2D<T>::height() const [with T = int; tensorflow::int64 = long long int]'\r\n./tensorflow/compiler/xla/service/computation_placer.h:47:45:   required from here\r\n./tensorflow/compiler/xla/array.h:406:13: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     CHECK(n < sizes_.size());\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/compiler/xla/array.h:406:5: note: in expansion of macro 'CHECK'\r\n     CHECK(n < sizes_.size());\r\n     ^\r\ntensorflow/compiler/aot/embedded_protocol_buffers.cc: At global scope:\r\ntensorflow/compiler/aot/embedded_protocol_buffers.cc:152:1: fatal error: opening dependency file bazel-out/host/bin/tensorflow/compiler/aot/_objs/embedded_protocol_buffers/embedded_protocol_buffers.pic.d: Structure needs cleaning\r\n }  // namespace tensorflow\r\n ^\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /tensorflow_src/tensorflow/python/tools/BUILD:82:1 C++ compilation of rule '//tensorflow/compiler/aot:embedded_protocol_buffers' failed (Exit 1)\r\nINFO: Elapsed time: 82780.701s, Critical Path: 581.17s\r\nINFO: 16036 processes: 16036 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n- I am trying to build tensorflow from source to enable the AVX and FMA instruction. i followed the exact set of instructions inorder to start building. As given [here](https://www.tensorflow.org/install/source#docker_linux_builds) i am building the cpu only version.\r\n- Since i am building from the latest one, I should be building from master branch. so I initiated using the below command: \r\n\r\n` bazel build --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --local_cpu_resources=1 --local_ram_resources=2048  //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n- after this the analysis phase was done. The error occurred during compilation.\r\n\r\nunable to diagnose the error. Please help\r\n\r\nEDIT: update: I tried to build again. but the error this arror occured: \r\n```ERROR: /tensorflow_src/tensorflow/compiler/aot/BUILD:295:1: failed to create output directory '/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/compiler/aot/_objs/aot_only_var_handle_op': /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/compiler/aot/_objs/aot_only_var_handle_op (Structure needs cleaning)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /tensorflow_src/tensorflow/python/tools/BUILD:99:1 failed to create output directory '/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/compiler/aot/_objs/aot_only_var_handle_op': /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/compiler/aot/_objs/aot_only_var_handle_op (Structure needs cleaning)```", "comments": ["Duplicate of #40141 \r\nProbably we can follow it on that thread.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40113\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40113\">No</a>\n"]}, {"number": 40112, "title": "import tensorflow as tf not working on WIN10", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIN10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tf2.2.0\r\n- TensorFlow version (use command below):\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-14-ebece4447484> in <module>\r\n      5 import re\r\n      6 from bs4 import BeautifulSoup\r\n----> 7 import tensorflow as tf\r\n      8 # from keras.preprocessing.text import Tokenizer\r\n      9 # from keras.preprocessing.sequence import pad_sequences\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\SHARATHWIN10\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\SHARATHWIN10\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\SHARATHWIN10\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\SHARATHWIN10\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\SHARATHWIN10\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n**Describe the expected behavior**\r\nShould work without any errors\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nimport tensorflow as tf\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["You have not configured the interpreter correctly. ", "@SharathChandra-AV \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest [microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).Please, go through this [link](https://stackoverflow.com/questions/49932993/importerror-dll-load-failed-a-dynamic-link-library-dll-initialization-routin) and see if it helps you\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #23683 #28713 #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "Hello,\n\nI am using win10 and x64 based.\nNo gpu avail able.\nAlso, I downloaded vc++ distributable as well. Still doesn't work\n\nSharath\n\nOn Wed, Jun 3, 2020, 16:49 Saduf2019 <notifications@github.com> wrote:\n\n> @SharathChandra-AV <https://github.com/SharathChandra-AV>\n>\n> What is make/model of your cpu?\n> I suspect your cpu model does not support AVX instructions sets.See hardware\n> requirements\n> <https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements>\n> Make sure to download the latest microsoft visual c++ redistributable\n> from here\n> <https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads>\n> .\n> .Also, please follow the instructions from to install from Tensorflow\n> website <https://www.tensorflow.org/install/source_windows>.Please, go\n> through this link\n> <https://stackoverflow.com/questions/49932993/importerror-dll-load-failed-a-dynamic-link-library-dll-initialization-routin>\n> and see if it helps you\n>\n> Please, check Your CPU/Python is on 32 bits?Please, refer #36167\n> <https://github.com/tensorflow/tensorflow/issues/36167> and see if it\n> helps you.Please, refer similar issue #23683\n> <https://github.com/tensorflow/tensorflow/issues/23683> #28713\n> <https://github.com/tensorflow/tensorflow/issues/28713> #36167\n> <https://github.com/tensorflow/tensorflow/issues/36167> #36151\n> <https://github.com/tensorflow/tensorflow/issues/36151> #36138\n> <https://github.com/tensorflow/tensorflow/issues/36138> #36054\n> <https://github.com/tensorflow/tensorflow/issues/36054> #36045\n> <https://github.com/tensorflow/tensorflow/issues/36045> #36020\n> <https://github.com/tensorflow/tensorflow/issues/36020> #36003\n> <https://github.com/tensorflow/tensorflow/issues/36003> #35988\n> <https://github.com/tensorflow/tensorflow/issues/35988> #35903\n> <https://github.com/tensorflow/tensorflow/issues/35903> #35880\n> <https://github.com/tensorflow/tensorflow/issues/35880> #35865\n> <https://github.com/tensorflow/tensorflow/issues/35865> #35805\n> <https://github.com/tensorflow/tensorflow/issues/35805> #35789\n> <https://github.com/tensorflow/tensorflow/issues/35789> #35773\n> <https://github.com/tensorflow/tensorflow/issues/35773> #35772\n> <https://github.com/tensorflow/tensorflow/issues/35772> #35767\n> <https://github.com/tensorflow/tensorflow/issues/35767> #35766\n> <https://github.com/tensorflow/tensorflow/issues/35766> #35749\n> <https://github.com/tensorflow/tensorflow/issues/35749> #35721\n> <https://github.com/tensorflow/tensorflow/issues/35721> #35618\n> <https://github.com/tensorflow/tensorflow/issues/35618> #35204\n> <https://github.com/tensorflow/tensorflow/issues/35204>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40112#issuecomment-638130870>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJKF47UMPSTQ5RWDG7QUSVLRUYWUHANCNFSM4NRLXXOA>\n> .\n>\n", "@SharathChandra-AV \r\nCan you please confirm if you have referred tot this comment:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156", "Give me till tomorrow\n\nOn Thu, Jun 4, 2020, 17:28 Saduf2019 <notifications@github.com> wrote:\n\n> @SharathChandra-AV <https://github.com/SharathChandra-AV>\n> Can you please confirm if you have referred tot this comment:\n>\n> #36167 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40112#issuecomment-638802950>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJKF47UBCNXXT7VN6V4XMVLRU6D73ANCNFSM4NRLXXOA>\n> .\n>\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40112\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40112\">No</a>\n"]}, {"number": 40110, "title": "Dynamically added layers do not show up in custom model layers member", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Os X 10.15.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI have a custom model https://gist.github.com/SandeepNaidu/d4d9482daa466fa21b22211693bbae82 which is used to create dynamically the architecture in terms of depth and breadth of the network. Even upon adding directly to self.layers it does not show up later when the layers are retrieved. The number of hidden layers and breadth can vary based on experiments. So need a better way to add them to self.layers. \r\n\r\n**Describe the expected behavior**\r\nAbility to add layers dynamically to layers object of Model in custom model\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://gist.github.com/SandeepNaidu/d4d9482daa466fa21b22211693bbae82\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.2, nightly versions(`2.3.0-dev20200602`) and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/7eac58669bbc68f9abe20f12764c2b37/untitled949.ipynb).Thanks!\r\n\r\n", "Was able to reproduce the issue in TF 2.6 as well. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/df9e145f02310b54f89aadf6112689ef/untitled95.ipynb) for reference.Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40110\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40110\">No</a>\n"]}, {"number": 40109, "title": "Fix the docs and parameters warning of Hessians with tf 2.0 upgrader", "body": "## Problem\r\nThis fixes the issue of https://github.com/tensorflow/tensorflow/issues/40051, where the description of API doc v2.0 on Hessians is not correct.\r\n\r\n\r\n## Solutions\r\n1. Fix HessianV2's docstring\r\n2. Add parameters warnings and test cases for tf 2.0 upgrader.", "comments": ["@HughKu  Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 40108, "title": "TF reports cublas64_10.dll not found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Version 1909\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.2\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: NVIDIA Quadro M1200 - 4 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to get tensorflow to make use of the NVIDIA GPU in my laptop.  Tensorflow is able to find all CUDA-related libraries *except* cublas64_10.dll.  After installing CUDA 10.1 (I've tried 10.1 and 10.1 update 2, both without any success), I copied all the .dll files that got installed in \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\" to C:\\Windows\\System32.  I included cublas64_10.dll when I copied these files.  (Before copying the .dll files, tensorflow could not find any of the CUDA-related libraries, even after adding \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\" to my PATH and rebooting my computer a couple times.)  I confirmed that the cublas64_10.dll in C:\\Windows\\System32 was bit-for-bit identical to the one in \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\", and that I had permission to read cublas64_10.dll.  (I copied this file from C:\\Windows\\System32 to a different directory to confirm this.)  I also downloaded just CUBLAS from CUDA 10.2, being very careful no other components got installed on my system, and then copied the cublas64_10.dll file from that version of CUDA to C:\\Windows\\System32.  None of my efforts were fruitful.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npython -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n>python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\r\n2020-06-02 21:13:03.752642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-02 21:13:06.095237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-02 21:13:06.576837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: Quadro M1200 computeCapability: 5.0\r\ncoreClock: 1.148GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-06-02 21:13:06.588734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-02 21:13:06.594961: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-06-02 21:13:06.604653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-06-02 21:13:06.615330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-06-02 21:13:06.626644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-06-02 21:13:06.635321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-06-02 21:13:06.652175: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-02 21:13:06.658845: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n[]\r\n\r\n>DIR \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\\cublas64_10.dll\" C:\\Windows\\System32\\cublas64_10.dll\r\n Volume in drive C is OSDisk\r\n Volume Serial Number is E295-9E24\r\n\r\n Directory of C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\r\n\r\n07/28/2019  10:30 PM        58,830,848 cublas64_10.dll\r\n               1 File(s)     58,830,848 bytes\r\n\r\n Directory of C:\\Windows\\System32\r\n\r\n07/28/2019  10:30 PM        58,830,848 cublas64_10.dll\r\n               1 File(s)     58,830,848 bytes\r\n               0 Dir(s)  64,581,230,592 bytes free\r\n\r\n>PATH\r\nPATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp;;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files (x86)\\WebEx\\Productivity Tools;C:\\Program Files\\osquery;C:\\Program Files\\Intel\\WiFi\\bin;C:\\Program Files\\Common Files\\Intel\\WirelessCommon;C:\\Program Files\\nodejs;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Perforce;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.4.0\\;C:\\Users\\figueroa\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\figueroa\\AppData\\Roaming\\npm;C:\\Users\\figueroa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\Scripts;\"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\";\"U:\\Perforce\\sw\\main\\apps\\p4review\";\"U:\\Perforce\\sw\\tools\\cygnus\\bin\";\"U:\\Perforce\\sw\\tools\\win64\\ActivePerl\\5.10.1.1006\\bin\"\r\n", "comments": ["@SamFigueroa Sometimes it feels difficult when compatibility issues arise. I can understand your position. Can you please uninstall completely CUDA as well as TF and then install CUDA and TF2.2. It worked for me when I had similar issue. Thanks!", "Would it help if I were to downgrade to TF2.1?", "@SamFigueroa Requirements are almost same for 2.1 except few as shown below. check [here](https://www.tensorflow.org/install/source_windows) for other versions.\r\n\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-2.2.0 | 3.5-3.8 | MSVC 2019 | Bazel 2.0.0 | 7.4 | 10.1\r\ntensorflow_gpu-2.1.0 | 3.5-3.7 | MSVC 2019 | Bazel 0.27.1-0.29.1 | 7.4 | 10.1\r\n\r\nI would suggest installing `TF2.2` would be better as there were lot of performance improvements in `TF2.2`. Thanks!", "Could the version of cuDNN be the problem?  I installed version 7.6.5, which is the latest on NVIDIA's website.  Version 7.4 does not seem to be available for CUDA 10.1.  The most recent version of CUDA for which cuDNN version 7.4.x is available is CUDA 10.0, but that's not the right version for tf 2.1 or 2.2.  Maybe I should use tf 2.0 so I can get all the right versions of CUDA and related libraries that are supposed to work together?", "I completely uninstalled tf and CUDA 10.1, and then reinstalled both, but this did not help.  On a hunch that the key clue was that I had to copy .dll files to C:\\Windows\\System32 in order for tf to find any of them at all, I decided to try the version of Python that came with MS Visual Studio, which is 3.7.4.  I reinstalled tf with this version of Python.  Now I get:\r\n\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin>PYTHON -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\r\n2020-06-04 21:08:33.482516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-04 21:08:35.007585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-04 21:08:35.477814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: Quadro M1200 computeCapability: 5.0\r\ncoreClock: 1.148GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-06-04 21:08:35.487926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-04 21:08:35.498249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-04 21:08:36.138902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-06-04 21:08:36.406474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-06-04 21:08:36.452724: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-06-04 21:08:36.931253: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-06-04 21:08:37.000022: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-04 21:08:37.006212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n\r\nI get the same thing, even after deleting all the .dll files I had copied to C:\\Windows\\System32.  My conclusion, then, is that I need to invest in a better version of Python, as the one I got from the MS Store is worth what I paid for, i.e., -0-.\r\n\r\nMy next experiment will be to update to CUDA 10.1 Update 2 (I installed plain 10.1 just to be safe), as well as to cuDNN 7.6.5.  (I installed cuDNN 7.4.2, again, just to be safe, but according to NVIDIA's website, this version of cuDNN is not for CUDA 10.1.)  Hopefully updating won't break anything.", "@SamFigueroa Did you check `CUDA 10.1 Update 2`? From the log above, i see some are from CUDA10 (cublas64_10.dll, curand64_10.dll etc) and one is from CUDA10.1 (cudart64_101.dll). Not sure whether it was completely removed or not. Thanks!", "@jgasperlin  \r\nI have been [asking something like this **since 6 months ago**](https://github.com/tensorflow/tensorflow/issues/35405). No progress for anything related to tensorflow2.*\r\n\r\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.0.0.zip\r\n\r\n\r\nAny chance if you can provide a link to your gpu version of libtensorflow?  So we can continue to test while Google finally makes up the mind to support Windows developers", "I updated to CUDA 10.1 Update 2 without uninstalling CUDA, and I also updated to cuDNN 7.6.5, which essentially involved just overwriting the .dll file from the previous version of cuDNN.  I verified that tf was still able to make use of my GPU.  (One way to confirm this beyond just the one-line Python program is to run the training for a model we are working on.  It runs 3-4X faster with the GPU I have, as compared to running just on the CPU.)\r\n\r\nNext, I downloaded Python 3.8.3 directly from python.org, and verified all of the above.  I also downloaded TensorRT, but I haven't tried to determine if the GPU is actually being used for inference.  What I can say is that when I run inference, it is finding the GPU in my computer.\r\n\r\nIn summary, at this point, my Windows 10 laptop is now fully set up for using tensorflow, and it is definitely using the GPU for training, and maybe even for inference.  I look forward to the next version of tensorflow, as that should allow me to use some other systems, such as my Linux desktop, which has CUDA 10.2.  I am unwilling to downgrade to CUDA 10.1 on that system, and my efforts to build tensorflow from source were unsuccessful, but that's a separate story.", "@SamFigueroa Is it fair to say the issue was resolved? If yes, then please close the issue. For installing TF from source, you could open a new issue so that it is easy for the users to follow the thread. Thanks!", "Closing, as this issue is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40108\">No</a>\n", "Just to summarize the root cause, the Python I got from the Microsoft Store was not able to find the CUDA .dll files I installed.  Using Python from a different source, such as python.org, fixed this issue.", "I accounted the same problem. After downgrading the version of tensorflow feom 2.3 to 2.2, the problem was solved quickily.", "Now I also account  the same problem.Follow the author's method to download python3.7 from python.org,but don't solve the problem.", "I try to download tensorflow2.2.0 version", "If python 3.7 works but python 3.8 doesn't, please read:\r\n\r\nhttps://github.com/python/cpython/commit/2438cdf0e932a341c7613bf4323d06b91ae9f1f1\r\nhttps://bugs.python.org/issue36085\r\n\r\nMaybe they are relevant.\r\n\r\nTo verify it, you may use WinDBG and turn on loadsnap debugging , it will show which dir it searched. \r\n", "Guys, I just found the answer. Somehow in bin folder should be the files: cublasLt64_10.dll and cublas64_10.dll. There will be no error if there is no cublasLt64_10 file there but it must be there. Also if you have cublasLt64_**11** just rename it to cublasLt64_**10**", "> There will be no error if there is no cublasLt64_10 file there but it must be there.\r\n\r\nThis was exactly the problem I was having and your comment helped me solve it. Thank you so much! \r\n", "> Guys, I just found the answer. Somehow in bin folder should be the files: cublasLt64_10.dll and cublas64_10.dll. There will be no error if there is no cublasLt64_10 file there but it must be there. Also if you have cublasLt64_**11** just rename it to cublasLt64_**10**\r\n\r\nI just had the same issue and your reply helped me alot lol - I'm both happy and pissed at how this can work....\r\nFor those of you having the same case where many of the dll files were missing, just download them and add them to your env>Library>bin folder.\r\nThen, download **cublas64_10.dll** online while at the same time renaming cublasLt64_**11** into  cublasLt64_**10** like he/she said.\r\nYou shouldn't have any more problems.", "> > Guys, I just found the answer. Somehow in bin folder should be the files: cublasLt64_10.dll and cublas64_10.dll. There will be no error if there is no cublasLt64_10 file there but it must be there. Also if you have cublasLt64_**11** just rename it to cublasLt64_**10**\r\n> \r\n> I just had the same issue and your reply helped me alot lol - I'm both happy and pissed at how this can work....\r\n> For those of you having the same case where many of the dll files were missing, just download them and add them to your env>Library>bin folder.\r\n> Then, download **cublas64_10.dll** online while at the same time renaming cublasLt64_**11** into cublasLt64_**10** like he/she said.\r\n> You shouldn't have any more problems.\r\n\r\nI tried it is working but kernel not working when training the model.\r\n", "> Guys, I just found the answer. Somehow in bin folder should be the files: cublasLt64_10.dll and cublas64_10.dll. There will be no error if there is no cublasLt64_10 file there but it must be there. Also if you have cublasLt64_**11** just rename it to cublasLt64_**10**\r\n\r\nThis solved it for me! You are a hero!"]}, {"number": 40107, "title": "[tf.data] preserve_cardinality in map()", "body": "I was wondering the reason of the `preserve_cardinality` argument in `MapDataset` and `ParallelMapDataset`. As far as I can see, the corresponding ops are both implemented with:\r\n```c++\r\nint64 Cardinality() const override { return input_->Cardinality(); }\r\n```\r\nHowever, in python, we have\r\n```python\r\n  @functools.wraps(DatasetV2.map)\r\n  def map(self, map_func, num_parallel_calls=None, deterministic=None):\r\n    if num_parallel_calls is None:\r\n      return DatasetV1Adapter(\r\n          MapDataset(self, map_func, preserve_cardinality=False))\r\n    else:\r\n      return DatasetV1Adapter(\r\n          ParallelMapDataset(\r\n              self,\r\n              map_func,\r\n              num_parallel_calls,\r\n              deterministic,\r\n              preserve_cardinality=False))\r\n```\r\nCould anyone tell me the purpose of `preserve_cardinality`? And if there are any modification need doing, I'd love to help. Thank you!", "comments": ["@zhuzilin \r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\n", "@Saduf2019 \r\nActually, I'm thinking about making an PR to remove the `preserve_cardinality` of these ops and that is why I asked the question here. And also, this is not an API opened to the user, but rather a parameter configured inside tensorflow, I suppose stackoverflow is more for tensorflow user instead of developer, right?", "@zhuzilin\r\nIt is not clear to me why tis parameter is an issue, when preserve_cardinality is an optional\u00a0bool. Defaults to\u00a0False. [its an optional paramater]\r\n\r\n\r\n\r\n", "@Saduf2019 \r\nFor me, the question is why the parameter is needed... Especially when the actual implementation will preserve cardinality...", "@zhuzilin `preserve_cardinality` determines what to do when the map function returns an OutOfRange error: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/map_dataset_op.cc#L153-L165\r\n\r\nThe flag is used internally so that we can support both TF 1.x and TF 2.x behaviors. In TF 1.x, the map function producing OutOfRange would cause the dataset to be truncated at the element producing OutOfRange. In TF 2.x we changed the behavior so that `map` will always have the same cardinality as its input. ", "@aaudiber \r\nThank you for your anwser! Could you give me an example of how we can produce OutOfRange error in tf1, the only api I found is `tf.errors.OutOfRangeError` and it doesn't seem like the right one... Also, should we return the cardinality as \"unknown\" when `preserve_cardinality` is false?", "Here's an example:\r\n\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\n\r\nds = tf.data.Dataset.range(10)\r\n\r\ndef map_py_fn(x):\r\n  if x == 3:\r\n    raise StopIteration\r\n  return x\r\n\r\ndef map_fn(x):\r\n  return tf.py_function(map_py_fn, [x], tf.int64)\r\n\r\nds = ds.map(map_fn)\r\nfor elem in ds:\r\n  print(elem.numpy())  # Prints only 0, 1, 2 since we raise StopIteration at 3\r\n```\r\nhttps://colab.research.google.com/drive/1Uv12BpCohpWseutii3hs31z4iQPBxp3x?usp=sharing", "@zhuzilin You're right, the cardinality should be \"unknown\" - I think it's a bug. Would you be interested in submitting a fix?", "@aaudiber I'd love to. :smile:"]}, {"number": 40106, "title": "GPU Layer Update", "body": "I am making changes to address issue: #40067\r\nHere is the link: https://github.com/tensorflow/tensorflow/issues/40067", "comments": ["Wrong file. We'll discuss on the bug."]}, {"number": 40105, "title": "Update gru_test.py", "body": "I am updating issue: #40067 (Change GRU Layer stateful parameter to False)\r\nHere is the link: https://github.com/tensorflow/tensorflow/issues/40067", "comments": []}, {"number": 40104, "title": "`tf.compat.v1.setdiff1d` documentation refers `out_idx` as an argument", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/setdiff1d\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIn the \"Args\" section, there is an input `out_idx`, but it is not in the signature, and the function doesn't accept the argument.\r\n\r\nRunning code: \r\n\r\n~~~python\r\ntf.compat.v1.setdiff1d([1],[1],out_idx=tf.dtypes.int32, name=None)\r\n~~~\r\n\r\ngot exception:\r\n\r\n~~~python\r\nTypeError: setdiff1d() got an unexpected keyword argument 'out_idx'\r\n~~~\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nNo\r\n\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2", "comments": ["That API is clearly marked v1 and deprecated, we're not going to invest more time in it."]}, {"number": 40103, "title": "Unclear type/dimension dependency of `filters` in  `conv1d/3d_transpose` documentation", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/conv1d_transpose\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nUnclear type and dimension dependency of input `filters`. According to the document, `filters` should have the same type as `value` and the `in_channel` dimension must match that of `value`, but it is unclear what `value` is.\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/conv1d_transpose: Yes\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose:  No, the \"Raises\" list is not provided or defined\r\n\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2\r\n  ", "comments": ["`input` is an alias of value here. I will try and update the docs. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 40102, "title": "Exception: could not rewrite use of immutable bound input", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.3.0-dev20200601\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ntf_converter = tf.lite.TFLiteConverter.from_saved_model(\"./saved_model_v2.3\")\r\n# tf_converter.allow_custom_ops = True\r\n# tf_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n# tf_converter.experimental_new_converter = True\r\ntflite_model = tf_converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n- On 2.3.0-dev20200601\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n~/VirtualEnv/ENV37-TFNT/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    179                                                  debug_info_str,\r\n--> 180                                                  enable_mlir_converter)\r\n    181       return model_str\r\n\r\n~/VirtualEnv/ENV37-TFNT/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     37       debug_info_str,\r\n---> 38       enable_mlir_converter)\r\n     39 \r\n\r\nException: <unknown>:0: error: loc(callsite(callsite(\"functional_1/crf__layer_v2/cond@__inference__wrapped_model_19200\" at \"StatefulPartitionedCall@__inference_signature_wrapper_34502\") at \"StatefulPartitionedCall\")): could not rewrite use of immutable bound input\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(\"functional_1/crf__layer_v2/cond@__inference__wrapped_model_19200\" at \"StatefulPartitionedCall@__inference_signature_wrapper_34502\") at \"StatefulPartitionedCall\")): see current operation: %96:2 = \"tf.If\"(%94, %91, %95, %arg2) {_lower_using_switch_merge = true, _read_only_resource_inputs = [3], device = \"\", else_branch = @functional_1_crf__layer_v2_cond_false_189510, is_stateless = false, output_shapes = [#tf.shape<?x?>, #tf.shape<?>], then_branch = @functional_1_crf__layer_v2_cond_true_189500} : (tensor<i1>, tensor<?x?x15xf32>, tensor<?xi32>, tensor<!tf.resource<tensor<15x15xf32>>>) -> (tensor<?x?xi32>, tensor<?xf32>)\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-6-23c7043d5029> in <module>\r\n----> 1 tflite_model = tf_converter.convert()\r\n\r\n~/VirtualEnv/ENV37-TFNT/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    575     return super(TFLiteSavedModelConverterV2,\r\n    576                  self).convert(meta_graph.graph_def, input_tensors,\r\n--> 577                                output_tensors)\r\n    578 \r\n    579 \r\n\r\n~/VirtualEnv/ENV37-TFNT/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    496         input_tensors=input_tensors,\r\n    497         output_tensors=output_tensors,\r\n--> 498         **converter_kwargs)\r\n    499 \r\n    500     if quant_mode.post_training_int8_no_float():\r\n\r\n~/VirtualEnv/ENV37-TFNT/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    553       input_data.SerializeToString(),\r\n    554       debug_info_str=debug_info_str,\r\n--> 555       enable_mlir_converter=enable_mlir_converter)\r\n    556   return data\r\n    557 \r\n\r\n~/VirtualEnv/ENV37-TFNT/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    181       return model_str\r\n    182     except Exception as e:\r\n--> 183       raise ConverterError(str(e))\r\n    184 \r\n    185   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(callsite(callsite(\"functional_1/crf__layer_v2/cond@__inference__wrapped_model_19200\" at \"StatefulPartitionedCall@__inference_signature_wrapper_34502\") at \"StatefulPartitionedCall\")): could not rewrite use of immutable bound input\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(\"functional_1/crf__layer_v2/cond@__inference__wrapped_model_19200\" at \"StatefulPartitionedCall@__inference_signature_wrapper_34502\") at \"StatefulPartitionedCall\")): see current operation: %96:2 = \"tf.If\"(%94, %91, %95, %arg2) {_lower_using_switch_merge = true, _read_only_resource_inputs = [3], device = \"\", else_branch = @functional_1_crf__layer_v2_cond_false_189510, is_stateless = false, output_shapes = [#tf.shape<?x?>, #tf.shape<?>], then_branch = @functional_1_crf__layer_v2_cond_true_189500} : (tensor<i1>, tensor<?x?x15xf32>, tensor<?xi32>, tensor<!tf.resource<tensor<15x15xf32>>>) -> (tensor<?x?xi32>, tensor<?xf32>)\r\n```\r\n\r\n- On v2.2.0\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n    496         results = c_api.TF_GraphImportGraphDefWithResults(\r\n--> 497             graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n    498         results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n\r\nInvalidArgumentError: Input 3 of node StatefulPartitionedCall/model/crf__layer_v2/cond was passed resource from Func/StatefulPartitionedCall/input/_18:0 incompatible with expected float.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-23c7043d5029> in <module>\r\n----> 1 tflite_model = tf_converter.convert()\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    457     frozen_func, graph_def = (\r\n    458         _convert_to_constants.convert_variables_to_constants_v2_as_graph(\r\n--> 459             self._funcs[0], lower_control_flow=False))\r\n    460     input_tensors = [\r\n    461         tensor for tensor in frozen_func.inputs\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)\r\n    705   graph_def, converted_inputs = _convert_variables_to_constants_v2_impl(\r\n    706       func, lower_control_flow, aggressive_inlining)\r\n--> 707   frozen_func = _construct_concrete_function(func, graph_def, converted_inputs)\r\n    708   return frozen_func, graph_def\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py in _construct_concrete_function(func, output_graph_def, converted_input_indices)\r\n    404   new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n    405                                                    new_input_names,\r\n--> 406                                                    new_output_names)\r\n    407 \r\n    408   # Manually propagate shape for input tensors where the shape is not correctly\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in function_from_graph_def(graph_def, inputs, outputs)\r\n    631     importer.import_graph_def(graph_def, name=\"\")\r\n    632 \r\n--> 633   wrapped_import = wrap_function(_imports_graph_def, [])\r\n    634   import_graph = wrapped_import.graph\r\n    635   return wrapped_import.prune(\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)\r\n    609           signature=signature,\r\n    610           add_control_dependencies=False,\r\n--> 611           collections={}),\r\n    612       variable_holder=holder,\r\n    613       signature=signature)\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    979         _, original_func = tf_decorator.unwrap(python_func)\r\n    980 \r\n--> 981       func_outputs = python_func(*func_args, **func_kwargs)\r\n    982 \r\n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in __call__(self, *args, **kwargs)\r\n     84 \r\n     85   def __call__(self, *args, **kwargs):\r\n---> 86     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n     87 \r\n     88   def call_with_variable_creator_scope(self, fn):\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in wrapped(*args, **kwargs)\r\n     90     def wrapped(*args, **kwargs):\r\n     91       with variable_scope.variable_creator_scope(self.variable_creator_scope):\r\n---> 92         return fn(*args, **kwargs)\r\n     93 \r\n     94     return wrapped\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in _imports_graph_def()\r\n    629 \r\n    630   def _imports_graph_def():\r\n--> 631     importer.import_graph_def(graph_def, name=\"\")\r\n    632 \r\n    633   wrapped_import = wrap_function(_imports_graph_def, [])\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/framework/importer.py in import_graph_def(***failed resolving arguments***)\r\n    403       return_elements=return_elements,\r\n    404       name=name,\r\n--> 405       producer_op_list=producer_op_list)\r\n    406 \r\n    407 \r\n\r\n~/VirtualEnv/ENV37-TF22/lib/python3.7/site-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n    499       except errors.InvalidArgumentError as e:\r\n    500         # Convert to ValueError for backwards compatibility.\r\n--> 501         raise ValueError(str(e))\r\n    502 \r\n    503     # Create _DefinedFunctions for any imported functions.\r\n\r\nValueError: Input 3 of node StatefulPartitionedCall/model/crf__layer_v2/cond was passed resource from Func/StatefulPartitionedCall/input/_18:0 incompatible with expected float.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n\r\n[saved model generated on v2.2.0](http://www.invencodes.com/saved_model_v2.2.tar.gz)\r\n[saved model generated on v2.3.0-dev20200601](http://www.invencodes.com/saved_model_v2.3.tar.gz)\r\n\r\n\r\n**Failure details**\r\n- Conversion failed both on 2.2.0 and 2.3.0-dev20200601.\r\n- The model includes a custom layer (crf_layer_v2) that uses tfa.text.crf_decode and tfa.text.crf_log_likelihood.\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@invencode \r\nPlease provide complete code to replicate the issue faced or if possible share a colab gist with the error faced.", "It was an issue from tfa.text.crf_decode.", "Hi @invencode, I'm experiencing the same issue. When you say it's an issue from the tfa.text.crf_decode, what do you mean? When I use the converter, I don't think I am calling this function at all."]}, {"number": 40101, "title": "Change accumulator count dtype to int64 in tf.keras.layers.experimental.preprocessing.Normalization", "body": "This fixes #40016. The dtype for the [outward-facing count](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/preprocessing/normalization.py#L116) is already int64.", "comments": []}, {"number": 40099, "title": "asking for dll that don't exist..", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): nope, I'm working in R\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): gpu 2.0.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 (but also 10.1 has the same issues)\r\n- GPU model and memory: GTX 1070\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe follow error pops up\r\n\r\n # Run if keras is installed on your machine\r\n> library(keras)\r\n\r\nAttaching package: \ufffdkeras\ufffd\r\n\r\nThe following object is masked from \ufffdpackage:tuneR\ufffd:\r\n\r\n    normalize\r\n\r\n> # Build the training set\r\n> Y_train <- to_categorical(as.integer(Train[,1]) - 1) # One hot encoding\r\n2020-06-02 20:44:42.684525: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> # X as matrix\r\n> X_train <- as.matrix(Train[,-1])\r\n> # Build the test set\r\n> Y_test <- to_categorical(as.integer(Test[,1]) - 1)\r\n> Y_test <- Y_test[,-1]\r\n> X_test <- as.matrix(Test[,-1])\r\n> # Build the sequential model\r\n> mod0 <- keras_model_sequential()\r\n> mod0 %>%\r\n+   # Input shape layer = c(samples, rows, cols, channels)\r\n+   layer_reshape(input_shape=ncol(X_train),target_shape=c(1,1,ncol(X_train))) %>% \r\n+   # First conv 2d layer with 128 neurons, kernel size of 8 x 8 and stride of 1 x 1\r\n+   layer_conv_2d(128, c(8,8), c(1,1), padding='same') %>%\r\n+   layer_batch_normalization() %>%\r\n+   layer_activation(\"relu\") %>%\r\n+   layer_dropout(0.2) %>%\r\n+   # Second conv 2d layer with 256 neurons, kernel size of 5 x 5 and stride of 1 x 1\r\n+   layer_conv_2d(256, c(5,5), c(1,1), padding='same') %>%\r\n+   layer_batch_normalization() %>%\r\n+   layer_activation(\"relu\") %>%\r\n+   layer_dropout(0.2) %>%\r\n+   # Third conv 2d layer with 128 neurons, kernel size of 3 x 3 and stride of 1 x 1\r\n+   layer_conv_2d(128, c(3,3), c(1,1), padding='same') %>%\r\n+   layer_batch_normalization() %>%\r\n+   layer_activation(\"relu\") %>%\r\n+   layer_dropout(0.2) %>%\r\n+   # Average pooling layer\r\n+   layer_global_average_pooling_2d() %>%\r\n+   # Activation output layer with 2 classes\r\n+   layer_dense(units = ncol(Y_train),  activation='softmax')\r\n2020-06-02 20:45:03.955606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-02 20:45:03.976379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 with Max-Q Design computeCapability: 6.1\r\ncoreClock: 1.379GHz coreCount: 16 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-06-02 20:45:03.976741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-02 20:45:03.978237: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-06-02 20:45:03.979542: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2020-06-02 20:45:03.980756: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2020-06-02 20:45:03.982018: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-06-02 20:45:03.983263: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2020-06-02 20:45:03.998222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-02 20:45:03.998400: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-06-02 20:45:03.999344: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-06-02 20:45:04.009529: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2126e31daa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-02 20:45:04.009767: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-02 20:45:04.010010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-02 20:45:04.010176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      \r\n> # Model compile\r\n> mod0 %>% compile(loss = 'categorical_crossentropy',\r\n+                  optimizer = \"adam\",\r\n+                  metrics = \"categorical_accuracy\")\r\n> # Add a callback to reduce the learning rate when reaching the plateau\r\n> reduce_lr <- callback_reduce_lr_on_plateau(monitor = 'loss', factor = 0.5,\r\n+                                            patience = 50, min_lr = 0.0001)\r\n> # Start learning\r\n> mod0 %>% fit(X_train, Y_train, batch_size = 32, epochs = 50,\r\n+              validation_data = list(X_test, Y_test),\r\n+              verbose = 1, callbacks = reduce_lr)\r\nEpoch 1/50\r\n7/7 [==============================] - 0s 50ms/step - loss: 0.4958 - categorical_accuracy: 0.7870\r\nError in py_call_impl(callable, dots$args, dots$keywords) : \r\n  ValueError: in user code:\r\n\r\n    C:\\Users\\axeld\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:941 test_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    C:\\Users\\axeld\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    C:\\Users\\axeld\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    C:\\Users\\axeld\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    C:\\Users\\axeld\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:912 test_step  **\r\n> \r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@spatialaudiolabs,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Hi @amahendrakar \r\n\r\nHere it is\r\n\r\n---\r\ntitle: \"Detection, Extraction and Classification of Bird and Bat Vocalizations in R\"\r\nauthor: \"Francois Fabianek, Jean Marchal\"\r\ndate: \"2018 October 30th\"\r\noutput:\r\n  rmarkdown::html_vignette:\r\n    fig_caption: yes\r\n    number_sections: yes\r\n    toc: yes\r\n    toc_depth: 4\r\n  rmarkdown::pdf_document:\r\n    fig_caption: yes\r\n    number_sections: yes\r\n    toc: yes\r\n    toc_depth: 4\r\nvignette: >\r\n  %\\VignetteIndexEntry{Tutorial: Detection, Extraction and Classification of Bird and Bat Vocalizations in R} \r\n  %\\VignetteEncoding{UTF-8}\r\n  %\\VignetteEngine{knitr::knitr} \r\n---\r\n\r\n_______\r\n_______\r\n\r\n# Load the necessary packages\r\n\r\nMake sure you have the latest version of the packages:\r\n\r\n```{r install_packages, message=FALSE, eval=FALSE}\r\ninstall.packages(\"bioacoustics\")\r\n# The bioacoustics package may also be installed from GitHub using devtools as follows:\r\ninstall.packages(\"devtools\")\r\ndevtools::install_github(\"wavx/bioacoustics\") # For the latest, unstable version\r\ninstall.packages(\"warbleR\")\r\ninstall.packages(\"randomForest\")\r\n```\r\n\r\n```{r load_packages, message=FALSE, eval=FALSE}\r\n# Load the packages\r\nlibrary(warbleR)\r\nlibrary(bioacoustics)\r\nlibrary(tools)\r\nlibrary(magrittr)\r\nlibrary(dplyr)\r\nlibrary(readr)\r\nlibrary(randomForest)\r\n```\r\n\r\n\r\n___________________\r\n\r\n# Load audio files\r\n\r\nWe can use the `quer_xc()` function from **warbleR** to download bird vocalizations from Xeno-Canto: <https://www.xeno-canto.org/>\r\n\r\nWe are going to choose calls from *Catharus bicknelli*, songs from *Passerella iliaca* and *Setophaga magnolia* recorded in the United States and Canada.\r\n\r\nWe will filter only \"A\" quality recordings, then, pick up only the first nine, and merge all the metadata into a single \"df\" data frame. This data frame will be used to download MP3 files in your working directory directly from Xeno-Canto with the `quer_xc()` function:\r\n\r\n```{r xeno1, message=FALSE, eval=FALSE}\r\ndf1 = quer_xc(qword ='Catharus bicknelli type:call cnt:\"United States\"', download = FALSE)\r\ndf1 = df1[df1$Vocalization_type==\"call\",]\r\ndf1 = df1[df1$Quality==\"A\",]\r\ndf1 = df1[1:9,]\r\n```\r\n  \r\n```{r xeno2, message=FALSE, results='hold', eval=FALSE}\r\ndf2 = quer_xc(qword ='Setophaga magnolia type:song cnt:\"Canada\"', download = FALSE)\r\ndf2 = df2[df2$Quality==\"A\",]\r\ndf2 = df2[1:9,]\r\n```\r\n  \r\n```{r xeno3, message=FALSE, results='hold', eval=FALSE}\r\ndf3 = quer_xc(qword ='Passerella iliaca type:song cnt:\"Canada\"', download = FALSE)\r\ndf3 = df3[df3$Vocalization_type==\"song\",]\r\ndf3 = df3[df3$Quality %in% c(\"A\", \"B\"),]\r\ndf3 = df3[1:9,]\r\ndf3 <- df3 %>%\r\n  select(-\"Other_species8\")\r\ndf = rbind(df1,df2,df3)\r\nrm(df1,df2,df3)\r\n```\r\n  \r\n```{r xeno4, eval=FALSE}\r\n# Visualize your data frame\r\nView(df)\r\n# We will work in the R temp directory\r\nwd <- tempdir()\r\n# Create a data directory if it does not exist\r\ndata_dir <- file.path(wd, \"data\")\r\nif(!dir.exists(data_dir))\r\n  dir.create(data_dir)\r\n# Download the MP3 files into your data directory\r\nquer_xc(X = df, download = TRUE, path = data_dir)\r\n```\r\n\r\nNow that we have recordings stored in the data directory, we can read one of them to look at its structure and content. Let's use the `read_audio()` function in **bioacoustics** to manually read a recording of *Catharus bicknelli*:\r\n\r\n```{r read_audio, eval=FALSE}\r\nCATBIC <- read_audio(file.path(data_dir, \"Catharus-bicknelli-54866.mp3\"))\r\nCATBIC\r\n```\r\n\r\nWe can see that the MP3 file has been converted into a Wave object with 7551360 samples, a duration of 157.32 seconds, a sampling rate of 48000 Hz, a bit depth of 16 bits, and that it contains one channel (mono, stereo being two channels).\r\n\r\nRemember that you just have to divide the number of samples by the sampling rate to retrieve the duration (s) of an audio file.\r\n\r\n___________________\r\n\r\n# Extract GUANO metadata\r\n\r\n[GUANO](https://guano-md.org/) stands for the \"Grand Unified Acoustic Notation Ontology\" and means to be a universal, extensible, open metadata format for bat (ultrasonic) and non-bat (audible, infrasonic) acoustic recordings [more here](https://github.com/riggsd/guano-r). GUANO format is now embeded directly in the WAV files generated from most Pettersson, Wildlife Acoustics, Titley Scientific acoustic recorders. It is possible to extract the metadata and GUANO embedded in the WAV file by using the `metadata()` function.\r\n\r\n```{r metadata,eval=FALSE}\r\nmetadata(CATBIC)\r\n```\r\n<br>\r\n\r\n\r\nNow that we have explored a WAV file, we will use the Fast Fourrier Transform (FFT) to compute a frequency-time representation of the recording, called a spectrogram. A spectrogram being the representation of the spectrum of frequencies in a recording as they vary through time. This representation, although not optimal, is still commonly used to detect animal vocalizations and extract acoustic features useful for classification with the purpose of animal identification.\r\n___________________\r\n\r\n# Plot audio files\r\n\r\nThere are several options to display animal vocalizations in audio files with R. You can use both `spectro()` or `fspec()` functions to generate spectrograms with **bioacoustics**. `fspec()` generates only a matrix of the spectrogram, and thus has to be used with the `image()` function to display the spectrogram. It is also possible to use the `spectro()` function in **Seewave**.  \r\n\r\nNext, we will search manually and display an audio event (here, a bird vocalization) from a recording of *Catharus bicknelli*. To display a Region Of Interest (ROI) of the recording we will use temporal and frequency filters. Let's start with a temporal slice from 1 to 10 secs and a FFT size of 512 samples.\r\n\r\n```{r spectro0, eval=FALSE}\r\n# Set plot margins to 0\r\npar(mar = c(0, 0, 0, 0), oma = c(0, 0, 0, 0))\r\n# Display with spectro()\r\nticks <- c(from = 1000, to = 20000, by = 1000) # frequency tick marks from 1 to \r\n                                               # 20 kHz, and steps at each 1 kHz\r\ntemp_slice <- c(from = 1, to = 10) # in seconds\r\nspectro(CATBIC, tlim = temp_slice, FFT_size = 512, ticks_y = ticks)\r\n```  \r\n\r\nLet's display spectrograms with various time / frequency limits (with `tlim=` and `flim=` arguments). You can also play with other arguments in `spectro()` and `fspec()` functions such as the percent of overlap between two FFTs (with `FFT_overlap=`) and various FFT resolutions (with `FFT_size=`). Note that the arguments are briefly explained in the documentation of each function:  \r\n  \r\n```{r help, eval=FALSE}\r\n# Access the arguments of the spectro function\r\n?spectro\r\n?fspec\r\n```\r\n\r\nFirst, let's shorten the temporal axis from 2 to 3.5 secs to work on a shorter time window and compare the outputs from `spectro()` and `fspec()`  functions. Note that spectrograms can also be generated automatically while using the detection functions from **bioacoustics**. We will explore that in details in section 4.1.\r\n\r\n```{r spectro1, eval=FALSE}\r\n# Set plot margins to 0\r\npar(mar = c(0, 0, 0, 0), oma = c(0, 0, 0, 0))\r\n# Display the spectrogram with spectro()\r\nticks <- c(from = 1000, to = 20000, by = 1000) # frequency tick marks from 1 to \r\n                                               # 20 kHz, 1 kHz steps\r\ntemp_slice <- c(from = 2, to = 3.5) # in seconds\r\nspectro(CATBIC, tlim = temp_slice, FFT_size = 512, ticks_y = ticks)\r\n# fspec() gives you the spectrogram matrix with energy values (dB)\r\nspec_mx <- fspec(CATBIC, tlim = temp_slice, FFT_size = 512, rotate = TRUE)\r\n# You can display the spectrogram with image()\r\nimage(spec_mx, xaxt = \"n\", yaxt = \"n\") \r\n```\r\n\r\nThe tick marks on the (frequency) y-axis were defined in the `spectro()` function starting from 1 to 20 kHz with an interval at each 1 kHz. The FFT size was 512 samples with an overlap between two FFT windows set by default at 0.875. Now try these settings: FTT size = 256, 1024 and 2048; FFT overlap = 0.3, 0.6, 0.9...\r\n\r\nAnother interesting thing to perform with the fspec outputs, is to implement your own set of filters. Let's try to reduce the background noise from a spectrogram with a narrower time and frequency bandwidth:\r\n\r\n```{r filter, eval=FALSE}\r\ntemp_slice <- c(from = 2.5, to = 3.5)\r\nfreq_slice <- c(from = 1500, to = 20000)\r\nspec_o <- fspec(CATBIC, tlim = temp_slice, flim = freq_slice, FFT_size = 512, rotate = TRUE)\r\n## min and max (range) dB intensity\r\nrange(spec_o) # -120 (min) to 0 dB (max)\r\n# Note that the tolerance of your recorders depends on the number of bits. \r\n# 16-bit recorders offer only around -96 dB tolerance and sound pressure above\r\n# this level is clipped to 0 dB.\r\n## Let's try a filter by mean + sd intensity\r\nspec_f <- fspec(CATBIC, tlim = temp_slice, flim = freq_slice, FFT_size = 512, rotate = TRUE)\r\nspec_f[spec_f < mean(spec_f) + sd(spec_f)] <- -120\r\n# Works well with high intensity audio events, but leads to\r\n# false negatives (missed events) otherwise.\r\npar(mar = c(0, 0, 0, 0), oma = c(0, 0, 0, 0))\r\nimage(spec_o, xaxt=\"n\", yaxt=\"n\")\r\nimage(spec_f, xaxt=\"n\", yaxt=\"n\")\r\n```\r\n\r\n___________________\r\n\r\n# The use of filters to detect and extract audio events\r\n\r\nThe functions used to detect and extract audio events in a recording also rely on \"generic\" filters based on frequency and duration, along with other \"specific\" filters. Let's take a quick look at the generic filters available in the `threshold_detection()` and `blob_detection()` functions:\r\n\r\n  * High Pass (HPF) and Low Pass filters (LPF) can be employed to reduce the amount of unwanted noise in the recording or to track particular audio events within a narrower frequency bandwidth than the recording sampling rate. Frequencies below the HPF and above the LPF cutoff are greatly attenuated. These frequency filters can be set using the `HPF=` and `LPF=` arguments in the `threshold_detection()` and `blob_detection()` functions.\r\n  * Minimum and maximum duration of an audio event, and a minimum time between two audio events also help reduce the amount of unwanted noise, or track a particular audio event within a narrower temporal window. These temporal filters can be set using the `min_dur=`, `max_dur=`, and `TBE=` arguments in both `threshold_detection()` and `blob_detection()` functions.\r\n  * Other set of filters are specific to each detection function and will be defined while working with these functions on bird vocalizations.\r\n\r\n___________________\r\n\r\n## Detect and extract audio events in a recording\r\n\r\n### Threshold detection\r\n\r\nLet's start with the `threshold_detection()` function on a recording containing calls from *Catharus bicknelli*. This function is an amplitude threshold detector that picks up audio events above the Signal to Noise Ratio (SNR). It combines several algorithms for detection, filtering and audio feature extraction. We will play with the arguments of this function to understand their implication in the detection and extraction of audio events (here, calls of *Catharus bicknelli*).  \r\n\r\n```{r threshold_help, eval=FALSE}\r\n# Access the arguments of the threshold_detection function\r\n?threshold_detection\r\n```\r\n\r\n```{r threshold1, eval=FALSE}\r\n# Set each argument according to the targeted audio events\r\nTD <- threshold_detection(\r\n  CATBIC, # Either a path to an audio file (see ?read_audio), or a Wave object\r\n  threshold = 12, # 12 dB SNR sensitivity for the detection algorithm\r\n  time_exp = 1, # Time expansion factor of 1. Only needed for bat recordings.\r\n  min_dur = 140, # Minimum duration threshold of 140 milliseconds (ms)\r\n  max_dur = 440, # Maximum duration threshold of 440 ms\r\n  min_TBE = 10, # Minimum time window between two audio events of 10 milliseconds\r\n  max_TBE = 5000, # Maximum time window between two audio events, here 5 seconds\r\n  EDG = 0.996, # Temporal masking with Exponential Decay Gain from 0 to 1\r\n  LPF = 10000, # Low-Pass Filter of 10 kHz\r\n  HPF = 1000, # High-Pass Filter of 1 kHz\r\n  FFT_size = 256, # Size of the Fast Fourrier Transform (FFT) window\r\n  FFT_overlap = 0.875, # Percentage of overlap between two FFT windows\r\n  \r\n  start_thr = 25, # 25 dB threshold at the start of the audio event\r\n  end_thr = 30, # 30 dB threshold at the end of the audio event\r\n  SNR_thr = 10, # 10 dB SNR threshold at which the extraction of the audio event stops\r\n  angle_thr = 45, # 45\u00b0 of angle at which the extraction of the audio event stops\r\n  duration_thr = 440, # Noise estimation is resumed after 440 ms\r\n  NWS = 1000, # Time window length of 1 s used for background noise estimation\r\n  KPE = 1e-05, # Process Error parameter of the Kalman filter (for smoothing)\r\n  KME = 1e-04, # Measurement Error parameter of the Kalman filter (for smoothing)\r\n  settings = FALSE, #  Save on a list the above parameters set with this function\r\n  acoustic_feat = TRUE, # Extracts the acoustic and signal quality parameters \r\n  metadata = FALSE, # Extracts on a list the metadata embedded with the Wave file\r\n  spectro_dir = file.path(tempdir(), \"Spectros\"), # Directory where to save the spectrograms\r\n  time_scale = 1, # Time resolution of 2 ms for spectrogram display\r\n  ticks = TRUE # Tick marks and their intervals are drawn on the y-axis (frequencies) \r\n) \r\n# Get the number of extracted audio events\r\nnrow(TD$data$event_data)\r\n```\r\n\r\nLet the HTML page open with the 57 spectrograms (each representing an extracted audio event). These settings will be our benchmark for the number of audio events that can be extracted with the `threshold_detection()` function. In the following exercise, you will try to reach or beat this number by exploring different combinations of parameters for each argument of the function.\r\n\r\n```{r threshold2, eval=FALSE}\r\n# Let's try various settings, starting with 1024 FFT size instead of 256.\r\nTD <- threshold_detection(\r\n  CATBIC, threshold = 12, time_exp = 1, min_dur = 140, max_dur = 440, \r\n  min_TBE = 10, max_TBE = 5000, EDG = 0.996, LPF = 10000, HPF = 1000, \r\n  FFT_size = 1024, FFT_overlap = 0.875, start_thr = 25, end_thr = 30, \r\n  SNR_thr = 10, angle_thr = 45, duration_thr = 440, NWS = 1000, \r\n  KPE = 1e-05, KME = 1e-04, settings = FALSE, acoustic_feat = TRUE,\r\n  metadata = FALSE, spectro_dir = file.path(tempdir(), \"Spectros\"), time_scale = 1, \r\n  ticks = c(1000, 10000, 1000) # Tick marks from 1 to 10 kHz with 1 kHz interval\r\n) \r\n# Take a look at the spectrograms and compare them with the previous extraction.\r\nnrow(TD$data$event_data) # Only three audio events!\r\n```\r\n\r\nWe will play with various detection thresholds: end_thr, SNR_thr, angle_thr, KPE and KME parameters. Try to reach 66 spectrograms extracted with a contour (*i.e.*, Kalman curve) that best matches the audio event (answer below). The FFT size will be set at 256 samples.\r\n\r\n```{r threshold3, eval=FALSE}\r\nCATBIC <- read_audio(file.path(data_dir, \"Catharus-bicknelli-5486.mp3\"))\r\nTD <- threshold_detection(\r\n  CATBIC, threshold = 12, time_exp = 1, min_dur = 140, max_dur = 440, min_TBE = 10, \r\n  max_TBE = Inf, EDG = 0.996, LPF = 10000, HPF = 1000, FFT_size = 256, FFT_overlap = 0.875, \r\n  start_thr = 22, end_thr = 30, SNR_thr = 10, angle_thr = 125, duration_thr = 440, NWS = 1000,\r\n  KPE = 1e-05, KME = 1e-05, settings = FALSE, acoustic_feat = TRUE, metadata = FALSE\r\n)\r\n```\r\n\r\nLet's take a look at the extracted audio features. Note that all the features are described and explained in the package vignette (`vignette(\"bioacoustics\")`).\r\n\r\n```{r features1, eval=FALSE}\r\n# Acoustic features are stored in a data frame called event_data,\r\n# stored by order of detection.\r\nView(TD$data$event_data) # Contains the filename and the time of detection in the \r\n                         # recording, and 26 extracted features.\r\n```\r\n\r\nThe location (in number of samples) of the audio event in the recording is saved in a list. \r\n\r\n```{r features2, eval=FALSE}\r\n# Start and end of the 5th extracted audio event (in samples)\r\nc(TD$data$event_start[[5]], TD$data$event_end[[5]])\r\n# Remember you just have to divide by the sample rate to retrieve the time (s)\r\nc(TD$data$event_start[[5]], TD$data$event_end[[5]]) / slot(CATBIC, \"samp.rate\")\r\n```\r\n\r\nThe amplitude (dB) and frequency (Hz) tracks (or bins) are also saved in a list. These can be used to build your own acoustic features.\r\n\r\n```{r features3, eval=FALSE}\r\npar(mar = c(1,1, 1, 1), oma = c(1, 1, 1, 1))\r\n# Amplitude track of the 5th audio event\r\nplot(TD$data$amp_track[[5]], type = \"l\")\r\n# Frequency track of the 5th audio event\r\nplot(TD$data$freq_track[[5]], type = \"l\")\r\n```\r\n\r\nThe whole energy and frequency content can also be used to classify audio events instead of using acoustic features that may result in a loss of information. We will get there soon, but first, let's discover another detection function, here applied on echolocation calls of bats.\r\n\r\n\r\n### Blob detection\r\n\r\nThe `blob_detection()` function will be used on a recording containing 10 bat echolocation calls from the *Myotis* genus. This function combines several image processing, filtering and image feature extraction. A blur and contrast boost is applied after mean background subtraction to increase the SNR of the audio event. The blob detection algorithm is applied on the processed spectrogram to detect the ROI (i.e., each preprocessed audio event). The blob detector simultaneously labels the connected FFT values and their contours in the spectrogram. Labelling is done in a single pass over the spectrogram, while contour points are revisited more than once and up to four times (see [Chang et al., 2004](https://www.iis.sinica.edu.tw/papers/fchang/1362-F.pdf)). We will play with the arguments of this function to extract bat echolocation calls.\r\n\r\n```{r blob0, eval=FALSE}\r\n# Access the arguments of the blob_detection function\r\n?blob_detection\r\n```\r\n\r\n```{r blob1, eval=FALSE}\r\n# Use the bat recording stored in the package\r\ndata(myotis)\r\n# Set each argument according to the targeted audio events\r\nBD <- blob_detection(\r\n  myotis, # Either a path to an audio file (see ?read_audio), or a Wave object\r\n  time_exp = 10, # Time expansion factor of 10 for time expanded recordings.\r\n  min_dur = 1.5, # Minimum duration threshold of 1.5 milliseconds (ms)\r\n  max_dur = 80, # Maximum duration threshold of 80 ms\r\n  min_area = 40, # minimum number of 40 pixels in the blob\r\n  min_TBE = 20, # Minimum time window between two audio events of 20 milliseconds\r\n  EDG = 0.996, # Temporal masking with Exponential Decay Gain from 0 to 1\r\n  LPF = slot(myotis, \"samp.rate\") * 10 / 2, # Low-Pass Filter at the Nyquist frequency\r\n  HPF = 16000, # High-Pass Filter of 16 kHz\r\n  FFT_size = 256, # Size of the Fast Fourrier Transform (FFT) window\r\n  FFT_overlap = 0.875, # Percentage of overlap between two FFT windows\r\n  blur = 2, # Gaussian smoothing function with a factor of 2 for blurring the spectrogram\r\n  bg_substract = 20, # Foreground extraction with a mean filter applied on the spectrogram\r\n  contrast_boost = 20, # Edge contrast enhancement filter of the spectrogram contour\r\n  settings = FALSE, #  Save on a list the above parameters set with this function\r\n  acoustic_feat = TRUE, # Extracts the acoustic and signal quality parameters \r\n  metadata = FALSE, # Extracts on a list the metadata embedded with the Wave file\r\n  spectro_dir = file.path(tempdir(), \"Spectros\"), # HTML page with spectrograms by order of detection \r\n  time_scale = 0.1, # Time resolution of 2 ms for spectrogram display\r\n  ticks = TRUE # Tick marks and their intervals are drawn on y-axis (frequencies)\r\n) \r\n# Get the number of extracted audio events\r\nnrow(BD$data$event_data)\r\n```\r\n\r\nDo not close the HTML page and tune the FFT size at 512. Let's play with the blur, contrast boost and background subtraction parameters to retrieve a number of 10 extracted echolocation calls.\r\n\r\n```{r blob2, eval=FALSE}\r\n# Let's try various settings, starting with 512 FFT size instead of 256.\r\nBD <- blob_detection(\r\n  myotis, time_exp = 10, FFT_size = 512, settings = FALSE, acoustic_feat = TRUE,\r\n  metadata = FALSE, spectro_dir = file.path(tempdir(), \"Spectros\"), time_scale = 0.1, ticks = TRUE\r\n) \r\n# Take a look at the spectrograms and compare them with the previous extraction.\r\nnrow(BD$data$event_data) # Only 6 audio events!\r\n```\r\n\r\nLet's take a look at the extracted audio features. All the features are described and explained in the package vignette.\r\n\r\n```{r blobfeat1, eval=FALSE}\r\n# Acoustic features\r\nhead(BD$data)\r\n```\r\n\r\nThis data frame is, for now, the only available set of acoustic features with the `blob_detection()` function. However, it combines well with the `fspec()` to make image analysis.\r\n\r\nNow that we have played with both detection functions with bird and bat vocalizations, let's go back to birds to explore batch analysis (*i.e.*, with several recordings) and audio event classification.\r\n\r\n___________________\r\n\r\n## Batch analysis and classification\r\n\r\nIn this section, we will learn how to analyze several recordings at the same time and train a simple classifier (with training set) that will be used to classify new data (*i.e.*, the test set).\r\n\r\nWe will work with 27 recordings of *Catharus-bicknelli* (*n* = 9), *Passerella iliaca* (*n* = 9), and *Setophaga-magnolia* (*n* = 9). We will split the extracted audio events in a 70 % training set (called \"Train\") and 30 % test set (called \"Test\").\r\n\r\nOur target audio events are calls of *Catharus-bicknelli*. We will use the threshold detector previously configured for this species (see section 4.1.1).\r\n\r\n```{r classification1, eval=FALSE}\r\n# Get the filepath for each MP3 file\r\nfiles <- dir(data_dir, recursive = TRUE, full.names = TRUE, pattern = \"[.]mp3$\")\r\n# Detect and extract audio events\r\nTDs <- setNames(\r\n  lapply(\r\n    files,\r\n    threshold_detection,\r\n    threshold = 12, min_dur = 140, max_dur = 440, min_TBE = 50, max_TBE = Inf,\r\n    LPF = 8000, HPF = 1500, FFT_size = 256, start_thr = 30, end_thr = 20, \r\n    SNR_thr = 10, angle_thr = 125, duration_thr = 400, spectro_dir = NULL,\r\n    NWS = 2000, KPE = 0.00001, time_scale = 2, EDG = 0.996\r\n  ),\r\n  basename(file_path_sans_ext(files))\r\n)\r\n# Keep only files with data in it\r\nTDs <- TDs[lapply(TDs, function(x) length(x$data)) > 0]\r\n# Keep the extracted feature and merge in a single data frame for further analysis\r\nEvent_data <- do.call(\"rbind\", c(lapply(TDs, function(x) x$data$event_data), list(stringsAsFactors = FALSE)))\r\nnrow(Event_data) # 355 audio events extracted\r\n# Compute the number of extracted CATBIC calls\r\nsum(startsWith(Event_data$filename, \"Cat\"))\r\n# Add a \"Class\" column: \"CATBIC\" vs. other species of birds \"OTHERS\"\r\nclasses <- as.factor(ifelse(startsWith(Event_data$filename, \"Cat\"), \"CATBIC\", \"OTHERS\"))\r\nEvent_data <- cbind(data.frame(Class = classes), Event_data)\r\n# Get rid of the filename and time in the recording\r\nEvent_data$filename <- Event_data$starting_time <- NULL\r\n```\r\n\r\nWe now have the necessary dataset to train a classifier: we will train a Random Forest on the training set and validate the results on the test set.\r\n\r\n```{r classification2, eval=FALSE}\r\n# Split the data in 60% Training / 40% Test sets\r\ntrain <- sample(1:nrow(Event_data), round(nrow(Event_data) * .6))\r\nTrain <- Event_data[train,]\r\ntest <- setdiff(1:nrow(Event_data), train)\r\nTest <- Event_data[test,]\r\n# Train a random forest classifier\r\nset.seed(666)\r\nrf <- randomForest(Class ~ duration + freq_max_amp + freq_max + freq_min +\r\n                           bandwidth + freq_start + freq_center + freq_end +\r\n                           freq_knee + fc + freq_bw_knee_fc + bin_max_amp + \r\n                           pc_freq_max_amp + pc_freq_max + pc_freq_min +\r\n                           pc_knee + temp_bw_knee_fc + slope + kalman_slope +\r\n                           curve_neg + curve_pos_start + curve_pos_end + \r\n                           mid_offset + smoothness + snr + hd + smoothness,\r\n                   data = Train, importance = FALSE, proximity = FALSE,\r\n                   replace = TRUE, ntree = 4000, mtry = 4)\r\n# Look at the confusion matrix of the training set\r\nrf$confusion # looks good, but...\r\n# Let's make predictions with our classifier on a test set\r\ntable(Test[,1], predict(rf, Test[,-1], type = \"response\")) # not bad!\r\n# To look at the predictions \r\nhead(predict(rf, Test[,-1], type = \"prob\"))\r\n```\r\n\r\nWe are now able to use this simple, but proven robust, classifier to detect new calls of your target species.\r\n\r\n\r\n___________________\r\n\r\n# Deep learning classification with the R interface to Keras\r\n\r\nWe will use Keras in R which requires to install several packages in [Python](https://www.python.org/downloads/)\r\nGuidelines to install Keras properly in R are available [here](https://keras.rstudio.com/)\r\n\r\nLet's now explore a ConvNet approach available on Keras. We will follow the approach of [Hatami et al. (2017)](https://arxiv.org/pdf/1710.00886.pdf) to analyze time series as images with 2D ConvNets. The difference is that we will only perform max pooling at the last layer before activation and add batch normalization with dropouts at each layer.\r\n\r\n```{r keras1, eval=FALSE}\r\n# Run if keras is installed on your machine\r\nlibrary(keras)\r\n# Build the training set\r\nY_train <- to_categorical(as.integer(Train[,1]) - 1) # One hot encoding\r\n# X as matrix\r\nX_train <- as.matrix(Train[,-1])\r\n# Build the test set\r\nY_test <- to_categorical(as.integer(Test[,1]) - 1)\r\nY_test <- Y_test[,-1]\r\nX_test <- as.matrix(Test[,-1])\r\n# Build the sequential model\r\nmod0 <- keras_model_sequential()\r\nmod0 %>%\r\n  # Input shape layer = c(samples, rows, cols, channels)\r\n  layer_reshape(input_shape=ncol(X_train),target_shape=c(1,1,ncol(X_train))) %>% \r\n  # First conv 2d layer with 128 neurons, kernel size of 8 x 8 and stride of 1 x 1\r\n  layer_conv_2d(128, c(8,8), c(1,1), padding='same') %>%\r\n  layer_batch_normalization() %>%\r\n  layer_activation(\"relu\") %>%\r\n  layer_dropout(0.2) %>%\r\n  # Second conv 2d layer with 256 neurons, kernel size of 5 x 5 and stride of 1 x 1\r\n  layer_conv_2d(256, c(5,5), c(1,1), padding='same') %>%\r\n  layer_batch_normalization() %>%\r\n  layer_activation(\"relu\") %>%\r\n  layer_dropout(0.2) %>%\r\n  # Third conv 2d layer with 128 neurons, kernel size of 3 x 3 and stride of 1 x 1\r\n  layer_conv_2d(128, c(3,3), c(1,1), padding='same') %>%\r\n  layer_batch_normalization() %>%\r\n  layer_activation(\"relu\") %>%\r\n  layer_dropout(0.2) %>%\r\n  # Average pooling layer\r\n  layer_global_average_pooling_2d() %>%\r\n  # Activation output layer with 2 classes\r\n  layer_dense(units = ncol(Y_train),  activation='softmax')\r\n# Model compile\r\nmod0 %>% compile(loss = 'categorical_crossentropy',\r\n                 optimizer = \"adam\",\r\n                 metrics = \"categorical_accuracy\")\r\n# Add a callback to reduce the learning rate when reaching the plateau\r\nreduce_lr <- callback_reduce_lr_on_plateau(monitor = 'loss', factor = 0.5,\r\n                                           patience = 50, min_lr = 0.0001)\r\n# Start learning\r\nmod0 %>% fit(X_train, Y_train, batch_size = 32, epochs = 50,\r\n             validation_data = list(X_test, Y_test),\r\n             verbose = 1, callbacks = reduce_lr)\r\n# Score on the test set\r\nscore <- mod0 %>% evaluate(X_test, Y_test, batch_size = 32)\r\nscore\r\n```\r\n\r\nLet's work a bit with the output to build a confusion matrix and use the predict function on the test set.\r\n\r\n```{r keras2, eval=FALSE}\r\n# Look at predictions and build a confusion matrix\r\nPred <- as.factor(predict_classes(mod0, X_test, batch_size = 32, verbose = 1))\r\ntable(Y_test[,2], Pred)\r\n# To look at the prediction values \r\nProb <- round(predict_proba(mod0, X_test, batch_size = 32, verbose = 1), 2)\r\n```\r\n\r\n\r\nWe obtained a val_loss < 0.2 and val_categorical_accuracy > 0.94 which is acceptable, but not better than the simplest RF approach we used in section 3.2.\r\nUsing only 26 acoustic features as model inputs instead of the whole spectrogram content (energy and frequency distribution, and harmonics) probably reduced the performances of the CNN model.\r\n\r\nThis tutorial is now complete. Comments and feedback are welcome:\r\n\r\nFrancois: francois.fabianek@wavx.ca  \r\nJean: jean.marchal@wavx.ca  \r\n[www.wavx.ca](https://www.wavx.ca)\r\n\r\n\r\n_______\r\n_______", "@spatialaudiolabs,\r\nPlease take a look at [this](https://stackoverflow.com/a/59823284) similar StackOverflow issue and let us know if it works. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I am still trying to sort it out, but I will be able to definitely check it tomorrow afternoon. apologies for the delay.", "Hi there\r\n\r\nI did follow the instructions, the error message changed now, the PATH is there, tensorflow is at 2.2, and CUDA 10.1. There are also 10.0 and 10.2 folders in the CUDA folder, which I removed and left only 10.1.\r\n\r\nError below.\r\n\r\n\r\nAttaching package: \ufffdkeras\ufffd\r\n\r\nThe following object is masked from \ufffdpackage:tuneR\ufffd:\r\n\r\n    normalize\r\n\r\n2020-06-19 17:10:22.423086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-19 17:10:49.200039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-19 17:10:49.235092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 with Max-Q Design computeCapability: 6.1\r\ncoreClock: 1.379GHz coreCount: 16 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-06-19 17:10:49.235560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-19 17:10:49.318336: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-19 17:10:49.372285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-06-19 17:10:49.400606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-06-19 17:10:49.474464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-06-19 17:10:49.515771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-06-19 17:10:49.627382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-19 17:10:49.628450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-19 17:10:49.631878: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-06-19 17:10:49.664140: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x163f68cde70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-19 17:10:49.664430: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-19 17:10:49.666188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 with Max-Q Design computeCapability: 6.1\r\ncoreClock: 1.379GHz coreCount: 16 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-06-19 17:10:49.666575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-06-19 17:10:49.666763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-19 17:10:49.666956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-06-19 17:10:49.667171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-06-19 17:10:49.667368: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-06-19 17:10:49.667556: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-06-19 17:10:49.667768: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-19 17:10:49.668406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-19 17:10:53.745006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-19 17:10:53.745230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-06-19 17:10:53.745348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-06-19 17:10:53.748494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6297 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-06-19 17:10:53.757665: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x163b05c8da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-19 17:10:53.758109: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070 with Max-Q Design, Compute Capability 6.1\r\n2020-06-19 17:10:56.282985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-19 17:10:57.186272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-19 17:10:59.475631: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\nEpoch 1/50\r\n7/7 [==============================] - 0s 18ms/step - loss: 0.5245 - categorical_accuracy: 0.7546\r\nError in py_call_impl(callable, dots$args, dots$keywords) : ValueError: in user code: C:\\Users\\axeld\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:941 test_function * outputs = self.distribute_strategy.run( C:\\Users\\axeld\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run ** return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs) C:\\Users\\axeld\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica return self._call_for_each_replica(fn, args, kwargs) C:\\Users\\axeld\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica return fn(*args, **kwargs) C:\\Users\\axeld\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:912 test_step **\r\n", "@spatialaudiolabs,\r\nLooks like the code you have given is written in R. Could you please share a minimal reproducible Python code?\r\n\r\nAlso, please run the below command and check if you are able to import tensorflow without any issues.\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n```\r\nThanks!", "The overall code is the same as the above one, the error code is just\ndifferent.\n\nI don\u2019t use Python yet, still learning, only R. I did install it to have it\nworking to run it on R.\n\nI will run the code below tomorrow but I did run a few lines to check the\nversion and it was detected.\n\nAxel\n\nOn Sat, 20 Jun 2020 at 18:33, Abhilash Mahendrakar <notifications@github.com>\nwrote:\n\n> @spatialaudiolabs <https://github.com/spatialaudiolabs>,\n> Looks like the code you have given is written in R. Could you please share\n> a minimal reproducible Python code?\n>\n> Also, please run the below command and check if you are able to import\n> tensorflow without any issues.\n>\n> import tensorflow as tf\n> print(tf.__version__)\n>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40099#issuecomment-647024765>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACDI4JDWGNA5KYG2SR3FG3DRXTXHLANCNFSM4NRA2QTA>\n> .\n>\n-- \n*Axel Drioli*\n*SpatialAudioLabs.com <http://spatialaudiolabs.com/>*\n\n*Creating sonic immersive experiences for XR and installations.*\n\n*SoundingWild.com <http://SoundingWild.com> for Wildlife and Conservation\nimmersive experiences.*\n\n*Tel-Facetime: +44 7460 223640*\n*E-mail: axel@spatialaudiolabs.com <axel@spatialaudiolabs.com>*\n\n\n\n\n*'Life On The Edge', a Sounding Wild <http://www.soundingwild.com>\nx Spatial Audio Labs production for Wildlife Alliance\n<https://www.wildlifealliance.org/> is part of EarthXR 2020\n<https://earthx.org/expo/main-attractions/earthxr/> official selection and\nFinalist at SXSW2020 Virtual Noise Showcase*\n", "@spatialaudiolabs Is this issue fixed?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40098, "title": "ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call", "body": "Hello,\r\n\r\nI am not able to train on multiple gpus using ` tf.distribute.MirroredStrategy`. The code works fine without this. \r\n\r\nfollowing is the code snippet of training:-\r\n```\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices=['/gpu:5','/gpu:6'])\r\nwith mirrored_strategy.scope():\r\n    train_X,frames_array = get_frames()\r\n    train_Y,heatmap = get_grids()\r\n    train_X, X_test, heatmap, y_test = train_test_split(train_X, heatmap, test_size=0.20, random_state=42)\r\n    opt = SGD(lr=0.0005, momentum=0.9, decay=1e-2)\r\n    model2 = fine_model()\r\n    model2.compile(loss= KL_loss, optimizer=opt)\r\n    model2.fit(train_X,heatmap,batch_size=2,epochs=100,validation_data=(X_test, y_test))\r\n````\r\n\r\nAlso, I have explicitly specified tf to use gpu 5 and 6, however it uses gpu 0 as well.\r\n\r\nbelow is the error,\r\n```\r\nTraceback (most recent call last):\r\n  File \"gaze_model.py\", line 260, in <module>\r\n    model2.fit(train_X,heatmap,batch_size=2,epochs=1,validation_data=(X_test, y_test))\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/keras/engine/training.py\", line 1213, in fit\r\n    self._make_train_function()\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/keras/engine/training.py\", line 316, in _make_train_function\r\n    loss=self.total_loss)\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 75, in symbolic_fn_wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/keras/optimizers.py\", line 192, in get_updates\r\n    grads = self.get_gradients(loss, params)\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/keras/optimizers.py\", line 91, in get_gradients\r\n    grads = K.gradients(loss, params)\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 75, in symbolic_fn_wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 3025, in gradients\r\n    return tf.gradients(loss, variables)\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_impl.py\", line 274, in gradients_v2\r\n    unconnected_gradients)\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 530, in _GradientsHelper\r\n    for x in xs\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 530, in <listcomp>\r\n    for x in xs\r\n  File \"/home/centos/Anaconda/envs/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py\", line 720, in handle\r\n    raise ValueError(\"`handle` is not available outside the replica context\"\r\nValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.\r\n```\r\n\r\n\r\n**I am using tensorflow 2.1.0 and keras 2.3.1**\r\n ", "comments": ["Can you please try with tf.keras (as part of TF 2.1) instead of keras 2.3.1? ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40098\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40098\">No</a>\n"]}, {"number": 40097, "title": "Please add additional check in TfLiteQuantizationFree", "body": "Hi!\r\nPlease add additional check for *quantization->params* before using it in the [function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/common.c#L89).\r\nThe bug  appears when default TfLiteConverterCalculator was used ([MediaPipe](https://github.com/google/mediapipe) framework).\r\nPlease look at [theirs source](https://github.com/google/mediapipe/blob/master/mediapipe/calculators/tflite/tflite_converter_calculator.cc#L317).\r\nAs you can see *quant.params* is set to *nullptr*.\r\nTherefore add additional check for similar situations (on other frameworks).\r\nFor example:\r\n```C\r\nif (q_params != NULL) {\r\n    if (q_params->scale) {\r\n        TfLiteFloatArrayFree(q_params->scale);\r\n        q_params->scale = NULL;\r\n    }\r\n    if (q_params->zero_point) {\r\n        TfLiteIntArrayFree(q_params->zero_point);\r\n        q_params->zero_point = NULL;\r\n    }\r\n    free(q_params);\r\n}\r\n````\r\nThanks.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40097\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40097\">No</a>\n"]}, {"number": 40096, "title": "Prediction failed: Could not import PIL.Image. The use of `load_img` requires PIL.", "body": "While using Custom Prediction Routine, I've received the following error according to Logs Viewer in console:\r\n\r\n`Prediction failed: Could not import PIL.Image. The use of 'load_img' requires PIL.\r\n`\r\n- TensorFlow version: `1.15`\r\n- Python version: `3.7`\r\n\r\nsetup.py:\r\n\r\n```\r\nfrom setuptools import setup\r\n\r\ninstall_requires=['pillow', 'opencv-python', 'matplotlib']\r\n\r\nsetup(\r\nname='my_custom_code',\r\nversion='0.1',\r\nscripts=['predictor.py'],\r\ninstall_requires=install_requires\r\n)\r\n```\r\n\r\npredictor.py imports:\r\n```\r\nimport os\r\nimport pandas as pd\r\nimport pickle\r\nimport cv2\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.lib.io import file_io\r\nfrom pandas.compat import StringIO\r\nfrom datetime import datetime\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.models import Model\r\nfrom PIL import Image\r\n```\r\n\r\nI need to use this library because of ImageDataGenerator (part of the predictor.py above):\r\n ```\r\n# Create ImageDataGenerator for test set\r\n    test_img_gen = ImageDataGenerator(preprocessing_function=self.remove_text,\r\n                                       samplewise_std_normalization=True, samplewise_center=True)\r\n    test_generator = test_img_gen.flow_from_dataframe(dataframe=setup_dict['TEST_SET'],\r\n                                                      directory=setup_dict['RAW_DATA_PATH'],\r\n                                                      x_col=\"filename\", y_col='label_str',\r\n                                                      target_size=tuple(setup_dict['IMG_DIM']), batch_size=1,\r\n                                                      class_mode='categorical', validate_filenames=False, shuffle=False)\r\n```\r\n\r\nCommand being used to create new versions:\r\n`gcloud beta ai-platform versions create v1   --model MyModel   --runtime-version 1.15   --python-version 3.7   --origin gs://custom-prediction/   --package-uris gs://custom-prediction/my_custom_code-0.1.tar.gz   --prediction-class predictor.MyPredictor`\r\n\r\nCan someone help me?", "comments": ["@miohana,\r\nPlease take a look at [this](https://stackoverflow.com/a/52230898) StackOverflow answer and [this](https://github.com/asataniAIR/Image_DL_Tutorial/issues/4#issuecomment-405046904) similar GitHub issue, and let us know if it works. Thanks!", "Hi @amahendrakar! Thanks for your reply.\r\n\r\nI've seen those two answers before. As I said, I'm already importing pillow in my setup.py.\r\n\r\n![image](https://user-images.githubusercontent.com/17110848/83631730-5ccdc500-a574-11ea-939d-b97f0137ad12.png)\r\n\r\nWhen adding other libraries, according to [this answer](https://github.com/asataniAIR/Image_DL_Tutorial/issues/4) (adding Keras), I receive the error below:\r\n\r\n`Model requires more memory than allowed. please try to decrease the model size and redeploy\r\n`\r\n\r\nModel settings:\r\nIt's a pretrained resnet101v2 with 491.82MB.", "Hi @miohana \r\n\r\nSaw the following dependency declaration from `keras-preprocessing/setup.py`:\r\n\r\n```\r\nextras_require={\r\n    'tests': ['pandas',\r\n              'Pillow' if sys.version_info >= (3, 0) else 'pillow',\r\n              ...]\r\n }, ...\r\n```\r\n\r\nReally didn't know what's the difference between `Pillow` and `pillow` but it won't hurt to give a try :)", "@miohana Did you deactivate and activate the environment after installing pillow? ", "Hi @ltung-cit!\r\n\r\nUnfortunatelly, I've tried to change to Pillow but the error continues.\r\n\r\nsetup.py:\r\n```\r\nfrom setuptools import setup\r\nimport sys\r\n\r\ninstall_requires=['Pillow' if sys.version_info >= (3,0) else 'pillow', 'opencv-python', 'matplotlib']\r\n\r\nsetup(\r\n    name='my_custom_code',\r\n    version='0.1',\r\n    scripts=['predictor.py'],\r\n    install_requires=install_requires)\r\n```\r\n\r\nResult:\r\n![image](https://user-images.githubusercontent.com/17110848/84031995-06de9000-a96d-11ea-8b33-3628e1055573.png)\r\n\r\nThanks for the reply! \ud83d\ude04 \r\n\r\n@gowthamkpr since we are using Custom Prediction Routine, the environment should deactivate and activate again everytime I create a new version, right?", "@miohana Makes sense.. Can I see full log message? Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Curious if you are able to import the other packages in install_requires (like matplotlib)? Or is it just pillow that is causing an error? \r\nLooking at this note in the [custom prediction routines docs](https://cloud.google.com/ai-platform/prediction/docs/custom-prediction-routines#predictor-tarball), it sounds like using install_requires is not recommended:\r\n\r\n`Note: If you specify PyPI dependencies as install_requires in setup.py, any availability issues with PyPI can affect the reliability of your deployment process. Instead, use packages included in your chosen AI Platform Prediction runtime version or code that you can include directly in the .tar.gz source distribution package.`\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> Hi @amahendrakar! Thanks for your reply.\r\n> \r\n> I've seen those two answers before. As I said, I'm already importing pillow in my setup.py.\r\n> \r\n> ![image](https://user-images.githubusercontent.com/17110848/83631730-5ccdc500-a574-11ea-939d-b97f0137ad12.png)\r\n> \r\n> When adding other libraries, according to [this answer](https://github.com/asataniAIR/Image_DL_Tutorial/issues/4) (adding Keras), I receive the error below:\r\n> \r\n> `Model requires more memory than allowed. please try to decrease the model size and redeploy `\r\n> \r\n> Model settings:\r\n> It's a pretrained resnet101v2 with 491.82MB.\r\n\r\nI am also getting the same error. Could you please help me resolving it. Thanks"]}, {"number": 40095, "title": "Can't set weights for exactly the same model", "body": "Hi there, \r\nI'm trying to save model based on its config and weights but it seems like some properties of config are missing when you initialize using model_from_json. \r\n\r\nThis simple core reproduce the issue, its related (I think with embedding feature columns)\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow import feature_column\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\n\r\n\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n    dataframe = dataframe.copy()\r\n    labels = dataframe.pop('target')\r\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n    if shuffle:\r\n        ds = ds.shuffle(buffer_size=len(dataframe))\r\n    ds = ds.batch(batch_size)\r\n    return ds\r\n\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndf = pd.read_csv(URL)\r\ndf.head()\r\n\r\ncountries = ['afghanistan', 'aland islands', 'albania', 'algeria', 'american samoa', 'andorra', 'angola', 'anguilla', 'antarctica', 'antigua and barbuda', 'argentina', 'armenia', 'aruba', 'australia', 'austria', 'azerbaijan', 'bahamas (the)', 'bahrain', 'bangladesh', 'barbados', 'belarus', 'belgium', 'belize', 'benin', 'bermuda']\r\n\r\ndf['country'] = pd.DataFrame(np.random.choice(list(countries), len(df)))\r\n\r\nfeature_columns = []\r\nfor f in list(df.columns):\r\n    if f!='target':\r\n        if df[f].dtype.name in ['int64','float64']:\r\n            num_feat = feature_column.numeric_column(f)\r\n            bucket_feat = feature_column.bucketized_column(num_feat, boundaries=[25,50,75,90,95,99])\r\n            feature_columns.append(bucket_feat)\r\n        else:\r\n            categ_feat = feature_column.categorical_column_with_vocabulary_list(f, df[f].unique())\r\n            categ_feat_embedding = feature_column.embedding_column(categ_feat, dimension=8)\r\n            feature_columns.append(categ_feat_embedding)\r\n\r\ntrain_df, val_df = train_test_split(df, test_size=0.2)\r\n\r\nbatch_size = 128\r\ntrain_ds = df_to_dataset(train_df, batch_size=batch_size)\r\nval_ds = df_to_dataset(val_df, shuffle=False, batch_size=batch_size)\r\n\r\nfeature_layer = keras.layers.DenseFeatures(feature_columns)\r\nmodel = keras.Sequential()\r\nmodel.add(feature_layer)\r\nmodel.add(keras.layers.Dense(128, activation='relu'))\r\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\r\nmodel.compile(optimizer=keras.optimizers.Adam(lr=1e-3),\r\n              loss=keras.losses.BinaryCrossentropy())\r\n\r\n\r\nhistory = model.fit(train_ds,\r\n                    validation_data=val_ds,\r\n                    epochs=2,\r\n                    verbose=1)\r\n\r\n\r\nmodel2 = tf.keras.models.model_from_json(model.to_json())\r\nmodel2.set_weights(model.get_weights())\r\n```\r\n\r\nThe error is:\r\n`ValueError: Weights for model sequential_5 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.`\r\n\r\nWhen I compare the config of model (original) against model2 (replica) the only feature that differ are embeddings one:\r\n\r\n`\r\n'initializer': {'class_name': 'TruncatedNormal',\r\n        'config': {'mean': 0.0,\r\n         'stddev': 0.35355339059327373,\r\n         'seed': None,\r\n         'dtype': 'float32'}},\r\n`\r\nSpecifically the last one dtype\r\n\r\n", "comments": ["@frarito \r\nCan you please refer to these links and let us know if it helps:\r\n[link](https://github.com/keras-team/keras-tuner/issues/54)  [link1](https://github.com/tensorflow/tensorflow/issues/28009)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40095\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40095\">No</a>\n"]}, {"number": 40094, "title": "[tflite/EglSync] Check whether sync extensions are exposed", "body": "According to [eglGetProcAddress documentation](https://www.khronos.org/registry/EGL/sdk/docs/man/html/eglGetProcAddress.xhtml), EGL implementations are not required to return `nullptr` if the function in question is not supported, and clients should query the extension string as well.\r\n\r\nThis P/R adds said check to [`egl_sync.cc`](https://github.com/tensorflow/tensorflow/blob/7280cb31337d73a2fc869d7ba7cdcaf5295ad26e/tensorflow/lite/delegates/gpu/cl/egl_sync.cc).", "comments": ["Thank you! Moved the checks higher so that they're performed before `eglGetProcAddress` gets called."]}, {"number": 40093, "title": "[ROCm] Fix for ROCm CSB breakage - 200529", "body": "This PR \r\n* adds no_rocm tag to one unit-test that recently regressed on the ROCm platform and \r\n* reverts PR #39914, and instead adds a no_rocm tag to the unit-test regression that was being addressed by that PR\r\n\r\nSee individual commit messages for details.\r\n\r\n---------------------\r\n\r\n/cc @chsigg @cheshire @nvining-work \r\n", "comments": []}, {"number": 40092, "title": "in resolution of [Wsign-compare] warning id 14", "body": "@mihaimaruseac ", "comments": []}, {"number": 40091, "title": "in resolution of [Wsign-compare] warning id 13", "body": "index `i` changed to type `size_t`\r\n\r\n@mihaimaruseac ", "comments": []}, {"number": 40090, "title": "run this:python freeze_graph.py \\ --input_graph C:/Users/Administrator.DESKTOP-5V6G6NA/tensorflowfiles/bird\\data2/slim/tmp/inception_v3_inf_graph.pb \\ --input_checkpoint .tmp/train_logs/model.ckpt-721 \\ --input_binary true \\ --output_node_names InceptionV3/Predictions/Reshape_1 \\ --output_graph .tmp/frozen_graph.pb", "body": "WARNING:tensorflow:From freeze_graph.py:124: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nW0602 23:33:08.629297 12556 deprecation.py:323] From freeze_graph.py:124: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x0000310c (most recent call first):\r\n  File \"D:\\91UserData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 384 in get_matching_files_v2\r\n  File \"D:\\91UserData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 363 in get_matching_files\r\n  File \"D:\\91UserData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 372 in checkpoint_exists\r\n  File \"D:\\91UserData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 324 in new_func\r\n  File \"freeze_graph.py\", line 124 in freeze_graph_with_def_protos\r\n  File \"freeze_graph.py\", line 357 in freeze_graph\r\n  File \"freeze_graph.py\", line 374 in main\r\n  File \"freeze_graph.py\", line 482 in <lambda>\r\n  File \"D:\\91UserData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 250 in _run_main\r\n  File \"D:\\91UserData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 299 in run\r\n  File \"D:\\91UserData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40 in run\r\n  File \"freeze_graph.py\", line 483 in run_main\r\n  File \"freeze_graph.py\", line 487 in <module>\r\n\r\n\r\nget this error;\r\ntf=1.14.0 cpu\r\n", "comments": ["@lucyLLLL \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Please, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).Request you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "windows10\uff0ctensorflow=1.14.0\uff0ccpu\uff0c\r\ndataset is flowers\r\nuse slim Module\r\ni learn from this [https://blog.csdn.net/stesha_chen/article/details/81976415?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.nonecase]\r\n\r\nbecause  inception_v3_inf_graph.pb only saves the network structure of Inception V3, and does not contain the model parameters obtained by training. You need to save the model parameters in checkpoint. so  i  use freeze_graph.py the error is appear  \r\n", "@lucyLLLL \r\n\r\nI believe you are using https://github.com/tensorflow/models/tree/master/research/slim models.\r\n\r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Models repo from [here.](https://github.com/tensorflow/models/issues) Thanks!", "the code is [https://github.com/lucyLLLL/slim-flowers.git],thank you for your help @ravikyram ", "@lucyLLLL \r\n\r\nCan we close this issue here and can be tracked in Models repo?.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40090\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40090\">No</a>\n"]}, {"number": 40089, "title": "in resolution of [Wsign-compare] warning id 10", "body": "Static cast  `int index` for the sake of the single `size_t` to `int` comparison on line 116.\r\n\r\n@mihaimaruseac ", "comments": []}, {"number": 40088, "title": "in resolution of [Wsign-compare] warning id 9", "body": "LargeAllocationWarningBytes() is implied to be a quantity ( it calls AvailableRam(), which unless improperly named, is a quantity/ i.e. non-negative ). \r\n\r\nGiven LargeAllocationWarningBytes() is a quantity, it can be cast to size_t with behavior constrained to be as-intend.\r\n\r\n@mihaimaruseac ", "comments": []}, {"number": 40087, "title": "micro_speech train example broken", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Windows 10\r\n- tensorflow 1.x\r\n- Arduino Nano 33\r\n\r\nThe training portion of the micro_speech example appears to be broken.  The only modification I have made is to train for the words \"up,down,on,off\" instead of \"yes,no\".  I ran all of the code cells except for the code cell that skips the training.  The following code cell generates an error:\r\n\r\nwith tf.Session() as sess:\r\n  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\r\n  float_tflite_model = float_converter.convert()\r\n  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\r\n  print(\"Float model is %d bytes\" % float_tflite_model_size)\r\n\r\n  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\r\n  converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n  converter.inference_input_type = tf.lite.constants.INT8\r\n  converter.inference_output_type = tf.lite.constants.INT8\r\n  def representative_dataset_gen():\r\n    for i in range(100):\r\n      data, _ = audio_processor.get_data(1, i*1, model_settings,\r\n                                         BACKGROUND_FREQUENCY, \r\n                                         BACKGROUND_VOLUME_RANGE,\r\n                                         TIME_SHIFT_MS,\r\n                                         'testing',\r\n                                         sess)\r\n      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\r\n      yield [flattened_data]\r\n  converter.representative_dataset = representative_dataset_gen\r\n  tflite_model = converter.convert()\r\n  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\r\n  print(\"Quantized model is %d bytes\" % tflite_model_size)\r\n\r\nError generated below:\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n11 frames\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'data/background_data' with dtype float and shape [16000,1]\r\n\t [[{{node data/background_data}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'data/background_data' with dtype float and shape [16000,1]\r\n\t [[node data/background_data (defined at /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]\r\n\r\n", "comments": ["@envy1400 Can you please check [this similar issue](https://github.com/tensorflow/tensorflow/issues/38684) and let us know whether it helped or not? Thanks!", "The link to a google colab page in there seemed to work.  I don't know how it's different or if it's been updated.  I guess this is resolved", "@envy1400 Yes, that colab was updated recently. \r\nI am closing this issue as you confirmed that this was resolved. But, feel free to reopen if this persists again later. Thanks!"]}, {"number": 40086, "title": "layers.set_weights() not working with tf.keras.optimizers.Ftrl", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v2.1.0-rc2-17-ge5bf8de410', '2.1.0')\r\n- Python version: Python 2.7.14\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI create a model and manually initialize its weights to W0 using layers.set_weights().\r\nThen I fit the model with tf.keras.optimizers.Ftrl and a tiny learning rate (1e-5).\r\nThe model weights became to small values around zero. It seems that model weights have been reinitialized to zeros.\r\n\r\n**Describe the expected behavior**\r\nBecause the learning rate is very small, the model weights are expected to remain near to W0 (the manually initialized value) after training.\r\nI tried other optimizers (SGD, Adam) from tf.keras.optimizers, the behavior is under expectation.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nvocabulary = ['word1', 'word2']\r\ncategorical_column = tf.feature_column.categorical_column_with_vocabulary_list(\r\n    'feat1', vocabulary)\r\nembedding_column = tf.feature_column.embedding_column(\r\n    categorical_column,\r\n    dimension=1,\r\n    initializer=tf.constant_initializer(0))\r\nfeature_columns = [embedding_column]\r\n\r\n# In this model, only embeddings are trainable variables\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.DenseFeatures(feature_columns),\r\n    tf.keras.layers.Dense(\r\n        units=1,\r\n        use_bias=False,\r\n        trainable=False,\r\n        kernel_initializer=tf.constant_initializer(1))\r\n])\r\n\r\ninstances = {'feat1': np.array(['word1', 'word1', 'word2', 'word2'])}\r\nlabels = np.array([0, 0, 0, 0])\r\n\r\n# Call the model to make variable initilized\r\nmodel(instances)\r\nprint(model.trainable_variables)   # output weight [0, 0]\r\n\r\n# Manually initialize mdoel weights to [-9, -7]\r\nweights = [np.array([[-9], [-7]])]\r\nmodel.layers[0].set_weights(weights)\r\nprint(model.trainable_variables)   # output weight [-9, -7]\r\n\r\n# Fit the model using Ftrl optimizer with a small learning rate\r\noptimizer = tf.keras.optimizers.Ftrl(learning_rate=1e-5)\r\nmodel.compile(\r\n    optimizer=optimizer,\r\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\r\nmodel.fit(instances, labels, epochs=1, verbose=2)\r\nprint(model.trainable_variables)  # output weight [-8.501399e-07, -7.271125e-06]\r\n```\r\n**Other info / logs** \r\n```\r\n[<tf.Variable 'sequential_12/dense_features_12/feat1_embedding/embedding_weights:0' shape=(2, 1) dtype=float32, numpy=\r\narray([[0.],\r\n       [0.]], dtype=float32)>]\r\n[<tf.Variable 'sequential_12/dense_features_12/feat1_embedding/embedding_weights:0' shape=(2, 1) dtype=float32, numpy=\r\narray([[-9.],\r\n       [-7.]], dtype=float32)>]\r\nTrain on 4 samples\r\n4/4 - 0s - loss: 5.1743e-04\r\n[<tf.Variable 'sequential_12/dense_features_12/feat1_embedding/embedding_weights:0' shape=(2, 1) dtype=float32, numpy=\r\narray([[-8.501399e-07],\r\n       [-7.271125e-06]], dtype=float32)>]\r\n```", "comments": ["i am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/064e0a3e3fd6abc3ca3f4ab766ffd834/untitled205.ipynb)", "@gmlu \r\nThe code produces warnings on tf 2.5 which would guide you to fix the issue,\r\nplease refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/d4c09f03af4c5655e88d3fc41a7c59b0/untitled597.ipynb) and upgrade your tf and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40086\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40086\">No</a>\n"]}, {"number": 40085, "title": "Lambda layer does not compute masked values properly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- TensorFlow installed from (source or binary):\r\nconda binary\r\n- TensorFlow version (use command below):\r\n'2.1.0'\r\n- Python version:\r\n3.7\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nOutput of the model with lambda subtracting 0.5 from input, and input as [1, -10], with -10 being masked value\r\n```\r\n<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\r\narray([[[ 0.5],\r\n        [-0.5]]], dtype=float32)\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nOutput\r\n```\r\n<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\r\narray([[[ 0.5],\r\n        [ 0. ]]], dtype=float32)\r\n```\r\nor\r\n```\r\n<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\r\narray([[[ 0.5],\r\n        [ -10. ]]], dtype=float32)\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow import keras\r\ninputs_shape = (2, 1)\r\ninputs = keras.layers.Input(shape=inputs_shape)\r\nmasked = keras.layers.Masking(mask_value=-10., input_shape=inputs_shape)(inputs)\r\noutput = keras.layers.Lambda(lambda x: x - 0.5)(masked)\r\nmodel = keras.Model(inputs=inputs, outputs=output)\r\ndata = np.array([[1, -10]], dtype=np.float32).reshape(-1, 2, 1)\r\nmodel(data)\r\n```", "comments": ["I have tried in colab with TF version 2.1, 2.2, nightly version(`2.3.0-dev20200602`) and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/5bbd839c334cc24d5b295f95313a375f/untitled948.ipynb).Thanks!", "I ran the code shared and the issue still persist on tf-nightly[2.4.0-dev20201012], please find the [gist here](https://colab.research.google.com/gist/Saduf2019/aa3f0249313757de0d0d83f25170565a/untitled431.ipynb).", "@Strateus I think this is intended behavior. If you look at the input to the `lambda` layer is [1,0] and during `lambda x: x - 0.5` we are deducting 0.5 which results in [0.5,-0.5]. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/18a34178226c88823b346b15191fad3b/untitled431.ipynb). Thanks!\r\n\r\n```\r\nmodel2 = keras.Model(inputs=inputs, outputs=masked)\r\nmodel2(data)\r\n```\r\n\r\nOutput is \r\n\r\n```\r\n<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\r\narray([[[ 1.],\r\n        [-0.]]], dtype=float32)>\r\n```\r\n\r\nPlease verify once and let me know if I am missing anything. Please close the issue if this was resolved for you. Thanks", "@jvishnuvardhan your interpretation of masking seems to be different from the official one: _Masking is a way to tell sequence-processing layers that certain timesteps in an input are missing, and thus should be **skipped** when processing the data._\r\n\r\ncheck here: https://www.tensorflow.org/guide/keras/masking_and_padding\r\n\r\n_Skipping_ means you **return the input value all the time**, no matter what processing you have inside the layer. An alternative would be to return a value of a mask itself (but this is less common).\r\n\r\nLambda layer **should not** deduct anything for the masked values, but it currently does, which is incorrect.\r\n\r\nYour gist shows something else: that lambda layer is working without mask. I was not claiming it is not working without masking. I was claiming **masking inside Lambda layer is not working properly**.", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "it still persists in 2.7", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40085\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40085\">No</a>\n"]}]