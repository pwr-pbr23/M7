[{"number": 44471, "title": "[ROCm] Update to use ROCm 3.9 (when building TF with --config=rocm)", "body": "\r\n~~PR https://github.com/tensorflow/tensorflow/pull/43636 is a pre-requisite for this PR.~~\r\n\r\n~~For the time being, this PR includes commits from it's pre-req as well.  Once the pre-req PR is merged, I will rebase this PR to remove those commits.~~\r\n\r\nupdate : PR  https://github.com/tensorflow/tensorflow/pull/43636 has been merged, and I have rebased this PR to pull it in\r\n\r\n--------------------------------------\r\n\r\n\r\n/cc @cheshire @chsigg @nvining-work \r\n", "comments": ["@deven-amd  Can you please resolve conflicts? Thanks!", "@gbaned done", "I see the cause of the errors in `Linux GPU` and `Ubuntu CPU` ... will push out a fix soon", "I have actually already rebased the earlier version of your PR and fixed a few issues, hopefully it will be merged soon.", "@akuegel \r\n\r\nThis PR does seem to have gotten merged...thank you for rebasing it and seeing it through.  \r\n\r\nThe merged PR seems to have dropped one of the commits ( https://github.com/tensorflow/tensorflow/commit/fbfdb64c3375f79674a4f56433f944e1e4fd6b6e ) even though it is mentioned in PR message here (https://github.com/tensorflow/tensorflow/commit/312e6bacca8377d94ca4f62ca899683817728c67). \r\n\r\nDo you want me to close out this PR and file another one to pick up the missing change?\r\n\r\nthanks\r\n\r\n\r\n", "Please create another one. I am not sure why this was not included, but in any case I was just happy that I could get this merged at all. It should be easier now to apply smaller fixes on top of that."]}, {"number": 44470, "title": "per_image_standardization Breaks Keras Model Loading", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu: 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nHere is a small example script that demonstrates the issue:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\n\r\ninputs = tf.keras.Input(shape=(100, 100, 3))\r\nnormalized = layers.Lambda(tf.image.per_image_standardization)(inputs)\r\ndense1 = layers.Dense(100, activation=\"relu\")(normalized)\r\ndense2 = layers.Dense(10, activation=\"softmax\")(dense1)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=dense2)\r\n\r\nmodel.save(\"my_model.hd5\", save_format=\"h5\")\r\n\r\nnew_model = tf.keras.models.load_model(\"my_model.hd5\")  # <-- Error here\r\n```\r\n\r\nTo my knowledge, this should work. However, when I run it, I get an error on the last line:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\nTypeError: 'str' object is not callable\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"min_loading_bug_example.py\", line 13, in <module>\r\n    new_model = tf.keras.models.load_model(\"my_model.hd5\")\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\", line 182, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 177, in load_model_from_hdf5\r\n    model = model_config_lib.model_from_config(model_config,\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/serialization.py\", line 171, in deserialize\r\n    return generic_utils.deserialize_keras_object(\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 354, in deserialize_keras_object\r\n    return cls.from_config(\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\", line 616, in from_config\r\n    input_tensors, output_tensors, created_layers = reconstruct_from_config(\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\", line 1214, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\", line 1162, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 925, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1117, in _functional_construction_call\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\", line 903, in call\r\n    result = self.function(inputs, **kwargs)\r\n  File \"/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 205, in wrapper\r\n    result = dispatch(wrapper, args, kwargs)\r\nTypeError: 'module' object is not callable\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect this to load the model without issue.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nPoking around in the debugger, the issue seems to be with [this code](https://github.com/tensorflow/tensorflow/blob/5681c179eff80bce00e526303950b67b23cad14c/tensorflow/python/util/dispatch.py#L205) in `python/util/dispatch.py`. Specifically, it looks like the `dispatch` variable is being set to the `dispatch` module instead of the `dispatch` function inside that module. I'm not an expert on the idiosyncrasies of Tensorflow or Python closures, but it seems likely that something weird is going on with scoping here. For some reason, it seems like it's using the global definition of `dispatch` from `image_ops_impl` (which would be active when the `add_dispatch_support` decorator is applied) instead of the `dispatch` function.\r\n", "comments": ["I have tried in colab with TF version 2.3.1 and nightly version(`2.5.0-dev20201029`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/4ba76c218a0ab0465aeecb5c47dbf3df/untitled483.ipynb#scrollTo=7LAXmC9S501o). Thanks!", "When creating functional models, you do not need to add Lambda around TensorFlow functions -- just this is fine:\r\n\r\n`normalized = tf.image.per_image_standardization(inputs)`\r\n\r\nGive this a try, and if you are still getting an error please reopen this bug. It passes with TF-nightly", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44470\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44470\">No</a>\n"]}, {"number": 44469, "title": "Add a build badge for TFLM.", "body": "Hopefully this is the starting point for additional CI builds.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44468, "title": "Can not enable XNNPack android as there is no setUseXNNPACK option. ", "body": "System information\r\n\r\nOS Platform and Distribution: Android v9.0\r\nMobile device: armv7 and v8\r\n\r\nTensorFlow version: 2.3.0 or nightly\r\n\r\nInstalled using virtualenv?\r\n    def tfl_version = \"0.0.0-nightly\";\r\n    implementation (\"org.tensorflow:tensorflow-lite:${tfl_version}\") { changing = true }\r\n    implementation (\"org.tensorflow:tensorflow-lite-gpu:${tfl_version}\") { changing = true }\r\n\r\nDescribe the problem\r\n\r\nI am using tflite on android using below repos. I have tried both version 2.3.0 as well as nightly builds. According to https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html I can enable XNNPack by setUseXNNPACK. But it can not find the symbol/option.\r\n", "comments": ["@kafan1986,\r\nCould you please provide the code / exact sequence of commands that you executed before running into the problem?\r\n\r\nAlso, please take a look at similar issue [#42056](https://github.com/tensorflow/tensorflow/issues/42056) and let us know if it helps. Thanks!", "@amahendrakar I am not sure what else can I share. The example inference code is pretty standard one. I am attaching a image inside android studio and it shows no member function with setUseXNNPACK name.\r\n![Screen Shot 2020-11-02 at 3 14 18 PM](https://user-images.githubusercontent.com/7953422/97853550-3fb54200-1d1e-11eb-91e1-92f5f5ad820a.png)\r\n", "Android studio cached old version somehow. It is OK now. Closing issue."]}, {"number": 44467, "title": "h5py==3.0.0 causes issues with keras model loads in tensorflow 2.1.0", "body": "h5py released version 3.0.0 today and it causes this code to fail:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/hdf5_format.py#L182\r\nwith error:\r\n\r\n```\r\nFile \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 146, in load_model\r\nreturn hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\nFile \"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 166, in load_model_from_hdf5\r\nmodel_config = json.loads(model_config.decode('utf-8'))\r\nAttributeError: 'str' object has no attribute 'decode'\r\n```\r\n\r\nIt looks like in version 2.1.0 the `h5py` version is not pinned (it is pinned in master), which is causing the issue. ", "comments": ["I think it will be hard to expect a backport on 2.1.0.", "the new h5py version just came out today: https://pypi.org/project/h5py/#history\r\n\r\nPinning it to a version <3.0.0 (or at least throwing a warning) is a very lightweight change with no risk.", "> the new h5py version just came out today: https://pypi.org/project/h5py/#history\n> \n> Pinning it to a version <3.0.0 (or at least throwing a warning) is a very lightweight change with no risk.\n\nYes but as you know generally we had very few patch releases (e.g. 2.1.x) so It Is quite hard to have wheels with these fixes on old versions.", "@bhack I'm not going to add any more messages after this, but I think you can see based on these other issues the Keras API is essentially broken because h5py new release. I'm not sure why the version was not pinned as it is in master, but I strongly advise that you pin to h5py==2.10.0 this for all TF >= 2.1.", "/cc @mihaimaruseac @angerson there are many Dockerfile and other file not constrained.", "FWIW, I got a similar error message with h5py 3.0, and removing `.decode('utf-8')` from `tensorflow/python/keras/saving/hdf5_format.py` allows me to load Keras models as before.", "We cannot pin the versions without doing a patch release. We only do patch releases for security issues.\r\n\r\nThis issue has a quick workaround: `pip install tensorflow h5py<3.0.0`.\r\n\r\nWe are in the process of releasing TF 2.4 which should not be affected by this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44467\">No</a>\n", "Thought it might be pertinent to link to the h5py repo:\r\nThis has been labeled as a bug in their 3.0.0 release (rather than a backwards incompatibility),\r\nand is slated to be fixed in an upcoming release: https://github.com/h5py/h5py/issues/1732\r\n\r\nedit:\r\nActually, there might be two separate but related issues here. The bug in the h5py thread occurs during the model-save codepath, but the bug in this TensorFlow thread is in the model-load codepath, which if I had to wildly guess _might_ require a typecheck/cast on TensorFlow's side", "> there might be two separate but related issues here\r\n\r\nThat seems the case. I tried h5py 3.1.0, and the error `AttributeError: 'str' object has no attribute 'decode'` still happens.", ":laughing: Thanks!!", "`pip install 'h5py<3.0.0'`", "This error is also present in tensorflow 1.15: https://stackoverflow.com/questions/64795784/google-ai-platform-unexpected-error-when-loading-the-model-str-object-has-no", "Is there a timeline for the fix/release? I am using Google AI Platform which unfortunately does not allow you to easily override the versions of its standard libraries including `h5py` ([stackoverflow](https://stackoverflow.com/questions/62816129/how-do-you-override-google-ai-platforms-standard-librarys-i-e-upgrade-scikit))", "also incapable for tensorflow3.2.1\r\n> update: I mean version 2.3.1", "> also incapable for tensorflow3.2.1\r\n\r\nof course, we cannot use tf3 right now unless you come from another universe", "Please keep in mind that we don't update older branches unless for security reasons. So, because our bounds on dependencies were too large (our mistake, fixed now, but see also #44654), you will have to manually downgrade `h5py` in your installation.\r\n\r\nYou can do this in two ways:\r\n\r\n* Either you install with an upper bound, a la:\r\n    ```console\r\n    $ pip install tensorflow 'h5py < 3.0.0'\r\n    ```\r\n\r\n* Or you install TF and then downgrade `h5py`, a la:\r\n\r\n    ```console\r\n    $ pip install tensorflow\r\n    $ pip uninstall -y h5py\r\n    $ pip install 'h5py < 3.0.0'\r\n    ```\r\n\r\nAlternatively, the issue should be fixed in nightly and in TF 2.4 RCs (in the sense that h5py is upper bounded to not get 3.0.0).\r\n\r\nWe will have people working on making TF work with `h5py >= 3` in the future, but this will only land in TF 2.5 or later.", "Has this been fixed yet for TF 2.3.1? ", "Hate to reply with a link to a comment just above yours, but here it is: https://github.com/tensorflow/tensorflow/issues/44467#issuecomment-729300306", "My bad, I saw the issue was still open and assumed it was waiting on a fix. Thanks @mihaimaruseac ", "The recommended fix of downgrading the h5py version:\r\n\r\n` pip install 'h5py < 3.0.0'`\r\n\r\ndoes not work on TensorFlow 2.3 because:\r\n\r\n```\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in load_weights(self, filepath, by_name, skip_mismatch, options)\r\n   2202           'first, then load the weights.')\r\n   2203     self._assert_weights_created()\r\n-> 2204     with h5py.File(filepath, 'r') as f:\r\n   2205       if 'layer_names' not in f.attrs and 'model_weights' in f:\r\n   2206         f = f['model_weights']\r\n\r\nAttributeError: module 'h5py' has no attribute 'File'\r\n```", "@danzafar \r\nPlease update if this is still an issue.", "Can you test if release candidate for TF 2.4 works?", "I'll leave this to @bhack ", "2.4.0-rc3 has `h5py~=2.10.0`", "Downgrading to h5py does not work for me.", "@mihaimaruseac Can you take a look at https://github.com/tensorflow/tensorflow/pull/45380 ?", "Downgrading to h5py worked some time ago. Now old h5py does not work and #40991 appears.", "Removing `.decode('utf-8')` on lines 176 and 190 in `/usr/lib/python3.9/site-packages/tensorflow/python/keras/saving/hdf5_format.py` worked for me.", "We are now at 3.1.0 on master https://github.com/tensorflow/tensorflow/pull/45380", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44467\">No</a>\n", "> Has this been fixed yet for TF 2.3.1?\r\n\r\nDowngrading TF to 2.3.0 works for me.", "**Just try**\r\npip install h5py==2.10.0\r\n\r\n**It completely worked in my case. \r\nHope it will work for you.**", "@bhack You said that master (I assume 2.4.0) is compatible with h5py 3.1, but I still have this issue...", "> @bhack You said that master (I assume 2.4.0) is compatible with h5py 3.1, but I still have this issue...\n\nNot in time for 2.4.0 so master from source of nightly TF wheels", "Hello I tried to install h5py == 2.10.0 version on my venv\r\nBut still got same error\r\n\r\n![image](https://user-images.githubusercontent.com/75870530/109180098-821af700-77c5-11eb-917e-4282dea2da00.png)\r\n![image](https://user-images.githubusercontent.com/75870530/109180260-a4ad1000-77c5-11eb-9c9a-6e34d6d3b7de.png)\r\n\r\nPlease help me\r\nThanks All!", "I went around doing this:\r\nEdit the hdf5_format.py file to delete all the references to `.decode('utf-8')` e.g: model_config = json.loads(model_config). Once you get the model loaded save the model in a json file and the weights in a h5 file. Then load the model like this: \r\n```py\r\njson_file = open('keras-facenet-h5/model.json', 'r')\r\nloaded_model_json = json_file.read()\r\njson_file.close()\r\nmodel = model_from_json(loaded_model_json)\r\nmodel.load_weights('keras-facenet-h5/model.h5')\r\n```\r\nWith that, you won't have the problem with the decode('utf-8') and moreover, it is 10x faster than loading the whole model and the weights from an h5 file\r\n\r\nAfter you can restore the original hdf5_format.py to the original one.", "Can you try with `tf-nightly`?", "@bhack can you let me know more about tf-nightly?\r\n", "https://pypi.org/project/tf-nightly/\nhttps://stackoverflow.com/questions/59500294/what-is-the-difference-between-tf-nightly-and-tensorflow-in-pypi", "@andcastillo Can you show full example or guide to do like your suggestion? Thanks", "> Can you try with `tf-nightly`?\r\n\r\nThank you, it works", "for info : \r\nI have the same error with tensorflow 2.4.1 and h5py 3.2.1", "> for info :\r\n> I have the same error with tensorflow 2.4.1 and h5py 3.2.1\r\n\r\nCan you try with tf-nightly?", "Posting here because Google search for this issue led me directly to this page. Presumably it will lead others here as well.\r\n\r\n```\r\nUbuntu 18.04.5\r\npip3 install tensorflow-gpu==2.0.0\r\n\r\n```\r\nYou get this same problem. \r\n\r\nSolution was:\r\n\r\n`pip3 install h5py=2.10.0\r\n`\r\n\r\nWhy do I install TF 2.0.0 in March 2021, you ask? Because the book I am studying uses TF 2.0.0 and hard experience has taught me there can be subtle differences between TF minor versions that are hard to understand for TF noobs like me.", "> > for info :\r\n> > I have the same error with tensorflow 2.4.1 and h5py 3.2.1\r\n> \r\n> Can you try with tf-nightly?\r\n\r\nI created a virtual env with tf-nightly instead of tensorflow-2.4.\r\nNow i can not use from tensorflow.python.keras.models import  load_model anymore .\r\nwas it replaced ?? ", "> Hello I tried to install h5py == 2.10.0 version on my venv\r\n> But still got same error\r\n> \r\n> ![image](https://user-images.githubusercontent.com/75870530/109180098-821af700-77c5-11eb-917e-4282dea2da00.png)\r\n> ![image](https://user-images.githubusercontent.com/75870530/109180260-a4ad1000-77c5-11eb-9c9a-6e34d6d3b7de.png)\r\n> \r\n> Please help me\r\n> Thanks All!\r\n\r\n\r\n\r\n> Hello I tried to install h5py == 2.10.0 version on my venv\r\n> But still got same error\r\n> \r\n> ![image](https://user-images.githubusercontent.com/75870530/109180098-821af700-77c5-11eb-917e-4282dea2da00.png)\r\n> ![image](https://user-images.githubusercontent.com/75870530/109180260-a4ad1000-77c5-11eb-9c9a-6e34d6d3b7de.png)\r\n> \r\n> Please help me\r\n> Thanks All!\r\n\r\nI solved this problem with tensorflow 1.13.1 and keras 2.2.4", "> I went around doing this:\r\n> Edit the hdf5_format.py file to delete all the references to `.decode('utf-8')` e.g: model_config = json.loads(model_config). Once you get the model loaded save the model in a json file and the weights in a h5 file. Then load the model like this:\r\n> \r\n> ```python\r\n> json_file = open('keras-facenet-h5/model.json', 'r')\r\n> loaded_model_json = json_file.read()\r\n> json_file.close()\r\n> model = model_from_json(loaded_model_json)\r\n> model.load_weights('keras-facenet-h5/model.h5')\r\n> ```\r\n> \r\n> With that, you won't have the problem with the decode('utf-8') and moreover, it is 10x faster than loading the whole model and the weights from an h5 file\r\n> \r\n> After you can restore the original hdf5_format.py to the original one.\r\n\r\nWhere is hdf5_format.py located, was unable to find it in \"h5py\" folder.\r\n\r\nThanks!", "> \r\n> \r\n> > I went around doing this:\r\n> > Edit the hdf5_format.py file to delete all the references to `.decode('utf-8')` e.g: model_config = json.loads(model_config). Once you get the model loaded save the model in a json file and the weights in a h5 file. Then load the model like this:\r\n> > ```python\r\n> > json_file = open('keras-facenet-h5/model.json', 'r')\r\n> > loaded_model_json = json_file.read()\r\n> > json_file.close()\r\n> > model = model_from_json(loaded_model_json)\r\n> > model.load_weights('keras-facenet-h5/model.h5')\r\n> > ```\r\n> > \r\n> > \r\n> > With that, you won't have the problem with the decode('utf-8') and moreover, it is 10x faster than loading the whole model and the weights from an h5 file\r\n> > After you can restore the original hdf5_format.py to the original one.\r\n> \r\n> Where is hdf5_format.py located, was unable to find it in \"h5py\" folder.\r\n> \r\n> Thanks!\r\n\r\nThe error message quotes the location of the file with the error.\r\n\r\nLook at the error message python is printing. About the third last line will be:\r\n\r\n`File: \"/path/to/your/hdf5_format.py\" line NNN in load_model_from_hdf5\r\n`\r\n\r\nYour hdf5_format.py file is in directory /path/to/your/.\r\n", "Is this still a problem in TF 2.4.0? I still get the error in this case. Need to downgrade h5py.", "> Is this still a problem in TF 2.4.0? I still get the error in this case. Need to downgrade h5py.\n\nhttps://pypi.org/project/tensorflow/2.5.0rc0/", "Hey guys, when Iam trying to downgrade the version of h5py I get the follwing error: \r\n\r\nERROR: Failed building wheel for h5py\r\n  Running setup.py clean for h5py\r\nFailed to build h5py\r\nWARNING: Ignoring invalid distribution -ip (d:\\tensorflow object detection\\tfodcourse\\tfod\\lib\\site-packages)\r\nInstalling collected packages: h5py\r\n  Attempting uninstall: h5py\r\n    WARNING: Ignoring invalid distribution -ip (d:\\tensorflow object detection\\tfodcourse\\tfod\\lib\\site-packages)\r\n    Found existing installation: h5py 3.1.0\r\n    Uninstalling h5py-3.1.0:\r\n      Successfully uninstalled h5py-3.1.0\r\n    Running setup.py install for h5py ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: 'd:\\tensorflow object detection\\tfodcourse\\tfod\\scripts\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = \"C:\\\\Users\\\\Arne'\"'\"'s PC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8sioo065\\\\h5py_f1f64cd207fd4e3fbbc73e1ddc4b40d2\\\\setup.py\"; __file__=\"C:\\\\Users\\\\Arne'\"'\"'s PC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8sioo065\\\\h5py_f1f64cd207fd4e3fbbc73e1ddc4b40d2\\\\setup.py\";f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Arne'\"'\"'s PC\\AppData\\Local\\Temp\\pip-record-prai98me\\install-record.txt' --single-version-externally-managed --compile --install-headers 'd:\\tensorflow object detection\\tfodcourse\\tfod\\include\\site\\python3.9\\h5py'\r\n         cwd: C:\\Users\\Arne's PC\\AppData\\Local\\Temp\\pip-install-8sioo065\\h5py_f1f64cd207fd4e3fbbc73e1ddc4b40d2\\\r\n\r\nRolling back uninstall of h5py\r\n  Moving to d:\\tensorflow object detection\\tfodcourse\\tfod\\lib\\site-packages\\h5py-3.1.0.dist-info\\\r\n   from D:\\Tensorflow Object Detection\\TFODCourse\\tfod\\Lib\\site-packages\\~5py-3.1.0.dist-info\r\n  Moving to d:\\tensorflow object detection\\tfodcourse\\tfod\\lib\\site-packages\\h5py\\\r\n   from D:\\Tensorflow Object Detection\\TFODCourse\\tfod\\Lib\\site-packages\\~5py\r\nERROR: Command errored out with exit status 1: 'd:\\tensorflow object detection\\tfodcourse\\tfod\\scripts\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = \"C:\\\\Users\\\\Arne'\"'\"'s PC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8sioo065\\\\h5py_f1f64cd207fd4e3fbbc73e1ddc4b40d2\\\\setup.py\"; __file__=\"C:\\\\Users\\\\Arne'\"'\"'s PC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8sioo065\\\\h5py_f1f64cd207fd4e3fbbc73e1ddc4b40d2\\\\setup.py\";f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Arne'\"'\"'s PC\\AppData\\Local\\Temp\\pip-record-prai98me\\install-record.txt' --single-version-externally-managed --compile --install-headers 'd:\\tensorflow object detection\\tfodcourse\\tfod\\include\\site\\python3.9\\h5py' Check the logs for full command output.\r\nWARNING: Ignoring invalid distribution -ip (d:\\tensorflow object detection\\tfodcourse\\tfod\\lib\\site-packages)\r\nWARNING: Ignoring invalid distribution -ip (d:\\tensorflow object detection\\tfodcourse\\tfod\\lib\\site-packages)\r\n\r\n\r\n**Would be awesome if someone can help me! :)** ", "I am facing the same issue. Is there any solution came out to fix this bug:\r\n\r\n` File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\", line 1424, in load_weights\r\n    saving.load_weights_from_hdf5_group(f, self.layers)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 711, in load_weights_from_hdf5_group\r\n    original_keras_version = f.attrs['keras_version'].decode('utf8')\r\nAttributeError: 'str' object has no attribute 'decode'\r\npython-BaseException`", "Wait, why is this closed? I'm certainly still having the problem. Reverting to an earlier version of h5py is not a solution, it's a work-around.", "TF 2.1.x is no longer supported. TF 2.4.x was supposed to have this fixed (but also is no longer updated). Any version of TF above 2.4.0 should not have this issue. If there is a similar issue, please open a new issue.\r\n\r\nEdit: locking to prevent notifications being sent to everyone that followed this issue over its lifetime and are no longer affected"]}, {"number": 44466, "title": "Add idempotent based folding to TF", "body": "This trait label will allow idempotent optimizations (i.e. multiples copies of op folded to one copy) to run automatically from the MLIR side", "comments": ["@andyly  @smit-hinsu ", "@ahmedsabie Can you please check @andyly's comments and keep us posted ? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@ahmedsabie Any update on this PR? Please. Thanks!"]}, {"number": 44465, "title": "\"training\" argument change after model is defined", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n\r\nSay I define two models.\r\n\r\nmodel1\r\n\r\n```\r\ninput_shape = (224, 224, 3)\r\ninputs = keras.Input(shape=input_shape)\r\nx = tf.keras.layers.experimental.preprocessing.RandomRotation(0.5)(inputs, training = True)\r\nx = tf.keras.layers.experimental.preprocessing.Rescaling(1./255.)(x)\r\nx = tf.keras.layers.Convolution2D(32,(3,3))(x)\r\nx = tf.keras.layers.MaxPool2D((2,2))(x)\r\nx = tf.keras.layers.Flatten()(x)\r\nout = tf.keras.layers.Dense(2,activation = 'softmax')(x)\r\nmodel1 = keras.Model(inputs, out)\r\n```\r\nmodel2\r\n```\r\ninput_shape = (224, 224, 3)\r\ninputs = keras.Input(shape=input_shape)\r\nx = tf.keras.layers.experimental.preprocessing.RandomRotation(0.5)(inputs, training = False)\r\nx = tf.keras.layers.experimental.preprocessing.Rescaling(1./255.)(x)\r\nx = tf.keras.layers.Convolution2D(32,(3,3))(x)\r\nx = tf.keras.layers.MaxPool2D((2,2))(x)\r\nx = tf.keras.layers.Flatten()(x)\r\nout = tf.keras.layers.Dense(2,activation = 'softmax')(x)\r\nmodel2= keras.Model(inputs, out)\r\n```\r\n\r\nWhere the only difference is that in model2 the preprocessing augmentation layer has \"training\" set to False. There have been many instances where I simply want to change this parameter to False/True, after the model was defined, and I can find no way to do this besides creating a new model. First time I encountered this issue was in fine tuning, but more recently when trying to generate gradcam explanations. I imagine this has countless other occurrences, and might also happen with layers like BatchNorm.\r\n\r\nIs there already a way to switch the training state of these types of layers? (Note I don't mean the layer.trainable attribute).\r\nIf not, could this be implemented? It would be very useful.", "comments": ["@palatos Do want to have a different behavior than `K.learning_phase()`?", "@bhack I think if I use set_learning_phase(1) for instance, model2 would still have 'training = False' (I just ran a test and it seems to indicate this). Is there any way to change model2's 'training' parameter to 'True' after the model was defined?", "I meant that if you don't set the argument the `call` will be in sync with the learning phase \r\nhttps://github.com/tensorflow/tensorflow/blob/a87725285625c151055aaf7e8bab10d0c50f410a/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py#L228-L229", "My problem is I'm using a model pretrained by someone else, and I believe they had set this parameter when generating the model. Once I load their model can I override whatever they set for training by using this learning_phase idea?", "> Once I load their model can I override whatever they set for training by using this learning_phase idea?\r\n\r\nIn your case I don't think you will override with  learning_phase", "Yeah I did a mock test here and it seems like indeed I can't. If someone set training = 'False' and saved the model it seems like that's it and it cannot be changed anymore.\r\nWhat I mean is, would it be hard to implement it so that this can be as easily changed as say, layer.trainable? Would that break something major?", "Do you mean something similar to https://github.com/tensorflow/tensorflow/issues/39036?", "Indeed, that user seems to have the same need as me. The solution he proposes would fix my problem too.", "I think you can close this ticket and upvote that one.", "Ok, will do.", "@bhack have you tried saving the weights instead of the Model object? \r\n\r\nLike this example (I changed it to BatchNormalization cause of different version of TF but I assume it still applies):\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput_shape = (224, 224, 3)\r\ninputs = tf.keras.Input(shape=input_shape)\r\nx = tf.keras.layers.BatchNormalization()(inputs, training=True)\r\nx = tf.keras.layers.Convolution2D(32,(3,3))(x)\r\nx = tf.keras.layers.MaxPool2D((2,2))(x)\r\nx = tf.keras.layers.Flatten()(x)\r\nout = tf.keras.layers.Dense(2,activation = 'softmax')(x)\r\nmodel1 = tf.keras.Model(inputs, out)\r\n\r\ninput_shape = (224, 224, 3)\r\ninputs = tf.keras.Input(shape=input_shape)\r\nx = tf.keras.layers.BatchNormalization()(inputs, training=False)\r\nx = tf.keras.layers.Convolution2D(32,(3,3))(x)\r\nx = tf.keras.layers.MaxPool2D((2,2))(x)\r\nx = tf.keras.layers.Flatten()(x)\r\nout = tf.keras.layers.Dense(2,activation = 'softmax')(x)\r\nmodel2 = tf.keras.Model(inputs, out)\r\n\r\n# have different weights\r\n[print(np.all((a==b)), a.shape, b.shape) for a, b in zip(model1.weights, model2.weights)]\r\n\r\nmodel1.save_weights(\"temp.json\")\r\nmodel2.load_weights(\"temp.json\")\r\n\r\n# have the same weights\r\n[print(np.all((a==b)), a.shape, b.shape) for a, b in zip(model1.weights, model2.weights)]\r\n\r\n# the respective calls that will be invoked on the models\r\n# model1.layers[1]._get_training_value(training=True)\r\n# model2.layers[1]._get_training_value(training=False)\r\n```\r\n\r\nYou will have your pre-trained weights in a different (but equivalent) model where you can control the __training__ parameter.  I understand this is not a solution to changing the __training__ parameter of a model (your question), but rather a workaround."]}, {"number": 44464, "title": "Move second half of prepare tf pass to legalize tf", "body": "These patterns are more related to the legalization aspect rather than preparation aspect of the TFL pipeline hence they are moved\r\n\r\n", "comments": ["@smit-hinsu ", "@ahmedsabie  Can you please resolve conflicts? Thanks!", "@gbaned \r\nThere are some build failures on this PR and Ahmed is no longer working on this so we can ignore this."]}, {"number": 44461, "title": "cannot fin symbol method setUseNNAPI(boolean)", "body": "Hi!\r\nI'm trying to run a model i have created on tensorflow lite for android and i got this  error\r\n![Captura de Tela 2020-10-30 a\u0300s 10 59 16](https://user-images.githubusercontent.com/2678092/97714896-7ed65f80-1aa0-11eb-91ce-3ade0e9d5c4b.png)\r\n\r\nHow can i solve this?\r\n", "comments": ["Hi, \r\nI encountered the same problem while trying to run my model.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 44460, "title": "[RNN] TfLite conversion issue in LSTM models", "body": "I have not found any pretrained lstm models to work with . \r\nDid tfLite provided any pretrained lstm models ? \r\nI tried to create tflite model but facing issues while conversion .\r\nCould you provide exact script to create tfLite model ? \r\nDoes tfLite has any script for creating tfLite LSTM models with latest version ?\r\nThis is my script to create tfLite model. But it has few issues \r\n\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\n\r\nmodel = tf.keras.Sequential()\r\n# Add an Embedding layer expecting input vocab of size 1000, and\r\n# output embedding dimension of size 64.\r\nmodel.add(tf.keras.layers.Embedding(input_dim=1000, output_dim=64))\r\n\r\n# Add a LSTM layer with 128 internal units.\r\nmodel.add(tf.keras.layers.LSTM(128))\r\n\r\n# Add a Dense layer with 10 units.\r\nmodel.add(tf.keras.layers.Dense(10))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\r\nmodel.summary()\r\n#model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\r\n#                        validation_data=valid_data_generator.generate(),\r\n#                        validation_steps=len(valid_data)//(batch_size*num_steps), callbacks=[checkpointer])\r\ntf.saved_model.save(model, \"saved_model_keras_dir\")\r\n\r\nmodel.save('my_lstm_model')\r\n# x_train = \r\n#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n#x_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n# Cast x_train & x_test to float32.\r\n#x_train = x_train.astype(np.float32)\r\n#x_test = x_test.astype(np.float32)\r\n\r\n#model.fit(x_train, y_train, epochs=5)\r\n#model.evaluate(x_test, y_test)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n#tflite_model = converter.convert()\r\n\r\n# Step 3: Convert the Keras model to TensorFlow Lite model.\r\nsess = tf.compat.v1.keras.backend.get_session()\r\ninput_tensor = sess.graph.get_tensor_by_name('embedding_1:0')\r\noutput_tensor = sess.graph.get_tensor_by_name('dense_1:0')\r\nconverter = tf.lite.TFLiteConverter.from_session(\r\n    sess, [input_tensor], [output_tensor])\r\n\r\ntflite = converter.convert()\r\nprint('Model converted successfully!')\r\n\r\n\r\n# Save the model.\r\nwith open('lstmmodel.tflite', 'wb') as f:\r\n  f.write(tflite_model) \r\n\r\n\r\n\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f89a67cbef070e7ee00280229af561ce/44460.ipynb#scrollTo=Q3Sv5ONa4z8y). Thanks!", "@pranathibl Can you please check [this example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb) and update your code to use with `tf-nightly`. Another very similar example is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb). Thanks!\r\n\r\nIf you face any bug/performance related issue, please post them here. If there is any general support question, post it in Stackoverflow where there is a large community is there to help. Thanks! ", "I am closing this issue as this was resolved. Please feel free to open a new issue if you face any bug/performance related issue. Thanks!", "How to pass the seq length. Is there any option to pass variable seq length. If seq length has to be passed, what is the workflow ?"]}, {"number": 44459, "title": "Freeze / crash occurring when batch size is reduced", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 Home 1909**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.3.1**\r\n- Python version: **3.7.5**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **10.1 / cudnn-10.1-windows10-x64-v7.6.5.32**\r\n- GPU model and memory: **GeForce GTX 1660 Ti computeCapability: 7.5, coreClock: 1.455GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s**\r\n\r\n**Describe the current behavior**\r\nWhen running with large batch sizes in `model.fit()`, code completes successfully. With smaller batch sizes, the training crashes/freezes in the middle of an epoch.\r\n\r\n**Describe the expected behavior**\r\nSmaller batch sizes do not cause TF to hang or crash.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n# -*- coding: utf-8 -*-\r\n\"\"\"Reproduce crash during fit\"\"\"\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.models import Sequential\r\n\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(9, activation='relu', input_dim=125))\r\nmodel.add(Dense(31, activation='softmax'))\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer='adam',\r\n               metrics=['accuracy'])\r\n\r\nX_train=np.random.rand(3225, 125)\r\ny_train=np.random.rand(3225, 31)\r\n\r\n# This works\r\nmodel.fit(X_train[:,:], y_train[:,:], \r\n          epochs=100, \r\n          batch_size=X_train.shape[0], \r\n          verbose=1)\r\n\r\n# This crashes\r\nmodel.fit(X_train[:,:], y_train[:,:], \r\n          epochs=100, \r\n          batch_size=100, \r\n          verbose=1)\r\n```\r\n\r\nThe exact point it freezes seems to vary based on the random seed, here is an example. The shell (`cmd.exe`) is frozen and must be killed through task manager.\r\n\r\n```\r\nEpoch 11/100\r\n33/33 [==============================] - 1s 18ms/step - loss: 607.7234 - accuracy: 0.0332\r\nEpoch 12/100\r\n24/33 [====================>.........] - ETA: 0s - loss: 645.0050 - accuracy: 0.0288\r\n```", "comments": ["@drjubbs \r\nI ran the code shared and do not see the issue reported on tf 2.3 or nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/fc06f34a494a4c162fc3a9527cb53f63/untitled457.ipynb),[ nightly](https://colab.research.google.com/gist/Saduf2019/0f09432a9a5f2a5c4826a86d388253bd/untitled458.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "FYI - I was using Nvidia \"Game Ready\" drivers (441.22), a newer one (457.09) was released, I upgraded, and I no longer get the crash. Not sure if you test against the \"Game Ready\" versions but this may have been the cause?", "@drjubbs \r\nAs you as see this is not a bug because of tensorflow and i was able to run your code without any crashing, as the issue is resolved please move this closed status.", "Moving to closed status as resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44459\">No</a>\n"]}, {"number": 44458, "title": "Fix linking error of Flex delegate for iOS", "body": "This cl fixes three different issues about Flex delegate on iOS:\r\n1. Duplicated symbols of C API exported by TensorFlowLiteC and\r\n   TensorFlowLiteSelectTfOps. This is fixed by using avoid_deps.\r\n2. Undefined symbol of uprv::getICUData::conversion\r\n3. TensorFlowLiteC do not export the weak symbol AcquireFlexDelegate.\r\n   This cl replaces the use of weak symbol by LoadLibrary::GetSymbol.\r\n\r\nPiperOrigin-RevId: 339798211\r\nChange-Id: I2e15c008a48b9638568c1a16b3c3b7bcc61e448f", "comments": []}, {"number": 44457, "title": "Speedup the TFLu Renode tests", "body": "This is a proposed way to reduce the execution time of TFLu Renode tests by generating all tests cases in Robot.\r\n**The changes are applied only to the Bluepill target related files and are not yet integrated with `make test`.**\r\nMain changes:\r\n* `test_bluepill_binary.sh` now takes directory containing test binaries as argument,\r\n* `.robot` runs test on all files found in given directory,\r\n* console output was changed to suit the new approach.\r\n\r\nRunning the `tensorflow/lite/micro/testing/test_bluepill_binary.sh  tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/` will ouput:\r\n* In case of PASS:\r\n```\r\nPreparing suites\r\nStarted Renode instance on port 9999; pid 62470\r\nStarting suites\r\nRunning /home/jakub/Dokumenty/renode-tensorflow/tensorflow/lite/micro/testing/bluepill.robot\r\n+++++ Starting test 'bluepill.Should Run All Bluepill Tests'\r\n\tdetection_responder_test - PASSED\r\n\tdetection_responder_test_int8 - PASSED\r\n\tgreedy_memory_planner_test - PASSED\r\n\thello_world_test - PASSED\r\n        [...]\r\n\tsimple_memory_allocator_test - PASSED\r\n\ttesting_helpers_test - PASSED\r\n+++++ Finished test 'bluepill.Should Run All Bluepill Tests' in 34.06 seconds with status OK\r\nCleaning up suites\r\nClosing Renode pid 62470\r\nAggregating all robot results\r\nOutput:  /tmp/test_bluepill_binary/robot_output.xml\r\nLog:     /tmp/test_bluepill_binary/log.html\r\nReport:  /tmp/test_bluepill_binary/report.html\r\nTests finished successfully :)\r\nPASS\r\n```\r\n* In case of FAIL:\r\n```\r\nPreparing suites\r\nStarted Renode instance on port 9999; pid 83495\r\nStarting suites\r\nRunning /home/jakub/Dokumenty/renode-tensorflow/tensorflow/lite/micro/testing/bluepill.robot\r\n+++++ Starting test 'bluepill.Should Run All Bluepill Tests'\r\n\tdetection_responder_test - PASSED\r\n\tdetection_responder_test_int8 - PASSED\r\n\tgreedy_memory_planner_test - PASSED\r\n\thello_world_test - PASSED\r\n        [...]\r\n\tkernel_elementwise_test - PASSED\r\n+++++ Finished test 'bluepill.Should Run All Bluepill Tests' in 9.14 seconds with status failed\r\n      \u2554\u2550\r\n      \u2551 kernel_floor_test - FAILED\r\n      \u255a\u2550\r\nCleaning up suites\r\nClosing Renode pid 83495\r\nAggregating all robot results\r\nOutput:  /tmp/test_bluepill_binary/robot_output.xml\r\nLog:     /tmp/test_bluepill_binary/log.html\r\nReport:  /tmp/test_bluepill_binary/report.html\r\nSome tests failed :( See logs for details!\r\nUART LOGS:\r\n[...]\r\n~~~SOME TESTS FAILED~~~\r\n```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @mansnils ", "Thanks @JakubJatczak. I think we're headed in the right direction.\r\n\r\nCouple of high level feedback:\r\n * This doesn't have to be part of this pull request but prior to turning this on as part of CI, we will need to be able to selectively turn off running certain tests, if needed.\r\n * In your sample output, I don't see the logs from the failed test. Was this intentional? We will want to see the logs for all the failed tests.\r\n * If more than one test fails, will we run through all of them regardless? That would be preferred so that we can see all the failing tests in one go.\r\n * The format of the .robot file is pretty opaque to me (though I haven't really looked at the Renode docs either). A description of what this test suite is doing and some pointers into the renode docs from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/docs/renode.md) would be very useful in making this easier to maintain in the future.\r\n", "> * This doesn't have to be part of this pull request but prior to turning this on as part of CI, we will need to be able to selectively turn off running certain tests, if needed.\r\n\r\nOn second thought, I take this back. With the current makefile system we can exclude a test from being built for a particular target. That is sufficient for now.", "> In your sample output, I don't see the logs from the failed test. Was this intentional? We will want to see the logs for all the failed tests.\r\n\r\nI omitted UART logs to reduce length of the example.  Please see the full fail (forced by manually changing golden values) output example below:\r\n``` \r\n$> tensorflow/lite/micro/testing/test_bluepill_binary.sh tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin '~~~ALL TESTS PASSED~~~'\r\n\r\nPreparing suites\r\nStarted Renode instance on port 9999; pid 53718\r\nStarting suites\r\nRunning /home/jakub/Dokumenty/renode-tensorflow/tensorflow/lite/micro/testing/bluepill.robot\r\n+++++ Starting test 'bluepill.Should Run All Bluepill Tests'\r\n\tdetection_responder_test - PASSED\r\n\tdetection_responder_test_int8 - PASSED\r\n\tgreedy_memory_planner_test - PASSED\r\n\thello_world_test - PASSED\r\n\timage_provider_test - PASSED\r\n\timage_provider_test_int8 - PASSED\r\n\tkernel_activations_test - PASSED\r\n+++++ Finished test 'bluepill.Should Run All Bluepill Tests' in 10.63 seconds with status failed\r\n      \u2554\u2550\r\n      \u2551 kernel_add_test - FAILED\r\n      \u255a\u2550\r\nCleaning up suites\r\nClosing Renode pid 53718\r\nAggregating all robot results\r\nOutput:  /tmp/test_bluepill_binary/robot_output.xml\r\nLog:     /tmp/test_bluepill_binary/log.html\r\nReport:  /tmp/test_bluepill_binary/report.html\r\nSome tests failed :( See logs for details!\r\nUART LOGS:\r\nTesting FloatAddNoActivation\r\ngolden[i] (-1.0999994*2^2) near output[i] (-1.8999992*2^0) failed at tensorflow/lite/micro/kernels/add_test.cc:87\r\ngolden[i] (1.0999994*2^2) near output[i] (1.5999993*2^-2) failed at tensorflow/lite/micro/kernels/add_test.cc:87\r\ngolden[i] (1.0999994*2^2) near output[i] (1.0*2^0) failed at tensorflow/lite/micro/kernels/add_test.cc:87\r\ngolden[i] (1.0999994*2^2) near output[i] (1.2999993*2^0) failed at tensorflow/lite/micro/kernels/add_test.cc:87\r\nTesting FloatAddActivationRelu1\r\nTesting FloatAddVariousInputShapes\r\nTesting FloatAddWithScalarBroadcast\r\nTesting QuantizedAddNoActivationUint8\r\nTesting QuantizedAddNoActivationInt8\r\nTesting QuantizedAddActivationRelu1Uint8\r\nTesting QuantizedAddActivationRelu1Int8\r\nTesting QuantizedAddVariousInputShapesUint8\r\nTesting QuantizedAddVariousInputShapesInt8\r\nTesting QuantizedAddWithScalarBroadcastUint8\r\nTesting QuantizedAddWithScalarBroadcastFloat\r\nTesting QuantizedAddWithScalarBroadcastInt8\r\nTesting QuantizedAddWithMixedBroadcastUint8\r\nTesting QuantizedAddWithMixedBroadcastInt8\r\n14/15 tests passed\r\n~~~SOME TESTS FAILED~~~\r\n```\r\n> If more than one test fails, will we run through all of them regardless? That would be preferred so that we can see all the failing tests in one go.\r\n\r\nIt will not - just like in the current approach, the tests will stop after first failure. It is possible to achieve what you ask for, but this will require significant changes to the approach. \r\n\r\n> The format of the .robot file is pretty opaque to me (though I haven't really looked at the Renode docs either). A description of what this test suite is doing and some pointers into the renode docs from here would be very useful in making this easier to maintain in the future.\r\n\r\nI added `Documentation` section to custom `Keywords` to make it a little bit cleaner, but I understand that it doesn't solve the problem. Do you think that adding single link to the [testing section of the docs](https://renode.readthedocs.io/en/latest/introduction/testing.html) will be enough or should we add something about the test suite flow? "]}, {"number": 44456, "title": "TFLu: Fix Renode test of CMSIS-NN conv kernel", "body": "This is partly addressing: https://github.com/tensorflow/tensorflow/issues/44455", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44455, "title": "Renode tests of CMSIS-NN kernels tests are not working for all", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source): 5b5960f4fda6a6ae9cbf5233873b9ea6910b3e4e\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nCMSIS-NN kernel tests conv and softmax are not working.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nmake -j4 -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=stm32f4  test_kernel_conv_test\r\nmake -j4 -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=stm32f4  test_kernel_softmax_test\r\n", "comments": ["Can be closed now", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44455\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44455\">No</a>\n"]}, {"number": 44454, "title": "Error while running TF Lite Model Maker Text Classification Tutorial", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/lite/tutorials/model_maker_text_classification#end-to-end_workflow\r\n\r\n## Description of issue (what needs changing): \r\nThe below code is resulting in error\r\n\r\n```python\r\nmodel = text_classifier.create(train_data, model_spec=spec)\r\n```\r\n\r\nError Trace is shown below:\r\n\r\n```python\r\nINFO:tensorflow:Retraining the models...\r\nINFO:tensorflow:Retraining the models...\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-7-89009ac171ed> in <module>()\r\n----> 1 model = text_classifier.create(train_data, model_spec=spec)\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_examples/lite/model_maker/core/task/text_classifier.py in train(self, train_data, validation_data, epochs, batch_size)\r\n    132     \"\"\"Feeds the training data for training.\"\"\"\r\n    133     if batch_size is None:\r\n--> 134       batch_size = self.model_spec.default_batch_size\r\n    135 \r\n    136     if train_data.size < batch_size:\r\n\r\nAttributeError: 'BertClassifierModelSpec' object has no attribute 'default_batch_size'\r\n```\r\n\r\n### Clear description:\r\n\r\nThis tutorial is useful to customize the `TF Lite Text Classification` project with our Dataset.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?  : NA\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? : NA\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : NA\r\n\r\n### Raises listed and defined : NA\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example? : NA\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content? : NA\r\n\r\n### Submit a pull request? : No\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1ad549b981d453f0fb0f7a75b065390e/44454.ipynb). Thanks!", "Thanks for catching this issue. As a workaround you can pass `batch_size` argument to proceed training.\r\n```python\r\n#Step 3. Customize the TensorFlow model.\r\nmodel = text_classifier.create(train_data, model_spec=spec, batch_size=48)\r\n```", "@ziyeqinghan ", "We have fixed it. Please try again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44454\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44454\">No</a>\n"]}, {"number": 44453, "title": "BrokenPipeError: [Errno 32] Broken pipe", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12.3\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\n- CUDA/cuDNN version:cuda 9.0.176/cudnn 7\r\n- GPU model and memory:Tesla K80/11GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I was training resnet50 model with some epochs,especially when the classification is reaching below 0.09, the following error appears. \r\n![1](https://user-images.githubusercontent.com/49138243/97657415-be9f4600-1aa4-11eb-8cfb-579b1a27427d.PNG)\r\n![2](https://user-images.githubusercontent.com/49138243/97657426-c65eea80-1aa4-11eb-8814-a38052035dcb.PNG)\r\n\r\n**Describe the expected behavior**\r\nWhat's the error reason and how to solve this problem?\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@philp123 \r\nAs youhave mentioned you are using tf version 1.12 and 1.x is not supported, please upgrade to 2.x and let us know if you face any issues.", "> @philp123\r\n> As youhave mentioned you are using tf version 1.12 and 1.x is not supported, please upgrade to 2.x and let us know if you face any issues.\r\n\r\nThe resource for tf is on HPC, I have no right to update to 2.x. Also what's the meaning of \"1.x is not supported\", thxs!", "@philp123 \r\nPlease provide with simple stand alone code or if possible share a colab gist with error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44453\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44453\">No</a>\n"]}, {"number": 44452, "title": "Removes `SetIsStateful` from op StatelessRandomGetKeyCounterAlg", "body": "PiperOrigin-RevId: 339147256\nChange-Id: I4444b47e9d974bd5bb643b80ae55fcbc347653df", "comments": ["Otherwise tf.random.stateless_XXX functions can be broken when used in tf.data pipelines. ", "> I do think this will cause issues / require more special-casing in optimizations without the stateful bit. DTensor has already run into it for the non-stateful ops with the device selector fused in.\r\n> \r\n> Maybe rather than omitting the stateful bit for this op we could check if there's a statically-known device assignment and if so not include it at all? Presumably tf.data pipelines would be on the CPU.\r\n> \r\n> But if something's broken now feel free to check this in. We should IMO add tf.data tests and put back the stateful bit later, though.\r\n\r\nMy goal for StatelessRandomGetKeyCounterAlg is to be a compatibility shim to reproduce all tf.random.stateless_XXX's behaviors, without attempt to fix any of them. Hopefully in the future all those problematic behaviors along with StatelessRandomGetKeyCounterAlg will be deprecated, leaving StatelessRandomNormalV2 etc as the sole ideal op.\r\n\r\nConditionally trimming StatelessRandomGetKeyCounterAlg sounds interesting, but it'll take some time. The SetIsStateful bit is already breaking one internal project, so I fear it'll break more OSS code.", "This cherrypick is not needed because the V2 stateless RNG ops are not activated in branch 2.4 (`_FORWARD_COMPATIBILITY_HORIZON` is [frozen](https://github.com/tensorflow/tensorflow/blob/d2dcde5017c5e16541c3d2fdb487ad124cd96be7/tensorflow/python/compat/compat.py#L36) to be the branch cut date, which is before the V2 ops' kick-in [date](https://github.com/tensorflow/tensorflow/blob/d2dcde5017c5e16541c3d2fdb487ad124cd96be7/tensorflow/python/ops/stateless_random_ops.py#L209))."]}, {"number": 44451, "title": "testing to see if CI results are visible.", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\"TFLite Micro\"-check. Nice! "]}, {"number": 44450, "title": "Update CODE_OF_CONDUCT.md", "body": "Added one point each in Examples of behaviour that contributes to creating a positive environment & Examples of behaviour that contributes to creating a negative environment", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44450) for more info**.\n\n<!-- need_sender_cla -->", "@pratamjain  Can you please sign CLA. Thanks!", " @googlebot I signed it\n\nOn Fri 30 Oct, 2020, 3:42 AM google-cla[bot], <notifications@github.com>\nwrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here with @googlebot\n> I signed it! and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44450>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/44450#issuecomment-719053527>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APLR6HS2WRCM4IW5CYKNX6LSNHSDZANCNFSM4TEJSHXA>\n> .\n>\n", "Hi. \r\n\r\nThanks for taking the time to make a PR. But this is a standard \"code of conduct\" file used across hundreds of projects. I don' think it needs these updates."]}, {"number": 44449, "title": "Printing on TPUs", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like to be able to print on TPUs. Currently printing on TPUs in functions decorated with tf.function does not work (tf.print doesn't work). \r\n\r\n**Will this change the current api? How?**\r\nI don't think the external api would need to be affected, as tf.print should work. \r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to debug their code running on TPUs easily!\r\n\r\n**Any Other info.**\r\nThis issue https://github.com/tensorflow/tensorflow/issues/43705 raised the same concern, but the solution provided doesn't work. I've provided a minimal example here: \r\nhttps://colab.research.google.com/drive/1kFdR9tpj7zJjwHIbWRVjVSiA9zwviErK?usp=sharing\r\n\r\nI wasn't sure where the config statement should be, so I tried placing it the three obvious scopes (the tf.function, normal python, and the TPU strategy) to no avail.", "comments": ["I'd like to say that I am also encountering this issue, and would appreciate clarification!", "Hello David, \r\n\r\nThank for requesting this feature. tf.print unfortunately is unsupported on TPU's currently as TPU runs on separate cloud cluster than the python program. \r\n\r\nWe'll be sure to update/highlight this if this feature is added to TPU's, however. :) \r\n"]}, {"number": 44448, "title": "Possible memory leak in tf.keras.layers.experimental.preprocessing.TextVectorization", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: Python 3.6.10 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NVIDIA Driver 450.51.05\r\n- GPU model and memory: Could not be determined (Deep Learning AMI (Ubuntu 18.04) Version 35.0 - ami-01aad86525617098d ; g3.8xlarge)\r\n\r\n**Describe the current behavior**\r\n\r\nA minimal TextVectorization builder works fine when run for the first time in a Jupyter notebook kernel. When the same minimal code is run in a parallel kernel, the below error is produced. The Jupyter notebook often needs to be restarted entirely (not just a kernel shutdown) in order for TextVectorization to work again.\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected to work properly across multiple notebook instances without need for kernel shutdown or Jupyter notebook restart. I have tried `tf.keras.backend.clear_session()` and `gc.collect()` before and after calls to `TextVectorization` but this has not worked.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nnum_words = 100000\r\ntag_processor = tf.keras.layers.experimental.preprocessing.TextVectorization(output_mode='int',\r\n                                                                              max_tokens=num_words,\r\n                                                                              name='tag_processor',\r\n                                                                              standardize=None)\r\ntag_processor.adapt(data['keywords_processed'].tolist())\r\nvectorized_text = tag_processor(data['keywords_processed'].tolist()[:100])\r\nprint(vectorized_text)\r\n\r\n\r\n# Example of data['keywords_processed'].tolist(): ['12 remove in disable\\\\how iphoneipad 14 disabled fix apple disable ios 2020 to how icloud iphone se',\r\n# 'uruguayo san gana se york estados cuesta new en vs 2020 cuanto ca espa\u00f1ol nueva woker neuva california francisco unidos vivir',\r\n# 'quero homem masculina namorado relacionamentos vono luiza comoconquistarumhomem que coach mente os serio pensam de pense gostam como do querem oqueoshomensquerem eles homens um meta buscam crush relacionamento coaching amor conquistar o',\r\n# 'is hoa back t\u00e0u twin and runs sun h\u1ecfa double through tunnel rail beautiful goes meeting watch 2020 red disappeared black tau beautifully cross station train of the appeared rushed trains to monsters nice',\r\n# '\u179f\u1798\u17b8\u1780\u17b6\u179a\u179a\u1784\u17d2\u179c\u1784\u17cb exercise for \u179f\u17b7\u1780\u17d2\u179f\u17b6\u17a2\u1793\u17bb\u1782\u1798\u1793\u17cf \u178a\u17c1\u179a\u17b8\u179c\u17c1\\u200b and \u179b\u17c6\u17a0\u17b6\u178f\u17cb\u179f\u17b7\u179f\u17d2\u179f\u1796\u17bc\u1780\u17c2\u1782\u178e\u17b7\u178f\u179c\u17b7\u1791\u17b6 \u179b\u17c6\u17a0\u17b6\u178f\u17cb\u1794\u17d2\u179a\u178f\u17b7\u1794\u178f\u17d2\u178f\u17b7 \u17a2\u17b6\u17c6\u1784\u178f\u17c1\u1780\u17d2\u179a\u17b6\u179b intergral grade \u179c\u17b7\u1789\u17d2\u1789\u17b6\u179f\u17b6\u1794\u17b6\u1780\u17cb\u178c\u17bb\u1794 \u179b\u17b8\u1798\u17b8\u178f\\u200b\\u200b\\u200b\\u200b studying \u1794\u1798\u17d2\u179b\u17c2\u1784\u17a1\u17b6\u1794\u17d2\u179b\u17b6\u179f \u179c\u17b7\u1789\u17d2\u1789\u17b6\u179f\u17b6\u179f\u17b7\u179f\u17d2\u179f\u1796\u17bc\u1780\u17c2\u1791\u17bc\u1791\u17b6\u17c6\u1784\u1794\u17d2\u179a\u1791\u17c1\u179f calculate math limite learning imo \u1785\u17c6\u1793\u17bd\u1793\u17a2\u179f\u1793\u17b7\u1791\u17b6\u1793 answers brain \u179f\u17d2\u1790\u17b7\u178f\u17b7 integral mathematics seamo algebra study of how tips \u178f\u17d2\u179a\u17b8\u1780\u17c4\u178e\u1798\u17b6\u178f\u17d2\u179a \u179f\u17b7\u179f\u17d2\u179f\u1796\u17bc\u1780\u17c2\u1782\u178e\u17b7\u178f\u179c\u17b7\u1791\u17d2\u1799\u17b6 \u179f\u1798\u17b8\u1780\u17b6\u179a\u178c\u17b8\u1795\u17c1\u179a\u17c9\u1784\u17cb\u179f\u17d2\u1799\u17c2\u179b 9 to',\r\n# '2 depress\u00e3o jesus da dia ora\u00e7\u00e3o hoje liberta tarde madrugada 91 de edifica nosso barros cura deus salmo palavra do pai dulcineia noite liberta\u00e7\u00e3o reis que',\r\n# ...\r\n# ]\r\n#\r\n# len(data.index) == 3907552\r\n\r\n# Rest of code cannot be shared, but above example provides size of data frame and format of input to TextVectorization\r\n```\r\n\r\n**Other info / logs** \r\n\r\njupyter core     : 4.6.1\r\njupyter-notebook : 6.0.3\r\nqtconsole        : 4.6.0\r\nipython          : 7.12.0\r\nipykernel        : 5.1.4\r\njupyter client   : 5.3.4\r\njupyter lab      : 1.2.6\r\nnbconvert        : 5.6.1\r\nipywidgets       : 7.5.1\r\nnbformat         : 5.0.4\r\ntraitlets        : 4.3.3\r\n\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-21-cfaee3ea4157> in <module>\r\n      5                                                                               name='tag_processor',\r\n      6                                                                               standardize=None)\r\n----> 7 tag_processor.adapt(data['keywords_processed'].tolist())\r\n      8 vectorized_text = tag_processor(data['keywords_processed'].tolist()[:100])\r\n      9 print(vectorized_text)\r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py in adapt(self, data, reset_state)\r\n    373         data = array_ops.expand_dims(data, axis=-1)\r\n    374       self.build(data.shape)\r\n--> 375       preprocessed_inputs = self._preprocess(data)\r\n    376     elif isinstance(data, dataset_ops.DatasetV2):\r\n    377       # TODO(momernick): Replace this with a more V2-friendly API.\r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py in _preprocess(self, inputs)\r\n    546         # This treats multiple whitespaces as one whitespace, and strips leading\r\n    547         # and trailing whitespace.\r\n--> 548         inputs = ragged_string_ops.string_split_v2(inputs)\r\n    549       elif callable(self._split):\r\n    550         inputs = self._split(inputs)\r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/ragged/ragged_string_ops.py in string_split_v2(input, sep, maxsplit, name)\r\n    522       return ragged_tensor.RaggedTensor.from_value_rowids(\r\n    523           values=sparse_result.values,\r\n--> 524           value_rowids=sparse_result.indices[:, 0],\r\n    525           nrows=sparse_result.dense_shape[0],\r\n    526           validate=False)\r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py in _slice_helper(tensor, slice_spec, var)\r\n   1022         ellipsis_mask=ellipsis_mask,\r\n   1023         var=var,\r\n-> 1024         name=name)\r\n   1025 \r\n   1026 \r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py in strided_slice(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\r\n   1194       ellipsis_mask=ellipsis_mask,\r\n   1195       new_axis_mask=new_axis_mask,\r\n-> 1196       shrink_axis_mask=shrink_axis_mask)\r\n   1197 \r\n   1198   parent_name = name\r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py in strided_slice(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\r\n  10318       return _result\r\n  10319     except _core._NotOkStatusException as e:\r\n> 10320       _ops.raise_from_not_ok_status(e, name)\r\n  10321     except _core._FallbackException:\r\n  10322       pass\r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6841   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6842   # pylint: disable=protected-access\r\n-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6844   # pylint: enable=protected-access\r\n   6845 \r\n\r\n~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run StridedSlice: Dst tensor is not initialized. [Op:StridedSlice] name: strided_slice/\r\n", "comments": ["Did u get that error resolved?\r\n", "Facing the same issue. ", "Heyy @sanilGh , I don't know if this will fix your error but I tried on this nvidia jetson nano and it worked...\r\nAt the top of the script write this before importing tensorflow..\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\n```", "@xanthan011 Nope that does not fix it.", "then there is a possibility that you laptop is going out of memory, while running this program..", "@mmitsui Can you please try new TF version like `tf-nighly` and let us know how it progresses. Also, Can you please share a simple standalone code to reproduce the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44448\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44448\">No</a>\n", "Going through the same issue.\r\nAccording to the tensorflow profile, I found that Standardize and Split Op both execute on CPU not GPU.\r\n\r\nI just wonder how can we put the above ops to GPU? Maybe that's the cause of memory leak?", "I am also having a similar issue. I was trying to adapt to 17M sentences and it was asking 835GB memory! ", "@raisularefin Please open a new issue with a simple standalone code to reproduce the issue. Thanks!"]}, {"number": 44446, "title": "Remove leading cmsis from the include paths.", "body": "This was not needed and was an artifact from how the original Arduino integration was done. In fact, having the leading cmsis/ was getting in the way of using a version of CMSIS that was not downloaded via the TFLM Makefiles.\r\n\r\nThis is progress towards addressing #44261\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Tagging @mansnils "]}, {"number": 44445, "title": "Revert \"[CherryPick r2.4]Disable test for macos.\"", "body": "Reverts tensorflow/tensorflow#44398\r\n\r\nTesting if this causes the memory issue (very unliekly)", "comments": ["Not the culprit, closing,"]}, {"number": 44444, "title": "Revert \"[Cherrypick r2.4] Fixed the markdown formatting of client.monitor and client.trace API documentation\"", "body": "Reverts tensorflow/tensorflow#44402\r\n\r\nTesting if this solves memory issue", "comments": ["Not the culprit, closing,"]}, {"number": 44443, "title": "Revert \"[Cherrypick r2.4] Delay start of profiler when configured\"", "body": "Reverts tensorflow/tensorflow#44411\r\n\r\nTemporarily reverting to test Windows memory issue", "comments": ["Closing as this is not the culprit."]}, {"number": 44442, "title": "Revert \"[CherryPick r2.4] [tf.data] Preserving accurate cardinality information for `group_by_window` transformation.\"", "body": "Reverts tensorflow/tensorflow#44401\r\n\r\nTemporarily reverting to test Windows memory issue", "comments": ["Closing as this is not the culprit."]}, {"number": 44441, "title": "Revert \"[Cherrypick] Force document private methods/functions\"", "body": "Reverts tensorflow/tensorflow#44236\r\n\r\nTemporarily reverting to test Windows memory issue", "comments": ["I rolled back all PRs since the last green and we're testing each branch now", "Closing as this is not the culprit."]}, {"number": 44439, "title": "Revert \"[Cherrypick r2.4] Save response on the client side for better backward compatibility.\"", "body": "Reverts tensorflow/tensorflow#44365\r\n\r\nTemporarily rolling back as it seems to be culprit for memory issues", "comments": ["Closing as this is not the culprit."]}, {"number": 44438, "title": "Tensorflow: TFLiteConverter (Saved Model -> TFLite) in SSD_mobilenet_v2_2", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):  pip install\r\n- TensorFlow version (or github SHA if from source): 2.3.1\r\n\r\nI have installed tensorflow 2.3.1, my ssd_mobile_net_v2_2 was downloaded from https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2. I want to convert this model to tf_lite version. My code is:\r\n![image](https://user-images.githubusercontent.com/43801775/97602599-76513b00-1a46-11eb-84ea-680d8b6180fd.png)\r\n\r\nBut error occurs:\r\n\r\ntensorflow.lite.python.convert.ConverterError: :0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types :0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x320x320x3x!tf.quint8>) -> tensor<1x320x320x3xui8>\r\n![image](https://user-images.githubusercontent.com/43801775/97602651-836e2a00-1a46-11eb-9764-cac65a8198ae.png)\r\n\r\nit seems I need to set the input-data-type, but I don't know how to do that.\r\n\r\nThanks a lot for your replying.\r\n\r\n\r\n", "comments": ["@ZuyongWu,\r\nI was able to reproduce the error with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/e9142a6f50f070cae36d30274459984b/44438.ipynb).\r\n\r\nHowever, looking [this comment](https://github.com/tensorflow/tensorflow/issues/42114#issuecomment-671593386) from a similar issue, I was able to convert the model with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/aa52e271dabd6ce4e05fc6d49a18c34a/44438-tf-nightly.ipynb). Please find the attached gist. Thanks!", "> @amahendrakar,\r\n> I was able to reproduce the error with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/e9142a6f50f070cae36d30274459984b/44438.ipynb).\r\n> \r\n> However, looking [this comment](https://github.com/tensorflow/tensorflow/issues/42114#issuecomment-671593386) from a similar issue, I was able to convert the model with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/aa52e271dabd6ce4e05fc6d49a18c34a/44438-tf-nightly.ipynb). Please find the attached gist. Thanks!\r\n\r\nHi, I have tried it in latest tf-nightly 2.5.0, however, the input_shape prints [1 1 1 3]\r\n![image](https://user-images.githubusercontent.com/43801775/97690800-6eda7200-1ad8-11eb-97c9-0751a642aa9a.png)\r\n\r\n", "@ZuyongWu,\r\nPlease check [this](https://colab.research.google.com/gist/amahendrakar/aa52e271dabd6ce4e05fc6d49a18c34a/44438-tf-nightly.ipynb#scrollTo=WOJZqlLw1S8P&line=1&uniqifier=1) updated gist which lists the files after the conversion. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Didn't work for me. Remains with [1 1 1 3] input size. :(\r\n\r\n> @ZuyongWu,\r\n> Please check [this](https://colab.research.google.com/gist/amahendrakar/aa52e271dabd6ce4e05fc6d49a18c34a/44438-tf-nightly.ipynb#scrollTo=WOJZqlLw1S8P&line=1&uniqifier=1) updated gist which lists the files after the conversion. Thanks!\r\n\r\n", "> Didn't work for me. Remains with [1 1 1 3] input size. :(\r\n\r\n@ricardoantonello\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "> > Didn't work for me. Remains with [1 1 1 3] input size. :(\r\n> \r\n> @ricardoantonello\r\n> Could you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n\r\nHi,\r\nI finally solve the issue using export_tflite_graph_tf2.py \r\nThanks!", "@ricardoantonello we are seeing the same problem. Could you explain how you solved your problem? For example, what script are you using, how did you run it, and what version of TensorFlow are you using? That would be much appreciated!", "@ricardoantonello Thanks! I've confirmed that using export_tflite_graph_tf2.py fixes the issue in TF 2.4.", "> @ricardoantonello we are seeing the same problem. Could you explain how you solved your problem? For example, what script are you using, how did you run it, and what version of TensorFlow are you using? That would be much appreciated!\r\n\r\nHi, i used de checkpoints i have trainning before, and the instructions here:\r\npython object_detection/export_tflite_graph_tf2.py \\\r\n    --pipeline_config_path path/to/ssd_model/pipeline.config \\\r\n    --trained_checkpoint_dir path/to/ssd_model/checkpoint \\\r\n    --output_directory path/to/exported_model_directory\r\n\r\npipeline_config_path is the same config file i used to trainning.\r\nMy version: TF 2.4 Gpu\r\n\r\nGood luck!"]}]