[{"number": 29055, "title": "Removed warning from the file.", "body": "", "comments": ["Internal review decided to not follow with the size_t changes only in a subset of all affected files\r\n\r\nClosing this but if you want to do it in all affected files that would be great."]}, {"number": 29054, "title": "mobile_ssd_v2_float_coco.tflite model :Cannot copy between a TensorFlowLite tensor with shape [1, 2034, 4] and a Java object with shape [1, 10, 4].", "body": "When using mobile_ssd_v2_float_coco.tflite model with Tensorflow Object Detection [example](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) code,  I am facing this error:\r\n\r\n` Process: org.tensorflow.lite.examples.detection, PID: 30765\r\n    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 2034, 4] and a Java object with shape [1, 10, 4].\r\n        at org.tensorflow.lite.Tensor.throwIfShapeIsIncompatible(Tensor.java:282)\r\n        at org.tensorflow.lite.Tensor.throwIfDataIsIncompatible(Tensor.java:249)\r\n        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:141)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:161)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:220)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:197)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)`\r\n\r\n- I did change the\r\n` `private static final int TF_OD_API_INPUT_SIZE = 320;`` \r\n - Followed this StackOverflow solution [https://stackoverflow.com/questions/54423649/tensorflow-lite-gpu-support-on-object-detector](url)\r\nThis gives only a partial soluton the problem. Can anyone suggest a proper solution. ", "comments": ["@Gmrevo Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@muddham Thanks for the suggestion.\r\nThe project  has been compiled in Android Studio using these dependencies:\r\n`dependencies {\r\n    implementation fileTree(dir: 'libs', include: ['*.jar','*.aar'])\r\n    implementation 'com.android.support:appcompat-v7:28.0.0'\r\n    implementation 'com.android.support:design:28.0.0'\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n}\r\n`\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device : Samsung A7: \r\n- TensorFlow installed from (source or binary):\r\n   Binary\r\n- TensorFlow version: \r\nused these in dependencies\r\nimplementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n**Defined input parameters to run ineference** `:\r\nd.tflite.setNumThreads(NUM_THREADS);\r\n    d.outputLocations = new float[1][NUM_DETECTIONS][4];\r\n    d.outputClasses = new float[1][NUM_DETECTIONS];\r\n    d.outputScores = new float[1][NUM_DETECTIONS];\r\n    d.numDetections = new float[1];`\r\n**Expected** `:\r\nd.tflite.setNumThreads(NUM_THREADS);\r\n    d.out1 = new float[1][2034][4];\r\n    d.out2 = new float[1][2034][91];\r\n    d.outputScores = new float[1][NUM_DETECTIONS];\r\n    d.numDetections = new float[1];`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nWhen using mobile_ssd_v2_float_coco.tflite model with Tensorflow Object Detection [example](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) code,  I am facing this error:\r\n\r\n` Process: org.tensorflow.lite.examples.detection, PID: 30765\r\n    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 2034, 4] and a Java object with shape [1, 10, 4].\r\n        at org.tensorflow.lite.Tensor.throwIfShapeIsIncompatible(Tensor.java:282)\r\n        at org.tensorflow.lite.Tensor.throwIfDataIsIncompatible(Tensor.java:249)\r\n        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:141)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:161)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:220)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:197)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)`\r\n\r\n- I did change the\r\n` `private static final int TF_OD_API_INPUT_SIZE = 320;`` \r\n - Followed this StackOverflow solution [https://stackoverflow.com/questions/54423649/tensorflow-lite-gpu-support-on-object-detector](url)\r\nThis gives only a partial solution to the problem. Can anyone please suggest a proper solution? \r\n\r\n**Code to reproduce the issue**\r\n[bugreport.tar.gz](https://github.com/tensorflow/tensorflow/files/3227256/bugreport.tar.gz)\r\nModel: mobile_ssd_v2_float_coco.tflite\r\nLabel: labels_mobilenet_quant_v1_224.txt\r\nJava: Classfier.java, TFLiteObjectDetectionAPIModel.java\r\n          \r\n", "So, these models aren't directly compatible. That is, the model included with the detection sample creates the following output tensors: locations, classes, scores, detections. The `mobile_ssd_v2_float_coco.tflite` model produces a 1x2034x4 encodings output tensor and a 1x2034x91 class predictions tensor. You'll need to modify the logic in TFLiteObjectDetectionAPIModel to handle this altered output format. You can use the [Netron](https://electronjs.org/apps/netron) visualization tool to help identify how the output tensors differ.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29054\">No</a>\n", "@jdduke Thanks for the suggestion. At least helps to understand the problem deeper. ", "@jdduke One more query, The [official documentation](https://www.tensorflow.org/lite/performance/gpu#supported_models_and_ops) suggests `mobile_ssd_v2_float_coco.tflite` model for enabling GPU delegate, but after analyzing the Model with Netron, \r\n`mobile_ssd_v2_float_coco.tflite`\r\nOUTPUT:\r\nraw_outputs/box_encodings\r\nid: raw_outputs/box_encodings\r\ntype: float32[1,2034,4]\r\nraw_outputs/class_predictions\r\nid: raw_outputs/class_predictions\r\ntype: float32[1,2034,91]\r\n\r\nWhereas the default model `detect.tflite` used in the demo has 4 different parameters.\r\nOUTPUT: \r\nTFLite_Detection_PostProcess\r\nid: TFLite_Detection_PostProcess\r\ntype: float32\r\nTFLite_Detection_PostProcess:1\r\nid: TFLite_Detection_PostProcess:1\r\ntype: float32\r\nTFLite_Detection_PostProcess:2\r\nid: TFLite_Detection_PostProcess:2\r\ntype: float32\r\nTFLite_Detection_PostProcess:3\r\nid: TFLite_Detection_PostProcess:3\r\ntype: float32\r\nie for **Location, Classes, Scores, Number and detections**\r\n\r\n**How can we use `mobile_ssd_v2_float_coco.tflite`or any model to get enable GPU with Object Detection, Please suggest?** ", "@Gmrevo \r\nHi,\r\n\r\nAny solution you got for above error. I am stuck in the same from last few days. Can you please assist.\r\n\r\nThanks", "Hi Vir, \r\n\r\n> @Gmrevo\r\n> Hi,\r\n> \r\n> Any solution you got for the above error. I am stuck in the same from the last few days. Can you please assist.\r\n> \r\n> Thanks\r\n\r\nEnabling GPU in my project still has been a mystery. I couldn't get the delegate working but you can always look for enabling **NNAPI** at the android side. You can refer to the NNAPI implementation in the class file attached below.\r\n[TFLiteObjectDetectionAPIModel_javaClass.txt](https://github.com/tensorflow/tensorflow/files/3496271/TFLiteObjectDetectionAPIModel_javaClass.txt)\r\n\r\n", "@Gmrevo \r\n\r\nHi,\r\n\r\nThanks for your quick reply.\r\nI have replaced the file with yours.\r\n\r\nEnded up with the same error as above.\r\n\r\n```\r\njava.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 1917, 4] and a Java object with shape [1, 10, 4].\r\n        at org.tensorflow.lite.Tensor.throwIfShapeIsIncompatible(Tensor.java:282)\r\n        at org.tensorflow.lite.Tensor.throwIfDataIsIncompatible(Tensor.java:249)\r\n        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:141)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:161)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:240)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n\r\n```", "You need to change the shape of the output buffer that you're feeding to the interpreter. The error message is saying that the Java array you've provided doesn't match the shape of the output tensor. In the sample, the number of detections is hardcoded to 10, but it looks like your model is outputting 1917. Might be an issue with how you've converted your model.", "The number of detections is hardcoded to 10, but it looks like your model is outputting 1917-> So hardcoding to 1917 should work? \r\n\r\nAlso this 1917 value is coming from model so what changes model needs?", "> The number of detections is hardcoded to 10, but it looks like your model is outputting 1917-> So hardcoding to 1917 should work?\r\n> \r\n> Also this 1917 value is coming from model so what changes model needs?\r\n\r\nHi Vir.\r\nCan you solve the problem?", "@Davari393 \r\n\r\nYes it got resolved.\r\nThe change was in the input tensor in tflite model.\r\nSomehow it is dependent on inter compatibility hence, [1,10,4] was the array our android code expects so same array (input tensor whilce creating model) needs to be passed in .tflite file. ", "> @Davari393\r\n> \r\n> Yes it got resolved.\r\n> The change was in the input tensor in tflite model.\r\n> Somehow it is dependent on inter compatibility hence, [1,10,4] was the array our android code expects so same array (input tensor whilce creating model) needs to be passed in .tflite file.\r\n\r\nHi Vir, I hardcoded the num_detection into 1917 but it still doesn't work. The .tflite file is compiled using TOCO so I thought it cannot be changed right? May I ask you how you solve this exactly?", "I solved the same problem by doing as this https://github.com/tensorflow/tensorflow/issues/22106#issuecomment-428409506", "For anyone who hasn't resolved the issue, this is how i resolved the issue; \r\n\r\nI noticed that the output tensor of the sample detection model is different from the output tensor of my custom model so i tweaked the TFLiteModelObjectDetectionAPIModel.java file by changing the `numBytesPerChannel`, `NUM_DETECTIONS`, variable to match mine. \r\n\r\nPS: IF you don't know your output tensors, you can check it with this wonderful too > https://lutzroeder.github.io/netron/", "I got the same issue,  I set num_classes as 5 to train my custom dataset. but I got the error: java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 10, 4] and a Java object with shape [1, 5, 4]. The 5 is the number of classes, Why is the output tensor 10?\r\n\r\nlabel_map.pbtxt\r\n```\r\nitem {\r\n  id: 1\r\n  name: 'me'\r\n}\r\n\r\nitem {\r\n  id: 2\r\n  name: 'teammate'\r\n}\r\n\r\nitem {\r\n  id: 3\r\n  name: 'enemy'\r\n}\r\n\r\nitem {\r\n  id: 4\r\n  name: 'enemy_no_threat'\r\n}\r\n\r\nitem {\r\n  id: 5\r\n  name: 'ignorance'\r\n}\r\n```\r\n\r\n\r\nssdlite_mobilenet_v3_small_320x320_coco.config\r\n```\r\nmodel {\r\n  ssd {\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: false\r\n    num_classes: 5\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    encode_background_as_zeros: true\r\n    anchor_generator {\r\n      ssd_anchor_generator {\r\n        num_layers: 6\r\n        min_scale: 0.2\r\n        max_scale: 0.95\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        aspect_ratios: 3.0\r\n        aspect_ratios: 0.3333\r\n      }\r\n    }\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 320\r\n        width: 320\r\n      }\r\n    }\r\n    box_predictor {\r\n      convolutional_box_predictor {\r\n        min_depth: 0\r\n        max_depth: 0\r\n        num_layers_before_predictor: 0\r\n        use_dropout: false\r\n        dropout_keep_probability: 0.8\r\n        kernel_size: 3\r\n        use_depthwise: true\r\n        box_code_size: 4\r\n        apply_sigmoid_to_scores: false\r\n        class_prediction_bias_init: -4.6\r\n        conv_hyperparams {\r\n          activation: RELU_6,\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.00004\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              stddev: 0.03\r\n              mean: 0.0\r\n            }\r\n          }\r\n          batch_norm {\r\n            train: true,\r\n            scale: true,\r\n            center: true,\r\n            decay: 0.97,\r\n            epsilon: 0.001,\r\n          }\r\n        }\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'ssd_mobilenet_v3_small'\r\n      min_depth: 16\r\n      depth_multiplier: 1.0\r\n      use_depthwise: true\r\n      conv_hyperparams {\r\n        activation: RELU_6,\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 0.00004\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            stddev: 0.03\r\n            mean: 0.0\r\n          }\r\n        }\r\n        batch_norm {\r\n          train: true,\r\n          scale: true,\r\n          center: true,\r\n          decay: 0.97,\r\n          epsilon: 0.001,\r\n        }\r\n      }\r\n      override_base_feature_extractor_hyperparams: true\r\n    }\r\n    loss {\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          alpha: 0.75,\r\n          gamma: 2.0\r\n        }\r\n      }\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n          delta: 1.0\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    normalize_loc_loss_by_codesize: true\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 1e-8\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n        use_static_shapes: true\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 512\r\n  sync_replicas: true\r\n  startup_delay_steps: 0\r\n  replicas_to_aggregate: 32\r\n  num_steps: 800000\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    ssd_random_crop {\r\n    }\r\n  }\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.4\r\n          total_steps: 800000\r\n          warmup_learning_rate: 0.13333\r\n          warmup_steps: 2000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  max_number_of_boxes: 100\r\n  unpad_groundtruth_tensors: false\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"/home/cy/workspace/ssd/tfdata/sausagetain.record\"\r\n  }\r\n  label_map_path: \"/home/cy/workspace/ssd/tfdata/sausage_label_map.pbtxt\"\r\n}\r\n\r\neval_config: {\r\n  num_examples: 8000\r\n}\r\n\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"/home/cy/workspace/ssd/tfdata/sausageval.record\"\r\n  }\r\n  label_map_path: \"/home/cy/workspace/ssd/tfdata/sausage_label_map.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n}\r\n```\r\n\r\nConvert pb to tflite.\r\n```\r\ntensorflow/lite/toco/toco \\\r\n--input_file=/Users/cy/PycharmProjects/sausage.pb \\\r\n--output_file=/Users/cy/PycharmProjects/detect5.tflite \\\r\n--output_format=TFLITE \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--input_shapes=1,320,320,3 \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n--inference_type=FLOAT \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--allow_custom_ops \\\r\n--change_concat_input_ranges=false \r\n```\r\n", "I hard coded the value and changed it to 1917 instead of 10, but again i am getting error as this .Please help.\r\n\r\n java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 1917, 2] and a Java object with shape [1, 1917].\r\n        at org.tensorflow.lite.Tensor.throwIfShapeIsIncompatible(Tensor.java:435)\r\n        at org.tensorflow.lite.Tensor.throwIfDataIsIncompatible(Tensor.java:392)\r\n        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:250)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:166)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:314)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:231)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:182)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:201)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)", "It's not clear where the [2] dimension value is coming from, but your output buffer appears to be the wrong shape. @srjoglekar246 can help advise.", "\r\nThis is my model which i trained in that [1,1917,2] scores  , is this and i am unable to make changes accordingly in my android project as its showing error as i mentioned above\r\n[Uploading \r\nTFLiteObjectDetectionAPIModel_javaClass.txt\u2026]()\r\n![Screenshot (45)](https://user-images.githubusercontent.com/48325131/83105918-2ce87280-a0d9-11ea-866a-02237a100520.png)\r\n\r\n", "You need to change the shape of the output Java array you provide to [1,1917,2]. ", "Is there anyone who solve this problem ? I got stuck. I 'm trying to modify .java code but it don't work :| ", "@Tezcan98 Which model are you using?", "Im just tryin on ssd_mobilenet_v2_320x320_coco since learn if tflite converter works but I can't get any successful result. \r\nI will deploy a website can convert from .pb or .h5 to .tflite.", "@Tezcan98 Did you modify the app code to accept 320x320 input? What error are you getting? Is it [this one](https://github.com/tensorflow/tensorflow/issues/29054#issuecomment-634617023)?", "@srjoglekar246 \r\nMy convert operation is this\r\n`\r\n!pip install tf-nightly  # Restart runtime after this line\r\n\r\nimport tensorflow as tf\r\n\r\nmodel = tf.saved_model.load(\"ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\")\r\nconcrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([1, 320, 320, 3])  \r\n \r\ntf.saved_model.save(model, \"saved_model_updated\",signatures=model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir='saved_model_updated',signature_keys=['serving_default'] )\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n`\r\n\r\nIf I use converter.optimizations = [tf.lite.Optimize.DEFAULT] line, model converted to quantization so Im getting Cannot copy ... [1,100] to a Java object with shape [1, 10, 4] \r\nIf I comment this line I got [1, 1917, 4] to ... with shape [1, 10, 4] error\r\n\r\nThank you for your interest.", "Please follow [these instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md) for converting SSD models for TFLite.", "Thanks I'll try it. I hope I can do it :)", "Thanks, it works. I used colab code they share. this model detect duck and there is 1 class at last layer and different from I've tried so they mean this code is work. Can I write you from mail If I get different issue ? @srjoglekar246 ", "Feel free to just follow up here, so that its better for me to track :-)", "Hello \r\n\r\nGreat comments to read that actually people solved it! \r\n\r\nIn my case (Not yet solved)- \r\n\r\nModel used ---> ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\r\n\r\n1. TF_OD_API_INPUT_SIZE = 320;  (changed to 320-->It was 300)\r\n2. TF_OD_API_MODEL_FILE = \"detect1.tflite\";\r\n3. TF_OD_API_LABELS_FILE = \"labelmap.txt\";\r\n\r\nBelow is my I/P and O/P tensor for models- \r\n![image](https://user-images.githubusercontent.com/58831041/137597274-a0a7d180-1f6b-4032-b22d-f93ba1337a87.png)\r\n\r\n\r\nGetting error as below - \r\n![image](https://user-images.githubusercontent.com/58831041/137597316-53368dd2-70ce-4f2d-8782-f4724db29cbb.png)\r\n\r\nCode part, is the input and output part correct? Please guide me. \r\n![image](https://user-images.githubusercontent.com/58831041/137597335-ae0cee91-03da-47d9-9dc7-e266ca9a798d.png)\r\n\r\nThank you!\r\n\r\n\r\nEDIT 1 - \r\nHello Sir @SirPhemmiey  and @jdduke  \r\n\r\nAs you have suggested in previous comments. \r\nNow somehow with basic understanding -\r\nif I change my code for INPUT vectors = OUTPUT vectors \r\nPFA image- \r\n![image](https://user-images.githubusercontent.com/58831041/137597880-8b04b08c-e947-40e5-8a59-554525e2b36d.png)\r\n\r\nI get below error- \r\n![image](https://user-images.githubusercontent.com/58831041/137597939-1273a5db-a4a5-46b3-b831-054cebc3c5db.png)\r\n\r\nany help is much appreciated. \r\n\r\n", "@tbikash62 Did you convert your model recently? I think there were some changes to our converter recently, due to which the order of the outputs might get changed. \r\n\r\nSee the 'Test TFLite model' section in [this colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb), where we look at the TFLite model's input & output details (`output_details = interpreter.get_output_details()`). You can `print()` that information in a Python script to check what shapes your model's outputs have. It is likely that the java code you mentioned above assumes an output ordering of tensors which isn't followed by the model, so you will have to change that part of the code.\r\n\r\nDon't change the input buffer dimensions, it needs to be `1*inputSize*inputSize*numBytesperchannel*3`.", "Hello @srjoglekar246 \r\n\r\nThank you for the guidance. \r\n\r\nDid you convert your model recently?\r\nYes, on 15th of this month. \r\n\r\n```\r\nCode Snippet used - \r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=os.path.join(paths['MODEL_PATH'], 'my_ssd_mobnet', 'tfliteexport', 'saved_model', 'detect1.tflite'))\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nprint(input_details)\r\nprint(output_details)\r\n\r\n# Test the model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n```\r\n\r\nOUTPUT - \r\n\r\nprint(input_details)\r\n[{'name': 'serving_default_input:0', 'index': 0, **'shape': array([  1, 320, 320,   3]),** 'shape_signature': array([  1, 320, 320,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\nprint(output_details)\r\n[{'name': 'StatefulPartitionedCall:1', 'index': 335**, '**shape': array([ 1, 10])**, '**shape_signature': array([ 1, 10]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:3', 'index': 333, 'shape': array([ 1, 10,  4]), 'shape_signature': array([ 1, 10,  4]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:0', 'index': 336, 'shape': array([1]), 'shape_signature': array([1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:2', 'index': 334, 'shape': array([ 1, 10]), 'shape_signature': array([ 1, 10]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\nprint(output_data)\r\n[[0.2532972  0.1299967  0.06841362 0.06171301 0.04120497 0.03646466 0.03233587 0.0318325  0.03004893 0.02726024]]\r\n```\r\n\r\nSo is it the java code or error in the model itself, I mean do I have to retrain? \r\n\r\nAs of now, keeping inputs as it is. (float32 so getting multiplied by 4--> 1 * 320 * 320 *4)\r\n![image](https://user-images.githubusercontent.com/58831041/137814034-f5f3e41d-026c-43c7-8d81-5a207d8bf671.png)\r\n\r\nExact error line for tensor mismatch is at - \r\n![image](https://user-images.githubusercontent.com/58831041/137814275-440c0a51-897a-4d2d-a029-b637d9a1a077.png)\r\n\r\nError Which I'm getting - \r\n![image](https://user-images.githubusercontent.com/58831041/137814415-1b57722e-a468-4fd7-9be1-c6a6710a4320.png)\r\n\r\n", "So if you look at the `output_details`, specifically the shape of each output tensor, you will see that the order of outputs is actually `[classes, boxes, num_detections, scores]` or `[scores, boxes, num_detections, classes]` (because the shape of output at index 3 is same as shape of output 0, so one of them is scores and other is classes - I am not sure which). Right now, its likely that your Java code assumes `[boxes, classes, scores, num_detections]` - which is why the error says that you are trying to copy a wrong-shaped tensor into another one.\r\n\r\nYou can run the model with some Python code with example images (as in the Colab I sent you) to debug.\r\n", "Hey @srjoglekar246 \r\n\r\nThank you so much. \r\nYou made my day! \r\n\r\n\r\nFor anyone who looks answer for future. \r\n\r\nas advised from @srjoglekar246 \r\n\r\n` \r\n // outputLocations: array of shape [Batchsize, NUM_DETECTIONS,4]\r\n  // contains the location of detected boxes\r\n  private float[][][] outputLocations;\r\n  // outputClasses: array of shape [Batchsize, NUM_DETECTIONS]\r\n  // contains the classes of detected boxes\r\n  private float[][] outputClasses;\r\n  // outputScores: array of shape [Batchsize, NUM_DETECTIONS]\r\n  // contains the scores of detected boxes\r\n  private float[][] outputScores;\r\n  // numDetections: array of shape [Batchsize]\r\n  // contains the number of detected boxes  \r\n  private float[] numDetections;\r\n`\r\nThe answer lies here - \r\n[numDetections:2, outputScores:0/3, outputLocations:1 outputClasses:3/0]\r\n\r\nThe changes are in indexing of outputMap.put and it works all fine! \r\nObject[] inputArray = {imgData};\r\n    Map<Integer, Object> outputMap = new HashMap<>();\r\n    outputMap.put(1, outputLocations);\r\n    outputMap.put(3, outputClasses);\r\n    outputMap.put(0, outputScores);\r\n    outputMap.put(2, numDetections);\r\n\r\nThank you once again, for swift replies.\r\nWill try to communicate this in all stack overflow to help others :)  \r\nGod speed. \r\n"]}, {"number": 29053, "title": "building from source failed ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: latest master from github\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.25.3\r\n- GCC/Compiler version (if compiling from source): gcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC)\r\n- CUDA/cuDNN version: CUDA 10/ cuDNN 7\r\n- GPU model and memory:V100 32G\r\n\r\n**Describe the problem**\r\nwhen I run: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`, I get error:\r\n```\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=129\r\nINFO: Reading rc options for 'build' from /home/minds/isaac/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from /home/minds/isaac/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/minds/.virtualenvs/venv_tf/bin/python --action_env PYTHON_LIB_PATH=/home/minds/.virtualenvs/venv_tf/lib/python3.6/site-packages --python_path=/home/minds/.virtualenvs/venv_tf/bin/python --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.0,7.0 --action_env GCC_HOST_COMPILER_PATH=/bin/gcc --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:cuda in file /home/minds/isaac/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/minds/isaac/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:opt in file /home/minds/isaac/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file /home/minds/isaac/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/minds/isaac/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: An error occurred during the fetch of repository 'io_bazel_rules_docker'\r\nINFO: Call stack for the definition of repository 'io_bazel_rules_docker':\r\n - /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - /home/minds/isaac/tensorflow/WORKSPACE:29:1\r\nERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n\tFile \"/home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 234\r\n\t\t_clone_or_update(ctx)\r\n\tFile \"/home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 74, in _clone_or_update\r\n\t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning io_bazel_rules_docker:\r\n+ cd /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external\r\n+ rm -rf /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker\r\n+ git clone '' https://github.com/bazelbuild/rules_docker.git /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker\r\nToo many arguments.\r\n\r\nusage: git clone [options] [--] <repo> [<dir>]\r\n\r\n    -v, --verbose         be more verbose\r\n    -q, --quiet           be more quiet\r\n    --progress            force progress reporting\r\n    -n, --no-checkout     don't create a checkout\r\n    --bare                create a bare repository\r\n    --mirror              create a mirror repository (implies bare)\r\n    -l, --local           to clone from a local repository\r\n    --no-hardlinks        don't use local hardlinks, always copy\r\n    -s, --shared          setup as shared repository\r\n    --recursive           initialize submodules in the clone\r\n    --recurse-submodules  initialize submodules in the clone\r\n    --template <template-directory>\r\n                          directory from which templates will be used\r\n    --reference <repo>    reference repository\r\n    -o, --origin <name>   use <name> instead of 'origin' to track upstream\r\n    -b, --branch <branch>\r\n                          checkout <branch> instead of the remote's HEAD\r\n    -u, --upload-pack <path>\r\n                          path to git-upload-pack on the remote\r\n    --depth <depth>       create a shallow clone of that depth\r\n    --single-branch       clone only one branch, HEAD or --branch\r\n    --separate-git-dir <gitdir>\r\n                          separate git dir from working tree\r\n    -c, --config <key=value>\r\n                          set config inside the new repository\r\n\r\n+ git clone https://github.com/bazelbuild/rules_docker.git /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker\r\n+ git -C /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker reset --hard 251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker fetch '' origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker fetch origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\nERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n\tFile \"/home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 234\r\n\t\t_clone_or_update(ctx)\r\n\tFile \"/home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 74, in _clone_or_update\r\n\t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning io_bazel_rules_docker:\r\n+ cd /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external\r\n+ rm -rf /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker\r\n+ git clone '' https://github.com/bazelbuild/rules_docker.git /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker\r\nToo many arguments.\r\n\r\nusage: git clone [options] [--] <repo> [<dir>]\r\n\r\n    -v, --verbose         be more verbose\r\n    -q, --quiet           be more quiet\r\n    --progress            force progress reporting\r\n    -n, --no-checkout     don't create a checkout\r\n    --bare                create a bare repository\r\n    --mirror              create a mirror repository (implies bare)\r\n    -l, --local           to clone from a local repository\r\n    --no-hardlinks        don't use local hardlinks, always copy\r\n    -s, --shared          setup as shared repository\r\n    --recursive           initialize submodules in the clone\r\n    --recurse-submodules  initialize submodules in the clone\r\n    --template <template-directory>\r\n                          directory from which templates will be used\r\n    --reference <repo>    reference repository\r\n    -o, --origin <name>   use <name> instead of 'origin' to track upstream\r\n    -b, --branch <branch>\r\n                          checkout <branch> instead of the remote's HEAD\r\n    -u, --upload-pack <path>\r\n                          path to git-upload-pack on the remote\r\n    --depth <depth>       create a shallow clone of that depth\r\n    --single-branch       clone only one branch, HEAD or --branch\r\n    --separate-git-dir <gitdir>\r\n                          separate git dir from working tree\r\n    -c, --config <key=value>\r\n                          set config inside the new repository\r\n\r\n+ git clone https://github.com/bazelbuild/rules_docker.git /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker\r\n+ git -C /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker reset --hard 251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker fetch '' origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/io_bazel_rules_docker fetch origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\nINFO: Elapsed time: 7.123s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nfollowed this [guide](https://www.tensorflow.org/install/source), until I faced the issue\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["What is the output of `git --version`?", "git version 1.8.3.1\r\n\r\nEDIT:\r\nI have udpated to git version 2.16.5\r\n\r\nHowever, error still persists:\r\nI ran command: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\nand got error:\r\n\r\n```\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/minds/isaac/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from /home/minds/isaac/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/minds/.virtualenvs/venv_tf/bin/python --action_env PYTHON_LIB_PATH=/home/minds/.virtualenvs/venv_tf/lib/python3.6/site-packages --python_path=/home/minds/.virtualenvs/venv_tf/bin/python --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.0,7.0 --action_env GCC_HOST_COMPILER_PATH=/bin/gcc --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:cuda in file /home/minds/isaac/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/minds/isaac/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:opt in file /home/minds/isaac/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file /home/minds/isaac/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/minds/isaac/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nWARNING: /home/minds/isaac/tensorflow/tensorflow/python/BUILD:3478:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/minds/isaac/tensorflow/tensorflow/python/BUILD:102:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/minds/isaac/tensorflow/tensorflow/contrib/metrics/BUILD:17:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/minds/isaac/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/minds/isaac/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/minds/isaac/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/minds/isaac/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/zlib_archive/BUILD.bazel:5:1: undeclared inclusion(s) in rule '@zlib_archive//:zlib':\r\nthis rule is missing dependency declarations for the following files included by 'external/zlib_archive/compress.c':\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1.224s, Critical Path: 0.18s\r\nINFO: 5 processes: 5 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "It is a different error now.", "I understand. Could you help me out?", "Can't reproduce. Can you try recloning the repo on a different directory and recompiling from scratch?", "# configuration\r\nthis is how I configured to the `./configure` command:\r\n```\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.25.3- (@non-git) installed.\r\nPlease specify the location of python. [Default is /home/minds/.virtualenvs/venv_tf/bin/python]:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'site' has no attribute 'getsitepackages'\r\nFound possible Python library paths:\r\n  /home/minds/.virtualenvs/venv_tf/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/minds/.virtualenvs/venv_tf/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.0 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.0,7.0]:\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /bin/gcc]:\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n```\r\n\r\n# Error message\r\nI have tried building again and failed with a different error:\r\n```\r\nERROR: /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/protobuf_archive/BUILD:91:1: undeclared inclusion(s) in rule '@protobuf_archive//:protobuf_lite':\r\nthis rule is missing dependency declarations for the following files included by 'external/protobuf_archive/src/google/protobuf/stubs/statusor.cc':\r\n  '/include/c++/4.8.5/new'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/c++config.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/os_defines.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/cpu_defines.h'\r\n  '/include/c++/4.8.5/exception'\r\n  '/include/c++/4.8.5/bits/atomic_lockfree_defines.h'\r\n  '/include/c++/4.8.5/bits/exception_ptr.h'\r\n  '/include/c++/4.8.5/bits/exception_defines.h'\r\n  '/include/c++/4.8.5/bits/nested_exception.h'\r\n  '/include/c++/4.8.5/string'\r\n  '/include/c++/4.8.5/bits/stringfwd.h'\r\n  '/include/c++/4.8.5/bits/memoryfwd.h'\r\n  '/include/c++/4.8.5/bits/char_traits.h'\r\n  '/include/c++/4.8.5/bits/stl_algobase.h'\r\n  '/include/c++/4.8.5/bits/functexcept.h'\r\n  '/include/c++/4.8.5/bits/cpp_type_traits.h'\r\n  '/include/c++/4.8.5/ext/type_traits.h'\r\n  '/include/c++/4.8.5/ext/numeric_traits.h'\r\n  '/include/c++/4.8.5/bits/stl_pair.h'\r\n  '/include/c++/4.8.5/bits/move.h'\r\n  '/include/c++/4.8.5/bits/concept_check.h'\r\n  '/include/c++/4.8.5/type_traits'\r\n  '/include/c++/4.8.5/bits/stl_iterator_base_types.h'\r\n  '/include/c++/4.8.5/bits/stl_iterator_base_funcs.h'\r\n  '/include/c++/4.8.5/debug/debug.h'\r\n  '/include/c++/4.8.5/bits/stl_iterator.h'\r\n  '/include/c++/4.8.5/bits/postypes.h'\r\n  '/include/c++/4.8.5/cwchar'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n  '/include/c++/4.8.5/cstdint'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdint.h'\r\n  '/include/c++/4.8.5/bits/allocator.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/c++allocator.h'\r\n  '/include/c++/4.8.5/ext/new_allocator.h'\r\n  '/include/c++/4.8.5/bits/localefwd.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/c++locale.h'\r\n  '/include/c++/4.8.5/clocale'\r\n  '/include/c++/4.8.5/iosfwd'\r\n  '/include/c++/4.8.5/cctype'\r\n  '/include/c++/4.8.5/bits/ostream_insert.h'\r\n  '/include/c++/4.8.5/bits/cxxabi_forced.h'\r\n  '/include/c++/4.8.5/bits/stl_function.h'\r\n  '/include/c++/4.8.5/backward/binders.h'\r\n  '/include/c++/4.8.5/bits/range_access.h'\r\n  '/include/c++/4.8.5/bits/basic_string.h'\r\n  '/include/c++/4.8.5/ext/atomicity.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/gthr.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/gthr-default.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/atomic_word.h'\r\n  '/include/c++/4.8.5/initializer_list'\r\n  '/include/c++/4.8.5/ext/string_conversions.h'\r\n  '/include/c++/4.8.5/cstdlib'\r\n  '/include/c++/4.8.5/cstdio'\r\n  '/include/c++/4.8.5/cerrno'\r\n  '/include/c++/4.8.5/bits/functional_hash.h'\r\n  '/include/c++/4.8.5/bits/hash_bytes.h'\r\n  '/include/c++/4.8.5/bits/basic_string.tcc'\r\n  '/include/c++/4.8.5/utility'\r\n  '/include/c++/4.8.5/bits/stl_relops.h'\r\n  '/include/c++/4.8.5/algorithm'\r\n  '/include/c++/4.8.5/bits/stl_algo.h'\r\n  '/include/c++/4.8.5/bits/algorithmfwd.h'\r\n  '/include/c++/4.8.5/bits/stl_heap.h'\r\n  '/include/c++/4.8.5/bits/stl_tempbuf.h'\r\n  '/include/c++/4.8.5/bits/stl_construct.h'\r\n  '/include/c++/4.8.5/ext/alloc_traits.h'\r\n  '/include/c++/4.8.5/bits/alloc_traits.h'\r\n  '/include/c++/4.8.5/bits/ptr_traits.h'\r\n  '/include/c++/4.8.5/random'\r\n  '/include/c++/4.8.5/cmath'\r\n  '/include/c++/4.8.5/limits'\r\n  '/include/c++/4.8.5/bits/random.h'\r\n  '/include/c++/4.8.5/vector'\r\n  '/include/c++/4.8.5/bits/stl_uninitialized.h'\r\n  '/include/c++/4.8.5/bits/stl_vector.h'\r\n  '/include/c++/4.8.5/bits/stl_bvector.h'\r\n  '/include/c++/4.8.5/bits/vector.tcc'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/opt_random.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/x86intrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/ia32intrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/mmintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/xmmintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/mm_malloc.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/emmintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/pmmintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/tmmintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/smmintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/popcntintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/wmmintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/immintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/avxintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/avx2intrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/lzcntintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/bmiintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/bmi2intrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/fmaintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/f16cintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/rtmintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/xtestintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/rdseedintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/prfchwintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/fxsrintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/xsaveintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/xsaveoptintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/adxintrin.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/pkuintrin.h'\r\n  '/include/c++/4.8.5/bits/random.tcc'\r\n  '/include/c++/4.8.5/numeric'\r\n  '/include/c++/4.8.5/bits/stl_numeric.h'\r\n  '/include/c++/4.8.5/functional'\r\n  '/include/c++/4.8.5/typeinfo'\r\n  '/include/c++/4.8.5/tuple'\r\n  '/include/c++/4.8.5/array'\r\n  '/include/c++/4.8.5/stdexcept'\r\n  '/include/c++/4.8.5/bits/uses_allocator.h'\r\n  '/include/c++/4.8.5/iostream'\r\n  '/include/c++/4.8.5/ostream'\r\n  '/include/c++/4.8.5/ios'\r\n  '/include/c++/4.8.5/bits/ios_base.h'\r\n  '/include/c++/4.8.5/bits/locale_classes.h'\r\n  '/include/c++/4.8.5/bits/locale_classes.tcc'\r\n  '/include/c++/4.8.5/streambuf'\r\n  '/include/c++/4.8.5/bits/streambuf.tcc'\r\n  '/include/c++/4.8.5/bits/basic_ios.h'\r\n  '/include/c++/4.8.5/bits/locale_facets.h'\r\n  '/include/c++/4.8.5/cwctype'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/ctype_base.h'\r\n  '/include/c++/4.8.5/bits/streambuf_iterator.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/ctype_inline.h'\r\n  '/include/c++/4.8.5/bits/locale_facets.tcc'\r\n  '/include/c++/4.8.5/bits/basic_ios.tcc'\r\n  '/include/c++/4.8.5/bits/ostream.tcc'\r\n  '/include/c++/4.8.5/istream'\r\n  '/include/c++/4.8.5/bits/istream.tcc'\r\n  '/include/c++/4.8.5/map'\r\n  '/include/c++/4.8.5/bits/stl_tree.h'\r\n  '/include/c++/4.8.5/bits/stl_map.h'\r\n  '/include/c++/4.8.5/bits/stl_multimap.h'\r\n  '/include/c++/4.8.5/memory'\r\n  '/include/c++/4.8.5/bits/stl_raw_storage_iter.h'\r\n  '/include/c++/4.8.5/ext/concurrence.h'\r\n  '/include/c++/4.8.5/bits/unique_ptr.h'\r\n  '/include/c++/4.8.5/bits/shared_ptr.h'\r\n  '/include/c++/4.8.5/bits/shared_ptr_base.h'\r\n  '/include/c++/4.8.5/backward/auto_ptr.h'\r\n  '/include/c++/4.8.5/set'\r\n  '/include/c++/4.8.5/bits/stl_set.h'\r\n  '/include/c++/4.8.5/bits/stl_multiset.h'\r\n  '/include/c++/4.8.5/cstddef'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\r\n  '/include/c++/4.8.5/unordered_map'\r\n  '/include/c++/4.8.5/bits/hashtable.h'\r\n  '/include/c++/4.8.5/bits/hashtable_policy.h'\r\n  '/include/c++/4.8.5/bits/unordered_map.h'\r\n  '/include/c++/4.8.5/unordered_set'\r\n  '/include/c++/4.8.5/bits/unordered_set.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 17.327s, Critical Path: 1.23s\r\nINFO: 4 processes: 4 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "It seems that your toolchain is incomplete", "I will suggest you to use a Docker container to build TF. Your toolchain seems a little bit..strange.", "@byronyi \r\nI have used docker but that failed as well.\r\n\r\nWhat can I do to amend the issue?\r\n\r\n## Docker Error message:\r\n\r\nI have folllowed the [guide](https://www.tensorflow.org/install/source) on building from source using docker, and answered such to configuration settings ( my `usr/local/cuda` is `cuda-10.0` not `cuda-9.0` but I don't think this should be an issue because cuda SDK is backward compatible ):\r\n\r\n```\r\n- default python path\r\n- default Python library\r\n- no XLA support\r\n- no OpenCL\r\n- no ROCM\r\n- default CUDA path (usr/local/cuda) \r\n- default TensorRT path (/usr/lib/x86_64-linux-gnu)\r\n- no clang as CUDA compiler\r\n- no MPI support\r\n\r\n```\r\n\r\n**question on configuration using docker**: is it required to have tensorRT installed in my native OS even when I am using docker?  I don't want TensorRT support\r\n\r\n## Error message\r\n\r\n```\r\nERROR: /tensorflow/tensorflow/lite/kernels/BUILD:57:1: C++ compilation of rule '//tensorflow/lite/kernels:eigen_support' failed (Exit 1)\r\nIn file included from external/eigen_archive/Eigen/Core:150:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/eigen_tensor_reduced_instantiations_oss.h:26,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/eigen_spatial_convolutions.h:37,\r\n                 from tensorflow/lite/kernels/eigen_support.cc:20:\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket EigenForTFLite::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(2) long long int; TgtPacket = __vector(4) float]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:25:40:   required from 'Packet EigenForTFLite::internal::pfrexp_float(const Packet&, Packet&) [with Packet = __vector(4) float]'\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/PacketMath.h:574:33:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:140:34: error: invalid static_cast from type 'const __vector(2) long long int' to type '__vector(4) float'\r\n   return static_cast<TgtPacket>(a);\r\n                                  ^\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket EigenForTFLite::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) float; TgtPacket = __vector(2) long long int]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:35:37:   required from 'Packet EigenForTFLite::internal::pldexp_float(Packet, Packet) [with Packet = __vector(4) float]'\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/PacketMath.h:578:33:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:140:34: error: invalid static_cast from type 'const __vector(4) float' to type '__vector(2) long long int'\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket EigenForTFLite::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) long long int; TgtPacket = __vector(8) float]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:25:40:   required from 'Packet EigenForTFLite::internal::pfrexp_float(const Packet&, Packet&) [with Packet = __vector(8) float]'\r\nexternal/eigen_archive/Eigen/src/Core/arch/AVX/PacketMath.h:423:33:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:140:34: error: invalid static_cast from type 'const __vector(4) long long int' to type '__vector(8) float'\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket EigenForTFLite::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(8) float; TgtPacket = __vector(4) long long int]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:35:37:   required from 'Packet EigenForTFLite::internal::pldexp_float(Packet, Packet) [with Packet = __vector(8) float]'\r\nexternal/eigen_archive/Eigen/src/Core/arch/AVX/PacketMath.h:427:33:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:140:34: error: invalid static_cast from type 'const __vector(8) float' to type '__vector(4) long long int'\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:157:1: warning: inline function 'Target EigenForTFLite::internal::preinterpret(const Packet&) [with Target = __vector(2) long long int; Packet = __vector(4) float]' used but never defined\r\n preinterpret(const Packet& a); /* { return reinterpret_cast<const Target&>(a); } */\r\n ^\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:157:1: warning: inline function 'Target EigenForTFLite::internal::preinterpret(const Packet&) [with Target = __vector(4) float; Packet = __vector(2) long long int]' used but never defined\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:157:1: warning: inline function 'Target EigenForTFLite::internal::preinterpret(const Packet&) [with Target = __vector(4) long long int; Packet = __vector(8) float]' used but never defined\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:157:1: warning: inline function 'Target EigenForTFLite::internal::preinterpret(const Packet&) [with Target = __vector(8) float; Packet = __vector(4) long long int]' used but never defined\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 67.020s, Critical Path: 29.12s\r\nINFO: 99 processes: 99 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "I have seen this problem when the compiler changes after running `./configure`.\r\nCould you run `bazel clean --expunge`, then retry building?", "@gunan \r\nI have done so:\r\n```\r\nbazel clean --expunge\r\n./configure\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nbut it failed again. I wonder if it is because I am using default compiler -- `gcc` -- as my `nvcc` compiler. \r\n\r\n## Error message\r\n```\r\nERROR: /home/minds/.cache/bazel/_bazel_minds/7c1f2bd7870d1ecc8754a91ebff0746c/external/com_googlesource_code_re2/BUILD:26:1: undeclared inclusion(s) in rule '@com_googlesource_code_re2//:re2':\r\nthis rule is missing dependency declarations for the following files included by 'external/com_googlesource_code_re2/re2/unicode_groups.cc':\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdint.h'\r\n  '/include/c++/4.8.5/string'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/c++config.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/os_defines.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/cpu_defines.h'\r\n  '/include/c++/4.8.5/bits/stringfwd.h'\r\n  '/include/c++/4.8.5/bits/memoryfwd.h'\r\n  '/include/c++/4.8.5/bits/char_traits.h'\r\n  '/include/c++/4.8.5/bits/stl_algobase.h'\r\n  '/include/c++/4.8.5/bits/functexcept.h'\r\n  '/include/c++/4.8.5/bits/exception_defines.h'\r\n  '/include/c++/4.8.5/bits/cpp_type_traits.h'\r\n  '/include/c++/4.8.5/ext/type_traits.h'\r\n  '/include/c++/4.8.5/ext/numeric_traits.h'\r\n  '/include/c++/4.8.5/bits/stl_pair.h'\r\n  '/include/c++/4.8.5/bits/move.h'\r\n  '/include/c++/4.8.5/bits/concept_check.h'\r\n  '/include/c++/4.8.5/type_traits'\r\n  '/include/c++/4.8.5/bits/stl_iterator_base_types.h'\r\n  '/include/c++/4.8.5/bits/stl_iterator_base_funcs.h'\r\n  '/include/c++/4.8.5/debug/debug.h'\r\n  '/include/c++/4.8.5/bits/stl_iterator.h'\r\n  '/include/c++/4.8.5/bits/postypes.h'\r\n  '/include/c++/4.8.5/cwchar'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n  '/include/c++/4.8.5/cstdint'\r\n  '/include/c++/4.8.5/bits/allocator.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/c++allocator.h'\r\n  '/include/c++/4.8.5/ext/new_allocator.h'\r\n  '/include/c++/4.8.5/new'\r\n  '/include/c++/4.8.5/exception'\r\n  '/include/c++/4.8.5/bits/atomic_lockfree_defines.h'\r\n  '/include/c++/4.8.5/bits/exception_ptr.h'\r\n  '/include/c++/4.8.5/bits/nested_exception.h'\r\n  '/include/c++/4.8.5/bits/localefwd.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/c++locale.h'\r\n  '/include/c++/4.8.5/clocale'\r\n  '/include/c++/4.8.5/iosfwd'\r\n  '/include/c++/4.8.5/cctype'\r\n  '/include/c++/4.8.5/bits/ostream_insert.h'\r\n  '/include/c++/4.8.5/bits/cxxabi_forced.h'\r\n  '/include/c++/4.8.5/bits/stl_function.h'\r\n  '/include/c++/4.8.5/backward/binders.h'\r\n  '/include/c++/4.8.5/bits/range_access.h'\r\n  '/include/c++/4.8.5/bits/basic_string.h'\r\n  '/include/c++/4.8.5/ext/atomicity.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/gthr.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/gthr-default.h'\r\n  '/include/c++/4.8.5/x86_64-redhat-linux/bits/atomic_word.h'\r\n  '/include/c++/4.8.5/initializer_list'\r\n  '/include/c++/4.8.5/ext/string_conversions.h'\r\n  '/include/c++/4.8.5/cstdlib'\r\n  '/include/c++/4.8.5/cstdio'\r\n  '/include/c++/4.8.5/cerrno'\r\n  '/include/c++/4.8.5/bits/functional_hash.h'\r\n  '/include/c++/4.8.5/bits/hash_bytes.h'\r\n  '/include/c++/4.8.5/bits/basic_string.tcc'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 26.030s, Critical Path: 2.50s\r\nINFO: 15 processes: 15 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n## Edit\r\nI have tried the same in my docker, but it failed as well.", "@meteorcloudy @hlopko in case they have any ideas.\r\n@av8ramit in case he ran into similar issues on centos.", "My issue was slightly different, it couldn't find the linker so I needed to manually provide the crosstool.", "I am waiting to make a PR but but I cant test out my code. It would be really great if we I can find a way to build from source or using docker.\r\n\r\nThanks again!", "Are you by any chance on a 32 bit system?", "No", "Cannot reproduce at all, sorry", "As far as I can see, this is a purely bazel issue, where bazel has a problem autodetecting the toolchain on the system.\r\nHowever, I have no direct solution for it.\r\nPlease try reaching out to bazel team, or the sommunity on stackoverflow may have some ideas.", "Closing the issue, as this is a bazel toolchain autodetection problem, on an operating system not supported by us.\r\nYou can reach out to bazel team, or try getting help through stackoverflow.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29053\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29053\">No</a>\n"]}, {"number": 29052, "title": "ArithmeticOptimizer fails for stack with axis=R and axis=-(R+1)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nIn the `stack` op, the allowed range for `axis` is `[ -(R+1),(R+1) )`. However, if you take a strided slice over the result with `axis=-(R+1)` or `axis=R`, the ArithmeticOptimizer will fail with a warning.\r\n\r\nIt also fails for scalars with any value in `axis`.\r\n\r\nThe results are still correct, but this disables the arithmetic optimizer for some (all?) of the code. See the repro code for an example.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would not expect ArithmeticOptimizer to fail.\r\n\r\n**Code to reproduce the issue**\r\nScalar example:\r\n```\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n    a = tf.Variable(tf.constant(0))\r\n    b = tf.Variable(tf.constant(1))\r\n    sess.run(tf.initializers.global_variables())\r\n    sess.run(tf.stack([a, b])[::2])\r\n```\r\n\r\nVector example\r\n```\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n    a = tf.Variable(tf.constant([0,1]))\r\n    b = tf.Variable(tf.constant([2,3]))\r\n    sess.run(tf.initializers.global_variables())\r\n    for axis in range(-2, 2):\r\n        print(axis)\r\n        sess.run(tf.stack([a, b], axis)[::2])\r\n```\r\n\r\nExecutes sucessfully, but the ArithmeticOptimizer fails with a warning:\r\n\r\n```\r\n-2\r\n2019-05-27 11:35:54.164917: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice. Error: Pack node (stack) axis attribute is out of bounds: -2\r\n2019-05-27 11:35:54.166279: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice. Error: Pack node (stack) axis attribute is out of bounds: -2\r\n-1\r\n0\r\n1\r\n2019-05-27 11:35:54.183014: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice_3. Error: Pack node (stack_3) axis attribute is out of bounds: 1\r\n2019-05-27 11:35:54.184915: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice_3. Error: Pack node (stack_3) axis attribute is out of bounds: 1\r\n```", "comments": ["I tried running on Colab with TensorFlow version 1.13.1 but unfortunately did not reproduce the mentioned warning. Can you please check and let us know once again if the issue still persists. Thanks!", "That seems odd. The bug seems to happen in the latest 1.13.1 docker container:\r\n\r\n```\r\n$ docker run -it tensorflow/tensorflow:1.13.1-py3 \r\nUnable to find image 'tensorflow/tensorflow:1.13.1-py3' locally\r\n1.13.1-py3: Pulling from tensorflow/tensorflow\r\n7e6591854262: Pull complete\r\n089d60cb4e0a: Pull complete\r\n9c461696bc09: Pull complete\r\n45085432511a: Pull complete\r\n29303e8416d5: Pull complete\r\n12bb05a3cac8: Pull complete\r\nfe293195091d: Pull complete\r\n22d8b84cd8f1: Pull complete\r\nb816d6e919ba: Pull complete\r\n0a5fb8dc4fa0: Pull complete\r\nDigest: sha256:47e7e3f536f8884218818dfd491387978189101deced7b46b191df19c15298c2\r\nStatus: Downloaded newer image for tensorflow/tensorflow:1.13.1-py3\r\n\r\n________                               _______________\r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ /\r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nWARNING: You are running this container as root, which can cause new files in\r\nmounted volumes to be created as the root user on your host machine.\r\n\r\nTo avoid this, run the container by specifying your user's userid:\r\n\r\n$ docker run -u $(id -u):$(id -g) args...\r\n\r\nroot@2d73a119b197:/# python3\r\nPython 3.5.2 (default, Nov 12 2018, 13:43:14)\r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> with tf.Session() as sess:\r\n...     a = tf.Variable(tf.constant(0))\r\n...     b = tf.Variable(tf.constant(1))\r\n...     sess.run(tf.initializers.global_variables())\r\n...     sess.run(tf.stack([a, b])[::2])\r\n...\r\n2019-05-28 10:31:08.791850: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-05-28 10:31:08.795887: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3504000000 Hz\r\n2019-05-28 10:31:08.797013: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4641fe0 executing computations on platform Host. Devices:\r\n2019-05-28 10:31:08.797191: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-05-28 10:31:08.813334: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice. Error: Pack node (stack) axis attribute is out of bounds: 0\r\n2019-05-28 10:31:08.814590: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice. Error: Pack node (stack) axis attribute is out of bounds: 0\r\narray([0], dtype=int32)\r\n```\r\n\r\nIt does not seem to happen in nightly, however:\r\n\r\n```\r\n$ docker pull tensorflow/tensorflow:nightly-py3\r\nnightly-py3: Pulling from tensorflow/tensorflow\r\n6abc03819f3e: Pull complete\r\n05731e63f211: Pull complete\r\n0bd67c50d6be: Pull complete\r\nb1cb43f7bb17: Pull complete\r\n8a51a4a898a7: Pull complete\r\n0b7a8225fec6: Pull complete\r\ndd97ada802b3: Pull complete\r\n469aff84f226: Pull complete\r\n6e397dac6060: Pull complete\r\nDigest: sha256:ad0803c032998bfa088960e2485a4230daddc7cb3f090d330c388e6f7578de1d\r\nStatus: Downloaded newer image for tensorflow/tensorflow:nightly-py3\r\n$ docker run -it tensorflow/tensorflow:nightly-py3\r\n\r\n________                               _______________\r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ /\r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nWARNING: You are running this container as root, which can cause new files in\r\nmounted volumes to be created as the root user on your host machine.\r\n\r\nTo avoid this, run the container by specifying your user's userid:\r\n\r\n$ docker run -u $(id -u):$(id -g) args...\r\n\r\nroot@2cb8493a2618:/# python3\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17)\r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> with tf.Session() as sess:\r\n...     a = tf.Variable(tf.constant(0))\r\n...     b = tf.Variable(tf.constant(1))\r\n...     sess.run(tf.initializers.global_variables())\r\n...     sess.run(tf.stack([a, b])[::2])\r\n...\r\n2019-05-28 10:37:10.232881: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-05-28 10:37:10.237511: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3504000000 Hz\r\n2019-05-28 10:37:10.238901: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3c79bc0 executing computations on platform Host. Devices:\r\n2019-05-28 10:37:10.238957: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-28 10:37:10.251942: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1388] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\narray([0], dtype=int32)\r\n```\r\n\r\nSo it might be fixed in the latest nightly. But it's odd why this does not happen in colab. Could there be a version difference between colab and docker/pip?\r\n\r\n~~**Edit**: While nightly does seem to fix it in the repro example case, it seems like my main program (from which I found this minimal test case) still has the problem. Might it be that nightly will not trigger the arithmetic optimizer in this case?~~ Turns out 1.13.1 was installed in the container running my main program. This is due to the nightly container not having a tensorflow pip package installed in the docker image. As one of my dependencies had a dependency on tensorflow, it would install TF 1.13.1 from pip.\r\n\r\n**Edit 2**: The RemoveStackStridedSliceSameAxis has not been changed much since its implmentation, so I doubt it's actually fixed there between 1.13.1 and nightly (although I suppose it could be fixed in some function it depends on directly or indirectly).", "I have tried on Colab, Docker and local machine with TF 1.13.1 and have noticed that only docker reproduces the issue. Nightly does not have the issue even when tried through docker. ", "Are you sure the TF 1.13.1 on your local machine matches the pip version? I just tried it using a base ubuntu container with tensorflow installed via pip and still get the warning:\r\n\r\n```\r\n$ docker run -it ubuntu\r\nroot@022a5a7e621b:/# apt update\r\nGet:1 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\r\nGet:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\r\nGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\r\nGet:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\r\nGet:5 http://security.ubuntu.com/ubuntu bionic-security/universe Sources [157 kB]\r\nGet:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [322 kB]\r\nGet:7 http://archive.ubuntu.com/ubuntu bionic/universe Sources [11.5 MB]\r\nGet:8 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [4168 B]\r\nGet:9 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [470 kB]\r\nGet:10 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [5436 B]\r\nGet:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\r\nGet:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\r\nGet:13 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\r\nGet:14 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\r\nGet:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe Sources [319 kB]\r\nGet:16 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [7236 B]\r\nGet:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [10.8 kB]\r\nGet:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1213 kB]\r\nGet:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [825 kB]\r\nGet:20 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3902 B]\r\nGet:21 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [2496 B]\r\nFetched 28.2 MB in 3s (10.8 MB/s)\r\nReading package lists... Done\r\nBuilding dependency tree\r\nReading state information... Done\r\n46 packages can be upgraded. Run 'apt list --upgradable' to see them.\r\nroot@022a5a7e621b:/# apt install python3-pip\r\nReading package lists... Done\r\nBuilding dependency tree\r\nReading state information... Done\r\nThe following additional packages will be installed:\r\n  binutils binutils-common binutils-x86-64-linux-gnu build-essential\r\n  ca-certificates cpp cpp-7 dbus dh-python dirmngr dpkg-dev fakeroot file g++\r\n  g++-7 gcc gcc-7 gcc-7-base gcc-8-base gir1.2-glib-2.0 gnupg gnupg-l10n\r\n  gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm gpgv\r\n  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\r\n  libapparmor1 libasan4 libasn1-8-heimdal libassuan0 libatomic1 libbinutils\r\n  libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libdbus-1-3 libdpkg-perl\r\n  libexpat1 libexpat1-dev libfakeroot libfile-fcntllock-perl libgcc-7-dev\r\n  libgcc1 libgdbm-compat4 libgdbm5 libgirepository-1.0-1 libglib2.0-0\r\n  libglib2.0-data libgomp1 libgssapi3-heimdal libhcrypto4-heimdal\r\n  libheimbase1-heimdal libheimntlm0-heimdal libhx509-5-heimdal libicu60\r\n  libisl19 libitm1 libkrb5-26-heimdal libksba8 libldap-2.4-2 libldap-common\r\n  liblocale-gettext-perl liblsan0 libmagic-mgc libmagic1 libmpc3 libmpdec2\r\n  libmpfr6 libmpx2 libnpth0 libperl5.26 libpython3-dev libpython3-stdlib\r\n  libpython3.6 libpython3.6-dev libpython3.6-minimal libpython3.6-stdlib\r\n  libquadmath0 libreadline7 libroken18-heimdal libsasl2-2 libsasl2-modules\r\n  libsasl2-modules-db libsqlite3-0 libssl1.1 libstdc++-7-dev libstdc++6\r\n  libtsan0 libubsan0 libwind0-heimdal libxml2 linux-libc-dev make manpages\r\n  manpages-dev mime-support netbase openssl patch perl perl-base\r\n  perl-modules-5.26 pinentry-curses python-pip-whl python3 python3-asn1crypto\r\n  python3-cffi-backend python3-crypto python3-cryptography python3-dbus\r\n  python3-dev python3-distutils python3-gi python3-idna python3-keyring\r\n  python3-keyrings.alt python3-lib2to3 python3-minimal python3-pkg-resources\r\n  python3-secretstorage python3-setuptools python3-six python3-wheel\r\n  python3-xdg python3.6 python3.6-dev python3.6-minimal readline-common\r\n  shared-mime-info xdg-user-dirs xz-utils\r\nSuggested packages:\r\n  binutils-doc cpp-doc gcc-7-locales default-dbus-session-bus\r\n  | dbus-session-bus dbus-user-session libpam-systemd pinentry-gnome3 tor\r\n  debian-keyring g++-multilib g++-7-multilib gcc-7-doc libstdc++6-7-dbg\r\n  gcc-multilib autoconf automake libtool flex bison gdb gcc-doc gcc-7-multilib\r\n  libgcc1-dbg libgomp1-dbg libitm1-dbg libatomic1-dbg libasan4-dbg\r\n  liblsan0-dbg libtsan0-dbg libubsan0-dbg libcilkrts5-dbg libmpx2-dbg\r\n  libquadmath0-dbg parcimonie xloadimage scdaemon glibc-doc git bzr gdbm-l10n\r\n  libsasl2-modules-gssapi-mit | libsasl2-modules-gssapi-heimdal\r\n  libsasl2-modules-ldap libsasl2-modules-otp libsasl2-modules-sql\r\n  libstdc++-7-doc make-doc man-browser ed diffutils-doc perl-doc\r\n  libterm-readline-gnu-perl | libterm-readline-perl-perl pinentry-doc\r\n  python3-doc python3-tk python3-venv python-crypto-doc\r\n  python-cryptography-doc python3-cryptography-vectors python-dbus-doc\r\n  python3-dbus-dbg gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0\r\n  python-secretstorage-doc python-setuptools-doc python3.6-venv python3.6-doc\r\n  binfmt-support readline-doc\r\nThe following NEW packages will be installed:\r\n  binutils binutils-common binutils-x86-64-linux-gnu build-essential\r\n  ca-certificates cpp cpp-7 dbus dh-python dirmngr dpkg-dev fakeroot file g++\r\n  g++-7 gcc gcc-7 gcc-7-base gir1.2-glib-2.0 gnupg gnupg-l10n gnupg-utils gpg\r\n  gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm libalgorithm-diff-perl\r\n  libalgorithm-diff-xs-perl libalgorithm-merge-perl libapparmor1 libasan4\r\n  libasn1-8-heimdal libassuan0 libatomic1 libbinutils libc-dev-bin libc6-dev\r\n  libcc1-0 libcilkrts5 libdbus-1-3 libdpkg-perl libexpat1 libexpat1-dev\r\n  libfakeroot libfile-fcntllock-perl libgcc-7-dev libgdbm-compat4 libgdbm5\r\n  libgirepository-1.0-1 libglib2.0-0 libglib2.0-data libgomp1\r\n  libgssapi3-heimdal libhcrypto4-heimdal libheimbase1-heimdal\r\n  libheimntlm0-heimdal libhx509-5-heimdal libicu60 libisl19 libitm1\r\n  libkrb5-26-heimdal libksba8 libldap-2.4-2 libldap-common\r\n  liblocale-gettext-perl liblsan0 libmagic-mgc libmagic1 libmpc3 libmpdec2\r\n  libmpfr6 libmpx2 libnpth0 libperl5.26 libpython3-dev libpython3-stdlib\r\n  libpython3.6 libpython3.6-dev libpython3.6-minimal libpython3.6-stdlib\r\n  libquadmath0 libreadline7 libroken18-heimdal libsasl2-2 libsasl2-modules\r\n  libsasl2-modules-db libsqlite3-0 libssl1.1 libstdc++-7-dev libtsan0\r\n  libubsan0 libwind0-heimdal libxml2 linux-libc-dev make manpages manpages-dev\r\n  mime-support netbase openssl patch perl perl-modules-5.26 pinentry-curses\r\n  python-pip-whl python3 python3-asn1crypto python3-cffi-backend\r\n  python3-crypto python3-cryptography python3-dbus python3-dev\r\n  python3-distutils python3-gi python3-idna python3-keyring\r\n  python3-keyrings.alt python3-lib2to3 python3-minimal python3-pip\r\n  python3-pkg-resources python3-secretstorage python3-setuptools python3-six\r\n  python3-wheel python3-xdg python3.6 python3.6-dev python3.6-minimal\r\n  readline-common shared-mime-info xdg-user-dirs xz-utils\r\nThe following packages will be upgraded:\r\n  gcc-8-base gpgv libgcc1 libstdc++6 perl-base\r\n5 upgraded, 135 newly installed, 0 to remove and 41 not upgraded.\r\nNeed to get 119 MB of archives.\r\nAfter this operation, 388 MB of additional disk space will be used.\r\nDo you want to continue? [Y/n]\r\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 perl-base amd64 5.26.1-6ubuntu0.3 [1390 kB]\r\nGet:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblocale-gettext-perl amd64 1.07-3build2 [16.6 kB]\r\nGet:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libssl1.1 amd64 1.1.0g-2ubuntu4.3 [1130 kB]\r\nGet:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6-minimal amd64 3.6.7-1~18.04 [531 kB]\r\nGet:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libexpat1 amd64 2.2.5-3 [80.2 kB]\r\nGet:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3.6-minimal amd64 3.6.7-1~18.04 [1604 kB]\r\nGet:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-minimal amd64 3.6.7-1~18.04 [23.7 kB]\r\nGet:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 mime-support all 3.60ubuntu1 [30.1 kB]\r\nGet:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmpdec2 amd64 2.4.2-1ubuntu1 [84.1 kB]\r\nGet:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 readline-common all 7.0-3 [52.9 kB]\r\nGet:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libreadline7 amd64 7.0-3 [124 kB]\r\nGet:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsqlite3-0 amd64 3.22.0-1 [496 kB]\r\nGet:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6-stdlib amd64 3.6.7-1~18.04 [1711 kB]\r\nGet:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3.6 amd64 3.6.7-1~18.04 [197 kB]\r\nGet:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3-stdlib amd64 3.6.7-1~18.04 [7240 B]\r\nGet:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3 amd64 3.6.7-1~18.04 [47.2 kB]\r\nGet:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 perl-modules-5.26 all 5.26.1-6ubuntu0.3 [2763 kB]\r\nGet:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgdbm5 amd64 1.14.1-6 [26.0 kB]\r\nGet:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgdbm-compat4 amd64 1.14.1-6 [6084 B]\r\nGet:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libperl5.26 amd64 5.26.1-6ubuntu0.3 [3527 kB]\r\nGet:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 perl amd64 5.26.1-6ubuntu0.3 [201 kB]\r\nGet:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gcc-8-base amd64 8.3.0-6ubuntu1~18.04 [18.6 kB]\r\nGet:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libstdc++6 amd64 8.3.0-6ubuntu1~18.04 [400 kB]\r\nGet:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgcc1 amd64 1:8.3.0-6ubuntu1~18.04 [40.7 kB]\r\nGet:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gpgv amd64 2.2.4-1ubuntu1.2 [198 kB]\r\nGet:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssl amd64 1.1.0g-2ubuntu4.3 [532 kB]\r\nGet:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 ca-certificates all 20180409 [151 kB]\r\nGet:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libapparmor1 amd64 2.12-4ubuntu5.1 [31.3 kB]\r\nGet:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdbus-1-3 amd64 1.12.2-1ubuntu1 [175 kB]\r\nGet:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 dbus amd64 1.12.2-1ubuntu1 [150 kB]\r\nGet:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.2 [184 kB]\r\nGet:32 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.2 [68.5 kB]\r\nGet:33 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.2 [22.1 kB]\r\nGet:34 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglib2.0-0 amd64 2.56.4-0ubuntu0.18.04.2 [1169 kB]\r\nGet:35 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgirepository-1.0-1 amd64 1.56.1-1 [82.0 kB]\r\nGet:36 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-glib-2.0 amd64 1.56.1-1 [131 kB]\r\nGet:37 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglib2.0-data all 2.56.4-0ubuntu0.18.04.2 [4720 B]\r\nGet:38 http://archive.ubuntu.com/ubuntu bionic/main amd64 libicu60 amd64 60.2-3ubuntu3 [8054 kB]\r\nGet:39 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxml2 amd64 2.9.4+dfsg1-6.1ubuntu1.2 [663 kB]\r\nGet:40 http://archive.ubuntu.com/ubuntu bionic/main amd64 netbase all 5.4 [12.7 kB]\r\nGet:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-dbus amd64 1.2.6-1 [89.9 kB]\r\nGet:42 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-gi amd64 3.26.1-2ubuntu1 [153 kB]\r\nGet:43 http://archive.ubuntu.com/ubuntu bionic/main amd64 shared-mime-info amd64 1.9-2 [426 kB]\r\nGet:44 http://archive.ubuntu.com/ubuntu bionic/main amd64 xdg-user-dirs amd64 0.17-1ubuntu1 [48.0 kB]\r\nGet:45 http://archive.ubuntu.com/ubuntu bionic/main amd64 xz-utils amd64 5.2.2-1.3 [83.8 kB]\r\nGet:46 http://archive.ubuntu.com/ubuntu bionic/main amd64 manpages all 4.15-1 [1234 kB]\r\nGet:47 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-common amd64 2.30-21ubuntu1~18.04.1 [193 kB]\r\nGet:48 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbinutils amd64 2.30-21ubuntu1~18.04.1 [501 kB]\r\nGet:49 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.30-21ubuntu1~18.04.1 [1854 kB]\r\nGet:50 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils amd64 2.30-21ubuntu1~18.04.1 [3396 B]\r\nGet:51 http://archive.ubuntu.com/ubuntu bionic/main amd64 libc-dev-bin amd64 2.27-3ubuntu1 [71.8 kB]\r\nGet:52 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-51.55 [1005 kB]\r\nGet:53 http://archive.ubuntu.com/ubuntu bionic/main amd64 libc6-dev amd64 2.27-3ubuntu1 [2587 kB]\r\nGet:54 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gcc-7-base amd64 7.4.0-1ubuntu1~18.04 [19.0 kB]\r\nGet:55 http://archive.ubuntu.com/ubuntu bionic/main amd64 libisl19 amd64 0.19-1 [551 kB]\r\nGet:56 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmpfr6 amd64 4.0.1-1 [243 kB]\r\nGet:57 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmpc3 amd64 1.1.0-1 [40.8 kB]\r\nGet:58 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 cpp-7 amd64 7.4.0-1ubuntu1~18.04 [6740 kB]\r\nGet:59 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 cpp amd64 4:7.4.0-1ubuntu2.2 [27.6 kB]\r\nGet:60 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcc1-0 amd64 8.3.0-6ubuntu1~18.04 [47.4 kB]\r\nGet:61 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgomp1 amd64 8.3.0-6ubuntu1~18.04 [76.4 kB]\r\nGet:62 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libitm1 amd64 8.3.0-6ubuntu1~18.04 [27.9 kB]\r\nGet:63 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libatomic1 amd64 8.3.0-6ubuntu1~18.04 [9180 B]\r\nGet:64 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libasan4 amd64 7.4.0-1ubuntu1~18.04 [359 kB]\r\nGet:65 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 liblsan0 amd64 8.3.0-6ubuntu1~18.04 [133 kB]\r\nGet:66 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtsan0 amd64 8.3.0-6ubuntu1~18.04 [288 kB]\r\nGet:67 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libubsan0 amd64 7.4.0-1ubuntu1~18.04 [126 kB]\r\nGet:68 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcilkrts5 amd64 7.4.0-1ubuntu1~18.04 [42.5 kB]\r\nGet:69 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmpx2 amd64 8.3.0-6ubuntu1~18.04 [11.6 kB]\r\nGet:70 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libquadmath0 amd64 8.3.0-6ubuntu1~18.04 [134 kB]\r\nGet:71 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgcc-7-dev amd64 7.4.0-1ubuntu1~18.04 [2381 kB]\r\nGet:72 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gcc-7 amd64 7.4.0-1ubuntu1~18.04 [7462 kB]\r\nGet:73 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gcc amd64 4:7.4.0-1ubuntu2.2 [5184 B]\r\nGet:74 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libstdc++-7-dev amd64 7.4.0-1ubuntu1~18.04 [1467 kB]\r\nGet:75 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 g++-7 amd64 7.4.0-1ubuntu1~18.04 [7575 kB]\r\nGet:76 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 g++ amd64 4:7.4.0-1ubuntu2.2 [1568 B]\r\nGet:77 http://archive.ubuntu.com/ubuntu bionic/main amd64 make amd64 4.1-9.1ubuntu1 [154 kB]\r\nGet:78 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdpkg-perl all 1.19.0.5ubuntu2.1 [211 kB]\r\nGet:79 http://archive.ubuntu.com/ubuntu bionic/main amd64 patch amd64 2.7.6-2ubuntu1 [102 kB]\r\nGet:80 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 dpkg-dev all 1.19.0.5ubuntu2.1 [608 kB]\r\nGet:81 http://archive.ubuntu.com/ubuntu bionic/main amd64 build-essential amd64 12.4ubuntu1 [4758 B]\r\nGet:82 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-lib2to3 all 3.6.7-1~18.04 [76.5 kB]\r\nGet:83 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-distutils all 3.6.7-1~18.04 [141 kB]\r\nGet:84 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-python all 3.20180325ubuntu2 [89.2 kB]\r\nGet:85 http://archive.ubuntu.com/ubuntu bionic/main amd64 libassuan0 amd64 2.5.1-2 [35.0 kB]\r\nGet:86 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gpgconf amd64 2.2.4-1ubuntu1.2 [123 kB]\r\nGet:87 http://archive.ubuntu.com/ubuntu bionic/main amd64 libksba8 amd64 1.3.5-2 [92.6 kB]\r\nGet:88 http://archive.ubuntu.com/ubuntu bionic/main amd64 libroken18-heimdal amd64 7.5.0+dfsg-1 [41.3 kB]\r\nGet:89 http://archive.ubuntu.com/ubuntu bionic/main amd64 libasn1-8-heimdal amd64 7.5.0+dfsg-1 [175 kB]\r\nGet:90 http://archive.ubuntu.com/ubuntu bionic/main amd64 libheimbase1-heimdal amd64 7.5.0+dfsg-1 [29.3 kB]\r\nGet:91 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhcrypto4-heimdal amd64 7.5.0+dfsg-1 [85.9 kB]\r\nGet:92 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwind0-heimdal amd64 7.5.0+dfsg-1 [47.8 kB]\r\nGet:93 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhx509-5-heimdal amd64 7.5.0+dfsg-1 [107 kB]\r\nGet:94 http://archive.ubuntu.com/ubuntu bionic/main amd64 libkrb5-26-heimdal amd64 7.5.0+dfsg-1 [206 kB]\r\nGet:95 http://archive.ubuntu.com/ubuntu bionic/main amd64 libheimntlm0-heimdal amd64 7.5.0+dfsg-1 [14.8 kB]\r\nGet:96 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgssapi3-heimdal amd64 7.5.0+dfsg-1 [96.5 kB]\r\nGet:97 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsasl2-modules-db amd64 2.1.27~101-g0780600+dfsg-3ubuntu2 [14.8 kB]\r\nGet:98 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsasl2-2 amd64 2.1.27~101-g0780600+dfsg-3ubuntu2 [49.2 kB]\r\nGet:99 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-common all 2.4.45+dfsg-1ubuntu1.2 [16.7 kB]\r\nGet:100 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-2.4-2 amd64 2.4.45+dfsg-1ubuntu1.2 [155 kB]\r\nGet:101 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnpth0 amd64 1.5-3 [7668 B]\r\nGet:102 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 dirmngr amd64 2.2.4-1ubuntu1.2 [316 kB]\r\nGet:103 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfakeroot amd64 1.22-2ubuntu1 [25.9 kB]\r\nGet:104 http://archive.ubuntu.com/ubuntu bionic/main amd64 fakeroot amd64 1.22-2ubuntu1 [62.3 kB]\r\nGet:105 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gnupg-l10n all 2.2.4-1ubuntu1.2 [49.6 kB]\r\nGet:106 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gnupg-utils amd64 2.2.4-1ubuntu1.2 [127 kB]\r\nGet:107 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gpg amd64 2.2.4-1ubuntu1.2 [467 kB]\r\nGet:108 http://archive.ubuntu.com/ubuntu bionic/main amd64 pinentry-curses amd64 1.1.0-1 [35.8 kB]\r\nGet:109 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gpg-agent amd64 2.2.4-1ubuntu1.2 [227 kB]\r\nGet:110 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gpg-wks-client amd64 2.2.4-1ubuntu1.2 [91.9 kB]\r\nGet:111 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gpg-wks-server amd64 2.2.4-1ubuntu1.2 [84.9 kB]\r\nGet:112 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gpgsm amd64 2.2.4-1ubuntu1.2 [215 kB]\r\nGet:113 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gnupg amd64 2.2.4-1ubuntu1.2 [249 kB]\r\nGet:114 http://archive.ubuntu.com/ubuntu bionic/main amd64 libalgorithm-diff-perl all 1.19.03-1 [47.6 kB]\r\nGet:115 http://archive.ubuntu.com/ubuntu bionic/main amd64 libalgorithm-diff-xs-perl amd64 0.04-5 [11.1 kB]\r\nGet:116 http://archive.ubuntu.com/ubuntu bionic/main amd64 libalgorithm-merge-perl all 0.08-3 [12.0 kB]\r\nGet:117 http://archive.ubuntu.com/ubuntu bionic/main amd64 libexpat1-dev amd64 2.2.5-3 [122 kB]\r\nGet:118 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfile-fcntllock-perl amd64 0.22-3build2 [33.2 kB]\r\nGet:119 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6 amd64 3.6.7-1~18.04 [1415 kB]\r\nGet:120 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6-dev amd64 3.6.7-1~18.04 [44.8 MB]\r\nGet:121 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3-dev amd64 3.6.7-1~18.04 [7328 B]\r\nGet:122 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsasl2-modules amd64 2.1.27~101-g0780600+dfsg-3ubuntu2 [48.7 kB]\r\nGet:123 http://archive.ubuntu.com/ubuntu bionic/main amd64 manpages-dev all 4.15-1 [2217 kB]\r\nGet:124 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1 [1652 kB]\r\nGet:125 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-asn1crypto all 0.24.0-1 [72.8 kB]\r\nGet:126 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-cffi-backend amd64 1.11.5-1 [64.6 kB]\r\nGet:127 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-crypto amd64 2.6.1-8ubuntu2 [244 kB]\r\nGet:128 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\r\nGet:129 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\r\nGet:130 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-cryptSetting up python3-pip (9.0.1-2.3~ubuntu1) ...\r\nSetting up g++ (4:7.4.0-1ubuntu2.2) ...\r\nupdate-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\r\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/c++.1.gz because associated file /usr/share/man/man1/g++.1.gz (of link group c++) doesn't exist\r\nSetting up python3-setuptools (39.0.1-2) ...\r\nSetting up python3-secretstorage (2.3.1-2) ...\r\nSetting up dirmngr (2.2.4-1ubuntu1.2) ...\r\nSetting up dh-python (3.20180325ubuntu2) ...\r\nSetting up python3-keyring (10.6.0-1) ...\r\nSetting up build-essential (12.4ubuntu1) ...\r\nSetting up python3-dev (3.6.7-1~18.04) ...\r\nSetting up gpg-wks-client (2.2.4-1ubuntu1.2) ...\r\nSetting up gnupg (2.2.4-1ubuntu1.2) ...\r\nProcessing triggers for libc-bin (2.27-3ubuntu1) ...\r\nProcessing triggers for ca-certificates (20180409) ...\r\nUpdating certificates in /etc/ssl/certs...\r\n0 added, 0 removed; done.\r\nRunning hooks in /etc/ca-certificates/update.d...\r\ndone.\r\nroot@022a5a7e621b:/# pip3 install tensorflow\r\nCollecting tensorflow\r\n  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\r\n    100% |################################| 92.5MB 14kB/s\r\nCollecting absl-py>=0.1.6 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\r\n    100% |################################| 102kB 4.6MB/s\r\nCollecting numpy>=1.13.3 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\r\n    100% |################################| 17.3MB 112kB/s\r\nCollecting grpcio>=1.8.6 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/99/83/18f374294bf34128a448ee2fae37651f943b0b5fa473b5b3aff262c15bf8/grpcio-1.21.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\r\n    100% |################################| 2.2MB 889kB/s\r\nCollecting termcolor>=1.1.0 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\r\nCollecting keras-preprocessing>=1.0.5 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\r\n    100% |################################| 51kB 5.8MB/s\r\nCollecting astor>=0.6.0 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\r\nRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow)\r\nCollecting gast>=0.2.0 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\r\nCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\r\n    100% |################################| 3.2MB 616kB/s\r\nCollecting keras-applications>=1.0.6 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\r\n    100% |################################| 51kB 6.1MB/s\r\nCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\r\n    100% |################################| 368kB 4.2MB/s\r\nRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow)\r\nCollecting protobuf>=3.6.1 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/d2/fb/29de8d08967f0cce1bb10b39846d836b0f3bf6776ddc36aed7c73498ca7e/protobuf-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\r\n    100% |################################| 1.2MB 1.5MB/s\r\nCollecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/9f/57/92a497e38161ce40606c27a86759c6b92dd34fcdb33f64171ec559257c02/Werkzeug-0.15.4-py2.py3-none-any.whl (327kB)\r\n    100% |################################| 327kB 4.2MB/s\r\nCollecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\r\n    100% |################################| 92kB 8.4MB/s\r\nCollecting h5py (from keras-applications>=1.0.6->tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\r\n    100% |################################| 2.8MB 637kB/s\r\nCollecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.6.1->tensorflow)\r\nBuilding wheels for collected packages: absl-py, termcolor, gast\r\n  Running setup.py bdist_wheel for absl-py ... done\r\n  Stored in directory: /root/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\r\n  Running setup.py bdist_wheel for termcolor ... done\r\n  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\r\n  Running setup.py bdist_wheel for gast ... done\r\n  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\r\nSuccessfully built absl-py termcolor gast\r\nInstalling collected packages: absl-py, numpy, grpcio, termcolor, keras-preprocessing, astor, gast, werkzeug, protobuf, markdown, tensorboard, h5py, keras-applications, mock, tensorflow-estimator, tensorflow\r\nSuccessfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 grpcio-1.21.1 h5py-2.9.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 mock-3.0.5 numpy-1.16.4 protobuf-3.8.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 werkzeug-0.15.4\r\nroot@022a5a7e621b:/# python3\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17)\r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> with tf.Session() as sess:\r\n...     a = tf.Variable(tf.constant(0))\r\n...     b = tf.Variable(tf.constant(1))\r\n...     sess.run(tf.initializers.global_variables())\r\n...     sess.run(tf.stack([a, b])[::2])\r\n...\r\n2019-06-05 06:59:49.673988: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-05 06:59:49.678262: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3504000000 Hz\r\n2019-06-05 06:59:49.679019: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1fb2f60 executing computations on platform Host. Devices:\r\n2019-06-05 06:59:49.679066: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-06-05 06:59:49.695569: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice. Error: Pack node (stack) axis attribute is out of bounds: 0\r\n2019-06-05 06:59:49.697161: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node strided_slice. Error: Pack node (stack) axis attribute is out of bounds: 0\r\narray([0], dtype=int32)\r\n```", "Seems to be fixed in 1.14:\r\n\r\n```\r\n$ docker run -it tensorflow/tensorflow:1.14.0-py3\r\nUnable to find image 'tensorflow/tensorflow:1.14.0-py3' locally\r\n1.14.0-py3: Pulling from tensorflow/tensorflow\r\n5b7339215d1d: Pull complete\r\n14ca88e9f672: Pull complete\r\na31c3b1caad4: Pull complete\r\nb054a26005b7: Pull complete\r\n8832e3773578: Pull complete\r\n5e671b828b2a: Pull complete\r\n2b940936f993: Pull complete\r\n016724bbd2c9: Pull complete\r\n5bd1cb597025: Pull complete\r\n68543864d644: Pull complete\r\nDigest: sha256:7c05dfab9ea82c95b571ca7dc3d92d4e0b289eeec6b3abf51f189caca0684488\r\nStatus: Downloaded newer image for tensorflow/tensorflow:1.14.0-py3\r\n\r\n________                               _______________\r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ /\r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nWARNING: You are running this container as root, which can cause new files in\r\nmounted volumes to be created as the root user on your host machine.\r\n\r\nTo avoid this, run the container by specifying your user's userid:\r\n\r\n$ docker run -u $(id -u):$(id -g) args...\r\n\r\nroot@17f576db50a9:/# python3\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34)\r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> with tf.Session() as sess:\r\n...     a = tf.Variable(tf.constant(0))\r\n...     b = tf.Variable(tf.constant(1))\r\n...     sess.run(tf.initializers.global_variables())\r\n...     sess.run(tf.stack([a, b])[::2])\r\n...\r\n2019-06-24 10:04:02.780483: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-24 10:04:02.786583: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3504000000 Hz\r\n2019-06-24 10:04:02.787030: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3600a30 executing computations on platform Host. Devices:\r\n2019-06-24 10:04:02.787067: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-24 10:04:02.801528: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\narray([0], dtype=int32)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29052\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29052\">No</a>\n"]}, {"number": 29051, "title": "How to avoid asking when executing \"python ./configure.py\"", "body": "This is not a truely issue with building or installing TF, but a important question for us.\r\n\r\nWhen we execute \"python ./configure.py\" it will ask the dependencies, configuration and others, we have to manually confirm. What I should do to have \"python ./configure.py\" read the default parameters(we use default parameters) directly and don't need us to confirm. Or is there some parameters to avoid these questions?\r\n\r\nBecause we need to integrate the build steps into a script for daily build testing, every time we run the script it will empty the configuration and get the latest source to rebuild the TF , but \"python ./configure.py\" will prevent the script from continuing, \r\n\r\nThis question is important to us. Can you provide solution to solve this problem? Thanks in advance.", "comments": ["We set environment variables to change the defaults o configure script, and then run it with `yes \"\" | $PYTHON_BIN_PATH configure.py` You can see an example of this here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/gpu/run_py3_core.sh", "Thanks for you information, now Tensorflow can be successfully built with script."]}, {"number": 29050, "title": "Fix a slip of the pen", "body": "", "comments": []}, {"number": 29049, "title": "Big bug, tf.keras is slower then keras.", "body": "I just change all keras api to tf.keras. But training become too slower, gpu is usable\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact minimal code snippet to reproduce the issue reported. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "`https://github.com/tsing-cv/tf.keras_Mask_rcnn`\r\nI didnt change anything, but instead of all keras api. \r\nThe performance drops down, and training is very very slow.\r\n@achandraa \r\n", "Can we have the details about the platform you are using (operating system, architecture). Also let us know which TensorFlow version you are using, whether you compiled it using source or installed using binary and your CUDA/cuDNN, GPU information. This will help us to dig further and proceed easily. Thanks!  ", "\r\nubuntu18.04 x86\r\nTITAN RTX 24G\r\ncuda 10.0\r\ncudnn 7.3.1\r\nINVIDIA driver 418.43\r\ntensorflow 1.12 or 1.13.1\r\nkeras 2.2.4\r\n\r\nI used conda install tensorflow-gpu  ", "Can you please use profiling tool and help us figure out what operations consume most time in computing? Thanks!", "OK, I will take a few hours to use this tool, to address the wasting time ops.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Speed has nothing to do with multi_preprocess and data io.\r\nWhen executing python coco.py evaluate, tf.keras is slower than keras. "]}, {"number": 29048, "title": "the output of `tf.sigmoid` is abnormal when input has nans.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 64bit, AWS EC2 p3.2xlarge instance\r\n- TensorFlow installed from (source or binary): `pip install tensorflow-gpu`\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA Tesla V100 16GB\r\n\r\n**Describe the current behavior**\r\n\r\nthe output of the `tf.sigmoid` function seems abnormal when the input has nans.\r\n```python\r\nIn [1]: import tensorflow as tf\r\nIn [2]: tf.enable_eager_execution()\r\nIn [3]: a = tf.constant([float('nan')]*5)\r\nIn [4]: tf.sigmoid(a)\r\nOut[4]: <tf.Tensor: id=1, shape=(5,), dtype=float32, numpy=array([ 1.,  1.,  1.,  1., nan], dtype=float32)>\r\n```\r\n**Describe the expected behavior**\r\n```python\r\nIn [4]: tf.sigmoid(a)\r\nOut[4]: <tf.Tensor: id=1, shape=(5,), dtype=float32, numpy=array([ nan,  nan,  nan,  nan, nan], dtype=float32)>\r\n```\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\na = tf.constant([float('nan')]*5)\r\nb = tf.sigmoid(a)\r\nprint(b)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@r05323028 Able to reproduce the issue in 1.13.1 and 2.0.0-alpha0", "I am unable to reproduce this issue with `1.13.1` (cpu version).\r\nPython version: `3.7.1` on MacOS.\r\n\r\n![image](https://user-images.githubusercontent.com/32885872/58580406-e9c39d00-8201-11e9-9045-5b9cf6543994.png)\r\n", "@r05323028 I agree. The output is not normal when you use T1.13.1. Gist is [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/08420246eaf7319304607499f604091d/tf29048_1p13.ipynb)\r\nOutput: `tf.Tensor([ 1.  1.  1.  1. nan], shape=(5,), dtype=float32)`\r\n\r\nBut in TF2.0.0-alpha0, everything works as expected. Gist is [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/3b9cb892db87a9ae38dfae8d5e56a0ad/tf29048.ipynb).\r\nOutput: `tf.Tensor([nan nan nan nan nan], shape=(5,), dtype=float32)`\r\n\r\nCan you upgrade to TF2.0? Thanks!", "@kmh4321 @jvishnuvardhan Thanks for your response!\r\nI tried TF2.0 on `AWS ec2 p3.2xlarge` gpu instance which uses `NVIDIA Tesla V100-SXM2-16GB` but it still has the same problem.\r\n\r\n<img width=\"1236\" alt=\"\u87a2\u5e55\u5feb\u7167 2019-05-30 \u4e0b\u53481 32 53\" src=\"https://user-images.githubusercontent.com/37167503/58610851-9c1d6200-82df-11e9-9cdf-080ae1439b36.png\">\r\n\r\nMachine's spec is as follows\r\n\r\n<img width=\"1234\" alt=\"\u87a2\u5e55\u5feb\u7167 2019-05-30 \u4e0b\u53481 33 22\" src=\"https://user-images.githubusercontent.com/37167503/58610942-eacafc00-82df-11e9-8837-9692afa4c45c.png\">\r\n", "@r05323028 Sorry for my mistake. I ran tf-cpu version in GPU and I don't see any error so I reported that as my last response. However, i can reproduce the issue with tf-gpu version. Thanks for finding this. Thanks!", "@rmlarsen @azaks2 this is the known issue around GPU kernels sometimes losing NaNs altogether, right?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "@r05323028 I think this is resolved in recent `tf-nightly-gpu`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/308ea7491c7923cc25705036afebc46f/tf29048.ipynb).\r\n\r\nOutput is \r\n`tf.Tensor([nan nan nan nan nan], shape=(5,), dtype=float32)`\r\n\r\nI am closing this issue. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29048\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29048\">No</a>\n"]}, {"number": 29047, "title": "one 404 page needs to be fixed", "body": "At the beginning of sub title [Train from tf.data datasets](https://www.tensorflow.org/alpha/guide/keras/overview#train_from_tfdata_datasets), there is a super link pointing to **Datasets API**.\r\n\r\nBut when you click it out, only 404 page is what you get.\r\n\r\nSince this API is very often been used, it would be better to correct this as soon as possible.", "comments": ["@duanw0916 Thanks for finding this broken link. I am closing this issue as PR related to this issue has been merged to `master` branch. Thanks!"]}, {"number": 29046, "title": "Website claims that there is no internet connection", "body": "JavaScript on the website runs some sort of detection to see if there is network connectivity or tries to establish a connection in a surprising way.\r\n\r\nThis fails and I get a message \"There is no Internet connection :(\" which is clearly wrong. I am writing this issue with the same internet connection.\r\n\r\nThe JS console says:\r\n\r\n    Failed to load \u2018https://www.google-analytics.com/analytics.js\u2019. A ServiceWorker passed a promise to FetchEvent.respondWith() that rejected with \u2018TypeError: NetworkError when attempting to fetch resource.\u2019.\r\n\r\n    Failed to load \u2018https://www.tensorflow.org/_d/profile/ogb\u2019. A ServiceWorker passed a promise to FetchEvent.respondWith() that rejected with \u2018TypeError: NetworkError when attempting to fetch resource.\u2019.\r\n\r\n    Failed to load \u2018https://www.tensorflow.org/_d/profile/user\u2019. A ServiceWorker passed a promise to FetchEvent.respondWith() that rejected with \u2018TypeError: NetworkError when attempting to fetch resource.\u2019.\r\n", "comments": ["I was facing exactly the same issue. Turns out it was caused by one of my browser's plugin which blocks trackers (Privacy Badger). It was blocking the loading of `https://www.tensorflow.org/_d/profile...`. Turning it off solved the issue for me.", "Automatically closing this out since I understand it to be resolved by @Breta01 suggestion, but please let me know if I'm mistaken.Thanks!", "Well, uninstalling privacy badger does make the website functional, but that means that anyone that wants to use the official documentation has to accept google analytics? Sounds like you're paying with your private data to use the documentation.", "Dealing with the same problem with Firebase docs, but disabling my adblocker (uBlock Origin) has no effect. Works fine in private browsing with the adblocker though."]}, {"number": 29045, "title": "[Intel MKL] Update TF to fuse Conv2d+Relu6/Elu", "body": "We first disable Conv2d+Relu6/Elu in MKL build as lack of the backend support, now add the support in backend and remove the limitation in Conv2d fusion.", "comments": ["@penpornk thanks for the comments, I have update my code based on your comments. Please help to check, thanks."]}, {"number": 29044, "title": "cuda9.1 can not support tensorflow", "body": "import tensorflow as tf\r\nTraceback (most recent call last):\r\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in\r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in\r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\Program Files\\Python36\\lib\\imp.py\", line 243, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\Program Files\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\nreturn _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n", "comments": ["I resolved it by the flowing  link\r\n- https://github.com/tensorflow/tensorflow/issues/26870\r\n- https://blog.csdn.net/qq_36556893/article/details/79433298", "@dorothy9297 Good to know the issue is resolved. Will close the issue. Thanks!"]}, {"number": 29043, "title": "[TF2.0] tf.feature_column.shared_embeddings trace twice and throw exception with @tf.function", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): osx 10.13.1 (17B1003)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tf-nightly-2.0-preview           2.0.0.dev20190506 \r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n[shared_embeddings](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/shared_embeddings) not surport eager mode, so i call it with @tf.function, but still throw exception:\r\nValueError: Variable color_color2_color3_shared_embedding already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?\r\n\r\nFind the reason is that the function will be traced twice after add line `df =  tf.keras.layers.DenseFeatures(color_column_embed)(color_data)`,  the print will run twice. \r\n\r\nfirst time stack:\r\n```\r\nembedding_weights, feature_column_v2.py:3247\r\n_get_dense_tensor_internal, feature_column_v2.py:3326\r\nget_dense_tensor, feature_column_v2.py:3349\r\n_call_unconverted, api.py:173\r\nconverted_call, api.py:271\r\nloop_body, tmpf4q73us7.py:51\r\n_py_for_stmt, control_flow.py:111\r\nfor_stmt, control_flow.py:102\r\ntf__call, tmpf4q73us7.py:70\r\nconverted_call, api.py:375\r\nwrapper, api.py:89\r\n__call__, base_layer.py:632\r\n_call_unconverted, api.py:173\r\nconverted_call, api.py:271\r\ntf__shared_embedding_column_with_hash_bucket, tmp5d_py4a8.py:16\r\nconverted_call, api.py:375\r\nwrapper, func_graph.py:705\r\nwrapped_fn, def_function.py:292\r\nfunc_graph_from_py_func, func_graph.py:713\r\n_create_graph_function, function.py:1529\r\n_maybe_define_function, function.py:1596\r\n_get_concrete_function_internal_garbage_collected, function.py:1333\r\n_initialize, def_function.py:342\r\n__call__, def_function.py:399\r\n<module>, main.py:43\r\n```\r\n\r\nsecond time call stack:\r\n```\r\nembedding_weights, feature_column_v2.py:3247\r\n_get_dense_tensor_internal, feature_column_v2.py:3326\r\nget_dense_tensor, feature_column_v2.py:3349\r\n_call_unconverted, api.py:173\r\nconverted_call, api.py:271\r\nloop_body, tmpf4q73us7.py:51\r\n_py_for_stmt, control_flow.py:111\r\nfor_stmt, control_flow.py:102\r\ntf__call, tmpf4q73us7.py:70\r\nconverted_call, api.py:375\r\nwrapper, api.py:89\r\n__call__, base_layer.py:632\r\n_call_unconverted, api.py:173\r\nconverted_call, api.py:271\r\ntf__shared_embedding_column_with_hash_bucket, tmp5d_py4a8.py:16\r\nconverted_call, api.py:375\r\nwrapper, func_graph.py:705\r\nwrapped_fn, def_function.py:292\r\nfunc_graph_from_py_func, func_graph.py:713\r\n_create_graph_function, function.py:1529\r\n_maybe_define_function, function.py:1596\r\n__call__, function.py:1307\r\n__call__, def_function.py:411\r\n<module>, main.py:43\r\n```\r\n\r\nThe only difference is before `_maybe_define_function, function.py:1596`.\r\n\r\n\r\nThe code in second time fails,  because it is new obj , so there's no cache in self._embedding_weights, then will run `variable_scope.get_variable` with no reuse:\r\n```\r\n  @property\r\n  def embedding_weights(self):\r\n    key = ops.get_default_graph()._graph_key  # pylint: disable=protected-access\r\n    if key not in self._embedding_weights:\r\n      embedding_shape = (self._num_buckets, self._dimension)\r\n      var = variable_scope.get_variable(\r\n          name=self._name,\r\n          shape=embedding_shape,\r\n          dtype=dtypes.float32,\r\n          initializer=self._initializer,\r\n          trainable=self._trainable)\r\n\r\n      if self._ckpt_to_load_from is not None:\r\n        to_restore = var\r\n        if isinstance(to_restore, variables.PartitionedVariable):\r\n          to_restore = to_restore._get_variable_list()  # pylint: disable=protected-access\r\n        checkpoint_utils.init_from_checkpoint(\r\n            self._ckpt_to_load_from, {self._tensor_name_in_ckpt: to_restore})\r\n      self._embedding_weights[key] = var\r\n    return self._embedding_weights[key]\r\n```\r\n\r\n\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nCould use the shared_embeddings.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column\r\n\r\n@tf.function\r\ndef shared_embedding_column_with_hash_bucket():\r\n    color_data = {'color': [[2], [5], [-1], [0]],\r\n                  'color2': [[2, 2], [5, 5], [0, -1], [0, 0]],\r\n                  'color3': [[2,2,2], [5,5,5], [-1,-1,-1], [0,0,0]]\r\n                 }\r\n    color_column = feature_column.categorical_column_with_hash_bucket('color', 5, dtype=tf.int32)\r\n    color_column2 = feature_column.categorical_column_with_hash_bucket('color2', 5, dtype=tf.int32)\r\n    color_column3 = feature_column.categorical_column_with_hash_bucket('color3', 5, dtype=tf.int32)\r\n    color_column_embed = tf.feature_column.shared_embeddings([color_column, color_column2, color_column3], 4, combiner='sum')\r\n    print(color_column_embed)\r\n    print(type(color_column_embed))\r\n    print('use input_layer' + '_' * 40)\r\n    df =  tf.keras.layers.DenseFeatures(color_column_embed)(color_data)\r\n    return df\r\n\r\ndense = shared_embedding_column_with_hash_bucket()\r\nprint(dense)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1758, in <module>\r\n    main()\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1752, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1147, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/lqk/project/PycharmProjects/TF2/main.py\", line 40, in <module>\r\n    dense = shared_embedding_column_with_hash_bucket()\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 411, in __call__\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1307, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1596, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1529, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 713, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 292, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 705, in wrapper\r\n    ), args, kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 375, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\n  File \"/var/folders/4x/njl257j95bx4mt08j65fxqg00000gp/T/tmp5y1g76hx.py\", line 16, in tf__shared_embedding_column_with_hash_bucket\r\n    df = ag__.converted_call(tf.keras.layers.DenseFeatures(color_column_embed), None, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (color_data,), None)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 271, in converted_call\r\n    return _call_unconverted(f, args, kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 173, in _call_unconverted\r\n    return f(*args)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 632, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 89, in wrapper\r\n    ), args, kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 375, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\n  File \"/var/folders/4x/njl257j95bx4mt08j65fxqg00000gp/T/tmpshzfamoc.py\", line 70, in tf__call\r\n    ag__.for_stmt(self._feature_columns, None, loop_body, ())\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 102, in for_stmt\r\n    return _py_for_stmt(iter_, extra_test, body, init_state)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 111, in _py_for_stmt\r\n    state = body(target, *state)\r\n  File \"/var/folders/4x/njl257j95bx4mt08j65fxqg00000gp/T/tmpshzfamoc.py\", line 51, in loop_body\r\n    tensor = ag__.converted_call('get_dense_tensor', column, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (transformation_cache, self._state_manager), None)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 271, in converted_call\r\n    return _call_unconverted(f, args, kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 173, in _call_unconverted\r\n    return f(*args)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 3349, in get_dense_tensor\r\n    return self._get_dense_tensor_internal(transformation_cache, state_manager)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 3326, in _get_dense_tensor_internal\r\n    embedding_weights = self.shared_embedding_column_creator.embedding_weights\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 3253, in embedding_weights\r\n    trainable=self._trainable)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1496, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1239, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 562, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 514, in _true_getter\r\n    aggregation=aggregation)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 856, in _get_single_variable\r\n    raise ValueError(err_msg)\r\nValueError: Variable color_color2_color3_shared_embedding already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?\r\n```\r\n", "comments": ["I was able to get the mentioned error on Colab with TensorFlow version 2.0.", "New error for 'tf-nightly-2.0-preview 2.0.0.dev20190714 ' with @tf.function\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1758, in <module>\r\n    main()\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1752, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1147, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/lqk/project/PycharmProjects/TF2/shared_embedding.py\", line 44, in <module>\r\n    df = shared_embedding_column_with_hash_bucket(dic_data)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 429, in __call__\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1661, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1991, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1877, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 791, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 310, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/lqk/anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 781, in wrapper\r\n    raise e.ag_error_metadata.to_exception(type(e))\r\nAttributeError: in converted code:\r\n    relative to /Users/lqk:\r\n\r\n    project/PycharmProjects/TF2/shared_embedding.py:36 shared_embedding_column_with_hash_bucket  *\r\n        df = tf.keras.layers.DenseFeatures(color_column_embed)(dic_data)\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:758 __call__\r\n        outputs = self.call(inputs, *args, **kwargs)\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:508 call\r\n        self._state_manager)\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3418 get_dense_tensor\r\n        return self._get_dense_tensor_internal(transformation_cache, state_manager)\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3395 _get_dense_tensor_internal\r\n        embedding_weights = self.shared_embedding_column_creator.embedding_weights\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3322 embedding_weights\r\n        trainable=self._trainable)\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py:1500 get_variable\r\n        aggregation=aggregation)\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py:1243 get_variable\r\n        aggregation=aggregation)\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py:567 get_variable\r\n        aggregation=aggregation)\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py:519 _true_getter\r\n        aggregation=aggregation)\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py:861 _get_single_variable\r\n        tb = var.op.traceback[::-1]\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:558 op\r\n        return self._handle.op\r\n    anaconda2/envs/p36t2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:986 op\r\n        \"Tensor.op is meaningless when eager execution is enabled.\")\r\n\r\n    AttributeError: Tensor.op is meaningless when eager execution is enabled.\r\n```", "Was able to replicate the issue with `Tf 2.1` and `tf-nightly`.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/5c02761608274ceb4720812d78d6390d/untitled444.ipynb). Thanks!", "Thanks @dulm for the report. The Feature Column shared embeddings are best used with Estimators, which internally escape into graph mode. If you want support for tf.function and eager execution, please look at Keras Embedding Layers: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding , as these are better suited for eager execution.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29043\">No</a>\n", "> Thanks @dulm for the report. The Feature Column shared embeddings are best used with Estimators, which internally escape into graph mode. If you want support for tf.function and eager execution, please look at Keras Embedding Layers: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding , as these are better suited for eager execution.\r\n\r\nI don't think it's right, feature_column is a method to solve feature transform, and it's easy and efficient, all of feature_columns are supported tf2.0's DenseFeatures except shared_embeddings.   Laziness is no excuse."]}, {"number": 29042, "title": "CMake build fails v1.13.1 fatal error: tensorflow/stream_executor/dnn.pb.h: No such file or directory  #include \"tensorflow/stream_executor/dnn.pb.h\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: v1.13.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): 0.25 NA\r\n- GCC/Compiler version (if compiling from source): gcc/4.8.5\r\n- CUDA/cuDNN version: 10.0.130, 7.4.2.24\r\n- GPU model and memory: GTX 1060 6Gb\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nCMake build doesnt complete\r\n\r\n```\r\n[ 18%] Building CXX object CMakeFiles/tf_core_profiler.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/core/profiler/internal/tfprof_scope.cc.o\r\n[ 18%] Building CXX object CMakeFiles/tf_core_profiler.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/core/profiler/internal/tfprof_op.cc.o\r\n[ 18%] Building CXX object CMakeFiles/tf_core_profiler.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/core/profiler/internal/tfprof_show_multi.cc.o\r\n[ 18%] Building CXX object CMakeFiles/tf_core_profiler.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/core/profiler/internal/tfprof_show.cc.o\r\n[ 18%] Building CXX object CMakeFiles/tf_stream_executor.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/stream_executor/blas.cc.o\r\n[ 18%] Building CXX object CMakeFiles/tf_stream_executor.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/stream_executor/device_description.cc.o\r\n[ 18%] Building CXX object CMakeFiles/tf_core_profiler.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/core/profiler/internal/tfprof_stats.cc.o\r\n[ 18%] Building CXX object CMakeFiles/tf_stream_executor.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/stream_executor/dnn.cc.o\r\n[ 18%] Linking CXX executable proto_text\r\nIn file included from /home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/stream_executor/dnn.cc:16:0:\r\n/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/stream_executor/dnn.h:34:47: fatal error: tensorflow/stream_executor/dnn.pb.h: No such file or directory\r\n #include \"tensorflow/stream_executor/dnn.pb.h\"\r\n                                               ^\r\ncompilation terminated.\r\n[ 18%] Built target proto_text\r\n[ 18%] Building CXX object CMakeFiles/tf_stream_executor.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/stream_executor/dso_loader.cc.o\r\nmake[2]: *** [CMakeFiles/tf_stream_executor.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/stream_executor/dnn.cc.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\n[ 18%] Building CXX object CMakeFiles/tf_core_profiler.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/core/profiler/internal/tfprof_tensor.cc.o\r\n[ 18%] Building CXX object CMakeFiles/tf_core_profiler.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/core/profiler/internal/tfprof_timeline.cc.o\r\nmake[1]: *** [CMakeFiles/tf_stream_executor.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[ 18%] Building CXX object CMakeFiles/tf_core_profiler.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/core/profiler/internal/tfprof_utils.cc.o\r\n[ 19%] Building CXX object CMakeFiles/tf_core_profiler.dir/home/kognat/dev/tensorflow-v1.13.1-static-gpu-centos6/tensorflow/tensorflow/core/profiler/tfprof_options.cc.o\r\n[ 19%] Built target tf_core_profiler\r\nmake: *** [all] Error 2\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ngit checkout https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow/tensorflow/contrib/cmake\r\nmkdir build && cd build\r\nCUDA_HOME=/usr/local/cuda-10.0 cmake3 .. -DCMAKE_PREFIX_PATH=/usr/local/cuda-10.0:/usr/local/cudnn-7.4.2.24/cuda -DCMAKE_INSTALL_PREFIX=/home/samh/opt/tensorflow-v1.13.1-static-gpu -Dtensorflow_BUILD_PYTHON_BINDING=off -Dtensorflow_ENABLE_GPU=on -Dtensorflow_ENABLE_GRPC_SUPPORT=on -Dtensorflow_ENABLE_POSITION_INDEPENDENT_CODE=on -Dtensorflow_ENABLE_SNAPPY_SUPPORT=on -Dtensorflow_OPTIMIZE_FOR_NATIVE_ARCH=on -Dtensorflow_PATH_CUDA_LIB=/usr/local/cuda-10.0/lib -Dtensorflow_PATH_CUDNN_LIB=/usr/local/cudnn-7.4.2.24/cuda/lib  -Dtensorflow_CUDNN_INCLUDE=/usr/local/cudnn-7.4.2.24/cuda/include -Dtensorflow_BUILD_SHARED_LIB=on -Dtensorflow_NCCL_INCLUDE=/usr/local/nccl-2.4.7/include -Dtensorflow_PATH_NCCL_LIB=/usr/local/nccl-2.4.7/lib -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-10.0 -DCUDA_cupti_LIBRARY=/usr/local/cuda-10.0/extras/CUPTI/lib -DOPENSSL_NO_ASM=1\r\nmake -j12\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI need a static build with GPU support, any process to do this would be considered an alternative.\r\n\r\nSam\r\n", "comments": ["@samhodge Request you to please follow the [link](https://www.tensorflow.org/install/source#tested_build_configurations), as per this reference it supports Bazel, Please use bazel.", "I need a static version of the library I will link the reasons why.", "@muddham \r\n\r\nUsing Bazel will produce libtensorflow_cc.so with GPU support which is excellent.\r\n\r\nBut the product I am writing a plugin for is also using Tensorflow, hence when Tensorflow::Port::init() is called this segfault the parent application.\r\n\r\nMy current workaround is to statically link tensorflow using contrib/makefile but this doesnt have CUDA support.\r\n\r\ncontrib/cmake does have CUDA support and static linking possible but doesn't work as detailed in the post above.\r\n\r\nI will attempt to edit:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/contrib/cmake/tf_stream_executor.cmake\r\n\r\nto make it more like:\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/contrib/cmake/tf_core_framework.cmake\r\n\r\nSo it processes the .proto file here:\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/stream_executor/dnn.proto\r\n\r\nTo make the missing\r\n#include \"tensorflow/stream_executor/dnn.pb.h\"\r\n\r\nAt some stage this change was made as I was able to compile the cmake project under windows with Tensorflow around v1.8.0\r\n\r\nThe contrib/cmake needs to be updated.\r\n\r\n\r\nI am not running Python I am creating a plugin in C++\r\n\r\nsee:\r\nhttps://github.com/tensorflow/tensorflow/issues/22810\r\n\r\nWhich was put up in September 2018 and is still not solved.", "@samhodge Can you please confirm if this is duplicate of #22810", "No this file:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/219bb5d8dc9d16fa2e9895d8d4c6adaf45ee3c34#diff-39f23d578b0023a4931f812a68725235\r\n\r\nWas added in v1.12.1 so the cmake needs to be updated to deal with this change I will see if I can work with v1.12.0", "No #22810 is the cause of the issue, needing the cmake build to work allows the workaround to #22810 and build a static library.", "https://github.com/tensorflow/tensorflow/commits/v1.13.1/tensorflow/contrib/cmake has not been updated since December 2018 and the dnn.proto file was added after that time.", "https://github.com/tensorflow/tensorflow/issues/29042#issuecomment-496163002 In response no it is not a duplicate ", "Building on v1.12.0 was a little better, but my build environment was  gcc 4.4.7 in /usr/bin/gcc and another gcc 4.8.5 in a non standard location on CentOS 6.10\r\n\r\nIt is demanding CUDA 9.0 although I know the source runs with CUDA 10.0.130\r\n\r\nCompiling issues were related to not finding a C++11 compiler.\r\n\r\nThe same CentOS 6.10 image has been used to compile Tensorflow 1.12.0 with Bazel for CPU, CUDA 9.0 CUDNN 7.3.1 and CUDA 10.0 CUDNN 7.4.24.2\r\n\r\nIt seems the CMake compiler directive is not passed to sub projects.\r\n\r\nI will find the fix for some relocation of files not updated in the .cmake files", "https://github.com/tensorflow/tensorflow/issues/22634#issuecomment-438616945 Is a link to fixes that need to be made to v1.12.0", "Here is the latest build command for my machine for v1.12.0\r\n\r\n```\r\nCUDA_HOME=/usr/local/cuda-10.0 cmake3 .. -DCMAKE_PREFIX_PATH=/usr/local/cuda-10.0:/usr/local/cudnn-7.4.2.24/cuda -DCMAKE_INSTALL_PREFIX=/home/samh/opt/tensorflow-v1.13.1-static-gpu -Dtensorflow_BUILD_PYTHON_BINDING=off -Dtensorflow_ENABLE_GPU=on -Dtensorflow_ENABLE_GRPC_SUPPORT=on -Dtensorflow_ENABLE_POSITION_INDEPENDENT_CODE=on -Dtensorflow_ENABLE_SNAPPY_SUPPORT=on -Dtensorflow_OPTIMIZE_FOR_NATIVE_ARCH=on -Dtensorflow_PATH_CUDA_LIB=/usr/local/cuda-10.0/lib -Dtensorflow_PATH_CUDNN_LIB=/usr/local/cudnn-7.4.2.24/cuda/lib -Dtensorflow_CUDNN_INCLUDE=/usr/local/cudnn-7.4.2.24/cuda/include -Dtensorflow_BUILD_SHARED_LIB=on -Dtensorflow_NCCL_INCLUDE=/usr/local/nccl-2.4.7/include -Dtensorflow_PATH_NCCL_LIB=/usr/local/nccl-2.4.7/lib -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-10.0 -Dtensorflow_PATH_NCCL_STATIC_LIB=/usr/local/nccl-2.4.7/lib -Dtensorflow_PATH_CUDNN_STATIC_LIB=/usr/local/cudnn-7.4.2.24/cuda/lib -DCUDA_cupti_LIBRARY=/usr/local/cuda-10.0/extras/CUPTI/lib -Dculibos_STATIC_LIBRARY=/usr/local/cuda-10.0/lib64/libculibos.a -DOPENSSL_NO_ASM=1\r\n\r\n````", "After trying to hack abseil-cpp into the include and CXX_FLAGS via the flags.make\r\n\r\nAfter the fact I am left with the following\r\n\r\n```\r\nCMakeFiles/tf_core_lib.dir/home/kognat/dev/tensorflow-1.12.0-static-gpu/tensorflow/tensorflow/core/lib/io/path.cc.o: In function `tensorflow::io::internal::SplitPath(absl::string_view)':\r\npath.cc:(.text+0x337): undefined reference to `absl::string_view::rfind(char, unsigned long) const'\r\nCMakeFiles/tf_core_lib.dir/home/kognat/dev/tensorflow-1.12.0-static-gpu/tensorflow/tensorflow/core/lib/io/path.cc.o: In function `tensorflow::io::internal::SplitBasename(absl::string_view)':\r\npath.cc:(.text+0x57b): undefined reference to `absl::string_view::rfind(char, unsigned long) const'\r\nCMakeFiles/tf_core_lib.dir/home/kognat/dev/tensorflow-1.12.0-static-gpu/tensorflow/tensorflow/core/lib/io/path.cc.o: In function `absl::string_view::substr(unsigned long, unsigned long) const':\r\npath.cc:(.text._ZNK4absl11string_view6substrEmm[_ZNK4absl11string_view6substrEmm]+0x33): undefined reference to `absl::base_internal::ThrowStdOutOfRange(char const*)'\r\nCMakeFiles/tf_core_lib.dir/home/kognat/dev/tensorflow-1.12.0-static-gpu/tensorflow/tensorflow/core/lib/monitoring/collection_registry.cc.o: In function `tensorflow::monitoring::CollectionRegistry::Register(tensorflow::monitoring::AbstractMetricDef const*, std::function<void (tensorflow::monitoring::MetricCollectorGetter)> const&)':\r\ncollection_registry.cc:(.text+0x5a4): undefined reference to `absl::operator<<(std::ostream&, absl::string_view)'\r\nCMakeFiles/tf_core_lib.dir/home/kognat/dev/tensorflow-1.12.0-static-gpu/tensorflow/tensorflow/core/lib/strings/str_util.cc.o: In function `tensorflow::str_util::TitlecaseString(std::string*, absl::string_view)':\r\nstr_util.cc:(.text+0x126a): undefined reference to `absl::string_view::find(char, unsigned long) const'\r\nCMakeFiles/tf_core_lib.dir/home/kognat/dev/tensorflow-1.12.0-static-gpu/tensorflow/tensorflow/core/lib/strings/str_util.cc.o: In function `std::vector<std::string, std::allocator<std::string> > tensorflow::str_util::Split<tensorflow::str_util::AllowEmpty>(absl::string_view, absl::string_view, tensorflow::str_util::AllowEmpty)':\r\nstr_util.cc:(.text._ZN10tensorflow8str_util5SplitINS0_10AllowEmptyEEESt6vectorISsSaISsEEN4absl11string_viewES7_T_[_ZN10tensorflow8str_util5SplitINS0_10AllowEmptyEEESt6vectorISsSaISsEEN4absl11string_viewES7_T_]+0xa1): undefined reference to `absl::string_view::find(char, unsigned long) const'\r\nCMakeFiles/tf_core_lib.dir/home/kognat/dev/tensorflow-1.12.0-static-gpu/tensorflow/tensorflow/core/platform/posix/port.cc.o: In function `tensorflow::port::NominalCPUFrequency()':\r\nport.cc:(.text+0x3de): undefined reference to `absl::base_internal::NominalCPUFrequency()'\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [proto_text] Error 1\r\nmake[1]: *** [CMakeFiles/proto_text.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n```\r\n\r\nThis contrib/cmake was probably only working back in v1.8.0 or before.", "@samhodge Hello, can you tell me how you modified tf_stream_executor.cmake?", "I never got a successful build", "That's really depressing.", "@samhodge, Is this still an issue? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It is still open, is the CMake build system abandoned by Tensorflow developers, last time I got it to work was 1.8.\r\n\r\nIs bazel the only way to built Tensorflow?\r\n\r\nIf this is so there is no straightfoward way to build an archive of .o files for statically linking Tensorflow to a C++ application.\r\n\r\nIf the CMake tree doesnt work it should be removed from the code base rather than giving users false hope that it actually works.", "@angersson can you please take a look? ", "Bazel is the only supported way to build TensorFlow, sorry. Since contrib is being removed as a part of TF 2.0, the cmake files will go away at the same time.", "> \r\n> \r\n> Bazel is the only supported way to build TensorFlow, sorry. Since contrib is being removed as a part of TF 2.0, the cmake files will go away at the same time.\r\n\r\nThat is sad and short sighted on the part of the developers, why support Bazel only, and not the widely used CMake? ", "> It is still open, is the CMake build system abandoned by Tensorflow developers, last time I got it to work was 1.8.\r\n> \r\n> Is bazel the only way to built Tensorflow?\r\n> \r\n> If this is so there is no straightfoward way to build an archive of .o files for statically linking Tensorflow to a C++ application.\r\n> \r\n> If the CMake tree doesnt work it should be removed from the code base rather than giving users false hope that it actually works.\r\n\r\n@samhodge Hi, I made a try to build tensorflow v1.13.1 with vs2017 using cuda 10.1.\r\nI builded without python and successed.\r\nhttps://github.com/yl255/Build-tensorflow-v1.31.1-with-vs2017-using-cuda-10.1)\r\nMaybe you can have a try.\r\nGood luck!", "I will see if this comes a priority again, thanks so much for sharing.", "We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.5  version or later and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29042\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29042\">No</a>\n"]}, {"number": 29041, "title": "TFLite model exported from Keras cannot be executed on Android", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows/Android\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Huawei P10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.13.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: Geforce Gtx1080 8Gb\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have exported a model from Keras using following code:\r\n```\r\n\r\nconverter = lite.TFLiteConverter.from_keras_model_file(weights_path_2)\r\ntfmodel = converter.convert()\r\nopen (\"model.tflite\" , \"wb\") .write(tfmodel)\r\n\r\n```\r\n\r\nExport and import into Android works fine, executing however throws following error:\r\n```\r\n\r\njava.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/fully_connected.cc:112 input_size != batch_size * filter->dims->data[1] (4096 != 0)Node number 7 (FULLY_CONNECTED) failed to prepare.\r\n\r\n```\r\nI also tried exporting the model using toco, but the same issue happens.\r\nThe model I used here can be found here https://github.com/krasserm/face-recognition/blob/master/model.py\r\nAny idea on how to fix this? \r\n### Describe expected behaviour\r\n\r\nIf the model can be exported without problems I should be able to executed it too.\r\n\r\n", "comments": ["Are you setting the size of your inputs? If you're using the Java bindings, how are you invoking the model? What shapes are you using?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29041\">No</a>\n", "I met a similar error. Did you fix this error? @PiotrSwietek "]}, {"number": 29040, "title": "Why are ResNet101/ResNeXt50/ResNeXt101 not included in TensorFlow\u00b4s Keras distribution?", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.keras.applications` contains only ResNet50, as browsing to the `site-packages/tensorflow/python/keras/applications` folder shows only `resnet50.py`. Why are ResNet101/ResNeXt50/ResNeXt101 not included? Is there a repo where I can get them from?\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nUsers who wish to use pretrained ResNet101/ResNeXt50/ResNeXt101, which are available in Keras\u00b4 stand-alone distribution (if the documentation is right).", "comments": ["[ResNet101](https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet101V2), [ResNet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50V2), [ResNet152](https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet152V2) have now been added (both V1 and V2 architectures). \r\nThanks!", "ResNeXt architectures are still missing. I need to train ResNeXt-101 for my application and it seems I have to write it from scratch since there's no 'ready-to-import' module in Tensorflow's keras distribution.", "You are right. This is a WIP. See https://github.com/tensorflow/tensorflow/pull/37146", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Is there any update on this issue, as it is difficult to not able to find ResNeXt as part of Tensorflow Keras API."]}, {"number": 29038, "title": "faild to retrain the Inception v3 model.", "body": "![2019-05-26 18-12-22 \u7684\u5c4f\u5e55\u622a\u56fe](https://user-images.githubusercontent.com/36025537/58380553-033bcd80-7fe5-11e9-88d5-1d6486a4ef6b.png)\r\n", "comments": ["@ry @jmhodges @eggie5 @bmabey  Hi~My network is okay, why I occur this? Thanks.", "@berylyellow  Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29037, "title": "Refactor {Concatenate, Filter, FlatMap} DatasetOps", "body": "This PR refactors `ConcatenateDatasetOp`, `FilterDatasetOp`, and `FlatMapDatasetOp`.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa The comments are addressed via this commit (https://github.com/tensorflow/tensorflow/pull/29037/commits/3b8aacbcec3fe5d34acab3919940a6673139cc67). Could you please take a look?"]}, {"number": 29036, "title": "Evaluating the value of a tensor from inside the model_function of a custom `tf.TPUEstimator`", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI am implementing an NLP model based on BERT, using `tf.TPUEstimator()`. I want to implement layer-wise training, where I need to select only one layer of the model to train for each epoch. In order to do this I wanted to change my model_fn and get the value of current_epoch. \r\n\r\nI know how to compute the value of current_epoch as a tensor using `tf.train.get_or_create_global_step()` inside the `model_fn` BUT, I need to evaluate the value of this tensor to select which layer to train and implement  return the correct `train_op` to the `tf.estimator` (**train_op pertaining to a single layer chosen accrding to the value of the current_epoch**).\r\n\r\nI am unable to evaluate this tensor (current_epoch / global_step) from inside the model_fn. I tried the following but the training hangs at the step `my_sess.run(my_global_step.initializer`\r\n\r\n```\r\nglobal_step = tf.train.get_or_create_global_step()\r\ngraph = tf.get_default_graph()\r\nmy_sess = tf.Session(graph=graph)\r\ncurrent_epoch = (global_step * full_bs) // train_size\r\n\r\nmy_sess.run(my_global_step.initializer)\r\ncurrent_epoch = sess.run(current_epoch)\r\n\r\n# My program hangs at the initialising step: my_sess.run(my_global_step.initializer)\r\n```\r\nQ) Is there currently any way to evaluate a tensor inside the `tf.estimator`\r\nQ)Is there any way to evaluate a tensor using the tf.Estimators default session? How do I get the default session/ Graph?\r\nQ)**Most importantly what is wrong in my code and why does the training hang when using tpu's and TPUEstimator**?\r\n", "comments": ["@vishagan1 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@muddham Thanks for the response. Please find below a code snippet, which creates the optimizer with different rate depending on the current_epoch and selecting a layer. Finally it returns the train_op that updates the weights.\r\n```\r\ndef create_optimizer(loss,\r\n                     init_lr_bert,\r\n                     init_lr_custom,\r\n                     num_train_steps,\r\n                     num_warmup_steps,\r\n                     use_tpu,\r\n                     # fixme added new params below\r\n                     is_layer_wise,\r\n                     epoch_per_layer,\r\n                     full_bs,\r\n                     train_size,\r\n                     steps_per_epoch,\r\n                     steps_per_layer\r\n                     ):\r\n\r\n  \"\"\"Creates an optimizer training op.\"\"\"\r\n  global_step = tf.train.get_or_create_global_step()\r\n\r\n    current_epoch = (global_step * full_bs) // train_size\r\n    eff_epoch = current_epoch // epoch_per_layer\r\n    \r\n  if is_layer_wise: \r\n     global_step = tf.train.get_or_create_global_step()\r\n     graph = tf.get_default_graph()\r\n     my_sess = tf.Session(graph=graph)\r\n     current_epoch = (global_step * full_bs) // train_size\r\n     my_sess.run(my_global_step.initializer)\r\n     current_epoch = sess.run(current_epoch)\r\n     \r\n      if current_epoch == 0:\r\n          # train just the custom layer\r\n          lr_custom = _learning_rate_decay_and_warmup(init_lr_custom, global_step, num_train_steps,\r\n                                                      num_warmup_steps)\r\n          _train_op_custom = _create_train_op(\"[^bert]\", lr_custom, loss, global_step, use_tpu=use_tpu,\r\n                                              weight_decay_rate=0.01,\r\n                                              beta_1=0.9,\r\n                                              beta_2=0.999,\r\n                                              epsilon=1e-6,\r\n                                              exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"]\r\n                                              )\r\n          tf.logging.info('*******Training Custom Layer')\r\n          train_ops.append(_train_op_custom)\r\n      else:\r\n          layer_to_train = 12-current_epoch\r\n          regex_match = 'bert/encoder/' + \"layer_{}\".format(layer_to_train) + \"/\"\r\n          tf.logging.info('*******Training: {}'.format(regex_match))\r\n          print('*******Training: {}'.format(regex_match))\r\n          decay = 2.6 ** (11 - layer_to_train)\r\n          decayed_lr = init_lr_bert/decay\r\n          tf.logging.info(\"Attaching learning rate of, {} to {}\".format(decayed_lr, regex_match))\r\n          lr_ = _learning_rate_decay_and_warmup(decayed_lr, global_step, num_train_steps,\r\n                                                                              num_warmup_steps)\r\n          _train_op = _create_train_op(regex_match, lr_, loss, global_step, use_tpu=use_tpu,\r\n                                       weight_decay_rate=0.01,\r\n                                       beta_1=0.9,\r\n                                       beta_2=0.999,\r\n                                       epsilon=1e-6,\r\n                                       exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"]\r\n                                       )\r\n          train_ops.append(_train_op)\r\n          \r\n    new_global_step = global_step + 1\r\n    train_op = tf.group(*train_ops, [global_step.assign(new_global_step)])\r\n    return train_op\r\n```\r\n     ", "@vishagan1 I ran the code provided in TF 1.13 TPU , its not showing any output.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29035, "title": "CUDA Toolkit 10.1 Update compile tf2.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):0.25.2\r\n- GCC/Compiler version (if compiling from source):gcc\r\n- CUDA/cuDNN version: 10.1 Update\r\n- GPU model and memory: GTX960M 2G\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen I get tensorflow source from \"git clone https://github.com/tensorflow.git\" and \"git checkout r2.0\",  why do I get that the package is tensorflow 1.13.1 yet, rather than the tensorflow 2.0 after compiling it successfully.\r\n", "comments": ["@yanhaiming56 Please have a look on this [link](https://www.tensorflow.org/install/source#download_the_tensorflow_source_code). Thanks!", "I have the same problem, even using the master branch with  --config=v2 option", "@lifeishard  Please post a new issue and provide all the information asked by the template. We ask this because it's more efficient to have one thread dedicated to one issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29034, "title": "GPU text classification notebook produces CUDNN error", "body": "**Error Message**\r\nUnknownError: Fail to find the dnn implementation.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Pop!_OS 19.04\r\n- TensorFlow installed from (source or binary): jupyter\r\n- TensorFlow version (use command below): !pip install tensorflow-gpu\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0, cudnn 7.4.5.1\r\n- GPU model and memory: See below RTX 2070 Max-q\r\n\r\n\r\n[I 21:20:12.056 LabApp] Saving file at /Downloads/text_classification_rnn.ipynb\r\n[W 21:20:12.057 LabApp] Notebook Downloads/text_classification_rnn.ipynb is not trusted\r\n[I 21:21:07.912 LabApp] Kernel interrupted: fe30308a-41da-4a83-8fa9-93bf6c87fb6c\r\n[I 21:22:12.102 LabApp] Saving file at /Downloads/text_classification_rnn.ipynb\r\n[W 21:22:12.103 LabApp] Notebook Downloads/text_classification_rnn.ipynb is not trusted\r\n2019-05-25 21:22:15.359388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0\r\n2019-05-25 21:22:15.359425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-25 21:22:15.359432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 \r\n2019-05-25 21:22:15.359436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N \r\n2019-05-25 21:22:15.359592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/device:GPU:0 with 6704 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-05-25 21:22:19.514494: W ./tensorflow/core/framework/model.h:202] Encountered a stop event that was not preceded by a start event.\r\n2019-05-25 21:22:20.872486: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-05-25 21:22:20.872526: W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at cudnn_rnn_ops.cc:1280 : Unknown: Fail to find the dnn implementation.\r\n2019-05-25 21:22:20.872567: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::StartAbort Unknown: Fail to find the dnn implementation.\r\n\t [[{{node bidirectional_2/CudnnRNN}}]]\r\n\t [[loss_2/dense_5_loss/binary_crossentropy/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_47/has_invalid_dims_0/_52]]\r\n2019-05-25 21:22:20.872626: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::StartAbort Unknown: Fail to find the dnn implementation.\r\n\t [[{{node bidirectional_2/CudnnRNN}}]]\r\n2019-05-25 21:22:20.872655: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function execution failed: Unknown: Fail to find the dnn implementation.\r\n\t [[{{node bidirectional_2/CudnnRNN}}]]\r\n\t [[loss_2/dense_5_loss/binary_crossentropy/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_47/has_invalid_dims_0/_52]]\r\n2019-05-25 21:22:20.875125: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-05-25 21:22:20.875145: W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at cudnn_rnn_ops.cc:1280 : Unknown: Fail to find the dnn implementation.\r\n2019-05-25 21:22:20.875197: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function execution failed: Unknown: Fail to find the dnn implementation.\r\n\t [[{{node bidirectional_2/CudnnRNN}}]]\r\n", "comments": ["I think the issue here *might be* that my card is on compute cudnn version 7.5 yet cuda toolkit 10.0 for my OS does not pull cudnn 7.5. If I install cuda toolkit 10.1, then tensorflow looks for 10.0 shared libraries only and doesn't see the new toolkit.", "I also tried with\r\n!pip install tensorflow-gpu==2.0.0-alpha0\r\n\r\nsame error.\r\n\r\nUnknownError: Fail to find the dnn implementation.\r\n\t [[{{node bidirectional/CudnnRNN}}]]\r\n\t [[loss/dense_1_loss/binary_crossentropy/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_47/has_invalid_dims/concat/_48]] [Op:__inference_keras_scratch_graph_2449]", "In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "It comes from this notebook from tensorflow team\r\nhttps://www.tensorflow.org/alpha/tutorials/text/text_classification_rnn\r\n\r\nAnd the segment throwing the exception is the \"Train the Model\" cell.\r\nNOTE: Keep in mind I am running this on my hardware, with RTX 2070 card.\r\n\r\nhistory = model.fit(train_dataset, epochs=10,\r\n                    validation_data=test_dataset)\r\n---------------------\r\nEpoch 1/10\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-15-f602eedbfd8a> in <module>\r\n      5 \r\n      6 history = model.fit(train_dataset, epochs=10,\r\n----> 7                     validation_data=test_dataset)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    789           workers=0,\r\n    790           shuffle=shuffle,\r\n--> 791           initial_epoch=initial_epoch)\r\n    792 \r\n    793     # Case 3: Symbolic tensors or Numpy array-like.\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1513         shuffle=shuffle,\r\n   1514         initial_epoch=initial_epoch,\r\n-> 1515         steps_name='steps_per_epoch')\r\n   1516 \r\n   1517   def evaluate_generator(self,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    255 \r\n    256       is_deferred = not model._is_compiled\r\n--> 257       batch_outs = batch_function(*batch_data)\r\n    258       if not isinstance(batch_outs, list):\r\n    259         batch_outs = [batch_outs]\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1257       else:\r\n   1258         self._make_fit_function()\r\n-> 1259         outputs = self._fit_function(ins)  # pylint: disable=not-callable\r\n   1260 \r\n   1261     if reset_metrics:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3215         value = math_ops.cast(value, tensor.dtype)\r\n   3216       converted_inputs.append(value)\r\n-> 3217     outputs = self._graph_fn(*converted_inputs)\r\n   3218     return nest.pack_sequence_as(self._outputs_structure,\r\n   3219                                  [x.numpy() for x in outputs])\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n    556       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n    557           list(kwargs.keys()), list(self._arg_keywords)))\r\n--> 558     return self._call_flat(args)\r\n    559 \r\n    560   def _filtered_call(self, args, kwargs):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    625     # Only need to override the gradient in graph mode and when we have outputs.\r\n    626     if context.executing_eagerly() or not self.outputs:\r\n--> 627       outputs = self._inference_function.call(ctx, args)\r\n    628     else:\r\n    629       self._register_gradient()\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    413             attrs=(\"executor_type\", executor_type,\r\n    414                    \"config_proto\", config),\r\n--> 415             ctx=ctx)\r\n    416       # Replace empty list with None\r\n    417       outputs = outputs or None\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nUnknownError: Fail to find the dnn implementation.\r\n\t [[{{node bidirectional/CudnnRNN}}]]\r\n\t [[loss/dense_1_loss/binary_crossentropy/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_47/has_invalid_dims/concat/_48]] [Op:__inference_keras_scratch_graph_2449]", "This is likely a duplicate of this issue https://github.com/tensorflow/tensorflow/issues/24496", "Please have a look on the issue mentioned by you and let us know if that helps. Looks like this [comment](https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-464909727) might give you a fix. Thanks!", "Hi,\r\n  Thank you for responding. I don't think that solution will work here as the example notebook is using Keras and the solution in the other ticket is not. Is there a way to enable that solution when using tensorflow through keras? If so, that might work.", "It might well be related to the memory allocation: TF grabs all the GPU memory and cuDNN can't initialize because there is nothing left. You could try to reduce the preallocated TF memory (I have to admit I don't know how to do that through Keras).\r\n\r\nYou are suspecting that you need to update cuDNN and CUDA because it doesn't support your model yet. Older CUDA software is generally compatible with newer hardware (because the driver API is the same).\r\n\r\nThe best shot at debugging this is running with cuDNN logging on: https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#api-logging", "@radiantone Is this still an issue? If not, please close the issue.\r\nIf it is still an issue, \r\n1) can you try recently release TF2.0 and check whether the issue is resolved or not?\r\n2) Can you try to run small TF model and see whether you have the same issue?\r\n3) I noticed that you are using Anaconda. Is there any limit set on GPU memory?\r\n\r\nPlease let us know what you think? Thanks!", "@radiantone Can you please provide the info I requested in the last response? If this was resolved, please close the issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@radiantone  did you finally solve the problem?\r\nI get the same problem right now.\r\nAlso in my how hardware2070s.And problem occour in the text classification example of tensorflow.\r\n[https://www.tensorflow.org/alpha/tutorials/text/text_classification_rnn](url)\r\n\r\n`CancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node Reshape_11/_38}}]] [Op:__inference_distributed_function_6315]\r\n\r\nFunction call stack:\r\ndistributed_function`\r\n\r\n` W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Fail to find the dnn implementation.\r\n\t [[{{node CudnnRNN}}]]`\r\n\r\nif the problem has solved,please give me some solutions.\r\nThank you!"]}, {"number": 29033, "title": "DOC: Review keras.losses ", "body": "- fix failing example for `tf.keras.losses.Poisson`\r\n- update numbers for `tf.keras.losses.Poisson`\r\n- update numbers for `tf.keras.losses.KLDivergence`\r\n- add example for `tf.keras.losses.kullback_leibler_divergence`\r\n\r\nbtw, @lamberta do we have any way to reduce duplication of such docs? \r\ncurrently we support support for losses as a function as well as object-style\r\n\r\nPushed during Munich TFDocSprint cc @dynamicwebpaige ", "comments": ["@lc0 Thank you, Sergii.\r\nReduce duplication from the non-TF Keras API reference? Unfortunately, no. The tf.keras docs were forked and there is no automatic way to keep them in sync. But there's now a [Keras SIG](https://github.com/keras-team/governance) and that may be worth bringing up. cc @fchollet ", "any updates on this cc @pavithrasv?", "Seems like the failing test is not related to my fixing a doc", "another ping on this one. The PR has a label `ready to pull` but not merged yet for quite some time already. Might be because of failing tests, but as I said above, these have nothing to do with my docs changes.\r\n\r\nWould anybody take a look from your side: what is still missing?", "Thank you everyone involved! "]}, {"number": 29032, "title": "[T.F 2.0 API Doc] Example for bin_count under tf.math", "body": "This PR reference to #25802\r\n\r\nAdding example and error message to bin_count method doc", "comments": ["@SSaishruthi Can you please check Ubuntu Sanity errors? Thanks!", "@gbaned Can you please trigger the test again? \r\nUpdated based on pylint result."]}, {"number": 29031, "title": "Where is \"tensorflow/lite/delegates/gpu/gl/compiled_model_generated.h\"?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: n/a\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the problem**\r\n\r\nI'm looking at [serialization.h#L28](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/gl/serialization.h#L28) where it includes `tensorflow/lite/delegates/gpu/gl/compiled_model_generated.h`, but where's the file located? Is it meant to be generated at compile time? How is it generated?\r\n\r\nThanks", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n\r\n", "Hello @hoonkai \r\nDid you solve this problem?\r\nI am facing it too.\r\n\r\nThanks in advance", "Hi @hoonkai,\r\nI'm facing the same issue while compiling too.\r\nHow have you solved this?", "me too", "@makihiro and @mlinxiang, Please post a new issue and provide all the information asked by the [template](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask this because it's more efficient to have one thread dedicated to one issue. Thanks!\r\n", "> @makihiro and @mlinxiang, Please post a new issue and provide all the information asked by the [template](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask this because it's more efficient to have one thread dedicated to one issue. Thanks!\r\n\r\ni fixed this, because i compile tflite use tools/make  tool, first i run bazel build --cxxopt=--std=c++11 -c opt --config android_arm64 tensorflow/lite/delegates/gpu:gl_delegate which will convert tensorflow/lite/delegates/gpu/gl/*.fbs file to *.h in the response path of bazel-genfiles, which contain compiled_model_generated.h, then i copy these head files to the original path where *.fbs, and my build task success.", "@mlinxiang Thank you. That works fine for me.\r\n"]}, {"number": 29030, "title": "eigen/get/a0d250e79c79.tar.gz: Error 404", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.25\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\n$./tensorflow/lite/tools/make/download_dependencies.sh\r\n\r\ndownloading http://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz\r\ntar: Unrecognized archive format\r\ntar: Error exit delayed from previous errors.\r\n```\r\n\r\nError 404.\r\n\r\nRelated #28926.", "comments": ["I assume the issue could be fixed with PR #29017 ", "@hoonkai Please let us know if this issue is resolved once the PR has been merged", "Automatically closing due to lack of recent activity. Please update if this still an issue or new information is  available. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29030\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29030\">No</a>\n"]}, {"number": 29029, "title": "updated tanh", "body": "updated tanh with example, Arguments and returns", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 29028, "title": "Tflite Qualcomm DSP acceleration ", "body": "Hello folks, it is exciting to see announcement of qcomm DSP acceleration, aimed to be released late this summer, creating issue to track it. \r\n\r\n\r\n![Screenshot_2019-05-25-20-01-49-069_com google android youtube](https://user-images.githubusercontent.com/11027129/58370789-3704ed80-7f28-11e9-8859-9d643a6cb605.png)\r\n![Screenshot_2019-05-25-20-02-11-683_com google android youtube](https://user-images.githubusercontent.com/11027129/58370790-379d8400-7f28-11e9-8619-294e108d4ed1.png)\r\n\r\n", "comments": ["https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx", "@startupgurukul Does tflite support DSP board?", "Not yet, they are planning to add it by the end of this summer. \r\n\r\nIf you are evaluating options, consider mediatek Helios p90,it even beats Snapdragon 855 performance at a cost of  710/730 . \r\n\r\nhttp://ai-benchmark.com/ranking_processors.html#ranking\r\n\r\nAccording to them, Mlkit supports APU(equivalent of DSP) \r\n\r\nhttps://www.mediatek.com/blog/googles-ml-kit-arrives-on-the-p90", "@startupgurukul  I mean the DSP  board without cpu on it, just only dsp. So does micro support it ?", "I hope so if your DSP supports hexagon SDK from Qualcomm, would rather wait for a answer from a tensorflower for a confirmation on support at microcontroller level ", "@zh794390558 Hi Hui, what kind of DSP are you looking into? If the TFL micro interpreter can be compiled by the DSP compiler and ran on the DSP, then hypothetically the DSP should be able to run TFL micro models.", "I'd like to ask about current third party module [hvx](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx) will it be working with tflite as of now?\r\n\r\nI see that currently hexagon kernel is built only for regular tensorflow (tensorflow/core/kernels/hexagon/BUILD)\r\nIf so does it mean that without adding it to tflite we will be unable to benefit from qualcomm addon?", "Hexagon Delegate is available now \r\nSee\r\nhttps://www.tensorflow.org/lite/performance/hexagon_delegate\r\n\r\n"]}, {"number": 29027, "title": "Cpu memory leak when using `tf.function` with gpu model.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-2319-g81f2165 2.0.0-dev20190520\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.1\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen using `tf.function`, the cpu memory usage increase forever.\r\n\r\n**Describe the expected behavior**\r\nNo cpu memory leak when training models on gpu device using 'tf.function'.\r\n\r\n**Code to reproduce the issue**\r\nA part of my code, since the whole source code is huge...\r\nAnd the `dataset_train` in the following code is a `tf.data.DataSet` object.\r\n```\r\n    DATA_TRAIN_SPEC = [\r\n        tf.TensorSpec(shape=(None, IMAGE_FIX_HEIGHT, None, 3), dtype=tf.float32),\r\n        tf.TensorSpec(shape=None, dtype=tf.int32),\r\n        tf.TensorSpec(shape=None, dtype=tf.int32),\r\n        tf.TensorSpec(shape=None, dtype=tf.int32),\r\n    ]\r\n\r\n    # @tf.function(input_signature=DATA_TRAIN_SPEC)\r\n    def train_step(*batch_data):\r\n        imgs, labels, imgs_width, labels_len = batch_data\r\n        with tf.GradientTape() as tape:\r\n            logits = model(imgs, training=True)\r\n            logits_len = tf.cast(tf.math.ceil(\r\n                tf.divide(imgs_width, block_size)), tf.int32)\r\n            loss, accuracy = loss_fn(\r\n                labels, logits, labels_len, logits_len, CLASSES)\r\n        grads = tape.gradient(loss, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n        return loss, accuracy\r\n\r\n    def train_epoch(epoch):\r\n        print(f'Epoch {epoch} training start ...')\r\n        for step, batch_data in enumerate(dataset_train.take(num)):\r\n            loss, accuracy = train_step(*batch_data)\r\n\r\n            if (step+1) % ARGS.show_per_iterations == 0:\r\n                loss, accuracy = get_numpy((loss, accuracy))\r\n\r\n                loss_mean = np.mean(loss)\r\n                accuracy_mean = np.mean(accuracy)*100\r\n\r\n                prefix = f'Epoch#{epoch}/Step#{step+1}/training'\r\n                print(f'Loss of {prefix} is: {\"%.6f\"%loss_mean} ...')\r\n                print(f'Accuracy of {prefix} is: {\"%.3f\"%accuracy_mean}% ...')\r\n        print(f'Epoch {epoch} training finish ...\\n')\r\n\r\n```\r\n", "comments": ["@muddham @ry @jmhodges @eggie5 Can anyone help me, please, thank you~", "@jiarenyf Can you check this [issue](https://github.com/tensorflow/tensorflow/issues/26108) which is similar to yours and try it with TF.2.0.0-beta0 and let us know how it works. Thanks!", "Possibly similar to https://github.com/tensorflow/tensorflow/issues/29075.", "@jvishnuvardhan Thank you for the suggestion.\r\nHowever, the problem still occurs after `pip install tensorflow-gpu==2.0.0-beta0`.\r\nI also tried `tf.keras.xxx.clear_session`, and it took no effect for me.\r\n\r\nI have uploaded the code in [the_codes](https://1drv.ms/u/s!Au2Wv1iKFA69hLpV3g9jjxsQa1AqtA?e=J24DMb), could you please help me to debug, thank you.\r\nYou could run the code by:\r\n```\r\nsh install.sh\r\nunzip new.zip\r\nunzip new_datas.zip\r\ncd new\r\npython train.py --config=./config/train/train001.json\r\n```\r\n\r\nI have refactor the code, so it look different from the one I mentioned 20 days ago, while the problem is the same:\r\nWhen running without `@tf.function` the cpu_memory cost is as most 4.8GB and the gpu_util is 70%~100%, while running with `@tf.function` the cpu_memory keep increasing till over 10GB and  the gpu_util is always 0%.\r\nThank you.", "Since no one answers this question, I will delete the codes uploaded in one drive and switch my codes to use pytorch instead. Bye bye.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29027\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29027\">No</a>\n", "Check if **batch_data** which in the train_step() function has changed its dimention(or some data in batch_data is accumulating) every epoch. Because if that **@tf.function** will create a new graph every epoch.\r\n\r\n(Have been troubled by this problem for a long time, and finally solved with help of a classmate)\r\nHave a good day.\r\n@jiarenyf ", "Sir have you solved your problem? I also met a problem like this but some reshape is used in my programme but i have no idea how to fixed the leak of CPU memory.", "> Sir have you solved your problem? I also met a problem like this but some reshape is used in my programme but i have no idea how to fixed the leak of CPU memory.\r\n\r\nHi, \r\nThe follow **@tf.function instruction** in Tensorflow may provide some clues:\r\n\r\n> Note that unlike other TensorFlow operations, we don't convert python numerical inputs to tensors. Moreover, a new graph is generated for each distinct python numerical value, for example calling g(2) and g(3) will generate two new graphs (while only one is generated if you call g(tf.constant(2)) and g(tf.constant(3))). Therefore, python numerical inputs should be restricted to arguments that will have few distinct values, such as hyperparameters like the number of layers in a neural network. \r\n\r\n> \r\nMore information about [@tf.function](https://www.tensorflow.org/api_docs/python/tf/function).\r\n\r\nSo, check if have obey this, otherwise it will generate new graph each batch, and that will significantly raise your CPU memory.\r\n\r\nGood luck.", "Thank you @Ain-Crad . tf-function decorator is main culprit for increase of ram .\r\nIn my case \r\n`total_loss = loss_1(inp,tar) + lambda*loss_2(inp,tar)`\r\ndoes lambda here cause a problem"]}, {"number": 29026, "title": "LSTM with sample_weight fails with batch_size > 1", "body": "**System information**\r\n\r\n* Have I written custom code: Yes\r\n* OS Platform and Distribution: Debian 9.9\r\n* TensorFlow installed from: pip\r\n* TensorFlow version: 1.13.1\r\n* Python version: 3.7.3\r\n* GPU model and memory: n/a - tested in CPU mode\r\n\r\n**Describe the current behavior**\r\n\r\nAn unexpected error occurs when training an LSTM with `sample_weight` and `batch_size > 1`. The error does not occur if `batch_size = 1`, or if omitting `sample_weight`.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect to be able to train an LSTM using `sample_weight` and `batch_size > 1`. As far as I understand it, `sample_weight` can be used for weighting the loss function, and according to the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model) it can be \"a flat (1D) Numpy array with the same length as the input samples\" - i.e. same length as `batch_size`.\r\n\r\n**Code to reproduce the issue**\r\n\r\nHere's a minimal example:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nbatch_size = 32\r\nsequence_len = 1\r\nembedding_size = 100\r\n\r\nx_train = np.random.randn(batch_size, sequence_len, embedding_size)\r\ny_train = np.random.randn(batch_size, embedding_size)\r\nsample_weight = np.random.randn(batch_size)\r\n\r\ntrain_input = tf.keras.Input(shape=(sequence_len, embedding_size),\r\n                             batch_size=batch_size)\r\n\r\nlstm_layer = tf.keras.layers.LSTM(200,\r\n                                  return_sequences=False,\r\n                                  )(train_input)\r\n\r\ndense_layer = tf.keras.layers.Dense(embedding_size,\r\n                                    )(lstm_layer)\r\n\r\nmodel = tf.keras.models.Model(inputs=train_input, outputs=dense_layer)\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\r\n              loss=tf.losses.mean_squared_error)\r\n\r\nloss = model.train_on_batch(x_train,\r\n                            y=y_train,\r\n                            sample_weight=sample_weight)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nTraceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"bug_minimal_example.py\", line 35, in <module>\r\n    sample_weight=sample_weight)\r\n  File \"/home/john/miniconda3/envs/py_main/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1188, in train_on_batch\r\n    outputs = self.train_function(ins)  # pylint: disable=not-callable\r\n  File \"/home/john/miniconda3/envs/py_main/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3076, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/john/miniconda3/envs/py_main/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\r\n    run_metadata_ptr)\r\n  File \"/home/john/miniconda3/envs/py_main/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got 32\r\n\t [[{{node loss/dense_loss/Squeeze}}]]\r\n```", "comments": ["I was able reproduce the mentioned issue. ", "I don't think this is the issue for LSTM, but more generally for loss and sample weights. Adding @pavithrasv.", "Tried to reproduce the issue, it repros on nightly but does not repro on 2.0 nightly. Yet to dig into what changed between the versions with regards to this issue. ", "The issue is because you are using `tf.losses.mean_squared_error` with keras. This is not supported in 1.x. TensorFlow losses do not work with Keras in 1.x. You can replace the loss with `tf.keras.losses.mean_squared_error` or simple the string 'mse' and your code should work. \r\n\r\nPlease let me know if that fixed the issue for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29026\">No</a>\n", "@pavithrasv Great, I agree that this resolves the issue. Thanks a lot!"]}, {"number": 29025, "title": "DOC: fixed formatting error", "body": "fixed formatting error by adding closing $$\r\n\r\ncc @dynamicwebpaige ", "comments": []}]