[{"number": 33256, "title": " [ROCm] Fix for the broken ROCm CSB. ", "body": "Now that the PR #32296, and the fix for PR #32444 is rolled back in, all the tests that were failing because of them have started passing again. However there are some other test failures that have crept in during the time the CSB was in a broken state. The test failures are due to new tests/subtests that check functionality that is currently not supported on the ROCm platform (complex data types in BLAS operations, double datatype in convolution and pooling operations). This PR skips such tests to get the ROCm CSB passing again.\r\n\r\n---------------------------\r\n\r\n@whchung @jeffdaily @chsigg \r\n\r\n", "comments": ["@whchung please re-approve. \r\n\r\nI pushed out a new commit to resolve the py_lint errors found in the `Ubuntu Sanity` CI run for the previous commit.\r\n\r\nthanks\r\n\r\ndeven"]}, {"number": 33255, "title": "tcmalloc: large alloc on  Colab and Tensorflow killed on local machine due to over consumption of RAM", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Conda\r\n- TensorFlow version (use command below): tensorflow-gpu version 1.9.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version:  V10.1.243\r\n- GPU model and memory: Quadro RTX 5000; and 16 GB RAM\r\n\r\n**Describe the current behavior**\r\n\r\nThe tensorflow API always tries to consume the maximum RAM even when I have a GPU and the kernel gets killed while training my deep learning algorithm. I referred online on multiple sources ([1](https://stackoverflow.com/questions/45077571/tensorflow-training-killed-by-system), [2](https://stackoverflow.com/questions/42205205/tensorflow-python-script-getting-killed), [3](https://stackoverflow.com/questions/49442670/my-process-being-killed-the-moment-it-start-training-tensorflow-object-detectio), [4](https://stackoverflow.com/questions/45150773/tensorflow-object-detection-training-killed-resource-starvation), [5](https://github.com/tensorflow/models/issues/3497), [6](https://github.com/tensorflow/tensorflow/issues/29365)) and tried the following things :\r\n\r\n1.  Reduce the batch size\r\n2. Change the optimizer from adam to momentum\r\n\r\nHowever, none of these suggestions helped to solve the problem.\r\n\r\n**Describe the expected behavior**\r\n\r\nBe able to train without over consumption of memory and not cause the tensorflow kernel to get killed\r\n\r\n**Code to reproduce the issue**\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI ran the following code in an ipython notebook in both my local machine (local GPU) and Google Colab :\r\n```\r\n!git clone https://github.com/charlesq34/pointnet.git\r\ncd pointnet/sem_seg/\r\n!sh download_data.sh\r\n!python train.py --log_dir log6 --test_area 6\r\n```\r\n**Other info / logs**\r\n\r\nThe error log is very long and hence I am attaching it in a separate text file here : \r\n[ERROR_LOG.txt](https://github.com/tensorflow/tensorflow/files/3718556/ERROR_LOG.txt)\r\n\r\n", "comments": ["@arunumd, You would like to try with latest Tenosrflow version 2.0.0. Thanks!", "@gadagashwini I tried to install tensorflow 2 using the following command `pip install --upgrade tensorflow==2.0.0-rc0` and then ran the ipython notebook. But the symptoms still don't seem to change. I get the same error still. As you can see below, the **_tcmalloc_** error still persists and there is also a new error related to lack of compatibility between tensorflow2 and tensorflow1\r\n\r\n```\r\ntcmalloc: large alloc 3477749760 bytes == 0xdbcc6000 @  0x7fdc37e4b1e7 0x7fdc33bf3d51 0x7fdc33c58a84 0x7fdc33c58bc3 0x7fdc33cf8ade 0x7fdc33cf9344 0x7fdc33cf9492 0x4f8925 0x4f98c7 0x4f6128 0x4f9023 0x6415b2 0x64166a 0x643730 0x62b26e 0x4b4cb0 0x7fdc37a48b97 0x5bdf6a\r\n(23585, 4096, 9)\r\n(23585, 4096)\r\ntcmalloc: large alloc 2992029696 bytes == 0x1b0d8c000 @  0x7fdc37e4b1e7 0x7fdc33bf3d51 0x7fdc33c58a84 0x7fdc33c58bc3 0x7fdc33ce618a 0x7fdc33ce65d8 0x4f9ba9 0x4f6128 0x4f9023 0x6415b2 0x64166a 0x643730 0x62b26e 0x4b4cb0 0x7fdc37a48b97 0x5bdf6a\r\n(20291, 4096, 9) (20291, 4096)\r\n(3294, 4096, 9) (3294, 4096)\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 276, in <module>\r\n    train()\r\n  File \"train.py\", line 126, in train\r\n    pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)\r\n  File \"/content/pointnet/sem_seg/model.py\", line 13, in placeholder_inputs\r\n    pointclouds_pl = tf.placeholder(tf.float32,\r\nAttributeError: module 'tensorflow' has no attribute 'placeholder'\r\n```", "@arunumd, Tensorflow supports manual device placement and limiting gpu memory growth. Please refer [this link](https://www.tensorflow.org/guide/gpu#manual_device_placement). Please let us know if that helps. Thanks!", "@gadagashwini After running through a bunch of problems with tensorflow during initial development stage, we felt it was better to look for another reliable API. Hence we made a decision to move to PyTorch. I will be glad to look into this problem again later when I will have time. For now the issue is not resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33255\">No</a>\n", "@arunumd In Tensorflow 2.0, there is no placeholder. If you are using TF 1.x code, You need to update your TF1.x code to TF2.0 code and then run it. \r\n\r\nPlease take a look at this [question.](https://stackoverflow.com/questions/56226284/why-do-i-get-attributeerror-module-tensorflow-has-no-attribute-placeholder).", "@gowthamkpr That suggestion sounds irrelevant to the problem. As per the original description, the stated Tensorflow version is 1.x. So technically there is no necessity to upgrade code to TF 2.x when the installed Tensorflow version is 1.x The code is supposed to run properly as a Tensorflow 1.x code. But it clearly doesn't (neither in 1.x nor in 2.x)", "> I tried to install tensorflow 2 using the following command pip install --upgrade tensorflow==2.0.0-rc0 and then ran the ipython notebook. But the symptoms still don't seem to change. I get the same error still. As you can see below, the tcmalloc error still persists and there is also a new error related to lack of compatibility between tensorflow2 and tensorflow1\r\n\r\nI was referring this comment @arunumd as there is no concept of palce holders in TF2.0", "> > I tried to install tensorflow 2 using the following command pip install --upgrade tensorflow==2.0.0-rc0 and then ran the ipython notebook. But the symptoms still don't seem to change. I get the same error still. As you can see below, the tcmalloc error still persists and there is also a new error related to lack of compatibility between tensorflow2 and tensorflow1\r\n> \r\n> I was referring this comment @arunumd as there is no concept of palce holders in TF2.0\r\n\r\n@gadagashwini suggested I try running the same code with TF2.0. So I tried. It didn't work neither. I am not sure if @gadagashwini also expected me to refactor the whole code ? I do not want to refactor anything.. I am interested in knowing whether or not TF1.x is capable of running this code without the `tcmalloc` error ? Why is this error being caused ? I think we have to focus on the root problem..", "We cannot exactly trace what the error is but please take a look at this issue [here](https://stackoverflow.com/questions/9077457/how-to-trace-tcmalloc-large-alloc) and try to replicate it.", "Closing this issue due to the lack of recent activity. Please add additional comments and we can open this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33255\">No</a>\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "@gowthamkpr i faced the same issue .\r\ntensorflow-gpu==1.15.0\r\nkeras==2.2.4\r\nin colab\r\n", "same here.\r\ntorch==1.4.0\r\nin colab", "I am interested in seeing a long term solution for this issue which seems to be quite common for many people who are using Tensorflow 1.x. For what it is worth based on the face value of it, it seems like an API issue which cannot be fixed by users. This issue is not resolved until it is proven that there is a permanent fix to this problem.", "I think it's because of loading a large file. I have the same problem but when I truncate my file error disappears", "What about tensorflow-cpu ? There is no options to lower memory usage like tensorflow for gpu. I've 32GB RAM with a 5900x and it just kill when memory is full like a bad c program"]}, {"number": 33254, "title": "Share memory between numpy and tensorflow doesn't work", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Python\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nAs described in introduction, tf will try to share memory between tf and numpy when possible. However I couldn't figure out how to do this.\r\n\r\n```python\r\na = tf.constant([3, 4]).cpu()\r\nb = tf.numpy()\r\nb[0] = 1\r\nprint(a)\r\n# [3, 4]\r\nprint(b)\r\n# [1, 4]\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n```python\r\na = tf.constant([3, 4]).cpu()\r\nb = tf.numpy()\r\nb[0] = 1\r\nprint(a)\r\n# [1, 4]\r\nprint(b)\r\n# [1, 4]\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@VoVAllen ,\r\nWhen tried executing the given code `AttributeError: module 'tensorflow' has no attribute 'numpy' `is faced, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/067537c18ac24ca5884996792c77085e/33254.ipynb) of colab. Thanks!", "@oanush Sorry I mistakenly write the code, it should be `a.numpy()` instead of `tf.numpy()`\r\n\r\nThe following code should work \r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint(tf.__version__) # 2.0.0\r\ntf.executing_eagerly() # True\r\n\r\na = tf.constant([3, 4]).cpu()\r\nb = a.numpy()\r\nb[0] = 1\r\nprint(a)\r\n# [3, 4], Expected to be [1, 4] if share memory with numpy array\r\nprint(b)\r\n# [1, 4]\r\n```", "As described in https://www.tensorflow.org/tutorials/customization/basics#numpy_compatibility, they should share the underlying memorys.", "Issue replicating with TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/3554926482eac5992e49569227accc10/33254.ipynb) of collab. Thanks!", "@VoVAllen,\r\nBelow Text from the [Tutorial](https://www.tensorflow.org/tutorials/customization/basics#numpy_compatibility) doesn't say that Numpy Array and Tensors share the Underlying Memory always, but it was a conditional statement (\"if possible\" was mentioned).\r\n\r\n> These conversions are typically cheap since the array and tf.Tensor share the underlying memory representation, if possible. However, sharing the underlying representation isn't always possible since the tf.Tensor may be hosted in GPU memory while NumPy arrays are always backed by host memory, and the conversion involves a copy from GPU to host memory.\r\n\r\nIt is evident from the [Implementation of Tensor.numpy()](https://github.com/tensorflow/tensorflow/blob/afa2418f90f7cc15e3ada12e7760c47b79404a23/tensorflow/python/framework/ops.py#L923#L939)]  as well that Sharing of Underlying Memory is not Mandatory.\r\n\r\nBelow sentence states that,\r\n\r\n>     TODO(ashankar,agarwal): Perhaps this should NOT reference the underlying\r\n>     buffer but instead always explicitly copy? Note that currently it may or may\r\n>     not copy based on whether the numpy data is properly aligned or not.\r\n\r\nConsidering all these observations, as per my understanding, this behavior is expected. Please let me know your thoughts. Thanks!", "Hi,\r\n\r\nThanks for your reply.\r\n\r\nActually I think I found the bug here. At here https://github.com/tensorflow/tensorflow/blob/afa2418f90f7cc15e3ada12e7760c47b79404a23/tensorflow/python/framework/ops.py#L939,\r\n\r\nIf it is a numpy.ndarray, why not directly return `maybe_arr`? The logic here says return a copy of numpy.ndarray, or directly return something which is not a numpy.ndarray. It's weird.\r\n\r\nAnd I checked `maybe_arr` shares memory with original tensor, which as expected.\r\n\r\nBy the following code I got what I expected:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint(tf.__version__) # 2.0.0\r\ntf.executing_eagerly() # True\r\n\r\na = tf.constant([3, 4]).cpu()\r\nb = a._numpy() # Change from numpy() to _numpy()\r\nb[0] = 1\r\nprint(a)\r\n# [1, 4]\r\nprint(b)\r\n# [1, 4]\r\n```\r\n\r\nAt least there're some inconsistency between the doc and the behavior,", "I checked some history commit (https://github.com/tensorflow/tensorflow/commit/7caec689acd6a4d173a5dd99aa0da9961063058a), the copy behavior seems intentional. The comment at `numpy` function seems outdated. I couldn't find way that share memory between tensorflow and numpy, no matter from numpy to tf or from tf to numpy.", "At https://github.com/tensorflow/tensorflow/blame/5278b8509e2cd1b2847315db46fc0f958824cfce/tensorflow/python/framework/ops.py#L709, numpy is zero-copyed from tf. \r\n\r\nThe next commit (https://github.com/tensorflow/tensorflow/commit/ca1b54a83ae352c41bb285f0a6ecace20f706ac1?diff=split), fixed that the `.cpu()` operation should not be recorded. To fix this, this author copied tensor to create a new one which is not recorded. This fix seems not ideal, since avoiding recording the operation doesn't have to copy a new one. From here, it becomes the copy behavior.\r\n\r\nLater at https://github.com/tensorflow/tensorflow/blob/7caec689acd6a4d173a5dd99aa0da9961063058a/tensorflow/python/eager/pywrap_tensor.cc#L631, @superbobry mentioned that it has to copy to avoid reference counting problem on the data memory.\r\n\r\n@superbobry Sorry to bother you. Could you comment a bit on this issue, that at what condition tf tensor can share memory with numpy? Also could you say a bit more about the reference counting problem mentioned above? Many thanks!\r\n", "NumPy arrays are mutable whereas tensors aren't, therefore `.numpy()` has to copy to avoid breaking the runtime's invariants. In theory, a NumPy array could be flagged as non-writable (via `a.flags[\"WRITABLE\"]`), but unfortunately a lot of internal (and I assume external) users already rely on the mutability of `.numpy()`.\r\n\r\nTwo caveats:\r\n\r\nTensor supports the buffer interface, so you could get a readonly NumPy array via\r\n\r\n```python\r\n>>> t = tf.constant([42])\r\n>>> a = np.asarray(memoryview(t))\r\n>>> a\r\narray([42], dtype=int32)\r\n```\r\n\r\nPassing a CPU tensor directly to any NumPy APIs is zero-copy, because NumPy uses buffer interface behind the scenes (but NumPy will allocate a new array for the result):\r\n\r\n```python\r\n>>> np.square(t)\r\narray([1764], dtype=int32)\r\n```", "Thanks a lot for the explanation. This makes much more sense to me. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33254\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33254\">No</a>\n"]}, {"number": 33253, "title": "sqlite dataset fails to raise a StopIteration: incorrect result", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): issue is reproducible with both\r\n- TensorFlow version (use command below): Confirmed on 2.0 and 1.14 \r\n- Python version: python2\r\n\r\n**Describe the current behavior**\r\nAfter creating a `tf.data.experimental.SqlDataset()` and performing a map and batch operaion, the dataset fails to raise a StopIteration after going through the entire database, and begins to repeat / recycle values incorrectly. \r\n\r\n**Describe the expected behavior**\r\nThe dataset stops after returning all the records in the sqlite database.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n# --- Create a dummy sqlite database ---\r\nimport os\r\nimport sqlite3\r\npth = \"/tmp/bug-report.sqlite\"\r\nif os.path.exists(pth): os.unlink(pth)\r\n\r\nquery = 'SELECT * FROM data'\r\n\r\ncon = sqlite3.connect(pth)\r\nc = con.cursor()\r\nc.execute('CREATE TABLE data (col1 Int)')\r\n\r\nfor i in range(3):\r\n  c.execute('INSERT INTO data VALUES (' + str(i) + ')')\r\n\r\ncon.commit()\r\n\r\n# print the db, just to show what's in there\r\nc.execute(query)\r\nprint \"Actual query results: \", c.fetchall()\r\ncon.close()\r\n\r\n# --- create a tf sqlite dataset ---\r\nimport tensorflow as tf\r\nprint tf.version.VERSION\r\n\r\nds = tf.data.experimental.SqlDataset('sqlite', pth, query, (tf.int32))\r\nds = ds.map(lambda x: tf.identity(x))\r\n\r\n\r\n# this is supposed to terminate after only two batchs since the sqlite db only\r\n# has 2 entries, but it goes forever\r\nprint \"Batch size of 2:\"\r\ni = 0\r\nfor e in ds.batch(2):\r\n  print e\r\n  \r\n  i += 1\r\n  if i > 2: print \"  Should have stopped by now\"\r\n  if i > 10: print \"    breaking early\"; break\r\n  \r\n# if batch size is larger than the db size, it also fails to stop\r\nprint \"Batch size of 4:\"\r\ni = 0\r\nfor e in ds.batch(4):\r\n  print e\r\n  \r\n  i += 1\r\n  if i > 1: print \"  Should have stopped by now\"\r\n  if i > 10: print \"    breaking early\"; break\r\n\r\n# if batch size is exactly a multiple of the sqlite db size, then it does\r\n# raise a StopIteration correctly\r\nprint \"Batch size of 3:\"\r\nfor e in ds.batch(3):\r\n  print e\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/8462255/66663537-ecef8080-ec18-11e9-84fd-afd02c8a66ab.png)\r\n\r\n\r\n**Other info / logs**\r\nThis has been reproduced on tensorflow versions 2.0, 1.14 and 1.15\r\n", "comments": ["Added PR #33271 for the fix.", "Thank you for the quick fix! Any chance this could make it into 1.15 too?", "@t-kalinowski We will have to wait for the PR to be merged into master. Once it is in the master, it might be cherry-picked into release 1.15 or 2.0. Though given 1.15's release schedule, I would not count on this fix being picked up in 1.15.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33253\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33253\">No</a>\n"]}, {"number": 33252, "title": "Variable name misspelt in example", "body": " Loading is spelled loadimg in one example(pose2seg)  And set to False. But loading is expected.  A text search will get you there. Although its in cluster_pose.py.  not a biggy, i know, still, one of those things that may create a problem if someone assumes its set", "comments": ["Its in Pose2Seg's cluster_pose.py      and its     loadimg=False    ", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@jaicie \r\nPlease, let us know if any update on this issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 33251, "title": "How to confiture TF2.0 compilation with Intel MPI ?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): source (configuration process pbm)\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: not concerned\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: 10.0 / 7.6.4 \r\n- GPU model and memory: Tesla K80 / 256Gb\r\n\r\n**Describe the problem**\r\nproblem in ./configure when asking for MPI Toolkit path\r\nI found description howto for openmpi (https://github.com/robsanpam/TensorFlow_From_Sources), but I want to use intel mpi.\r\nwhat I need to indicate in path for intel mpi (intel cluster edition 2019) ?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI input : /opt/intel/compilers_and_libraries/linux/mpi/intel64\r\n\r\n**Any other info / logs**\r\nInvalid path to the MPI Toolkit. /opt/intel/compilers_and_libraries/linux/mpi/intel64 or True or False or False cannot be found\r\n", "comments": ["@VitaMusic ,\r\nCan you please provide `./configure ` output? also provide more information on sequence of commands / steps that was followed before facing the issue. Thanks!", "please find here :\n\n$ ./configure\nWARNING: --batch mode is deprecated. Please instead explicitly shut down\nyour Bazel server using the command \"bazel shutdown\".\nYou have bazel 0.29.1 installed.\nPlease specify the location of python. [Default is /usr/bin/python]:\n\n\nFound possible Python library paths:\n  /opt/anaconda3/lib/python3.6/site-packages\nPlease input the desired Python library path to use.  Default is\n[/opt/anaconda3/lib/python3.6/site-packages]\n\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]:\nXLA JIT support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]:\nNo OpenCL SYCL support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with ROCm support? [y/N]:\nNo ROCm support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\nCUDA support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\nTensorRT support will be enabled for TensorFlow.\n\nCould not find any NvInferVersion.h matching version '' in any subdirectory:\n        ''\n        'include'\n        'include/cuda'\n        'include/*-linux-gnu'\n        'extras/CUPTI/include'\n        'include/cuda/CUPTI'\nof:\n        '/lib'\n        '/lib64'\n        '/usr'\n        '/usr/lib64/R/lib'\n        '/usr/lib64/dyninst'\n        '/usr/lib64/iscsi'\n        '/usr/lib64/mysql'\n        '/usr/lib64/qt-3.3/lib'\n        '/usr/local/cuda'\nAsking for detailed CUDA configuration...\n\nPlease specify the CUDA SDK version you want to use. [Leave empty to\ndefault to CUDA 10]: 10.0\n\n\nPlease specify the cuDNN version you want to use. [Leave empty to default\nto cuDNN 7]: 7.6.4\n\n\nPlease specify the TensorRT version you want to use. [Leave empty to\n default to TensorRT 5]: 6.0.1\n\n\nPlease specify the locally installed NCCL version you want to use. [Leave\nempty to use http://github.com/nvidia/nccl]:\n\n\nPlease specify the comma-separated list of base paths to look for CUDA\nlibraries and headers. [Leave empty to use the default]:\n/usr/local/cuda,/usr/local/cuda/bin,/usr/local/cuda/lib64,/usr/local/cuda/include,/opt/TensorRT-6.0.1.5/bin,/opt/TensorRT-6.0.1.5/lib,/opt/TensorRT-6.0.1.5/include\n\n\nFound CUDA 10.0 in:\n    /usr/local/cuda/lib64\n    /usr/local/cuda/include\nFound cuDNN 7 in:\n    /usr/local/cuda/lib64\n    /usr/local/cuda/include\nFound TensorRT 6 in:\n    /opt/TensorRT-6.0.1.5/lib\n    /opt/TensorRT-6.0.1.5/include\n\n\nPlease specify a list of comma-separated CUDA compute capabilities you want\nto build with.\nYou can find the compute capability of your device at:\nhttps://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases\nyour build time and binary size, and that TensorFlow only supports compute\ncapabilities >= 3.5 [Default is: 3.7,3.7]: 3.7,6.0,7.0,7.5\n\n\nDo you want to use clang as CUDA compiler? [y/N]:\nnvcc will be used as CUDA compiler.\n\nPlease specify which gcc should be used by nvcc as the host compiler.\n[Default is /usr/bin/gcc]:\n\n\nDo you wish to build TensorFlow with MPI support? [y/N]: y\nMPI support will be enabled for TensorFlow.\n\nPlease specify the MPI toolkit folder. [Default is\n/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64]:\n\n\nInvalid path to the MPI Toolkit.\n/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/include or\nTrue or False or False cannot be found\nPlease specify the MPI toolkit folder. [Default is\n/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64]:\n\n\n----------------------------------\n\nsourcing of mpi vars does not help\nsource\n/home/apps/intel2018u3/compilers_and_libraries/linux/mpi/intel64/bin/mpivars.sh\nrelease\n\n\n\nOn Tue, Oct 15, 2019 at 11:19 AM oanush <notifications@github.com> wrote:\n\n> @VitaMusic <https://github.com/VitaMusic> ,\n> Can you please provide ./configure output? also provide more information\n> on sequence of commands / steps that was followed before facing the issue.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33251?email_source=notifications&email_token=AC7AFFPT4K5STY4ACKONC7DQOWDKNA5CNFSM4I72RUOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBIA6TQ#issuecomment-542117710>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFNDXBLZQXTNEBWNVETQOWDKNANCNFSM4I72RUOA>\n> .\n>\n", "Intel MPI support has been moved to https://github.com/tensorflow/networking. Please follow the instructions there.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33251\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33251\">No</a>\n"]}, {"number": 33250, "title": "ValueError: Tensor Tensor by tf.keras.backend.function", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: Tensorflow 2.0.0\r\n- Python version: Python 3.7.2\r\n- Installed using virtualenv? pip? conda?: Anaconda\r\n\r\n**Describe the problem**\r\n> ValueError: Tensor Tensor(\"Mean:0\", shape=(512,), dtype=float32) is not an element of this graph.\r\n\r\nI'm run this line of code show above error message, `iterate = tf.keras.backend.function([model.input], [pooled_grads, last_conv_layer.output[0]])`\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nfrom tensorflow.keras.applications.vgg16 import VGG16\r\nfrom tensorflow.keras.preprocessing import image\r\nfrom tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\ntf.keras.backend.clear_session()\r\n\r\nimg_path = '/Users/Klaus/downloads/flower_photos/daisy/5547758_eea9edfd54_n.jpg'\r\n\r\nimg = image.load_img(img_path, target_size=(224, 224))\r\nx = image.img_to_array(img)\r\nx = np.expand_dims(x, axis=0)\r\nx = preprocess_input(x)\r\n\r\n# Bug: loading multiple models is by manually specifying different graph and session for each loaded model\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n       session = tf.compat.v1.Session()\r\n       with session.as_default():\r\n          ## your load model code\r\n          model = VGG16(weights='imagenet')\r\n# save each graph and session value to some variable for later prediction use.\r\ngraph_var = graph\r\nsession_var = session\r\nwith graph_var.as_default():\r\n     with session_var.as_default():\r\n            preds = model.predict(x)\r\n\r\nprint('Predicted:', decode_predictions(preds, top=3)[0])\r\n\r\nnp.argmax(preds[0])\r\n\r\nflower_output = model.output[:, 309]\r\nlast_conv_layer = model.get_layer('block5_conv3')\r\n\r\ngrads = tf.keras.backend.gradients(flower_output, last_conv_layer.output)[0]\r\npooled_grads = tf.keras.backend.mean(grads, axis=(0, 1, 2))\r\n\r\niterate = tf.keras.backend.function([model.input], [pooled_grads, last_conv_layer.output[0]])\r\n```\r\n\r\n**Whole error mesage**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-360-5ea87dd3ba6a> in <module>\r\n----> 1 iterate = tf.keras.backend.function([model.input], [pooled_grads, last_conv_layer.output[0]])\r\n\r\n~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/backend.py in function(inputs, outputs, updates, name, **kwargs)\r\n   3780                'backend') % key\r\n   3781         raise ValueError(msg)\r\n-> 3782   return GraphExecutionFunction(inputs, outputs, updates=updates, **kwargs)\r\n   3783 \r\n   3784 \r\n\r\n~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/backend.py in __init__(self, inputs, outputs, updates, name, **session_kwargs)\r\n   3432     # dependencies in call.\r\n   3433     # Index 0 = total loss or model output for `predict`.\r\n-> 3434     with ops.control_dependencies([self.outputs[0]]):\r\n   3435       updates_ops = []\r\n   3436       for update in updates:\r\n\r\n~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py in control_dependencies(control_inputs)\r\n   5255     return NullContextmanager()\r\n   5256   else:\r\n-> 5257     return get_default_graph().control_dependencies(control_inputs)\r\n   5258 \r\n   5259 \r\n\r\n~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py in control_dependencies(self, control_inputs)\r\n   4689           (hasattr(c, \"_handle\") and hasattr(c, \"op\"))):\r\n   4690         c = c.op\r\n-> 4691       c = self.as_graph_element(c)\r\n   4692       if isinstance(c, Tensor):\r\n   4693         c = c.op\r\n\r\n~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)\r\n   3608 \r\n   3609     with self._lock:\r\n-> 3610       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n   3611 \r\n   3612   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):\r\n\r\n~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)\r\n   3687       # Actually obj is just the object it's referring to.\r\n   3688       if obj.graph is not self:\r\n-> 3689         raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\r\n   3690       return obj\r\n   3691     elif isinstance(obj, Operation) and allow_operation:\r\n\r\nValueError: Tensor Tensor(\"Mean:0\", shape=(512,), dtype=float32) is not an element of this graph.\r\n```", "comments": ["I have tried on Jupyter notebook with TF version 2.0  and was able to reproduce the issue.Please, find the files in the attachment. Thanks!\r\n[test.tar.gz](https://github.com/tensorflow/tensorflow/files/3728768/test.tar.gz)\r\n", "@KLzC,\r\nCan you please refer to [Github Issue 1](https://github.com/keras-team/keras/issues/6462) and [Github  Issue 2](https://github.com/keras-team/keras/issues/2397) and this [Stack Overflow Issue](https://stackoverflow.com/questions/47115946/tensor-is-not-an-element-of-this-graph) and let us know if it helps. Thanks!", "@rmothukuru, Thanks for your information. \r\nThose posts were try to slove by set paremeter `graph = tf.Graph()` , `tf.compat.v1.disable_eager_execution()`, `tf.keras.backend.clear_session()` but occur the same error.\r\n\r\nI infer there might be a bug by **tensorflow2.0** and **Keras** that code working fine on TensorFlow 1.X. Here is sample code from [Class Activation Map](https://github.com/nickbiso/Keras-Class-Activation-Map/blob/master/Class%20Activation%20Map(CAM).ipynb)\r\n\r\n", "Have there been any other developments on this? I'm running into the same issue (attempting to run almost the same exact code) with TF 2.1.0.", "I\u2019m not receiving any update for this issue.\n\nSincerely,\nKlaus\n\n> On Feb 13, 2020, at 01:33, Daniel Brannock <notifications@github.com> wrote:\n> \n> \ufeff\n> Have there been any other developments on this? I'm running into the same issue (attempting to run almost the same exact code) with TF 2.1.0.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "@KLzC As the error shows, some of the ops are outside of the session graph. So I arranged ops under same same and everything works as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/382ab1f45023244d3e21230e79861948/untitled94.ipynb). Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@KLzC \r\n\r\nAny update on this issue please. Thanks!", "Hi @jvishnuvardhan , @ravikyram :\r\n\r\nThanks so much, It's working to access image :)\r\n\r\n![flower_cam](https://user-images.githubusercontent.com/5106837/80194285-bb865180-864c-11ea-9874-e2139f3dacdc.jpg)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33250\">No</a>\n"]}, {"number": 33249, "title": "Use tf.nn.selu in keras.activations.selu", "body": "`tf.nn` has an [implementation of `selu`](https://github.com/tensorflow/tensorflow/blob/e1c98eeb8ff8a25d2ecdcf1961974d6c9c1e3df4/tensorflow/core/kernels/relu_op_functor.h#L160-L198) that does not rely on [`tf.where`](https://github.com/tensorflow/tensorflow/blob/cb2cdd81f6c2bf2ae22e7ff16d80d8fae0a8587e/tensorflow/python/keras/backend.py#L4397).\r\n\r\nThis simplifies the code and the new implementation is approximately 4 - 5 times faster on my laptop CPU:\r\n```python\r\nIn [3]: x = tf.random.uniform((1024, 12, 12, 1024), -3., 3.)                                                                                                \r\n\r\nIn [4]: %timeit tf.nn.selu(x)                                                                                                                               \r\n240 ms \u00b1 8.76 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [5]: %timeit tf.keras.activations.selu(x)                                                                                                                \r\n1.02 s \u00b1 39.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```", "comments": ["@tanzhenyu Could I get a review for this PR?"]}, {"number": 33248, "title": "Dimensional Reasoning", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow2.0\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nNo\r\n**Any Other info.**\r\n\r\n\r\n`    \r\n\r\ndef roi_align_one_layer(feat, bbox, s, align_c):\r\n\r\n            s_y, s_x = s\r\n            num_bbox = tf.shape(bbox)[0]\r\n            feat_shape = tf.shape(feat)\r\n\r\n# tensorflow know c_i here\r\n            h_i, w_i, c_i = feat_shape[0], feat_shape[1], feat_shape[2]\r\n\r\n\r\n            h_f, w_f = tf.cast(h_i, tf.float32), tf.cast(w_i, tf.float32)\r\n            bbox_y1 = bbox[:, 0:1]\r\n            bbox_x1 = bbox[:, 1:2]\r\n            bbox_y2 = bbox[:, 2:3]\r\n            bbox_x2 = bbox[:, 3:4]\r\n            bbox_h = bbox_y2 - bbox_y1\r\n            bbox_w = bbox_x2 - bbox_x1\r\n            if align_c:\r\n                off_y = 0. if s_y > 1 else bbox_h / 2.\r\n                off_x = 0. if s_x > 1 else bbox_w / 2.\r\n                grid_y = tf.linspace(0.0, 1.0, s_y)\r\n                grid_x = tf.linspace(0.0, 1.0, s_x)\r\n            else:\r\n                off_y = bbox_h / (2. * s_y)\r\n                off_x = bbox_w / (2. * s_x)\r\n                grid_y = tf.linspace(0.0, 1.0, s_y + 1)[:-1]\r\n                grid_x = tf.linspace(0.0, 1.0, s_x + 1)[:-1]\r\n            grid_y = tf.expand_dims(tf.matmul(bbox_h, tf.expand_dims(grid_y, axis=0)) + off_y + bbox_y1, axis=-1)\r\n            grid_x = tf.expand_dims(tf.matmul(bbox_w, tf.expand_dims(grid_x, axis=0)) + off_x + bbox_x1, axis=-1)\r\n\r\n            grid_y = tf.where(grid_y < 0., 0., grid_y)\r\n            grid_x = tf.where(grid_x < 0., 0., grid_x)\r\n            grid_y = tf.where(grid_y > h_f-1., h_f-1., grid_y)\r\n            grid_x = tf.where(grid_x > w_f-1., w_f-1., grid_x)\r\n            grid_y = tf.tile(grid_y, [1, 1, s_y])\r\n            grid_x = tf.tile(grid_x, [1, 1, s_x])\r\n            grid_y = tf.reshape(grid_y, [num_bbox, -1])\r\n            grid_x = tf.reshape(grid_x, [num_bbox, -1])\r\n            grid_y1 = tf.math.floor(grid_y)\r\n            grid_y2 = tf.math.floor(grid_y+1.)\r\n            grid_x1 = tf.math.floor(grid_x)\r\n            grid_x2 = tf.math.floor(grid_x+1.)\r\n            wey1 = tf.expand_dims(grid_y - grid_y1, axis=-1)\r\n            wey2 = tf.expand_dims(grid_y2 - grid_y, axis=-1)\r\n            wex1 = tf.expand_dims(grid_x - grid_x1, axis=-1)\r\n            wex2 = tf.expand_dims(grid_x2 - grid_x, axis=-1)\r\n            grid_y2 = tf.where(grid_y2 > h_f - 1., h_f - 1., grid_y2)\r\n            grid_x2 = tf.where(grid_x2 > w_f - 1., w_f - 1., grid_x2)\r\n            grid_y1 = tf.cast(grid_y1, tf.int32)\r\n            grid_y2 = tf.cast(grid_y2, tf.int32)\r\n            grid_x1 = tf.cast(grid_x1, tf.int32)\r\n            grid_x2 = tf.cast(grid_x2, tf.int32)\r\n            grid_11 = grid_y1 * w_i + grid_x1\r\n            grid_12 = grid_y1 * w_i + grid_x2\r\n            grid_21 = grid_y2 * w_i + grid_x1\r\n            grid_22 = grid_y2 * w_i + grid_x2\r\n            feat = tf.reshape(feat, [h_i*w_i, c_i])\r\n            feat_11 = tf.gather(feat, grid_11)\r\n            feat_12 = tf.gather(feat, grid_12)\r\n            feat_21 = tf.gather(feat, grid_21)\r\n            feat_22 = tf.gather(feat, grid_22)\r\n            feat_bilinear = wey2 * (feat_11 * wex2 + feat_12 * wex1) + wey1 * (feat_21 * wex2 + feat_22 * wex1)\r\n\r\n# tensorflow don't know c_i here\r\n            feat_bilinear = tf.reshape(feat_bilinear, [num_bbox, s_y, s_x, c_i])\r\n\r\n\r\n            return feat_bilinear\r\n`\r\n\r\nthe data_format is the 'channels_last'\r\n\r\nwhen I use `y = roi_align_one_layer(feat, bbox, s, align_c)` the tensorflow2.0 know the number of channels of `feat` but don't know the number of channels of `y`,\r\nwhen I use `tf.keras.layers.Conv()(y)`, tensorflow2.0 raise error that tensorflow2.0 don't know the number of input channels", "comments": []}, {"number": 33247, "title": "Saving GRU with dropout to SavedModel fails", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: python 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen Model containing GRU layer, dropout is set and activation='relu', the model is not savable.\r\n\r\nError:\r\nAttempted to save a function b'__inference_GRU_layer_call_fn_8041' which references a symbolic Tensor Tensor(\"dropout/mul_1:0\", shape=(None, 3), dtype=float32) that is not a simple constant. This is not supported.\r\n\r\n**Describe the expected behavior**\r\nModel gets saved.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\ninputs = keras.Input(shape=(784,3), name='digits')\r\nx = layers.GRU(64, activation='relu', name='GRU',dropout=0.1)(inputs)\r\nx = layers.Dense(64, activation='relu', name='dense')(x)\r\noutputs = layers.Dense(10, activation='softmax', name='predictions')(x)\r\n\r\nmodel = keras.Model(inputs=inputs, outputs=outputs, name='3_layer')\r\nmodel.summary()\r\nmodel.save('model',save_format='tf')\r\n```\r\nBased on: https://www.tensorflow.org/guide/keras/save_and_serialize\r\n", "comments": ["Issue replicating for TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/4a4201abcbe95c5eb2e19bc5d3f5bde1/33247.ipynb) of colab.Thanks!\r\n", "@fhausmann,\r\nThe error disappears if `dropout=0` or if `layers.Dense` instead of `layers.GRU`. The link, https://www.tensorflow.org/guide/keras/save_and_serialize, provided by you doesn't demonstrate on how to Save the Model with `layers.GRU`.\r\n\r\nCan you please refer [Image_Captioning_Tutorial](https://www.tensorflow.org/tutorials/text/image_captioning), [Text_Generation_Tutorial](https://www.tensorflow.org/tutorials/text/text_generation) and [NMT_With_Attention_Tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention) where, usage of `layers.GRU` is demonstrated and let us know if it helps. Thanks!", "@rmothukuru Thanks for your answer.\r\nThe error also disappers if 'activation' is not set and yes the link is maybe a bit missleading, since the example there is for Dense layer only. However, I thought this was a good minimal working example also with GRU.\r\nI had a look into your links, but unfortunately non of them demonstrate how to save a model with GRU layer in the SavedModel format.\r\n\r\nI tried for the Text_Generation_Tutorial (added `model.save('model',save_format=\"tf\")` after training, but it also failed here (with another error: AssertionError: Tried to export a function which references untracked object Tensor(\"StatefulPartitionedCall/args_2:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.)\r\n\r\nSeems that GRU models does not allow saving in SavedModel format at the moment?", "Maybe related to #33355 ?", "@fhausmann,\r\nModel is Saved Successfully if you replace `model.save('model',save_format=\"tf\")` with `model.save('model.h5')` . Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/72984b3a326970c85edb8f03312cb199/33247.ipynb).", "Yes, this probably solves the issue for most people, but its a different format , binary HDF5 vs. SavedModel. \r\n@rmothukuru is the SavedModel format deprecated? ", "@fhausmann,\r\nIn your case, you can either use `keras.models.save_model(model=model, filepath='model')` or \r\n`model.save('model')`. \r\nHere is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/01572d4bff7572acba853ef91ebe2efb/33247.ipynb). Please let me know if your issue is resolved. Thanks!", "There is an issue, probably with the argument, `save_format='tf'` because, the code, `model.save('model', save_format='tf')` is resulting in the Error mentioned by you. ", "@rmothukuru \r\nI tried both on your gist and both give me the error. Only the 'model.h5' works in my hands", "I also encountered the same problem. Have you solved it?", "`tf.keras.experimental.export_saved_model(model, 'modelfolder')` and `tf.keras.experimental.load_from_saved_model('modelfolder')`\r\nseems to be a workaround, however they are deprecated.", "To add more information, Model is Saved without any Error if we build the `Model` using `Dense` or using `LSTM and Dense`. But if we build the Model with `GRU` as one of the Layers, it is resulting in either the Error,\r\n`\r\nError:\r\nAttempted to save a function b'__inference_GRU_layer_call_fn_8041' which references a symbolic Tensor Tensor(\"dropout/mul_1:0\", shape=(None, 3), dtype=float32) that is not a simple constant. This is not supported.\r\n`\r\n\r\nor the Error, \r\n\r\n`AssertionError: Tried to export a function which references untracked object Tensor(\"StatefulPartitionedCall/args_2:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.`\r\n\r\nHere is [Gist_of_Error1](https://colab.sandbox.google.com/gist/rmothukuru/00386fc1a493b344282b1b8aa97e20e7/33247.ipynb) and [Gist_Of_Error2](https://colab.sandbox.google.com/gist/rmothukuru/78f245f68f3729eb1c8f0da1e0df93a1/33355.ipynb).\r\n\r\nThe issue, #33355 is similar.", "See the above link. I don't think LSTM works as well", "tf.keras.experimental.export_saved_model still works", "@fhausmann This was resolved in `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/28eb34025ba304b45f2a8bb74c2cd2a4/untitled774.ipynb). \r\n\r\nAs this was resolved, I am closing this issue. Please feel free to reopen if the issue persists later. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33247\">No</a>\n", "Does anyone have the commit ID which resolved this issue? I don't want to run a nightly release, and need to backport the fix into a stable version.", "@phemmer New stable version (`TF2.2`) will be release soon. Please check the recent [announcements here](https://groups.google.com/a/tensorflow.org/forum/#!forum/build). Thanks!", "Can we not just have the commit ID? Yes 2.2 may be close, but it's still a few weeks off I'd imagine. And in addition, I'm using [IBM's Large Model Support patch](https://github.com/IBM/tensorflow-large-model-support), which will likely take an additional several weeks to be updated to 2.2."]}, {"number": 33246, "title": "Better  Dimension reasoning", "body": "`    \r\n\r\ndef roi_align_one_layer(feat, bbox, s, align_c):\r\n\r\n            s_y, s_x = s\r\n            num_bbox = tf.shape(bbox)[0]\r\n            feat_shape = tf.shape(feat)\r\n            h_i, w_i, c_i = feat_shape[0], feat_shape[1], feat_shape[2]\r\n            h_f, w_f = tf.cast(h_i, tf.float32), tf.cast(w_i, tf.float32)\r\n            bbox_y1 = bbox[:, 0:1]\r\n            bbox_x1 = bbox[:, 1:2]\r\n            bbox_y2 = bbox[:, 2:3]\r\n            bbox_x2 = bbox[:, 3:4]\r\n            bbox_h = bbox_y2 - bbox_y1\r\n            bbox_w = bbox_x2 - bbox_x1\r\n            if align_c:\r\n                off_y = 0. if s_y > 1 else bbox_h / 2.\r\n                off_x = 0. if s_x > 1 else bbox_w / 2.\r\n                grid_y = tf.linspace(0.0, 1.0, s_y)\r\n                grid_x = tf.linspace(0.0, 1.0, s_x)\r\n            else:\r\n                off_y = bbox_h / (2. * s_y)\r\n                off_x = bbox_w / (2. * s_x)\r\n                grid_y = tf.linspace(0.0, 1.0, s_y + 1)[:-1]\r\n                grid_x = tf.linspace(0.0, 1.0, s_x + 1)[:-1]\r\n            grid_y = tf.expand_dims(tf.matmul(bbox_h, tf.expand_dims(grid_y, axis=0)) + off_y + bbox_y1, axis=-1)\r\n            grid_x = tf.expand_dims(tf.matmul(bbox_w, tf.expand_dims(grid_x, axis=0)) + off_x + bbox_x1, axis=-1)\r\n\r\n            grid_y = tf.where(grid_y < 0., 0., grid_y)\r\n            grid_x = tf.where(grid_x < 0., 0., grid_x)\r\n            grid_y = tf.where(grid_y > h_f-1., h_f-1., grid_y)\r\n            grid_x = tf.where(grid_x > w_f-1., w_f-1., grid_x)\r\n            grid_y = tf.tile(grid_y, [1, 1, s_y])\r\n            grid_x = tf.tile(grid_x, [1, 1, s_x])\r\n\r\n            grid_y = tf.reshape(grid_y, [num_bbox, -1])\r\n            grid_x = tf.reshape(grid_x, [num_bbox, -1])\r\n            grid_y1 = tf.math.floor(grid_y)\r\n            grid_y2 = tf.math.floor(grid_y+1.)\r\n            grid_x1 = tf.math.floor(grid_x)\r\n            grid_x2 = tf.math.floor(grid_x+1.)\r\n            wey1 = tf.expand_dims(grid_y - grid_y1, axis=-1)\r\n            wey2 = tf.expand_dims(grid_y2 - grid_y, axis=-1)\r\n            wex1 = tf.expand_dims(grid_x - grid_x1, axis=-1)\r\n            wex2 = tf.expand_dims(grid_x2 - grid_x, axis=-1)\r\n            grid_y2 = tf.where(grid_y2 > h_f - 1., h_f - 1., grid_y2)\r\n            grid_x2 = tf.where(grid_x2 > w_f - 1., w_f - 1., grid_x2)\r\n            grid_y1 = tf.cast(grid_y1, tf.int32)\r\n            grid_y2 = tf.cast(grid_y2, tf.int32)\r\n            grid_x1 = tf.cast(grid_x1, tf.int32)\r\n            grid_x2 = tf.cast(grid_x2, tf.int32)\r\n            grid_11 = grid_y1 * w_i + grid_x1\r\n            grid_12 = grid_y1 * w_i + grid_x2\r\n            grid_21 = grid_y2 * w_i + grid_x1\r\n            grid_22 = grid_y2 * w_i + grid_x2\r\n\r\n            feat = tf.reshape(feat, [h_i*w_i, c_i])\r\n            feat_11 = tf.gather(feat, grid_11)\r\n            feat_12 = tf.gather(feat, grid_12)\r\n            feat_21 = tf.gather(feat, grid_21)\r\n            feat_22 = tf.gather(feat, grid_22)\r\n\r\n            feat_bilinear = wey2 * (feat_11 * wex2 + feat_12 * wex1) + wey1 * (feat_21 * wex2 + feat_22 * wex1)\r\n            feat_bilinear = tf.reshape(feat_bilinear, [num_bbox, s_y, s_x, c_i])\r\n            return feat_bilinear\r\n`\r\n\r\nthe input the 'channels_last'\r\n\r\nwhen I use `y = roi_align_one_layer(feat, bbox, s, align_c)` the tensorflow2.0 know the number of channels of feat but don't know the number of channels of y,\r\nwhen I use `tf.keras.layers.Conv()(y)`, tensorflow2.0 raise error that tensorflow2.0 don't know the number of input channels", "comments": []}, {"number": 33245, "title": "Cannot use dict base datasets with keras.Model.fit.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 7.6.2\r\n\r\nIn order to make both dataset and keras model have good structures, I create a dataset and a vanilla model like this.\r\n```python\r\n# dataset is something like <BatchDataset shapes: ({input: (None, 100)}, {output: (None, 10)}), types: ({input: tf.float32}, {output: tf.float32})>\r\n\r\n# subscale model is something like\r\nclass VanillaModel(Model):\r\n\r\n  def __init__(self, num_units, **kwargs):\r\n    super(VanillaModel, self).__init__(**kwargs)\r\n    self.num_units = num_units\r\n\r\n    # One linear projection layer.\r\n    self.dense_proj = tf.keras.layers.Dense(num_units, activation='relu')\r\n\r\n  def call(self, features):\r\n    \"\"\"Forward pass.\"\"\"\r\n    output = self.dense_proj(features['input'])\r\n    return {\r\n        'output': output\r\n    }\r\n```\r\n\r\nWhen I use the dict based dataset (from tfds) with keras.Model.fit, the first call will cause expection as \r\n```python\r\n# Compile model using dict with same keys.\r\nmodel.compile('adam', {'output': 'mse'})\r\n\r\n# The errors\r\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 1248, in cast_if_floating_dtype_and_mismatch\r\n    if target.dtype != out.dtype:\r\nAttributeError: 'str' object has no attribute 'dtype'\r\n```\r\n\r\nI checked the code and found that, when dict is passed, iterating through `zip(targets, outputs)` will just get the keys of the dict, so the string keys have no dtyle.\r\n(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_utils.py#L1246)\r\n![image](https://user-images.githubusercontent.com/11533479/66652073-59916c00-ec67-11e9-8974-eda50bf98e18.png)\r\n\r\nSo how can I use dict based dataset and model with keras.Model.fit?\r\n", "comments": ["@npuichigo, Will it be possible to provide the standalone code to replicate the reported issue. Thanks!", "@npuichigo, Any update on code. Thanks!", "The problem is that, if our model and dataset return dictionaries, how can we compile and fit the model.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass VanillaModel(tf.keras.Model):\r\n\r\n  def __init__(self, num_units, **kwargs):\r\n    super(VanillaModel, self).__init__(**kwargs)\r\n    self.num_units = num_units\r\n\r\n    # One linear projection layer.\r\n    self.dense_proj1 = tf.keras.layers.Dense(num_units, activation='relu')\r\n    self.dense_proj2 = tf.keras.layers.Dense(num_units, activation='relu')\r\n\r\n  def call(self, features):\r\n    \"\"\"Forward pass.\"\"\"\r\n    proj1_output = self.dense_proj1(features['input'])\r\n    proj2_output = self.dense_proj2(features['input'])\r\n    return {\r\n        'proj1_output': proj1_output,\r\n        'proj2_output': proj2_output\r\n    }\r\n\r\ninput_tensor = np.random.normal(size=(50, 32)).astype(np.float32)\r\noutput_tensor1 = np.random.normal(size=(50, 16)).astype(np.float32)\r\noutput_tensor2 = np.random.normal(size=(50, 16)).astype(np.float32)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(({'input': input_tensor}, {'proj1_output': output_tensor1, 'proj2_output': output_tensor2}))\r\nmodel = VanillaModel(16)\r\n\r\nmodel.compile('adam', {'proj1_output': 'mse', 'proj2_output': 'mae'})\r\nmodel.fit(dataset)\r\n```", "any updates?", "I am facing the same issue when I am retrieving data from **TFRecords**.", "Assigning to Tom who will be working on this as part of ideal fit/compile change.\r\n", "For a quick workaround, note the Keras expects Datasets passed to `fit` to be of the form `(features, labels` or `(features, labels, sample_weights)`\r\n\r\nHere's an example of grabbing MNIST from `tensorflow_datasets` and formatting it in the way Keras expects:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\ndef convert_dataset(item):\r\n    \"\"\"Puts the mnist dataset in the format Keras expects, (features, labels).\"\"\"\r\n    image = item['image']\r\n    label = item['label']\r\n    image = tf.dtypes.cast(image, 'float32') / 255.\r\n    return image, label\r\n\r\nmnist_data = tfds.load('mnist')\r\nmnist_train, mnist_test = mnist_data['train'], mnist_data['test']\r\nmnist_train = mnist_train.map(convert_dataset).shuffle(1000).batch(100).repeat()\r\nmnist_test = mnist_test.map(convert_dataset).batch(100)\r\n\r\nmodel.fit(mnist_train, epochs=10, validation_data=mnist_test)\r\n```", "@omalleyt12 That's true. But sometimes we have complicated models with multiple inputs and outputs, so it's more elegant if we can use dict based model and dataset.", "@npuichigo For those Models, each of `features` and `labels` can be a dictionary:\r\n\r\n```python\r\nds = tf.data.Dataset.from_tensor_slices(\r\n  ({'my_feature_1': ..., 'my_feature_2': ...}, {'my_label_1': ..., 'my_label_2': ...}))\r\n```", "@omalleyt12 So it looks like my example.\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices(({'input': input_tensor}, {'proj1_output': output_tensor1, 'proj2_output': output_tensor2}))\r\n```\r\nBut it seems that keras restrict the input and output names to something like input_1, input_2, output_1 and so on. Can you give a correct example?", "If I use ` tf.feature_column.categorical_column_with_vocabulary_list` I was able to specify the column name in keras. But I faced the issue when I called `model_to_estimator` API to convert back to tensorflow model as the converted model was expecting inputs like `input1,input2,...` as you mentioned", "same problem", "I faced same issue. My workaround is not using the subclass way.  When I change to tf.keras.Model(inputs=...,outputs=...), the problem solved.\r\n\r\nOr before model.complie, add two lines:\r\n    inputs = tf.keras.layer.Input(shape=(...))\r\n    model(inputs)      # you need to run the model once. model.build() doen't help.\r\n   \r\nIn fact I found there's quite some limitation on subclass model, and there's no good guidline about how to avoid these limitations.Hope tensorflow can improve the usability of subclass model. I really prefer subclass model than sequence model becuase that's the way how we can easily debug the model with eager.", "Thanks for the issue all! This is now fixed in the latest tf-nightlym you can use any arbitrary nested structure of data with subclassed Models.\r\n\r\nCode below works (note I had to add a call to `dataset.batch`, this is needed to create batches of data when using `from_tensor_slices`):\r\n\r\n```python\r\nclass VanillaModel(tf.keras.Model):\r\n\r\n  def __init__(self, num_units, **kwargs):\r\n    super(VanillaModel, self).__init__(**kwargs)\r\n    self.num_units = num_units\r\n\r\n    # One linear projection layer.\r\n    self.dense_proj1 = tf.keras.layers.Dense(num_units, activation='relu')\r\n    self.dense_proj2 = tf.keras.layers.Dense(num_units, activation='relu')\r\n\r\n  def call(self, features):\r\n    \"\"\"Forward pass.\"\"\"\r\n    proj1_output = self.dense_proj1(features['input'])\r\n    proj2_output = self.dense_proj2(features['input'])\r\n    return {\r\n        'proj1_output': proj1_output,\r\n        'proj2_output': proj2_output\r\n    }\r\n\r\ninput_tensor = np.random.normal(size=(50, 32)).astype(np.float32)\r\noutput_tensor1 = np.random.normal(size=(50, 16)).astype(np.float32)\r\noutput_tensor2 = np.random.normal(size=(50, 16)).astype(np.float32)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(({'input': input_tensor}, {'proj1_output': output_tensor1, 'proj2_output': output_tensor2}))\r\ndataset = dataset.batch(10)  # This needs to be called to create batches of data.\r\nmodel = VanillaModel(16)\r\n\r\nmodel.compile('adam', {'proj1_output': 'mse', 'proj2_output': 'mae'})\r\nmodel.fit(dataset)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33245\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33245\">No</a>\n", "@omalleyt12 \r\nWhen eager execution is disabled, the code snippet you posted above still fails under version `2.2.0-dev20200501` with the same error (`AttributeError: 'str' object has no attribute 'dtype'`).  ", "@jgmakin Disabling eager execution is a v1 compatibility feature, in general we're only making critical security or regression fixes in the v1 compatibility code for tf.keras. I'd recommend migrating to the TF2 style", "Ah, ok, thanks.  I tend to disable eager execution under the assumption that this improves performance, but perhaps that isn't the case anymore?", "@jgmakin When using `Model.fit`, everything runs inside a graph anyway (a `tf.function` graph) so performance should be good\r\n\r\nFor more, check out [this guide](https://www.tensorflow.org/guide/function)", "Hello, is there any way to do this too for validation_data?\r\nI can't get it work for validation after every epoch end"]}, {"number": 33244, "title": "Model has not been built yet", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version:3.7..3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): [GCC 7.3.0] :: Anaconda, Inc. on linux\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)\r\n\r\nI have downloaded the official example of Convolutional Variational Autoencoder from this [link](https://www.tensorflow.org/tutorials/generative/cvae). \r\n\r\nIf one starts to execute this file everything looks fine but the problem arises when one wants to save the weight or call the summary() method. (i.e.) model.summary(). This produces the following error:\r\n\r\nValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.\r\n\r\nPS, I faced the same issue in  my costume code as well. since the file is large I prevent to provide it here but you can find it in this [link](https://github.com/Sorooshi/ML4DC/blob/development/my_ope.ipynb). \r\n\r\n\r\nAny Idea what's going on?! And how to overcome this challenge.\r\n\r\nThanks in advance. \r\n\r\n#tensorflow ", "comments": ["Issue replicating for the given gist when tried using `model.summary()`.\r\n@Sorooshi can you also provide working link for your custom model,as the provided link is not working.Thanks!", "> Issue replicating for the given gist when tried using `model.summary()`.\r\n> @Sorooshi can you also provide working link for your custom model,as the provided link is not working.Thanks!\r\n\r\n@oanush Sorry for the inconvenience Please find it [here](https://github.com/Sorooshi/ML4DC/blob/master/my_ope.ipynb) \r\n\r\nPlease let me know if it doesn't work", "@Sorooshi,\r\nCan you please refer [this link](https://stackoverflow.com/questions/55908188/this-model-has-not-yet-been-built-error-on-model-summary) and let us know if it helps. Thanks!", "> @Sorooshi,\r\n> Can you please refer [this link](https://stackoverflow.com/questions/55908188/this-model-has-not-yet-been-built-error-on-model-summary) and let us know if it helps. Thanks!\r\n\r\n@rmothukuru I have seen this post previously, but this post has nothing to do with my problem. \r\nIn that (stack overflow) post the model is not build because input shape is not passed in the first layer. But the problem which I reported completely different.\r\n\r\n", "Could reproduce the issue in TF Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/2b25e9e918c489d0edac1865b888f5fb/33244.ipynb). Thanks!", "> Could reproduce the issue in TF Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/2b25e9e918c489d0edac1865b888f5fb/33244.ipynb). Thanks!\r\n\r\n@rmothukuru  No I can't reproduce this error with TF Version 2.0. \r\n\r\n", "@Sorooshi This is intended behavior. The error clearly mentions root-cause of the issue.\r\n\r\nThis is the response from @fchollet for a [similar issue](https://github.com/tensorflow/tensorflow/issues/25036) as yours.\r\n> You can do `model.summary` or `saving weights` with Sequential or functional models because those models are static graphs. In contrast, a subclassed model is a piece of Python code (a `call` method). There is no graph of layers here. We cannot know how layers are connected to each other (because that's defined in the body of `call`, not as an explicit data structure), so we cannot infer input / output shapes.\r\n> \r\n> Please see this detailed explanation of the differences between Functional/Sequential models and imperative models: https://medium.com/tensorflow/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021\r\n\r\nI am closing the issue as it was resolved. Please feel free to open a new issue if there is any other related issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33244\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33244\">No</a>\n"]}, {"number": 33243, "title": "README Edit", "body": "", "comments": []}, {"number": 33242, "title": "CTC Beam Search Decoder in TensorFlow android", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\nHow to decode output float array into string using CTC beam search decoder in android. I'm not using tflite as of now. \r\n\r\n### Clear description\r\n\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\nFloat Array\r\nAre return values defined? \r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@im-ranu ,\r\nCan you provide documentation link where the issue is being faced ?Thanks!", "@im-ranu ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 33241, "title": "some Ops tensorflow could but tflite", "body": "hi,\r\nI have some Ops which are valid in Tensorflow,but not valid in Tensorflow Lite,and if I want use the tflite file,how to solve the problem ?thx\r\n`2019-10-11 17:30:39.943242: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListReserve is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943256: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListFromTensor is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943266: W tensorflow/lite/toco/tflite/operator.cc:2707] Op While is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943275: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListStack is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943359: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListReserve is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943373: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListFromTensor is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943382: W tensorflow/lite/toco/tflite/operator.cc:2707] Op While is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943392: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListStack is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943422: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListReserve is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943456: I tensorflow/lite/toco/tflite/operator.cc:2054] Writing flex op: TensorListReserve\r\n2019-10-11 17:30:39.943471: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListFromTensor is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943483: I tensorflow/lite/toco/tflite/operator.cc:2054] Writing flex op: TensorListFromTensor\r\n2019-10-11 17:30:39.943500: W tensorflow/lite/toco/tflite/operator.cc:2707] Op While is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943566: I tensorflow/lite/toco/tflite/operator.cc:2054] Writing flex op: While\r\n2019-10-11 17:30:39.943591: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListStack is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-11 17:30:39.943606: I tensorflow/lite/toco/tflite/operator.cc:2054] Writing flex op: TensorListStack`\r\n\r\nI just want to use the tflite file in cellphone ,what should I do ?\r\nThanks a lot\r\n\r\n\r\n", "comments": ["@ucasiggcas, Is this similar to [#33239](https://github.com/tensorflow/tensorflow/issues/33239). Thanks!", "@ucasiggcas, Is this simile issue [#33239](https://github.com/tensorflow/tensorflow/issues/33239). If not please provide more information about issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 33240, "title": "build: Support Bazel 1.0", "body": "Ping @gunan @seanpmorgan @dslomov; is there any known blockers?", "comments": ["/cc @meteorcloudy @laszlocsomor ", "@byronyi I'm upgrading Bazel to 0.29.1 for TF internally, because TensorFlow is in the middle of 1.15 release, I can not submit the change yet. After it's done, I'll try to upgrade Bazel to 1.0, but there might be some incompatible changes in 1.0 we need to adapt.", "@meteorcloudy , how can I help?", "@laszlocsomor I'll finish the work to upgrading Bazel to 0.29.1. Can you help making TensorFlow build with Bazel 1.0? Currently [the build is red](https://buildkite.com/bazel/tensorflow) after we released 1.0 and TensorFlow is temporarily disabled at [Bazel's Downstream pipeline](https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/1232). I believe the main blocker is https://github.com/tensorflow/tensorflow/issues/32835. ", "@laszlocsomor Any update on this PR, please.", "@gbaned This is staled as TF builds with Bazel >= 1.1.0.\r\n\r\nClosing for now."]}, {"number": 33239, "title": "need some Ops in RNN", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version (github):\r\n\r\n\r\n\r\n\r\n```\r\nbazel-bin/tensorflow/lite/toco/toco \\\r\n--input_file=../mydlstm.pb --output_file=../mydlstmfloat.tflite \\\r\n--output_format=TFLITE --input_shapes=1,5,513 --input_arrays=x_mixed \\\r\n--output_arrays=y_out1,y_out2 --inference_type=FLOAT \\\r\n--inference_input_type=FLOAT\r\n```\r\nand the error is here\r\n\r\n'''\r\n2019-10-11 17:02:20.357549: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2019-10-11 17:02:20.357612: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-10-11 17:02:20.357628: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2019-10-11 17:02:20.357638: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-10-11 17:02:20.360078: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2019-10-11 17:02:20.360126: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-10-11 17:02:20.360134: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-10-11 17:02:20.360162: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n\r\n'''\r\n\r\nmodel is here\r\n[mydlstm.zip](https://github.com/tensorflow/tensorflow/files/3716604/mydlstm.zip)\r\n\r\n", "comments": ["if I use the tensorflow Ops,what should I do?\r\nhttps://github.com/tensorflow/tensorflow/issues/33241", "Hi,\r\n\r\nWe are working on support of RNN/LSTM now and hopefully we will release a new converter soon. The new converter will be able to convert RNN models. Please stay tuned!", "@haozha111 Thanks\r\nmay I ask some question,\r\nis the tf model .meta getting bigger in training ?\r\nthx", "Do you mean the checkpoint file that stores the model weights or the metagraph?\r\n\r\nI don't think either the metagraph or a single checkpoint file will get bigger during training, the size of the  checkpoint file is related with the number of parameters in your model. Does that answer your question?", "emm,you can have a look\r\nfrom 3.0M to 3.2M\r\n```\r\n3.2M Oct 13 08:12 checkpoint-186000.meta\r\n91 Oct 13 08:12 checkpoint\r\n8.3M Oct 13 08:12 checkpoint-186000.data-00000-of-00001\r\n1.6K Oct 13 08:12 checkpoint-186000.index\r\n3.2M Oct 13 08:08 checkpoint-185000.meta\r\n8.3M Oct 13 08:08 checkpoint-185000.data-00000-of-00001\r\n1.6K Oct 13 08:08 checkpoint-185000.index\r\n3.2M Oct 13 08:04 checkpoint-184000.meta\r\n8.3M Oct 13 08:04 checkpoint-184000.data-00000-of-00001\r\n1.6K Oct 13 08:04 checkpoint-184000.index\r\n3.1M Oct 13 08:00 checkpoint-183000.meta\r\n1.6K Oct 13 08:00 checkpoint-183000.index\r\n8.3M Oct 13 08:00 checkpoint-183000.data-00000-of-00001\r\n3.1M Oct 13 07:56 checkpoint-182000.meta\r\n8.3M Oct 13 07:56 checkpoint-182000.data-00000-of-00001\r\n1.6K Oct 13 07:56 checkpoint-182000.index\r\n3.1M Oct 13 07:51 checkpoint-181000.meta\r\n8.3M Oct 13 07:51 checkpoint-181000.data-00000-of-00001\r\n1.6K Oct 13 07:51 checkpoint-181000.index\r\n3.1M Oct 13 07:47 checkpoint-180000.meta\r\n8.3M Oct 13 07:47 checkpoint-180000.data-00000-of-00001\r\n1.6K Oct 13 07:47 checkpoint-180000.index\r\n3.1M Oct 13 07:43 checkpoint-179000.meta\r\n8.3M Oct 13 07:43 checkpoint-179000.data-00000-of-00001\r\n1.6K Oct 13 07:43 checkpoint-179000.index\r\n3.1M Oct 13 07:39 checkpoint-178000.meta\r\n8.3M Oct 13 07:39 checkpoint-178000.data-00000-of-00001\r\n1.6K Oct 13 07:39 checkpoint-178000.index\r\n3.1M Oct 13 07:35 checkpoint-177000.meta\r\n8.3M Oct 13 07:35 checkpoint-177000.data-00000-of-00001\r\n1.6K Oct 13 07:35 checkpoint-177000.index\r\n3.0M Oct 13 07:31 checkpoint-176000.meta\r\n8.3M Oct 13 07:31 checkpoint-176000.data-00000-of-00001\r\n1.6K Oct 13 07:31 checkpoint-176000.index\r\n3.0M Oct 13 07:27 checkpoint-175000.meta\r\n8.3M Oct 13 07:27 checkpoint-175000.data-00000-of-00001\r\n1.6K Oct 13 07:27 checkpoint-175000.index\r\n3.0M Oct 13 07:23 checkpoint-174000.meta\r\n8.3M Oct 13 07:23 checkpoint-174000.data-00000-of-00001\r\n1.6K Oct 13 07:23 checkpoint-174000.index\r\n3.0M Oct 13 07:19 checkpoint-173000.meta\r\n8.3M Oct 13 07:19 checkpoint-173000.data-00000-of-00001\r\n1.6K Oct 13 07:19 checkpoint-173000.index\r\n3.0M Oct 13 07:15 checkpoint-172000.meta\r\n8.3M Oct 13 07:15 checkpoint-172000.data-00000-of-00001\r\n1.6K Oct 13 07:15 checkpoint-172000.index\r\n3.0M Oct 13 07:11 checkpoint-171000.meta\r\n8.3M Oct 13 07:11 checkpoint-171000.data-00000-of-00001\r\n1.6K Oct 13 07:11 checkpoint-171000.index\r\n```\r\n@haozha111 ", "Maybe there is some additional information being accumulated in the meta graph during training, maybe you can do a file diff and see what is changed. Does that cause any issues for you?", "Could you try with our new converter to convert this model?\r\n\r\nYou will need to install the latest tf-nightly pip package, and then set converter.experimental_new_converter = True.\r\n\r\nLet me know if you have any questions.\r\nThanks.", "sorry\uff0c\r\nbecause of the [ issue](https://github.com/tensorflow/tensorflow/issues/34214),\r\ncouldn't try anything,\r\n", "@ucasiggcas Could  you please try to  install the latest tf-nightly pip package or the latest stable version of TF 2.6.0, and then set converter.experimental_new_converter = True? Please let us know if this issue still persists ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33239\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33239\">No</a>\n"]}, {"number": 33238, "title": "redzone_checker appears a lot while profiling tf-2.0", "body": "I have downloaded and built tf-2.0.0 from source. As I profile the label_image example with nvprof, I see that `redzone_checker` kernel is the most time consuming kernel (25%). Is that normal? I remember that this kernel wasn't time consuming in previous versions.\r\n\r\nAlso I didn't find any useful information about what does this kernel doing in a typical run. The same happens with multibox_detector example with a large percent of 87%. I don't think that is normal.\r\n\r\n![Screenshot from 2019-10-11 12-22-49](https://user-images.githubusercontent.com/11626212/66638471-de599700-ec21-11e9-8c9c-c7f82257c86b.png)\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). \r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "I pulled the latest git version as below\r\n\r\n```\r\nmahmood@m2000:tensorflow$ git describe --tags\r\nv1.12.1-15953-gf03ac35efa\r\n```\r\n\r\nThe normal run of the `label_image` example with the default image is\r\n\r\n```\r\nmahmood@m2000:tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image\r\n2019-10-18 10:26:13.258049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-18 10:26:13.617164: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-10-18 10:26:13.630304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-10-18 10:26:13.653735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:13.654625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 0 with properties: \r\nname: Quadro M2000 major: 5 minor: 2 memoryClockRate(GHz): 1.1625\r\npciBusID: 0000:26:00.0\r\n2019-10-18 10:26:13.654656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-18 10:26:13.691001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-18 10:26:13.710213: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-18 10:26:13.714586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-18 10:26:13.753618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-18 10:26:13.781256: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-18 10:26:13.844641: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-18 10:26:13.844783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:13.845497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:13.846128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0\r\n2019-10-18 10:26:16.925681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1087] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-18 10:26:16.925727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093]      0 \r\n2019-10-18 10:26:16.925742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 0:   N \r\n2019-10-18 10:26:16.937666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:16.938290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:16.938846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:16.939393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1232] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3499 MB memory) -> physical GPU (device: 0, name: Quadro M2000, pci bus id: 0000:26:00.0, compute capability: 5.2)\r\n2019-10-18 10:26:17.193150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:17.193772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 0 with properties: \r\nname: Quadro M2000 major: 5 minor: 2 memoryClockRate(GHz): 1.1625\r\npciBusID: 0000:26:00.0\r\n2019-10-18 10:26:17.193803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-18 10:26:17.193815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-18 10:26:17.193825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-18 10:26:17.193835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-18 10:26:17.193845: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-18 10:26:17.193854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-18 10:26:17.193864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-18 10:26:17.193918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:17.194466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:17.194967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0\r\n2019-10-18 10:26:17.194998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1087] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-18 10:26:17.195006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093]      0 \r\n2019-10-18 10:26:17.195013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 0:   N \r\n2019-10-18 10:26:17.195160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:17.195698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:17.196216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1232] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3499 MB memory) -> physical GPU (device: 0, name: Quadro M2000, pci bus id: 0000:26:00.0, compute capability: 5.2)\r\n2019-10-18 10:26:19.186270: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-18 10:26:21.076662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-18 10:26:23.492343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:23.492940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 0 with properties: \r\nname: Quadro M2000 major: 5 minor: 2 memoryClockRate(GHz): 1.1625\r\npciBusID: 0000:26:00.0\r\n2019-10-18 10:26:23.492978: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-18 10:26:23.492992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-18 10:26:23.493002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-18 10:26:23.493016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-18 10:26:23.493028: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-18 10:26:23.493039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-18 10:26:23.493052: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-18 10:26:23.493106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:23.493688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:23.494193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0\r\n2019-10-18 10:26:23.494235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1087] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-18 10:26:23.494247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093]      0 \r\n2019-10-18 10:26:23.494257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 0:   N \r\n2019-10-18 10:26:23.494403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:23.494958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-18 10:26:23.495487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1232] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3499 MB memory) -> physical GPU (device: 0, name: Quadro M2000, pci bus id: 0000:26:00.0, compute capability: 5.2)\r\n2019-10-18 10:26:23.521971: I tensorflow/examples/label_image/main.cc:252] military uniform (653): 0.834306\r\n2019-10-18 10:26:23.522016: I tensorflow/examples/label_image/main.cc:252] mortarboard (668): 0.0218695\r\n2019-10-18 10:26:23.522028: I tensorflow/examples/label_image/main.cc:252] academic gown (401): 0.0103581\r\n2019-10-18 10:26:23.522041: I tensorflow/examples/label_image/main.cc:252] pickelhaube (716): 0.00800823\r\n2019-10-18 10:26:23.522051: I tensorflow/examples/label_image/main.cc:252] bulletproof vest (466): 0.00535092\r\n```\r\n\r\nThe `nvprof` command I use is:\r\n```\r\nnvprof -o kernel_list.nvvp --log-file kernel_list.log bazel-bin/tensorflow/examples/label_image/label_image\r\n```\r\nI have attached the profiler's output [here](https://srv-file2.gofile.io/download/dieu7f/kernel_list.zip).\r\n\r\nPlease note the following specs:\r\n\r\nHardware: Quadro M2000\r\nCUDA version: 10.0\r\nCuDNN version: 7.6.2\r\nNCCL version: 2.4.8\r\nPython version: 2.7", "@jvishnuvardhan @rmothukuru \r\nDid you have time to verify the issue. Do you have any idea what can cause this issue?", "@MattConley and I are seeing this as well when profiling MobileNet on Jetson Xavier.", "@jvishnuvardhan \r\n@caisq \r\nSorry for bothering. Is there any good news for that?", "What is the redzone_checker kernel? I see it on the application I am trying to profile, and can't make any sense of it", "@mahmoodn @jakemdaly Sorry for the delay in my response. Can you please use recent profiler which has more details about bottlenecks in your code. Please check the guide [here](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) and also check [this resource](https://www.tensorflow.org/guide/profiler) on how to optimize tensorflow performance. Thanks! ", "@mahmoodn @jakemdaly Can you please check my last comment? thanks!", "Thanks jvishnuvardhan\r\nTensorFlow Profiler looks very similar to Visual Profiler, is it using nvprof in the background?\r\nThe guide is helpful, but I am still not sure what redzone_checker kernel is", "We can't answer your question regarding nvprof, as it is not developed by the TensorFlow team.\r\nYou can try our TensorFlow Profiler (https://www.tensorflow.org/guide/profiler). It is not using nvprof in the background (or more precisely, both TF Profiler and nvprof use the CUPTI library to instrument GPU kernels). Our TF Profiler has a number of tools in addition to the timeline view. ", "I haven't an installed copy of TF right now. I will try as soon as possible.\r\n", "@ckluk @rmothukuru @jvishnuvardhan \r\nHey guys, do we have any update on this? My profiling shows similar result that redzone_checker kernel takes a noticeable portion of E2E inference time. What's the purpose of this kernel? I believe this issue consistently persist no matter what profiler does one uses (e.g., Tensorboard, nvprof, nsight), tooling does not matter here, result is all the same. ", "@tensorflowbutler ", "Hi @mahmoodn! \r\nwe are  checking to see if you are still looking for assistance in this issue.\r\nCould you please try on latest stable version of  TF 2.6  and let us know if this is still an issue.Thanks!", "@mohantym That was about three years ago. I saw that figure on Titan V with CUDA 9 (I think) and the profiler was nvprof. Since nvprof is not compatible with architectures after Turing, I am not able to test with the same profiler. I also don't have access to that Titan V.\r\nIf you don't see that redzone_checker in your internal tests, then the problem is hopefully solved.", "Is this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33238\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33238\">No</a>\n"]}, {"number": 33237, "title": "Implement reference kernel for Softmax using CMSIS-NN", "body": "", "comments": ["Thanks for the contribution. The main logic is similar to the 8bit quantized softmax code before we switched to look-up-tables ( https://github.com/tensorflow/tensorflow/commit/96b38db29c3ad3b1c1397f57bca85e3df3a1ac5f#diff-77415d2cb3521cc7859a3349382057ad) and looks good.\r\n\r\nAssigning to Pete who knows the micro code better.", "@giorgio-arenarm it looks that the file: cmsis.inc (..... micro/tools/ext_libs) has not been edit. I suppose we should append $(CMSIS_PATH)/CMSIS/NN/Source/SoftmaxFunctions/arm_softmax_s8.c to THIRD_PARTY_CC_SRCS", "Hello @petewarden are internal tests failing here because of my patch?", "@giorgio-arenarm here is the internal error we are getting , can you please check this\r\n`Testing SimpleTestQuantizedSigned4D\r\nOnly float32 and uint8_t supported currently, got 9.\r\nOnly float32 and uint8_t supported currently, got 9.\r\nkTfLiteOk == registration->invoke(&context, &node) failed at third_party/tensorflow/lite/experimental/micro/kernels/softmax_test.cc:212 (0 vs 1)\r\nexpected_output_data.begin()[i] == output_data[i] failed at third_party/tensorflow/lite/experimental/micro/kernels/softmax_test.cc:217 (-117 vs -125)`\r\ncc @petewarden ", "@giorgio-arenarm  Could you please check above internal error and resolve the conflicts? Thanks!", "Hi, I have tried to reproduce those internal errors but unfortunately I haven't been able to do so.\r\nThe error you get (\"Only float32 and uint8_t supported currently\") shouldn't even say what it does since in this same commit I changed the error string inside (both) softmax.cc(s) to \"Only float32, uint8_t and int8_t supported currently\". I tried to rebase and push, let's see how that goes", "About the mismatch, I do not get that either. Could I ask what board this test runs on? So to check whether the error might be in the MVE portion of the code or not", "@petewarden @advaitjain, gentle ping for comment", "Rebased. @petewarden @advaitjain, gentle ping for review", "Hi @petewarden is this ready to be merged? Thanks :)", "@giorgio-arenarm Can you please check rthadur's comments and keep us posted? Thanks!", "Done", "Sorry for the lack of comment, I missed that this was still open! We actually had a parallel thread going adding int8 support to softmax internally, so I think internally, so I think this PR is now redundant? Apologies for the duplication of work, please re-open if I'm incorrect:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/softmax.cc", "Hi @petewarden this is about enabling CMSIS-NN acceleration for softmax int8. Maybe the title is a bit misleading (sorry about that :) ) but it should still be missing. The committed files here are under the cmsis-nn folder.", "Sorry about that, and thanks for the correction! I've reopened this now.", "@rthadur Is this ready to be merged?", "Hi @rthadur thank you for validating this patch. Could I ask you how you've obtained this result?\r\nI haven't been able to reproduce it just yet, but from the output log it seems to be unrelated to this change.\r\nThe failure seems to be happening in third_party/tensorflow/lite/kernels/internal/reference/integer_ops/softmax.h (the reference code provided in tensorflow) which I did not touch in this patch and moreover this seems to be called by third_party/tensorflow/lite/micro/kernels/softmax.cc which - again - shouldn't have anything to do with this patch (the file created here was under the cmsis-nn subfolder rather than just micro/). Thanks!", "@giorgio-arenarm those errors are from internal CI tests , @petewarden can you please assist with the above error ?", "Done. Patch rebased, could you please check whether the error has been resolved?", "This PR is failing with the asan builds. \r\n\r\n@giorgio-arenarm, could you take a look and fix the issue?\r\n\r\nI was able to patch the PR and reproduce the error locally by building with bazel with this command:\r\n```\r\nCC=clang BAZEL_COMPILER=llvm bazel run --copt=-DADDRESS_SANITIZER  --copt=-fsanitize=address --linkopt=-fsanitize=address tensorflow/lite/micro/kernels:softmax_test_binary\r\n```\r\n\r\nError log:\r\n```\r\nTesting SimpleTestQuantizedSigned2D\r\n=================================================================\r\n==27528==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7fff88d81c25 at pc 0x00000052b69a bp 0x7fff88d7a6a0 sp 0x7fff88d7a698\r\nWRITE of size 1 at 0x7fff88d81c25 thread T0\r\n    #0 0x52b699 in void tflite::reference_integer_ops::Softmax<signed char>(tflite::SoftmaxParams const&, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char*) (/usr/local/google/home/advaitjain/.cache/bazel/_bazel_advaitjain/8c393587092df3f3f8b949c07577ca69/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/softmax_test_binary+0x52b699)\r\n    #1 0x5277ca in tflite::ops::micro::activations::SoftmaxEval(TfLiteContext*, TfLiteNode*) (/usr/local/google/home/advaitjain/.cache/bazel/_bazel_advaitjain/8c393587092df3f3f8b949c07577ca69/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/softmax_test_binary+0x5277ca)\r\n    #2 0x4f803b in tflite::testing::(anonymous namespace)::TestSoftmaxQuantizedSigned(std::initializer_list<int>, std::initializer_list<signed char>, float, float, std::initializer_list<signed char>, std::initializer_list<int>, float, float, signed char*) (/usr/local/google/home/advaitjain/.cache/bazel/_bazel_advaitjain/8c393587092df3f3f8b949c07577ca69/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/softmax_test_binary+0x4f803b)\r\n    #3 0x4f51a1 in main (/usr/local/google/home/advaitjain/.cache/bazel/_bazel_advaitjain/8c393587092df3f3f8b949c07577ca69/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/softmax_test_binary+0x4f51a1)\r\n    #4 0x7f0873b6bbba in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x26bba)\r\n    #5 0x41f139 in _start (/usr/local/google/home/advaitjain/.cache/bazel/_bazel_advaitjain/8c393587092df3f3f8b949c07577ca69/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/softmax_test_binary+0x41f139)\r\n\r\nAddress 0x7fff88d81c25 is located in stack of thread T0 at offset 19301 in frame\r\n    #0 0x4f300f in main (/usr/local/google/home/advaitjain/.cache/bazel/_bazel_advaitjain/8c393587092df3f3f8b949c07577ca69/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/softmax_test_binary+0x4f300f)\r\n\r\n  This frame has 49 object(s):\r\n    [32, 256) 'tensors.i1291'\r\n    [320, 480) 'context.i1292'\r\n    [544, 8752) 'resolver.i1293'\r\n    [9008, 9012) 'builtin_data.i1294'\r\n    [9024, 9032) 'inputs_array_data.i1295'\r\n    [9056, 9064) 'outputs_array_data.i1296'\r\n    [9088, 9160) 'node.i1298'\r\n    [9200, 9536) 'tensors.i'\r\n    [9600, 9760) 'context.i'\r\n    [9824, 18032) 'resolver.i'\r\n    [18288, 18292) 'builtin_data.i'\r\n    [18304, 18312) 'inputs_array_data.i'\r\n    [18336, 18344) 'outputs_array_data.i'\r\n    [18368, 18372) 'temporaries_array_data.i'\r\n    [18384, 18456) 'node.i'\r\n    [18496, 18504) 'error_reporter'\r\n    [18528, 18568) 'output_data'\r\n    [18608, 18620) 'ref.tmp'\r\n    [18640, 18680) 'ref.tmp3'\r\n    [18720, 18732) 'ref.tmp15'\r\n    [18752, 18757) 'output_data35'\r\n    [18784, 18796) 'ref.tmp37'\r\n    [18816, 18821) 'ref.tmp45'\r\n    [18848, 18853) 'ref.tmp66'\r\n    [18880, 18892) 'ref.tmp87'\r\n    [18912, 18922) 'output_data124'\r\n    [18944, 18956) 'ref.tmp126'\r\n    [18976, 18986) 'ref.tmp134'\r\n    [19008, 19018) 'ref.tmp170'\r\n    [19040, 19056) 'agg.tmp205'\r\n    [19072, 19084) 'ref.tmp206'\r\n    [19104, 19109) 'output_data245'\r\n    [19136, 19144) 'ref.tmp247'\r\n    [19168, 19173) 'ref.tmp254'\r\n    [19200, 19205) 'ref.tmp275'\r\n    [19232, 19248) 'agg.tmp295'\r\n    [19264, 19272) 'ref.tmp296'\r\n    [19296, 19301) 'output_data334' <== Memory access at offset 19301 overflows this variable\r\n    [19328, 19340) 'ref.tmp336'\r\n    [19360, 19370) 'ref.tmp344'\r\n    [19392, 19402) 'ref.tmp380'\r\n    [19424, 19440) 'agg.tmp415'\r\n    [19456, 19468) 'ref.tmp416'\r\n    [19488, 19493) 'output_data455'\r\n    [19520, 19540) 'ref.tmp457'\r\n    [19584, 19704) 'ref.tmp463'\r\n    [19744, 19864) 'ref.tmp829'\r\n    [19904, 19920) 'agg.tmp1194'\r\n    [19936, 19956) 'ref.tmp1195'\r\nHINT: this may be a false positive if your program uses some custom stack unwind mechanism, swapcontext or vfork\r\n      (longjmp and C++ exceptions *are* supported)\r\nSUMMARY: AddressSanitizer: stack-buffer-overflow (/usr/local/google/home/advaitjain/.cache/bazel/_bazel_advaitjain/8c393587092df3f3f8b949c07577ca69/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/softmax_test_binary+0x52b699) in void tflite::reference_integer_ops::Softmax<signed char>(tflite::SoftmaxParams const&, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char*)\r\nShadow bytes around the buggy address:\r\n  0x1000711a8330: f2 f2 f8 f8 f8 f8 f8 f2 f2 f2 f2 f2 f8 f8 f2 f2\r\n  0x1000711a8340: f8 f2 f2 f2 f8 f8 f2 f2 f8 f2 f2 f2 f8 f2 f2 f2\r\n  0x1000711a8350: f8 f8 f2 f2 f8 f8 f2 f2 f8 f8 f2 f2 f8 f8 f2 f2\r\n  0x1000711a8360: f8 f8 f2 f2 00 00 f2 f2 f8 f8 f2 f2 f8 f2 f2 f2\r\n  0x1000711a8370: f8 f2 f2 f2 f8 f2 f2 f2 f8 f2 f2 f2 00 00 f2 f2\r\n=>0x1000711a8380: f8 f2 f2 f2[05]f2 f2 f2 00 04 f2 f2 00 02 f2 f2\r\n  0x1000711a8390: 00 02 f2 f2 00 00 f2 f2 00 04 f2 f2 f8 f2 f2 f2\r\n  0x1000711a83a0: f8 f8 f8 f2 f2 f2 f2 f2 f8 f8 f8 f8 f8 f8 f8 f8\r\n  0x1000711a83b0: f8 f8 f8 f8 f8 f8 f8 f2 f2 f2 f2 f2 f8 f8 f8 f8\r\n  0x1000711a83c0: f8 f8 f8 f8 f8 f8 f8 f8 f8 f8 f8 f2 f2 f2 f2 f2\r\n  0x1000711a83d0: 00 00 f2 f2 f8 f8 f8 f3 f3 f3 f3 f3 00 00 00 00\r\nShadow byte legend (one shadow byte represents 8 application bytes):\r\n  Addressable:           00\r\n  Partially addressable: 01 02 03 04 05 06 07 \r\n  Heap left redzone:       fa\r\n  Freed heap region:       fd\r\n  Stack left redzone:      f1\r\n  Stack mid redzone:       f2\r\n  Stack right redzone:     f3\r\n  Stack after return:      f5\r\n  Stack use after scope:   f8\r\n  Global redzone:          f9\r\n  Global init order:       f6\r\n  Poisoned by user:        f7\r\n  Container overflow:      fc\r\n  Array cookie:            ac\r\n  Intra object redzone:    bb\r\n  ASan internal:           fe\r\n  Left alloca redzone:     ca\r\n  Right alloca redzone:    cb\r\n  Shadow gap:              cc\r\n==27528==ABORTING\r\n```", "Looks like line 365 of softmax_test.cc should be 10.", "Done. My bad, fixed it!", "approved, I'll make sure this gets landed."]}, {"number": 33236, "title": "How to load a keras model saved by tf 1.14 with tf 1.15?", "body": "I run tensorflow on colab.research.google.com.\r\nSome days before, when the tensorflow version was 1.14,\r\nI saved a keras model to \"drive/Colab/mo_big7_12345789bcdfghijklm\" . it is a file, not a path.\r\n```\r\nModel = tf.keras.models.Model(...)\r\nModel.save(...)\r\n```\r\nNow, the tensorflow upgraded to v1.15. when loading the file by\r\n`tf.keras.models.load_model( 'drive/Colab/mo_big7_12345789bcdfghijklm' )`\r\nit raises: OSError: SavedModel file does not exist at: drive/Colab/mo_big7_12345789bcdfghijklm/{saved_model.pbtxt|saved_model.pb}\r\nI havn't seen the .pb or .pbtxt files before. how to load it?", "comments": ["It turns OK after I did nothing.", "unfortunately, the error comes back.", "it turns ok after restarting. it seems the mounted path turns unavailable after some time."]}, {"number": 33235, "title": "Remove microfrontend/lib from the MICROLITE_CC_HDRS/SRCS.", "body": "The inclusion of microfrontend/lib (more specifically fft.cc) causes a\r\ncompile error when building Tensorflow Lite Micro for Arm Mbed OS. It is not\r\nused in any other example than micro_speech, so they can safely be\r\nomitted.", "comments": ["I had a look at the builds that are failing, and none of them seems to have anything to do with anything related to this patch.", "The \"Ubuntu CC\" test have been \"Waiting for status\" for a few days, and that seems to be the case for other PRs as well.", "Ready to merge."]}, {"number": 33234, "title": "A small optimization for VirtualScheduler::Init ", "body": "fix a TODO in virtual_scheduler.cc :\r\n```\r\n // TODO(dyoon): this is a bit inefficient as name_to_node is already built in\t\r\n // ComputeTransitiveFanin().\r\n```\r\n\r\nadd a prototype for ComputeTransitiveFanin, get the name to fanin node map when constructing transitive fanin node , so we do not need to  build it again after this function call ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33234) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33234) for more info**.\n\n<!-- ok -->", "@doehyun modified according to your comments, please take a look again ,thanks for your time!"]}, {"number": 33233, "title": "[tflite2.0] tf.lite.Interpreter fails with 'post-training quantization' (tf.matmul).", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from source\r\n- TensorFlow version (use command below): ('v2.0.0-0-g64c3d38', '2.0.0')\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\n- CUDA/cuDNN version: 10.0 / 7.5\r\n- GPU model and memory: GeForce GTX TITAN\r\n\r\n**Describe the current behavior**\r\n'Post-training integer quantization' fails with tf.matmul() in a certain condition.\r\n(https://www.tensorflow.org/lite/performance/post_training_integer_quant)\r\n\r\nFollowing code is a testing code for converting a dummy tensorflow graph with quantization. This code fails if shape_b[-1] > 15 for me. When I analyzed the visualization result ((bazel run //tensorflow/lite/tools:visualize model.tflite visualized_model.html)), I think it is a result of graph transformation issue.\r\n\r\nSuppose that shape_b = (None, 16, 4, 5). Then each tf.matmul op is replaced by 16 FULLY_CONNECTED ops in the tflite model. And each FULLY_CONNECTED op has its own bias tensor, with shape=[5], as third input. In this case, tflite_convert works in both cases that quantization=True or quantization=False.\r\n\r\nWhen Suppose that shape_b = (None, 16, 4, 16). When quantization=True, the interpreter fails at the allocation step with following output:\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nTraceback (most recent call last):\r\n  File \"tflite_matmul_v2.py\", line 89, in <module>\r\n    main()\r\n  File \"tflite_matmul_v2.py\", line 67, in main\r\n    interpreter.allocate_tensors()\r\n  File \"/home/hh1208-kang/venv_py2_tf_nightly/local/lib/python2.7/site-packages/tensorflow_core/lite/python/interpreter.py\", line 244, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/hh1208-kang/venv_py2_tf_nightly/local/lib/python2.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/kernel_util.cc:119 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 282 (FULLY_CONNECTED) failed to prepare.\r\n```\r\nBut the code still works if quantization=False. When I check the visualization result, I noticed that the bias tensors for FULLY_CONNECTED ops are merged into a single bias tensor, with shape [16]. It seems that tflite_convert try to remove redundant zero bias tensors when it becomes large.\r\n\r\nHowever that makes problem in the quantization. Because in that case, the quantization parameters of the bias tensor is shared by all the FULLY_CONNECTED ops. Thus the check std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) fails, since 'input_product_scale' may be different in some FULLY_CONNECTED op, while 'bias_scale' are the same.\r\n\r\nHow can I suppress the transformation behavior? I think somehow if I could prevent the transformation that merges all the bias tensors, the quantization success.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test_tflite_model(tflite_filename, examples):\r\n    print(\"Loading TFLite interpreter for %s...\" % tflite_filename)\r\n    interpreter = tf.lite.Interpreter(model_path=tflite_filename)\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    print(\"input details: %s\" % input_details)\r\n    print(\"output details: %s\" % output_details)\r\n\r\n    for i, input_tensor in enumerate(input_details):\r\n        interpreter.set_tensor(input_tensor['index'], examples[i])\r\n    interpreter.invoke()\r\n    model_output = []\r\n    for i, output_tensor in enumerate(output_details):\r\n        model_output.append(interpreter.get_tensor(output_tensor['index']))\r\n    return model_output\r\n\r\ndef main():\r\n    nsamples=1\r\n    quantization=True\r\n    tflite_filename = \"matmul_model_v2.tflite\"\r\n    shape_a = (None, 16, 3, 4)\r\n    shape_b = (None, 16, 4, 15) # quantization fails if shape_b[-1] >= 16\r\n\r\n    @tf.function(input_signature=[\r\n    \ttf.TensorSpec(shape=shape_a, dtype=tf.float32, name='a'), \r\n        tf.TensorSpec(shape=shape_b, dtype=tf.float32, name='b')])\r\n    def model(a,b):\r\n\tc = tf.matmul(a,b) #[16, 3, 5]\r\n\r\n\tdef submodule(c,b):\r\n\t\tnb = tf.transpose(b, [0, 1, 3, 2])\r\n\t\t#nb = tf.nn.softmax(nb)\r\n\t\tc = tf.matmul(c,nb) # [16,3,4]\r\n\t\tc = tf.matmul(c,b)  # [16,3,5]\r\n\t\treturn c\r\n\tc = submodule(c,b)\r\n\treturn c\r\n\r\n\r\n    def _representative_dataset_gen():\r\n\tfor i in range(50):\r\n\t\ta_ = np.random.random_sample((1,)+shape_a[1:]).astype(np.float32)\r\n\t        b_ = np.random.random_sample((1,)+shape_b[1:]).astype(np.float32)\r\n\t\tyield [a_,b_]\r\n\r\n    # tflite_convert\r\n    cfunc = model.get_concrete_function()\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([cfunc])\r\n    if (quantization):\r\n\tconverter.representative_dataset = _representative_dataset_gen # with quantization\r\n\tconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    tflite_model = converter.convert()\r\n    open(tflite_filename, \"wb\").write(tflite_model)\r\n\r\n    # run tensorflow model (not converted)\r\n    np.random.seed(1234)\r\n    a_ = np.random.random_sample((nsamples,)+shape_a[1:]).astype(np.float32)\r\n    b_ = np.random.random_sample((nsamples,)+shape_b[1:]).astype(np.float32)\r\n    session_output = model(a_,b_)\r\n\r\n    # load and run tflite model (converted_)\r\n    interpreter = tf.lite.Interpreter(model_path=tflite_filename)\r\n    interpreter.allocate_tensors()\r\n    output_details = interpreter.get_output_details()\r\n    output_shape = interpreter.get_tensor(output_details[0]['index']).shape\r\n    tflite_output = np.zeros((nsamples,)+output_shape[1:], np.float32)\r\n    for n in range(nsamples):\r\n\ttflite_output[n,:] = test_tflite_model(tflite_filename, \r\n\t\t[np.expand_dims(a_[n,:],0), np.expand_dims(b_[n,:],0)])[0]\r\n\r\n    print(\"Input example:\")\r\n    print(a_)\r\n    print(a_.shape)\r\n    print(b_)\r\n    print(b_.shape)\r\n    print(\"Session output:\")\r\n    print(session_output)\r\n    print(session_output.shape)\r\n    print(\"TFLite output:\")\r\n    print(tflite_output)\r\n    print(tflite_output.shape)\r\n    print(np.allclose(session_output, tflite_output))\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nNote that this code is inspired by https://github.com/tensorflow/tensorflow/issues/27640.\r\n\r\n\r\np.s. (2019.10.17)\r\nI corrected the post since I made a mistake. The problem is not about converting, but about the interpreter, especially in the allocation. I'm sorry.\r\n", "comments": ["The operation responsible to this is the following line of code:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/64c3d382cadf7bbe8e7e99884bede8284ff67f56/tensorflow/lite/toco/toco_tooling.cc#L416\r\n\r\nWhen I commented out the line, I confirmed that the reproducing code works. But I think this is a kind of temporary bypass, not the right solution.\r\n```\r\n//DedupeConstantArrays(model, toco_flags.dedupe_array_min_size_bytes());\r\n```", "@paanguin,\r\nI was able to run the given code without any issues on TensorFlow v2.3, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f94691b4df92ad4940449c8b584047f5/33233.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33233\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33233\">No</a>\n"]}, {"number": 33232, "title": "Tensor not eagerly evaluated when using train_on_batch", "body": "Dear all, \r\n\r\nI am using the train_on_batch() function to train my model and which to redirect the outputs in a log file. I am using the following configuration TensorFlow 2.0.0.rc2, Kera 2.3.0 and Python 3.7.3. For\r\nthe moment, I'm just training my model with few batches within a loop\r\n\r\n```python\r\n    print(\"{}\".format(model.metrics_names))\r\n    for batch_i in range(batch_count):\r\n        x_i, y_i = next(training_generator)\r\n        history_i = model.train_on_batch(x_i, y_i, reset_metrics=True)\r\n        print(\"{}: {}\".format(batch_i, history_i))\r\n        print(model.outputs[0])\r\n```\r\nMy output is supposed to be a 5x3 tensor. However, the tensor is not evaluated and I've got the following output:\r\n```\r\n0: [1.9101071, 1.0, 0.0, 10.0, 0.0, 5.0]\r\nTensor(\"fc3/sigmoid/Identity:0\", shape=(None, 3), dtype=float32)\r\n...\r\n```\r\nWhen I'm using ```model.outputs[0].numpy()```, I've got an attribute error:\r\n```\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\nWith TensorFlow v1, I would have said that the session is not correctly initialize. WIth V2, I'm a bit puzzled. How eager evaluation is supposed to work with train_on_batch? \r\n\r\nNick,", "comments": ["After searcging for answers in the doc and on miscellaneous forum, I have to conclude that my model is not supposed to be executed eagerly. My model is defined in a function besed on the tensorflow.keras functional API. "]}, {"number": 33231, "title": "java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'TFLite_Detection_PostProcess' with version 1 ", "body": "I used tensorflow lite android object detection demo in my android project , but it crashs , I set ndk {\r\n            abiFilters 'armeabi-v7a'\r\n        } \r\n       in build.gradle, it still crashs\r\n     Error Log \uff1a\r\n java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'TFLite_Detection_PostProcess' with version 1\r\n    Registration failed. at com.tfdetection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:125)\r\n  crash code \uff1ad.tfLite = new Interpreter(loadModelFile(assetManager, modelFilename));", "comments": ["@WestbrookZero  , your error logs indicates that tflite model used here has operation which are not supported. Check for your model file if that is correctly converted to tflite. ", "@avaish1  my tflite can be used by an empty android project , but can't be used by my android project ,and official pre-training tflite is still this error in my android project . ", "I solve it in build.gradle :  implementation 'org.tensorflow:tensorflow-lite:2.0.0'"]}, {"number": 33230, "title": "Embedding visualization in tf.keras / TensorFlow 2.0", "body": "I'm trying to visualize some embeddings in Tensorboard. In pure keras this was possible, for example:\r\n- https://keras.io/examples/tensorboard_embeddings_mnist/\r\n\r\nThere was already a regression in `tf.keras`, breaking this functionality, see [this bug](https://github.com/keras-team/keras/issues/12808).\r\n\r\nI thought I'd try out tensorflow 2.0 and it seems that the functionality is removed altogether. I now get the message:\r\n\r\n> WARNING:tensorflow:`embeddings_layer_names` is not supported in TensorFlow 2.0. Instead, all `Embedding` layers will be visualized.\r\n\r\nI generally don't care about visualizing my `Embedding` layers. Instead, I'd like to visualize the output of my bottleneck layer, which is typically just a `Dense` layer.\r\n\r\nAs I'm writing this, I do realize that the way tensorboard/projector works is that is extracts the embedding `Variable` (weights) from a checkpoint. In the case of an `Embedding` layer this is very natural. The interesting case, I would argue, is the visualization of **activations**, i.e. a generic `Tensor`.\r\n\r\nIs this supported by `tf.keras` or should I write a custom callback class to do this?\r\n\r\nThanks!\r\n", "comments": ["@KristianHolsheimer  It looks like you are using an older Version of Tensorflow 2.0. Many bugs have been fixed in the latest version. Can you please execute your code using Latest stable Version(2.5) and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33230\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33230\">No</a>\n"]}, {"number": 33229, "title": "Load saved custom Loss class", "body": "Attempt to fix https://github.com/tensorflow/tensorflow/issues/32612\r\n\r\nHere's an example with a custom Loss class, including config to ensure it's properly reloaded:\r\n```python\r\nclass HuberLoss(keras.losses.Loss):\r\n    def __init__(self, threshold=1.0, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.threshold = threshold\r\n    \r\n    @tf.function\r\n    def call(self, y_true, y_pred):\r\n        error = tf.abs(y_true - y_pred)\r\n        is_small_error = error <= self.threshold\r\n        squared_loss = tf.square(error) / 2\r\n        linear_loss = error * self.threshold - 0.5 * self.threshold**2\r\n        return tf.where(is_small_error, squared_loss, linear_loss)\r\n    \r\n    def get_config(self):\r\n        cfg = super().get_config()\r\n        cfg['threshold'] = self.threshold\r\n        return cfg\r\n```\r\nThe class can be used for a regression task this way:\r\n```python\r\nmodel = keras.Sequential([\r\n  keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\r\n  keras.layers.Dense(1)\r\n])\r\n\r\nmodel.compile(loss=HuberLoss(2.0), optimizer=\"sgd\")\r\nmodel.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\r\n\r\n# save and reload\r\nmodel.save('model_with_huber_loss_class.h5')\r\nmodel = keras.models.load_model('model_with_huber_loss_class.h5', custom_objects={'HuberLoss': HuberLoss})\r\n\r\n# continue training\r\nmodel.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\r\n```\r\n\r\nThe problem was that `saving_util.compile_args_from_training_config(training_config, custom_objects=None)` supports `custom_objects` but `custom_objects` was not propagated to `losses.get()`.\r\n`losses.get()` later called `deserialize(identifier)` without specifying `custom_objects` while the `custom_objects` argument is supported by `deserialize()`: the default `None` was used and the custom class load failed.\r\n", "comments": ["@omalleyt12 \r\nI tested the changes you've suggested (and pushed them to this PR).\r\nI only had to rename `metrics` --> `metrics_obj` to not conflict with the `metrics` module.\r\nIt works indeed: `losses.deserialize(...)` is called and\r\nthe loss custom object is correctly deserialized. I can resume training with the reloaded model.\r\n```python\r\n  if isinstance(loss_config, dict) and 'class_name' in loss_config:\r\n    loss = losses.deserialize(loss_config, custom_objects)  # <<< this is executed\r\n```\r\n\r\n\r\nSince your proposed changes also involve metrics, I've tested with a custom metrics class.\r\nI wrote a class HuberMetric(keras.metrics.Metric) for the test.\r\nFull code: https://gist.github.com/thierryherrmann/550906c4f97ff685cf21b0418a485063\r\nModel training goes well, then I save the model but it fails on reload.\r\n\r\nIn this part of `saving_utils.py`:\r\n```python\r\n  metrics_config = training_config['metrics']\r\n  if isinstance(metrics_config, dict) and 'class_name' in metrics_config:\r\n    metrics_obj = metrics.deserialize(metrics_config, custom_objects)\r\n  else:\r\n    metrics_obj = nest.map_structure(\r\n      lambda obj: metrics.deserialize(obj, custom_objects),\r\n      metrics_config)\r\n```\r\n`metrics_config` is not an instance of dict but of list. Indeed it is this:\r\n`[{'class_name': 'HuberMetric', 'config': {'dtype': 'float32', 'name': 'huber_metric', 'threshold': 0.2}}]`\r\nSo the code executes the else part and `nest.map_structure(...)`\r\nand fails when trying to deserialize 'float32'. \r\nI can provide more details but looks like we're drifting from the original bug that is about reloading custom losses.\r\n\r\nIn summary (excerpt from the gist above):\r\n```python\r\nif True:\r\n    # LOSS custom class: FIXED by the PR (thanks @omalleyt12)\r\n    model.compile(loss=HuberLoss(2.0), optimizer=\"sgd\")\r\n    model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\r\n    model.save('model.h5')\r\n    model = keras.models.load_model('model.h5', custom_objects={'HuberLoss': HuberLoss})\r\n\r\nif False:\r\n    # LOSS custom function: was working before the PR, still works with the PR\r\n    model.compile(loss=huber_fn, optimizer=\"sgd\")\r\n    model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\r\n    model.save('model.h5')\r\n    model = keras.models.load_model('model.h5', custom_objects={'huber_fn': huber_fn})\r\n\r\nif False:\r\n    # METRICS custom class: was broken before the PR, still broken with the PR\r\n    model.compile(loss=keras.losses.mean_squared_error, optimizer=\"sgd\", metrics=[HuberMetric(threshold=.2)])\r\n    model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\r\n    model.save('model.h5')\r\n    model = keras.models.load_model('model.h5', custom_objects={'HuberMetric':HuberMetric})\r\n\r\nif False:\r\n    # METRICS custom function: was working before the PR, still works with the PR\r\n    model.compile(loss=keras.losses.mean_squared_error, optimizer=\"sgd\", metrics=[huber_fn])\r\n    model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\r\n    model.save('model.h5')\r\n    model = keras.models.load_model('model.h5', custom_objects={'huber_fn':huber_fn})\r\n\r\n# continue training with the reloaded model\r\nmodel.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\r\n```\r\n\r\nSo the PR with your suggested changes fixes the original bug and doesn't break things.\r\nI'll leave up to reviewers if this should be merged. Custom metrics class reloading might be considered as another issue.\r\nThanks", "@thierryherrmann Could you please check failed build errors? Thanks!", "> So the PR with your suggested changes fixes the original bug and doesn't break things.\r\n> I'll leave up to reviewers if this should be merged. Custom metrics class reloading might be considered as another issue.\r\n> Thanks\r\n\r\n@thierryherrmann\r\n\r\nHm thanks for looking into this, yes the issue is the `nest.map_structure`. Instead it should be safer to assume that metrics_config is either a dict or a list of dicts. This should probably be the same for weighted_metrics and indeed for losses as well:\r\n\r\n```python\r\nmetrics_config = training_config['metrics']\r\n  if isinstance(metrics_config, dict) and 'class_name' in metrics_config:\r\n    metrics_obj = metrics.deserialize(metrics_config, custom_objects)\r\n  else:\r\n    metrics_obj = [\r\n      lambda obj: metrics.deserialize(obj, custom_objects)\r\n      for obj in metrics_config]\r\n```\r\n", "@omalleyt12 agreed. I'll look at this over the week-end. Thanks again", "Thanks again for your suggestion @omalleyt12. I just removed the lambda from your proposal and it works. You've done all the work.\r\n\r\nI've added `elif isinstance(..., list)` for the `metrics_config` and `weighted_metrics_config` but not for losses since my understanding is that we can only have one loss.\r\n\r\nAlso I've left an `else` fallback executing the original `nest.map_structure(...)` in case the original designers rely on this behavior now or in the future.\r\nThis fallback branch is actually still executed if we specify no metric or weigthted metric (in which case `metrics_config` (resp. `weighted_metrics_config`) are None)\r\n\r\nAll failing tests pass now. But @omalleyt12, is it just me who has a very hard time to run the test suite with `bazel test` (either by building on my laptop directly or by using a docker image) ? I actually executed the tests by taking a copy of `tensorflow/python/keras/saving/hd5_format_test.py` and execute it separately.\r\n", "Thanks a lot for the changes @thierryherrmann and @omalleyt12 . Sorry i am late in the game, is it possible to add tests for the custom loss and metric configs?", "@pavithrasv I'm a bit busy at the moment. Should be ok for this week-end. Let me know if you really need the tests earlier.", "Oh ok, no worries then, we can get this submitted and add the tests as soon as possible after that. Thank you!\r\n", "@thierryherrmann Could you please check failed build errors? Thanks!", "@thierryherrmann I took a look at the failures:\r\n\r\n1. There is a circular dependency with the import of metrics.\r\n2. There are linter errors: There is a 80 character limit for every line.\r\n3. failure in losses_test\r\n4. failure in integration_test\r\n\r\nFor the test case `test_serializing_model_with_loss_class`, the loss_config looks like:\r\n`<type 'dict'>: {u'model_output': {u'class_name': u'_MSEMAELoss', u'config': {u'mse_fraction': 0.3}}}`\r\n\r\n`if isinstance(loss_config, dict) and 'class_name' in loss_config:` will fail when the loss input to compile is given as a dictionary: For example: {'model_output': my_loss_class(...)}. There are other loss and metric configurations for which we will see failures.\r\n\r\nI think the following fix should handle all the above issues, can you please try it? \r\n\r\n```\r\ndef convert_output_metrics(metrics_config, custom_objects):\r\n  from google3.third_party.tensorflow.python.keras import metrics as metrics_module  # pylint:disable=g-import-not-at-top\r\n  if isinstance(metrics_config, list):\r\n    return [convert_output_metrics(mc, custom_objects) for mc in metrics_config]\r\n  elif (isinstance(metrics_config, dict) or\r\n        (metrics_config not in ['accuracy', 'acc', 'crossentropy', 'ce'])):\r\n    # Do not deserialize accuracy and cross-entropy strings as we have special\r\n    # case handling for these in compile, based on model output shape.\r\n    return metrics_module.deserialize(metrics_config, custom_objects)\r\n  return metrics_config\r\n\r\n\r\ndef compile_args_from_training_config(training_config, custom_objects=None):\r\n  \"\"\"Return model.compile arguments from training config.\"\"\"\r\n  if custom_objects is None:\r\n    custom_objects = {}\r\n\r\n  optimizer_config = training_config['optimizer_config']\r\n  optimizer = optimizers.deserialize(\r\n      optimizer_config, custom_objects=custom_objects)\r\n\r\n  # Recover losses.\r\n  loss_config = training_config['loss']\r\n  if isinstance(loss_config, list):  # Loss fed to compile as a list.\r\n    loss = [losses.deserialize(lc, custom_objects) for lc in loss_config]\r\n  elif isinstance(loss_config, dict) and 'class_name' not in loss_config:\r\n    # Loss fed to compile as a dict.\r\n    loss = {\r\n        k: losses.deserialize(v, custom_objects)\r\n        for (k, v) in loss_config.items()\r\n    }\r\n  else:  # Loss fed to compile as a str/ function/ class instance.\r\n    loss = losses.deserialize(loss_config, custom_objects)\r\n\r\n  # Recover metrics.\r\n  metrics_config = training_config['metrics']\r\n  if isinstance(metrics_config, dict):  # Metrics fed to compile as a dict.\r\n    metrics = {\r\n        k: convert_output_metrics(v, custom_objects)\r\n        for (k, v) in metrics_config.items()\r\n    }\r\n  elif isinstance(metrics_config, list):  # Metrics fed to compile as a list.\r\n    metrics = [\r\n        convert_output_metrics(m, custom_objects) for m in metrics_config\r\n    ]\r\n  else:  # No metrics.\r\n    metrics = None\r\n\r\n  # Recover weighted metrics.\r\n  weighted_metrics_config = training_config['weighted_metrics']\r\n  if isinstance(weighted_metrics_config, dict):\r\n    # Metrics fed to compile as a dict.\r\n    weighted_metrics = {\r\n        k: convert_output_metrics(v, custom_objects)\r\n        for (k, v) in weighted_metrics_config.items()\r\n    }\r\n  elif isinstance(weighted_metrics_config, list):\r\n    # Metrics fed to compile as a list.\r\n    weighted_metrics = [\r\n        convert_output_metrics(m, custom_objects)\r\n        for m in weighted_metrics_config\r\n    ]\r\n  else:  # No metrics.\r\n    weighted_metrics = None\r\n\r\n  sample_weight_mode = training_config['sample_weight_mode']\r\n  loss_weights = training_config['loss_weights']\r\n\r\n  return dict(\r\n      optimizer=optimizer,\r\n      loss=loss,\r\n      metrics=metrics,\r\n      weighted_metrics=weighted_metrics,\r\n      loss_weights=loss_weights,\r\n      sample_weight_mode=sample_weight_mode)\r\n```\r\n", "@pavithrasv yes I will try, thanks. Sorry I didn't find the time so far but I'll try to fix it.", "Small edit to the above fix: \r\nPlease replace `metrics_config = training_config['metrics']` with \r\n`metrics_config = training_config.get('metrics', None)` and \r\n`weighted_metrics_config = training_config['weighted_metrics']` with \r\n`weighted_metrics_config = training_config.get('weighted_metrics', None)`\r\n\r\nThank you!", "@pavithrasv can you contact me at thierry.herrmann@gmail.com ?", "Sure, left you a message, please feel free to email me if you have any questions. Thank you!"]}, {"number": 33228, "title": "fix for #32612 : load saved custom Loss class", "body": "Attempt to fix https://github.com/tensorflow/tensorflow/issues/32612\r\n\r\nHere's an example with a custom Loss class, including config to ensure it's properly reloaded:\r\n```python\r\nclass HuberLoss(keras.losses.Loss):\r\n    def __init__(self, threshold=1.0, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.threshold = threshold\r\n    \r\n    @tf.function\r\n    def call(self, y_true, y_pred):\r\n        error = tf.abs(y_true - y_pred)\r\n        is_small_error = error <= self.threshold\r\n        squared_loss = tf.square(error) / 2\r\n        linear_loss = error * self.threshold - 0.5 * self.threshold**2\r\n        return tf.where(is_small_error, squared_loss, linear_loss)\r\n    \r\n    def get_config(self):\r\n        cfg = super().get_config()\r\n        cfg['threshold'] = self.threshold\r\n        return cfg\r\n```\r\nThe class can be used for a regression task this way:\r\n```python\r\nmodel = keras.Sequential([\r\n  keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\r\n  keras.layers.Dense(1)\r\n])\r\n\r\nmodel.compile(loss=HuberLoss(2.0), optimizer=\"sgd\")\r\nmodel.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\r\n\r\n# save and reload\r\nmodel.save('model_with_huber_loss_class.h5')\r\nmodel = keras.models.load_model('model_with_huber_loss_class.h5', custom_objects={'HuberLoss': HuberLoss})\r\n\r\n# continue training\r\nmodel.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\r\n```\r\n\r\nThe problem was that `saving_util.compile_args_from_training_config(training_config, custom_objects=None)` supports `custom_objects` but `custom_objects` was not propagated to `losses.get()`.\r\n`losses.get()` later called `deserialize(identifier)` without specifying `custom_objects` while the `custom_objects` argument is supported by `deserialize()`: the default `None` was used and the custom class load failed.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33228) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33228) for more info**.\n\n<!-- need_author_cla -->", "closing that PR. Wrong email address preventing CLA validation"]}, {"number": 33227, "title": "Custom OptimizerV2 frozen", "body": "I seek to implement [AdamW](https://github.com/OverLordGoldDragon/keras-adamw) for `OptimizerV2`, using syntax from [Keras](https://github.com/CyberZHG/keras-radam/blob/5bda32f5e26c6dc8140f7c6c95c13e9128341282/keras_radam/optimizer_v2.py) and [TF2](https://github.com/taki0112/RAdam-Tensorflow) implementations of RAdam - but am getting nowhere.  Below is a minimal reproducible example for a rewritten portion of [Adam](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/adam.py#L32); the complete version is just this revision, with `_prepare_local` deleted entirely (see [gist](https://gist.github.com/OverLordGoldDragon/aad492db1ff6f20a7983e600611799f7)). `print` statements for introspection. Observations:\r\n\r\n - Loss doesn't change at all (tested for 20 iters)\r\n - Weights don't change at all via `state_ops.assign_sub` (or via `.assign` w/ `math_ops.sub`)\r\n - `var_t` _does_ change, showing that subtracted value isn't negligibly small - but the change isn't applied to `var`\r\n - `var_t` changes slightly across iterations, possibly per GPU parallelism randomness (values do vary within 1%, not \"almost zero\")\r\n\r\n<hr>\r\n\r\nIf linked implementations are correct, why is mine failing?\r\n\r\n<hr>\r\n\r\n```python\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\nfrom adam_v2 import Adam  # custom local module w/ described changes\r\nreset_seeds()  # see below\r\n\r\nipt   = Input(shape=(4,))\r\nx     = Dense(2, activation='relu', name='dense_1')(ipt)\r\nout   = Dense(1, activation='sigmoid')(x)\r\nmodel = Model(ipt, out)\r\nmodel.compile(Adam(lr=1e-3), 'binary_crossentropy')\r\n\r\nX = np.random.randn(32, 4)\r\nY = np.random.randint(0, 2, (32, 1))\r\nprint(model.train_on_batch(X,Y))\r\n```\r\n```python\r\n<tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=\r\narray([[-0.73267365,  0.13570619],\r\n       [-0.00733662, -0.9662695 ],\r\n       [ 0.00908351,  0.4974625 ],\r\n       [-0.73932624, -0.32877016]], dtype=float32)>\r\n      [[-0.7358344   0.13886832]\r\n       [-0.00417447 -0.9631073 ]\r\n       [ 0.00592144  0.49430028]\r\n       [-0.73616403 -0.32560796]]\r\n<tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=\r\narray([[-0.73267365,  0.13570619],\r\n       [-0.00733662, -0.9662695 ],\r\n       [ 0.00908351,  0.4974625 ],\r\n       [-0.73932624, -0.32877016]], dtype=float32)>\r\n\r\n0.8960571  # loss\r\n```\r\n\r\n<hr>\r\n\r\n```python\r\ndef _resource_apply_dense(self, grad, var):\r\n    var_dtype = var.dtype.base_dtype\r\n    lr_t = self._decayed_lr(var_dtype)\r\n    m = self.get_slot(var, 'm')\r\n    v = self.get_slot(var, 'v')\r\n    beta_1_t = array_ops.identity(self._get_hyper('beta_1', var_dtype))\r\n    beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))\r\n    epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\r\n    \r\n    m_t = state_ops.assign(m,\r\n    \t\t   beta_1_t * m + (1.0 - beta_1_t) * grad,\r\n    \t\t   use_locking=self._use_locking)\r\n    v_t = state_ops.assign(v,\r\n    \t\t   beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\r\n    \t\t   use_locking=self._use_locking)\r\n    if self.amsgrad:\r\n    \tvhat = self.get_slot(var, 'vhat')\r\n    \tvhat_t = state_ops.assign(vhat, math_ops.maximum(vhat, v_t),\r\n                                  use_locking=self._use_locking)\r\n    \tvar_delta = m_t / (K.sqrt(vhat_t) + epsilon_t)\r\n    else:\r\n    \tvar_delta = m_t / (K.sqrt(v_t) + epsilon_t)\r\n    var_t = math_ops.sub(var, lr_t * var_delta)\r\n\r\n    if 'dense_1/kernel' in var.name:\r\n    \tprint(var)\r\n    \tprint(K.eval(var_t))\r\n    var_update = state_ops.assign_sub(var, lr_t * var_delta,\r\n    \t\t\t              use_locking=self._use_locking)\r\n    if 'dense_1/kernel' in var.name:\r\n    \tprint(var)\r\n    \t\r\n    updates = [var_update, m_t, v_t]\r\n    if self.amsgrad:\r\n    \tupdates.append(vhat_t)\r\n    return control_flow_ops.group(*updates)\r\n```\r\n```python\r\nimport numpy as np\r\nimport random\r\nimport tensorflow as tf\r\n\r\ndef reset_seeds():\r\n    np.random.seed(1)\r\n    random.seed(2)\r\n    if tf.__version__[0] == '2':\r\n        tf.random.set_seed(3)\r\n    else:\r\n        tf.set_random_seed(3)\r\n```", "comments": ["**Solved**: change `K.sqrt` to `math_ops.sqrt`. Why it works: don't know. [Relevant issue](https://github.com/tensorflow/tensorflow/issues/33264)"]}]