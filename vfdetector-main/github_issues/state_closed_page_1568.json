[{"number": 5874, "title": "changed global_variables_initializer line", "body": "Changed global_variables_initializer to initialize_all_variables on master.", "comments": ["Can one of the admins verify this patch?", "@Goddard, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @keveman and @tensorflower-gardener to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@Goddard  `global_variables_initializer` is the new API in latest TF, why to change it back then ? I guess `initialize_all_variables` probably to be deprecated soon. ", "@Goddard , I ditto what @zuoxingdong said. If I'm missing anything, please let me know and we can re-open this PR."]}, {"number": 5873, "title": "tensorboard HTTP 404 errors", "body": "Observing the following, it seems chromedeveditor is obsolete, is there a recommended replacement library?\r\n\r\nINFO:tensorflow:path ../external\\core_overlay/core-overlay.html not found, sending 404\r\nINFO:tensorflow:returning 404 to 127.0.0.1 for /core-overlay/core-overlay.html\r\nINFO:tensorflow:path ../external\\core_transition/core-transition-css.html not found, sending 404\r\nINFO:tensorflow:returning 404 to 127.0.0.1 for /core-transition/core-transition-css.html\r\nINFO:tensorflow:path ../external\\core_media_query/core-media-query.html not found, sending 404\r\nINFO:tensorflow:returning 404 to 127.0.0.1 for /core-media-query/core-media-query.html\r\n", "comments": ["Please can you provide the information requested in the issue reporting template.\r\n", "NOTE: Only file GitHub issues for bugs and feature requests.  All other\ntopics will be closed.\n\nFor general support from the community, see [StackOverflow](\nhttps://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close\nissues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n\n### What related GitHub issues or StackOverflow threads have you found by\nsearching the web for your problem?\n\n### Environment info\nOperating System: Windows 10/64\n\nInstalled version of CUDA and cuDNN:\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n8.0, 5.1\n\nIf installed from binary pip package, provide:\n\n1. A link to the pip package you installed:\nhttp://www.lfd.uci.edu/~gohlke/pythonlibs/#tensorflow\n\n2. The output from `python -c \"import tensorflow;\nprint(tensorflow.__version__)\"`.\n0.11.0\n\n\n\n\n\nIf installed from source, provide\n\n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n\n### If possible, provide a minimal reproducible example (We usually don't\nhave time to read hundreds of lines of your code)\n\n\n### What other attempted solutions have you tried?\n\n\n### Logs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\n\n\nOn Sat, Nov 26, 2016 at 5:34 PM, Paul Barham <notifications@github.com>\nwrote:\n\n> Please can you provide the information requested in the issue reporting\n> template.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5873#issuecomment-263089978>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABXVTcwi5siFPnl1mTNMLQKpfUnnlOcRks5rCLPrgaJpZM4K84_r>\n> .\n>\n", "@danmane I'm not sure I understand the bug report at all, but this is probably worth noting:\r\n> Operating System: Windows 10/64", "There appear to be certain JavaScript files required but missing when trying to use tensorboard on windows. Tensorboard supports the graph, however trying to viewi scalars cause the HTTP 404 messages.", "@apiszcz Is TensorBoard working on Windows except for the console output?", "The GRAPH of the network is fine.\nThe scalar displays are not working, i followed a few examples that are\nsupposed to work.\n\n\nOn Mon, Nov 28, 2016 at 3:05 PM, Daniel W Mane <notifications@github.com>\nwrote:\n\n> @apiszcz <https://github.com/apiszcz> Is TensorBoard working on Windows\n> except for the console output?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5873#issuecomment-263378551>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABXVTeBTLxdhRsVKSPTVvmXJaaAwrXehks5rCzQBgaJpZM4K84_r>\n> .\n>\n"]}, {"number": 5872, "title": "Not able to download Mac GPU build. 404. ", "body": "Link in README is broken.\r\n\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0-py2-none-any.whl", "comments": ["It's [https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0-py2-none-any.whl](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0-py2-none-any.whl), see [https://ci.tensorflow.org/view/Nightly/](https://ci.tensorflow.org/view/Nightly/) for other versions.\r\n\r\nOr, unless you have a good reason, get something a bit more stable than the nightlies: \r\n[https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#using-pip](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#using-pip):\r\n\r\n\r\n```\r\n# Mac OS X, CPU only, Python 2.7:\r\n(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0-py2-none-any.whl\r\n\r\n# Mac OS X, GPU enabled, Python 2.7:\r\n(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0-py2-none-any.whl\r\n\r\n# Mac OS X, CPU only, Python 3.4 or 3.5:\r\n(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0-py3-none-any.whl\r\n\r\n# Mac OS X, GPU enabled, Python 3.4 or 3.5:\r\n(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0-py3-none-any.whl\r\n```", "> It's https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0-py2-none-any.whl, \r\n\r\ngives 404.", "> Link in README is broken.\r\n\r\nCould you please be a little more specific?", "@prb12 \r\n\r\nREADME -> [Installation](https://github.com/tensorflow/tensorflow#installation) -> Mac GPU : [Python 2](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0-py2-none-any.whl)\r\n\r\ngives 404", "@yifeif  Looks like this is maybe related to the version change from 0.11.0.rc2 to 0.11.0?", "Sorry for the confusion, we updated the names and it is currently only reflected in release branch. Could you use the link from release branch for now https://github.com/tensorflow/tensorflow/tree/r0.12 . We will merge it back to master soon.", "@yifeif Got it. Thanks :)"]}, {"number": 5871, "title": "Check the Status after call builder.Finalize() in generated io_ops.cc", "body": "e.g.\r\n\r\nLet's assume someone passed an int32 tensor into ReaderReadUpTo as the third input(num_records), which should have a type of int64.\r\n```\r\nReaderReadUpTo::ReaderReadUpTo(const ::tensorflow::Scope& scope,\r\n                               ::tensorflow::ops::Input reader_handle,\r\n                               ::tensorflow::ops::Input queue_handle,\r\n                               ::tensorflow::ops::Input num_records) {\r\n  if (!scope.ok()) return;\r\n  auto _reader_handle = ::tensorflow::ops::AsNodeOut(scope, reader_handle);\r\n  if (!scope.ok()) return;\r\n  auto _queue_handle = ::tensorflow::ops::AsNodeOut(scope, queue_handle);\r\n  if (!scope.ok()) return;\r\n  auto _num_records = ::tensorflow::ops::AsNodeOut(scope, num_records);\r\n  if (!scope.ok()) return;\r\n  ::tensorflow::Node* ret;\r\n  const auto  unique_name = scope.GetUniqueNameForOp(\"ReaderReadUpTo\");\r\n  auto builder = ::tensorflow::NodeBuilder(unique_name, \"ReaderReadUpTo\")\r\n                     .Input(_reader_handle)\r\n                     .Input(_queue_handle)\r\n                     .Input(_num_records)\r\n  ;\r\n  scope.UpdateBuilder(&builder);\r\n  scope.UpdateStatus(builder.Finalize(scope.graph(), &ret));\r\n  //HERE: builder.Finalize may fails. We should check if ret==NULL, otherwise it will cause a core dump.\r\n  ::tensorflow::NameRangeMap _outputs_range;\r\n  ::tensorflow::Status _status_ = ::tensorflow::NameRangesForNode(ret->def(), ret->op_def(), nullptr, &_outputs_range);\r\n  if (!_status_.ok()) {\r\n    scope.UpdateStatus(_status_);\r\n    return;\r\n  }\r\n\r\n  this->keys = Output(ret, _outputs_range[\"keys\"].first);\r\n  this->values = Output(ret, _outputs_range[\"values\"].first);\r\n}\r\n```\r\n\r\nWe need modify the function at tensorflow\\tensorflow\\cc\\framework\\cc_op_gen.cc:655 \r\n\r\nstring OpInfo::GetConstructorBody() const \r\n\r\nto add the check.", "comments": ["@josh11b This looks like maybe a C++ op framework issue?", "I'm on leave this week, assigning to @keveman ", "@snnn Thanks for spotting this. Fix on its way."]}, {"number": 5870, "title": "Feature Request: Bivariate/multivariate Gaussian CDF", "body": "Would it be possible to support computing the CDF of multivariate Gaussian distributions? The multivariate Gaussian implementation under https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distributions/python/ops doesn't seem to support the CDF for multivariate Gaussian yet.", "comments": ["Is this still open? I can work on this.", "Hi @devanshdalal , it is still open. Please feel free.", "Still open? Can I try it?", "Hi @YiMX , please go ahead."]}, {"number": 5869, "title": "Model Exporting Does not Use optimize_for_inference_lib", "body": "Looking at the code in master, It looks like neither [`tensorflow.python.training.saver`](https://github.com/tensorflow/tensorflow/blob/eea96fc81bb1ae22eeb1d2141e47e182cecb7608/tensorflow/python/training/saver.py) nor [`tensorflow.contrib.session_bundle.exporter`](https://github.com/tensorflow/tensorflow/blob/eea96fc81bb1ae22eeb1d2141e47e182cecb7608/tensorflow/contrib/session_bundle/exporter.py) use [`optimize_for_inference_lib`](https://github.com/tensorflow/tensorflow/blob/df871edcff2faf643975b9863100ed41b6da9c3f/tensorflow/python/tools/optimize_for_inference_lib.py) which is described as:\r\n>There are several common transformations ... that help reduce the amount of computation needed when the network is used only for inference.\r\n\r\nFurther these optimizations are not mentioned / suggested in the tensorflow serving docs:\r\n* https://tensorflow.github.io/serving/\r\n* https://github.com/tensorflow/tensorflow/blob/55cb1b37133e6c0409a708a763fccf566580a90a/tensorflow/contrib/session_bundle/README.md\r\n\r\nArguable the serving logic could be changed to [use frozen models](https://github.com/tensorflow/tensorflow/blob/5a5a25ea3ebef623e07fb9a46419a9df377a37a5/tensorflow/g3doc/how_tos/tool_developers/index.md#freezing) (see [`freeze_graph`](https://github.com/tensorflow/tensorflow/blob/df871edcff2faf643975b9863100ed41b6da9c3f/tensorflow/python/tools/freeze_graph.py)) which calls `convert_variables_to_constants`.", "comments": ["They shouldn't use optimize_for_inference_lib by default since not all exportings will be only used for inference."]}, {"number": 5868, "title": "how to continues tensor flow training after crash", "body": "I am training inception v2 slim model, but it is crash during the training. which cmd can let me continue the training using the latest checkpoint file? I can't find it in the link below. it has fine tune but not continue training.\r\n\r\nhttps://github.com/tensorflow/models/tree/master/slim\r\n\r\n", "comments": ["From https://www.tensorflow.org/versions/r0.11/resources/index.html\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n"]}, {"number": 5867, "title": "optimize_for_inference.py should remove Dropout operations", "body": "When I first tried using an exported MNIST model with TensorFlow on iOS, I got the following error:\r\n\r\n    Invalid argument: No OpKernel was registered to support Op 'RandomUniform' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n      <no registered kernels>\r\n    \r\n         [[Node: dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0](dropout/Shape)]]\r\n\r\nSince Dropout operations are no-ops during inference (we pass in a keep probability of 1), it would be nice if they were removed (or turned into no-ops of some kind that can be parsed and ignored by the iOS library).\r\n\r\nWhile I was able to work around this by explicitly exporting a separate graph that does not contain Dropout, it was pretty tedious and it would be nice if the `optimize_for_inference.py` script did this automatically.\r\n\r\n### Environment info\r\nOperating System: macOS 10.12\r\n\r\nInstalled version of CUDA and cuDNN: \r\nNone\r\n\r\nSource:\r\nThis week's tip-of-tree (around d93d526cb804896004c1c20a41586ce0e2415b9c)\r\n", "comments": ["Yes, it would be nice if the `optimize_for_inference.py` script did this automatically.  \r\nMy current manual solution https://dato.ml/drop-dropout-from-frozen-model/", "related / dupe: https://github.com/tensorflow/tensorflow/issues/6124", "There is also this SO (attempt at a) solution: http://stackoverflow.com/questions/40358892/wipe-out-dropout-operations-from-tensorflow-graph\r\nand my attempt at this:\r\n```python\r\nfor node in temp_graph_def.node:\r\n    for idx, i in enumerate(node.input):\r\n        input_clean = node_name_from_input(i)\r\n        if input_clean.endswith('/cond/Merge') and input_clean.split('/')[-3].startswith('dropout'):\r\n            identity = node_from_map(input_node_map, i).input[0]\r\n            assert identity.split('/')[-1] == 'Identity'\r\n            parent = node_from_map(input_node_map, node_from_map(input_node_map, identity).input[0])\r\n            pred_id = parent.input[1]\r\n            assert pred_id.split('/')[-1] == 'pred_id'            \r\n            good = parent.input[0]\r\n            node.input[idx] = good\r\n```", "i'am getting the following error in android \r\n`ERROR: Analysis of target '//tensorflow/contrib/util:convert_graphdef_memmapped_format' failed; build aborted.\r\n`", "Is there anyway this can be accomplished using the transform_graph tool and remove_nodes?", "Is there a way to get rid of the dropout in a model? I am currently trying to remove it but I haven't found a way yet : \r\n\r\n```\r\nOperation Name : dropout_1/keep_prob\r\nTensor Stats : (<tf.Tensor 'dropout_1/keep_prob:0' shape=() dtype=float32>,)\r\nOperation Name : dropout_1/Shape\r\nTensor Stats : (<tf.Tensor 'dropout_1/Shape:0' shape=(4,) dtype=int32>,)\r\nOperation Name : dropout_1/random_uniform/min\r\nTensor Stats : (<tf.Tensor 'dropout_1/random_uniform/min:0' shape=() dtype=float32>,)\r\nOperation Name : dropout_1/random_uniform/max\r\nTensor Stats : (<tf.Tensor 'dropout_1/random_uniform/max:0' shape=() dtype=float32>,)\r\nOperation Name : dropout_1/random_uniform/RandomUniform\r\nTensor Stats : (<tf.Tensor 'dropout_1/random_uniform/RandomUniform:0' shape=(?, ?, ?, 4096) dtype=float32>,)\r\nOperation Name : dropout_1/random_uniform/sub\r\nTensor Stats : (<tf.Tensor 'dropout_1/random_uniform/sub:0' shape=() dtype=float32>,)\r\nOperation Name : dropout_1/random_uniform/mul\r\nTensor Stats : (<tf.Tensor 'dropout_1/random_uniform/mul:0' shape=(?, ?, ?, 4096) dtype=float32>,)\r\nOperation Name : dropout_1/random_uniform\r\nTensor Stats : (<tf.Tensor 'dropout_1/random_uniform:0' shape=(?, ?, ?, 4096) dtype=float32>,)\r\nOperation Name : dropout_1/add\r\nTensor Stats : (<tf.Tensor 'dropout_1/add:0' shape=(?, ?, ?, 4096) dtype=float32>,)\r\nOperation Name : dropout_1/Floor\r\nTensor Stats : (<tf.Tensor 'dropout_1/Floor:0' shape=(?, ?, ?, 4096) dtype=float32>,)\r\nOperation Name : dropout_1/Inv\r\nTensor Stats : (<tf.Tensor 'dropout_1/Inv:0' shape=() dtype=float32>,)\r\nOperation Name : dropout_1/mul\r\nTensor Stats : (<tf.Tensor 'dropout_1/mul:0' shape=(?, ?, ?, 4096) dtype=float32>,)\r\nOperation Name : dropout_1/mul_1\r\nTensor Stats : (<tf.Tensor 'dropout_1/mul_1:0' shape=(?, ?, ?, 4096) dtype=float32>,)\r\n```\r\nAny updates on this ?", "Is using a tensor for keep_prob which is set to 1 during inference not enough?", "Any updates on this? Just tried optimize_for_inference.py but dropout still there confusing NVIDIA TensorRT and Jetson TX2 inference.\r\n", "There is a way to create a new graph and copy all the nodes you want (except dropout) from the old graph to the new one, fixing the input/output names of the nodes before and after the dropout layer. This way you can create a new graph without the dropout layer.", "Please let optimize_for_inference remove dropout layers from the graph.\r\nI am trying to convert a MNIST model from TF to TFLite.\r\nIt seems to me that this should be the very first thing to try when one is learning to use TFLite, since MNIST is the first tutorial in TensorFlow.\r\n\r\nTo allow the conversion to TFLite, I need to remove the dropout layer in my MNIST model frozen graph. \r\n\r\nIf I get it right, the only way to do that (without writing a python script for that) is to use the Graph Transform Tool.\r\nWhich can only run with Bazel. \r\nWhich requires to download TF from sources.\r\nThis feels like a **huge overkill**.\r\n\r\nThe funny thing is that in prediction mode the dropout would not even be used.\r\nSo I'm spending a day downloading and configuring components in my environment to just remove something that would not even be used.\r\n\r\nIf I am missing something please tell me - I would be overjoyed to know I got it wrong.", "Run training script -> export_inference_graph -> freeze_graph . This removes all the extra nodes added during training. \r\n\r\nI was trying to convert finetuned inception model to uff format required by Jetson, and got issues due to the same RandomUniform node and a few others dropout related nodes. Running export_inference_graph on finetuned model before freezing fixed the issue.", "Hi @shrutim90 , can you please provide more information on the export_inference_graph script?", "@AndRossi : In this link: https://github.com/tensorflow/models/tree/master/research/slim#Pretrained , they have used export_inference_graph script to export default inception graph. If you'll check the code for that script, you'll see there is an option to specify dataset_name as well. \r\n\r\npython export_inference_graph.py \\\r\n  --alsologtostderr \\\r\n  --model_name=inception_v3 \\\r\n  --dataset_name=flowers \\\r\n  --output_file=/tmp/inception_v3_inf_graph_flowers.pb\r\n\r\nSo you can specify any dataset predefined in slim library or add you own data in the slim dataset and then use above script, with required dataset_name. It will create graph with all the extra layers removed and correct number of output nodes.", "@shrutim90 It didn't work for me. When I use a tf.slim model with my own dataset and is_training=True I obtain a checkpoint with 97% accuracy. If I then import this checkpoint into the same tf.slim model but with is_training=False I get only around 40% accuracy.", "You should freeze the training checkpoint into a separate eval/inference graph. You should have separate python code to constructor this eval_graph. Take a look at tf_estimator model_fn to see how it encourages constructing a different graph for training/eval/inference. That graph should be compatible with TF Lite. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py\r\n\r\nIn general, it's difficult to convert a training graph into an inference graph because batch norm and dropout create different graphs for training or eval.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@tofulawrence I bet you had a look on this https://github.com/tensorflow/tensorflow/pull/19264,  can you comment why it can't be upstreamed ? Does it fail some internal test cases ?", "I got dropout ops convert fail in tensorrt 4.0\r\n```\r\n2018-08-30 11:11:20.711520: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] subgraph conversion error for subgraph_index:10 due to: \"Unimplemented: Require 4 dimensional input. Got 3 query_lookup/Dropout/cond/dropout/Shape/Switch\" SKIPPING......( 3 nodes)\r\n```\r\n", "I've just approved the PR related to this one. Sorry for the delay in reviewing.", "which version can solve this problem\u3002hope comming soon", "Any update on the solution yet ?", "I think that the only way to remove dropout for now is to comment out the dropout lines in your python source code and export the model without them ...", "Any update on this issue yet?", "Any update on this issue yet?", "> There is also this SO (attempt at a) solution: http://stackoverflow.com/questions/40358892/wipe-out-dropout-operations-from-tensorflow-graph\r\n> and my attempt at this:\r\n> \r\n> ```python\r\n> for node in temp_graph_def.node:\r\n>     for idx, i in enumerate(node.input):\r\n>         input_clean = node_name_from_input(i)\r\n>         if input_clean.endswith('/cond/Merge') and input_clean.split('/')[-3].startswith('dropout'):\r\n>             identity = node_from_map(input_node_map, i).input[0]\r\n>             assert identity.split('/')[-1] == 'Identity'\r\n>             parent = node_from_map(input_node_map, node_from_map(input_node_map, identity).input[0])\r\n>             pred_id = parent.input[1]\r\n>             assert pred_id.split('/')[-1] == 'pred_id'            \r\n>             good = parent.input[0]\r\n>             node.input[idx] = good\r\n> ```\r\n\r\nHi @cancan101, what is `node_name_from_input`? \r\nI'm trying to implement this manual fix for a conversion `Tensorflow > CoreML`.", "One comment to add - it's hard to debug if you don't know what this means: \r\n\r\n```Here is a list of operators for which you will need custom implementations: RandomUniform.```\r\n\r\nor \r\n\r\n```Exception: Placeholder Placeholder should be specied by input_arrays.```\r\n\r\nwhich either indicate the use for a ```tf.placeholder``` for the dropout probability or the dropout node in the ```tf.lite``` conversion process. Outcommenting as suggested helped!"]}, {"number": 5866, "title": "variable_summaries(...) missing parameter 'name'", "body": "Versions 0.11 and 0.10 had a parameter named 'name' on variable_summaries function but this is now missing on master and 0.12", "comments": ["Can one of the admins verify this patch?", "@oliveirabc, thanks for your PR! By analyzing the history of the files in this pull request, we identified @danmane, @keveman and @nsthorat to be potential reviewers.", "The removal of this argument was deliberate. The new summary ops under the tf.summary module use the TensorFlow name scoping system for name de-duplication, so client code no longer needs to manage name scopes by hand. "]}, {"number": 5865, "title": "Update tutorial example ", "body": "Correct bracketing in \"building the input function\" example", "comments": ["@nicholasbutlin, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @vrv to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "PR merged. Thanks, @nicholasbutlin "]}, {"number": 5864, "title": "Fixed wrong documentation on ctc_beam_search_decoder", "body": "The documentation for `ctc_beam_search_decoder` has an error. It claims that it is equivalent to `ctc_greedy_decoder` when `top_paths=1`. This gives the impression that if we only care about the best result, we should always use `ctc_greedy_decoder`. However, `ctc_greedy_decoder` just does a simple argmax at each timestep, which is not equivalent result to a beam_search with `beam_width=100` and `top_paths=1`.\r\n\r\nThe only case they are equivalent is when both `beam_width` and `top_paths` are `1`. This pull request corrects this documentation error.", "comments": ["Can one of the admins verify this patch?", "@lingz, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @tensorflower-gardener and @jhseu to be potential reviewers.", "@tensorflow-jenkins test this please", "Thanks for catching this!", "PR merged. Thank you, @lingz !"]}, {"number": 5863, "title": "Feature request: before throwing OutOfRangeError, dequeue the rest samples from a queue", "body": "When using a FIFOQueue as the input for a pipeline, we often encounter the case that `epoch_size % batch_size == M` where `M != 0`. It will lead the following issue:\r\n\r\nif we would like to evaluate our model with the entire validation dataset, we cannot test the last `M` samples before getting the exception `OutOfRangeError` if we set `num_epoches` to 1. Even if we set `num_epoches` to `None` to avoid `OutOfRangeError`, we cannot exactly get the result because of the indivisibility issue.\r\n\r\nSo what I can think of is, we need a feature like this:\r\n1) if `num_epoches` is set to None, we need to dequeue the rest M samples from the queue before every epoch is finished. For example, a dataset's size is 10, saying they are `[a,b,c,d,e,f,g,h,i,j]`, and `batch_size` is 3, the size of dequeued samples would be `[a,b,c], [d,e,f], [g,h,i], [j], [a,b,c], ...`;\r\n2) if `num_epoches` is set to 1, we need to dequeue the rest M samples from the queue before throwing `OutOfRangeError`.For the above example, the result would be `[a,b,c], [d,e,f], [g,h,i], [j], OutOfRangeError`.", "comments": ["@mrry I'm not sure how `DequeueMany` and epochs are supposed to interact with `FIFOQueues`.  Seems to me like changing semantics like this is likely to break a lot of other people's code? \r\n\r\nWouldn't it be easier to arrange that `dataset_size % batch_size == 0`, e.g. by padding if necessary?", "This is what `FIFOQueue.dequeue_up_to(n)` does. The various wrapper functions for batching have an `allow_small_batch` optional argument that use this op internally.", "@wangchuan Does this solve your problem?", "I tried `allow_small_batch` option, it meets part of my requirement, i.e. if `num_epoches` is set to an integer, the last iteration will dequeue the rest samples before throwing OutOfRangeError. However, if `num_epoches` is set to None, I think the results would be `[a,b,c], [d,e,f], [g,h,i], [j,a,b], [c,d,e], ...` instead of my expecting `[a,b,c], [d,e,f], [g,h,i], [j], [a,b,c], ...`. Am I correct?", "You're correct. The queue-based input pipelines (such as `tf.train.batch()` etc.) don't support any form of intermediate punctuation, so only the final batch will be smaller if the dataset size is not divisible by the batch size.\r\n\r\nCan you explain your use case for having the intermediate smaller batch? As far as I can tell, the smaller batch isn't a reliable way of detecting the transition from one epoch to the next.", "Closing this due to lack of response. I doubt we'll implement this in the current input pipeline scheme, which is currently being redesigned, but it would be great if you could still share your use case so that we can take it into account."]}, {"number": 5862, "title": "Feature request: easier access to all variables inside a scope", "body": "Right now, in order to access the variables inside a scope, AFAIK we have to do the following:\r\n\r\n    variables = tf.get_collection(tf.GraphKeys.VARIABLES, scope)\r\n\r\nIt would be far easier if we could just write\r\n\r\n    variables = scope.get_all_variables()\r\n\r\nor maybe\r\n\r\n    variables = tf.get_scope_variables(scope)\r\n\r\nIs it a reasonable request?", "comments": ["@josh11b can you comment or reassign?", "Would love to work on this if @josh11b doesn't have bandwidth :)", "I think it's not easy to introduce new APIs for TF 1.0 now. Any new API will have to go through a design review, and I don't feel like doing that. On the other hand, we could just add a property to the VariableScope object that would select all variables. That's a very small API change, I think. But I'll wait for Martin to comment more, he knows way more about the API process.", "It's not a problem to add new features, we're a little more conservative in what we accept. This seems like a reasonable request to me, I'd go for the `vars = scope.get_variables()` option.", "I think we should have scope.global_variables() (or just scope.variables() ?)  and scope.trainable_variables() to match the global tf functions.Guys (josh11b/firstprayer), will you send a PR?", "This is done, correct? Closing.", "Yes, done!"]}, {"number": 5861, "title": "compute_gradients error, if there are unneeded variables", "body": "I hope this hasn't been reported already. A Google and GitHub search came up empty.\r\n\r\n### Minimum example\r\n```python\r\ntf.reset_default_graph()\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(0.1)\r\n\r\nx1_var = tf.Variable([1, 2, 3], dtype=tf.float32)\r\nx2_var = tf.Variable([2, 3, 4], dtype=tf.float32)\r\n\r\ngrad_op = optimizer.compute_gradients(x1_var)\r\n\r\ninit = tf.initialize_all_variables()\r\n\r\nsess = tf.Session()\r\n\r\nsess.run(init)\r\n\r\ngrads = sess.run(grad_op)\r\n\r\nprint grads\r\n```\r\n\r\n### Expected Output\r\nA list of tuples containing all variables needed to compute the input, `x1_var` in this example, and their gradients. Alternativly, a list of all variables in the graph with zeros or `None` values for the gradients of those variables that are not needed to compute the input to `compute_gradients`.\r\n\r\n### Actual Output\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-9-868cc989ac50> in <module>()\r\n     15 \r\n     16 sess.run(init)\r\n---> 17 print sess.run(grad_op)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    715     try:\r\n    716       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 717                          run_metadata_ptr)\r\n    718       if run_metadata:\r\n    719         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    900 \r\n    901     # Create a fetch handler to take care of the structure of fetches.\r\n--> 902     fetch_handler = _FetchHandler(self._graph, fetches, feed_dict_string)\r\n    903 \r\n    904     # Run request and get response.\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in __init__(self, graph, fetches, feeds)\r\n    356     \"\"\"\r\n    357     with graph.as_default():\r\n--> 358       self._fetch_mapper = _FetchMapper.for_fetch(fetches)\r\n    359     self._fetches = []\r\n    360     self._targets = []\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in for_fetch(fetch)\r\n    179     elif isinstance(fetch, (list, tuple)):\r\n    180       # NOTE(touts): This is also the code path for namedtuples.\r\n--> 181       return _ListFetchMapper(fetch)\r\n    182     elif isinstance(fetch, dict):\r\n    183       return _DictFetchMapper(fetch)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in __init__(self, fetches)\r\n    286     \"\"\"\r\n    287     self._fetch_type = type(fetches)\r\n--> 288     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n    289     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)\r\n    290 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in for_fetch(fetch)\r\n    179     elif isinstance(fetch, (list, tuple)):\r\n    180       # NOTE(touts): This is also the code path for namedtuples.\r\n--> 181       return _ListFetchMapper(fetch)\r\n    182     elif isinstance(fetch, dict):\r\n    183       return _DictFetchMapper(fetch)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in __init__(self, fetches)\r\n    286     \"\"\"\r\n    287     self._fetch_type = type(fetches)\r\n--> 288     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n    289     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)\r\n    290 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in for_fetch(fetch)\r\n    176     if fetch is None:\r\n    177       raise TypeError('Fetch argument %r has invalid type %r' %\r\n--> 178                       (fetch, type(fetch)))\r\n    179     elif isinstance(fetch, (list, tuple)):\r\n    180       # NOTE(touts): This is also the code path for namedtuples.\r\n\r\nTypeError: Fetch argument None has invalid type <type 'NoneType'>\r\n```\r\n\r\n### Further examples\r\nThis works:\r\n```python\r\n# ...\r\n\r\nx1_var = tf.Variable([1, 2, 3], dtype=tf.float32)\r\n# x2_var = tf.Variable([2, 3, 4], dtype=tf.float32)\r\n\r\ngrad_op = optimizer.compute_gradients(x1_var)\r\n\r\n# ...\r\n```\r\n```\r\n[(array([ 1.,  1.,  1.], dtype=float32), array([ 1.,  2.,  3.], dtype=float32))]\r\n```\r\n\r\nThis works:\r\n```python\r\n# ...\r\n\r\nx1_var = tf.Variable([1, 2, 3], dtype=tf.float32)\r\nx2_var = tf.Variable([2, 3, 4], dtype=tf.float32)\r\n\r\ncombined_op = tf.concat(0, [x1_var, x2_var])\r\n\r\ngrad_op = optimizer.compute_gradients(combined_op)\r\n\r\n# ...\r\n```\r\n```\r\n[(array([ 1.,  1.,  1.], dtype=float32), array([ 1.,  2.,  3.], dtype=float32)), (array([ 1.,  1.,  1.], dtype=float32), array([ 2.,  3.,  4.], dtype=float32))]\r\n```\r\n\r\nThis doesn't:\r\n```python\r\n# ...\r\n\r\nx1_var = tf.Variable([1, 2, 3], dtype=tf.float32)\r\nx2_var = tf.Variable([2, 3, 4], dtype=tf.float32)\r\n\r\ncombined_op = tf.concat(0, [x1_var, x2_var])\r\n\r\ngrad_op = optimizer.compute_gradients(x1_var)\r\n\r\n# ...\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-18-23fbf6b769d2> in <module>()\r\n     16 sess.run(init)\r\n     17 \r\n---> 18 grads = sess.run(grad_op)\r\n     19 \r\n     20 print grads\r\n...\r\n```\r\n\r\n### Workaround\r\nKeep track of all variables needed to compute the input and pass them explicitly to `compute_gradients`.\r\n\r\n```python\r\n# ...\r\n\r\nx1_var = tf.Variable([1, 2, 3], dtype=tf.float32)\r\nx2_var = tf.Variable([2, 3, 4], dtype=tf.float32)\r\n\r\ngrad_op = optimizer.compute_gradients(x1_var, [x1_var])\r\n\r\n# ...\r\n```\r\n```\r\n[(array([ 1.,  1.,  1.], dtype=float32), array([ 1.,  2.,  3.], dtype=float32))]\r\n```\r\n\r\nTensorflow version: 0.11.0rc0", "comments": ["This looks a little confused to me:\r\n```\r\ngrad_op = optimizer.compute_gradients(x1_var)\r\n```\r\nThe documentation for compute_gradients is here:\r\nhttps://www.tensorflow.org/versions/r0.11/api_docs/python/train.html#Optimizer\r\nThis method returns a list of `(gradient, variable)` pairs, not an 'op', and the results are intended for passing into the `apply_gradients()` method.\r\n\r\nIt doesn't make sense (at least to me) to `call session.run` on this?", "Thanks for your response. Actually I have found the problem in the meantime. `optimizer.compute_gradients` returns `None` as the gradient value for all variables not related to its input and `session.run` throws an error when you pass it a `None` value. Both are expected behaviour individually and I just didn't think of the connection, so I'm going to close this issue and would like to apologise for the trouble.\r\n\r\nRegarding the last part of your comment: I call `session.run` on the gradients because I'm interested in their value. I just passed the output of `optimizer.compute_gradients` as is to `session.run` because I wanted to keep the example as simple as possible. In my code I actually do `grads = sess.run([grad for grad, _ in grad_op])`. I did a bit of googling and apparently you had to use the second approach in previous versions of tf, but now the first one works too (apart from the `None` values ;)", "@FelixGruen Yes- I guess this makes sense.  The grad of any tensor wrt an unrelated Variable will be `None`, so the list will presumably be `[(grad1, Var1), (None, Var2)]`.\r\n\r\nI'm actually surprised that calling `session.run` on this list of tuples didn't raise an error immediately (The first argument is supposed to be a list of tensors/ops).  I guess there must be some code to flatten the list...\r\n\r\nThanks for following up.\r\n"]}, {"number": 5860, "title": "Add load method to tf.Variable", "body": "Allows loading new value into variable without adding new operations to the graph.\r\n\r\nSometimes it is convenient to just load new value into variable without adding new operations to the graph. For example one may want to load values into some weights from other model or model trained with other framework.", "comments": ["Can one of the admins verify this patch?", "@dm0, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @sherrym to be potential reviewers.", "Should the new method have unit-test coverage? @dm0 ", "@caisq Probably yes. Is it correct place to add unit test to: tensorflow/python/kernel_tests/variable_ops_test.py?", "I think python/kernel_tests/variables_test.py will be better. Thanks!", "@tensorflow-jenkins Test this please.", "@tensorflow-jenkins Test this please.", "Failure is a flake. Thank you!"]}, {"number": 5859, "title": "Ignore", "body": "", "comments": []}, {"number": 5858, "title": "ImportError: cannot import name descriptor", "body": "I installed tensorflow from source follow[ tutorial](https://www.tensorflow.org/versions/master/get_started/os_setup.html) and the os is macOS Sierra,tensorflow version is v0.11.0, i set up TensorFlow such that all files are linked (instead of copied) from the system directories, run the following commands inside the TensorFlow root directory:\r\n\r\n        bazel build -c opt //tensorflow/tools/pip_package:build_pip_package_\r\n        mkdir _python_build\r\n        cd _python_build\r\n        ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/* .\r\n        ln -s ../tensorflow/tools/pip_package/* .\r\n        python setup.py develop\r\n\r\nwhen i follow the tutorial to retrain Inception's Final Layer for classifying flowers with  scripts as follows:\r\n\r\n        \u201cbazel build tensorflow/examples/image_retraining:retrain\u201c\r\n        \u201cbazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos\u201c\r\n\r\n**i got the same error as follows:**\r\n\r\n         Traceback (most recent call last):\r\n         File \"/Users/jway/Documents/tensorflow/bazel-       bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 79, in <module>\r\n         import tensorflow as tf\r\n         File \"/Users/jway/Documents/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n         from tensorflow.python import *\r\n         File \"/Users/jway/Documents/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 63, in <module>\r\n         from tensorflow.core.framework.graph_pb2 import *\r\n         File \"/Users/jway/Documents/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n         from google.protobuf import descriptor as _descriptor\r\n         ImportError: cannot import name descriptor\r\n\r\nI don't know how to solve it,can somebody give me some advices?", "comments": ["Please can you provide the information requested in the issues reporting template. \r\n\r\nIt seems likely that you have an installation issue (protobuf library?) but it's impossible to triage this without OS and tensorflow versions, installation method, and the command lines needed to reproduce.", "Automatically closing due to lack of recent activity. When further information is posted we will reopen this. Thanks.", "I'm having the same problem. Tensorflow was installed with pip on macOS Sierra.", "Me too", "I think I found a solution for this. Mine is on Windows however and I made my error \"go away\". I don't know if it is actually solved.\r\n\r\nFor other OS besides Windows, try:\r\n`pip uninstall protobuf`\r\n`brew install protobuf`\r\n`mkdir -p /Library/Python/2.7/lib/python/site-packages`\r\n`echo 'import site; site.addsitedir(\"/usr/local/lib/python2.7/site-packages\")' >> /Library/Python/2.7/lib/python/site-packages/homebrew.pth\r\n`\r\n\r\nOn Windows for me I uninstalled protobuf and tensorflow and reinstalled tensorflow only. \"brew\" doesn't work on cmd."]}, {"number": 5857, "title": "Error while Retraining Inception", "body": "On running this command:\r\n\r\npython tensorflow/examples/image_retraining/retrain.py \\\r\n--bottleneck_dir=/tf_files/bottlenecks \\\r\n--how_many_training_steps 500 \\\r\n--model_dir=/tf_files/inception \\\r\n--output_graph=/tf_files/retrained_graph.pb \\\r\n--output_labels=/tf_files/retrained_labels.txt \\\r\n--image_dir /tf_files/flower_photos\r\n\r\nI am getting: \r\nIOError: CRC check failed 0x76f1f85e != 0x6caceac0L\r\n", "comments": ["Please can you provide all of the information requested in the issues reporting template.", "I am following these steps on the codelabs. (Link : https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#4)\r\n\r\nI am getting stuck at Step 5. The error shown is as mentioned in my previous query.\r\nCan you please help?", "https://www.mikeash.com/getting_answers.html", "I'm having this same issue. :( ", "I would like to help, but this kind of open-ended question without providing the background information requested in the issues reporting template poses the prospect that it may take me a long time to run through the codelab myself, in a setup different in unknown ways from yours, and not discover any problem.  This forum is intended for well-characterized bug reports and feature requests.  Usage difficulties should be posted to stackoverflow.", "I run into the same issue. Machine: MacOS 10.12.1\r\n\r\nI follow the 9-step tutorial to run the 1st Tensorflow \r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/\r\n\r\nStep 1-4 are smooth ( install docker, install tensorflow, retrieve images for training).\r\n\r\nStep 5 presents a method to train the imagenet last layer based on local image with the following python command:\r\n\r\npython tensorflow/examples/image_retraining/retrain.py \r\n--bottleneck_dir=/tf_files/bottlenecks \r\n--how_many_training_steps 500 \r\n--model_dir=/tf_files/inception \r\n--output_graph=/tf_files/retrained_graph.pb \r\n--output_labels=/tf_files/retrained_labels.txt \r\n--image_dir /tf_files/flower_photos\r\n\r\nBelow is the error log:\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 930, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 809, in main\r\n    maybe_download_and_extract()\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 371, in maybe_download_and_extract\r\n    tarfile.open(filepath, 'r:gz').extractall(dest_directory)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 2051, in extractall\r\n    self.extract(tarinfo, path)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 2088, in extract\r\n    self._extract_member(tarinfo, os.path.join(path, tarinfo.name))\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 2164, in _extract_member\r\n    self.makefile(tarinfo, targetpath)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 2205, in makefile\r\n    copyfileobj(source, target)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 265, in copyfileobj\r\n    shutil.copyfileobj(src, dst)\r\n  File \"/usr/lib/python2.7/shutil.py\", line 49, in copyfileobj\r\n    buf = fsrc.read(length)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 818, in read\r\n    buf += self.fileobj.read(size - len(buf))\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 736, in read\r\n    return self.readnormal(size)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 745, in readnormal\r\n    return self.fileobj.read(size)\r\n  File \"/usr/lib/python2.7/gzip.py\", line 261, in read\r\n    self._read(readsize)\r\n  File \"/usr/lib/python2.7/gzip.py\", line 308, in _read\r\n    self._read_eof()\r\n  File \"/usr/lib/python2.7/gzip.py\", line 347, in _read_eof\r\n    hex(self.crc)))\r\nIOError: CRC check failed 0x76f1f85e != 0x6caceac0L\r\n", "I fixed it by installing tensor flow on my Mac and running it through Virtual Environment. Also, I downloaded the inception file manually and pasted it in the tf_files folder manually.", "@Research2  \"I fixed it by installing tensor flow on my Mac and running it through Virtual Environment. Also, I downloaded the inception file manually and pasted it in the tf_files folder manually.\"\r\n\r\nWorks for me.\r\n", "So it appears this error is thrown when a previous copy of the inception v3 graph is downloaded and you try to run retrain.py (may or may not depend on whether the previous downloaded model is corrupted). \r\n\r\nSolution: `cd` into the `model_dir` folder  (where the model is downloaded) and delete the previously downloaded model + .zip file. My hunch is that Python isn't rewriting the old corrupted files and instead throwing and error if you rerun the retrainer.py (fyi the default directory is `/tmp/imagenet` if you didn't specify a specific `model_dir`).", "Deleting half complete download + restarting container worked for me", "retrained_graph.pb and retrained_labels.txt are not created when run \r\npython retrain.py \\\r\n  --bottleneck_dir=bottlenecks \\\r\n  --how_many_training_steps=500 \\\r\n  --model_dir=inception \\\r\n  --summaries_dir=training_summaries/basic \\\r\n  --output_graph=retrained_graph.pb \\\r\n  --output_labels=retrained_labels.txt \\\r\n  --image_dir=flower_photos\r\n\r\nbut it created train all flowers into bottlenecks folder .Please any idea\r\n", "@delphiy Did you get any solution? Even I'm stuck with that same error. \r\nretrained_graph.pb and retrained_labels.txt files have problem creating inside tf_files folder\r\nits says : \"tensorflow.python.framework.errors_impl.UnknownError: Failed to create a NewWriteableFile: C:/Program Files/Git/tf_files/retrained_graph.pb : Access is denied. ; Input/output error"]}, {"number": 5856, "title": "[Anaconda Installation Tutorial]As in issue #5832, pip installed TF s\u2026", "body": "\u2026hould be uninstalled if one want to use TF with Anaconda environment, because Anaconda will search system site-packages with higher priority, this might result in the latest upgrade of TF within Anaconda environment inaccessible.", "comments": ["Can one of the admins verify this patch?", "@zuoxingdong, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @martinwicke to be potential reviewers.", "@tensorflow-jenkins test this please"]}, {"number": 5855, "title": "[Tutorial->MNIST]`write_version=tf.train.SaverDef.V2` is already the default setting i\u2026", "body": "\u2026n latest TF version", "comments": ["Can one of the admins verify this patch?", "@zuoxingdong, thanks for your PR! By analyzing the history of the files in this pull request, we identified @zheng-xq, @vrv and @tensorflower-gardener to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@googlebot  I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "PR merged. Thanks, @zuoxingdong !"]}, {"number": 5854, "title": "std::system_error after tensor flow install", "body": "As suggested from stack overflow, i move this issue here.\r\n\r\nI installed tensorflow in a virtual environment via pip (Ubuntu/Linux 64-bit, CPU only, Python 2.7), and tried to run the test example:\r\n\r\n```\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\n```\r\n\r\nI get:\r\n\r\n```\r\nterminate called after throwing an instance of 'std::system_error'\r\nwhat():  Resource temporarily unavailable\r\nAborted\r\n```\r\n\r\nI've looked around for the common troubleshooting with no success. Any ideas where to start?", "comments": ["Please can you provide all of the information requested in the issue reporting template.", "Automatically closing due to lack of recent activity. When further information is posted we will reopen this. Thanks.", "Updating with requested info (sorry for the delay).\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nFound no threads matching the exact output of the problem, and troubleshooting with the tensor flow website was unsuccessful.\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nCentOS Linux release 7.2.1511 (Core)\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nN/A, we're using only CPUs for the moment. The tensor flow documentation suggests this is only an optional requirement?\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0rc0-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n0.11.0\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nSee above.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nNone.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\nSee above.\r\n\r\nAny useful ideas of where to start debugging would be useful. \r\n", "Hi. I was informed we can reopen the issue with the updates from above, but its still closed - can we re-open please?", "I'll just open a new issue.\r\n"]}, {"number": 5853, "title": "sparse_tensor_dense_matmul not working with float64 on gpu", "body": "tensorflow (I tried 0.11, 0.12 and master) claims sparse_tensor_dense_matmul be not supported on gpu, yet the docs\r\n\r\nhttps://www.tensorflow.org/versions/r0.11/api_docs/python/sparse_ops.html#sparse_tensor_dense_matmul\r\n\r\nand the fact that there is a source file\r\n\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc\r\n\r\nwhich apparently gets compiled\r\n\r\n.cache/bazel/_bazel_panzer/eb98e8482caa36ec89e479cdd4c996e7/execroot/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/libsparse_tensor_dense_matmul_op_gpu.pic.lo\r\n\r\nsuggest it should.\r\n\r\n############ example script\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nwith tf.device('/gpu:0'):\r\n    M = tf.SparseTensor(np.arange(4).reshape(2, 2), np.ones((2,)), (3, 3))\r\n    v = tf.constant(np.ones((3, 1)))\r\n    p = tf.sparse_tensor_dense_matmul(M, v)\r\n\r\n    init = tf.initialize_all_variables()\r\n    sess = tf.Session()\r\n    sess.run(init)\r\n    p = sess.run (p)\r\n```\r\n############ output\r\n```\r\npanzer:~$ python3 sparse_tensor_dense_matmul.py\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nWARNING:tensorflow:From sparse_tensor_dense_matmul.py:9 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.8095\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\nTraceback (most recent call last):\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\r\n    return fn(*args)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 999, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1048, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'SparseTensorDenseMatMul/SparseTensorDenseMatMul': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n         [[Node: SparseTensorDenseMatMul/SparseTensorDenseMatMul = SparseTensorDenseMatMul[T=DT_DOUBLE, adjoint_a=false, adjoint_b=false, _device=\"/device:GPU:0\"](SparseTensor/indices, SparseTensor/values, SparseTensor/shape, Const)]]\r\n```\r\nDuring handling of the above exception, another exception occurred:\r\n```\r\nTraceback (most recent call last):\r\n  File \"sparse_tensor_dense_matmul.py\", line 11, in <module>\r\n    sess.run(init)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'SparseTensorDenseMatMul/SparseTensorDenseMatMul': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n         [[Node: SparseTensorDenseMatMul/SparseTensorDenseMatMul = SparseTensorDenseMatMul[T=DT_DOUBLE, adjoint_a=false, adjoint_b=false, _device=\"/device:GPU:0\"](SparseTensor/indices, SparseTensor/values, SparseTensor/shape, Const)]]\r\n\r\nCaused by op 'SparseTensorDenseMatMul/SparseTensorDenseMatMul', defined at:\r\n  File \"sparse_tensor_dense_matmul.py\", line 7, in <module>\r\n    p = tf.sparse_tensor_dense_matmul(M, v)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/ops/sparse_ops.py\", line 1339, in sparse_tensor_dense_matmul\r\n    adjoint_b=adjoint_b)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_sparse_ops.py\", line 975, in _sparse_tensor_dense_mat_mul\r\n    adjoint_b=adjoint_b, name=name)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'SparseTensorDenseMatMul/SparseTensorDenseMatMul': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n         [[Node: SparseTensorDenseMatMul/SparseTensorDenseMatMul = SparseTensorDenseMatMul[T=DT_DOUBLE, adjoint_a=false, adjoint_b=false, _device=\"/device:GPU:0\"](SparseTensor/indices, SparseTensor/values, SparseTensor/shape, Const)]]\r\n```\r\n\r\n### Environment info\r\nOperating System:\r\nlinux (ubuntu 16.04)\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.5\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\ne5d44ffa3d78271e2619c577f8240e530538bb61\r\n\r\n2. The output of `bazel version`\r\nBuild label: 0.4.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n\r\n", "comments": ["I haven't had chance to check the OpKernel registration details, but you appear to be using int32 tensors as input data.  Most GPU ops are only registered for float types... \r\n\r\nCould you please see if this makes a difference?\r\n", "Thanks, prb12,  for the quick response.\r\n\r\nUnfortunately it wasn't that, see below. I also tried using float64 for the index and shape tensors but that didn't work either.\r\n\r\nCheers, Paul\r\n\r\n####### file:  sparse_tensor_dense_matmul_v2.py\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nwith tf.device('/gpu:0'):\r\n    M = tf.SparseTensor(np.arange(4, dtype=np.int64).reshape(2, 2),\r\n                        np.ones((2,), dtype=np.float64),\r\n                        np.array((4, 4), dtype=np.int64))\r\n    v = tf.constant(np.ones((4, 1), dtype=np.float64))\r\n    p = tf.sparse_tensor_dense_matmul(M, v)\r\n\r\n    init = tf.initialize_all_variables()\r\n    sess = tf.Session()\r\n    sess.run(init)\r\n    p = sess.run (p)\r\n\r\n\r\n############### output\r\n\r\npanzer:~$ python3 sparse_tensor_dense_matmul_v2.py \r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nWARNING:tensorflow:From sparse_tensor_dense_matmul_v2.py:11 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.8095\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\nTraceback (most recent call last):\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\r\n    return fn(*args)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 999, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1048, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'SparseTensorDenseMatMul/SparseTensorDenseMatMul': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n         [[Node: SparseTensorDenseMatMul/SparseTensorDenseMatMul = SparseTensorDenseMatMul[T=DT_DOUBLE, adjoint_a=false, adjoint_b=false, _device=\"/device:GPU:0\"](SparseTensor/indices, SparseTensor/values, SparseTensor/shape, Const)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"sparse_tensor_dense_matmul_v2.py\", line 13, in <module>\r\n    sess.run(init)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'SparseTensorDenseMatMul/SparseTensorDenseMatMul': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n         [[Node: SparseTensorDenseMatMul/SparseTensorDenseMatMul = SparseTensorDenseMatMul[T=DT_DOUBLE, adjoint_a=false, adjoint_b=false, _device=\"/device:GPU:0\"](SparseTensor/indices, SparseTensor/values, SparseTensor/shape, Const)]]\r\n\r\nCaused by op 'SparseTensorDenseMatMul/SparseTensorDenseMatMul', defined at:\r\n  File \"sparse_tensor_dense_matmul_v2.py\", line 9, in <module>\r\n    p = tf.sparse_tensor_dense_matmul(M, v)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/ops/sparse_ops.py\", line 1339, in sparse_tensor_dense_matmul\r\n    adjoint_b=adjoint_b)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_sparse_ops.py\", line 975, in _sparse_tensor_dense_mat_mul\r\n    adjoint_b=adjoint_b, name=name)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/panzer/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'SparseTensorDenseMatMul/SparseTensorDenseMatMul': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n         [[Node: SparseTensorDenseMatMul/SparseTensorDenseMatMul = SparseTensorDenseMatMul[T=DT_DOUBLE, adjoint_a=false, adjoint_b=false, _device=\"/device:GPU:0\"](SparseTensor/indices, SparseTensor/values, SparseTensor/shape, Const)]]\r\n\r\n", "@paul-the-noob : looks like this is because of the usage of float64? This works for me:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nwith tf.device('/gpu:0'):\r\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=False)) as sess:\r\n        M = tf.SparseTensor(np.arange(4, dtype=np.float32).reshape(2, 2),\r\n                            np.ones((2,), dtype=np.float32),\r\n                            np.array((4, 4), dtype=np.int32))\r\n        v = tf.constant(np.ones((4, 1), dtype=np.float32))\r\n        p = tf.sparse_tensor_dense_matmul(M, v)\r\n\r\n        init = tf.initialize_all_variables()\r\n        sess = tf.Session()\r\n        sess.run(init)\r\n        print sess.run(p)\r\n```\r\n\r\n\r\n```bash\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\nWARNING:tensorflow:From <ipython-input-3-ddf78d11c31f>:12 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\n[[ 1.]\r\n [ 0.]\r\n [ 1.]\r\n [ 0.]]\r\n```", "@jramapuram \r\nthanks, for that. Using 32bit floats for v and the second (\"values\") argument to the SparseTensor constructor also works for me. (The \"indices\" and \"shape\" arguments are more lenient; the docs specify them as int64, perhaps constructor converts them?)\r\n\r\n@concretevitamin \r\nI'm leaving this open, anyway. It would certainly be nice being able to use double precision: for example, the application I'm working on definitely requires it.\r\nBeing essentially ignorant about the nitty-gritty of gpu programming I don't know how much of an ask this is. But if you could make it work it would help me a lot and be very much appreciated.\r\n\r\nCheers, Paul\r\n", "I did a tiny bit of research and found the following:\r\n\r\nThe problem appears to lie with a CUDA function called atomicAdd, see [the relevant commit](https://github.com/tensorflow/tensorflow/commit/7824e4043ee5229a6872a54a2ba11caf865610bd).\r\n\r\nThe good news is that, as of CUDA 8, [atomicAdd supports double precision with hardware acceleration on the latest (Pascal) cards](http://stackoverflow.com/questions/12626096/why-has-atomicadd-not-been-implemented-for-doubles).\r\n\r\nSo I guess this is now a feature request. I had a look at the relevant code myself, but sadly, being C++ it is far outside my comfort zone.", "A related question: is it true that backpropagation involving sparse_tensor_dense_matmul is NOT supported on GPU in version r0.11? \r\n\r\nMore specifically, the following code failed to run on GPU:\r\n\r\n```\r\nwith tf.device(\"/gpu:0\"):\r\n    x = tf.sparse_placeholder(tf.float32)\r\n    v = tf.Variable(tf.random_normal([4, 1]))\r\n    cost = tf.reduce_sum(tf.sparse_tensor_dense_matmul(x, v))\r\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\r\n    \r\n   with tf.Session() as sess:\r\n        sess.run(tf.initialize_all_variables())\r\n        indices = np.array([[2, 0], [3, 1]], dtype=np.int64)\r\n        values = np.array([1.0, 2.0], dtype=np.float32)\r\n        shape = np.array([4, 4], dtype=np.int64)\r\n        sess.run(optimizer, feed_dict={x: tf.SparseTensorValue(indices, values, shape)})\r\n```\r\n\r\nwith error:\r\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\r\nI wanted to know if it's because I'm not using `tf.sparse_tensor_dense_matmul` correctly in backpropagation, or it's simply because not supported. \r\n\r\nThanks a lot in advance. \r\n", "If I understand correctly [sparse_tensor_dense_matmul_op](https://github.com/tensorflow/tensorflow/blob/ce02c770fb269e5e607da459bde4f580ef108137/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc#L221) still only registers a float kernel on GPU.\r\n\r\nBut more generally it looks like these \"float64 kernels not implemented\" bugs are being collected in #1140.\r\n\r\n"]}, {"number": 5852, "title": "tf.contrib.metrics.accuracy doesn't have expected weights behavior", "body": "My assumption here is that 'weights' is intended to allow us to indicate that certain indices should be ignored entirely.\r\n\r\nBelow code generates accuracy of .67, which seems wrong.  There are no weights-related unit tests for accuracy(), so I can't verify that this is unintended, but it seems inconsistent with the rest of the tf code base.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\npreds=tf.Variable([1,1,1])\r\nlabels=tf.Variable([1,1,0])\r\nweights=tf.Variable([1,1,0])\r\n\r\nacc=tf.contrib.metrics.accuracy(preds, labels, tf.to_float(weights))\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.initialize_all_variables())\r\n\r\n    acc=sess.run(acc)\r\n    print(acc)\r\n```", "comments": ["...just noticed r0.12 in github.  Looks like this was fixed there.  At least this issue will stand as a warning to anyone else later."]}, {"number": 5851, "title": "Fix typo in tutorial", "body": "", "comments": ["@tweksteen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @zheng-xq and @benoitsteiner to be potential reviewers.", "Can one of the admins verify this patch?"]}, {"number": 5850, "title": "adding zero-sized layer costs huge amount of memory, but without increasing total # of trainable parameters", "body": "In order to make code generic, my code does append a layer with a variable-sized layer which might be of length 0, i.e.\r\n`\r\ntv_concat = tf.concat(1, [tf.reshape(tf.slice(X,[0,time_step,0],[-1,Ncontext,-1]), [tp_current_batch_size,embed_size*Ncontext]), tv_hh])`\r\n\r\nIn the above code, X is a tensor of shape [batch_size, max_time_step, state_size], tv_hh of shape [batch_size, state_size]. Depending on the value of Ncontext, a differently sized layer will be concatenated with the existing layer tv_hh. So when Ncontext=0, a zero-size layer will be concatenated so that tv_concat will be the same as tv_hh. I have explicitly checked and confirmed that tv_concat is indeed of the same shape as tv_hh. I have also checked that the total number of trainable parameters are the same.\r\n\r\nHowever, when Ncontext=0, the graphics memory consumption is >3G, if I use tv_concat=tv_hh, the graphics memory consumption is only 2G.\r\n\r\nSo is this behaviour expected?", "comments": ["Please can you provide all of the information requested in the issues reporting template.", "1. Operating System:\r\nUbuntu 14.04.5 LTS\r\n\r\n2. Installed version of CUDA and cuDNN:\r\n/usr/local/cuda-8.0\r\ncudnn-8.0-linux-x64-v5.1.tgz\r\n\r\n3. It is installed from binary pip package\r\n\r\n4. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n\r\n5. The output from python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nxuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n0.11.0\r\n\r\n", "Backprop through multiple tf.slice is memory expensive . Try to rearrange\nyour code to do one tf.split upfront before any per-timestep calculation.\n\nOn Fri, Nov 25, 2016, 6:56 PM xuancong84 <notifications@github.com> wrote:\n\n>\n>    1.\n>\n>    Operating System:\n>    Ubuntu 14.04.5 LTS\n>    2.\n>\n>    Installed version of CUDA and cuDNN:\n>    /usr/local/cuda-8.0\n>    cudnn-8.0-linux-x64-v5.1.tgz\n>    3.\n>\n>    It is installed from binary pip package\n>    4.\n>\n>    A link to the pip package you installed:\n>\n>    https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\n>    5.\n>\n>    The output from python -c \"import tensorflow; print(tensorflow.\n>    *version*)\"\n>    xuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow;\n>    print(tensorflow.*version*)\"\n>    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened\n>    CUDA library libcublas.so locally\n>    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened\n>    CUDA library libcudnn.so locally\n>    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened\n>    CUDA library libcufft.so locally\n>    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened\n>    CUDA library libcuda.so.1 locally\n>    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened\n>    CUDA library libcurand.so locally\n>    0.11.0\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5850#issuecomment-263040595>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHk2brhwRvp1-qbzNpNaYOiLbvDB2_Kkks5rB5_ngaJpZM4K8MF9>\n> .\n>\n", "@zffchen78 I am aware of that backprop through multiple tf.slice is memory expensive. However, slicing out a zero-sized layer should not cost that much memory I presume.", "Does rearranging the code as suggested avoid the high memory cost?", "@michaelisard It is not possible to rearrange the code as suggested because Ncontext is an integer (>=0) parameter to be tuned. Take a closer look at my code, you will find out why.", "@sherrym would you reassign to someone who can take a look?", "@xuancong84  btw, it could help troubleshooting if you dumped memory\ntimeline and checked which op is responsible for this extra 1GB allocation,\nusing timeline as here:\nhttps://github.com/tensorflow/tensorflow/issues/6019#issuecomment-264544202\n\nOn Wed, Dec 7, 2016 at 12:45 PM, Michael Isard <notifications@github.com>\nwrote:\n\n> @sherrym <https://github.com/sherrym> would you reassign to someone who\n> can take a look?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5850#issuecomment-265568106>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHH_M_niMSTyjFS4z4GnrEgME-OSwks5rFxrvgaJpZM4K8MF9>\n> .\n>\n", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 5849, "title": "how to generate tfrecord files in pure python without tensorflow?", "body": "Is there a way to convert csv file to tfrecord  in python without tensorflow?\r\nI want to convert large data , but my hadoop cluster cannot run tensorflow because of low version of glibc ", "comments": ["From https://www.tensorflow.org/versions/r0.11/resources/index.html\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n"]}, {"number": 5848, "title": "/usr/local/computecpp not found when install tensorflow.", "body": "when run tensorflow configure, it shows /usr/local/computecpp not found when install tensorflow.\r\nI followed the instrcution, But i can not find how to install computecpp. plz help.", "comments": ["Hi @michaeldengchangshun \r\ncomputecpp is not part of TensorFlow package / codebase. You need to install it manually https://www.codeplay.com/products/computesuite/computecpp. Then, during configuration step point tensorflow to the installation directory. \r\nHope that helps", "@michaeldengchangshun If you don't have a good reason to build from source then we would recommend using one of the binary install packages. \r\nClosing this issue since isn't really a bug.", "Building tensorflow only requires computecpp if you said yes to using OpenCL. For a simple installation, select no to all the additional options.", "@prb12 We would use the binary install packages but these instructions: http://www.nvidia.com/object/gpu-accelerated-applications-tensorflow-installation.html suggest otherwise\r\n(these instructions also suggest to build an older version of tensorflow, they are not flexible enough)\r\nAre there better instruction somewhere else?", "No support for Computecpp on Mac OS currently, so spare yourself the trouble of creating an account to download from its website. Just say N to OpenCL."]}, {"number": 5847, "title": "the embedding_lookup() returns zeros when the index exceed embedding matrix size?", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nI wrote an issues a month ago about the same problem #5260 \r\nI recently update the TF to latest master branch(`0.11.head`) without any custom modification and the commit hash is 55dbc54192a378c5e685c52595f42503f037320e\r\n\r\nThe tf.embedding_lookup() still do not raise error even though the index is exceeding the embedding matrix size. It automatically fills zeros as you can see in the below example. \r\n\r\nI tested same code with two different machines and the result was same.\r\nIn the previous issue(#5260), @strategist333 said it raise an `InvalidArgumentError` in binary release.\r\nBut when I tested TF installed from source, it didn't...\r\n \r\n\r\n```\r\n import tensorflow as tf\r\n import numpy as np\r\n\r\n embd_mat = np.linspace(1,10,10).reshape([10,1])*np.array([1,2,3]).reshape([1,3])\r\n idx = np.linspace(0,19,20)\r\n\r\n embd_in = tf.placeholder(tf.float32,[10,3])\r\n idx_in = tf.placeholder(tf.int32,[20])\r\n\r\n output = tf.nn.embedding_lookup(embd_in,idx_in)\r\n\r\n with tf.Session() as sess:\r\n     sess.run(tf.initialize_all_variables())\r\n\r\n     embd_out = sess.run(output,feed_dict={embd_in:embd_mat, idx_in:idx})\r\n\r\n     print embd_out\r\n```\r\n\r\nThe output of above code is,\r\n\r\n```\r\n[[  1.   2.   3.]\r\n [  2.   4.   6.]\r\n [  3.   6.   9.]\r\n [  4.   8.  12.]\r\n [  5.  10.  15.]\r\n [  6.  12.  18.]\r\n [  7.  14.  21.]\r\n [  8.  16.  24.]\r\n [  9.  18.  27.]\r\n [ 10.  20.  30.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]]\r\n```", "comments": ["BTW, the binary release 0.11rc0 (Linux CPU version) does raise an error:\r\n```\r\nInvalidArgumentError (see above for traceback): indices[10] = 10 is not in [0, 10)\r\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Placeholder\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Placeholder_0, _recv_Placeholder_1_0)]]\r\n```", "Thanks for the answer. But it's weird that it does not raise an error in source version... By the way, I'm using GPU version.", "Oh, that makes a difference. When using binary 0.11rc0 Linux *GPU* version, no error is raised and the same output as yours is returned:\r\n```\r\n[[  1.   2.   3.]\r\n [  2.   4.   6.]\r\n [  3.   6.   9.]\r\n [  4.   8.  12.]\r\n [  5.  10.  15.]\r\n [  6.  12.  18.]\r\n [  7.  14.  21.]\r\n [  8.  16.  24.]\r\n [  9.  18.  27.]\r\n [ 10.  20.  30.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]\r\n [  0.   0.   0.]]\r\n```\r\nI have Cuda 7.5, CuDNN 5 and Tesla K40c GPU.", "@foxik thanks for the testing. It seems it happens only on GPU version. I tested on two machines which are (Cuda 7.0, cuDNN4, Titan black) and (Cuda 8.0, cuDNN5, 1080). I think it should be fixed...", "This is most likely due to differences between the CPU and GPU OpKernels for the embedding lookup, and your code doesn't constrain where the variables are placed, so they are possibly on CPU in one case and on GPU in the other.\r\n\r\nYou could validate this by adding `log_device_placement=True` to your session ConfigProto, or explicitly constrain the device placement of the variables.\r\n", "@prb12 I changed the code as below as you suggested,\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    \r\n    config = tf.ConfigProto(log_device_placement=True)\r\n    \r\n    with tf.device('/gpu:0'):\r\n        embd_mat = np.linspace(1,10,10).reshape([10,1])*np.array([1,2,3]).reshape([1,3])\r\n        idx = np.linspace(0,19,20)\r\n        \r\n        embd_in = tf.placeholder(tf.float32,[10,3])\r\n        idx_in = tf.placeholder(tf.int32,[20])\r\n    \r\n        output = tf.nn.embedding_lookup(embd_in,idx_in)\r\n    \r\n    with tf.Session(config=config) as sess:\r\n        sess.run(tf.initialize_all_variables())\r\n    \r\n        embd_out = sess.run(output,feed_dict={embd_in:embd_mat, idx_in:idx})\r\n    \r\n        print embd_out \r\n\r\nand the output is,\r\n\r\n    init: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\n    I tensorflow/core/common_runtime/simple_placer.cc:827] init: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\n    embedding_lookup: (Gather): /job:localhost/replica:0/task:0/gpu:0\r\n    I tensorflow/core/common_runtime/simple_placer.cc:827] embedding_lookup: (Gather)/job:localhost/replica:0/task:0/gpu:0\r\n    Placeholder_1: (Placeholder): /job:localhost/replica:0/task:0/gpu:0\r\n    I tensorflow/core/common_runtime/simple_placer.cc:827] Placeholder_1: (Placeholder)/job:localhost/replica:0/task:0/gpu:0\r\n    Placeholder: (Placeholder): /job:localhost/replica:0/task:0/gpu:0\r\n    I tensorflow/core/common_runtime/simple_placer.cc:827] Placeholder: (Placeholder)/job:localhost/replica:0/task:0/gpu:0\r\n    [[  1.   2.   3.]\r\n     [  2.   4.   6.]\r\n     [  3.   6.   9.]\r\n     [  4.   8.  12.]\r\n     [  5.  10.  15.]\r\n     [  6.  12.  18.]\r\n     [  7.  14.  21.]\r\n     [  8.  16.  24.]\r\n     [  9.  18.  27.]\r\n     [ 10.  20.  30.]\r\n     [  0.   0.   0.]\r\n     [  0.   0.   0.]\r\n     [  0.   0.   0.]\r\n     [  0.   0.   0.]\r\n     [  0.   0.   0.]\r\n     [  0.   0.   0.]\r\n     [  0.   0.   0.]\r\n     [  0.   0.   0.]\r\n     [  0.   0.   0.]\r\n     [  0.   0.   0.]]\r\n    ", "What happens when you constrain to /cpu:0 in the with clause?  Do you not get an error exception?  As prb12@ commented, there are known subtle semantic differences between the CPU and GPU versions of some kernels.  When they involve error handling, like in this case, it's because the GPU kernels are launched asynchronously and there doesn't seem to be a good way of replicating the CPU kernel error handling without compromising performance.  As a matter of software engineering practice for code quality, it can be useful to test your models on CPU only to look for this kind of issue.\r\n", "BTW, see https://github.com/tensorflow/tensorflow/blob/05ce371d64ce0c427f7706f0f58a9260a5a0470b/tensorflow/core/kernels/gather_functor_gpu.cu.h#L75-L77 which explains the checking is disabled in the GPU kernel. In case the indices are out of range, zeros are used: https://github.com/tensorflow/tensorflow/blob/05ce371d64ce0c427f7706f0f58a9260a5a0470b/tensorflow/core/kernels/gather_functor_gpu.cu.h#L42-L44. The links are to the latest version of the file.", "@poxvoculi yes, I got an error when it is changed to CPU. I don't know much about CPU and GPU difference, but it seems that I need to test the model first on CPU before running on the GPU.\r\n@foxik thanks! it seems because of the performance issue as explained above.", "It is hard to send errors back from the GPU, thus we probably don't perform this check in the GPU implementation.  One solution might be to have the GPU version fill in NaNs instead of 0s; so you'll get failures earlier; and the offending kernel (embedding lookup) will be identified by adding finiteness checks.", "I encountered the same issue. And if I change this line `embd_in = tf.placeholder(tf.float32, [10,3])`  to `embd_in = tf.placeholder(tf.int32, [10,3])` and run the above code, I got InvalidArgumentError as expected. \r\nCannot figure out why is this related to dtype?\r\nAlso, the `tf.gather()` behaves the same as `tf.embedding_lookup()`", "Have you tried placing everything on CPU?  Do you now consistently get the\nerror?\n\nProblem is probably caused by us not being able to efficiently perform\nerror checking on GPU kernels.\n\nOn Wed, Jan 10, 2018 at 5:27 PM, jkyu <notifications@github.com> wrote:\n\n> I encountered the same issue. And if I change this line embd_in =\n> tf.placeholder(tf.float32,[10,3]) to embd_in =\n> tf.placeholder(tf.int32,10,3]) and run the above code, I got\n> InvalidArgumentError as expected.\n> Cannot figure out why is this related to dtype?\n> Also, the tf.gather() behaves the same as tf.embedding_lookup()\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5847#issuecomment-356794338>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0zqwvFbace_9L7ZJLge4XRr8Q0Nks5tJWNqgaJpZM4K8KZv>\n> .\n>\n"]}, {"number": 5846, "title": "Fix an error in 'Adding a New Op' example code", "body": "There is an error in the 'Adding a New Op' example code. By chance, the test code failed to catch it because only 1-D tensor is tested.", "comments": ["Can one of the admins verify this patch?", "@llhe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @tensorflower-gardener to be potential reviewers.", "@tensorflow-jenkins test this please"]}, {"number": 5845, "title": "How to speed up\uff1f why so so slower than caffe", "body": "I use tensoflow r11+Titan x+cuda8+cudnnv5\r\nbut the speed of training is much slower than caffe.  It can run about 5W epochs with caffe in a night, but only 350 epochs with tensorflow, I don't know why the gap of speed is so big.\r\nIf anywhere need I to change? I follow the cifar10 example to suit my experiment\r\n\r\nthanks~", "comments": ["Please can you at least provide the information requested in the issue reporting template.\r\nThis sort of vague question is impossible to answer without detailed repro instructions.\r\n\r\n> I follow the cifar10 example to suit my experiment\r\nAre you comparing two different cifar10 implementations on different frameworks, or is this a program you wrote yourself? \r\n\r\nAlso, please note that many equivalently named benchmarks tend to do very different amounts of work - e.g. the amount of data preprocessing (image distortions, cropping, scaling) seems to vary massively between various \"inception\" benchmarks I've looked at.  For example, MXnet samples usually work from a pre-processed image dataset whereas TF examples work from the un-preprocessed imagenet data.\r\n  \r\nIf you believe there is a specific bug/issue then it would be useful to provide some supporting data - e.g. a Timeline.\r\n\r\nFailing that, I would suggest that this sort of discussion is best suited to StackOverflow:\r\n>For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n \r\n\r\n  ", "I also have this issue. Tensorflow r1.0 and Caffe both use cudnn 5.1 as the backend on the same server. Although the speed of a small ConvNet on cifar10 is similar, when I train AlexNet, the speed of TF is >2x slower. Weird!"]}]