[{"number": 33408, "title": "Time Series Forecasting with LSTM and different/mixed frequencies", "body": "I could not find anything in the docs about how to handle different frequencies of time series. I have a Dataset A with monthly data that i want to use to predict the values from Dataset B that contains quarterly based data. So the target value e.g. quarter 1 is based on the values from month 1-3.\r\n\r\nDataset A (Features):\r\n\r\n| Month  | Value1 | Value2 | Value3 |\r\n| ------------- | ------------- | ------------- | ------------- |\r\n| 1  | 91  | 72 | 23 |\r\n| 2  | 93  | 68 | 33 |\r\n| 3  | 92  | 57 | 27 |\r\n| 4  | 91  | 72 | 23 |\r\n| 5  | 93  | 68 | 33 |\r\n| 6  | 92  | 57 | 27 |\r\n\r\nDataset B (Target):\r\n\r\n| Quarter   | Target_Value |\r\n| ------------- | ------------- |\r\n| 1  | 51  |\r\n| 2  | 57  |", "comments": ["@janwendt ,\r\nHi,This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nYou can also refer this link related to [LSTM time series with mixed frequency data](https://stats.stackexchange.com/questions/210930/lstm-time-series-with-mixed-frequency-data) and also [article](http://roseyu.com/time-series-workshop/submissions/TSW2017_paper_10.pdf).Thanks!", "@janwendt ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 33407, "title": "Eager mode not being disabled tf 1.14.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nTrying out example from Tensorflow Probability with the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\nimport numpy as np\r\n\r\ntfd = tfp.distributions\r\ndtype = np.float32\r\n\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n  # Set up random seed for the optimizer\r\n  tf.set_random_seed(42)\r\n  true_mean = dtype([0, 0, 0])\r\n  true_cov = dtype([[1, 0.25, 0.25], [0.25, 1, 0.25], [0.25, 0.25, 1]])\r\n  # Loss is defined through the Cholesky decomposition\r\n  chol = tf.linalg.cholesky(true_cov)\r\n\r\n  var_1 = tf.Variable(name='var_1', initial_value=[1., 1.])\r\n  var_2 = tf.Variable(name='var_2', initial_value=[1.])\r\n\r\n  def loss_fn():\r\n    var = tf.concat([var_1, var_2], axis=-1)\r\n    loss_part = tf.linalg.cholesky_solve(chol, tf.expand_dims(var, -1))\r\n    return tf.linalg.matvec(loss_part, var, transpose_a=True)\r\n\r\n  # Set up the learning rate with a polynomial decay\r\n  step = tf.Variable(0, dtype=tf.int64)\r\n  starter_learning_rate = .3\r\n  end_learning_rate = 1e-4\r\n  decay_steps = 1e4\r\n  learning_rate = tf.compat.v1.train.polynomial_decay(\r\n      starter_learning_rate,\r\n      step,\r\n      decay_steps,\r\n      end_learning_rate,\r\n      power=1.)\r\n\r\n  # Set up the optimizer\r\n  optimizer_kernel = tfp.optimizer.StochasticGradientLangevinDynamics(\r\n      learning_rate=learning_rate, preconditioner_decay_rate=0.99)\r\n  optimizer_kernel.iterations = step\r\n  optimizer = optimizer_kernel.minimize(loss_fn, var_list=[var_1, var_2])\r\n\r\n  # Number of training steps\r\n  training_steps = 5000\r\n  # Record the steps as and treat them as samples\r\n  samples = [np.zeros([training_steps, 2]), np.zeros([training_steps, 1])]\r\n  sess.run(tf.compat.v1.global_variables_initializer())\r\n  for step in range(training_steps):\r\n    sess.run(optimizer)\r\n    sample = [sess.run(var_1), sess.run(var_2)]\r\n    samples[0][step, :] = sample[0]\r\n    samples[1][step, :] = sample[1]\r\n\r\n  samples_ = np.concatenate(samples, axis=-1)\r\n  sample_mean = np.mean(samples_, 0)\r\n  print('sample mean', sample_mean)\r\n\r\n```\r\nGetting following error:\r\n\r\n```\r\nFile \"/Users/shashank.gupta/miniconda2/lib/python2.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 318, in minimize\r\n    loss, var_list=var_list, grad_loss=grad_loss)\r\n  File \"/Users/shashank.gupta/miniconda2/lib/python2.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 351, in _compute_gradients\r\n    tape.watch(var_list)\r\n  File \"/Users/shashank.gupta/miniconda2/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 816, in watch\r\n    for t in nest.flatten(tensor):\r\nAttributeError: 'RefVariable' object has no attribute '_id'\r\n\r\n```\r\nLooks like eager mode is enabled. Have added a command to disable it still it's getting activated.", "comments": ["Issue replicating for TF-1.14, please find the [gist](https://colab.sandbox.google.com/gist/oanush/3bb01b691f398c4e0763ee69609b00bc/33407.ipynb) of colab.", "@shashankg7,\r\nThe problem might not be because of Eager Execution because, when I executed the command, `tf.executing_eagerly()`, it resulted in `False`. So, the problem might be something else. \r\n\r\nAre you just executing One of the Examples of Tensorflow Probability. If so,can you please let us know which example are you trying out from Tensorflow Probability, so that we can dig deeper, and if needed, change the Example as well, accordingly. Thanks!", "@rmothukuru \r\n\r\nI am trying an example from here: https://www.tensorflow.org/probability/api_docs/python/tfp/optimizer/StochasticGradientLangevinDynamics", "@shashankg7 Thanks for filing this issue. I was able to reproduce even with `TF1.15`. Error is coming from this line in the code.\r\n`optimizer = optimizer_kernel.minimize(loss_fn, var_list=[var_1, var_2])`\r\nI am not familiar with `tfp`. May be it is better to raise an issue in the `tfp` repo. Thanks!\r\n", "@shashankg7 I think this is a stale issue. Please note that there are no upgrades to TF1.x unless there is any security related issue. So, please use TF2.x. I have used `TF2.3` and I was able to run the code without any issue. Also, note that I did  update your code in order to run it in TF2.3. Thanks!\r\n\r\nPlease take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/3f8a43e8a76d7d3fa66ecf3478891c5a/33407.ipynb).\r\n\r\nPlease verify once and close the issue. Thanks !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33407\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33407\">No</a>\n"]}, {"number": 33406, "title": "Fix issue with SparseCategoricalCrossentropy: 'list' object has no attribute 'op'", "body": "This fix tries to address the issue raised in #33394 where usage of SparseCategoricalCrossentropy causes the issue:\r\n```\r\nAttributeError: 'list' object has no attribute 'op'\r\n```\r\nwhen values are passed directly (not calling ops.convert_to_tensor first).\r\n\r\nThis fix fixes the issue by performing the ops.convert_to_tensor\r\non y_true and y_pred, similiar to what are done in other losses.\r\n\r\nThis fix fixes #33394.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 33405, "title": "[INTEL MKL] Reuse input tensor in mkl conv2d", "body": "When reorder is not needed, The \"allocate\" operation in L1005 and \"copy\" operation in L1016 could be replaced by \"forward\" operation in L1010.\r\nThis PR improves mkl resnet50 fp32 inference performance about 10% along with #33404.", "comments": ["There seems to be a merge conflict. Could you please help resolve the conflict?"]}, {"number": 33404, "title": "[INTEL MKL] Reimplement CompareMklDnnLayouts", "body": "The implementation of `CompareMklDnnLayouts` in mkl_util.h was not correct. The code compared `mkldnn_memory_desc_t` by bytes and didn't exclude the garbage values, which causing the comparison always failing thus unnecessary reorders were executed. We reimplement the function by comparing class members one by one.", "comments": ["> Thank you for the PR! Just curious, why does the struct has garbage in the first place?\r\n\r\nFor example, `dims` field in `mkldnn_memory_desc_t` is a C array with size 12. Most of the time, only the first four values are used and the rest are uninitialized. So we couldn't compare it by bytes.\r\n", "Any updates?", "> > Thank you for the PR! Just curious, why does the struct has garbage in the first place?\r\n> \r\n> For example, `dims` field in `mkldnn_memory_desc_t` is a C array with size 12. Most of the time, only the first four values are used and the rest are uninitialized. So we couldn't compare it by bytes.\r\n\r\nThanks for the explanation as well! :)"]}, {"number": 33403, "title": "max_pool_with_argmax can gives different results on GPU than on CPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GPU 1080-Ti\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running the test in the code below, one of the tests fail. Somehow, the argmax result of max_pool_with_argmax is different when running with self.session(use_gpu=True).\r\nInterestingly, the failing test behaves as if it was ignoring the batch index in the argmax.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nAll 3 tests should pass.\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    \r\n    \r\n    class TestMaxPool(tf.test.TestCase):\r\n\r\n        def setUp(self):\r\n            tf.reset_default_graph()\r\n\r\n        def _test(self, sess):\r\n             # Image with 2 channels, H = 4, W = 4\r\n             image_t = tf.constant([[[1.0, 12.0], [3.0, 14.0], [5.0, 16.0], [7.0, 18.0]],\r\n                               [[11.0, 2.0], [13.0, 4.0], [15.0, 6.0], [17.0, 8.0]],\r\n                               [[38.0, 37.0], [36.0, 35.0], [34.0, 33.0], [32.0, 31.0]],\r\n                               [[28.0, 27.0], [26.0, 25.0], [24.0, 23.0], [22.0, 21.0]]])\r\n             input_t = tf.stack([image_t, image_t])\r\n             _, pool_indices = tf.nn.max_pool_with_argmax(input_t, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n             pool_indices_out = sess.run(pool_indices)\r\n\r\n            expected_image_indices = np.array([[[10, 3], [14, 7]], [[16, 17], [20, 21]]])\r\n            self.assertAllEqual(pool_indices_out, np.stack([expected_image_indices, expected_image_indices + 32]))\r\n\r\n        def test_with_normal_session(self):\r\n            with tf.Session() as sess:\r\n                self._test(sess)\r\n\r\n        def test_with_self_session_and_gpu(self):\r\n            with self.session(use_gpu=True) as sess:\r\n                self._test(sess)\r\n\r\n        def test_with_self_session_and_cpu(self):\r\n            with self.session(use_gpu=False) as sess:\r\n                self._test(sess)\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@jnd77, Thanks for reporting the issue.\r\nPlease provide the complete code to reproduce the reported issue. Thanks!", "@gadagashwini Thanks for taking a look. The complete code can be found in my original message. Only 2 out of 3 tests are passing ...", "@jnd77, I tried replicating the reported issue. Please take a look at the [gist](https://colab.sandbox.google.com/gist/gadagashwini/696ae88444b31f485f5a8a97c723bfb5/untitled210.ipynb). And provide the more information to reproduce the issue. Thanks!", "@gadagashwini . Here is the [gist ](https://colab.research.google.com/gist/jnd77/e7193cb2eb57c23a35e460c6538751ff/untitled210.ipynb) reproducing the issue\r\n\r\nIt looks like the argmax returned by the CPU op follows the documentation and is equal to:\r\n`((b * height + y) * width + x) * channels + c`\r\n\r\nWhile the one returned by the GPU op ignores the batch:\r\n\r\n`(y * width + x) * channels + c`\r\n\r\nAm very unclear why test_with_normal_session is passing, as I thought it would be sent to the GPU.", "@jnd77 Is this still an issue? Can you please try with `TF1.15` and let us know whether the issue persists. If possible, test it in `tf-nightly` or `TF2.1`. Thanks!", "@jvishnuvardhan I did a check with TF1.15.\r\nThe function has this new argument: include_batch_in_index\r\nBoth CPU and GPU versions give the same (correct) result, whether include_batch_in_index is True or False. And documentation has been updated as well.\r\n\r\nAll good on my side ! :) Thanks a lot.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33403\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33403\">No</a>\n"]}, {"number": 33402, "title": "Please add TPU kernels for tf.linalg.[[s]log]det", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.15\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAdding determinant kernels (along with gradients) will allow us to train Normalizing Flow models on TPUs.\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["I want to support for Tensorflow == 2.2.0, too.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "AFAIK, this feature request has not been implemented.", "Nice."]}, {"number": 33401, "title": "Uniform APIs for Tensor etc regardless of graph mode or runtime mode", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14+\r\n- Are you willing to contribute it (Yes/No): N/A\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis feature request is related to the issue \"Tensor object has no attribute numpy\" (maybe other issues as well) due to the fact Tensorflow implementation distinguishes graph mode (using evel()) and runtime mode (using numpy()) at public API level. However, such API difference is not desirable. From developer's perspective, a good design would be uniform APIs (e.g. value()) that hides away internal different treatment between graph time and runtime. So, what a developer wants is to have a UNIFORM mechanism/APIs for setting up business logic regardless of the modes even though actual data may not be known yet during building model (graph mode). This has been the case at macro level for example tf.keras modelling. It needs to extend to micro level APIs/classes like Tensor. (Particularly, since eager execution is default from v2.0.0+)\r\n\r\n\r\n**Will this change the current api? How?**\r\nIdeally expect a uniform API like value/data. A compromise is just to implement numpy for graph mode as well to avoid break existing code.\r\n\r\n\r\n**Who will benefit with this feature?**\r\nDevelopers who use Tensorflow.\r\n\r\n \r\n**Any Other info.**\r\nN/A", "comments": ["Did a a bit more experiment. Realize that working on tensor itself is just like working on variable. No need to calling numpy()/eval(). And find Tensor having computational operators defined. So, it was my misperception of how tensor works. The feature is solved."]}, {"number": 33400, "title": "tensor object has no attribute numpy ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@chris-opendata \r\n\r\nCan you please fill the issue template.Can you please elaborate about the issue & the context.Will it be possible to provide simple standalone code, then it is easy for localizing the issue faster. Thanks!\r\n", "System information\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n    TensorFlow installed from (source or binary): binary\r\n    TensorFlow version (use command below): tensorflow-gpu  1.14.0\r\n    Python version: 3.6.8\r\n    Bazel version (if compiling from source): N/A\r\n    GCC/Compiler version (if compiling from source): 8.3.0\r\n    CUDA/cuDNN version: 10.1\r\n    GPU model and memory: GeForce RTX 2070 / 7949MiB\r\n    Exact command to reproduce: as follows\r\n\r\nThis is more or less an improvement request related to Tensor. Tensor has implemented a set of arithmetic operators to behave closely like a native type or numpy type. However, it may be necessary to have an implicit conversion operator to the underlying type and structure for a few reasons:\r\n1. to hide away eval and numpy difference at API level between graph mode and runtime mode when tensorflow paradigm has moved to the eager execution.\r\n2. to work around the other issues easier. One of example in my case is that RaggedTensor loses shape information (after combined_non_max_suppression and ragged.boolean_mask) and its shape information is necessary for some other Tensorflow operations. The workaround is to recover such shape information from row_lengths API and by set_shape API. This is what I try to do in my Yolo application for batch process:\r\n```\r\n        # boxes is a RaggedTensor\r\n        n_batched_boxes = boxes.row_lengths()\r\n        batch_size = len(n_batched_boxes)\r\n        for i in range(batch_size):\r\n            n = n_batched_boxes[i] # For each in the batch\r\n            box = boxes[i,...] # box is a tensor and its shape information partially lost\r\n            box.set_shape([n, box.shape[-1]]) # Recover first dim shape\r\n            process(box)\r\n```\r\nHowever, because n is a tensor, box.set_shape will fail on int() cast type error because of lack of implicit conversion operator of the tensor. I can neither use n.numpy() because the code has to be parsed in graph mode nor n.eval(K.get_session()) because it raises error as (ValueError: Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`). Even if eval can be worked around, it would still be messy and purpose-defeat for eager execution. So, I think having Tensor a transparent type conversion may be warranted. The challenge is when the exact data not known at graph mode but the shape may be deterministic and can be supplied by the user. It is a thought for improvement. Meanwhile, I will appreciate if you can suggest me a solution for my case above.\r\n\r\nIn addition, the tensor shape inference is a bit of problematic in the current Tensorflow implementation as one example in my case above. So, this is something that Tensorflow team may also need to look into it because it is important in order for a developer to write consistent source code efficiently.", "Some similar issues #33401, #33046, #29463, #27519", "@chris-opendata \r\nCan you please go through @mihaimaruseac  work around and see if it helps you.Thanks!", "> @chris-opendata\r\n> Can you please go through @mihaimaruseac work around and see if it helps you.Thanks!\r\n\r\nThanks. I managed to work around the problem. This is more or less a feature request for uniform api for both graph mode and execution mode; improvement on shape inference.", "Hi @chris-opendata \r\nI am facing similar problem but I want to send an extra list along with batch(containing ground truth and image data) while training. when parsing graph it takes unknown shape and I can't even extract values from it.\r\nI can't use another session for eval. Since eager execution is false I can't use numpy() as well.\r\nCould you please share your workaround or can help me in resolving the issue?\r\nThanks\r\n"]}, {"number": 33399, "title": "keras.metrics.MeanSquaredError update_state, sample_weight not working as expected", "body": "**System information**\r\n- Have I written custom code: **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **pip**\r\n- TensorFlow version (use command below): **Version: 2.0.0**\r\n- Python version: **Python 3.6.8**\r\n- Bazel version (if compiling from source):**N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **CUDA Version: 10.2**\r\n- GPU model and memory: **GeForce GTX 1660 Ti, 6GB**\r\n\r\n**Describe the current behavior**\r\nRunning `update_state` on `tf.keras.metrics.MeanSquaredError` ,raises exception when passing in `sample_weight` even if sample_weight and y_true shape are the same.\r\n\r\nWeighting in other metrics works fine so not sure if this is a user error, in which case the docs could be updated, or an acutal bug. \r\nI haven't had time to look into this deeper but would be open too applying fixes if someone could point me in the right direction, thanks.\r\n\r\n```python\r\n\r\nm = tf.keras.metrics.MeanSquaredError()\r\nm.update_state([0., 0., 1., 1.], [1., 1., 1., 0.], sample_weight=[1, 1, 1, 0])\r\n\r\n```\r\nError:\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got 4 [Op:Squeeze]`\r\n\r\n**Describe the expected behavior**\r\nShould work the same as all other weighted metrics.\r\n\r\neg:\r\n```python\r\nm = tf.keras.metrics.Mean()\r\nm.update_state([1, 3, 5, 7], sample_weight=[1,1,1,0])\r\n\r\n# output: <tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=3.0>\r\n\r\n```\r\n\r\n**Code to reproduce the issue**\r\nSee above\r\n\r\n**Other info / logs**\r\n```\r\nm = tf.keras.metrics.MeanSquaredError()\r\nm.update_state([0., 0., 1., 1.], [1., 1., 1., 0.], sample_weight=[1, 1, 1, 0])\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/shaun/src/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py\", line 128, in __call__\r\n    losses, sample_weight, reduction=self._get_reduction())\r\n  File \"/home/shaun/src/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/losses_utils.py\", line 107, in compute_weighted_loss\r\n    losses, sample_weight)\r\n  File \"/home/shaun/src/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/util.py\", line 145, in scale_losses_by_sample_weight\r\n    losses = math_ops.cast(losses, dtypes.float32)\r\n  File \"/home/shaun/src/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/util.py\", line 97, in squeeze_or_expand_dimensions\r\n    # Use static rank.\r\n  File \"/home/shaun/src/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/shaun/src/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/shaun/src/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\", line 3649, in squeeze\r\n    return gen_array_ops.squeeze(input, axis, name)\r\n  File \"/home/shaun/src/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 10061, in squeeze\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got 4 [Op:Squeeze]\r\n\r\n```\r\n", "comments": ["Could replicate the issue with Tf 2.0.0. Please see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/4dcdd7b3b7a9d423c9f5a075465ae0fa/untitled202.ipynb). Thanks!", "@ShaunSpinelli Sorry that the docs on this are not very clear. Please take a look at the docs that we have for losses here, the same applies to metrics as well. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L100. \r\n\r\nThe input `y_true` and `y_pred` should atleast be 2D with the shape `[batch_size, d0, .. dN]`. \r\n\r\nPlease see the test here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/metrics_test.py#L746", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33399\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33399\">No</a>\n"]}, {"number": 33398, "title": "Improve docs for make_tensor_proto", "body": "", "comments": []}, {"number": 33397, "title": "LEAKY_RELU not supported in INT8 quantization", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n<ipython-input-54-6f97bfc97449> in <module>\r\n----> 1 workflow(False, 10, 'int8', True, True)\r\n\r\n<ipython-input-53-3889d2128b41> in workflow(dw, n_conv, dtype, v1, leaky)\r\n     11     model.save(keras_file_name)\r\n     12     if v1:\r\n---> 13         tflite_model = convert2tflitev1(keras_file_name, qmode=dtype)\r\n     14     else:\r\n     15         tflite_model = convert2tflite(model, qmode=dtype)\r\n\r\n<ipython-input-27-637084f295d5> in convert2tflitev1(model_file, qmode)\r\n     19     elif qmode == 'float32':\r\n     20         pass\r\n---> 21     tflite_model = converter.convert()\r\n     22 \r\n     23     return tflite_model\r\n\r\n~/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    991     if self._is_calibration_quantize():\r\n    992       result = self._calibrate_quantize_model(result, inference_input_type,\r\n--> 993                                               inference_output_type)\r\n    994 \r\n    995     return result\r\n\r\n~/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type)\r\n    237     return calibrate_quantize.calibrate_and_quantize(\r\n    238         self.representative_dataset.input_gen, inference_input_type,\r\n--> 239         inference_output_type, allow_float)\r\n    240 \r\n    241   def _get_base_converter_args(self):\r\n\r\n~/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float)\r\n     76     return self._calibrator.QuantizeModel(\r\n     77         np.dtype(input_type.as_numpy_dtype()).num,\r\n---> 78         np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n\r\n~/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\n    113 \r\n    114     def QuantizeModel(self, input_py_type, output_py_type, allow_float):\r\n--> 115         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\n    116 CalibrationWrapper_swigregister = _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_swigregister\r\n    117 CalibrationWrapper_swigregister(CalibrationWrapper)\r\n\r\nRuntimeError: Quantization not yet supported for op: LEAKY_RELU\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\nModel definition\r\n```\r\ndef build_lrelu_model(n_layers):\r\n    layer_list = []\r\n    for idx in range(n_layers):\r\n        conv_name = f'conv{idx}'\r\n        if idx == 0:\r\n            conv = layers.Conv2D(filters=32, \r\n                                 kernel_size=3, \r\n                                 padding='same', \r\n                                 name=conv_name, \r\n                                 input_shape=(28, 28, 3))\r\n        else:\r\n            conv = layers.Conv2D(filters=32, \r\n                                 kernel_size=3, \r\n                                 padding='same', \r\n                                 name=conv_name)\r\n        bn = layers.BatchNormalization(name=f'bn{idx}')\r\n        activation = layers.LeakyReLU(alpha=0.1, name=f'lrelu{idx}')\r\n        layer_list.extend([conv, bn, activation])\r\n    model = tf.keras.models.Sequential(layer_list)\r\n    model.summary()\r\n    return model\r\n```\r\n\r\nConversion options:\r\n\r\n```\r\n        converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(model_file)\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n        converter.inference_input_type = tf.uint8\r\n        converter.inference_output_type = tf.uint8\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nI saw there were some PR about the implementation of Leaky_ReLU in tflite and quantization. But I still cannot get it to work. \r\n\r\nRef PR: https://github.com/tensorflow/tensorflow/pull/27061\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["BTW, I am not using `tflite_convert.py`\r\n\r\nI am using `TFLiteConverter`, as shown in the code snippet above.", "I am not sure if I am looking at the right place.\r\n\r\nBut the error might be thrown from here: https://github.com/tensorflow/tensorflow/blob/265126ca309d9e9502ab072f033308192d1367b0/tensorflow/lite/tools/optimize/quantize_model.cc#L590\r\n\r\nwhich calls `operator_property.cc` ([link](https://github.com/tensorflow/tensorflow/blob/d08bf910b3ea5d8895a07553379436ad543f2da7/tensorflow/lite/tools/optimize/operator_property.cc)), where LEAKY_RELU and RESIZE_NEAREST_NEIGHBOUR are not defined. ", "Fixed in https://github.com/tensorflow/tensorflow/pull/36876", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33397\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33397\">No</a>\n", "Hi @wuhy08 \r\n\r\nAfter adding Leaky_Relu into operator property, I can generate the INT8 model which include Leaky_Relu . But the int8 implementation of Leaky_Relu is still missing, right? When I called invoked() tflite model, it will return fail. Did you meet the same problem?\r\n\r\nError detail:\r\nValueError: Didn't find op for builtin opcode 'LEAKY_RELU' version '2'\r\nRegistration failed.\r\n\r\nThanks\r\nChunjue", "Hi @cjtang \r\n\r\nThere was a PR about Leaky_RELU_INT8 a while ago (https://github.com/tensorflow/tensorflow/pull/27061). I believe that part of code was never touched since nobody successfully quantized a Leaky_RELU op. Let me investigate a little.", "It looks the error comes from this line:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3a8c575c4efe57560f36670ddc028f6fc889060a/tensorflow/lite/core/api/op_resolver.cc#L41 \r\n\r\nWill continue to investigate.\r\n\r\n", "A PR is created for this issue: https://github.com/tensorflow/tensorflow/pull/37279. Will wait for approval.", "Great, I also added my version of INT8 Leaky_RELU implementation, but the result is always not good. Now I know that is because model doesn't restrict the same scale for input and output. Thank you for pointing that out. Let's wait for approval.", "@cjtang \r\n\r\nThe PR has just been merged. Expect a few days before it is in tf-nightly.\r\n\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33397\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33397\">No</a>\n"]}, {"number": 33396, "title": "python -c \"import tensorflow as tf; print(tf.contrib.eager.num_gpus())\" from docs doesn't work with ", "body": "python -c \"import tensorflow as tf; print(tf.contrib.eager.num_gpus())\" Doesn't work from https://www.tensorflow.org/install/source\r\nwith tf2\r\n", "comments": ["@rlewkowicz ,\r\nHi, The reason for that is `contrib` has been removed from TF 2.0, please use command \r\n`print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))` . Refer this [link](https://www.tensorflow.org/guide/gpu) for more info on the same.Thanks!"]}, {"number": 33395, "title": "test", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 33394, "title": "AttributeError: 'list' object has no attribute 'op' when calling SparseCategoricalCrossentropy", "body": "**Describe the current behavior**\r\nWhen running the following code in the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) of `SparseCategoricalCrossentropy`:\r\n\r\n```\r\ncce = tf.keras.losses.SparseCategoricalCrossentropy()\r\nloss = cce(\r\n  [0, 1, 2],\r\n  [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\r\nprint('Loss: ', loss.numpy())  # Loss: 0.3239\r\n```\r\n\r\nI obtained the following error:\r\n\r\n> ---------------------------------------------------------------------------\r\n> AttributeError                            Traceback (most recent call last)\r\n> <ipython-input-2-e7331c659215> in <module>()\r\n>       3 loss = cce(\r\n>       4   [0, 1, 2],\r\n> ----> 5   [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\r\n>       6 print('Loss: ', loss.numpy())  # Loss: 0.3239\r\n> \r\n> 3 frames\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in sparse_categorical_crossentropy(target, output, from_logits, axis)\r\n>    4397   if not from_logits:\r\n>    4398     if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\r\n> -> 4399         output.op.type != 'Softmax'):\r\n>    4400       epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\r\n>    4401       output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\r\n> \r\n> AttributeError: 'list' object has no attribute 'op'\r\n\r\n", "comments": ["Added a PR #33406 for the fix.", "@netw0rkf10w The above PR will fix the issue for you. In the mean time you can provide converted tensors of `y_pred` and `y_true`  as follows. Thanks!\r\n`loss = cce(\r\n  tf.convert_to_tensor([0, 1, 2]),\r\n  tf.convert_to_tensor([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]]))` ", "@jvishnuvardhan Thanks a lot!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33394\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33394\">No</a>\n", "Still doesn't work. Can't reproduce the example from documentation.\r\n`---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-21-8338a7552986> in <module>()\r\n----> 1 sce(y_true, y_pred)\r\n\r\n/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py in __call__(self, y_true, y_pred, sample_weight)\r\n    124         y_true, y_pred, sample_weight)\r\n    125     with K.name_scope(scope_name or self.__class__.__name__), graph_ctx:\r\n--> 126       losses = self.call(y_true, y_pred)\r\n    127       return losses_utils.compute_weighted_loss(\r\n    128           losses, sample_weight, reduction=self._get_reduction())\r\n\r\n/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py in call(self, y_true, y_pred)\r\n    219       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\r\n    220           y_pred, y_true)\r\n--> 221     return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n    222 \r\n    223   def get_config(self):\r\n\r\n/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py in sparse_categorical_crossentropy(y_true, y_pred, from_logits, axis)\r\n    976 def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\r\n    977   return K.sparse_categorical_crossentropy(\r\n--> 978       y_true, y_pred, from_logits=from_logits, axis=axis)\r\n    979 \r\n    980 \r\n\r\n/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py in sparse_categorical_crossentropy(target, output, from_logits, axis)\r\n   4528   if not from_logits:\r\n   4529     if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\r\n-> 4530         output.op.type != 'Softmax'):\r\n   4531       epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\r\n   4532       output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\r\n\r\nAttributeError: 'list' object has no attribute 'op'`", "@ilyarudyak Can you please create a new issue and provide a standalone code to reproduce the issue?  Please ping me in that issue. Thanks!", "I'm getting `AttributeError: 'list' object has no attribute 'op'` when I try to get the output node name of a loaded model as follows:\r\n\r\n```\r\nmodel = tf.keras.models.load_model('test_model.h5')\r\noutput_names = model.outputs.op.name\r\n```\r\n\r\nMy Tensorflow version is: 2.5.0"]}, {"number": 33393, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONCATENATION, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MEAN, RELU, SOFTMAX, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: AddV2.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@ammu11, Please Provide the exact sequence of commands / steps that you executed before running into the problem. And also provide the Tensorflow version. Thanks!", "While converting pb file to tflite, facing this issue. And my tensorflow\nversion is 1.14.0. And here is the tflite command which I have used\n-> tflite_convert --output_file=D:/Images/car-damage-dataset/fnp.tflite\n--graph_def_file=D:/Images/car-damage-dataset/fnp.pb\n--input_shapes=1,224,224,3 --input_arrays=input_1 --output_arrays=output_0\n--mean_values=0 --std_dev_values=128 --allow_custom_ops\n\nOn Wed, Oct 16, 2019 at 4:49 AM gadagashwini <notifications@github.com>\nwrote:\n\n> @ammu11 <https://github.com/ammu11>, Please Provide the exact sequence of\n> commands / steps that you executed before running into the problem. And\n> also provide the Tensorflow version. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33393?email_source=notifications&email_token=AJLB5LDHKULAN745EDYSGULQO3ITDA5CNFSM4JBB66GKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBLVDEQ#issuecomment-542593426>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJLB5LC5YUVN6KR5VQFHRCLQO3ITDANCNFSM4JBB66GA>\n> .\n>\n", "Could you paste your .pb file here so that i can give it a try?", "I have the exactly the same problem. Running the sample code from doc:\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.applications.MobileNetV2(\r\n    weights=\"imagenet\", input_shape=(224, 224, 3))\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n```\r\nI used tf2.0, tf2.0-rc1 but these operations are still not found:\r\n```\r\nHere is a list of operators for which you will need custom implementations: AddV2, FusedBatchNormV3.\r\n```\r\n\r\n", "Thanks for your reply.\n\nI have updated the tensorflow version to 1.15.0 and worked for me.\n\nOn Thu, Oct 24, 2019 at 5:24 AM Dzung Nguyen <notifications@github.com>\nwrote:\n\n> I have the exactly the same problem. Running the sample code from doc:\n>\n> import tensorflow as tf\n>\n> model = tf.keras.applications.MobileNetV2(\n>     weights=\"imagenet\", input_shape=(224, 224, 3))\n>\n> # Convert the model.\n> converter = tf.lite.TFLiteConverter.from_keras_model(model)\n> tflite_model = converter.convert()\n>\n> I used tf2.0, tf2.0-rc1 but these operations are still not found:\n>\n> Here is a list of operators for which you will need custom implementations: AddV2, FusedBatchNormV3.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33393?email_source=notifications&email_token=AJLB5LH6W2JZNULOTDOWJWLQQFSVLA5CNFSM4JBB66GKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECEJM7Q#issuecomment-545822334>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJLB5LEOUDBDMTZOFGP7W4TQQFSVLANCNFSM4JBB66GA>\n> .\n>\n"]}, {"number": 33392, "title": "SEGBUS on tf.data.Dataset when generating large numpy arrays", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["opened incomplete by mistake"]}, {"number": 33391, "title": "TensorFlow documentation should be associated with the applicable specific versions of TensorFlow", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/guide/gpu\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI was reading this documentation page https://www.tensorflow.org/guide/gpu, but it is **unclear** which TensorFlow version the documentation applies to. Especially now that a stable version of TF 2 has been released, it is important to clarify which TF version the documentation refers to. \r\n\r\nOn the top bar of the linked webpage, there is a menu called API, where you can select the API you are interested in. Right now, the sub-menu corresponding to the specific version of TF I am reading is not even highlighted, so I don't know if I am reading the documentation for TF 1 or 2. To confuse people even further, even though I think I am reading the documentation for TF 2, in another top bar, it is written `TF 1`. \r\n\r\nGiven that I am confused, another person can also be confused, so the documentation needs to be clarified. So, I suggest that every documentation page is associated (**in a very clear way**) with all applicable TF versions.", "comments": ["Closing this issue now since the TF guide is for TF2 and there is a separate TF1 tab in the navigation that redirects to the TF1 docs repo\r\n![Screen Shot 2021-02-12 at 5 16 36 PM](https://user-images.githubusercontent.com/16965738/107832476-22990200-6d56-11eb-97aa-c79200458dbf.png)\r\n"]}, {"number": 33390, "title": "Docker Image with a pre-built Tensorflow-gpu with a compute capability 3.0 GPUs", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently if someone has an older GPU - they have to build from source. However this is time-consuming and error-prone. I tried building it as per instructions with no luck.\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n If there was a docker image for legacy GPU CUDA support - it would help many people who still have a legacy GPU cards to make use of them to train their models without having to dash out hundreds of dollars on a new GPU card right away. Building from source is very time consuming and error prone.\r\n\r\n**Any Other info.**\r\n", "comments": ["We're unable to provide official support for this, unfortunately.", "@dmitryr117 You can use https://puzl.ee so as not to buy a new gpu", "@dmitryr117 \r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33390\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33390\">No</a>\n"]}, {"number": 33389, "title": "[TF2.0] tf.make_ndarray() not functional", "body": "* environment: colab\r\n* python3\r\n* TF 2.0\r\n\r\nRunning ```tf.make_ndarray()``` throws an attribute error ```AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'tensor_shape' ```\r\n\r\nAs per the documentation [here](https://www.tensorflow.org/api_docs/python/tf/make_ndarray)\r\nit's supposed to create a numpy array with the same shape and content as the input tensor.\r\nI've also tried with lazy eval mode but the same attribute error is thrown. could it be a bug??\r\n\r\n```python\r\na=tf.constant([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\r\nx = tf.make_ndarray(a)\r\nprint(x)\r\n```\r\nFull error log:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-039c2e207451> in <module>()\r\n      1 a=tf.constant([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\r\n----> 2 tf.make_ndarray(a)\r\n      3 #print(x)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py in MakeNdarray(tensor)\r\n    576 \r\n    577   \"\"\"\r\n--> 578   shape = [d.size for d in tensor.tensor_shape.dim]\r\n    579   num_elements = np.prod(shape, dtype=np.int64)\r\n    580   tensor_dtype = dtypes.as_dtype(tensor.dtype)\r\n\r\nAttributeError: 'Tensor' object has no attribute 'tensor_shape'\r\n```", "comments": ["@steph-en-m I am not sure what exactly you are trying to do, but 2.0 allows to just run .numpy on a tensor\r\n\r\n```\r\na = tf.constant([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\r\na_n = a.numpy()\r\n\r\nprint(f\"{a_n.shape} - {a.shape}\")\r\n```\r\nthat gives you \r\n```\r\n(2, 2, 3) - (2, 2, 3)\r\n```", "I think I found the use-case you might have under 2.0\r\n\r\nIn case you are trying to form a request for TensorFlow serving you would need to `tf. make_tensor_proto` and from there you can convert back with mentioned your function above.\r\n\r\nSomething like this \r\n![image](https://user-images.githubusercontent.com/775466/66874275-f2e3b980-efaa-11e9-99fe-e74aecb3ff51.png)\r\n", "@steph-en-m \r\n\r\nHave a look on @lc0 's suggestion and let us know if that helps to resolve the issue. Thanks!", "Thanks @lc0 your second suggestion explains/resolves my issue."]}, {"number": 33388, "title": "TypeError: reduce() missing 1 required positional argument: 'per_replica_value'", "body": "```\r\n> with mirrored_strategy.scope():\r\n>     model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\r\n>     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n> \r\n> global_batch_size = 10 * mirrored_strategy.num_replicas_in_sync\r\n> \r\n> def input_fn():\r\n>     dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(\r\n>     global_batch_size)\r\n>     dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\r\n>     return dist_dataset\r\n>     \r\n> dataset = input_fn()\r\n> \r\n> @tf.function\r\n> def train_step(dist_inputs):\r\n>     def step_fn(inputs):\r\n>         features, labels = inputs\r\n>         with tf.GradientTape() as tape:\r\n>             logits = model(features)\r\n>             cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\r\n>                 logits=logits, labels=labels\r\n>             )\r\n>             loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)\r\n>         grads = tape.gradient(loss, model.trainable_variables)\r\n>         optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\r\n>         return cross_entropy\r\n> \r\n>     per_example_losses = mirrored_strategy.experimental_run_v2(\r\n>         step_fn, args=(dist_inputs,)\r\n>     )\r\n>     mean_loss = mirrored_strategy.reduce(\r\n>         tf.distribute.ReduceOp.MEAN, per_example_losses, axis=0\r\n>     )    \r\n>     return mean_loss\r\n> \r\n> with mirrored_strategy.scope():\r\n>     for input in dataset:\r\n>         train_step(inputs)\r\n```\r\n\r\nIts one of the code snippets from the distributed strategy guide for custom training loops.\r\nIm getting this, \r\n\r\nTypeError:TypeError: reduce() missing 1 required positional argument: 'per_replica_value' \r\n\r\nbut then when I ran the same code today morning it was working perfectly. ", "comments": ["@sourcecode369 ,\r\nThank you for reporting, Can you please provide complete code to reproduce the above issue ? Also let us know TF version being used.", "I'm using TensorFlow 2.0, there is some kind of bug. I am getting this error again by trying out a different code. \r\n\r\n```\r\nimport tensorflow as tf\r\nprint(\"TensorFlow version: \", tf.__version__)\r\n```\r\n\r\n```\r\ntf.debugging.set_log_device_placement(True)\r\ntf.config.set_soft_device_placement(True)\r\n\r\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\nif gpus:\r\n    try:\r\n        tf.config.experimental.set_virtual_device_configuration(\r\n            gpus[0],\r\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\r\n             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]\r\n        )\r\n        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\r\n        print(\"Physical GPUs\", len(gpus), \"Logical GPUs\", len(logical_gpus))\r\n    except RuntimeError as e:\r\n        print(e)\r\nelse:\r\n    print(\"No GPUs found.\")\r\n```\r\n\r\n```\r\ntf.debugging.set_log_device_placement(True)\r\ntf.config.set_soft_device_placement(True)\r\n\r\nmirrorerd_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice)\r\nwith mirrorerd_strategy.scope():\r\n    inputs = tf.keras.layers.Input(shape=(1,))\r\n    predictions = tf.keras.layers.Dense(1)(inputs)\r\n    model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\r\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\r\n```\r\n\r\nI am getting the same error again when I try to run the above code. Please help me out. I have confirmed twice that Im using TensorFlow 2.0 on Google Colaboratory and have installed by, \r\n\r\n> pip install --upgrade tensorflow\r\n\r\n and my gpu runtime is active.\r\n\r\n\r\nFor the above one, that is pretty much the complete code. Just that there was one more line of code above where I create an object of tf.distribute.MirroredStrategy by using the below code,\r\n\r\n`mirrorerd_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice)\r\n`\r\n\r\n\r\n> Below I am pasting the complete error message that I am receiving when trying to run both of the code. Hope this helps,\r\n> \r\n> INFO:tensorflow:Error reported to Coordinator: reduce() missing 1 required positional argument: 'per_replica_value'\r\n> Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/coordinator.py\", line 297, in stop_on_exception\r\n>     yield\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 190, in _call_for_each_replica\r\n>     **merge_kwargs)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/metrics_utils.py\", line 118, in merge_fn_wrapper\r\n>     result = distribution.experimental_local_results(merge_fn)[0](*args)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/metrics.py\", line 368, in result\r\n>     return math_ops.div_no_nan(self.total, self.count)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\r\n>     return target(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\", line 1111, in div_no_nan\r\n>     x = ops.convert_to_tensor(x, name=\"x\")\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1184, in convert_to_tensor\r\n>     return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1242, in convert_to_tensor_v2\r\n>     as_ref=False)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py\", line 1271, in _tensor_conversion_sync_on_read\r\n>     return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py\", line 1261, in _dense_var_to_tensor\r\n>     self.get(), dtype=dtype, name=name, as_ref=as_ref)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py\", line 322, in get\r\n>     return self._get_cross_replica()\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py\", line 1237, in _get_cross_replica\r\n>     self, axis=None)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 805, in reduce\r\n>     return self._extended._reduce(reduce_op, value)  # pylint: disable=protected-access\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1436, in _reduce\r\n>     device_util.current() or \"/device:CPU:0\"))[0]\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 703, in _reduce_to\r\n>     reduce_op, value, destinations=destinations)\r\n> TypeError: reduce() missing 1 required positional argument: 'per_replica_value'\r\n\r\n---------------------------------------------------------------------------\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-18-2e63ea6eaf5a> in <module>()\r\n      7     predictions = tf.keras.layers.Dense(1)(inputs)\r\n      8     model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\r\n----> 9     model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\r\n```\r\n\r\n**27 frames**\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py in _reduce_to(self, reduce_op, value, destinations)\r\n    701           reduce_op, self._device_map, value, destinations)\r\n    702     return self._get_cross_device_ops().reduce(\r\n--> 703         reduce_op, value, destinations=destinations)\r\n    704 \r\n    705   def _batch_reduce_to(self, reduce_op, value_destination_pairs):\r\n```\r\n\r\n### TypeError: reduce() missing 1 required positional argument: 'per_replica_value'\r\n\r\n\r\nPlease help me out.\r\n ", "is there anybody who can help me out with this?\r\nim facing this issue time and again.", "I finally was able to solve this for the keras by removing the metrics argument, though I dont understand why thats the case. \r\nHowever for custom tensorflow training loop the error still persists. I guess the error is somewhere here,\r\n\r\n```\r\n> per_example_losses = mirrored_strategy.experimental_run_v2(\r\n>         step_fn, args=(dist_inputs,)\r\n>     )\r\n> mean_loss = mirrored_strategy.reduce(\r\n>         tf.distribute.ReduceOp.MEAN, per_example_losses, axis=0\r\n> )\r\n```\r\n\r\nI'll try to look into it and report back.", "I finally solved it. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33388\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33388\">No</a>\n", "sourcecode369@ can you tell us how you solved the issue? I was able to repro the bug with the given cross_device_ops argument. ", "> sourcecode369@ can you tell us how you solved the issue? I was able to repro the bug with the given cross_device_ops argument.\r\n\r\nYup the problem I found was due toi `tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice)` and rather the proper definition would be,\r\n`tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())`. Forgot to put the parenthesis due to which a default argument was not getting passed. \r\n "]}, {"number": 33387, "title": "[TF 2.0] Custom objects", "body": "Hello all,\r\n\r\nI have a keras file (.hdf5) that I've saved using TF 1.14 with custom operations (eg loss function). \r\nI recently upgraded to Tensorflow 2.0 and I was trying to convert this model to .tflite but I've seen that there is no 'custom_objects' argument. \r\nIs this an unnecessary argument had I saved the .hdf5 using the latest TF 2.0? Or is there any other new way to include this custom object when converting to .tflite?\r\n\r\nThank you in advance", "comments": ["@tgpsantos ,\r\nCan you please refer the [link](https://github.com/tensorflow/tensorflow/issues/20878) of similar issue ?Thanks!", "Thank you for the answer @oanush . The whole link seems like a mix of different issues but I assume you're referring to the answer that advices converting with an itermediate step, ie .h5 to .pb. Is this going to be in the right format to use in convert_from_saved_model() in TF2.0? I believe there is no convert_from_saved_graph in the latest version\r\n\r\nPS: The original post was referring to TF1.9, I can make it work with TF1.14. My problem is with TF2.0", "@tgpsantos This is a stale issue. Is this still an issue for you?\r\n\r\nIf yes, can you please share a simple standalone code to reproduce the issue. Since the issue opened, there is a lot of TFLite tutorials added to convert keras model/saved model into different levels of precisions. Please check the [guide here](https://www.tensorflow.org/lite/convert). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33387\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33387\">No</a>\n"]}, {"number": 33386, "title": "Refactor `PrefetchDatasetOpTest` and `ReduceDatasetOp()`", "body": "This PR refactors `PrefetchDatasetOpTest` and `ReduceDatasetOp()`.", "comments": []}, {"number": 33385, "title": "hub.Module(\"https://tfhub.dev/google/elmo/2\")` doesn't work.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow-hub version: 0.6.0\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7.4\r\n\r\n\r\n**Describe the current behavior**\r\nThe module load has not been completed and is waiting indefinitely.\r\n\r\n**Code to reproduce the issue**\r\n`elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\")`\r\n", "comments": ["@fairy-of-9 I am closing this issue as it has already been updated [here](https://github.com/tensorflow/hub/issues/385) and it belongs to tensorflow/hub repo and you will get an expedited answer. Thanks!"]}, {"number": 33384, "title": "Remove unnecessary `else` clauses", "body": "Title self explanatory", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33384) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33384) for more info**.\n\n<!-- ok -->"]}, {"number": 33383, "title": "Desynchronized zipped datasets when using tf.data.experimental.ignore_errors", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nna\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nv2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version:\r\nPython 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\ncuda 10.0\r\n- GPU model and memory:\r\nP100 / 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen an error is caught using the `tf.data.experimental.ignore_errors` on a zipped dataset, only the faulty dataset drops an element. The datasets are therefore desynchronized.\r\n\r\n**Describe the expected behavior**\r\n\r\nDatasets should stay synchronized by dropping an element from both datasets.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\ngood_dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])\r\nbad_dataset = good_dataset.map(lambda x: tf.debugging.check_numerics(1. / x, \"error\"))\r\n\r\ndataset = tf.data.Dataset.zip((bad_dataset, good_dataset))\r\ndataset = dataset.apply(tf.data.experimental.ignore_errors())\r\n\r\nfor bad, good in dataset:\r\n    print(float(good), float(bad))\r\n\r\n1.0 1.0\r\n2.0 0.5\r\n0.0 0.25\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I could reproduce the issue with Tf 2.0.0. Please see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/48e54114ef599b79cd9d09e4a7fb9736/untitled201.ipynb). Thanks!", "This is working as intended. The good dataset dropped `0.0` while the bad dataset dropped `1 / 0.0`.\r\n\r\n@scharron What output would you expect for your example?", "@jsimsa Oops sorry I went too fast, I fixed the code to reproduce\r\n\r\n(if zipping `good` then `bad` it works, but the reverse does not)\r\n ", "I think the issue is that, in case of zip, when error or end-of-sequence encountered the remaining components are not \"flushed out\".  So out-of-sync happens when ignore_errors is in play.\r\n\r\nCreated a PR #33887, think this could fix the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33383\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33383\">No</a>\n", "Thanks !\r\n\r\nWouldn't it be possible and useful to also issue a warning if the `tf.data.experimental.ignore_errors` is applied to any Dataset used as input to `tf.data.Dataset.zip`.\r\n\r\nThis could also result in datasets desynchronization if any of the zipped datasets drops any item, and in my opinion shouldn't be the behaviour of a `zip` function.\r\n\r\n\r\n\r\n "]}, {"number": 33382, "title": "Bugs about dequantize operator calculation formula in SCALED mode", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: Python 2.7.12\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nUsing numpy to simulate the calculation formula in SCALED mode of the dequantize operator in tensorflow, you can't get the correct result.\r\n```\r\n# This is the method of dequantize operator in tensorflow kernel code written by me.\r\nif mode == \"SCALED\":\r\n    num_bits = size * 8\r\n    if dtype == \"uint8\":\r\n        limit_min = 0\r\n        limit_max = 1 << num_bits - 1\r\n        scalar_factor = max_range / limit_max\r\n    if dtype in (\"int8\", \"int32\"):\r\n        limit_min = -(1 << (num_bits - 1))\r\n        limit_max = -1 * limit_min - 1\r\n        scalar_factor = max(min_range / limit_min, max_range / limit_max)\r\n    res = input_tensor * scalar_factor\r\n```\r\n```\r\n# This is the code of dequantize operator in tensorflow kernel code written by c++.\r\nelse if (mode_ == QUANTIZE_MODE_SCALED) {\r\n    const float scale_factor =\r\n        std::numeric_limits<T>::min() == 0\r\n            ? (max_range / std::numeric_limits<T>::max())\r\n            : std::max(min_range / std::numeric_limits<T>::min(),\r\n                       max_range / std::numeric_limits<T>::max());\r\n    const auto& input_tensor = input.flat<T>();\r\n    output->flat<float>() =\r\n    input_tensor.template cast<int>().template cast<float>() * scale_factor;\r\n}\r\n```\r\n**Describe the expected behavior**\r\nthe code written by me should get the same result as the tensorflow c++ code.\r\nHowever, I got much different result.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\n# Then, I write the code that can get the same result as the actual result of the tensorflow\r\nif mode == \"SCALED\":\r\n    m = max_range  # float32\r\n    if dtype == \"int8\" or dtype == \"int16\" or dtype == \"int32\":\r\n        # min&max fixed is float32\r\n        [min_fixed, max_fixed] = [-((1 << (num_bits - 1)) - 1), (1 << (num_bits - 1))-1]\r\n        s = 2.0 * m / (max_fixed - min_fixed)\r\n    if dtype == \"uint8\" or dtype == \"uint16\":\r\n        [min_fixed, max_fixed] = [0, (1 << num_bits) - 1]\r\n        s = m / (max_fixed - min_fixed)\r\n    z = input_tensor * s\r\n```\r\nIn my code, the _input_min_range_ param is redundant.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@BluePaladin ,\r\nThank you for reporting, can you please provide complete code to reproduce the issue reported \r\nabove ? Thanks!", "\r\n[dequantize_data.txt](https://github.com/tensorflow/tensorflow/files/3751297/dequantize_data.txt)\r\n", "@BluePaladin ,\r\nWhen tried running the code I got `UnboundLocalError: local variable 'z_offset' referenced before assignment`, can you please check ?Also find the [gist](https://colab.sandbox.google.com/gist/oanush/6e783d0ffe4cad26c25cf2ded8b79c63/33382.ipynb) of colab. Thanks", "@oanush, I am very sorry to offer a wrong code. Please try this one,  thank you!\r\n[dequantize_data.txt](https://github.com/tensorflow/tensorflow/files/3761531/dequantize_data.txt)\r\n", "@ymodak Bro, is there any progress in this issue now?", "Apologies BluePaladin this issue fell through the cracks. \r\nIs this still an issue? If yes then can you please try testing it with latest TF (2.2) version?\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33381, "title": "[TFLite, Converter] Converting error on unusual dense layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): GIT_VERSION: 'v1.12.1-15611-g025365a736', VERSION: '2.0.0''\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nWhen run the code below, conversion errors appear that are in the logs below.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MyDense(tf.keras.layers.Layer):\r\n  def __init__(self, num_units, **kwargs):\r\n    super(MyDense, self).__init__(**kwargs)\r\n    self.num_units = num_units\r\n\r\n  def build(self, input_shape):\r\n    kernel_shape = [input_shape[-1], self.num_units * 2, self.num_units]\r\n    bias_shape = [self.num_units]\r\n\r\n    self.kernel = self.add_weight(\"kernel\", shape=kernel_shape, trainable=True)\r\n    self.bias = self.add_weight(\"bias\", shape=bias_shape, trainable=True)\r\n    super(MyDense, self).build(input_shape)\r\n\r\n  def call(self, inputs):\r\n    return tf.einsum(\"ac,cde->ade\", inputs, self.kernel) + self.bias\r\n\r\ninputs = tf.keras.Input(shape=(10,), dtype=tf.float32)\r\noutputs = MyDense(15)(inputs)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\nmodel.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nprint(\"SUCCESS!\")\r\n```\r\n\r\n**Logs**\r\n```\r\n2019-10-15 14:43:59.548878: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2019-10-15 14:43:59.566412: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3100000000 Hz\r\n2019-10-15 14:43:59.567598: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3502840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-10-15 14:43:59.567616: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 10)]              0\r\n_________________________________________________________________\r\nmy_dense (MyDense)           (None, 30, 15)            4515\r\n=================================================================\r\nTotal params: 4,515\r\nTrainable params: 4,515\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2019-10-15 14:43:59.640959: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-10-15 14:43:59.641037: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-10-15 14:43:59.642744: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:829] Optimization results for grappler item: graph_to_optimize\r\n2019-10-15 14:43:59.642759: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:831]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2019-10-15 14:43:59.642763: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:831]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-10-15 14:43:59.657830: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-10-15 14:43:59.657896: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-10-15 14:43:59.660184: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:829] Optimization results for grappler item: graph_to_optimize\r\n2019-10-15 14:43:59.660198: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:831]   constant_folding: Graph size after: 21 nodes (-3), 23 edges (-4), time = 0.76ms.\r\n2019-10-15 14:43:59.660203: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:831]   constant_folding: Graph size after: 21 nodes (0), 23 edges (0), time = 0.248ms.\r\nTraceback (most recent call last):\r\n File \"convert_error.py\", line 25, in <module>\r\n   tflite_model = converter.convert()\r\n File \"/workspace/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 447, in convert\r\n   **converter_kwargs)\r\n File \"/workspace/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n   enable_mlir_converter=enable_mlir_converter)\r\n File \"/workspace/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n   raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-10-15 14:44:00.554744: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 10 operators, 19 arrays (0 quantized)\r\n2019-10-15 14:44:00.554825: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 10 operators, 19 arrays (0 quantized)\r\n2019-10-15 14:44:00.554930: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 3 operators, 8 arrays (0 quantized)\r\n2019-10-15 14:44:00.554945: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:118] Check failed: dim_x == dim_y (450 vs. 15)Dimensions must match\r\nFatal Python error: Aborted\r\nCurrent thread 0x00007f4cae2e4740 (most recent call first):\r\n File \"/workspace/.local/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n File \"/workspace/.local/lib/python3.6/site-packages/absl/app.py\", line 250 in _run_main\r\n File \"/workspace/.local/lib/python3.6/site-packages/absl/app.py\", line 299 in run\r\n File \"/workspace/.local/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n File \"/workspace/.local/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n File \"/workspace/.local/bin//toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n```\r\n", "comments": ["Issue replicating for TF-2.0,kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/a016f0f856fc999081bfa50b99ee4f81/33381.ipynb) of colab.", "@Vooblin,\r\nCould you please add the below two lines of code before converting the model and check if it works.\r\n```\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] \r\n```\r\n\r\nI was able to run the code without any issues on TensorFlow v2.2.0-rc4. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/881306cb7e11cc4acc09e609dffc3550/33381.ipynb#scrollTo=WVmPWKimGbfq). Thanks!", "@Vooblin,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33381\">No</a>\n"]}, {"number": 33380, "title": "get_config missing from AdditiveAttention", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nModel containing AdditiveAttention cannot be saved due to missing get_config\r\n\r\n**Describe the expected behavior**\r\nAdditiveAttention has get_config and can be saved\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nmax_tokens=6\r\ndimension = 2\r\n\r\n# Variable-length int sequences.\r\nquery_input = tf.keras.Input(shape=(None,), dtype='int32')\r\nvalue_input = tf.keras.Input(shape=(None,), dtype='int32')\r\n\r\n# Embedding lookup.\r\ntoken_embedding = tf.keras.layers.Embedding(max_tokens, dimension)\r\n# Query embeddings of shape [batch_size, Tq, dimension].\r\nquery_embeddings = token_embedding(query_input)\r\n# Value embeddings of shape [batch_size, Tv, dimension].\r\nvalue_embeddings = token_embedding(query_input)\r\n\r\n# CNN layer.\r\ncnn_layer = tf.keras.layers.Conv1D(\r\n    filters=100,\r\n    kernel_size=4,\r\n    # Use 'same' padding so outputs have the same shape as inputs.\r\n    padding='same')\r\n# Query encoding of shape [batch_size, Tq, filters].\r\nquery_seq_encoding = cnn_layer(query_embeddings)\r\n# Value encoding of shape [batch_size, Tv, filters].\r\nvalue_seq_encoding = cnn_layer(value_embeddings)\r\n\r\n# Query-value attention of shape [batch_size, Tq, filters].\r\nquery_value_attention_seq = tf.keras.layers.AdditiveAttention()(\r\n    [query_seq_encoding, value_seq_encoding])\r\n\r\n# Reduce over the sequence axis to produce encodings of shape\r\n# [batch_size, filters].\r\nquery_encoding = tf.keras.layers.GlobalAveragePooling1D()(\r\n    query_seq_encoding)\r\nquery_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\r\n    query_value_attention_seq)\r\n\r\n# Concatenate query and document encodings to produce a DNN input layer.\r\ninput_layer = tf.keras.layers.Concatenate()(\r\n    [query_encoding, query_value_attention])\r\n\r\n\r\nmodel = tf.keras.Model(inputs=(query_input,value_input),outputs=input_layer)\r\nmodel.save('test.h5')\r\n#NotImplementedError: Layers with arguments in `__init__` must override `get_config`.\r\n```\r\n\r\n**Other info / logs**\r\nCode based on https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention\r\n\r\n", "comments": ["I could reproduce the issue with tf 2.0.0. Please take a look at colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/6ab8b86252382e26b668b41815b19305/untitled197.ipynb). Thanks!", "I'm having a similar issue with `tf.keras.layers.Attention` with TensorFlow 2.0.0:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nattention = tf.keras.layers.Attention()\r\nattention.get_config()\r\n```\r\n\r\nError:\r\n\r\n```\r\nNotImplementedError: Layers with arguments in `__init__` must override `get_config`.\r\n```\r\n\r\nI didn't want to start a new issue, but this is also preventing me from saving models with the HDF5 format.\r\n\r\nMy temporary workaround is something like the following:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nattention_kwargs = dict(use_scale=True, name='attention')\r\nattention = tf.keras.layers.Attention(**attention_kwargs)\r\nattention.get_config = lambda: attention_kwargs  # manually make a get_config()\r\n\r\n# Build and save the model\r\nquery = tf.keras.layers.Input((None, 10))\r\nvalue = tf.keras.layers.Input((None, 10))\r\noutputs = attention([query, value])\r\nmodel = tf.keras.Model(inputs=[query, value], outputs=outputs)\r\nmodel.save('model.h5', save_format='h5')\r\n\r\n# Restore the model\r\nrestored_model = tf.keras.models.load_model(\r\n    filepath='model.h5',\r\n    custom_objects={'Attention': tf.keras.layers.Attention},\r\n)\r\n```\r\n\r\nNote that if I omit `custom_objects` when loading the model, I get the error\r\n\r\n```\r\nValueError: Unknown layer: Attention\r\n```\r\n\r\nRelated: #32662", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33380\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33380\">No</a>\n"]}, {"number": 33379, "title": "Tensorflow 1.13.1 cannot detect my GPUs", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\n```python\r\ntf.test.gpu_device_name()\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): conda \r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n|   0  Quadro RTX 6000     On   | 00000000:1A:00.0 Off |                  Off |\r\n| 33%   27C    P8    14W / 260W |      0MiB / 24219MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Quadro RTX 6000     On   | 00000000:1B:00.0 Off |                  Off |\r\n| 33%   24C    P8    16W / 260W |      0MiB / 24220MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Quadro RTX 6000     On   | 00000000:60:00.0 Off |                  Off |\r\n| 34%   26C    P8     8W / 260W |      0MiB / 24220MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Quadro RTX 6000     On   | 00000000:61:00.0 Off |                  Off |\r\n| 55%   74C    P2   103W / 260W |  17128MiB / 24220MiB |     58%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Quadro RTX 6000     On   | 00000000:B1:00.0 Off |                  Off |\r\n| 56%   76C    P2   257W / 260W |  18782MiB / 24220MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Quadro RTX 6000     On   | 00000000:B2:00.0 Off |                  Off |\r\n| 55%   75C    P2   258W / 260W |  17128MiB / 24220MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Quadro RTX 6000     On   | 00000000:DA:00.0 Off |                  Off |\r\n| 45%   68C    P2   240W / 260W |  19408MiB / 24220MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Quadro RTX 6000     On   | 00000000:DB:00.0 Off |                  Off |\r\n| 33%   42C    P2    62W / 260W |   5968MiB / 24220MiB |     11%      Default |\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```\r\n2019-10-15 10:29:55.961860: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2019-10-15 10:29:55.997917: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz\r\n2019-10-15 10:29:56.003864: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c322117980 executing computations on platform Host. Devices:\r\n2019-10-15 10:29:56.003932: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n```\r\n**Describe the expected behavior**\r\nWhen I use TF2.0 in another conda environment, it gives me:\r\n\r\n```\r\n2019-10-15 10:31:36.984764: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2019-10-15 10:31:37.046278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz\r\n2019-10-15 10:31:37.052904: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561b33274680 executing computations on platform Host. Devices:\r\n2019-10-15 10:31:37.052993: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-10-15 10:31:37.055456: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-10-15 10:31:39.414744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:1a:00.0\r\n2019-10-15 10:31:39.416616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:\r\nname: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:1b:00.0\r\n2019-10-15 10:31:39.418257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:\r\nname: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:60:00.0\r\n2019-10-15 10:31:39.419261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:\r\nname: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:61:00.0\r\n2019-10-15 10:31:39.420278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:\r\nname: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:b1:00.0\r\n2019-10-15 10:31:39.421270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:\r\nname: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:b2:00.0\r\n2019-10-15 10:31:39.422273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:\r\nname: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:da:00.0\r\n2019-10-15 10:31:39.423781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties:\r\nname: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:db:00.0\r\n2019-10-15 10:31:39.424026: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-10-15 10:31:39.425587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-10-15 10:31:39.427313: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2019-10-15 10:31:39.427584: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2019-10-15 10:31:39.429138: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2019-10-15 10:31:39.429937: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2019-10-15 10:31:39.433385: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-15 10:31:39.455050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2019-10-15 10:31:39.455089: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-10-15 10:31:39.468145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-15 10:31:39.468164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7\r\n2019-10-15 10:31:39.468172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y Y Y Y Y\r\n2019-10-15 10:31:39.468179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y Y Y Y Y\r\n2019-10-15 10:31:39.468185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y Y Y Y Y\r\n2019-10-15 10:31:39.468192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N Y Y Y Y\r\n2019-10-15 10:31:39.468198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   Y Y Y Y N Y Y Y\r\n2019-10-15 10:31:39.468206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   Y Y Y Y Y N Y Y\r\n2019-10-15 10:31:39.468213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   Y Y Y Y Y Y N Y\r\n2019-10-15 10:31:39.468223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   Y Y Y Y Y Y Y N\r\n2019-10-15 10:31:39.482310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 22843 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:1a:00.0, compute capability: 7.5)\r\n2019-10-15 10:31:39.486638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 22845 MB memory) -> physical GPU (device: 1, name: Quadro RTX 6000, pci bus id: 0000:1b:00.0, compute capability: 7.5)\r\n2019-10-15 10:31:39.490720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 22845 MB memory) -> physical GPU (device: 2, name: Quadro RTX 6000, pci bus id: 0000:60:00.0, compute capability: 7.5)\r\n2019-10-15 10:31:39.493272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 4348 MB memory) -> physical GPU (device: 3, name: Quadro RTX 6000, pci bus id: 0000:61:00.0, compute capability: 7.5)\r\n2019-10-15 10:31:39.496165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 4974 MB memory) -> physical GPU (device: 4, name: Quadro RTX 6000, pci bus id: 0000:b1:00.0, compute capability: 7.5)\r\n2019-10-15 10:31:39.498872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 4348 MB memory) -> physical GPU (device: 5, name: Quadro RTX 6000, pci bus id: 0000:b2:00.0, compute capability: 7.5)\r\n2019-10-15 10:31:39.501375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 4348 MB memory) -> physical GPU (device: 6, name: Quadro RTX 6000, pci bus id: 0000:da:00.0, compute capability: 7.5)\r\n2019-10-15 10:31:39.504819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 17184 MB memory) -> physical GPU (device: 7, name: Quadro RTX 6000, pci bus id: 0000:db:00.0, compute capability: 7.5)\r\n2019-10-15 10:31:39.509288: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561b387dc0c0 executing computations on platform CUDA. Devices:\r\n2019-10-15 10:31:39.509305: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\r\n2019-10-15 10:31:39.509311: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Quadro RTX 6000, Compute Capability 7.5\r\n2019-10-15 10:31:39.509317: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Quadro RTX 6000, Compute Capability 7.5\r\n2019-10-15 10:31:39.509322: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Quadro RTX 6000, Compute Capability 7.5\r\n2019-10-15 10:31:39.509328: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): Quadro RTX 6000, Compute Capability 7.5\r\n2019-10-15 10:31:39.509334: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): Quadro RTX 6000, Compute Capability 7.5\r\n2019-10-15 10:31:39.509339: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): Quadro RTX 6000, Compute Capability 7.5\r\n2019-10-15 10:31:39.509344: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (7): Quadro RTX 6000, Compute Capability 7.5\r\n'/device:GPU:0'\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nI need to work on TF 1.13.1 because some specific code requires this version.\r\n", "comments": []}]