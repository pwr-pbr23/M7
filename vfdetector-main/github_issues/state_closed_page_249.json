[{"number": 47020, "title": "tf.nn.embedding_lookup outputs NaN", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.nn.embedding_lookup` outputs NaN if `params` is large value\r\n\r\n\r\n**Describe the expected behavior**\r\nExpect a grace exception message if the input is invalid instead of Nan as output\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.nn.embedding_lookup(params=np.array([1e+38, 1e+38],dtype=np.float32), ids=1, max_norm=10)\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n<tf.Tensor: shape=(), dtype=float32, numpy=nan>\r\n~~~", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5274acb10c36e926ce950dde22f4e5fc/47020.ipynb). Thanks!", "This is expected floating point behavior. TensorFlow does not guarantee that floating point approximations will not result in NaN values, in line with other numerical toolkits. You can use `tf.math.is_nan` to check for NaN values.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47020\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47020\">No</a>\n"]}, {"number": 47019, "title": "`tf.nn.log_poisson_loss` outputs NaN", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.nn.log_poisson_loss` outputs NaN if `log_input` and `targets` contain large values\r\n\r\n\r\n**Describe the expected behavior**\r\nExpect a grace exception message if the input is invalid instead of Nan as output\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.nn.log_poisson_loss(log_input=np.array([1e+38], dtype=np.float32), targets=np.array([1e+38], dtype=np.float32))\r\n~~~\r\nOutput:\r\n~~~python\r\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>\r\n~~~", "comments": ["I am able to replicate the issue reported on tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/98c7843a6a19d3d28efaff943f99c892/untitled524.ipynb)", "This is an expected result from floating point approximation. Checking for NaNs on every operation would result in unacceptable performance degradation and isn't consistent with other floating point libraries.\r\n\r\nYou can use tf.math.is_nan to check if a result is nan.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47019\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47019\">No</a>\n", "Ideally this should return infinity.  Currently\r\n```\r\ntf.nn.log_poisson_loss(log_input=np.array([100], dtype=np.float32), targets=np.array([100], dtype=np.float32))\r\ntf.Tensor([inf], shape=(1,), dtype=float32)\r\n```\r\ncontributions welcome (for investigation and potential fix if there is non-expensive way)!", "@kkimdev \r\nWhat is the expected output that we are looking for here?\r\n", "Hi @DNXie  @kkimdev !checked this issue is giving \"inf\" output when compiled with float64 in TF 2.6 (used tensor instead of numpy array ) providing [gist ](https://colab.research.google.com/gist/mohantym/cc05b28af3fee349917513a55ccc2908/untitled524.ipynb#scrollTo=HRA_hmkgVC4J)for reference", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47019\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47019\">No</a>\n"]}, {"number": 47018, "title": "Remove the sources containing the models from the TFLM static lib.", "body": "These sources are only needed for specific tests and are now explicitly specified as part of creating a test target.\r\n\r\nFrom this change onwards, we will explicitly create test targets for tests that depend on sources outside of libtensorflow-microlite.a. This avoids putting unnecessary files into MICROLITE_CC_SRCS.\r\n\r\nManually verified that the following command does not error out:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile generate_keyword_benchmark_make_project && cd tensorflow/lite/micro/tools/make/gen/linux_x86_64_default/prj/keyword_benchmark/make/ && make -j8\r\n```\r\n\r\nFixes #46860\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47017, "title": "Thrown out ValueError when alpha=None is passed for tf.keras.layers.LeakyReLU", "body": "This PR tries to address the issue raised in #46993 where\r\nincorrect nan value is returned when alpha=None is passed for\r\ntf.keras.layers.LeakyReLU. The nan could be misleading to users.\r\n\r\nThis PR address the issue and throw out ValueError instead.\r\n\r\nThis PR fixes #46993.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 47016, "title": "Some Optimizers Don't Work on GPU With Embedding Layers", "body": "**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS\r\nTensorFlow installed from (source or binary): Binary\r\nTensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\nPython version: 3.7.9\r\nCUDA/cuDNN version: CUDA Version: 11.0\r\nGPU model and memory: Tesla T4, 15gb\r\n\r\n**Describe the current behavior**\r\nWith some optimizers, I get this error consistently:\u00a0\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation .../ReadVariableOp: Could not satisfy explicit device specification '' because the node {{colocation_node .../ReadVariableOp}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, ... , /job:localhost/replica:0/task:0/device:GPU:0...]. \r\n\r\nFull error message is in this [gist](https://gist.github.com/iprovalo/7172d7b67535bc48e6ca7bae95a56ce1).\r\n\r\nIt looks like these three optimizers have an issue with embeddings:\u00a0Adadelta,\u00a0Adagrad, Ftrl\r\n\r\nNone of the other optimizers have this issue.\u00a0 I tested all [eight](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) of them.\r\n\r\n[Code](https://github.com/iprovalo/tf/blob/main/input_pipeline_stats_benchmark.py) to reproduce this.\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nAll optimizers should work with Embeddings, or the documentation needs to reflect which ones are not compatible.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Code](https://github.com/iprovalo/tf/blob/main/input_pipeline_stats_benchmark.py) to reproduce this.\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nlog in the [gist](https://gist.github.com/iprovalo/7172d7b67535bc48e6ca7bae95a56ce1)\r\n", "comments": ["@iprovalo \r\n\r\nI have tried in colab with TF-GPU version 2.4 and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a3958c26febbb4f5f405d765e927c894/untitled665.ipynb).Thanks!", "@ravikyram my bad, the [optimizer on line 18](https://github.com/iprovalo/tf/blob/main/input_pipeline_stats_benchmark.py#L18) has to be changed to one of the breaking ones, e.g. Adadelta.  Then this [colab](https://colab.research.google.com/drive/1ning0Osj2yo5Pd2sonz3Et1sU5smm1gL#scrollTo=QFc5y8bVRNsO) code reproduces the issue.", "@iprovalo I ran your code with `Adadelta()` using `tf-nightly` and I don't see any issue. Can you please look at [this gist](https://colab.research.google.com/gist/jvishnuvardhan/0ac2507ad309149c1933f6540bb9679c/untitled665.ipynb) and let me know if I am missing anything. It could be that the issue was resolved in `tf-nightly`.\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan thank you for a follow up.  Although, I don't see this issue in tf-nightly, that's not the release I work with.  The issue is reproducible in TF-2.4.  If these optimizers don't work on GPU for 2.4, the documentation probably should state that.  I don't consider this resolved for TF-2.4.\r\n\r\nThank you!", "Was able to replicate the issue with TF 2.6.0-dev20210603  ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5fca6c190020efb59530323dfbd55030/untitled161.ipynb?authuser=1)..Thanks !", "Hi @iprovalo ! The above error seems to be fixed in the 2.8 version. Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/1c3d2d8885a6ce8a4699dcfafedd99a9/untitled161.ipynb#scrollTo=0wu1ILqtnNgV) for reference.  Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Hi @iprovalo ! The above error seems to be fixed in the 2.8 version. Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/1c3d2d8885a6ce8a4699dcfafedd99a9/untitled161.ipynb#scrollTo=0wu1ILqtnNgV) for reference. Thank you!\r\n\r\nThank you, @mohantym !", "Ok @iprovalo ! Closing this issue as it has been resolved in the 2.8 version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47016\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47016\">No</a>\n"]}, {"number": 47015, "title": "[tf.data] eager mode support for experimental benchmarks part2", "body": "This PR is a follow up of https://github.com/tensorflow/tensorflow/pull/46761 and extends the eager mode support to the remaining subset of experimental benchmarks.\r\n\r\nSample result:\r\n```\r\n# DATASET Based benchmarking\r\nentry {\r\n  name: \"ParallelInterleaveBenchmark.single_cycle_experimental_parallel.eager\"\r\n  iters: 100\r\n  wall_time: 9.162362813949585e-06\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 200000.0\r\n    }\r\n  }\r\n}\r\n\r\nentry {\r\n  name: \"ParallelInterleaveBenchmark.single_cycle_core_parallel.eager\"\r\n  iters: 100\r\n  wall_time: 6.957079768180847e-06\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 200000.0\r\n    }\r\n  }\r\n}\r\n\r\n# OP based benchmarking\r\n\r\nentry {\r\n  name: \"MapDefunBenchmark.with_defun_size_10000\"\r\n  iters: 1\r\n  wall_time: 7.152557373046875e-07\r\n  extras {\r\n    key: \"examples_per_sec\"\r\n    value {\r\n      double_value: 1398101.3333333333\r\n    }\r\n  }\r\n}\r\n\r\nentry {\r\n  name: \"MapDefunBenchmark.without_defun_size_10000\"\r\n  iters: 1\r\n  wall_time: 8.392333984375e-05\r\n  extras {\r\n    key: \"examples_per_sec\"\r\n    value {\r\n      double_value: 11915.636363636364\r\n    }\r\n  }\r\n}\r\n```\r\n\r\ncc: @jsimsa \r\n", "comments": ["@jsimsa thanks for the review. Now that the benchmarks are covered, I will take a look at the work items that you suggested and raise the required PR's. ", "@kvignesh1420 sounds great, thank you very much for your contributions and looking forward to future PRs!"]}, {"number": 47014, "title": "Remove duplicated implementation of keras.backend.ndim", "body": "This PR simplifies `keras.backend.ndim` by reusing the identical implementation from `TensorShape.rank`:\r\nhttps://github.com/tensorflow/tensorflow/blob/7516f6ee75063d8e4f3cfa9d574de64769185de9/tensorflow/python/framework/tensor_shape.py#L825-L830", "comments": []}, {"number": 47013, "title": "Distributed mode is giving lower accuracy than standalone mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Laptop\r\n- TensorFlow installed from (source or binary): No\r\n- TensorFlow version (use command below):  2.4.1\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: ParameterServer Strategy\r\n\r\n**The current behavior**\r\n\r\nI wrote the below code in distributed mode using ParameterServer strategy and got an accuracy of 0.33 you can copy paste the code and execute the code it will work. However, in the standalone mode, it produces 100% accuracy. I believe there is some issue.\r\n\r\n**The expected behavior**\r\nThe accuracy should be the same in standalone and distributed mode as the model and the data is the same.\r\n\r\n**Standalone code to reproduce the issue**\r\n```import multiprocessing\r\nimport os\r\nimport portpicker\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow_hub as hub\r\n\r\nprint (tf.__version__)\r\n#1. Define Workers\r\ndef create_in_process_cluster(num_workers, num_ps):\r\n  \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\r\n  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\r\n  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\r\n\r\n  cluster_dict = {}\r\n  cluster_dict[\"worker\"] = [\"localhost:%s\" % port for port in worker_ports]\r\n  if num_ps > 0:\r\n    cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\r\n\r\n  cluster_spec = tf.train.ClusterSpec(cluster_dict)\r\n\r\n  # Workers need some inter_ops threads to work properly.\r\n  worker_config = tf.compat.v1.ConfigProto()\r\n  if multiprocessing.cpu_count() < num_workers + 1:\r\n    worker_config.inter_op_parallelism_threads = num_workers + 1\r\n\r\n  for i in range(num_workers):\r\n    tf.distribute.Server(\r\n        cluster_spec, job_name=\"worker\", task_index=i, config=worker_config,\r\n        protocol=\"grpc\")\r\n\r\n  for i in range(num_ps):\r\n    tf.distribute.Server(\r\n        cluster_spec, job_name=\"ps\", task_index=i, protocol=\"grpc\")\r\n\r\n  cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\r\n      cluster_spec, task_id=0, task_type=\"worker\",rpc_layer=\"grpc\")\r\n  return cluster_resolver\r\n\r\nNUM_WORKERS = 3\r\nNUM_PS = 2\r\ncluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)\r\n\r\n# Set the environment variable to allow reporting worker and ps failure to the\r\n# coordinator. This is a workaround and won't be necessary in the future.\r\nos.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\n\r\nvariable_partitioner = (\r\n    tf.distribute.experimental.partitioners.FixedShardsPartitioner(\r\n        num_shards=NUM_PS))\r\n\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\r\n\r\nword = \"Elephant\"\r\nsentence = \"I am a sentence for which I would like to get its embedding.\"\r\nparagraph = (\r\n    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\r\n    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\r\n    \"the more 'diluted' the embedding will be.\")\r\nmessages = [word, sentence, paragraph, paragraph]\r\n#labels=[\"1\",\"2\",\"3\"]\r\nreviews = [[1,0,0],[0,1,0],[0,0,1],[0,0,1]]\r\n\r\n\r\nencoder=hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\r\n\r\nX_train=encoder(messages)\r\n\r\n# from sentence_transformers import SentenceTransformer\r\n#\r\n# bertmodel = SentenceTransformer('stsb-roberta-large')\r\n# X_train=bertmodel.encode(messages)\r\n\r\nBUFFER_SIZE = len(X_train)\r\nBATCH_SIZE_PER_REPLICA = 3\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\nEPOCHS = 4\r\n\r\n\r\nwith strategy.scope():\r\n\r\n    model = keras.Sequential()\r\n\r\n    model.add(\r\n        keras.layers.Dense(\r\n            units=256,\r\n            input_shape=(X_train.shape[1],),\r\n            activation='relu'\r\n        )\r\n    )\r\n    model.add(\r\n        keras.layers.Dropout(rate=0.5)\r\n    )\r\n\r\n    model.add(\r\n        keras.layers.Dense(\r\n            units=128,\r\n            activation='relu'\r\n        )\r\n    )\r\n    model.add(\r\n        keras.layers.Dropout(rate=0.5)\r\n    )\r\n\r\n    model.add(keras.layers.Dense(3, activation='softmax'))\r\n    # model.compile(\r\n    #     loss='categorical_crossentropy',\r\n    #     optimizer=keras.optimizers.Adam(0.001),\r\n    #     metrics=['accuracy']\r\n    # )\r\n\r\n    # history = model.fit(\r\n    #     np.array(X_train), np.array(reviews),\r\n    #     epochs=10,\r\n    #     batch_size=16,\r\n    #     verbose=1,\r\n    #     shuffle=True\r\n    # )\r\n    optimizer=keras.optimizers.Adam(0.001)\r\n    accuracy = keras.metrics.Accuracy()\r\n\r\n\r\ndef step_fn(x_train_slice):\r\n    x_train, y_train = next(x_train_slice)\r\n    with tf.GradientTape() as tape:\r\n\r\n        pred=model(x_train,training=True)\r\n        # tf.print(x_train)\r\n        # tf.print(pred)\r\n        # tf.print(y_train)\r\n\r\n        per_example_loss = keras.losses.CategoricalCrossentropy(\r\n            reduction=tf.keras.losses.Reduction.NONE)(y_train, pred)\r\n        loss = tf.nn.compute_average_loss(per_example_loss)\r\n        gradients = tape.gradient(loss, model.trainable_variables)\r\n\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n    # actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\r\n    argmax_pred = tf.one_hot(tf.math.argmax(pred, axis=1), depth=pred.shape[1])\r\n    tf.print(\"train values are\",x_train)\r\n    tf.print(\" pred Values are : \", pred)\r\n    tf.print(\" ArgMAx Values are \",type(tf.math.argmax(pred,axis=0)))\r\n    # tf.print(\" actual_pred Values are : \", actual_pred)\r\n    tf.print(\" Labels  are : \", y_train)\r\n    tf.print(\" Labels Max Values are : \", tf.argmax(y_train))\r\n    tf.print(\" tf.math.argmax(pred, axis=1) \", tf.math.argmax(pred, axis=1))\r\n    tf.print(\"argmax_pred \",argmax_pred)\r\n    accuracy.update_state(y_train, argmax_pred)\r\n    tf.print(\"Accuracy is : \",accuracy.result())\r\n\r\n    return (loss,tf.math.argmax(pred,axis=0))\r\n\r\n@tf.function\r\ndef distributed_train_step(per_worker_iterator):\r\n    (losses,argmaxes) = strategy.run(step_fn,args=(per_worker_iterator,))\r\n    strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)\r\n    return argmaxes\r\n\r\n\r\n@tf.function\r\ndef per_worker_dataset_fn():\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, reviews)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\r\n    print(train_dataset)\r\n    tf.print(\"train_dataset \",train_dataset)\r\n    # test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\r\n    train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\n    # test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\r\n    return train_dist_dataset\r\n\r\n\r\ncoordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(strategy)\r\nper_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\r\nper_worker_iterator = iter(per_worker_dataset)\r\nnum_epoches = 5\r\nsteps_per_epoch = 1\r\nfor i in range(num_epoches):\r\n  accuracy.reset_states()\r\n  for _ in range(steps_per_epoch):\r\n    argmaxes=coordinator.schedule(distributed_train_step, args=(per_worker_iterator,))\r\n\r\n    # Wait at epoch boundaries.\r\n  coordinator.join()\r\n  print(\"argmaxes\", argmaxes.fetch())\r\n  print (\"Finished epoch %d, accuracy is %f.\",(i,accuracy.result().numpy()))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nFinished epoch %d, accuracy is %f. (4, 0.33333334)\r\n", "comments": ["@cmuniyappa,\r\nCould you please provide the individual scripts you have run for ParameterServer strategy and standalone mode.\r\n\r\nAlso, please provide the exact sequence of commands and steps that you executed before running into the problem. Thanks!", "@amahendrakar The above is the ParameterServer strategy code. And the standalone code is below\r\n\r\n```\r\nimport tensorflow.keras as keras\r\nimport tensorflow_hub as hub\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nword = \"Elephant\"\r\nsentence = \"I am a sentence for which I would like to get its embedding.\"\r\nparagraph = (\r\n    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\r\n    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\r\n    \"the more 'diluted' the embedding will be.\")\r\nmessages = [word, sentence, paragraph]\r\nreviews = [[1,0,0],[0,1,0],[0,0,1]]\r\n\r\nencoder=hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\r\n\r\nX_train=encoder(messages)\r\n\r\n# def UniversalEmbeddingsHook(sentences):\r\n#      print(X_train.shape[1])\r\n#      return X_train\r\n#\r\n# # with strategy.scope():\r\n# inputSentences=keras.layers.Input(shape=(1,),dtype=tf.string)\r\n# embeddings=keras.layers.Lambda(UniversalEmbeddingsHook,output_shape=(512,))(inputSentences)\r\n# dense=keras.layers.Dense(256,activation='relu')(embeddings)\r\n# dropouts=keras.layers.Dropout(rate=0.5)(dense)\r\n# ll=keras.layers.Dense(128,activation='relu')(dropouts)\r\n# pred=keras.layers.Dense(3,activation='softmax')(ll)\r\n# models=keras.Model(inputs=[inputSentences],outputs=pred)\r\n# models.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n# models.summary()\r\n# models.fit(messages,reviews,epochs=60)\r\n\r\n\r\n\r\nmodel = keras.Sequential()\r\n\r\nmodel.add(\r\n  keras.layers.Dense(\r\n    units=256,\r\n    input_shape=(X_train.shape[1], ),\r\n    activation='relu'\r\n  )\r\n)\r\nmodel.add(\r\n  keras.layers.Dropout(rate=0.5)\r\n)\r\n\r\nmodel.add(\r\n  keras.layers.Dense(\r\n    units=128,\r\n    activation='relu'\r\n  )\r\n)\r\nmodel.add(\r\n  keras.layers.Dropout(rate=0.5)\r\n)\r\n\r\nmodel.add(keras.layers.Dense(3, activation='softmax'))\r\nmodel.compile(\r\n    loss='categorical_crossentropy',\r\n    optimizer=keras.optimizers.Adam(0.001),\r\n    metrics=['accuracy']\r\n)\r\n\r\n\r\nhistory = model.fit(\r\n    np.array(X_train), np.array(reviews),\r\n    epochs=10,\r\n    batch_size=16,\r\n    verbose=1,\r\n    shuffle=True\r\n)\r\nprint(history)\r\n\r\ntf.print(\"Accuracy \",model.evaluate(np.array(X_train), np.array(reviews)))\r\n\r\nvalidate=X_train[1:2]\r\ny_pred = model.predict(np.array(X_train))\r\nprint(y_pred)\r\nmodel.save(\"/tmp/transfer/classifier/\",history)\r\n\r\n```\r\n\r\nThis produces 99% accuracy; however, distributed code above produces only 33%. Just execute the code with Python 3.8 no other steps required. \r\n\r\nAfter spending sometime, I found that the standalone mode I pass labels and features both in the `model.fit()` call however I don't see labels being passed in the Keras functional API may be that is the reason? How do I pass the labels to distributed code in the `model(x_train)` call without calling the `model.fit`?", "@amahendrakar \r\nOr similar to this https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/parameter_server_training.ipynb#scrollTo=gmPvactfa6Eh. I may have to write Keras functional model, but I don't know how to pass the labels. Even in this example, I don't see the labels beings passed only the feature is passed to train the data. Any idea how this is working? Is there an example for Categorical classification like the one above?", "Hi @cmuniyappa, Parameter Server Strategy currently does not work with the Keras compile/fit API. So if you want to use this strategy you will need to write a custom training loop. To get a good benchmark of performance, you should first make sure that your custom training loop code is equivalent to the compile/fit code and then from there add the distribution strategy.\r\n\r\nYou can learn more about custom training loops [in this guide here,](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch) which shows a classification example.\r\n\r\nAnd you can learn more about using custom training loops with distribution strategies [here.](https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_custom_training_loops)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47013\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47013\">No</a>\n"]}, {"number": 47012, "title": "Simplify tf.keras.activations.softmax codepath", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/35199 although TFLite has already optimized that pattern into single softmax op. Since `tf.nn.softmax` supports input of any rank, there is no reason to have other implementations. For common 3D input with `axis=-1`, `tf.nn.softmax` is also faster than original implementation.\r\n\r\nhttps://colab.research.google.com/drive/1E1rS69LJ7e-pH4bcyfxEJ_wC6IiRISvS?usp=sharing", "comments": ["Hum, it seems that this PR gets rolled back since it breaks one of the internal test. Let me take a look and fix forward."]}, {"number": 47011, "title": "Simplify shape inference in Keras resize_images", "body": "This PR simplifies shape inference of `keras.backend.resize_images`, since the shape of the output tensor is already correctly set in `image_ops.resize_images_v2`:\r\nhttps://github.com/tensorflow/tensorflow/blob/0921a80d56107e19c5c1cebf9500a683d7d41b10/tensorflow/python/ops/image_ops_impl.py#L1390\r\n\r\nThis also removes the need for the `tf.shape` op in cases where the spatial shape of the input tensor is fully defined which fixes #25086.", "comments": ["Please add a unit test!", "@mattdangerw the correct output shape computations should already be covered in the following test cases\r\nhttps://github.com/tensorflow/tensorflow/blob/c4d6c698022f1527fc786ad498f5b1866694ca33/tensorflow/python/keras/backend_test.py#L599-L615\r\n\r\nBut let me know if there is still a case I am missing.", "Look like we are missing a case for a partially defined shape to me. Something that exercises the else block in your change.", "@mattdangerw Sorry about that, I added a test case for dynamic input shapes."]}, {"number": 47010, "title": "Construct sparse tensor using keras inputs (keras symbolic tensor)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): r2.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWe want to construct sparse tensors using keras inputs (dens inputs, not sparse inputs). We pass dense_shape, values, and indices as dense inputs and will be using these three to construct a sparse tensor. Particularly, we dont want to use keras inputs with sparse flag on as keras models with sparse input can not be export for serving directly (see this [issue](https://github.com/tensorflow/tensorflow/issues/42018) ).\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nUsers need to build and serve models with sparse tensors\r\n**Any Other info.**\r\n#42018 ", "comments": ["Hi @stephenzwj ,\r\n\r\nTo treat TF APIs as if they are Keras layers during model construction, we rely on the existing internal tf dispatching mechanism for CompositeTensors (e.g. raggedtensors, sparsetensors) to allow for custom behaviors when built-in tf apis see unknown CompositeTensor types.\r\n\r\nBecause `__new__` doesn't call `__init__` but the failure triggers in `__init__`, we can't trigger the current fallback-based dispatching approach on SparseTensor instance construction. And, the fact that tf.SparseTensor is a class means we can't change the api to just be a method for backwards compatibility reasons.\r\n\r\nSo, this isn't fixable (without bad hacks) until the proactive type-checking dispatching described in the extensiontypes rfc is implemented: https://github.com/tensorflow/community/pull/269/files#diff-d7a12dc407449b1bdf3121f46aebaaf1\r\n\r\n\r\nYou should be able to work around this for now by:\r\n1. putting the tf.SparseTensor creation inside of a keras lambda layer\r\n2. Putting the tf.SparseTensor creation inside of a custom layer\r\n3. If you're okay with using the nightlies, you can pass a type_spec that matches your needs directly to tf.keras.Input creation. This should be available in TF 2.5\r\n", "Closing this issue since its addressed above. Feel free to reopen if necessary. Thanks!"]}, {"number": 47009, "title": "[TFLite] Fix resize bilinear tests by raising a too low error_threshold when align_corners is true", "body": "Hi,\r\n\r\nThe '//tensorflow/lite/kernels/internal:resize_bilinear_test' is failing on our side with the following errors since commit 297ffd8b0b827b2c61352a2d2e181f481738e0c4:\r\n```\r\ntensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\nExpected: (relative_error) < (error_threshold), actual: 0.000531915 vs 0.0003\r\ntensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\nExpected: (relative_error) < (error_threshold), actual: 0.000400641 vs 0.0003\r\ntensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\nExpected: (relative_error) < (error_threshold), actual: 0.000674764 vs 0.0003\r\ntensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\n Expected: (relative_error) < (error_threshold), actual: 0.000558036 vs 0.0003\r\ntensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\nExpected: (relative_error) < (error_threshold), actual: 0.000300481 vs 0.0003\r\n```\r\n\r\nThe threshold when comparing the reference and optimized resize bilinear kernel with align_corners is too low. This PR raises it to 1e-3 as it seems the kernel is correct and the problem comes from the too tight threshold.\r\n\r\nThibaut", "comments": ["@Tessil Is this PR required because the current optimized kernels in TFLite have not succeeded with your system and configuration only? Or are you optimizing the kernels further?\r\n\r\nIf this failure happens at your environment only, could you share your testing environment's details to understand the problem better?", "It fails on our CI with the latest commit in the TF master branch without any further change. \r\n\r\nIt also fails locally on my desktop though the error is slightly different:\r\n```\r\ntensorflow/lite/kernels/internal/resize_bilinear_test.cc:73: Failure\r\nExpected: (relative_error) < (error_threshold), actual: 0.000365497 vs 0.0003\r\n```\r\n\r\nThe CI uses the `tensorflow/tensorflow:custom-op-gpu-ubuntu16` docker image with `USE_NEON` enabled. My desktop uses g++ 7.5.0 without `USE_NEON` (Ubuntu 18.04).\r\n\r\nRaising the `kTestsToRun` generates more similar errors on our side (can be useful for a quick test)."]}, {"number": 47008, "title": "RNN + CUDA crashing with a generator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home Edition\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 10.1 and 7.6.5\r\n- GPU model and memory: GeForce GTX 1650 4GB\r\n\r\n**Describe the current behavior**\r\n\r\nI have multiple .npy files that cannot be loaded into memory all at once. I created a generator that would shuffle the files, load them in batches and use them for training. It works well on CPU, but on GPU it gives an error:\r\n\r\n> InternalError:    Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2, 3, 1, 99, 103830, 0] \r\n\t [[{{node cond_40/then/_0/cond/CudnnRNNV3}}]]\r\n\t [[sequential_1/gru_1/PartitionedCall]] [Op:__inference_train_function_9121]\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n\r\nIf I generate the data with next() function call, I can make training happen (obviously only on the current set of files generated). So the generator behaves properly and I can train on whatever's yielded out of it.. but not when I put the generator inside the fit method.\r\n\r\nsame happens on Google Colab.\r\n\r\nI have looked into other issues that show a very similar error, however only one of them was related to generators: https://github.com/tensorflow/tensorflow/issues/44553 .Otherwise, the error does not happen to me.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would like to be able to pass the generated dataset into the .fit method directly and train it.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nthe code for generator\r\n```\r\nno_of_files = len(x_tr_f)\r\nfile_batch = 5\r\nloop_iterations = np.floor(no_of_files / file_batch)\r\ninit_alpha = 47.05906039370126\r\nmask_value = -1.3371337\r\n\r\ndef tf_data_generator5(directory = [], batch_size = 5):\r\n    i = 0\r\n    x_t = os.listdir(directory[0])\r\n    y_t = os.listdir(directory[1])\r\n    x_t, y_t = shuffle(x_t, y_t)\r\n    while True:\r\n        if i*batch_size >= len(x_t):\r\n            i = 0\r\n            x_t, y_t = shuffle(x_t, y_t)\r\n        file_chunk = x_t[i*batch_size:(i+1)*batch_size] \r\n        X_a = []\r\n        Y_a = []\r\n        for fname in file_chunk:\r\n            x_info = np.load(directory[0]+fname)\r\n            y_info = np.load(directory[1]+fname)\r\n            X_a.append(x_info)\r\n            Y_a.append(y_info)\r\n        X_a = np.concatenate(X_a)\r\n        Y_a = np.concatenate(Y_a)\r\n       #just some masking stuff and weight generation, irrelevant to the problem\r\n        tte_mean_train = np.nanmean(Y_a[:,:,0])\r\n        mask_value = -1.3371337\r\n        X_a,Y_a, W_a = nanmask_to_keras_mask(X_a,Y_a,mask_value,tte_mean_train)\r\n        yield X_a,Y_a\r\n        i = i + 1\r\n```\r\n\r\nHere I generate the data\r\n```\r\ngenerated_train_data = tf_data_generator2(['./data/x_train_m/', './data/y_train_m/'], batch_size = 5)\r\n```\r\n\r\nHere I define the model (using WTTE-RNN):\r\n```\r\ndef base_model():\r\n    model = Sequential()\r\n    model.add(Masking(mask_value=mask_value,input_shape=(None, 2)))\r\n    model.add(GRU(3,activation='tanh',return_sequences=True))\r\n    return model\r\ndef wtte_rnn():\r\n    model = base_model()\r\n    model.add(TimeDistributed(Dense(2)))\r\n    model.add(Lambda(wtte.output_lambda, \r\n                     arguments={\"init_alpha\":init_alpha, \r\n                                \"max_beta_value\":4.0,\r\n                                \"alpha_kernel_scalefactor\":0.5}))\r\n\r\n    loss = wtte.loss(kind='discrete',reduce_loss=False).loss_function\r\n    model.compile(loss=loss, optimizer=Adam(lr=.01,clipvalue=0.5),sample_weight_mode='temporal')\r\n    return model\r\n```\r\n\r\nAnd finally I am trying to fit:\r\n\r\n```\r\nmodel = wtte_rnn()\r\nmodel.summary()\r\n\r\nK.set_value(model.optimizer.lr, 0.01)\r\nmodel.fit(generated_train_data,\r\n          epochs=100,\r\n          steps_per_epoch=loop_iterations)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-3-bcd428d0a8b2> in <module>\r\n      3 \r\n      4 K.set_value(model.optimizer.lr, 0.01)\r\n----> 5 model.fit(generated_train_data,\r\n      6           epochs=10,\r\n      7           verbose=1,\r\nC:\\Anaconda3\\envs\\tf-gpu-test2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\nC:\\Anaconda3\\envs\\tf-gpu-test2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1096                 batch_size=batch_size):\r\n   1097               callbacks.on_train_batch_begin(step)\r\n-> 1098               tmp_logs = train_function(iterator)\r\n   1099               if data_handler.should_sync:\r\n   1100                 context.async_wait()\r\nC:\\Anaconda3\\envs\\tf-gpu-test2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\nC:\\Anaconda3\\envs\\tf-gpu-test2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    838         # Lifting succeeded, so variables are initialized and we can run the\r\n    839         # stateless function.\r\n--> 840         return self._stateless_fn(*args, **kwds)\r\n    841     else:\r\n    842       canon_args, canon_kwds = \\\r\nC:\\Anaconda3\\envs\\tf-gpu-test2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2827     with self._lock:\r\n   2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2830 \r\n   2831   @property\r\nC:\\Anaconda3\\envs\\tf-gpu-test2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _filtered_call(self, args, kwargs, cancellation_manager)\r\n   1841       `args` and `kwargs`.\r\n   1842     \"\"\"\r\n-> 1843     return self._call_flat(\r\n   1844         [t for t in nest.flatten((args, kwargs), expand_composites=True)\r\n   1845          if isinstance(t, (ops.Tensor,\r\nC:\\Anaconda3\\envs\\tf-gpu-test2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1921         and executing_eagerly):\r\n   1922       # No tape is watching; skip to running the function.\r\n-> 1923       return self._build_call_outputs(self._inference_function.call(\r\n   1924           ctx, args, cancellation_manager=cancellation_manager))\r\n   1925     forward_backward = self._select_forward_and_backward_functions(\r\nC:\\Anaconda3\\envs\\tf-gpu-test2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543       with _InterpolateFunctionError(self):\r\n    544         if cancellation_manager is None:\r\n--> 545           outputs = execute.execute(\r\n    546               str(self.signature.name),\r\n    547               num_outputs=self._num_outputs,\r\nC:\\Anaconda3\\envs\\tf-gpu-test2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57   try:\r\n     58     ctx.ensure_initialized()\r\n---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\nInternalError:    Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2, 3, 1, 99, 108196, 0] \r\n\t [[{{node cond_40/then/_0/cond/CudnnRNNV3}}]]\r\n\t [[sequential/gru/PartitionedCall]] [Op:__inference_train_function_7361]\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```", "comments": ["@ILikeNNs \r\n\r\nPlease, share the colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ravikyram please find all the code with supporting files in this repo: https://github.com/ILikeNNs/tensorflow_problem\r\n\r\nOne package that is necessary is wtte (but I put everything in environment.yml for you to be able to compare)", "@ILikeNNs,\r\nCould you please install the latest stable version of **TensorFlow v2.4.1** along with **CUDA 11.0** and **cuDNN 8** and check if it helps.\r\n\r\nAlso, try limiting the total GPU memory using any of the methods listed [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if you're facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47008\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47008\">No</a>\n"]}, {"number": 47007, "title": "[TF Lite] How to use XNNPACK delegate on Windows?", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.5 (nightly)\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): Visual C++ 2019\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nI was successfully building tensorflow lite on Windows, for CPU only, not with any delegate. I also verified that I was able to against `tensorflowlite_c.dll.if.lib` and to execute a tf lite model and produce the correct outputs.\r\n\r\nThen I rebuilt tensorflow lite with the `xnnpack` delegate. The build was successful. However, now, when I try to build my test program and link against `tensorflowlite_c.dll.if.lib`, I now get:\r\n\r\n![err1](https://user-images.githubusercontent.com/67419721/107218568-c80e4780-69dd-11eb-89f2-cebdc47dad81.PNG)\r\n\r\nI tried to add `xnnpack_delegate.lib` to the list of library dependencies, only to get more linker errors:\r\n\r\n![err2](https://user-images.githubusercontent.com/67419721/107218646-e2482580-69dd-11eb-960a-9ac33144f1e2.PNG)\r\n\r\nI also tried linking against of libs, like `c_api.lo.lib` or `common.lo.lib`, but it made no difference. \r\nAnybody know which `.lib` files should I be linking against on Windows?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThe command I used to build TF Lite with xnnpack delegate was:\r\n```\r\nbazel build -c opt --cxxopt=-std=c++14 --define tflite_with_xnnpack=true //tensorflow/lite/c:tensorflowlite_c.dll\r\n```\r\nWhen I ran the `configure.py` script, I left everything to default, with the only exception that I changed the `-c opt` flag from default `/arch:AVX` to `/arch:AVX512`. ", "comments": ["@multiverse-tf Would you be able to answer the XNNPack question or could you find someone who can answer? Thanks", "@DwayneDuane \r\nCan you try updating to the latest stable version 2.6.0 and let us know if the issue still persists? ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "You get unresolved symbols because `TfLiteXNNPack*()` stuff doesn't get exported into the DLL on Windows. See #51882 for a fix, you can safely apply it against 2.6.0.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47007\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47007\">No</a>\n"]}, {"number": 47006, "title": "Remove unnecessary set_model overwrite in BackupAndRestore calllback", "body": "`keras.callbacks.BackupAndRestore` subclasses `keras.callbacks.Callback` which already implements `set_model()` so there is no need to reimplement it here.\r\nhttps://github.com/tensorflow/tensorflow/blob/bbeb7264026e216eba9f0479017d429e2e9d06f6/tensorflow/python/keras/callbacks.py#L636-L637\r\n\r\nThis PR removes the overwrite, as it doesn't change the behaviour of the code.", "comments": []}, {"number": 47005, "title": "Don't run nightly update job in forks.", "body": "There are currently ~84000 forks of TensorFlow on GitHub, and this job currently runs daily in all of them (or at least the ones which were last synced more recently than this job was added).\r\n\r\nAs I understand it, the point of this job is to run nightly so that the TF team can run a suite of nightly tests, so there's no reason for it to happen anywhere other than the upstream `tensorflow/tensorflow` repo - please correct me if I'm wrong about this!\r\n\r\nLooking at the list of [runs for the last few days](https://github.com/tensorflow/tensorflow/actions?query=workflow%3A%22Set+nightly+branch+to+master+HEAD%22), some jobs were under 20 seconds but some were more than 10 minutes. That's an awful lot of unnecessary compute that's being used performing the job on forks.", "comments": []}, {"number": 47004, "title": "Error in keras.tokenizer.texts_to_sequences", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nWhen few texts are given to the keras.tokenizer.texts_to_sequences, it can produce the right sequences but when we have loarge number of texts, it produces wrong sequences.\r\n\r\n**Describe the expected behavior**\r\nExpect the same \"number of non zero values\" for few texts and large texts.\r\n\r\nIn example below, for \"few texts\", rows 0 and 5 have 2 and 4 non-zero values respectively, but for \"texts\" that we have many rows (as shown below) for these rows we have 1 and 3 non-zero values (the first 8 rows in \"texts\" are the same as in \"few texts\"). These are just two rows. There are many rows like this.\r\n\r\n**Standalone code to reproduce the issue**\r\n[samples.txt](https://github.com/tensorflow/tensorflow/files/5942989/samples.txt)\r\n\r\n```from keras.preprocessing.text import Tokenizer\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nimport numpy as np\r\nimport pickle\r\n\r\nfew_texts=['product market',\r\n           'business marketing',\r\n           'entrepreneur business',\r\n           'invest money',\r\n           'money invest .',\r\n           'business money investment investing',\r\n           'strategies market .',\r\n           'marketing investment . .']\r\n\r\n\r\ntokenizer = Tokenizer(num_words=25)\r\ntokenizer.fit_on_texts(few_texts)\r\nsequences = tokenizer.texts_to_sequences(few_texts)\r\nprint('sequences:',sequences)\r\nword_index = tokenizer.word_index\r\ndata = pad_sequences(sequences, maxlen=25, padding='post')\r\nprint('Shape of data tensor:', data.shape)\r\ndata\r\n```\r\nsequences: [[7, 3], [1, 4], [8, 1], [5, 2], [2, 5], [1, 2, 6, 9], [10, 3], [4, 6]]\r\nShape of data tensor: (8, 25)\r\narray([[ 7,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n       [ 1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n       [ 8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n       [ 5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n       [ 2,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n       [ 1,  2,  6,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n       [10,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n       [ 4,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0,  0,  0,  0]])\r\n\r\n```\r\nwith open(\"samples.txt\", \"rb\") as fp:   \r\n    seed = pickle.load(fp)\r\ntexts=seed\r\nlen(texts)\r\n```\r\n1000\r\n```\r\ntexts[0],texts[1],texts[2],texts[3],texts[4],texts[5],texts[6],texts[7]\r\n```\r\n('product market',\r\n 'business marketing',\r\n 'entrepreneur business',\r\n 'invest money',\r\n 'money invest .',\r\n 'business money investment investing',\r\n 'strategies market .',\r\n 'marketing investment . .')\r\n```\r\ntokenizer = Tokenizer(num_words=25)\r\ntokenizer.fit_on_texts(texts)\r\nsequences = tokenizer.texts_to_sequences(texts)\r\n#print('sequences:',sequences)\r\nword_index = tokenizer.word_index\r\n#print('Found %s unique tokens.' % len(word_index))\r\n\r\ndata = pad_sequences(sequences, maxlen=25, padding='post')\r\n#print(data)\r\nprint('Shape of data tensor:', data.shape)\r\n\r\ndata[0],data[5]\r\n```\r\nShape of data tensor: (1000, 25)\r\n(array([5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0]),\r\n array([ 1,  3, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0,  0,  0]))", "comments": ["@JafarMansouri \r\nThe stand alone code seems incomplete to replicate the issue reported, with syntax errors as well.\r\nPlease share stand alone code such that we can replicate the issue reported, or a colab gist with code and the error to analyse.", "@Saduf2019 I edited", "I ran the code shared on tf 2.4 and nightly please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e7042dfc76ce20efc2273eadfcd93f38/untitled523.ipynb)", "@JafarMansouri @Saduf2019 Since you used num_words=25, it would truncate the number of unique words to 25 or keep atmost 25 words (if no. of unique words > 25) from the input dataset based on the word frequencies.\r\nIn the 1st dataset, the number of unique words being less than 25 caused no issue.\r\nIn the 2nd dataset (samples.txt), the number of unique words are greater than 25. So it filtered out 25 words based on their frequencies and tokenized them. Hence, upon encountering any new word in \"text_to_sequence\" function, it would return 0 as its tokenized value . \r\n", "Yes, that was the reason of the error. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47004\">No</a>\n"]}, {"number": 47003, "title": "Update pooling.py", "body": "Updated the value of strides argument in example snippet.\r\n\r\nFixes the change as suggested in #46998.", "comments": []}, {"number": 47002, "title": "Import tensorflow -> Illegal instruction", "body": "System information |  Version\r\n|----------------|---------|\r\nOS Platform | Ubuntu 18.04 LTS\r\nDevice | Orange Pi 4\r\nPython version | 3.6\r\nInstalled using | pip\r\n\r\nHi everyone!\r\nFind only one work solution for me:\r\n```\r\npip3 install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.8.0-py3-none-any.whl\r\n<...>\r\nInstalling collected packages: tensorflow\r\nSuccessfully installed tensorflow-1.8.0\r\n```\r\nAnd:\r\n```\r\norangepi@orangepi4:~$ python3.6.9\r\n>>> import tensorflow\r\nIllegal instruction (core dumped)\r\n```\r\nI am installing version 1.5.0 and the problem persists\r\n", "comments": ["@RarDay \r\n\r\nPlease, see tested build configuration from [here](https://www.tensorflow.org/install/source#cpu).\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!\r\n", "@ravikyram Sorry, I don't understand a little how to download \"tensorflow-2.4.0\" build and run it\r\nI must use doker? CPU or GPU?", "@RarDay \r\n\r\nPlease refer this [link](https://hub.docker.com/r/tensorflow/tensorflow/) and see if it helps you. Thanks!", "@ravikyram  Can I install tensorflow without docker in python 3.6? I'm using an orange pi and I don't have that much memory and time to constantly install docker", "@RarDay \r\n\r\nYou can install Tensorflow from source and using pip.Please, refer this [document.](https://www.tensorflow.org/install).\r\n\r\nBuilding TF on orangepi refer this [document](https://github.com/snowsquizy/tensorflow-on-orange-pi/blob/master/GUIDE.md) and [link](https://poddingue.medium.com/installing-tensorflow-on-the-orange-pi-4b-63428377499cBuilding). Thanks!", "@RarDay,\r\nAny updates regarding this issue?\r\n\r\nPlease feel free to close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47002\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47002\">No</a>\n"]}, {"number": 47001, "title": "Tensorflow 2.4.1running with cpu but not gpu", "body": "Hi all,\r\n\r\nI am trying to set up an environment on WSL2(windows 10) to run my deep learning project with Tensorflow on GPU.\r\nPlease see below for my system and related packages I have installed:\r\n##\r\nMain OS: Windows Pro, Insider Preview (21301.1010)\r\nGPU : Nvidia 1060 3GB\r\nCPU : I5-2500\r\n##\r\nSubsystem: Ubuntu20.04 LTS ( install from windows store)\r\nTensorflow: 2.4.1\r\nPython: 3.8.5\r\nCUDA and other necessary toolkits were installed as following url:\r\n[Tensorflow gpu support guide](https://www.tensorflow.org/install/gpu), in the section of **Ubuntu 18.04 (CUDA 11.0)**.\r\n_p.s. Although I am using Ubuntu20.04(WSL2), tensorflow was not able to detect my gpu when I installed CUDA toolkit for Ubuntu20.04. Therefore, I followed the guide of Ubuntu18.04 and it worked fine._\r\n##\r\n\r\nEverything worked fine at the beginning, all tests showing that tensorflow was able to detect my gpu device.\r\nHowever, when I ran [Tensorflow sample code](https://www.tensorflow.org/tutorials/quickstart/beginner?hl=zh_tw), the message showed that tensorflow created a gpu device but running on cpu when code executed to model.fit\r\n\r\n`2021-02-08 16:32:23.944331: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-08 16:32:26.591081: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 16:32:26.599408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-02-08 16:32:26.902834: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 16:32:26.903131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 3GB computeCapability: 6.1\r\ncoreClock: 1.7085GHz coreCount: 9 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2021-02-08 16:32:26.903196: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-08 16:32:26.907413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-08 16:32:26.907593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-08 16:32:26.910460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-08 16:32:26.911433: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-08 16:32:26.915959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-08 16:32:26.916820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-02-08 16:32:26.917147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-08 16:32:26.918042: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 16:32:26.918932: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 16:32:26.919158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-08 16:32:26.920533: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 16:32:26.921406: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 16:32:26.921676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 3GB computeCapability: 6.1\r\ncoreClock: 1.7085GHz coreCount: 9 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2021-02-08 16:32:26.921762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-08 16:32:26.921851: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-08 16:32:26.921914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-08 16:32:26.921976: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-08 16:32:26.922038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-08 16:32:26.922073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-08 16:32:26.922133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-02-08 16:32:26.922195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-08 16:32:26.923004: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 16:32:26.923906: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 16:32:26.924155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-08 16:32:26.924245: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-08 16:32:29.425230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-08 16:32:29.425326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-02-08 16:32:29.425343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-02-08 16:32:29.427287: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 16:32:29.427562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1489] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\r\n2021-02-08 16:32:29.428461: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 16:32:29.429563: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.`\r\n\r\n### Created TensorFlow device successfully\r\n**2021-02-08 16:32:29.429890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2074 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)**\r\n\r\n`2021-02-08 16:32:29.888992: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-08 16:32:31.514620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-08 16:32:32.694044: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188160000 exceeds 10% of free system memory.\r\n2021-02-08 16:32:33.834778: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)`\r\n\r\n### Model was running on cpu but not cpu\r\n**2021-02-08 16:32:33.836445: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3292520000 Hz**\r\n\r\nIs there anything I did not execute properly? I would like to utilize my gpu power on my deep learning project but tensorflow only work on my cpu but not gpu even it showed it created a tensorflow device.\r\n\r\nAny comments and suggestion are appreciate. Thanks you all.\r\n", "comments": ["@stevetsaoch,\r\nLooks like TensorFlow has successfully detected the GPU on your machine. Could you please run the below code snippet and share the output with us\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\n\r\nThanks!", "@amahendrakar, \r\nThanks for your reply, here is the output: \r\n`2021-02-08 18:59:28.276199: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-08 18:59:31.780809: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 18:59:31.794955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-02-08 18:59:32.147151: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 18:59:32.147798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 3GB computeCapability: 6.1\r\ncoreClock: 1.7085GHz coreCount: 9 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2021-02-08 18:59:32.147888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-08 18:59:32.177582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-08 18:59:32.177808: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-08 18:59:32.188968: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-08 18:59:32.190904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-08 18:59:32.222702: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-08 18:59:32.226548: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-02-08 18:59:32.228268: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-08 18:59:32.229280: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 18:59:32.230369: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2021-02-08 18:59:32.230930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\nNum GPUs Available:  1`", "@amahendrakar,\r\nHere are some other further information might help, I ran some code and also watching task manager at the same time. Please see screenshot below when I ran code:\r\n![image](https://user-images.githubusercontent.com/49383776/107227328-355dbf00-6a56-11eb-8634-5a54e3db9786.png)\r\n\r\nAs you can see, RAM in my GPU was occupied but power of cores was not utilized, is this a normal situation? Or should the \"3D\", \"Copy\", \"Video Enocode\" and \"Video Decode\" showed higher percentage of usage when I training a model?\r\nBesides, when I test with some easy code like : \r\n\r\n```\r\nimport tensorflow as tf\r\nwith tf.device('/GPU:0'):\r\nA = tf.constant([[3, 2], [5, 2]])\r\nprint(tf.eye(2,2))\r\n````\r\nThere was a peak showed up in both \"3D\" and \"Copy\" when the code is executed, which seems TensorFlow uses GPU for the test.\r\n\r\nAt last, the screenshot of my nvidia-smi showed ERR! on GPU-Util,\r\n\r\n![image](https://user-images.githubusercontent.com/49383776/107231515-5b399280-6a5b-11eb-8043-0a4c8bfa0227.png)\r\n\r\nHope these information will help.\r\n\r\nBest.\r\n", "> Num GPUs Available: 1\r\n\r\n@stevetsaoch,\r\nAs we can see from the logs, TensorFlow is able to detect the GPU on your machine.\r\n\r\nInstead of the Task Manager, please use the [memory profiler tool](https://www.tensorflow.org/guide/profiler#memory_profile_summary) to monitor GPU usage. Thanks!  ", "@amahendrakar \r\nThank you, I will try it out later. For now, I managed to utilize my GPU power with TensorFlow on my windows main system and it worked perfectly. I will stay on Windows10 main system to run my project and try your suggestion to monitor whether GPU is utilized when model(with TensorFlow) was executed on WSL2. \r\n\r\nAnyway, thank you for your kindly assistance, please close this issue.", "@stevetsaoch,\r\nThank you for the update. \r\n\r\nMarking this issue as closed. Please feel free to re-open if necessary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47001\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47001\">No</a>\n", "![Screenshot from 2021-03-12 10-50-20](https://user-images.githubusercontent.com/36214675/110884903-c9a29680-8320-11eb-9606-ebcd4fc55a37.png)\r\nI ran into a similar problem. I don't know if I am using GPU. the GPU MEM is high, but POW is low. and my CPU is 44% high.\r\n![Screenshot from 2021-03-12 10-54-36](https://user-images.githubusercontent.com/36214675/110885194-6107e980-8321-11eb-872b-334998ea3a7a.png)\r\nanother question is why this warn occurs: \"Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.16GiB with freed_by_count=0.\"\r\n\r\n", "@amahendrakar please re-open~", "@ccl-private,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 47000, "title": "tf.train.Server and mirrored strategy..", "body": "Hello I am a new user of tensorflow.\r\nI am implementing one reinforcement learning algorithm using tf2 ,I have it's implementation with tf1 there the authors have used tf.train.Server for the distributed learning I want to use tf.distribute.MirroredStrategy to do the distributed training. But I am not able to link these 2 methods the \"tf.train.Server\" and \"tf.distribute.MirroredStrategy\". can anyone help to understand the difference between these and how to go about it? Thanks you..\r\n\r\n", "comments": ["Hi @Pranav-India, please see the [guide here for an overview of distribution strategies in TF](https://www.tensorflow.org/guide/distributed_training) and how to use them. I took a look at the [2.4 ](https://www.tensorflow.org/api_docs/python/tf/train)and [1.15](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train) API docs and cannot find `tf.train.Server`, so I'm guessing that symbol is deprecated. \r\n\r\nClosing this issue now as Github is for bugs/performance issues. Support requests are best handled on Stack Overflow where there is a larger community.", "Hi @nikitamaia . Thank you for the suggestion, I have asked it on the stack overflow as well hope to find an answer I am looking for."]}, {"number": 46999, "title": "How could I know the label is correctly read from image_dataset_from_directory?", "body": "I'd save the directory structure as indicated in document, separate classes of images stored in corresponding folder with corresponding label as folder name.\r\n\r\nHowever, I could not find a way to return the labels read from image_dataset_from_directory function, and I'm afraid the class name is not correctly read.\r\n\r\nThe performance of my training is that, after 10 training epochs the training and validate accuracy reached 0.99, but the model prediction would only predict one class.\r\n\r\nHow could I check the label read by this function?\r\n", "comments": ["@laurence-lin \r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "> @laurence-lin\r\n> \r\n> This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n\r\nAlright, because I couldn't find the solution on stackoverflow. Thanks!", "You can find the labels in the `class_names` attribute on the datasets created by `image_dataset_from_directory` .\r\n\r\nIf your directory structure is like;\r\n```python\r\n  main_directory/\r\n  ...class_a/\r\n  ......a_image_1.jpg\r\n  ......a_image_2.jpg\r\n  ...class_b/\r\n  ......b_image_1.jpg\r\n  ......b_image_2.jpg\r\n  ```\r\n\r\n```python\r\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir, validation_split=0.2, \r\n                  subset=\"training\", seed=123,  \r\n                  image_size=(img_height, img_width), batch_size=batch_size)\r\n# It outputs something like:\r\nFound X files belonging to 2 classes.\r\nUsing X files for training.\r\n```\r\nThese classes correspond to the directory names in alphabetical order and can be inferred as,\r\n```python\r\nclass_names = train_ds.class_names\r\nprint(class_names)\r\n# class_a, class_b\r\n```"]}, {"number": 46998, "title": "Documentation mistake in tf.keras.layers.MaxPool2D python section", "body": "## URL(s) with the issue: \r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D\r\n\r\n## Description of issue (what needs changing):\r\n\r\nA Documentation mistake in tf.keras.layers.MaxPool2D python section(example)\r\n\r\n### Clear description\r\n\r\nThere is a documentation mistake in tf.keras.layers.MaxPool2D python section(example). \r\nIn the description part of there are few examples given with their respective code snippets.\r\nUnder this text-\"For example, for stride=(2,2) and padding=\"valid\":\" in the code snippet of example instead of **strides=(2,2)**, **strides=(1,1)** is written.\r\n\r\n\r\n<img width=\"939\" alt=\"Screenshot 2021-02-08 at 10 16 05 AM\" src=\"https://user-images.githubusercontent.com/38719024/107178001-33b1de00-69f9-11eb-8c7a-6114457686c0.png\">\r\n", "comments": ["@chinya07,\r\nThank you for reporting the error. \r\n\r\nThe change is being tracked in PR [#47003](https://github.com/tensorflow/tensorflow/pull/47003) and will reflect as soon as it gets merged."]}, {"number": 46997, "title": "hello0 alternative error message for failed shape broadcasting", "body": "This just ensures that the test also passes with alternative generated kernels.\r\n\r\nPiperOrigin-RevId: 355814751\r\nChange-Id: I2a148427ebc059244c9f94ba91e294c460bb5544", "comments": []}, {"number": 46994, "title": "tf.nn.avg_pool/1d/2d/3d outputs NaN if `ksize=0`", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A:\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nThe following APIs output NaN if `ksize=0`\r\n- `tf.nn.avg_pool1d`\r\n- `tf.nn.avg_pool2d`\r\n- `tf.nn.avg_pool3d`\r\n- `tf.nn.avg_pool`\r\n\r\n\r\n**Describe the expected behavior**\r\nExpect a grace exception message if the input is unexpected(e.g. `ksize=0`) other than a NaN as output\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.nn.avg_pool1d(input=np.ones((1, 1, 1)), ksize=0, strides=1, padding='SAME')\r\ntf.nn.avg_pool2d(input=np.ones((1, 1, 1, 1)), ksize=0, strides=1, padding='SAME')\r\ntf.nn.avg_pool3d(input=np.ones((1, 1, 1, 1, 1)), ksize=0, strides=1, padding='SAME')\r\ntf.nn.avg_pool(input=np.ones((1, 1, 1)), ksize=0, strides=1, padding='SAME')\r\n~~~\r\n\r\nThe output of `tf.nn.avg_pool1d`\r\n~~~python\r\n<tf.Tensor: shape=(1, 1, 1), dtype=float64, numpy=array([[[nan]]])>\r\n~~~\r\n", "comments": ["Was able to reproduce the error with TF v2.3, TF v2.4 and TF-nightly for `tf.nn.avg_pool1d`, `tf.nn.avg_pool2d` and `tf.nn.avg_pool`. Whereas, `tf.nn.avg_pool3d` throws an error stating `NotFoundError: Could not find device for node: {{node AvgPool3D}}`\r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3440b1b620422cd182cbe75a70e75cc1/46994.ipynb). Thanks!", "The associated PR is merged and this should be now fixed with tomorrows tf-nightly version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46994\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46994\">No</a>\n"]}, {"number": 46993, "title": "`tf.keras.layers.ELU` and  `tf.keras.layers.LeakyReLU` outputs nan if `alpha=None`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.keras.layers.ELU` and  `tf.keras.layers.LeakyReLU` outputs nan if `alpha=None`\r\n\r\n\r\n**Describe the expected behavior**\r\nexpect no nan as output\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nlayer = tf.keras.layers.ELU(alpha=None)\r\nout=layer(np.array([-2, 6.]))\r\nprint(out)\r\n~~~\r\n\r\nOutput:\r\n~~~\r\ntf.Tensor([nan  6.], shape=(2,), dtype=float32)\r\n~~~\r\n\r\n\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nlayer = tf.keras.layers.LeakyReLU(alpha=None)\r\nout=layer(np.array([-2, 6]))\r\n~~~\r\nOutput:\r\n~~~\r\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan,  6.], dtype=float32)>\r\n~~~\r\n\r\n\r\nRelated: #13787", "comments": ["I am able to replicate the issue reported on tf 2.4 and tf-nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/cdb1c001d0fc15d73735806bba9a2cbd/untitled522.ipynb).\r\nThanks!\r\n\r\n", "I think when `alpha=None` is passed a ValueError could be thrown. Added a PR #47017 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46993\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46993\">No</a>\n"]}, {"number": 46992, "title": "Converting tf.argmax(predicted,axis=0) to numpy array to index the y_labels", "body": "I am new to Tensorflow and wrote the following distributed training code. The code works fine.\r\n\r\n    import multiprocessing\r\n    import os\r\n    import portpicker\r\n    import tensorflow as tf\r\n    import tensorflow.keras as keras\r\n    import tensorflow_hub as hub\r\n    import tensorflow.python.keras.backend as K\r\n    #1. Define Workers\r\n    def create_in_process_cluster(num_workers, num_ps):\r\n      \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\r\n      worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\r\n      ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\r\n    \r\n      cluster_dict = {}\r\n      cluster_dict[\"worker\"] = [\"localhost:%s\" % port for port in worker_ports]\r\n      if num_ps > 0:\r\n        cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\r\n    \r\n      cluster_spec = tf.train.ClusterSpec(cluster_dict)\r\n    \r\n      # Workers need some inter_ops threads to work properly.\r\n      worker_config = tf.compat.v1.ConfigProto()\r\n      if multiprocessing.cpu_count() < num_workers + 1:\r\n        worker_config.inter_op_parallelism_threads = num_workers + 1\r\n    \r\n      for i in range(num_workers):\r\n        tf.distribute.Server(\r\n            cluster_spec, job_name=\"worker\", task_index=i, config=worker_config,\r\n            protocol=\"grpc\")\r\n    \r\n      for i in range(num_ps):\r\n        tf.distribute.Server(\r\n            cluster_spec, job_name=\"ps\", task_index=i, protocol=\"grpc\")\r\n    \r\n      cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\r\n          cluster_spec, task_id=0, task_type=\"worker\",rpc_layer=\"grpc\")\r\n      return cluster_resolver\r\n    \r\n    NUM_WORKERS = 3\r\n    NUM_PS = 2\r\n    cluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)\r\n    \r\n    # Set the environment variable to allow reporting worker and ps failure to the\r\n    # coordinator. This is a workaround and won't be necessary in the future.\r\n    os.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\n    \r\n    variable_partitioner = (\r\n        tf.distribute.experimental.partitioners.FixedShardsPartitioner(\r\n            num_shards=NUM_PS))\r\n    \r\n    strategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\r\n    \r\n    word = \"Elephant\"\r\n    sentence = \"I am a sentence for which I would like to get its embedding.\"\r\n    paragraph = (\r\n        \"Universal Sentence Encoder embeddings also support short paragraphs. \"\r\n        \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\r\n        \"the more 'diluted' the embedding will be.\")\r\n    messages = [word, sentence, paragraph]\r\n    #labels=[\"1\",\"2\",\"3\"]\r\n    reviews = [[1,0,0],[0,1,0],[0,0,1]]\r\n    \r\n    \r\n    encoder=hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\r\n    \r\n    X_train=encoder(messages)\r\n    \r\n    BUFFER_SIZE = len(X_train)\r\n    BATCH_SIZE_PER_REPLICA = 2\r\n    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n    EPOCHS = 4\r\n    \r\n    \r\n    with strategy.scope():\r\n    \r\n        model = keras.Sequential()\r\n    \r\n        model.add(\r\n            keras.layers.Dense(\r\n                units=256,\r\n                input_shape=(X_train.shape[1],),\r\n                activation='relu'\r\n            )\r\n        )\r\n        model.add(\r\n            keras.layers.Dropout(rate=0.5)\r\n        )\r\n    \r\n        model.add(\r\n            keras.layers.Dense(\r\n                units=128,\r\n                activation='relu'\r\n            )\r\n        )\r\n        model.add(\r\n            keras.layers.Dropout(rate=0.5)\r\n        )\r\n    \r\n        model.add(keras.layers.Dense(3, activation='softmax'))\r\n        # model.compile(\r\n        #     loss='categorical_crossentropy',\r\n        #     optimizer=keras.optimizers.Adam(0.001),\r\n        #     metrics=['accuracy']\r\n        # )\r\n    \r\n        # history = model.fit(\r\n        #     np.array(X_train), np.array(reviews),\r\n        #     epochs=10,\r\n        #     batch_size=16,\r\n        #     verbose=1,\r\n        #     shuffle=True\r\n        # )\r\n        optimizer=keras.optimizers.Adam(0.001)\r\n        accuracy = keras.metrics.Accuracy()\r\n    \r\n    \r\n    def step_fn(x_train_slice):\r\n    \r\n        x_train, y_train = next(x_train_slice)\r\n        with tf.GradientTape() as tape:\r\n            pred=model(x_train,training=True)\r\n            # tf.print(x_train)\r\n            # tf.print(pred)\r\n            # tf.print(y_train)\r\n    \r\n            per_example_loss = keras.losses.CategoricalCrossentropy(\r\n                reduction=tf.keras.losses.Reduction.NONE)(y_train, pred)\r\n            loss = tf.nn.compute_average_loss(per_example_loss)\r\n            gradients = tape.gradient(loss, model.trainable_variables)\r\n    \r\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n        actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\r\n        tf.print(\"train values are\",x_train)\r\n        tf.print(\" pred Values are : \", pred)\r\n        tf.print(\" ArgMAx Values are \",tf.math.argmax(pred,axis=0)) #problem\r\n        tf.print(\" actual_pred Values are : \", actual_pred)\r\n        tf.print(\" Labels  are : \", y_train)\r\n        tf.print(\" Labels Max Values are : \", tf.argmax(y_train))\r\n        accuracy.update_state(y_train, actual_pred)\r\n        tf.print(\"Accuracy is : \",accuracy.result())\r\n        return loss\r\n    \r\n    @tf.function\r\n    def distributed_train_step(x_train_slice):\r\n        losses = strategy.run(step_fn,args=(x_train_slice,))\r\n        return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)\r\n    \r\n    \r\n    @tf.function\r\n    def per_worker_dataset_fn():\r\n        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, reviews)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\r\n        # test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\r\n        train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\n        # test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\r\n        return train_dist_dataset\r\n    \r\n    \r\n    coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(strategy)\r\n    per_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\r\n    per_worker_iterator = iter(per_worker_dataset)\r\n    num_epoches = 5\r\n    steps_per_epoch = 1\r\n    for i in range(num_epoches):\r\n      accuracy.reset_states()\r\n      for _ in range(steps_per_epoch):\r\n        coordinator.schedule(distributed_train_step, args=(per_worker_iterator,))\r\n        # Wait at epoch boundaries.\r\n      coordinator.join()\r\n      print (\"Finished epoch %d, accuracy is %f.\",(i,accuracy.result().numpy()))\r\n\r\nThe problem is, in the step_fn once I get the prediction values I would like to get the corresponding labels, for this I have used this line of code\r\n`tf.print(\" ArgMAx Values are \",tf.math.argmax(pred,axis=0)) #problem`\r\n\r\nThe argmax gives the array of indices for max probabilities. I would like to extract this as numpy array and index it to reviews array (One-Hot encoded values) to get the confusion matrix. \r\n\r\nBut I'm not able to convert `tf.math.argmax(pred,axis=0)` tensor to numpy array. I tried many approaches like eval(K.get_session()) and so on but nothing worked. Any help is appreciated.\r\n\r\nThanks much", "comments": ["I have already tried \r\n1. https://stackoverflow.com/questions/34097281/convert-a-tensor-to-numpy-array-in-tensorflow#34097344\r\n2.reported the same on Stackoverflow https://stackoverflow.com/questions/66094011/convert-tf-argmax-results-to-numpy-array\r\n\r\nThe suggestion was to use gather but wasn't sure how ? Stuck on this for Two days now. Could someone please help me out?\r\nThanks", "@cmuniyappa \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know which TF version you are using?\r\nThanks!", "@ravikyram there was nothing much in the template format for  Other issues support so wrote the above. The version is `2.4.1`", "@ravikyram @jvishnuvardhan based on this https://stackoverflow.com/questions/66094011/convert-tf-argmax-results-to-numpy-array?noredirect=1#comment116855383_66094011 I was able to resolve the issue now the accuracy is computed correctly. However, the accuracy is low for few slices it is giving 0.33 could you please let me know how to improve the accuracy? ", "@ravikyram @jvishnuvardhan Any updates on this?", "Hi @cmuniyappa, Github is a place for bugs. For support, it's best to use Stack Overflow.\r\nI would suggest you try an experiment to see what the accuracy is without a distribution strategy. This can help you understand if the model is not converging due to the distribution, or if it's something due to your model/data/etc. If your model is underfitting to the training data, then you might want to try training for longer or increasing the complexity of your model.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46992\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46992\">No</a>\n"]}, {"number": 46991, "title": "[TFL] Fix unbounded activation clip range.", "body": "Fixes #45312. For floating point with unbounded activation such as relu and none, set max/min to positive/negative infinity in case infinity is clipped. The change only affects floating point as `std::numeric_limits<T>::has_infinity` is always false for non-floating point. Doing `min(max(x, -inf), inf)` is safe because positive infinity is larger than all values except itself and NaN and negative infinity is smaller than all values expect itself and NaN. With the change, TFLite is the same with TF for relu and none activation when input is positive or negative infinity.\r\n\r\nhttps://www.gnu.org/software/libc/manual/html_node/Infinity-and-NaN.html\r\nhttps://en.cppreference.com/w/cpp/types/numeric_limits/has_infinity", "comments": ["This PR will be hold until we can invest to make a thorough investigation about how current ops are handling inf value to ensure there is no compatibility issues."]}, {"number": 46990, "title": "Yet Another -1073740791 (0xC0000409)", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: `pip freeze` gave me this:\r\n```\r\ntensorboard==2.4.1\r\ntensorboard-plugin-wit==1.7.0\r\ntensorflow==2.4.0\r\ntensorflow-estimator==2.4.0\r\ntensorflow-gpu==2.4.1\r\n```\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: ```nvcc --version``` gave me this:\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020\r\nCuda compilation tools, release 11.0, V11.0.194\r\nBuild cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0\r\n\r\n```\r\nAs referenced here - \r\n\r\n*The main reason is cuda version information is not stored in ```cudnn.h``` anymore in CUDA 11.X. In CUDA 11.X, the version information seems to stored in  new ```cudnn_version.h``` file. So many build tools such as cmake depends on ```cudnn.h``` for CUDA version information cannot guess CUDA version anymore.*\r\n\r\n_Originally posted by @hongsoog in https://github.com/tensorflow/tensorflow/issues/8264#issuecomment-723446642_\r\n\r\nAm sure its cudnn 11.2 (The name of zip file)\r\n\r\n- GPU model and memory:\r\n\r\nGTX 1650 8 GB\r\n\r\n**Describe the problem**\r\n\r\nSo I have been trying to accelerate my Deep Learning program using GPU since ages, for the first time I was able to find my GPU and after going through the whole process, I ran\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\nIt found my GPU\r\n\r\nResult - \r\n```\r\nC:\\Users\\asus\\Music_Classification\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\r\nC:\\Users\\asus\\Music_Classification\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\r\n  warnings.warn(\"loaded more than 1 DLL from .libs:\"\r\n2021-02-08 04:01:40.630572: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-08 04:01:45.995174: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 04:01:45.997294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-02-08 04:01:46.037247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2021-02-08 04:01:46.038203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-08 04:01:46.086937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-08 04:01:46.087068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-02-08 04:01:46.117926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-02-08 04:01:46.127343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-02-08 04:01:46.196088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-02-08 04:01:46.215917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\nNum GPUs Available:  1\r\n2021-02-08 04:01:46.219540: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-08 04:01:46.220028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n```\r\n\r\n**Question 1** - Funny enough it found 4gig of memory only, is this because of some Shared vs DEdicated memory?\r\n\r\nI then went on to train my CNN model, and it gave me traceback:\r\n\r\n```\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n```\r\n\r\nThe whole error:\r\n```\r\n2021-02-08 04:05:05.982102: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 04:05:05.982898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-02-08 04:05:06.021203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2021-02-08 04:05:06.021514: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-08 04:05:06.029225: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-08 04:05:06.029425: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-02-08 04:05:06.034370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-02-08 04:05:06.036450: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-02-08 04:05:06.047235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-02-08 04:05:06.050880: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-02-08 04:05:06.051983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-08 04:05:06.052288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-08 04:05:06.060950: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-02-08 04:05:06.064289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2021-02-08 04:05:06.064478: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-08 04:05:06.064582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-08 04:05:06.064710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-02-08 04:05:06.064841: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-02-08 04:05:06.064948: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-02-08 04:05:06.065057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-02-08 04:05:06.065150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-02-08 04:05:06.065240: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-08 04:05:06.065392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-08 04:05:07.277891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-08 04:05:07.278012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-02-08 04:05:07.278075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-02-08 04:05:07.279959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2903 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2021-02-08 04:05:07.284087: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n  0%|          | 0/8091 [00:00<?, ?it/s]\r\nC:/Users/asus/PycharmProjects/Auto_Caption_Generator/caption_generator_collab.py:139: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\r\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\r\n  for img in tqdm(os.listdir(directory)):\r\n2021-02-08 04:05:09.583142: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-02-08 04:05:10.755831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n```\r\n\r\n**Problem 2** Can someone please help me with this error.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nSummed up above\r\n\r\n**Any other info / logs**\r\nPip Freeze \r\n```\r\nabsl-py==0.11.0\r\nappdirs==1.4.4\r\nargon2-cffi==20.1.0\r\nastunparse==1.6.3\r\nasync-generator==1.10\r\nattrs==20.3.0\r\naudioread==2.1.9\r\nbackcall==0.2.0\r\nbleach==3.2.1\r\ncachetools==4.2.0\r\ncertifi==2020.12.5\r\ncffi==1.14.4\r\nchardet==4.0.0\r\ncolorama==0.4.4\r\ncycler==0.10.0\r\ndecorator==4.4.2\r\ndefusedxml==0.6.0\r\nentrypoints==0.3\r\nflatbuffers==1.12\r\ngast==0.3.3\r\ngoogle-auth==1.24.0\r\ngoogle-auth-oauthlib==0.4.2\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.32.0\r\nh5py==2.10.0\r\nidna==2.10\r\nipykernel==5.4.3\r\nipython==7.19.0\r\nipython-genutils==0.2.0\r\nipywidgets==7.6.3\r\njedi==0.18.0\r\nJinja2==2.11.2\r\njoblib==1.0.0\r\njsonschema==3.2.0\r\njupyter==1.0.0\r\njupyter-client==6.1.11\r\njupyter-console==6.2.0\r\njupyter-core==4.7.0\r\njupyter-http-over-ws==0.0.8\r\njupyterlab-pygments==0.1.2\r\njupyterlab-widgets==1.0.0\r\nKeras==2.4.3\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.3.1\r\nlibrosa==0.8.0\r\nllvmlite==0.35.0\r\nMarkdown==3.3.3\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.3.3\r\nmistune==0.8.4\r\nnbclient==0.5.1\r\nnbconvert==6.0.7\r\nnbformat==5.1.2\r\nnest-asyncio==1.4.3\r\nnotebook==6.2.0\r\nnumba==0.52.0\r\nnumpy==1.19.5\r\noauthlib==3.1.0\r\nopt-einsum==3.3.0\r\npackaging==20.8\r\npandas==1.2.0\r\npandocfilters==1.4.3\r\nparso==0.8.1\r\npickleshare==0.7.5\r\nPillow==8.0.1\r\npooch==1.3.0\r\nprometheus-client==0.9.0\r\nprompt-toolkit==3.0.10\r\nprotobuf==3.14.0\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npycparser==2.20\r\nPygments==2.7.4\r\npyparsing==2.4.7\r\nPyQt5==5.15.2\r\nPyQt5-sip==12.8.1\r\npyrsistent==0.17.3\r\npython-dateutil==2.8.1\r\npython-speech-features==0.6\r\npytz==2020.5\r\npywin32==300\r\npywinpty==0.5.7\r\nPyYAML==5.3.1\r\npyzmq==21.0.1\r\nqtconsole==5.0.1\r\nQtPy==1.9.0\r\nrequests==2.25.1\r\nrequests-oauthlib==1.3.0\r\nresampy==0.2.2\r\nrsa==4.7\r\nscikit-learn==0.24.0\r\nscipy==1.6.0\r\nSend2Trash==1.5.0\r\nsix==1.15.0\r\nSoundFile==0.10.3.post1\r\ntensorboard==2.4.1\r\ntensorboard-plugin-wit==1.7.0\r\ntensorflow==2.4.0\r\ntensorflow-estimator==2.4.0\r\ntensorflow-gpu==2.4.1\r\ntermcolor==1.1.0\r\nterminado==0.9.2\r\ntestpath==0.4.4\r\nthreadpoolctl==2.1.0\r\ntornado==6.1\r\ntqdm==4.56.0\r\ntraitlets==5.0.5\r\ntyping-extensions==3.7.4.3\r\nurllib3==1.26.2\r\nwcwidth==0.2.5\r\nwebencodings==0.5.1\r\nWerkzeug==1.0.1\r\nwidgetsnbextension==3.5.1\r\nwrapt==1.12.1\r\n```\r\n", "comments": ["I went through multiple Stackoverflow threads to sort this out, sorry but had to post it here finally.", "Event Viewer Log:\r\n\r\n```\r\nFaulting application name: python.exe, version: 3.8.5150.1013, time stamp: 0x5f15bf71\r\nFaulting module name: cudnn64_8.dll, version: 6.14.11.6050, time stamp: 0x5eded186\r\nException code: 0xc0000409\r\nFault offset: 0x00000000000185fd\r\nFaulting process id: 0x3264\r\nFaulting application start time: 0x01d6fd9adb2269a7\r\nFaulting application path: C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\r\nFaulting module path: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\\cudnn64_8.dll\r\nReport Id: 4f3820ce-9d62-4173-b17a-7adebfdb6edc\r\nFaulting package full name: \r\nFaulting package-relative application ID: \r\n```", "Update:\r\nI tried running on CLI and found this error - \r\n\r\n```\r\nould not load library cudnn_ops_infer64_8.dll. Error code 126\r\nPlease make sure cudnn_ops_infer64_8.dll is in your library path!\r\n```\r\nI followed this comment:\r\n\r\n@ymodak \r\nUPDATE:\r\n\r\nFor whatever reason when run in the IDE terminal an error message was being suppressed and ```Process finished with exit code -1073740791 (0xC0000409)``` was logged as the error message. \r\n\r\nWhen run from the command line the below error messages were displayed instead of logging the exit code error.\r\n```\r\nCould not load library cudnn_ops_infer64_8.dll. Error code 126\r\nPlease make sure cudnn_ops_infer64_8.dll is in your library path!\r\n```\r\nI recognized this was a package included in the cudnn library and copy and pasted it from the bin folder in cudnn to NVIDIA GPU computing toolkit > CUDA > V11.0 > bin. This process was repeated for the below packages and the issue was resolved.\r\n```\r\ncudnn_adv_infer64_8.dll\r\ncudnn_adv_train64_8.dll\r\ncudnn_cnn_infer64_8.dll\r\ncudnn_cnn_train64_8.dll\r\ncudnn_ops_infer64_8.dll\r\ncudnn_ops_train64_8.dll\r\n```\r\n\r\n_Originally posted by @taylrcawte in https://github.com/tensorflow/tensorflow/issues/45611#issuecomment-757488650_\r\n\r\n\r\nStill, the problem persists", "I started with a Clean Slate.\r\n\r\nThis time upgraded the `tensorflow` using pip to match `tensorflow-gpu`\r\n\r\nMoreover Reinstalled cuDNN, followed these [steps](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#:~:text=Navigate%20to%20your%20directory,Unzip%20the%20cuDNN%20package.&text=v8.x.x.x.zip-,Copy%20the%20following%20files%20into%20the%20CUDA%20Toolkit%20directory.,cuda%5Cbin%20%5Ccudnn*.) apart from the last step, i.e. this one `Include cudnn.lib in your Visual Studio project`, cause i dont use VS code, i use PyCharm IDE, i have VS Code but not sure what was this all about, so skipped. Moreover the version this time is `cudnn-11.0-windows-x64-v8.0.5.39`.\r\n\r\nIt can identify my GPu, but throws the error this time:\r\n\r\n\r\n```\r\n2021-02-08 14:06:47.748712: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-02-08 14:06:48.785624: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-08 14:06:54.090490: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2021-02-08 14:06:54.227258: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2021-02-08 14:06:54.312157: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-08 14:06:55.475591: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:55.667517: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:55.690053: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:55.718915: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:55.831265: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:55.855201: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:55.881479: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:55.913845: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:55.934313: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:55.960155: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:55.998642: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:56.029488: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:56.059281: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:56.093092: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:56.114581: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.40GiB with freed_by_count=0. The cal\r\nler indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-02-08 14:06:56.120274: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.40GiB with freed_by_count=0. The cal\r\nler indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-02-08 14:06:56.143009: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-02-08 14:06:56.185661: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n```\r\nAnd was stuck in this position till i forcefully interrupted.\r\nI don't know how to proceed now.\r\n\r\n\r\n", "@vybhav72954,\r\nRegarding the `CUBLAS_STATUS_ALLOC_FAILED` error, could you please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/46209#issuecomment-756163315) from issue #46209 with a similar error and check it works. Thanks!", "@amahendrakar I am unable to access my Machine as of now, I will confirm by Wednesday, hopefully won't be a problem.", "I was able to compile my Network one time. Then I tried to train one more network just to be sure.\r\n\r\nNow, this is the error am encountering, this is amusing considering that CUDA did run one time successfully.\r\n```\r\nC:\\Users\\asus\\Music_Classification\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\r\nC:\\Users\\asus\\Music_Classification\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\r\n  warnings.warn(\"loaded more than 1 DLL from .libs:\"\r\n2021-02-09 11:57:00.811686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-09 11:57:03.254087: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-09 11:57:03.255200: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-02-09 11:57:03.291252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2021-02-09 11:57:03.291478: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-09 11:57:03.299423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-09 11:57:03.299925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-02-09 11:57:03.300559: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2021-02-09 11:57:03.301206: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2021-02-09 11:57:03.301934: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2021-02-09 11:57:03.302779: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\r\n2021-02-09 11:57:03.303800: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-09 11:57:03.303899: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\nTraceback (most recent call last):\r\n  File \"C:/Users/asus/PycharmProjects/Auto_Caption_Generator/caption_generator_Inception.py\", line 43, in <module>\r\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\nIndexError: list index out of range\r\n```", "![cuda](https://user-images.githubusercontent.com/49750343/107324771-9f19af80-6ace-11eb-923d-684857274e96.PNG)\r\nI definitely have these libs.\r\n\r\nAnd the bin is in PATH variable.", "Hello @vybhav72954 i was just looking over your original post and i wanted to clarify; are you using a GTX1650 with CUDA 11.2 / cuDNN 8?\r\n\r\nIt looks like tensorflow is trying to load libraries from CUDA 10, since you are using a GTX1650 you may want to revert back to tensorflow 2.3, and CUDA 10 with associated cuDNN (found here https://developer.nvidia.com/rdp/cudnn-archive)\r\n\r\n", "> Hello @vybhav72954 i was just looking over your original post and i wanted to clarify; are you using a GTX1650 with CUDA 11.2 / cuDNN 8?\r\n> \r\n> It looks like tensorflow is trying to load libraries from CUDA 10, since you are using a GTX1650 you may want to revert back to tensorflow 2.3, and CUDA 10 with associated cuDNN (found here [ https://developer.nvidia.com/rdp/cudnn-archive](url)).\r\n\r\nIs that so? Thank you for the suggestions.\r\n\r\nI would implement the suggested changes and would report here.\r\n\r\nOne more thing, I believe that the URL is broken, in the case in future, someone will try to access that, they won't reach the required webpage, would be great if you correct that \ud83d\ude04 \r\n", "Updated!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46990\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46990\">No</a>\n"]}, {"number": 46989, "title": "Failed to deploy custom Tensorflow lite select ops .aar to Android", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Huawei P30 Lite\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: master - 2.4.1+\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI have followed the instructions here: https://www.tensorflow.org/lite/guide/build_android and here: https://www.tensorflow.org/lite/guide/reduce_binary_size to build custom versions of tensorflow-lite.aar and tensorflow-lite-select-tf-ops.aar. However, when I try to run the model on Android, I get this error:\r\n\r\n```\r\njava.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():\r\n      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZN6tflite3ops6custom38Register_TFLITE_DETECTION_POST_PROCESSEv\" referenced by \"/data/app/com.example.myapplication-2NF_GHdjAnMJHFkmSCcPvw==/lib/arm64/libtensorflowlite_jni.so\"...\r\n        at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:80)\r\n        at com.example.myapplication.MainActivity.onCreate(MainActivity.java:26)\r\n```\r\n\r\nThis appears to be happening before loading the model, it seems to be a linking issue, though I have just build the two .aar files from source.\r\n\r\nSteps followed:\r\n\r\nI'm using Docker to build the aar files. I downloaded the Docker file from [here](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile) and ran:\r\n`docker build . -t tflite-builder -f tflite-android.Dockerfile`\r\nfollowed by:\r\n`docker run -it -v \"local_dir\":/host_dir tflite-builder bash`\r\n\r\nInside the docker container, I ran:\r\n`curl -o build_aar_with_docker.sh   https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/tools/build_aar_with_docker.sh &&   chmod +x build_aar_with_docker.sh`\r\n\r\nfollowed by:\r\n`echo y | /bin/bash build_aar_with_docker.sh --input_models=/host_dir/my_model.tflite   --target_archs=arm64-v8a,armeabi-v7a   --checkpoint=master `\r\n\r\nMy model is defined (here)[https://github.com/barrypitman/tensorflow_LPRnet] and uses 1 custom op - ctc_beam_search_decoder. \r\n\r\nAfter several hours, the build finished with this message:\r\n```\r\nINFO: Analyzed target //tmp:tensorflow-lite-select-tf-ops (308 packages loaded, 33657 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tmp:tensorflow-lite-select-tf-ops up-to-date:\r\n  bazel-bin/tmp/tensorflow-lite-select-tf-ops.aar\r\nINFO: Elapsed time: 35622.438s, Critical Path: 3942.80s\r\nINFO: 14237 processes: 861 internal, 13376 local.\r\nINFO: Build completed successfully, 14237 total actions\r\nOutput can be found here:\r\ntensorflow-lite.aar\r\ntensorflow-lite-select-tf-ops.aar\r\n```\r\n\r\nI copied the tensorflow-lite.aar and tensorflow-lite-select-tf-ops.aar files into a sample Android application (attached), and I got the above-mentioned error:\r\n```\r\njava.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():\r\n      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZN6tflite3ops6custom38Register_TFLITE_DETECTION_POST_PROCESSEv\" referenced by \"/data/app/com.example.myapplication-2NF_GHdjAnMJHFkmSCcPvw==/lib/arm64/libtensorflowlite_jni.so\"...\r\n```\r\n\r\nI've tried generating the tensorflow-lite-select-tf-ops.aar file against the 2.4.0 and 2.4.1 branches, but I'm getting similar errors. This issue appears to be similar to [45153](https://github.com/tensorflow/tensorflow/issues/45153). \r\n\r\nI built the two .aar files against the following tensorflow commit:\r\n```\r\nroot@8eb4e1fb67bf:/tensorflow_src# git rev-parse HEAD\r\n6ac306c372287e522168da6931fd751203915d46\r\n```\r\n\r\nI've attached the two .aar files as well as the demo Android app. I haven't included the model, because the app is crashing before trying to load the model.\r\n[tensorflow-lite-master-20210207-docker.zip](https://github.com/tensorflow/tensorflow/files/5939881/tensorflow-lite-master-20210207-docker.zip)\r\n[AndroidHelloWorld.zip](https://github.com/tensorflow/tensorflow/files/5939884/AndroidHelloWorld.zip)\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Copy & pasting my comment from 45153 in case this solves your problem too: I found that I had to add `build --config=monolithic` to `.bazelrc` to avoid the `_ZNK6google8protobuf7Message11GetTypeNameEv` error.", "@ojj11 Is this the file? https://github.com/tensorflow/tensorflow/blob/master/.bazelrc\r\nPlease point out in what line do we have to insert your suggestion?", "@ojj11 Is this the file? https://github.com/tensorflow/tensorflow/blob/master/.bazelrc\r\nPlease point out in what line do we have to insert your suggestion?", "> Copy & pasting my comment from 45153 in case this solves your problem too: I found that I had to add `build --config=monolithic` to `.bazelrc` to avoid the `_ZNK6google8protobuf7Message11GetTypeNameEv` error.\r\n\r\nThank you - this has helped, I added the option as you suggested, `echo -n \"build --config=monolithic\" >> /tensorflow_src/.bazelrc`\r\n\r\nMy original issue - \r\n```dlopen failed: cannot locate symbol \"_ZN6tflite3ops6custom38Register_TFLITE_DETECTION_POST_PROCESSEv\" referenced by \"/data/app/com.example.myapplication-2NF_GHdjAnMJHFkmSCcPvw==/lib/arm64/libtensorflowlite_jni.so\"```. \r\nis still happening. This happens when I try to use both the custom `tensorflow-lite.aar` (1.3MB) and `tensorflow-lite-select-tf-ops.aar` (4.4MB) files. It seems that the built libtensorflowlite_jni.so is missing some dependencies, even when built using the monolithic option.\r\n\r\nHowever, if I try to use the custom `tensorflow-lite-select-tf-ops.aar` file with the vanilla `tensorflow-lite-2.4.0.aar` that I downloaded from e.g. jcenter, then its working. Thanks for the suggestion!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46989\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46989\">No</a>\n", "Update: I sent a pull request to update `build_aar.sh`. If you're using this script, you should no longer require the `build --config=monolithic` change in `.bazelrc`. ", "> > Copy & pasting my comment from 45153 in case this solves your problem too: I found that I had to add `build --config=monolithic` to `.bazelrc` to avoid the `_ZNK6google8protobuf7Message11GetTypeNameEv` error.\r\n> \r\n> Thank you - this has helped, I added the option as you suggested, `echo -n \"build --config=monolithic\" >> /tensorflow_src/.bazelrc`\r\n> \r\n> My original issue -\r\n> `dlopen failed: cannot locate symbol \"_ZN6tflite3ops6custom38Register_TFLITE_DETECTION_POST_PROCESSEv\" referenced by \"/data/app/com.example.myapplication-2NF_GHdjAnMJHFkmSCcPvw==/lib/arm64/libtensorflowlite_jni.so\"`.\r\n> is still happening. This happens when I try to use both the custom `tensorflow-lite.aar` (1.3MB) and `tensorflow-lite-select-tf-ops.aar` (4.4MB) files. It seems that the built libtensorflowlite_jni.so is missing some dependencies, even when built using the monolithic option.\r\n> \r\n> However, if I try to use the custom `tensorflow-lite-select-tf-ops.aar` file with the vanilla `tensorflow-lite-2.4.0.aar` that I downloaded from e.g. jcenter, then its working. Thanks for the suggestion!\r\n\r\nCan you share the model? I took a look at your model in AndroidHelloWorld.zip. But the model does not contain DetectionPostProcess custom op.\r\n", "Commit fc85b2000383b09cbe40e3f2565f0b0c1a1cee53 should fix this error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46989\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46989\">No</a>\n", "Thank you @thaink , I can confirm that the custom build is working using both the custom tensorflow-lite.aar and tensorflow-lite-select-tf-ops.aar files.", "Hi @thaink , I am still getting this error after adding ```implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'``` dependency on my android project. The following are the error logs:\r\n```\r\n04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Node number 65 (FlexSize) failed to prepare.\r\n04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Node number 65 (FlexSize) failed to prepare.\r\n04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before infer\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:204)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:378)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.yolo.Yolo.predict(Yolo.java:71)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.POSM.predict(POSM.java:45)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.MainActivity.run_posm(MainActivity.java:110)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.MainActivity.lambda$onCreate$1$MainActivity(MainActivity.java:75)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.-$$Lambda$MainActivity$w2a9VvlIKLLPHcyHZBUHuFlJhQA.onClick(lambda)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at android.view.View.performClick(View.java:4640)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.google.android.material.button.MaterialButton.performClick(MaterialButton.java:1119)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.view.View$PerformClick.run(View.java:19431)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.os.Handler.handleCallback(Handler.java:733)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.os.Handler.dispatchMessage(Handler.java:95)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.os.Looper.loop(Looper.java:146)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.app.ActivityThread.main(ActivityThread.java:5598)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at java.lang.reflect.Method.invokeNative(Native Method)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at java.lang.reflect.Method.invoke(Method.java:515)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1283)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1099)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at dalvik.system.NativeStart.main(Native Method)\r\n04-14 09:18:33.952 8903-8903/com.example.posm E/ERROR: Exception java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 65 (FlexSize) failed to prepare.\r\n    \r\n    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 65 (FlexSize) failed to prepare.\r\n    \r\n    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before infer\r\n```\r\nThe error happened in Android Kitkat 4.4 (API 19), but on my Android 10 (API 29) and Android 5.0 (API 21) it working correctly. Am I missing something?", "@malik-anhar Can you file a new issue and cc me?\r\nYour issue is different that this one."]}]