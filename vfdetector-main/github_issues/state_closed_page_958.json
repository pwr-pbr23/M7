[{"number": 24688, "title": "Fix several DeprecationWarning: invlid escape sequence", "body": "Hello,\r\n\r\nThis little patch fixes several invalid sequence warnings.", "comments": []}, {"number": 24687, "title": "I am getting the following error when exporting keras model to tensorflow", "body": "from tensorflow.python.saved_model import builder as saved_model_builder\r\nfrom tensorflow.python.saved_model import utils\r\nfrom tensorflow.python.saved_model import tag_constants, signature_constants\r\nfrom tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\r\nfrom tensorflow.contrib.session_bundle import exporter\r\n\r\nexport_path = 'plantmodelproduction'\r\nbuilder = saved_model_builder.SavedModelBuilder(export_path)\r\n\r\nsignature = predict_signature_def(inputs={'images': new_model.input},\r\n                                  outputs={'scores': new_model.output})\r\n\r\nwith K.get_session() as sess:\r\n    builder.add_meta_graph_and_variables(sess=sess,\r\n                                         tags=[tag_constants.SERVING],\r\n                                         signature_def_map={'predict': signature})\r\n    builder.save()\r\n\r\n\r\n\r\nINFO:tensorflow:No assets to save.\r\nINFO:tensorflow:No assets to write.\r\nERROR:tensorflow:==================================\r\nObject was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):\r\n<tf.Tensor 'IsVariableInitialized_3579:0' shape=() dtype=bool>\r\nIf you want to mark it as used call its \"mark_used()\" method.\r\nIt was originally created here:\r\n  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3287, in run_code\r\n    return outflag  File \"<ipython-input-96-dc248f1de322>\", line 2, in <module>\r\n    model=load_model('plantdiseasemodel.h5')  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\keras\\engine\\saving.py\", line 422, in load_model\r\n    h5dict.close()  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\keras\\engine\\saving.py\", line 288, in _deserialize_model\r\n    K.batch_set_value(weight_value_tuples)  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2581, in batch_set_value\r\n    get_session().run(assign_ops, feed_dict=feed_dict)  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 206, in get_session\r\n    session.run(tf.variables_initializer(uninitialized_vars))  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 199, in <listcomp>\r\n    [tf.is_variable_initialized(v) for v in candidate_vars])  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 189, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n==================================\r\nERROR:tensorflow:==================================\r\nObject was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):\r\n<tf.Tensor 'IsVariableInitialized_3578:0' shape=() dtype=bool>\r\nIf you want to mark it as used call its \"mark_used()\" method.\r\nIt was originally created here:\r\n  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3287, in run_code\r\n    return outflag  File \"<ipython-input-96-dc248f1de322>\", line 2, in <module>\r\n    model=load_model('plantdiseasemodel.h5')  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\keras\\engine\\saving.py\", line 422, in load_model\r\n    h5dict.close()  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\keras\\engine\\saving.py\", line 288, in _deserialize_model\r\n    K.batch_set_value(weight_value_tuples)  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2581, in batch_set_value\r\n    get_session().run(assign_ops, feed_dict=feed_dict)  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 206, in get_session\r\n    session.run(tf.variables_initializer(uninitialized_vars))  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 199, in <listcomp>\r\n    [tf.is_variable_initialized(v) for v in candidate_vars])  File \"C:\\Users\\isheunesu\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 189, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n==================================", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n\r\n"]}, {"number": 24686, "title": "tf.print  can't print log in jupyter notebook. ", "body": "When I tried to use tf.print() or get the log_device_placement information in google colabratory. I just can't get the print output. I tried in my own jupyter, and I find the print log is in the cmd.exe windows. This behavior is very strange and I think it should be fixed.  The code is like below.\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nprint(sess.run(c))\r\n```\r\n\r\nthe print output is just like this:\r\n\r\n> v1.12.0-0-ga6d8ffae09 1.12.0\r\n[[22. 28.]\r\n [49. 64.]]\r\n\r\nbut it should be with the log_device_placement information.\r\n\r\n", "comments": ["By adding **tf.logging.set_verbosity(tf.logging.INFO)** in your script you should be able to see the logging in the ipython console.", "> By adding **tf.logging.set_verbosity(tf.logging.INFO)** in your script you should be able to see the logging in the ipython console.\r\n\r\nThank you very much. But I tried this, it does't work. The reason of this problem maybe in the cooperation of tensorflow and Jupyter notebook. Because I can see the log information in the cmd.exe window which runs  jupyter.", "I think you're right, @lyhue1991 . It's a known limitation of tf.print, see RFC:\r\n\r\nhttps://github.com/tensorflow/community/pull/14/files#diff-6dc73e00aed7c8a7fbc0f53d7981f296R365", "@facaiy Thank you very much. This limitation is really painful.", "@tomerk Hi, Tomer. Could you answer the question?", "Hi @facaiy what question still needs answering?\r\nAs you mentioned this is still a known limitation, because Jupyter notebooks don't capture C++ output.\r\n\r\nWe've decided to not add python-specific logic at this time (which we could do by running py_funcs internally) because it would prevent universal exporting of models that use `tf.print`\r\n\r\nI've bumped the relevant issue on the ipython kernel's github:\r\nhttps://github.com/ipython/ipykernel/issues/110\r\n\r\nFor anyone who needs a workaround there's a number of scattered code on the internet that tries to redirect C++ outputs to ipython, e.g.\r\nhttps://github.com/minrk/wurlitzer", "Thanks for your detailed explanation, and workaround :-) So can we close the issue, @lyhue1991 ?", "@facaiy Yeah, maybe the best approach to solve this problem is to let ipykernel capture output coming from C and C++ libraries. @tomerk Thank you for your explanation.", "Just an update to this: We've actually come up with a solution to this inside of tensorflow, that should now be in the nightlies.", "If you are here for `tf.Print` in tf1: \r\nWhen you use google colab, you can see the output in the runtime logs (runtime menu).\r\nWhen you use jupyter lab (if it's your server) you need to check your terminal (from which you have launched jupyter lab).", "@Matoran is `tf.print` not currently printing in the cell output in both colab notebooks and jupyter notebooks? It should be, and if it's not we've had a regression and should fix it.\r\n\r\n(Note that `tf.Print` in tf1 is not the same as `tf.print`)", "@tomerk I'm talking about `tf.Print` in tf1. `tf.print` works just fine. I had a bunch of pages talking about `tf.Print` and I answered here thinking it was another page. I will edit my text to avoid any confusion."]}, {"number": 24685, "title": "Image Style Transfer Model not generating fake nodes", "body": "HI,\r\nI am trying to quantize the following model\r\nhttps://github.com/hwalsuklee/tensorflow-fast-style-transfer\r\n\r\n I have trained the model with following \r\ntf.contrib.quantize.create_training_graph(quant_delay=0)\r\nand for Inference I am using the following \r\n tf.contrib.quantize.create_eval_graph()\r\n\r\nI am freezing the graph with following command\r\npython -m tensorflow.python.tools.freeze_graph \\\r\n--input_graph=FST_out.pb \\\r\n--input_checkpoint=q_out_trt1/final.ckpt-14000 \\\r\n--input_binary=true \\\r\n--output_graph=q_out_trt1/frozen_FST.pb \\\r\n--output_node_names=Tanh\r\n\r\nWhat I have noticed after freezing the graph is that there is no Fake Quant Nodes .We have also noticed that most of the models which are generating fake nodes are slim based.\r\nPlease let us know the reason for not generating the fake nodes and if the quantization aware training works only for tf.contrib.slim based model.\r\n\r\nRegards\r\nShailendra", "comments": ["Hi,\r\n\r\nWe have another observation that only when bias is added after convolution than only we are able to \r\nget the fake quantization layer.\r\nIn our network mentioned above we are not able to get the fake layer as there are no bias layer after Convolution layers.But if we add bias layers after convolution we are able to see fake layers.\r\nCan you also please explain the behaviour.?\r\n\r\nI request you to please check and  reply asap.", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)", "Hi ,\r\n System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution** : Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:Binary\r\n- **TensorFlow version (use command below)**:tf1.10\r\n- **Python version**:Python 2.7.14  and Python 3.6.3.\r\n\r\n**Details :**\r\nI am trying to quantize the following model\r\nhttps://github.com/hwalsuklee/tensorflow-fast-style-transfer\r\n\r\nI have trained the model with following\r\ntf.contrib.quantize.create_training_graph(quant_delay=0)\r\nand for Inference I am using the following\r\ntf.contrib.quantize.create_eval_graph()\r\n\r\nI am freezing the graph with following command\r\npython -m tensorflow.python.tools.freeze_graph \r\n--input_graph=FST_out.pb \r\n--input_checkpoint=q_out_trt1/final.ckpt-14000 \r\n--input_binary=true \r\n--output_graph=q_out_trt1/frozen_FST.pb \r\n--output_node_names=Tanh\r\n\r\nWhat I have noticed after freezing the graph is that there is no Fake Quant Nodes .We have also noticed that most of the models which are generating fake nodes are slim based.\r\nPlease let us know the reason for not generating the fake nodes and if the quantization aware training works only for tf.contrib.slim based model.\r\n\r\nWe have another observation that only when bias is added after convolution than only we are able to\r\nget the fake quantization layer.\r\nIn our network mentioned above we are not able to get the fake layer as there are no bias layer after Convolution layers.But if we add bias layers after convolution we are able to see fake layers.\r\nPlease look into the issue.\r\n\r\n**How to train the model** : \r\npython run_train.py --style <style file> --output <output directory> --trainDB <trainDB directory> --vgg_model <model directory>\r\n\r\nYou need to download the Train DB from the link provided in the above mentioned link.\r\n\r\n**How to take inference**\r\npython run_test.py --content <content file> --style_model <style-model file> --output <output file> \r\n\r\nRegards\r\nShailendra Singh Yadav\r\n\r\n", "Hi @petermattson , can you advise who would be able to address a fast style transfer query. It looks like a question about quantization aware training, and whether fast style transfer works like slim model. Thanks.", "Any update on this...?", "Please also find the attached frozen file snippet generated using quantization aware training for further analysis\r\n![frozen](https://user-images.githubusercontent.com/41136882/51240114-1092dc80-19a1-11e9-9322-5763d964075c.PNG)\r\n\r\n \r\n\r\nRegards\r\nShailendra", "Hi Shailendra -- sorry, I am completely unfamiliar with this issue. Maybe start with one of the slim primary contributors since nodes are slim based?", "@Shailendra2803 , can you please communicate with the original contributors of Slim model:\r\nhttps://github.com/tensorflow/models/tree/master/research/slim\r\nThanks.", "@msymp  @petermattson \r\nOur model is not slim based.We are jst comparing with the slim based model for quantization aware training.\r\nYou can ignore all the slim based information provided here.The graph attached is for our model and its not slim based.\r\nMain problem is that when we try to do quantization aware training for our model \r\n![network](https://user-images.githubusercontent.com/41136882/51296670-2b1d9200-1a43-11e9-8de1-b0874be2fc2e.PNG)\r\n\r\nWe don't see any fake node generated.Below are function definitions.\r\n\r\n![functions](https://user-images.githubusercontent.com/41136882/51296994-88fea980-1a44-11e9-9673-bf521972daa7.PNG)\r\n\r\nYou can download the complete source code from following location if required.\r\nhttps://github.com/hwalsuklee/tensorflow-fast-style-transfer\r\n\r\nIn the above when we add a bias layer we can see fake node generated for all except the Transpose Conv.\r\nSnippet of the code after bias layer is added is below\r\n![bias](https://user-images.githubusercontent.com/41136882/51296753-79cb2c00-1a43-11e9-926c-14b8d89eb89e.PNG)\r\n\r\n\r\nSo can you please provide input for the following\r\n1.)Why fake node is not generated for all the layers without addition of bias layer ?\r\n2.)Why fake node is not generated for Transpose conv even when bias layer is added?\r\n\r\n\r\nRegards\r\nShailendra\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@Shailendra2803 i don't know if quantization only works for slim, but there seems to be convolution_transpose layers in style transfer model, which is not yet supported in tf lite quantize aware training. "]}, {"number": 24684, "title": "Large Python call overhead in eager mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6\r\n- GCC/Compiler version (if compiling from source): 4.2.1\r\nv1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n\r\nWe are currently working with Tensorflow Probability as part of PyMC4. The model logp graph is static and rather cheap but the samplers are in python. Thus, for every logp evaluation, we have to evaluate the TF graph. Compared to Theano, this is rather slow (about 20x slower). I suspect that the Python call overhead is just very high, especially in graph mode (see https://github.com/tensorflow/tensorflow/issues/120). \r\n\r\nEager mode makes things better but it's still about 10x slower, even when using defun. I guess I'm asking how much additional work needs to be done other than giving a call hook directly into compiled code and whether that part is optimized at all.\r\n\r\nHere is a notebook with some very basic speed comparisons to Theano: https://gist.github.com/twiecki/43d6b78455ef5812bb90b5522fe7686c\r\n\r\n The difference grows more dramatic in a real-world scenario but the notebook is more involved: https://github.com/aseyboldt/pymc4/blob/london/pymc4_experiment.ipynb", "comments": ["Thanks for the report @twiecki \r\n@derifatives and @iganichev have been looking into \"compiling\" away the per-operation overheads of a `defun` defined function using XLA (motivated by some TensorFlow probability uses as well).\r\n\r\nPerhaps they can provide an update.", "@derifatives has indeed experimented using XLA on CPU to remove TensorFlow Executor's overhead. In his benchmarks, we saw almost 100x improvement. The aesthetics of getting XLA to work well in Eager are pretty bad at the moment and we are actively working through various integration issues.\r\n\r\nIf this is not pressing, we should have a reasonable story for Eager + Defun + XLA in a couple of weeks. If it is, your best bet is probably to:\r\n  - Use graph mode and [make_callable](https://www.tensorflow.org/api_docs/python/tf/Session#make_callable) to minimize session.run overheads.\r\n  - Use [xla.compile](https://www.tensorflow.org/xla/tutorials/xla_compile) API to eliminate TF Executor overhead. This is important if you have a lot of very cheap ops.\r\n\r\nI am not following your exact question \"how much additional work needs to be done other than giving a call hook directly into compiled code\". What is the compiled code? TF does not compile generated C++ files like Theano does.", "@iganichev Thanks, good to hear there is development there.\r\n\r\nI tried your suggestions. `make_callable` actually leads to a small improvement.\r\n\r\nUsing `xla.compile` in eager mode gave:\r\n```\r\nValueError: When eager execution is enabled, use_resource cannot be set to false.\r\n```\r\nI don't set it to `True` anywhere, fwiw.\r\n\r\nand sometimes also:\r\n```\r\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\r\n```\r\n\r\nIn any case, I'm also happy to wait a few weeks and test it again when XLA + Eager play nicely, sounds like that's exactly what I need.", "@twiecki `xla.compile` will not work with eager execution enabled (there are ways to use it inside tf.contrib.eager.defun but I would rather not go into though details). I was suggesting to use graph execution above. Please ping this issue in a couple of weeks for an update.", "Thanks @iganichev . As the work on the Eager mode for XLA / Defun proceeds to completion in a couple of weeks, @twiecki shall resubmit this issue. Closing for now. Thanks.", "@msymp Why reopen this issue? Not challenging the decision, just curious!", "@canyon289 , we keep it open so that @twiecki can rejoin this thread in a couple of weeks after the work on Eager mode completed.", "@msymp any progress so far? ", "Sorry, but nothing significant happened. Other more critical issues have taken precedence. Not sure when somebody will have time to pick this up again.", "What's the current status here? Internally, we've been using xla.compile in\nEager mode extensively and successfully.\n\nOn Fri, Feb 22, 2019 at 2:24 PM Igor Ganichev <notifications@github.com>\nwrote:\n\n> Sorry, but nothing significant happened. Other more critical issues have\n> taken precedence. Not sure when somebody will have time to pick this up\n> again.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24684#issuecomment-466568612>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AY408J0D9LfNtisgVrrq0XUWybHi7Gp9ks5vQG4pgaJpZM4ZoBop>\n> .\n>\n", "@derifatives Do you have a small code example to demonstrate how it can be used?", "Here is an option to request XLA compilation of tf.functions:\r\nhttps://github.com/tensorflow/tensorflow/blob/e3b2203323c578cc9a3e1a5bca51d00c050cb18e/tensorflow/python/eager/def_function.py#L977\r\n\r\nThere is a bit more documentation here:\r\nhttps://github.com/tensorflow/tensorflow/blob/e3b2203323c578cc9a3e1a5bca51d00c050cb18e/tensorflow/python/eager/def_function.py#L344"]}, {"number": 24683, "title": "Replacing the Depricated API", "body": "tf.initialize_all_variables() is not supported replace with tf.global_variables_initializer", "comments": ["@aselle , can you pls review this PR", "@amitsrivastava78 gentle  ping to check reviewer comments", "@rthadur , thanks for the reminder, i just missed these comments, ", "@amitsrivastava78 please resolve conflicts", "@rthadur , this PR is quite old and some one has already merged these changes, so i am closing the PR.\r\n\r\nRegards\r\nAmit"]}, {"number": 24682, "title": "Replacing the Depricated API", "body": "", "comments": []}, {"number": 24681, "title": "Update 00-bug-performance-issue.md", "body": "", "comments": []}, {"number": 24680, "title": "Replacing depricated API", "body": "Replacing the tf.global_variables_initializer() with\r\ntf.global_variables_initializer", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 24679, "title": "Out of bounds error while iterating on the rows of a RaggedTensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=\"1.13.0-dev20181226\" (this is the TF 2.0-preview)\r\nGIT_VERSION=\"b'v1.12.0-5133-gc343196842'\"\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nIterating over the elements of a `RaggedTensor` results in an `tf.errors.InvalidArgumentError` after the last element.\r\n\r\n**Describe the expected behavior**\r\nI expect to be able to iterate over the rows of a `RaggedTensor` without any error.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nr = tf.ragged.constant([[1., 2.], [3., 4., 5.], [6.]])\r\nfor elem in r:\r\n    print(elem)\r\n```\r\n\r\n**Other info / logs**\r\nHere is the output of the program above:\r\n\r\n```\r\ntf.Tensor([1. 2.], shape=(2,), dtype=float32)\r\ntf.Tensor([3. 4. 5.], shape=(3,), dtype=float32)\r\ntf.Tensor([6.], shape=(1,), dtype=float32)\r\n2019-01-03 16:27:36.084103: W tensorflow/core/framework/op_kernel.cc:1408] OP_REQUIRES failed at strided_slice_op.cc:106 : Invalid argument: slice index 3 of dimension 0 out of bounds.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/ragged/ragged_getitem.py\", line 104, in ragged_tensor_getitem\r\n    return _ragged_getitem(self, key)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/ragged/ragged_getitem.py\", line 153, in _ragged_getitem\r\n    row = rt_input.values[starts[row_key]:limits[row_key]]\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 654, in _slice_helper\r\n    name=name)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 820, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 9334, in strided_slice\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: slice index 3 of dimension 0 out of bounds. [Op:StridedSlice] name: RaggedGetItem/strided_slice/\r\n```", "comments": ["Here is a simple workaround:\r\n\r\n```python\r\nfor elem in r.to_list():\r\n    print(elem)\r\n```\r\n\r\nHowever, it moves all the data to Python. To preserve Tensors and remain within TensorFlow:\r\n\r\n```python\r\nfor row_index in tf.range(r.nrows()):\r\n    print(r[row_index])\r\n```\r\n", "The issue was that, the `__getitem__` in RaggedTensor should throw out `IndexError` (vs. `InvalidArgumentError`) in order to allow python to process iteration correctly (translated to `StopIteration` in `next/__next__()`). \r\n\r\nCreated a PR #24723 for the fix.\r\n\r\nThe fix will only work with eager mode as non-eager mode will not be able to find out the out-of-bound errors before session run."]}, {"number": 24678, "title": "Fix quotation typo around \"task\"", "body": "It just replaces inconsistent single quotation marks with backticks.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@alextp  can please check 4 failing issues ?", "They look like unrelated flakes."]}, {"number": 24677, "title": "GPU peer-to-peer access", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): NVIDIA DOCKER Image\r\n- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NVIDIA DOCKER Image\r\n- GPU model and memory: RTX 2080Ti (4 pcs.)\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nHello, I'm trying to utilize 4 RTX2080Ti GPUs at once. My mainboard is ASUS X99-E WS and my CPU is i7-6850K. However, tensorflow does not show peer-to-peer accessibility. I checked `nvidia-smi topo -m` and it seems every GPUs is able to use peer-to-peer access. Is there any opthons I have to modify before using tensorflow?\r\n\r\n\r\n```Python\r\n# tensorflow session output\r\n2019-01-03 07:07:29.683339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-01-03 07:07:31.374026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-03 07:07:31.374063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 3 \r\n2019-01-03 07:07:31.374070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N N N N \r\n2019-01-03 07:07:31.374075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   N N N N \r\n2019-01-03 07:07:31.374079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   N N N N \r\n2019-01-03 07:07:31.374084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 3:   N N N N \r\n2019-01-03 07:07:31.374707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9854 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5)\r\n2019-01-03 07:07:31.468037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10165 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:06:00.0, compute capability: 7.5)\r\n2019-01-03 07:07:31.564214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10165 MB memory) -> physical GPU (device: 2, name: GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, compute capability: 7.5)\r\n2019-01-03 07:07:31.660232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10165 MB memory) -> physical GPU (device: 3, name: GeForce RTX 2080 Ti, pci bus id: 0000:0a:00.0, compute capability: 7.5)\r\n\r\n# nvidia-smi\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  Off  | 00000000:05:00.0  On |                  N/A |\r\n| 30%   34C    P8     1W / 250W |    500MiB / 10988MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 208...  Off  | 00000000:06:00.0 Off |                  N/A |\r\n| 30%   34C    P8    21W / 250W |      0MiB / 10989MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce RTX 208...  Off  | 00000000:09:00.0 Off |                  N/A |\r\n| 30%   29C    P8    22W / 250W |      0MiB / 10989MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce RTX 208...  Off  | 00000000:0A:00.0 Off |                  N/A |\r\n| 29%   27C    P8    12W / 250W |      0MiB / 10989MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n\r\n# nvidia-smi topo -m output\r\n\tGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\r\nGPU0\t X \tPIX\tPHB\tPHB\t0-11\r\nGPU1\tPIX\t X \tPHB\tPHB\t0-11\r\nGPU2\tPHB\tPHB\t X \tPIX\t0-11\r\nGPU3\tPHB\tPHB\tPIX\t X \t0-11\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing a single PCIe switch\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n\r\n# lspci -t output\r\n-+-[0000:ff]-+-0b.0\r\n |           +-0b.1\r\n |           +-0b.2\r\n |           +-0b.3\r\n |           +-0c.0\r\n |           +-0c.1\r\n |           +-0c.2\r\n |           +-0c.3\r\n |           +-0c.4\r\n |           +-0c.5\r\n |           +-0f.0\r\n |           +-0f.1\r\n |           +-0f.4\r\n |           +-0f.5\r\n |           +-0f.6\r\n |           +-10.0\r\n |           +-10.1\r\n |           +-10.5\r\n |           +-10.6\r\n |           +-10.7\r\n |           +-12.0\r\n |           +-12.1\r\n |           +-13.0\r\n |           +-13.1\r\n |           +-13.2\r\n |           +-13.3\r\n |           +-13.4\r\n |           +-13.5\r\n |           +-13.6\r\n |           +-13.7\r\n |           +-14.0\r\n |           +-14.1\r\n |           +-14.2\r\n |           +-14.3\r\n |           +-14.4\r\n |           +-14.5\r\n |           +-14.6\r\n |           +-14.7\r\n |           +-15.0\r\n |           +-15.1\r\n |           +-15.2\r\n |           +-15.3\r\n |           +-16.0\r\n |           +-16.6\r\n |           +-16.7\r\n |           +-17.0\r\n |           +-17.4\r\n |           +-17.5\r\n |           +-17.6\r\n |           +-17.7\r\n |           +-1e.0\r\n |           +-1e.1\r\n |           +-1e.2\r\n |           +-1e.3\r\n |           +-1e.4\r\n |           +-1f.0\r\n |           \\-1f.2\r\n \\-[0000:00]-+-00.0\r\n             +-01.0-[01]--\r\n             +-01.1-[02]--\r\n             +-02.0-[07-0a]----00.0-[08-0a]--+-08.0-[0a]--+-00.0     // GPU 0\r\n             |                               |            +-00.1\r\n             |                               |            +-00.2\r\n             |                               |            \\-00.3\r\n             |                               \\-10.0-[09]--+-00.0     // GPU 1\r\n             |                                            +-00.1\r\n             |                                            +-00.2\r\n             |                                            \\-00.3\r\n             +-03.0-[03-06]----00.0-[04-06]--+-08.0-[06]--+-00.0     // GPU 2\r\n             |                               |            +-00.1\r\n             |                               |            +-00.2\r\n             |                               |            \\-00.3\r\n             |                               \\-10.0-[05]--+-00.0     // GPU 3\r\n             |                                            +-00.1\r\n             |                                            +-00.2\r\n             |                                            \\-00.3\r\n             +-05.0\r\n             +-05.1\r\n             +-05.2\r\n             +-05.4\r\n             +-11.0\r\n             +-14.0\r\n             +-16.0\r\n             +-19.0\r\n             +-1a.0\r\n             +-1b.0\r\n             +-1c.0-[0b]--\r\n             +-1c.3-[0c-0f]----00.0-[0d-0f]--+-01.0-[0e]----00.0\r\n             |                               \\-05.0-[0f]----00.0\r\n             +-1c.4-[10]----00.0\r\n             +-1c.6-[11]----00.0\r\n             +-1d.0\r\n             +-1f.0\r\n             +-1f.2\r\n             \\-1f.3\r\n\r\n```\r\n", "comments": ["[https://devtalk.nvidia.com/default/topic/1043300/linux/2080-tis-cudadevicecanaccesspeer-failure-without-nvlink-bridge/](url)\r\n\r\nFound out that 2080 Ti does not support p2p access without NVLink bridge. \r\n"]}, {"number": 24676, "title": "NNAPI doesn't support tensors with rank 0 - LeakyRelu?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04 + Android 8.1\r\n- TensorFlow installed from (source or binary): Anaconda  \r\n- TensorFlow version (or github SHA if from source): tensorflow 1.12.0 + keras 2.2.4 for converting model using tflite_convert and tensorflow-lite:1.12.0 in Android gradle build config\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n(update_36) m@m-hp:~/repos/keras_ocr_preproc$ tflite_convert --output_file=android/some_name.tflite --keras_model_file=android/some_name.hdf5\r\n2019-01-03 01:45:35.173445: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-01-03 01:45:35.178794: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\n```\r\n..which sounds OK.\r\n\r\n\r\n**Any other info / logs**\r\n\r\nLog from Android 8.1, tensorflow-lite 1.12.0 device: Alcatel 1 dual sim:\r\n```\r\nE/tflite: NNAPI doesn't support tensors with rank 0 (index 16 name class_2/LeakyRelu/alpha)\r\n    Returning error since TFLite returned failure nnapi_delegate.cc:742.\r\n    Failed to build graph for NNAPI\r\nE/AndroidRuntime: FATAL EXCEPTION: DefaultFallbackThread\r\n    Process: eu.yesse.readerdemo.release, PID: 27460\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: @\r\n```\r\n\r\nIt crashes only when setUseNNAPI is set to true (I know - usage is deprecated in tflite:1.12.0). \r\nWhen setUseNNAPI is commented, it seems that fallback to CPU happens and everything works, but slow. \r\nAlso for other Androids <=8.0, it works extremaly slow comparing to evaluation of the same model on iOS devices using CoreML. \r\n", "comments": ["Hello @aselle , since NNAPI is only available on devices with Android 8.1 or later (by setUseNNAPI), and the fallback is on CPU implementation on older devices, for this user the TFLite throws an error on tensors with rank 0. Can you please advise. Thanks.", "As far as I understand NNAPI does not support LeakyRelu only Relu Relu1 and Relu6. \r\n\r\nSee operator mapping\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc\r\n\r\n\r\nNNAPI documentation also does not list it. https://developer.android.com/ndk/reference/group/neural-networks\r\n\r\n\r\n\r\n", "hello guys\uff0cI met the same case but my error log is\uff1a\r\n2019-02-27 16:37:37.125 29351-29398/? E/tflite: NNAPI doesn't support tensors with rank 0 (index 13 name inference/generator/add_1/y)\r\n2019-02-27 16:37:37.125 29351-29398/? E/tflite: Returning error since TFLite returned failure nnapi_delegate.cc:748.\r\n2019-02-27 16:37:37.125 29351-29398/? E/tflite: Failed to build graph for NNAPI\r\n\r\nthe op with  tanh as activation and  *0.53+0.5. besides,my relu funtion is self-define leaky_relu,then it\r\n's maxmum in the graph", "This should be supported now, can you give it another try? ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24676\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24676\">No</a>\n"]}, {"number": 24675, "title": "TFTRT: Fix segfault+crash involving INT32 ops/inputs", "body": "This PR fixes the following issues:\r\n1. TRTEngineOp definition did not allow int32 as an input or output type, so creating the op would cause a crash if int32 was an input or output. int32_test.py covers this change.\r\n2. When determining the device for a segment, the device of the first node is used. If the first node was assigned to CPU, these nodes caused a segfault in GetDeviceAllocator because we assumed that if a node had a device set, that device was a GPU. This caused a segfault when an op such as matmul with int32 was being placed on CPU because there is no GPU implementation. Now, we treat this case the same as if no device was specified. int32_test.py covers this change.\r\n3. Adds validator checks to some ops which don't support int32. Almost no ops support int32 in TRT so more validators still need to be added. int32_test.py covers this change.\r\n4. There was a bug with connections when an engine had more than one outputs (the inside port was always being set to 0). topk_test.py covers this change.\r\n5. Output tensor types must be specified manually (they default to float). TopK has integer outputs so now we set the type of that tensor properly. topk_test.py covers this change.\r\n\r\nNotes:\r\n* TRT doesn't support any arithmetic in INT32, so we need the validators to catch this. INT32 in TRT is only used for `index` type parameters (for example `indicies` in  `INetworkDefinition::addGather(ITensor& data, ITensor& indices, int axis)`).", "comments": []}, {"number": 24674, "title": "TFTRT: Support Dilated Convolutions", "body": "This PR upstreams code from @benbarsdell to support dilated convolutions in TFTRT.\r\nIt also includes a new python test (test/conv2d_test.py) and some unit tests for ConvertConv2D. ConvertConv2D helper has been tidied up a bit to help with validation.", "comments": []}, {"number": 24673, "title": "[Intel MKL] Updating the public MKL Dockerfiles to reflect Ubuntu 18.04", "body": "use of python 3.6; also updating the default TensorFlow branch.", "comments": ["Nagging Reviewer @yifeif: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24672, "title": "Incorrect relative include paths in include/tensorflow/core/framework/op_def.pb.h when building external c++ files using Windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.6\r\n- Installed using: pip\r\n- CUDA/cuDNN version: 9.2/7\r\n- GPU model and memory: GeForce GTX 1050 Ti\r\n\r\n\r\n**Describe the problem**\r\nNOTE this error is not in building Tensorflow, but in building an external library that needs to include Tensorflow, and we believe the problem may be internal to Tensorflow. Apologies if this is in the wrong place.\r\n\r\nHello, I am no expert in Tensorflow or C++ but will try to describe the problem as best as I can. When trying to build an external package that contains .cpp files that need to include Tensorflow the C++ compiler (Visual Studio cl.exe) throws an error as the file Anaconda3\\lib\\site-packages\\tensorflow\\include\\tensorflow/core/framework/op_def.pb.h cannot find the relevant files in the include directory. It is looking for:\r\n- google/protobuf/stubs/common.h\r\n- unsupported/Eigen/CXX11/Tensor\r\n- absl/strings/string_view.h\r\n\r\nThe reason I believe this is is that the relative paths have changed on tensorflow 1.12, these files exist but are now stored in different paths. I have tried adding more include paths but then I get further errors and this fix doesn't feel like the correct way to go. \r\n\r\nOn older versions of tensorflow (tested 1.4) these include files are at the correct relative paths (as stated above) to tensorflow's include directory\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n>> D:\\programs_hdd\\visual_studio\\VC\\Tools\\MSVC\\14.13.26128\\bin\\HostX86\\x64\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\\Users\\peter\\Anaconda3\\lib\\site-packages\\numpy\\core\\include -IC:\\Users\\peter\\Anaconda3\\lib\\site-packages\\tensorflow\\include -ID:\\documents\\work\\bibliotecas_terceiro\\tensorflow_qrnn\\src -ID:\\documents\\work\\bibliotecas_terceiro\\tensorflow_qrnn\\src\\third_party \"-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\include\" \"-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\include\\cuda\" -IC:\\Users\\peter\\Anaconda3\\Lib\\site-packages\\tensorflow\\include\\third_party\\eigen3 -IC:\\Users\\peter\\Anaconda3\\Lib\\site-packages\\tensorflow\\include\\external\\com_google_absl -IC:\\Users\\peter\\Anaconda3\\include -IC:\\Users\\peter\\Anaconda3\\include -ID:\\programs_hdd\\visual_studio\\VC\\Tools\\MSVC\\14.13.26128\\ATLMFC\\include -ID:\\programs_hdd\\visual_studio\\VC\\Tools\\MSVC\\14.13.26128\\include \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\cppwinrt\" /EHsc /TpD:\\documents\\work\\bibliotecas_terceiro\\tensorflow_qrnn\\src\\fo_pool_op_cpu.cpp /Fobuild\\temp.win-amd64-3.6\\Release\\documents\\work\\bibliotecas_terceiro\\tensorflow_qrnn\\src\\fo_pool_op_cpu.obj\r\n\r\n>> fo_pool_op_cpu.cpp\r\n>> C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\tensorflow\\include\\tensorflow/core/framework/op_def.pb.h(9): fatal error C1083: Cannot open include file: 'google/protobuf/stubs/common.h': No such file or directory\r\nerror: command \r\n>> 'D:\\\\programs_hdd\\\\visual_studio\\\\VC\\\\Tools\\\\MSVC\\\\14.13.26128\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit status 2\r\n\r\nThank you\r\n", "comments": ["@Usherwood,\r\nSorry for the delayed response. Since this issue is more than 2 years old, can you please let us know if it is still relevant? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24672\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24672\">No</a>\n"]}, {"number": 24671, "title": "alternative for tf.contrib.losses.metric_learning.triplet_semihard_loss", "body": "Hi, \r\nI read that tf contrib is deprecated soon, is there an alternative for triplet loss that will remain in the api?", "comments": ["Any updates? These functions are super useful.", "@cgarciae \r\n\r\nI wrote one of my own, looking for some guidance for how to create a PR for it, and maybe someone to give another look at it before that. want to give me a hand? ", "@thebeancounter you a repo for this?", "@thebeancounter You can send PR on [tensorflow addons repo](https://github.com/tensorflow/addons/blob/master/CONTRIBUTING.md) to add this feature\r\ncc @facaiy ", "Thanks for all :-)\r\nIt has been moved to Addons: `tfa.losses.TripletSemiHardLoss`\r\nPlease check out, take a try and have fun: \r\n+ https://github.com/tensorflow/addons\r\n+ https://github.com/tensorflow/addons/blob/master/tensorflow_addons/losses/python/triplet.py"]}, {"number": 24670, "title": "AddSymbolicGradients segfaults internally C++ API", "body": "**System information**\r\n- Have I written custom code:\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed from source:\r\n- TensorFlow version: r1.13 #24493 \r\n- CPU only\r\n- Bazel version: Build label: 0.21.0\r\n- GCC/Compiler version: gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- memory: 6GB\r\n\r\n**Describe the current behavior**\r\nAddSymbolicGradients call results in a segmentation fault.\r\n\r\n**Describe the expected behavior**\r\nAddSymbolicGradients should return grads or fail safely within TF_CHECK_OK\r\n\r\n**Code to reproduce the issue**\r\n```\r\nauto input_slices = Split(scope, 1, x, window_size);\r\n\r\nauto initial_state = Fill(scope, {batch_size, state_size}, 0);\r\n\r\nvector<Output> states;\r\nstates.reserve(window_size+1);\r\nstates.push_back(initial_state);\r\n\r\nfor (int i=0; i!=window_size; i++) {\r\n    auto concat = Concat(scope, InputList(initializer_list<Input>{input_slices[i], states[i]}), 1);\r\n    auto new_state = Tanh(scope, Add(scope, MatMul(scope, concat, w_rnn), b_rnn));\r\n    states.push_back(new_state);\r\n}\r\n\r\n// dense output\r\nauto out = Tanh(scope, Add(scope, MatMul(scope, states[window_size], w_dense), b_dense));\r\n\r\n// loss function\r\nauto loss = ReduceMean(scope, Square(scope, Sub(scope, out, y)), {0, 1});\r\n\r\nvector<Output> grad_outputs;\r\nTF_CHECK_OK(AddSymbolicGradients(scope, {loss}, {w_rnn, w_dense, b_rnn, b_dense}, &grad_outputs));\r\n```\r\n\r\n**Other info / logs**\r\ngdb where command output:\r\n\r\n> (gdb) where\r\n> #0  0x00005555559b45aa in tensorflow::(anonymous namespace)::SymbolicGradientBuilder::Initialize() ()\r\n> #1  0x00005555559b6f1c in tensorflow::AddSymbolicGradients(tensorflow::Scope const&, std::vector<tensorflow::Output, std::allocator<tensorflow::O\r\n> utput> > const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Output, std::allocator<tens\r\n> orflow::Output> > const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output> >*) ()\r\n> #2  0x00005555559b9bb2 in tensorflow::AddSymbolicGradients(tensorflow::Scope const&, std::vector<tensorflow::Output, std::allocator<tensorflow::O\r\n> utput> > const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Output, std::allocator<tens\r\n> orflow::Output> >*) ()\r\n> #3  0x000055555597ce6c in Rnn::train(Dataset, int, int, int) ()\r\n> #4  0x00005555558410d6 in main ()\r\n\r\nVariables scope nor vectors should be the issue here. i've tried to unfold the loop without luck. Any hint? ", "comments": ["Hello @Proch92 , \"Fix python 3.7 build error due to astor\" #24493 is still awaiting OWNERS approval. We may need to wait. Thanks.\r\nHi @ilango100 / @gunan  , Can you please let us know what versions of **_gcc & bazel_**  does r1.13 sources have reliable tf-nightly builds . Thanks.\r\n", "1.13 still has not passed all release tests.\r\nUntil 1.13.0rc0 is out, I cannot tell if the branch can build reliably.", "Hi @Proch92 , PR #24493 has now been closed. Also @gunan has stated that until r1.13.0rc0 is released, the stable build configuration versions for bazel and gcc would not be declared. Please let us know your progress after the closure of the PR #24493 . Thanks.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I know the topic is closed, but since I ran into the same problem, I figured I should mention my insights  here. My guess is that the bug is related to the `ConcatGrad` operation, which gives an empty result when computing gradients with respect to a variable which is one of the constituents of the `Concat` operation. This could lead to seg-faults, when propagating your gradient to further nodes. \r\n\r\nConsider the following examle:\r\n```\r\nauto x1 = Variable(scope.WithOpName(\"x1\"), {2, 1}, DT_FLOAT);                                                            \r\nauto x2 = Variable(scope.WithOpName(\"x2\"), {2, 1}, DT_FLOAT);                                                            \r\nauto x = Concat(scope.WithOpName(\"x\"), InputList(std::initializer_list<Output>{x1, x2}), 0);                             \r\nauto y = MatMul(scope.WithOpName(\"y\"), x, x, MatMul::TransposeA(true));                                                                                                                                                                          \r\nstd::vector<Output> grad;                                                                                                                                                                          \r\nAddSymbolicGradients(scope.WithOpName(\"gradient\"), {y}, {x1}, &grad);       // grad will be empty                                              \r\n```                          \r\nIn this case, `tf` cannot   compute dy/dx1 (thus `grad` will be an empty list). As a work-around,  either compute dy/dx or  (if you need the derivative of `y` with respect to `x1`) eliminate the `Concat` operation whatsoever by representing the matmul operation explicitly, e.g. in my example u add two `MatMul` operations and add them to get the end result. "]}, {"number": 24669, "title": "no such package '@png_archive//", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): 'Build from source on Windows' TensorFlow Official Website\r\n- TensorFlow version: \r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: using cmd.exe\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI was trying to build TensorFlow from source in order to solve \"Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\" problem.\r\nAlthough I installed Bazel properly and added necessary environment variables, everytime I tried to build pip packages, which is \"bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\", an error called \"Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@png_archive//'\" always occurs.\r\nI also referred to some issues on the same problem, but none of them solved my problem. Is there anything I need to do? Or should I just install it through pip and ignore \"Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\" this message?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nC:\\tensorflow>python ./configure.py\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nnul\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 2c23affd-49a0-4cde-94b2-236aa0a37d76\r\nYou have bazel 0.21.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\wltjd\\AppData\\Local\\Programs\\Python\\Python36\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\wltjd\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\wltjd\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apacha Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\nC:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nc:\\tensorflow/.bazelrc\r\nStarting local Bazel server and connecting to it...\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: 61281b1e-9316-4ac5-9241-238712f7de45\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@png_archive//': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 106\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 73, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing '?C:\\msys64_2\\usr\\bin\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/wltjd/_bazel_wltjd/xv6zejqw/external/png_archive\" \"-i\" \"C:/tensorflow/third_party/png_fix_rpi.patch\"':\r\nStdout:\r\nStderr: java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(383): CreateProcessW(\"C:\\users\\wltjd\\_bazel_wltjd\\xv6zejqw\\external\\png_archive\\?C:\\msys64_2\\usr\\bin\\bash.exe\" -l -c \"\\\"patch\\\" \\\"-p1\\\" \\\"-d\\\" \\\"C:/users/wltjd/_bazel_wltjd/xv6zejqw/external/png_archive\\\" \\\"-i\\\" \\\"C:(...)): ??? ??? ?? ? ????.\r\nINFO: Elapsed time: 14.881s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (319 packages loaded, 8559 targets configured)\r\n    Fetching @swig; fetching 8s\r\n    Fetching @grpc; fetching 7s\r\n    Fetching @cython; fetching 7s\r\n    Fetching @eigen_archive; fetching 6s\r\n    Fetching @llvm; fetching 5s\r\n    Fetching @icu; fetching 4s\r\n    Fetching @boringssl; fetching 4s\r\n    Fetching @png_archive; fetching\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["You can safely ignore the warning message and continue to use TensorFlow.", "same problem! Have you solved it?", "Hi, I encountered with the same problem(maybe?), and turned to find that the patch command can only be used in the MSYS2 command terminal, otherwise you need to set the environment. I try the build command in the MSYS2 terminal and it worked fine. Hope helps someone!"]}, {"number": 24668, "title": "Quantized tflite model has large sized apk than unquantized", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n I have trained inceptionV3 classification model with custom dataset and got retrained.pb and retrained_labels.txt. I want to use the model in mobile phone and I converted the model using the following code\r\n```\r\nimport tensorflow as tf\r\ngraph_def_file = \"retrained_graph.pb\"\r\ninput_arrays = [\"Mul\"]\r\noutput_arrays = [\"final_result\"]\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file,input_arrays,output_arrays)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n**Describe the current behavior**\r\nI got the tflite model which is of size 85 MB and built an apk file which comes with size 113 MB and I quantized the model with following command\r\n\r\n```\r\ntflite_convert \\\r\n  --output_file=foo.tflite \\\r\n  --graph_def_file=retrained_graph.pb \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_arrays=Mul \\\r\n  --output_arrays=final_result \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=127 \\\r\n  --default_ranges_min=0 \\\r\n  --default_ranges_max=6 \r\n```\r\nIt gives me a tflite model of size 21 MB and when i use this quantized model to build an apk it generates an apk file of size 135 MB\r\n**Describe the expected behavior**\r\n1. Why the size of the apk file is increased when i use the quantized model ?\r\n\r\n2. How to reduce the apk size built with tflite model ?\r\n\r\n", "comments": ["Hello @gargn , can you please let us know if the user's query has been reviewed and being addressed?", "@msymp It has not.", "5 days passed.Please address the issue", "@jennings1716, 113 + 21 = 134 ~= 135. Mostly you didn't remove old model.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24667, "title": "InvalidArgumentError when use Keras loss function categorical_crossentropy", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: python3.6\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.17.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: Tesla K20c\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I build model with tensorflow.keras loss function **sparse_categorical_crossentropy**, the model run very well. But I try use **categorical_crossentropy**, I get error **InvalidArgumentError: Incompatible shapes: [2] vs. [0]**\r\n\r\n**My code:**\r\n\r\n```\r\nBATCH_SIZE = 64\r\nIMG_SIZE = 98 \r\nN_CLASSES = 179   \r\nN_EPOCHS = 5\r\n\r\nmodel = Sequential([\r\n        Conv2D(filters=32, kernel_size=3, strides=1, activation=tf.nn.relu, input_shape=(IMG_SIZE, IMG_SIZE, 1), padding=\"same\"),\r\n\tAveragePooling2D(pool_size=(2, 2), strides=2, padding=\"same\"),\r\n\tConv2D(filters=64, kernel_size=3, strides=1, activation=tf.nn.relu, padding=\"same\"),\r\n\tAveragePooling2D(pool_size=(2, 2), padding=\"same\"),\r\n\tConv2D(filters=64, kernel_size=3, strides=1, activation=tf.nn.relu, padding=\"same\"),\r\n\tAveragePooling2D(pool_size=(2, 2), strides=2, padding=\"same\"),\r\n\tFlatten(),\r\n\tDense(units=1024, activation=tf.nn.relu),\r\n\tDropout(rate=0.8),\r\n\tDense(units=N_CLASSES, activation=tf.nn.softmax)\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nprint(model.summary())\r\n\r\ntrain_data = train_data.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\r\nval_data = val_data.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\r\ntest_data = test_data.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\r\n\r\ntrain_label = to_categorical(train_label, N_CLASSES)\r\nval_label = to_categorical(val_label, N_CLASSES)\r\ntest_label = to_categorical(test_label, N_CLASSES)\r\nmodel.fit(train_data, train_label, batch_size=BATCH_SIZE, epochs=N_EPOCHS, validation_data=(val_data, val_label))\r\n```\r\n**Any other info / logs**\r\n```\r\nTrain on 803 samples, validate on 98 samples\r\nEpoch 1/100\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-18-30f90ff839f0> in <module>\r\n----> 1 model.fit(train_data, train_label, batch_size=BATCH_SIZE, epochs=N_EPOCHS, validation_data=(val_data, val_label))\r\n\r\nc:\\users\\ktmt\\tensor-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n   1637           initial_epoch=initial_epoch,\r\n   1638           steps_per_epoch=steps_per_epoch,\r\n-> 1639           validation_steps=validation_steps)\r\n   1640 \r\n   1641   def evaluate(self,\r\n\r\nc:\\users\\ktmt\\tensor-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\r\n    213           ins_batch[i] = ins_batch[i].toarray()\r\n    214 \r\n--> 215         outs = f(ins_batch)\r\n    216         if not isinstance(outs, list):\r\n    217           outs = [outs]\r\n\r\nc:\\users\\ktmt\\tensor-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py in __call__(self, inputs)\r\n   2984 \r\n   2985     fetched = self._callable_fn(*array_vals,\r\n-> 2986                                 run_metadata=self.run_metadata)\r\n   2987     self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n   2988     return fetched[:len(self.outputs)]\r\n\r\nc:\\users\\ktmt\\tensor-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py in __call__(self, *args, **kwargs)\r\n   1437           ret = tf_session.TF_SessionRunCallable(\r\n   1438               self._session._session, self._handle, args, status,\r\n-> 1439               run_metadata_ptr)\r\n   1440         if run_metadata:\r\n   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nc:\\users\\ktmt\\tensor-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    526             None, None,\r\n    527             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 528             c_api.TF_GetCode(self.status.status))\r\n    529     # Delete the underlying status object from memory otherwise it stays alive\r\n    530     # as there is a reference to status from this from the traceback due to\r\n\r\nInvalidArgumentError: Incompatible shapes: [2] vs. [0]\r\n\t [[{{node training/Adam/gradients/loss/dense_1_loss/Sum_grad/floordiv}} = FloorDiv[T=DT_INT32, _class=[\"loc:@training/Adam/gradients/loss/dense_1_loss/Sum_grad/Tile\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/loss/dense_1_loss/Sum_grad/Shape, training/Adam/gradients/loss/dense_1_loss/Sum_grad/Maximum)]]\r\n\t [[{{node loss/dense_1_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_2/_127}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_343_l...t/Switch_2\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\nThanks!\r\n", "comments": ["Please take a look at [similar issue](https://github.com/dennybritz/chatbot-retrieval/issues/15) to get help.\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 24666, "title": "cloud oauth_client: update for OpenSSL 1.1.0 compatibility", "body": "EVP_MD_CTX_cleanup was removed in OpenSSL 1.1.0. There is a call to\r\nEVP_MD_CTX_destroy right after and _destroy will call _cleanup if\r\nrequired. EVP_MD_CTX_destroy exists in OpenSSL 1.0, 1.1 and BoringSSL so\r\nremoving the call to _cleanup works everywhere.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["Related Gentoo bug: https://bugs.gentoo.org/673968", "@penpornk Good idea, I pushed an update with the comment and a note to not change it till support is dropped for 1.0.x. \r\nDistros will take a while to fully move over to OpenSSL 1.1 so that part is in no hurry."]}, {"number": 24665, "title": "why invoke() take so much time? use pos_training_quantized=True to convert pb into a tflite model.", "body": "**MY QUESTION**\r\n\r\nTraining a model with FLOAT-32 and converts into a saved model. Then I use **'pos_training_quantized=True '** to generate .tflite model (int8). \r\naccording to official website: https://www.tensorflow.org/lite/performance/post_training_quantization\r\n```\r\nAt inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels. This conversion is done once and cached to reduce latency.\r\n```\r\nHowever, at inference, tflite model costs more than pb, and almost costs in **invoke()** op. now ,the speed is much slow than saved model. Why this happens, the latency is not reduced like the official description.\r\n\r\nCan anyone explain? or pointing out my coding faults and any procedure faults about using tflite to inference.\r\n**Appreciate much!!**\r\n\r\n**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.12\r\n- GPU: Tesla P4 (server)\r\n- CPU: Intel\u00ae Core\u2122 i5-5300U CPU @ 2.30GHz \u00d7 4 (local)\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```from __future__ import print_function\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport cv2\r\nimport time\r\nfrom scipy import misc\r\n\r\n\r\n# convert saved model to tflite\r\nsaved_model_path = '/home/tom/PycharmProjects/transwarp/wxn/tmp/1'\r\ntflite_model_path = '/home/tom/PycharmProjects/transwarp/wxn/post_quantized_model.tflite'\r\nimages_root_path = '/home/tom/PycharmProjects/transwarp/wxn/combine_train'\r\nval_file = '/home/tom/PycharmProjects/transwarp/wxn/combine_train/val1.txt'\r\ntfrecord_dir = '/home/tom/PycharmProjects/transwarp/wxn/combine_train/combine_train_tfrecord'\r\n\r\n\r\ndef post_training_convert(tag_set, signature_key, tflite_model_path):\r\n    converter = tf.contrib.lite.TocoConverter.from_saved_model(saved_model_path, tag_set = tag_set, signature_key = signature_key)\r\n    converter.post_training_quantize = True\r\n    tflite_post_quantized_model = converter.convert()\r\n    open(tflite_model_path, 'wb').write(tflite_post_quantized_model)\r\n    print('tflite model transformation done!')\r\n    print('format: ', converter.inference_type, 'post_training_quantize: ', str(converter.post_training_quantize))\r\n\r\npost_training_convert(['SERVING'],'predict_signature',tflite_model_path)\r\n\r\ndef image_preprocess(image, resized_height, resized_width):\r\n    image = tf.image.resize_images(image, (resized_height, resized_width))\r\n    image = tf.image.per_image_standardization(image)\r\n    image = tf.cast(image, tf.float32)\r\n    image = tf.expand_dims(image, axis = 0)\r\n    return image\r\n\r\ndef main():\r\n    \"\"\"\r\n    use ./lite model to predict\r\n    \"\"\"\r\n    with tf.Graph().as_default() as g:\r\n        with tf.Session() as sess:\r\n            interpreter = tf.contrib.lite.Interpreter(model_path = tflite_model_path)\r\n            interpreter.allocate_tensors()\r\n\r\n            input_details = interpreter.get_input_details()\r\n            output_details = interpreter.get_output_details()\r\n            print('intput_details: ', input_details)\r\n            print('output_details: ', output_details)\r\n\r\n            input_shape = input_details[0]['shape']\r\n            print('input_shape: ', input_shape)\r\n            val_file_list = open(val_file, 'r').readlines()\r\n\r\n            count = 0\r\n            time_count = []\r\n            f = open('/home/tom/PycharmProjects/transwarp/wxn/result.txt', 'a')\r\n\r\n            for file in val_file_list:\r\n                file_name, file_label = file.strip('\\n').split()\r\n                image = cv2.imread(os.path.join(images_root_path, file_name))\r\n                if image is None:\r\n                    print('Warning, fail to read image: ', file_name)\r\n                    continue\r\n\r\n                image = image_preprocess(image, 224, 224)\r\n\r\n                image_array = sess.run(image)\r\n                start_time = time.time()\r\n                start_time1 = time.time()\r\n                interpreter.set_tensor(input_details[0]['index'], image_array)\r\n                print('set time: ', time.time()-start_time1)\r\n                start_time2 = time.time()\r\n                interpreter.invoke()\r\n                print('invoke time: ', time.time()-start_time2)\r\n                start_time3 = time.time()\r\n                output_data = interpreter.get_tensor(output_details[0]['index'])\r\n                print('out time: ', time.time()-start_time3)\r\n                print(output_data)\r\n                end_time = time.time()\r\n                duration = end_time - start_time\r\n                count += 1\r\n                time_count.append(duration)\r\n                f.write(file_name + ' ' +\r\n                        file_label +' ' +\r\n                        str(output_data[0][0])+' ' +\r\n                        str(round(duration, 4)) + '\\n')\r\n            print('total processing images: ', count)\r\n            print('average processing time: ', np.mean(time_count))\r\n    f.close()\r\nmain()\r\n\r\n\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n      **inference 25 images, just list some results.**\r\n```\r\nset time: cost of 'interpreter.set_tensor() '\r\ninvoke time: cost of 'interpreter.invoke()'\r\nout_time : cost of 'interpreter.get_tensor()'\r\n```\r\n#In GPU, the inference time\r\nset time:  0.000224828720093\r\ninvoke time:  7.69320392609\r\nout time:  2.00271606445e-05\r\n[[4]]\r\nset time:  0.000519990921021\r\ninvoke time:  7.68318390846\r\nout time:  2.88486480713e-05\r\n[[4]]\r\nset time:  0.000233888626099\r\ninvoke time:  7.56485295296\r\nout time:  1.90734863281e-05\r\n[[4]]\r\nset time:  0.000152111053467\r\ninvoke time:  7.75410199165\r\nout time:  2.71797180176e-05\r\n[[4]]\r\ntotal processing images:  25\r\naverage processing time:  7.540134401321411\r\n********************************\r\n#In CPU, the inference time\r\nset time:  7.772445678710938e-05\r\ninvoke time:  7.645012378692627\r\nout time:  1.52587890625e-05\r\n[[4]]\r\nset time:  9.489059448242188e-05\r\ninvoke time:  7.672791957855225\r\nout time:  1.9788742065429688e-05\r\n[[4]]\r\nset time:  0.00010085105895996094\r\ninvoke time:  7.442874908447266\r\nout time:  1.52587890625e-05\r\n[[4]]\r\nset time:  7.891654968261719e-05\r\ninvoke time:  7.478809833526611\r\nout time:  1.7881393432617188e-05\r\n[[4]]\r\ntotal processing images:  25\r\naverage processing time:  7.6469355392456055\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hello @gargn , can you please let us know if the user's query has been reviewed and being addressed?", "@msymp It has not.", "Hi @zhuohuiyuan ,\r\n\r\nThe speedup can be seen when comparing to a float *TensorFlow Lite* model.\r\nYou are comparing a TensorFlow Lite model on CPU (optimized for ARM on mobile) with a TensorFlow model running on GPU. If the hardware is different, the comparison isn't really realistic. \r\n\r\nHope that helps!", "Hi @zhuohuiyuan have you resolved this issue?\r\n@suharshs  I am facing the same issue and I understand your point. So I went ahead and tested the pb and keras models on the same cpu as well- turns out tflite model (optimized) still takes double the time taken by the h5 model without any optimization. The invoke function actually in itself takes up that much time. What am I doing wrong? I don't think I can call invoke once, can I? Is there any other way I can optimize inference time?", "@tonmoyborah @suharshs  I have the same problem, invoke() takes up so much time that any time eventually gained by converting to tflite is eaten up (all comparisons done on CPU). Is there a solution to this?", "Right now post training quantization is only optimized for ARM CPU, not x86. So you will see speed up on mobile but not desktop just yet. We are working on x86 support.", "@suharshs Thanks for clarifying. I understand that we can see a significant speed up only in ARM CPUs. But my concern is that I am doing something wrong - since, as reported on tflite docs, [here](https://www.tensorflow.org/lite/performance/benchmarks) all these complex models take 10 or 100 ms for prediction. This is very strange since one call to `invoke()` function takes around 500ms-800ms for me on any desktop cpu or gpu thus making it a bottleneck for any subsequent task. I must be doing something really wrong. This is what I am doing for a siamese 2 input network:\r\n\r\n    tflite_model_path = '/path/to/model'\r\n\tlogger.debug('starting...')\r\n\tinterpreter = Interpreter(model_path=str(tflite_model_path))\r\n\tlogger.debug('interpreter initialised, allocating tensors...')\r\n\tinterpreter.allocate_tensors()\r\n\tlogger.debug('tensors allocated')\r\n\tinput_index1 = interpreter.get_input_details()[0][\"index\"]\r\n\tinput_index2 = interpreter.get_input_details()[1][\"index\"]\r\n\toutput_index = interpreter.get_output_details()[0][\"index\"    \r\n    img2 = cv2.imread('/path/to/image')\r\n\timg2 = cv2.resize(img2, (128, 128))\r\n\timg2 = tf.cast(img2, tf.float32)   # /255.0\r\n\tlogger.debug('image2 ops done...')\r\n\r\n\timg3 = cv2.imread('/path/to/image')\r\n\timg3 = cv2.resize(img3, (128, 128))\r\n\timg3 = tf.cast(img3, tf.float32)   # /255.0\r\n\tlogger.debug('3 ops done...')\r\n\r\n\r\n\r\n\r\n\tstart_time = time.time()\r\n\tinterpreter.set_tensor(input_index1, img2)\r\n\tinterpreter.set_tensor(input_index2, img3)\r\n\tlogger.debug('tensors set, starting invoke...')\r\n\tinterpreter.invoke()\r\n    logger.debug('time taken for invoke {}'.format(time.time() - start_time))\r\n\tlogger.debug('invoked')\r\n\tpredictions = interpreter.get_tensor(output_index)\r\n\tlogger.info(predictions)\r\n\tlogger.debug('time taken {}'.format(time.time() - start_time))", "> Right now post training quantization is only optimized for ARM CPU, not x86. So you will see speed up on mobile but not desktop just yet. We are working on x86 support.\r\n\r\nSo can you estimate when support for x86 comes out? \r\nI'm currently working on a model on my desktop computer and I see longer inference time with quantized TfLite model rather than normal TfLite model."]}, {"number": 24664, "title": "[TF port] Disable tests for GetCurrentCpu() on iOS/OS X because MacOS\u2026", "body": "\u2026 has a __cpuid bug.\r\n\r\nPiperOrigin-RevId: 226125575", "comments": []}, {"number": 24663, "title": "TFLite App error with SSD_MobileNet V2 Quantized 300*300 CoCo", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 6 and Moto G5sPlus\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5.5 (anaconda)\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory: 8GB Tesla K80 on VM\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nTFLite app crashes with log shown below.\r\n\r\n**Describe the expected behavior**\r\nRuns and detects objects smoothly\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n[Tensorflow lite convertion tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) and used android source code from [Tensorflow/lite/Java/Demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo), Changed classifier to ```ImageClassifierQuantizedMobileNet``` for Quantized model and ```ImageClassifierFloatMobileNet``` for Float model.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nUsed steps as on [Tensorflow lite convertion tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md), I generated both UNIT8 and FLOAT type tflite models. I've used [SSD_MobileNet_V2_COCO](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v2_quantized_300x300_coco.config) to train my dataset for object detection.\r\nFollowing are the errors I get while trying to build for android:\r\n\r\n**Using Quantized (UINT8)**\r\n\r\n```\r\n01-02 14:41:39.721 5697-5697/? E/SchedPolicy: open of /dev/cpuctl/bg_non_interactive/tasks failed: No such file or directory\r\n01-02 14:41:39.727 5697-5697/? I/zygote: Late-enabling -Xcheck:jni\r\n01-02 14:41:39.789 5697-5707/? E/zygote: Failed sending reply to debugger: Broken pipe\r\n01-02 14:41:39.790 5697-5707/? I/zygote: Debugger is no longer active\r\n01-02 14:41:40.050 5697-5719/? D/TfLiteCameraDemo: Created a Tensorflow Lite Image Classifier.\r\n01-02 14:41:40.069 5697-5725/? D/OpenGLRenderer: HWUI GL Pipeline\r\n01-02 14:41:40.163 5697-5725/? I/Adreno: QUALCOMM build                   : 908a5ce, I77d3059488\r\n    Build Date                       : 06/07/18\r\n    OpenGL ES Shader Compiler Version: EV031.22.00.01_06\r\n    Local Branch                     : \r\n    Remote Branch                    : refs/tags/AU_LINUX_ANDROID_LA.UM.6.5.R1.08.01.00.312.086\r\n    Remote Branch                    : NONE\r\n    Reconstruct Branch               : NOTHING\r\n01-02 14:41:40.163 5697-5725/? D/vndksupport: Loading /vendor/lib/hw/gralloc.msm8953.so from current namespace instead of sphal namespace.\r\n01-02 14:41:40.168 5697-5725/? I/Adreno: PFP: 0x005ff087, ME: 0x005ff063\r\n01-02 14:41:40.173 5697-5725/? I/zygote: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0\r\n01-02 14:41:40.174 5697-5725/? I/OpenGLRenderer: Initialized EGL, version 1.4\r\n01-02 14:41:40.174 5697-5725/? D/OpenGLRenderer: Swap behavior 2\r\n01-02 14:41:40.258 5697-5705/? I/zygote: Do partial code cache collection, code=28KB, data=30KB\r\n    After code cache collection, code=28KB, data=30KB\r\n    Increasing code cache capacity to 128KB\r\n01-02 14:41:40.395 5697-5705/? I/zygote: Do partial code cache collection, code=62KB, data=58KB\r\n01-02 14:41:40.396 5697-5705/? I/zygote: After code cache collection, code=62KB, data=58KB\r\n    Increasing code cache capacity to 256KB\r\n01-02 14:41:40.523 5697-5697/? I/CameraManagerGlobal: Connecting to camera service\r\n01-02 14:41:40.682 5697-5725/? D/vndksupport: Loading /vendor/lib/hw/android.hardware.graphics.mapper@2.0-impl.so from current namespace instead of sphal namespace.\r\n01-02 14:41:40.683 5697-5725/? D/vndksupport: Loading /vendor/lib/hw/gralloc.msm8953.so from current namespace instead of sphal namespace.\r\n01-02 14:41:40.746 5697-5724/? D/TfLiteCameraDemo: Timecost to put values into ByteBuffer: 67\r\n01-02 14:41:41.101 5697-5724/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n    Process: android.example.com.tflitecamerademo, PID: 5697\r\n    java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite tensor with type FLOAT32 and a Java object of type [[B (which is compatible with the TensorFlowLite type UINT8).\r\n        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:261)\r\n        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:141)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:161)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)\r\n        at com.example.android.tflitecamerademo.ImageClassifierQuantizedMobileNet.runInference(ImageClassifierQuantizedMobileNet.java:95)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:126)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:815)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Unknown Source:0)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:710)\r\n        at android.os.Handler.handleCallback(Handler.java:790)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n01-02 14:41:41.806 5697-5697/? I/Choreographer: Skipped 73 frames!  The application may be doing too much work on its main thread.\r\n01-02 14:41:41.809 5697-5707/? I/zygote: Ignoring second debugger -- accepting and dropping\r\n```\r\n\r\n\r\n**Using Float type**\r\n```\r\n01-02 14:39:44.097 5364-5364/? E/SchedPolicy: open of /dev/cpuctl/bg_non_interactive/tasks failed: No such file or directory\r\n01-02 14:39:44.101 5364-5364/? I/zygote: Late-enabling -Xcheck:jni\r\n01-02 14:39:44.164 5364-5371/? E/zygote: Failed sending reply to debugger: Broken pipe\r\n01-02 14:39:44.165 5364-5371/? I/zygote: Debugger is no longer active\r\n01-02 14:39:44.408 5364-5387/? D/TfLiteCameraDemo: Created a Tensorflow Lite Image Classifier.\r\n01-02 14:39:44.427 5364-5395/? D/OpenGLRenderer: HWUI GL Pipeline\r\n01-02 14:39:44.520 5364-5395/? I/Adreno: QUALCOMM build                   : 908a5ce, I77d3059488\r\n    Build Date                       : 06/07/18\r\n    OpenGL ES Shader Compiler Version: EV031.22.00.01_06\r\n    Local Branch                     : \r\n    Remote Branch                    : refs/tags/AU_LINUX_ANDROID_LA.UM.6.5.R1.08.01.00.312.086\r\n    Remote Branch                    : NONE\r\n    Reconstruct Branch               : NOTHING\r\n01-02 14:39:44.521 5364-5395/? D/vndksupport: Loading /vendor/lib/hw/gralloc.msm8953.so from current namespace instead of sphal namespace.\r\n01-02 14:39:44.526 5364-5395/? I/Adreno: PFP: 0x005ff087, ME: 0x005ff063\r\n01-02 14:39:44.532 5364-5395/? I/zygote: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0\r\n01-02 14:39:44.532 5364-5395/? I/OpenGLRenderer: Initialized EGL, version 1.4\r\n01-02 14:39:44.533 5364-5395/? D/OpenGLRenderer: Swap behavior 2\r\n01-02 14:39:44.615 5364-5369/? I/zygote: Do partial code cache collection, code=28KB, data=30KB\r\n01-02 14:39:44.616 5364-5369/? I/zygote: After code cache collection, code=28KB, data=30KB\r\n    Increasing code cache capacity to 128KB\r\n01-02 14:39:44.752 5364-5369/? I/zygote: Do partial code cache collection, code=62KB, data=58KB\r\n    After code cache collection, code=62KB, data=58KB\r\n    Increasing code cache capacity to 256KB\r\n01-02 14:39:44.865 5364-5364/? I/CameraManagerGlobal: Connecting to camera service\r\n01-02 14:39:44.988 5364-5395/? D/vndksupport: Loading /vendor/lib/hw/android.hardware.graphics.mapper@2.0-impl.so from current namespace instead of sphal namespace.\r\n01-02 14:39:44.989 5364-5395/? D/vndksupport: Loading /vendor/lib/hw/gralloc.msm8953.so from current namespace instead of sphal namespace.\r\n01-02 14:39:45.072 5364-5392/? D/TfLiteCameraDemo: Timecost to put values into ByteBuffer: 87\r\n01-02 14:39:45.900 5364-5392/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n    Process: android.example.com.tflitecamerademo, PID: 5364\r\n    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 10, 4] and a Java object with shape [1, 1].\r\n        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:270)\r\n        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:141)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:161)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)\r\n        at com.example.android.tflitecamerademo.ImageClassifierFloatMobileNet.runInference(ImageClassifierFloatMobileNet.java:92)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:126)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:815)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Unknown Source:0)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:710)\r\n        at android.os.Handler.handleCallback(Handler.java:790)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n01-02 14:39:45.931 5364-5364/android.example.com.tflitecamerademo I/Choreographer: Skipped 60 frames!  The application may be doing too much work on its main thread.\r\n```\r\n\r\nKindly let me know where I am wrong.", "comments": ["Hello @gargn , can you please let us know if the user's query has been reviewed and being addressed?", "@msymp  It has not.", "@sachin-ranka you are not supposed to run object detection models with image classifier code! You may want read through the [introduction article ](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) by @achowdhery and others.", "@freedomtan The sample app code link: [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/BUILD) file doesn't exists, only python code exists in that folder. ( as per the article provided) I've generated TFLite files using the same steps. Pls help me out here...", "TFLite related code moved from tensorflow/contrib/lite into tensorflow/lite, use https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/android/BUILD instead", "I am getting this error while trying to follow the article (as suggested above) \r\n```\r\n$bazel run -c opt tensorflow/lite/toco:toco -- \\\r\n--input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect_Quant.tflite \\\r\n--input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n --inference_type=QUANTIZED_UINT8 --mean_values=128 \\\r\n --std_values=128 --change_concat_input_ranges=false --allow_custom_ops\r\n\r\n\r\n2019-01-14 09:02:14.634927: F tensorflow/lite/toco/tooling_util.cc:1702] Array FeatureExtractor/MobilenetV2/Conv/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV2/expanded_conv/depthwise/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nAborted (core dumped)\r\n```\r\n\r\nNot sure what to do further, pls help me out...", "@sachin-ranka As the error message mentions, try adding the `--default_ranges_min=` and `--default_ranges_max=` flags to the toco command. Say `0` and `6`.\r\n\r\nIn any case this isn't an issue with the tensorflow sample code but your modification of it, so it is out of scope for github. Try www.stackoverflow.com instead.", "Closing per latest advice."]}, {"number": 24662, "title": "Optimize FusedBatchNorm with DepthwiseConv2dNative for inference.", "body": "", "comments": ["Sorry for the slow response. Can you add a test for this path to optimize_for_inference_test.py? Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F24662) for more info**.\n\n<!-- need_author_cla -->", "> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors. If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\r\n> In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F24662) for more info**.\r\n\r\nsolved", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F24662) for more info**.\n\n<!-- ok -->", "> Sorry for the slow response. Can you add a test for this path to optimize_for_inference_test.py? Thanks!\r\n\r\nDone.", "Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "Please review @petewarden ", "@petewarden Hi, Could you PTAL and approve.", "@petewarden can you please review this? Thanks!", "@vv1133 Can you please check build failures? Thanks!", "Hi @jhseu ,  I have corrected pylint issue, can you please review it again? thanks!", "@vv1133 Can you please address build failures? Thanks!", "@jhseu There 2 failed tests in Ubuntu Sanity. From the logs, it seems it is irrelevant to this PR. Could you please help check? Thanks!", "It's just pylint errors. You can look at the sanity invocation log here:\r\nhttps://source.cloud.google.com/results/invocations/c2c2e4a2-e57d-4c9a-8f6a-098fa32b6f79/log", "Copying here for convenience:\r\n```\r\ntensorflow/python/tools/optimize_for_inference_test.py:178: [C0330(bad-continuation), ] Wrong hanging indentation (remove 4 spaces).\r\ntensorflow/python/tools/optimize_for_inference_test.py:179: [C0330(bad-continuation), ] Wrong hanging indentation (remove 4 spaces).\r\ntensorflow/python/tools/optimize_for_inference_test.py:180: [C0330(bad-continuation), ] Wrong hanging indentation (remove 4 spaces).\r\ntensorflow/python/tools/optimize_for_inference_test.py:181: [C0330(bad-continuation), ] Wrong hanging indentation (remove 4 spaces).\r\ntensorflow/python/tools/optimize_for_inference_test.py:182: [C0330(bad-continuation), ] Wrong hanging indentation.\r\n```", "> It's just pylint errors. You can look at the sanity invocation log here:\r\n> https://source.cloud.google.com/results/invocations/c2c2e4a2-e57d-4c9a-8f6a-098fa32b6f79/log\r\n\r\n@jhseu Thanks! Could you please review it again?", "@vv1133 Can you please check build failures? Thanks you!", "> @vv1133 Can you please check build failures? Thanks you!\r\n\r\n@gbaned @jhseu From the logs, it seems it is irrelevant to this PR. Could you please help check? Thanks!", "Yeah, the failures look unrelated to me."]}, {"number": 24661, "title": "tensorflow.lite.python.convert.ConverterError: TOCO failed.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):pip install\r\n- TensorFlow version (or github SHA if from source):1.9\r\n\r\n**Command Used**\r\n tflite_convert   --output_file=detect.tflite   --graph_def_file=frozen_inference_graph.pb    --input_arrays=image_tensor   --input_shape=1,300,300,3   --output_arrays='detection_boxes','detection_scores','num_detections','detection_classes'\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2019-01-02 06:50:15.359432: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 320, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 316, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 121, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 309, in convert\r\n    allow_custom_ops=self.allow_custom_ops)\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 225, in toco_convert\r\n    input_data.SerializeToString())\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 107, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2019-01-02 06:50:19.643688: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.643862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.643913: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.643951: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.643990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.644022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.644052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.644085: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.644118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.644185: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-02 06:50:19.644216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.644248: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.644282: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LoopCond\r\n2019-01-02 06:50:19.644317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\r\n2019-01-02 06:50:19.644338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-02 06:50:19.644370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\r\n2019-01-02 06:50:19.644391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-02 06:50:19.644420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-02 06:50:19.644471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-02 06:50:19.644517: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-02 06:50:19.644572: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-02 06:50:19.644621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-02 06:50:19.644652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.644674: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.644700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.665993: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.666097: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.666417: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.666452: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666479: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.666507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.666554: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666588: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.666616: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666640: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.666663: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666690: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.666744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666771: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666794: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.666817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2019-01-02 06:50:19.666892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.666940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.667007: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-02 06:50:19.667034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.667092: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-02 06:50:19.667118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.667175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-02 06:50:19.667200: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.667261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-02 06:50:19.667285: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.667312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2019-01-02 06:50:19.667342: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LoopCond\r\n2019-01-02 06:50:19.667371: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\r\n2019-01-02 06:50:19.667391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-02 06:50:19.667418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\r\n2019-01-02 06:50:19.667441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-02 06:50:19.667468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\r\n2019-01-02 06:50:19.667488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-02 06:50:19.667513: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\r\n2019-01-02 06:50:19.667533: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-02 06:50:19.667560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-02 06:50:19.667582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-02 06:50:19.667606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-02 06:50:19.667627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-02 06:50:19.667648: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667670: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667691: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667732: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667839: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667859: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667880: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667900: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667962: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.667982: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668018: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668039: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668060: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668101: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668142: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668162: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668247: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668268: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668310: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668330: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668376: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668502: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668526: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668568: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668630: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668670: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668691: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668711: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668732: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668848: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668970: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.668991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669072: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669093: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669134: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669194: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669214: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669235: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669296: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669316: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669336: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669480: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669565: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2019-01-02 06:50:19.669766: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\r\n2019-01-02 06:50:19.672088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.672220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.672312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.672346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.672383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.672495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.672586: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.672620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.672656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.672774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.672863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.672898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.672935: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.673048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.673134: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.673167: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.673232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.673352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.673439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.673472: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.673510: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.673631: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.673719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.673752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.673789: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.673902: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.673997: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.674031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.674067: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.674187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.674290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.674328: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.674367: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.674488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.674579: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.674614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.674652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.674783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.674894: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.674929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.674967: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.675082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.675206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.675243: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.675281: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.675407: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.675496: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.675530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.675568: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.675687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.675772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.675805: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.675843: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.675962: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.676048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.676082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.676120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.676240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.676329: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.676363: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.676401: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.676521: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.676609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.676669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.676714: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.676834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.676919: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.676954: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.676991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.677112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.677203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.677237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.677276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.677398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.677485: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.677519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.677556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.677676: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.677763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.677795: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.677833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.677953: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.678036: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.678070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.678113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.678249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.678343: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.678378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.678416: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.678536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.678624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.678664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.678702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.678827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.678916: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.678949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.678985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.679103: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.679186: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.679220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.679255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.679367: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.679449: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.679486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.679523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.679633: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.679719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.679752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.679788: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.679897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.679980: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.680013: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.680049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.680173: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.680257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.680291: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.680326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.680482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.680571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.680605: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.680642: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.680759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.680876: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.680912: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.680950: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.681068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.681156: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.681189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.681225: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.681342: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.681427: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.681461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.681499: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.681615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.681704: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.681737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.681774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.681898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.681985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.682020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.682058: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.682175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.682279: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.682317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.682356: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.682478: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.682566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.682600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.682637: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.682760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.682857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.682892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.682930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.683054: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.683141: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.683175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.683212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.683337: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.683426: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.683459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.683497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.683626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.683713: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.683746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.683785: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.683904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.683991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.684025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.684062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.684178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.684267: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.684302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.684338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.684459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.684546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.684579: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.684617: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.684733: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.684821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.684853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.684891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.685012: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.685100: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.685134: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.685172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.685290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.685376: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.685409: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.685446: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.685566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.685651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.685684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.685721: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.685834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.685922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.685955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.685992: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.686118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.686221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.686258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.686298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.686424: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.686515: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.686548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.686586: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.686702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.686787: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.686826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.686868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.686987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.687070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.687102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.687138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.687249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.687334: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.687367: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.687404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.687517: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.687601: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.687634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.687672: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.687795: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.687880: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.687913: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.687951: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.688067: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.688156: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.688189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.688228: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.688349: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.688437: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.688471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.688507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.688624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.688711: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.688744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.688782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.688904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.688991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.689025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.689062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.689223: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.689313: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.689347: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.689383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.689503: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.689590: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.689625: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.689662: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.689781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.689871: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.689904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.689941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.690052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.690135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.690171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.690220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.690341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.690432: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.690466: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.690505: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.690627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.690715: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.690749: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.690786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.690914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.691005: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.691039: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.691076: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.691193: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.691280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.691314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.691351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.691462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.691553: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.691587: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.691626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.691742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.691830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.691864: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.691902: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.692025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.692112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.692145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.692182: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.692297: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.692385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.692419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.692456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.692575: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.692663: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.692702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.692740: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.692852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.692934: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.692967: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.693004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.693119: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.693207: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.693241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.693280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.693402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.693493: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.693528: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.693567: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.693683: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.693770: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.693804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.693841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.693960: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.694048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.694082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.694121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.694255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.694347: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.694381: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.694419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.694532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.694618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.694652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.694693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.694812: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.694901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.694940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.694982: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.695196: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.695284: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.695318: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.695357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.695474: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.695561: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.695595: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.695633: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.695752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.695840: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.695873: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.695909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.696021: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.696111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.696145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.696182: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.696294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.696380: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.696414: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.696451: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.696566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.696652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.696686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.696727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.696863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.696952: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.696985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.697022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.697138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.697221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.697253: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.697289: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.697402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\r\n2019-01-02 06:50:19.697489: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-01-02 06:50:19.697523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\r\n2019-01-02 06:50:19.697659: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Size\r\n2019-01-02 06:50:19.697708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\r\n2019-01-02 06:50:19.698007: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-02 06:50:19.698241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-02 06:50:19.698460: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-02 06:50:19.698516: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-02 06:50:19.698569: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-02 06:50:19.698610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-02 06:50:19.698649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-02 06:50:19.698684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-02 06:50:20.238716: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4322 operators, 7127 arrays (0 quantized)\r\n2019-01-02 06:50:20.644583: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4296 operators, 7080 arrays (0 quantized)\r\n2019-01-02 06:50:21.296853: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4296 operators, 7080 arrays (0 quantized)\r\n2019-01-02 06:50:21.959362: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge\r\nAborted (core dumped)\r\n\r\nNone\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n**Graph Used**\r\nI used the frozen  graph [ssd_inception_v2_coco](http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz)\r\n\r\n**Any other info / logs**\r\nI have tried with ssdlite_mobilenet_v2_coco_2018_05_09 also.Please suggest a object detection model which can be converted to tflite.\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hello @gargn , can you please let us know if the user's query has been reviewed and being addressed?", "@msymp It has not.", "@jennings1716 : Have you tried below example link?\r\nhttps://www.tensorflow.org/lite/models/object_detection/overview\r\nCurrently Control Flow Ops are under future development, you can refer[https://github.com/tensorflow/community/pull/83].\r\n\r\nPlease confirm, if above helps, and close this issue. As post RFC conclusion, your issue will be automatically solved.", "I will try this @ANSHUMAN87  Thanks"]}, {"number": 24660, "title": "cuDNN launch failure when implementing custom kernel_regularizer function within [tf.layers] module", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): SOURCE (pip)\r\n- TensorFlow version (use command below): tensorflow-gpu API r1.12 \r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): not compiled with bazel\r\n- GCC/Compiler version (if compiling from source): gcc 5.4.0 20160609\r\n- CUDA/cuDNN version: CUDA 9.0.176 (wth patch 9.0.176.1 and 9.0.176.2), cuDNN version 7.21\r\n- GPU model and memory: two NVIDIA GeForce GTX 1080ti s, 11171mb memory each\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI implemented a spectral normalization regularizer that can be passed as the [kernel_regularizer] argument in the [tf.layers] module. This implementation causes three problems which are not expected.\r\n\r\n1. Slows down the first epoch of the training process by around ten-fold.\r\n2. Training process gets slower after every epoch by around 1.5 times. (No new ops are being added in the graph and this was made sure by using finalize() method of the graph within the session.)\r\n3. Eventually crashes after a few epochs and throws the following InternalError\r\n\r\nCaused by op u'GRAN_model/encoder_models/encoder_real/encoder/encoder_conv_3/Conv2D', defined at:\r\n  File \"main.py\", line 70, in <module>\r\n    gran.initialize()\r\n  File \"main.py\", line 33, in initialize\r\n    config=self.config)\r\n  File \"/home/bispl/github/generative_recon/network.py\", line 32, in build_network\r\n    self.graph.build_model(model=self.model)\r\n  File \"/home/bispl/github/generative_recon/graph.py\", line 45, in build_model\r\n    self.model.build_model(image_input=self.image_input, bold_input=self.bold_input, unpaired_image_input=self.unpaired_image_input, training=self.training)\r\n  File \"/home/bispl/github/generative_recon/model.py\", line 38, in build_model\r\n    self.encoder_real_model_output = self.encoder.build_model(image_input=self.image_input, bold_input=self.bold_input, model_scope='encoder_real', reuse=False)\r\n  File \"/home/bispl/github/generative_recon/model_components.py\", line 37, in build_model\r\n    _x = tf.layers.conv2d(inputs=_x, filters=256, kernel_size=[7,7], kernel_initializer=self.initializer, kernel_regularizer=self.regularizer, kernel_constraint=self.co\r\nnstraint, strides=1, padding='SAME', name=self.scope+'_conv_3')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.py\", line 417, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 817, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 374, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 757, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 194, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 868, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 520, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 204, in __call__\r\n    name=self.name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 957, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): cuDNN launch failure : input shape([16,128,24,32]) filter shape([7,7,128,256])\r\n         [[node GRAN_model/encoder_models/encoder_real/encoder/encoder_conv_3/Conv2D (defined at /home/bispl/github/generative_recon/model_components.py:37)  = Conv2D[T\r\n=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:\r\n0\"](GRAN_model/encoder_models/encoder_real/encoder/encoder_maxpool_0/MaxPool, encoder/encoder_conv_3/kernel/read)]]\r\n         [[{{node GRAN_graph_metrics/generator_real_loss/update_op/_215}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", s\r\nend_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_23669_GRAN_graph_metrics/generator_real_loss/update_op\", tensor_\r\ntype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior are\r\n1. Training process gets slower by at most two-fold using this implementation\r\n2. Does not further slow down after every epoch\r\n3. No error thrown.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n1. Implementation of the spectral normalization function to be passed to kernel_regularizer argument of the tf.layers module (e.g. tf.layers.conv2d)\r\n\r\n\r\n```\r\ndef spectral_normalization(kernel):\r\n    with tf.variable_scope(\"spectral_normalization\", reuse=tf.AUTO_REUSE):\r\n        print (kernel)\r\n        w = kernel\r\n        _w = tf.reshape(w, [-1, w.shape[-1]], name=\"reshape_weight_to_2d\")\r\n\r\n        u_tilde = tf.get_local_variable(name=\"u_tilde\", shape=[1, w.shape[-1].value], initializer=tf.initializers.truncated_normal)\r\n        _u_tilde = tf.identity(u_tilde, name=\"u_tilde_update\")\r\n        for i in range(1):\r\n            _v_tilde = tf.nn.l2_normalize(tf.matmul(_u_tilde, _w, transpose_b=True), name=\"v_tilde_update_{}\".format(i))\r\n            _u_tilde = tf.nn.l2_normalize(tf.matmul(_v_tilde, _w), name=\"u_tilde_update_{}\".format(i))\r\n\r\n        update_u_tilde = u_tilde.assign(_u_tilde)\r\n        sigma_w = tf.squeeze(tf.matmul(tf.matmul(_v_tilde, _w), _u_tilde, transpose_b=True), name='sigma_w')\r\n        _w_sn = _w / sigma_w\r\n        w_sn = tf.reshape(_w_sn, w.shape, name=\"reshape_weight_to_original\")\r\n        kernel_spectral_normalized = tf.identity(w_sn, name=\"kernel_spectral_normalized\")\r\n        with tf.control_dependencies([update_u_tilde]):\r\n            apply_regularization = kernel.assign(kernel_spectral_normalized, name=\"apply_regularization\")\r\n            tf.add_to_collection(name=\"SPECTRAL_NORMALIZATION\", value=apply_regularization)\r\n\r\n```\r\n\r\n2. the function is passed to a layer in [tf.layers] module\r\n\r\n`conv_layer = tf.layers.conv2d(inputs=input,  filters=128, kernel_regularizer=spectral_normalization)\r\n`\r\n\r\n3. run the regularizer after each update\r\n\r\n\r\n```\r\nweight_regularization_op = tf.get_collection(\"SPECTRAL_NORMALIZATION\")\r\n.\r\n.\r\n.\r\nwith tf.Session() as sess:\r\n    for _ in range(num_epoch):\r\n        sess.run(train_op)\r\n        sess.run(weight_regularization_op)\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n2018-12-31 17:47:41.987355: E tensorflow/stream_executor/cuda/cuda_dnn.cc:82] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(2531): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd.handle(), input_data.opaque(), filter.handle(), filter_d\r\nata.opaque(), conv.handle(), ToConvForwardAlgo(algo_desc), scratch.opaque(), scratch.size(), beta, output_nd.handle(), output_data->opaque())'\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 72, in <module>\r\n    gran.train()\r\n  File \"main.py\", line 43, in train\r\n    save_epoch=self.base_option['save_epoch'])\r\n  File \"/home/bispl/github/generative_recon/network.py\", line 51, in train\r\n    self.session.train_graph(savedir=savedir, save_epoch=save_epoch)\r\n  File \"/home/bispl/github/generative_recon/session.py\", line 112, in train_graph\r\n    feed_dict={self.graph.image_input: train_image_input, self.graph.bold_input: train_bold_input, self.graph.unpaired_image_input: train_unpaired_image_input, self.gra\r\nph.training: True, self.graph.generator_learning_rate: self.generator_learning_rate})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: cuDNN launch failure : input shape([16,128,24,32]) filter shape([7,7,128,256])\r\n         [[node GRAN_model/encoder_models/encoder_real/encoder/encoder_conv_3/Conv2D (defined at /home/bispl/github/generative_recon/model_components.py:37)  = Conv2D[T\r\n=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:\r\n0\"](GRAN_model/encoder_models/encoder_real/encoder/encoder_maxpool_0/MaxPool, encoder/encoder_conv_3/kernel/read)]]\r\n         [[{{node GRAN_graph_metrics/generator_real_loss/update_op/_215}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", s\r\nend_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_23669_GRAN_graph_metrics/generator_real_loss/update_op\", tensor_\r\ntype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op u'GRAN_model/encoder_models/encoder_real/encoder/encoder_conv_3/Conv2D', defined at:\r\n  File \"main.py\", line 70, in <module>\r\n    gran.initialize()\r\n  File \"main.py\", line 33, in initialize\r\n    config=self.config)\r\n  File \"/home/bispl/github/generative_recon/network.py\", line 32, in build_network\r\n    self.graph.build_model(model=self.model)\r\n  File \"/home/bispl/github/generative_recon/graph.py\", line 45, in build_model\r\n    self.model.build_model(image_input=self.image_input, bold_input=self.bold_input, unpaired_image_input=self.unpaired_image_input, training=self.training)\r\n  File \"/home/bispl/github/generative_recon/model.py\", line 38, in build_model\r\n    self.encoder_real_model_output = self.encoder.build_model(image_input=self.image_input, bold_input=self.bold_input, model_scope='encoder_real', reuse=False)\r\n  File \"/home/bispl/github/generative_recon/model_components.py\", line 37, in build_model\r\n    _x = tf.layers.conv2d(inputs=_x, filters=256, kernel_size=[7,7], kernel_initializer=self.initializer, kernel_regularizer=self.regularizer, kernel_constraint=self.co\r\nnstraint, strides=1, padding='SAME', name=self.scope+'_conv_3')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.py\", line 417, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 817, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 374, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 757, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 194, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 868, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 520, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 204, in __call__\r\n    name=self.name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 957, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): cuDNN launch failure : input shape([16,128,24,32]) filter shape([7,7,128,256])\r\n         [[node GRAN_model/encoder_models/encoder_real/encoder/encoder_conv_3/Conv2D (defined at /home/bispl/github/generative_recon/model_components.py:37)  = Conv2D[T\r\n=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:\r\n0\"](GRAN_model/encoder_models/encoder_real/encoder/encoder_maxpool_0/MaxPool, encoder/encoder_conv_3/kernel/read)]]\r\n         [[{{node GRAN_graph_metrics/generator_real_loss/update_op/_215}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", s\r\nend_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_23669_GRAN_graph_metrics/generator_real_loss/update_op\", tensor_\r\ntype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]", "comments": ["Hello @egyptdj , Thank you for the detailed issue template response. Can we please try to step back to gcc 4.8 ? Please let us know how the runs on (1), (2) & (3) transpire. Thanks.", "Hello @msymp , \r\nThank you for your help.\r\n\r\nI removed and re-compiled the tensorflow-gpu with gcc --version 4.8.5.\r\nIt still seems that the same problem occurs when running (1), (2), and (3), but at a different layer. (I did not change the layer numbers or capacity, i.e. same network.). The time for running the network was almost the same as before. \r\n\r\nI will attach the new error message at the bottom of this comment.\r\nPlease let me know if I need to check anything further.\r\nThanks.\r\n\r\n=================================================\r\n\r\n2019-01-08 11:07:37.148026: E tensorflow/stream_executor/cuda/cuda_dnn.cc:82] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(3197): 'cudnnConvolutionBackwardData(cudnn.handle(), alpha, filter.handle(), filter_data.opaque(), out_back_nd.handle(), backward_output_data.opaque()\r\n, conv.handle(), ToConvBackwardDataAlgo(algo_desc), scratch.opaque(), scratch.size(), beta, in_back_nd.handle(), backward_input_data->opaque())'\r\n2019-01-08 11:07:37.218704: I tensorflow/stream_executor/stream.cc:5049] [stream=0x7fc2252e23d0,impl=0x7fc2252e2470] did not memzero GPU location; source: 0x7fc21d7f8dc0\r\nTraceback (most recent call last):\r\nFile \"main.py\", line 43, in train                                                                                                                                                        \r\n    save_epoch=self.base_option['save_epoch'])\r\n  File \"/home/bispl/github/generative_recon/network.py\", line 51, in train\r\n    self.session.train_graph(savedir=savedir, save_epoch=save_epoch)\r\n  File \"/home/bispl/github/generative_recon/session.py\", line 112, in train_graph\r\n    feed_dict={self.graph.image_input: train_image_input, self.graph.bold_input: train_bold_input, self.graph.unpaired_image_input: train_unpaired_image_input, self.graph.training: True, self.graph\r\n.generator_learning_rate: self.generator_learning_rate})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: cuDNN Backward Data function launch failure : input shape([16,256,12,16]) filter shape([7,7,256,256])\r\n         [[node GRAN_graph_op/gradients_2/GRAN_model/generator_models/generator_noise/generator/generator_conv_2/Conv2D_grad/Conv2DBackpropInput (defined at /home/bispl/github/generative_recon/grap\r\nh.py:163)  = Conv2DBackpropInput[T=DT_FLOAT, _class=[\"loc:@GRAN_...dependency\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/j\r\nob:localhost/replica:0/task:0/device:GPU:0\"](GRAN_graph_op/gradients_2/GRAN_model/generator_models/generator_noise/generator/generator_conv_2/Conv2D_grad/ShapeN, generator/generator_conv_2/kernel/r\r\nead, GRAN_graph_op/gradients_2/AddN_53, ^GRAN_graph_op/gradients_2/GRAN_model/generator_models/generator_noise/generator/generator_conv_2/BiasAdd_grad/tuple/group_deps)]]\r\n         [[{{node GRAN_graph_metrics/ssim/update_op/_221}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/\r\ndevice:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_23675_GRAN_graph_metrics/ssim/update_op\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op u'GRAN_graph_op/gradients_2/GRAN_model/generator_models/generator_noise/generator/generator_conv_2/Conv2D_grad/Conv2DBackpropInput', defined at:\r\n  File \"main.py\", line 70, in <module>\r\n    gran.initialize()\r\n  File \"main.py\", line 33, in initialize\r\n    config=self.config)\r\n  File \"/home/bispl/github/generative_recon/network.py\", line 33, in build_network\r\n    self.graph.build_graph()\r\n  File \"/home/bispl/github/generative_recon/graph.py\", line 163, in build_graph\r\n    self.generator_train = tf.train.AdamOptimizer(learning_rate=self.generator_learning_rate, beta1=0.5, name='generator_optimizer').minimize(self.generator_weighted_loss, var_list=self.generator_v\r\nariables, name='generator_train')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 400, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 519, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 630, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 814, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 408, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 814, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py\", line 517, in _Conv2DGrad\r\n    data_format=data_format),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1229, in conv2d_backprop_input\r\n    dilations=dilations, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n...which was originally created as op u'GRAN_model/generator_models/generator_noise/generator/generator_conv_2/Conv2D', defined at:\r\n  File \"main.py\", line 70, in <module>\r\n    gran.initialize()\r\n[elided 0 identical lines from previous traceback]\r\n  File \"main.py\", line 33, in initialize\r\n    config=self.config)\r\n  File \"/home/bispl/github/generative_recon/network.py\", line 32, in build_network\r\n    self.graph.build_model(model=self.model)\r\n  File \"/home/bispl/github/generative_recon/graph.py\", line 45, in build_model\r\n    self.model.build_model(image_input=self.image_input, bold_input=self.bold_input, unpaired_image_input=self.unpaired_image_input, training=self.training)\r\n  File \"/home/bispl/github/generative_recon/model.py\", line 33, in build_model\r\n    self.generator_model_output = self.generator.build_model(image_input=self.image_input, bold_input=self.noise_bold_input, model_scope='generator_noise', reuse=False)\r\n  File \"/home/bispl/github/generative_recon/model_components.py\", line 169, in build_model\r\n    _y = tf.layers.conv2d(inputs=_y, filters=256, kernel_size=[7,7], kernel_initializer=self.initializer, kernel_regularizer=self.regularizer, kernel_constraint=self.constraint, strides=1, padding='SAME', name=self.scope+'_conv_2')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.py\", line 417, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 817, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 374, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 757, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 194, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 868, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 520, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 204, in __call__\r\n    name=self.name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 957, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n\r\nInternalError (see above for traceback): cuDNN Backward Data function launch failure : input shape([16,256,12,16]) filter shape([7,7,256,256])\r\n         [[node GRAN_graph_op/gradients_2/GRAN_model/generator_models/generator_noise/generator/generator_conv_2/Conv2D_grad/Conv2DBackpropInput (defined at /home/bispl/github/generative_recon/graph.py:163)  = Conv2DBackpropInput[T=DT_FLOAT, _class=[\"loc:@GRAN_...dependency\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](GRAN_graph_op/gradients_2/GRAN_model/generator_models/generator_noise/generator/generator_conv_2/Conv2D_grad/ShapeN, generator/generator_conv_2/kernel/read, GRAN_graph_op/gradients_2/AddN_53, ^GRAN_graph_op/gradients_2/GRAN_model/generator_models/generator_noise/generator/generator_conv_2/BiasAdd_grad/tuple/group_deps)]]\r\n         [[{{node GRAN_graph_metrics/ssim/update_op/_221}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_23675_GRAN_graph_metrics/ssim/update_op\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]", "Hello @azaks2  & @asimshankar , this user has written a custom regularizer (spectral normalization) that is passed to the tf.layers module, and the behavior is as follows: there is a significant slowdown in the first epoch, and further slowdown in successive epochs, and finally aborts with an error after several epochs. Can you please advise. Thanks.", "Hello, @azaks2 and @asimshankar . Is this issue on the fix by Tensorflowers or is it still pending? Sorry to make a rush, but if this issue is something that is not to be solved (or which takes a long time to solve), then I need to work on another implementation approach of the algorithm. Thanks.", "CC @tatianashp ", "I do not know what spectral_normalization is, and am not sure why it's crashing But, a kernel_regularizer is supposed to be a callable that takes in a variable, and returns a regularization loss. A  Instead of doing\r\n\r\n```\r\nconv_layer = tf.layers.conv2d(inputs=input, filters=128, kernel_regularizer=spectral_normalization)\r\n```\r\n\r\nTry doing something like\r\n```\r\nconv_layer = tf.layers.Conv2D(filters=128)\r\noutput = conv_layer(input)\r\nweight_regularization_op = spectral_normalization(conv_layer.kernel)\r\n```\r\n\r\n", "Closing the issue since there hasn't been an update since Apr 3.  Please reopen if this is still relevant.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24660\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24660\">No</a>\n", "even i had the same issue which resolve by installing the right version of cudatoolkit"]}, {"number": 24659, "title": "TensorArrays in tf.map_fn are being placed on CPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version:3.5.2\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Compute capability 7, ~30GB\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\ndef get_index(code_book, tensor):\r\n    def index(t):\r\n        a = tf.where(tf.equal(code_book, tf.fill(code_book.shape, t)))[:,-1]\r\n        return a\r\n    tensor_flat = tf.reshape(tensor, [-1])\r\n    return tf.reshape(tf.map_fn(index, tensor_flat, dtype=tf.int64, back_prop=False, parallel_iterations=20), tensor.shape)\r\n```    \r\nI am using the above function to find the indexes of elements in `tensor`, in `code_book`. `tensor` is a rank 1 float16 tensor.           \r\nWhen running,          \r\nI noticed that after sometime, GPU is sitting idle and CPU is being used.        \r\nI checked the device placements of the ops and found these,      \r\n```\r\nmap_52/TensorArray_1: (TensorArrayV3): /job:localhost/replica:0/task:0/device:CPU:0\r\nmap_52/while/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/device:CPU:0\r\nmap_52/while/Shape: (Shape): /job:localhost/replica:0/task:0/device:CPU:0\r\nmap_52/while/TensorArrayWrite/TensorArrayWriteV3/Enter: (Enter): /job:localhost/replica:0/task:0/device:CPU:0\r\nmap_52/while/TensorArrayWrite/TensorArrayWriteV3: (TensorArrayWriteV3): /job:localhost/replica:0/task:0/device:CPU:0\r\nmap_52/TensorArrayStack/TensorArraySizeV3: (TensorArraySizeV3): /job:localhost/replica:0/task:0/device:CPU:0\r\nmap_52/TensorArrayStack/range: (Range): /job:localhost/replica:0/task:0/device:CPU:0\r\nmap_52/TensorArrayStack/TensorArrayGatherV3: (TensorArrayGatherV3): /job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\n\r\nWhen forcing 'tf.map_fn' to be placed on GPU using `tf.device`, I get these errors,          \r\n```Cannot assign a device for operation map/TensorArray_1: Could not satisfy explicit device specification '' because the node no[11/1932$\r\nnsorArray_1 (defined at weights_changer.py:63) having device No device assignments were active during op 'map/TensorArray_1' creation. \r\n was colocated with a group of nodes that required incompatible device '/device:GPU:0'\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nTensorArrayGatherV3: CPU XLA_CPU XLA_GPU \r\nRange: GPU CPU XLA_CPU XLA_GPU \r\nConst: GPU CPU XLA_CPU XLA_GPU \r\nTensorArraySizeV3: CPU XLA_CPU XLA_GPU \r\nTensorArrayWriteV3: CPU XLA_CPU XLA_GPU \r\nEnter: CPU XLA_CPU XLA_GPU \r\nStridedSlice: GPU CPU XLA_CPU XLA_GPU \r\nTensorArrayV3: GPU CPU XLA_CPU XLA_GPU \r\n\r\nColocation members and user-requested devices:\r\n  map/TensorArray_1 (TensorArrayV3) \r\n  map/while/strided_slice (StridedSlice) /device:GPU:0\r\n  map/while/TensorArrayWrite/TensorArrayWriteV3/Enter (Enter) /device:GPU:0\r\n  map/while/TensorArrayWrite/TensorArrayWriteV3 (TensorArrayWriteV3) /device:GPU:0\r\n  map/TensorArrayStack/TensorArraySizeV3 (TensorArraySizeV3) \r\n  map/TensorArrayStack/range/start (Const) \r\n  map/TensorArrayStack/range/delta (Const) \r\n  map/TensorArrayStack/range (Range) \r\n  map/TensorArrayStack/TensorArrayGatherV3 (TensorArrayGatherV3) \r\n[[node map/TensorArray_1 (defined at weights_changer.py:63)  = TensorArrayV3[clear_after_read=true, dtype=DT_INT64, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name=\"\"](map/TensorArray_1/size)]]\r\n\r\nNo node-device colocations were active during op 'map/TensorArray_1' creation.\r\nNo device assignments were active during op 'map/TensorArray_1' creation.\r\n```\r\nIs this the expected behavior?", "comments": ["Apparently it was happening for int64 type. Works fine for float16/32"]}]