[{"number": 53195, "title": "Addition of GradCAM visualization option for output visualization of CNNs", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.6.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nGradCAM visualization is a great algorithm to showcase the activation part of an image from a CNN. As described in this paper(https://arxiv.org/pdf/1610.02391.pdf), it can allow users to visualize the issues their model is making in a more interactive way and fix the models.\r\n\r\n**Will this change the current api? How?**\r\nYes, it will change the current API by the addition of a GradCAM visualization function in utils or the addition of a new visualization module itself. Depends on the admins.\r\n\r\n**Who will benefit from this feature?**\r\nIt will help users who are using CNN to visualize the issue their model is making in a more interactive way by looking at the class activation map of the image and having a look at which part of the image the model gets activated to predict the class. \r\n\r\n**Any Other info.**\r\nNo. \r\n", "comments": ["Hi @MrinalTyagi ! But There is already[ GradCAM visualization](https://keras.io/examples/vision/grad_cam/) in Keras ? Are you indicating to include everything as single method? Thanks!", "@mohantym yeah. Currently, the user has to copy-paste the code from the website and modify it according to their model data and all that. I was thinking of the addition of a single function in utils or a new visualization section in which all these visualization techniques can be included in form of user handy functions. Would love to know your thoughts in the same. ", "@MrinalTyagi I like your proposal. There are some 3rd party implementations ([tf.keras viz](https://github.com/keisen/tf-keras-vis)) but to have them on core API would be great of course. However, I think such a request is more fit on [keras](https://github.com/keras-team/keras). ", "@innat Thank you for your response. Let's see what @mohantym thinks. Will it be better to add just a method or add a separate section in TensorFlow for visualization. ", "@mohantym like in the link @innat provided, there are multiple visualization algorithms. What I was suggesting was to like alongside utils we could add another module of visualization having all these multiple visualization algorithms. ", "Ok @MrinalTyagi ! Please post this issue with this clarity on [keras-team/keras repo](https://github.com/keras-team/keras/issues) too.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Sure. I have updated it under keras issues.", "Ok! Could you please this issue here as It will be tracked there in Keras repo?", "@mohantym As Keras team is not interested in adding this feature in their core API and wants to rely the users on third-party implementation for the same, is there any option of adding it into TensorFlow or should I scrap off the idea completely?", "Hi @MrinalTyagi ! Could you please mention that Keras ticket here?", "> Hi @MrinalTyagi ! Could you please mention that Keras ticket here?\r\nhttps://github.com/keras-team/keras/issues/15704 \r\n", "Closing this issue as per the comment here https://github.com/keras-team/keras/issues/15704#issuecomment-984897075"]}, {"number": 53194, "title": "If args is a list, the value of parameter `shell` should be true.", "body": "If args is a list, shouldn't the value of parameter a be true? Otherwise, only the first item in the args list will be executed as a shell character. ", "comments": ["If args is a list, shouldn't the value of parameter `shell` be true? Otherwise, only the first item in the args list will be executed as a shell character. ", "@Doswind Can you please sign CLA. Thanks!", "> @Doswind Can you please sign CLA. Thanks!\r\n\r\nI've signed the CLA, but it's still showing me as unsigned.  And i've checked  i'm using the same email/username in my GitHub and Google CLA.  Can you rescan this PR?  Thanks!.", "Is there something that fails without this patch and passes with it?", "> Is there something that fails without this patch and passes with it?\r\n\r\nNo errors were found. However,i think the usage is wrong. In the actual test, only the first command in the list will be executed, and the other command parameters will not be executed unless the program does so intentionally", "Please add a unit test", "@Doswind Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "@Doswind Any update on this PR? Please. Thanks!"]}, {"number": 53193, "title": "programming failure", "body": "a\r\n", "comments": ["@petche \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53192, "title": "tf.function(jit_compile=True) cannot access int32 weights on GPU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 20.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\nv2.6.0-0-g919f693420e 2.6.0\r\n- Python version:\r\nPython 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n11.3 8.2\r\n- GPU model and memory:\r\nA100 40GB\r\n\r\n**Describe the current behavior**\r\n\r\nFunctions with tf.function(jit_compile=True) cannot access weights of type int32 on the GPU.  It gives error at runtime\r\n```\r\n2021-11-24 15:12:28.148837: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at xla_ops.cc:238 : Invalid argument: Trying to access resource _AnonymousVar2 (defined @ /lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/keras/engine/base_layer_utils.py:117) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nno error\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\nno\r\n- Briefly describe your candidate solution(if contributing):\r\nno solution\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nPlease just run the following code,\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as tk\r\n\r\nclass Scale(tk.layers.Layer):\r\n    def __init__(self, c, dt):\r\n        super(Scale, self).__init__(name='Scale')\r\n        self.c = self.add_weight(initializer=tf.keras.initializers.Constant(c), dtype=dt)\r\n    @tf.function(jit_compile=True)\r\n    def call(self, x):\r\n        return self.c*x\r\n\r\nprint('float32:', Scale(2, tf.float32)(tf.ones(4, dtype=tf.float32)))\r\nprint('int64:', Scale(2, tf.int64)(tf.ones(4, dtype=tf.int64)))\r\nprint('int32:', Scale(2, tf.int32)(tf.ones(4, dtype=tf.int32)))  # This line gives error\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nBelow is the output from running the above code,\r\n```\r\n$ python issue_int32.py \r\n2021-11-24 15:16:53.015406: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-24 15:16:53.423483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38458 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\r\n2021-11-24 15:16:53.776487: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x562deb0ee730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-11-24 15:16:53.776535: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\r\n2021-11-24 15:16:53.818100: I tensorflow/compiler/jit/xla_compilation_cache.cc:363] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\nfloat32: tf.Tensor([2. 2. 2. 2.], shape=(4,), dtype=float32)\r\nint64: tf.Tensor([2 2 2 2], shape=(4,), dtype=int64)\r\n2021-11-24 15:16:53.865986: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at xla_ops.cc:238 : Invalid argument: Trying to access resource _AnonymousVar2 (defined @ /lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/keras/engine/base_layer_utils.py:117) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0\r\nTraceback (most recent call last):\r\n  File \"issue_int32.py\", line 14, in <module>\r\n    print('int32:', Scale(2, tf.int32)(tf.ones(4, dtype=tf.int32)))\r\n  File \"/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1037, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 956, in _call\r\n    return self._concrete_stateful_fn._call_flat(\r\n  File \"/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 591, in call\r\n    outputs = execute.execute(\r\n  File \"/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource _AnonymousVar2 (defined @ /lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/keras/engine/base_layer_utils.py:117) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_call_44]\r\n```", "comments": ["@sachinprasadhs ,\r\nI was able to reproduce the issue in tf [v2.7](https://colab.research.google.com/gist/tilakrayal/1f2a2a9dd6b5d7d61e533d476e72c6f5/2-7-53192.ipynb), [v2.5](https://colab.research.google.com/gist/tilakrayal/bc1f649e43e2e0cff0392ff7ae4fc9a4/2-6-53192.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/9be703622882e48c696679017784b2b2/nightly.ipynb).Please find the gist.", "There is a known issue with `tf.Variable`: https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\r\n\r\nThe `self.v` here is an int32 `tf.Variable` that can't be placed on GPU. You are suggested to use `int64` or `int32 + jit_compile=False`.", "Assuming the developers have a good reason for this odd behavior of `int32`, it would be useful for users to make this behavior controllable.  Is there a way to force int32 tensors on a GPU?\r\n\r\nAt the very least the error message could be much more useful that what it is now.  Complaining about an `_AnonymousVarNNN` located on a different device that where it is supposed to be is useless.", "> Assuming the developers have a good reason for this odd behavior of `int32`\r\n\r\nThere is no good reason to do that. It behaves that way because TF lacks a dedicated type for shapes, which must reside on CPU. We're looking into ways to fix this, but it's complicated and will take a while.\r\n\r\n+1  to improving the error message.", "@jxy @mdanatg From what I see the real bug is that device assignment is ignored and the variable is placed on CPU despite explicit GPU placement (speaking of shapes, do those ever reside inside variables though?). Once the placement on CPU happens, there's nothing much we can do: what would you expect to happen? Extra H2D copy potentially ruining performance? We can't really pattern match this case to a specific error message, as that would just create a more fragile system with more levels of indirection swallowing errors.", "Just making it clearer that the variable (and which variable) was placed on CPU would help. The information is there (e.g. we have the internal name, and line number), so surfacing more clearly would help (e.g. include the actual line of code). \"_AnonymousVarNNN\" and \"__inference_call_44\" don't say much.\r\nAgree that there's otherwise not much to do by the time things got here. Might be worth trying `tf.config.set_soft_device_placement(False)`, though that has caveats too.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Does leaving random comments count as activity?", "@jxy I'm not sure to be honest. Filing a new bug about improved error message would be probably better, what would you like to see? Just a better stack trace presentation, or TF somehow pattern-matching the error message into \"ah you are using int32 weights inside a compiled function run on GPU, why not make them int64\"? Or would you prefer TF to automatically copy the weights? (CC @smit-hinsu )", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53192\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53192\">No</a>\n"]}, {"number": 53191, "title": "Converted TFlite image segmentation model crashes in Android.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android sdk version 29\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung A10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3,9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI have implemented image segmentation model with two stream and was able convert the model into TFlite format. There is no issue when prediction taken using Python API from coverted TFlite. When model in used in Android application using task api library it crashes without any errors in the line below.  It seems there is memory allocation issue when segment method is called. \r\n```\r\nimageSegmenter.segment(image);\r\n``` \r\nI have included the link for converted TFlite model below.\r\nhttps://drive.google.com/drive/folders/1sBRCNNa8c59315wXZ3tPzkINnV_F2o3E?usp=sharing\r\n\r\nAndroid dependancy used.\r\nimplementation 'org.tensorflow:tensorflow-lite-task-vision:0.3.0'\r\n", "comments": ["@Sudhan97 Could you please try with the latest `TF v2.7.0` and refer to the similar [issue](https://stackoverflow.com/questions/54762245/tensorflow-tf-lite-android-app-crashing-after-detection) , [link](https://www.tensorflow.org/lite/inference_with_metadata/task_library/image_segmenter) of TFlite doc  ? Please let us know if it helps? Thanks!", "Hi @sushreebarsa, I trained the model and converrted into TFlite format using TF v.2.7.0 it work fine with python tf interpreter api but fails in android.  When imageSegmenter.segment() method is called in Android application it crashes without raising any exceptions. I used the following dependancy in Android and referred to instruction in [link](https://www.tensorflow.org/lite/inference_with_metadata/task_library/image_segmenter#step_2_using_the_model).\r\n`\r\nimplementation 'org.tensorflow:tensorflow-lite-task-vision:0.3.0+'\r\n`\r\n\r\nI have included the link for converted TFlite model below.\r\nhttps://drive.google.com/drive/folders/1sBRCNNa8c59315wXZ3tPzkINnV_F2o3E?usp=sharing", "Hi @Sudhan97, are you using Android Studio to run the code on your device? Android Studio must have a `Logcat` tab that displays detailed error messages associated with the crash. If you can find it, can you share the error message here?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53191\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53191\">No</a>\n"]}, {"number": 53190, "title": "Importing models directly into Python Variables from model files stored in S3 bucket", "body": "Hello,\r\n\r\nI have been thinking from quite some time that if it would have been possible to ***import the ML models directly into python variables*** when the model file is actually not at the local disk where the code is running but rather is ***stored in an S3 bucket***.\r\n\r\nAs we can load a model file/folder present in the local disk using the tensorflow functions like `tf.keras.models.load_model(\"<model_file_name>\")`, is there any possible way to import the model from the model files stored in the S3 bucket?\r\n\r\nBecause there can be scenarios where we are running some code to import model inside a container and that container has a limited storage bandwidth but we know the model sizes can be extremely large and thus it would become difficult for it to be stored inside a container or a persistent storage.\r\n\r\nFor, eg. I am able to use the below specified code to import a dataset (CSV File) directly into memory (basically into a python variable) using the ***Pandas*** and the ***Boto3*** library:\r\n\r\n```\r\nfilename = \"dataset/items_dataset.csv\"\r\ns3 = boto3.client('s3',\r\n                  aws_access_key_id = '<AWS_ACCESS_KEY_ID>',\r\n                  aws_secret_access_key = '<AWS_SECRET_ACCESS_KEY>')\r\n\r\nobj = s3.get_object(Bucket='<AWS_BUCKET_NAME>', Key=filename)\r\ndf = pd.read_csv(io.BytesIO(obj['Body'].read()))\r\ndf.head()\r\n```\r\nwhere `dataset/items_dataset.csv` is the path of the dataset file in the S3 bucket. \r\n<br>\r\nThus, I wanted to know whether is it possible to have a similar way of importing the model file stored in the S3 bucket without having the need of downloading it to the local disk before loading it to memory through tensorflow functions.\r\n\r\nAlso, if there is any way of achieving this, then is there any way to do the vice-versa as well, i.e. *to store the model directly into the S3 bucket?*", "comments": ["@altruistcoder ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or if possible share a colab gist with the issue reported.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@tilakrayal I am sorry if it was not clear from the Title but this is not any issue specifically or it maybe I am not sure. I have general query which might be valid for all the versions of Tensorflow and there are no specific steps to reach this.  \r\nI just want to know whether is it possible to directly load a model (using any Tensorflow function) from the model file which is stored in a S3 bucket.  \r\nIf yes, can you please instruct me to navigate somewhere where I can get more information regarding how to achieve this. It would be really very helpful for me.", "@altruistcoder ,\r\nThis question is better asked on TensorFlow Forum. Kindly open a tensorflow discussion [forum](https://discuss.tensorflow.org/) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hello @tilakrayal, I have tried asking this on the Tensorflow's forum as well but I haven't got any reply there as well. You can find the topic I created here:\r\n[https://discuss.tensorflow.org/t/importing-models-directly-into-python-variables-from-model-files-stored-in-s3-bucket/6538](https://discuss.tensorflow.org/t/importing-models-directly-into-python-variables-from-model-files-stored-in-s3-bucket/6538)\r\n", "@altruistcoder ,\r\nAs it is not a bug or feature request from tensorflow side, we suggest to follow the raise tensorflow forum discussion.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53189, "title": "Unit test //tensorflow/compiler/xla/tests:xla_hlo_profile_test_cpu gives illegal instruction on AARCH64", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): git HEAD\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 11.2.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nTest fails as illegal instruction is thrown.\r\n\r\n**Describe the expected behavior**\r\n\r\nTest passes\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=\"\"  --remote_cache_proxy=\"\" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --cxxopt=-ffp-contract=off --copt=-Og --copt=-ggdb --verbose_failures -- //tensorflow/compiler/xla/tests:xla_hlo_profile_test_cpu\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nbazel-bin/tensorflow/compiler/xla/tests/xla_hlo_profile_test_cpu\r\n[==========] Running 2 tests from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 2 tests from HloProfileTest\r\n[ RUN      ] HloProfileTest.ProfileSingleComputation\r\n2021-11-24 17:03:03.560320: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x3c904070 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-11-24 17:03:03.560415: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Host, Default Version\r\n2021-11-24 17:03:03.560854: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x3c904d60 initialized for platform Interpreter (this does not guarantee that XLA will be used). Devices:\r\n2021-11-24 17:03:03.560885: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Interpreter, <undefined>\r\nIllegal instruction (core dumped)\r\n\r\nWhen running under gdb\r\n\r\nThread 70 \"xla_hlo_profile\" received signal SIGILL, Illegal instruction.\r\n[Switching to Thread 0xfffec6ffcd80 (LWP 420853)]\r\n0x0000fffff7ff8014 in ProfileSingleComputation.5 ()\r\n(gdb) disass\r\nDump of assembler code for function ProfileSingleComputation.5:\r\n   0x0000fffff7ff8000 <+0>:\tstr\td12, [sp, #-48]!\r\n   0x0000fffff7ff8004 <+4>:\tstp\td11, d10, [sp, #16]\r\n   0x0000fffff7ff8008 <+8>:\tstp\td9, d8, [sp, #32]\r\n   0x0000fffff7ff800c <+12>:\tmov\tx10, xzr\r\n   0x0000fffff7ff8010 <+16>:\tldp\tx9, x13, [x3, #8]\r\n=> 0x0000fffff7ff8014 <+20>:\tmrs\tx8, pmccntr_el0\r\n   0x0000fffff7ff8018 <+24>:\tadd\tx11, x9, #0x20\r\n   0x0000fffff7ff801c <+28>:\tldr\tx9, [x3]\r\n   0x0000fffff7ff8020 <+32>:\tadd\tx12, x9, #0x30\r\n   0x0000fffff7ff8024 <+36>:\tadd\tx13, x13, #0x20\r\n   0x0000fffff7ff8028 <+40>:\tmov\tx14, xzr\r\n   0x0000fffff7ff802c <+44>:\tadd\tx15, x11, x14\r\n   0x0000fffff7ff8030 <+48>:\tadd\tx16, x13, x14\r\n   0x0000fffff7ff8034 <+52>:\tldp\tq0, q1, [x15, #-32]\r\n   0x0000fffff7ff8038 <+56>:\tldp\tq2, q3, [x16, #-32]\r\n   0x0000fffff7ff803c <+60>:\tldp\tq4, q5, [x15]\r\n   0x0000fffff7ff8040 <+64>:\tfadd\tv0.4s, v0.4s, v2.4s\r\n   0x0000fffff7ff8044 <+68>:\tfadd\tv1.4s, v1.4s, v3.4s\r\n   0x0000fffff7ff8048 <+72>:\tldp\tq2, q3, [x16]\r\n   0x0000fffff7ff804c <+76>:\tfadd\tv2.4s, v4.4s, v2.4s\r\n   0x0000fffff7ff8050 <+80>:\tadd\tx15, x12, x14\r\n   0x0000fffff7ff8054 <+84>:\tstp\tq0, q1, [x15, #-48]\r\n   0x0000fffff7ff8058 <+88>:\tfadd\tv0.4s, v5.4s, v3.4s\r\n   0x0000fffff7ff805c <+92>:\tstp\tq2, q0, [x15, #-16]\r\n   0x0000fffff7ff8060 <+96>:\tadd\tx14, x14, #0x40\r\n   0x0000fffff7ff8064 <+100>:\tcmp\tx14, #0x400\r\n   0x0000fffff7ff8068 <+104>:\tb.ne\t0xfffff7ff802c <ProfileSingleComputation.5+44>  // b.any\r\n   0x0000fffff7ff806c <+108>:\tadd\tx10, x10, #0x1\r\n   0x0000fffff7ff8070 <+112>:\tadd\tx11, x11, #0x400\r\n   0x0000fffff7ff8074 <+116>:\tadd\tx12, x12, #0x400\r\n   0x0000fffff7ff8078 <+120>:\tadd\tx13, x13, #0x400\r\n   0x0000fffff7ff807c <+124>:\tcmp\tx10, #0x100\r\n   0x0000fffff7ff8080 <+128>:\tb.ne\t0xfffff7ff8028 <ProfileSingleComputation.5+40>  // b.any\r\n   0x0000fffff7ff8084 <+132>:\tmov\tx10, xzr\r\n   0x0000fffff7ff8088 <+136>:\tmrs\tx11, pmccntr_el0\r\n   0x0000fffff7ff808c <+140>:\tmov\tw12, #0xb717                \t// #46871\r\n   0x0000fffff7ff8090 <+144>:\tmovk\tw12, #0x39d1, lsl #16\r\n   0x0000fffff7ff8094 <+148>:\tdup\tv0.4s, w12\r\n   0x0000fffff7ff8098 <+152>:\tmov\tw12, #0x25c0                \t// #9664\r\n   0x0000fffff7ff809c <+156>:\tmovk\tw12, #0xa59f, lsl #16\r\n   0x0000fffff7ff80a0 <+160>:\tdup\tv1.4s, w12\r\n   0x0000fffff7ff80a4 <+164>:\tmov\tw12, #0x337e                \t// #13182\r\n   0x0000fffff7ff80a8 <+168>:\tmovk\tw12, #0x2a61, lsl #16\r\n   0x0000fffff7ff80ac <+172>:\tdup\tv2.4s, w12\r\n   0x0000fffff7ff80b0 <+176>:\tmov\tw12, #0x37ff                \t// #14335\r\n   0x0000fffff7ff80b4 <+180>:\tmovk\tw12, #0xaebd, lsl #16\r\n   0x0000fffff7ff80b8 <+184>:\tdup\tv3.4s, w12\r\n   0x0000fffff7ff80bc <+188>:\tldr\tx12, [x5, #24]\r\n   0x0000fffff7ff80c0 <+192>:\tsub\tx11, x11, x8\r\n   0x0000fffff7ff80c4 <+196>:\tadd\tx11, x11, x12\r\n   0x0000fffff7ff80c8 <+200>:\tstr\tx11, [x5, #24]\r\n   0x0000fffff7ff80cc <+204>:\tmov\tw11, #0x41                  \t// #65\r\n   0x0000fffff7ff80d0 <+208>:\tmovk\tw11, #0x335c, lsl #16\r\n--Type <RET> for more, q to quit, c to continue without paging--q\r\n\r\nSo the problem seems to be reading the performance counter register as the illegal instruction flagged is \"mrs\tx8, pmccntr_el0\"", "comments": ["@cfRod @nSircombe ", "Hi @sanatmpa1! Could you please look at this issue?", "Reading the documentation at https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/arm64/cpu-feature-registers.rst?h=v5.16-rc2 it seems that it is not possible for a user space application like TensorFlow to read the performance counters directly on AARCH64 Linux.", "It seems like the use of llvm::Intrinsic::readcyclecounter at https://github.com/tensorflow/tensorflow/blob/f37b7b1f619a424b420fcaebb7826c76a6eb9627/tensorflow/compiler/xla/service/cpu/ir_emitter.cc#L2939 is not going to work on AARCH64 Linux.", "XLA's profiling feature is not going to work on ARM/AArch64 unless the system is configured to set `PMUSERENR.EN`. I think the only reasonable thing we can do for now is disabling that test on ARM.\r\n\r\nDo you want to send a PR for that? Otherwise I can take a look.", "I propose to add the same tag as suggested in #53068 to exclude the test. I will add a PR for this shortly.\r\n\r\nThis does only deal with the unit tests however. It still leaves the feature in TF and if ever anyone attempts to make use of it on AARCH64, it will fail with SIGILL which is not really that nice for the unlucky user. So if you have some idea of how best to avoid that situation, that would be great, thanks @d0k .", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53189\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53189\">No</a>\n"]}, {"number": 53188, "title": "Fixed performance degradation with RUY library on some platforms (e.g. A53)", "body": "The RUY library changed the way to detect target processor to choose btw. kernel implementation optimized for in-order vs. out-of-order CPUs. Instead of micro-benchmark it uses `cpuinfo` library. \r\nRuy started to provide its own `CMakeLists.txt`, where this change is reflected. This patch switches to use the RUY provided CMakeLists.txt, and ensures the RUY is built with CPUINFO library.", "comments": ["@robert-kalmar Can you please resolve conflicts? Thanks!", "@kruglov-dmitry  this patch had conflict with your commit e891422915ff031da3eb2089161e34eff19e824f. \r\nRUY library is shipped with its own CMakeLists.txt. This patch removes the tensorflow/lite/cmake/ruy/CMakeLists.txt and uses the CMakeLists.txt from RUY project. \r\nIs this change OK with your patch?\r\n", "@robert-kalmar yes!\r\n\r\nI've copied changes from this [pr](https://github.com/tensorflow/tensorflow/pull/51455/files) into `tensorflow/lite/CMakeLists.txt` only \r\n\r\nBuild was configured using those flags `-D TFLITE_ENABLE_METAL=ON -D TFLITE_ENABLE_GPU=ON` and successfully produced all metal related libraries for iOS! ", "@terryheo , @gbaned  The conflict is resolved. Also kruglov-dmitry has confirmed this PR does not break his earlier contribution. You can continue with the review, \r\nThanks"]}, {"number": 53186, "title": "TensorFlow Lite Quantization quality parameters.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version 2.6.1:\r\n- Are you willing to contribute it (Yes):\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIt would be great to have quantization pamrameters / threshold to be exposed in API. Usually quantization algorithms have something like desired similarity between original activations histogram and int8 activations or something like this. I assume TFLite has such parameters buried inside. It would be great to have control over them. Perhaps per layer?\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo api breaks.\r\n\r\n**Who will benefit with this feature?**\r\n\r\npeople who use tflite for int8 inference.\r\n\r\n**Any Other info.**\r\n\r\nI could look into it, please give me pointers.\r\n", "comments": ["@bkovalenkocomp ,\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!", "> @bkovalenkocomp ,\r\n> Can you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!\r\n\r\nTFLite neural network layer quantization algorithm has parameters, that control speed of quantization vs quality of quantization, IT would be great to have these parameters exposed in api.\r\n\r\nUse case: users can choose tradeoff between need to quantize fast with lower precision or slow quantise with good precision.", "@bkovalenkocomp This is more related to [TF model optimization](https://github.com/tensorflow/model-optimization/issues) tool. If possible, post it in that repository where model-optimization team is focussing on their efforts to resolve issues faster. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53185, "title": "Valgrind complains that tflite has \"source and destination overlap in memcpy\"", "body": "## What I want\r\n\r\nHi thanks for the lib! I want to know whether this is a known thing (e.g. tflite uses a hack and valgrind wrongly thinks it is a bug), or it is a bug. If it is indeed a bug, I can try to provide more details and reproducible samples or demangled stack traces, etc. But if it is a known feature and it is Valgrind who is wrong, then I do not need to spend any more time.\r\n\r\n---\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): tflite C api compled from source\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: n/a\r\n- Bazel version (if compiling from source): na\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: na\r\n- GPU model and memory: na\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using tflite to run inference I see:\r\n\r\n```\r\n==26809== Thread 9:\r\n==26809== Source and destination overlap in memcpy_chk(0x2c30e490, 0x2c30e490, 4)\r\n==26809==    at 0x4C3DE50: __memcpy_chk (vg_replace_strmem.c:1617)\r\n==26809==    by 0x50A8A5F: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A9008: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A9586: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x7D2F906: __pthread_once_slow (pthread_once.c:116)\r\n==26809==    by 0x50A4EB6: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4A59: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4BD0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4CE0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A1FC2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A167C: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x4E972A2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809== \r\n==26809== Source and destination overlap in memcpy_chk(0x2c30e490, 0x2c30e490, 4)\r\n==26809==    at 0x4C3DE50: __memcpy_chk (vg_replace_strmem.c:1617)\r\n==26809==    by 0x50A8A5F: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A9058: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A9592: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x7D2F906: __pthread_once_slow (pthread_once.c:116)\r\n==26809==    by 0x50A4EB6: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4A59: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4BD0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4CE0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A1FC2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A167C: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x4E972A2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809== \r\n==26809== Source and destination overlap in memcpy_chk(0x2c30e490, 0x2c30e490, 4)\r\n==26809==    at 0x4C3DE50: __memcpy_chk (vg_replace_strmem.c:1617)\r\n==26809==    by 0x50A8A5F: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A90AB: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A95F2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x7D2F906: __pthread_once_slow (pthread_once.c:116)\r\n==26809==    by 0x50A4EB6: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4A59: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4BD0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4CE0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A1FC2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A167C: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x4E972A2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809== \r\n==26809== Source and destination overlap in memcpy_chk(0x2c30e490, 0x2c30e490, 4)\r\n==26809==    at 0x4C3DE50: __memcpy_chk (vg_replace_strmem.c:1617)\r\n==26809==    by 0x50A8A5F: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A90DB: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A9614: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x7D2F906: __pthread_once_slow (pthread_once.c:116)\r\n==26809==    by 0x50A4EB6: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4A59: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4BD0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A4CE0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A1FC2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x50A167C: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809==    by 0x4E972A2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)\r\n==26809== \r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nshould not have error\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing): not know...\r\n\r\n**Standalone code to reproduce the issue**\r\nn/a\r\n\r\n**Other info / logs**\r\nn/a\r\n", "comments": ["Hi @fzyzcjy ! Could you please  check this [thread](https://stackoverflow.com/questions/6317417/valgrind-says-source-and-destination-overlap-in-memcpy-about-two-buffers-but-t) once for answer?", "@mohantym Hi thanks for the reply! I have checked that thread. As the last sentence says, `EDIT Before last memcpy dim_payload is transformed with htonl() to network format and passes from 512 to 131072.. It needs to return 512 by using ntohl().`, so I suspect that is indeed a bug in his C code and valgrind is right?", "Hi @sachinprasadhs! Could you please look at this issue?", "Could you please provide some sample code to reproduce the mentioned behavior, Thanks!", "@sachinprasadhs I want to know whether this is a known thing (e.g. tflite uses a hack and valgrind wrongly thinks it is a bug), or it is a bug. If it is indeed a bug, I can try to provide more details and reproducible samples or demangled stack traces, etc. But if it is a known feature and it is Valgrind who is wrong, then I do not need to spend any more time.", "We're running several automated tests with Sanitizers. \u200bhttps://github.com/google/sanitizers \r\n\r\nBut we don't have a known issue on this yet. If you think it's a bug of TFLite, please let us know with reproduction steps.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53185\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53185\">No</a>\n"]}, {"number": 53184, "title": "warning: successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero", "body": "When running a simple script from a Tensorflow docker container I get the above mentioned warning:\r\n```\r\n$ sudo docker run --net=host  -it --gpus all tensorflow/tensorflow:2.6.0-gpu /bin/bash\r\nroot@gpu-esparig:/# python3\r\nPython 3.6.9 (default, Jan 26 2021, 15:33:00) \r\n[GCC 8.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n2021-11-24 09:01:58.877869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-24 09:01:58.899255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-24 09:01:58.900051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nNum GPUs Available:  1\r\n```\r\nI tried the following:\r\nTo get rid of this non-fatal warning use this command:\r\n`for a in /sys/bus/pci/devices/*; do echo 0 | tee -a $a/numa_node; done`\r\n\r\nBut I get `tee: '/sys/bus/pci/devices/0000:00:09.0/numa_node': Read-only file system` error.\r\n\r\nFor details go to [this question](https://stackoverflow.com/questions/44232898/memoryerror-in-tensorflow-and-successful-numa-node-read-from-sysfs-had-negativ) in stackoverflow.\r\n\r\n_Originally posted by @nsssayom in https://github.com/tensorflow/tensorflow/issues/42738#issuecomment-922422874_", "comments": ["I realized that the script needed to be run in the host machine, since /sys inventories the hardware in the system. D'oh!"]}, {"number": 53183, "title": "How to build tensorflow lite C api for Linux on a MacOS machine?", "body": "Hi thanks for the lib! I need to build tensorflow lite C api for Linux on a MacOS machine. If I do `bazel build -c opt //tensorflow/lite/c:tensorflowlite_c`, I get a `.dylib` that is of mach-o (macos) format instead of a `.so` that is of linux format. Thus I wonder what should I do? Thanks!\r\n\r\nI have tried: `--cpu=linux` and its friends (no luck). `--config-linux` (also no use).", "comments": ["@fzyzcjy ,\r\nCould you please take a look at this [link](https://www.tensorflow.org/lite/guide/build_ios) which helps for the installation of tensorflowlite.Thanks!", "no it is for ios", "@fzyzcjy ,\r\nCan you please refer this [link](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite) for the installation.Thanks", "I have tried that", "@fzyzcjy ,\r\nCan you please provide the error log where you are facing issue.It helps to analyse the issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53183\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53183\">No</a>\n"]}, {"number": 53182, "title": "tflite_runtime: Loosen numpy requirement", "body": "It's a common use case that using OpenCV with tflite_runtime. Since OpenCV requires minimum 1.19.3, we'd better loosen the numpy requirement of tflite_runtime.\r\n\r\nPiperOrigin-RevId: 411936173\r\nChange-Id: Idfda9382e5af534a2fc1beffcadc6428acc12201", "comments": []}, {"number": 53180, "title": "group conv2d can't backprop properly", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n\r\n**Describe the current behavior**\r\n\r\nI extend tf.keras.layers.Conv2D with the public member function convolution_op() to do group convolution. error occurs at backpropagation. if I change the group number to 1, backpropagation is ok.\r\n\r\n**Describe the expected behavior**\r\n\r\nextended convolution op should support backpropagation of group convolution.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass WSConv2D(tf.keras.layers.Conv2D):\r\n  def build(self, input_shape):\r\n    super(WSConv2D, self).build(input_shape);\r\n    self.gain = self.add_weight(shape = (tf.shape(self.kernel)[-1],), dtype = tf.float32, initializer = tf.keras.initializers.Ones()); # self.gain.shape = (cout,)\r\n  def call(self, inputs):\r\n    input_shape = inputs.shape;\r\n    if self._is_causal:  # Apply causal padding to inputs for Conv1D.\r\n      inputs = array_ops.pad(inputs, self._compute_causal_padding(inputs));\r\n    # standardize weight\r\n    mean, var = tf.nn.moments(self.kernel, axes = [0, 1, 2], keepdims = True);\r\n    fan_in = tf.cast(tf.math.reduce_prod(tf.shape(self.kernel)[:-1]), dtype = tf.float32); # fan_in.shape = ()\r\n    kernel = self.gain * (self.kernel - mean) / tf.math.sqrt(tf.math.maximum(var * fan_in, 1e-4));\r\n    # convolution\r\n    outputs = self.convolution_op(inputs, kernel);\r\n    if self.use_bias:\r\n      outputs = outputs + self.bias;\r\n    if self.activation is not None:\r\n      return self.activation(outputs);\r\n    return outputs;\r\n\r\ninputs = tf.random.normal(shape = (4,224,224,256));\r\nconv2d = WSConv2D(256, (3,3), groups = 4, padding = 'same');\r\nwith tf.GradientTape() as tape:\r\n  outputs = conv2d(inputs);\r\n# NOTE: error occurs here\r\ngrads = tape.gradient(outputs, conv2d.trainable_weights);\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 27, in <module>\r\n    grads = tape.gradient(outputs, conv2d.trainable_weights);\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\", line 1084, in gradient\r\n    flat_grad = imperative_grad.imperative_grad(\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\", line 71, in imperative_grad\r\n    return pywrap_tfe.TFE_Py_TapeGradient(\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\", line 159, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py\", line 581, in _Conv2DGrad\r\n    gen_nn_ops.conv2d_backprop_input(\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1247, in conv2d_backprop_input\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 7107, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Computed input depth 256 doesn't match filter input depth 64 [Op:Conv2DBackpropInput]\r\n```", "comments": ["@breadbread1984 Could you please refer to the [api_docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and similar [issue](https://stackoverflow.com/questions/65417415/tf-keras-layers-conv2d-usage), [issue2](https://github.com/tensorflow/tensorflow/issues/11327),[issue3](https://stackoverflow.com/questions/60803445/tensorflow-keras-error-using-conv2d-input-depth-and-filter-depth-are-not-compati) ,[issue4](https://stackoverflow.com/questions/35488717/confused-about-conv2d-transpose) ?Please let us know if it helps? Thank you!", "the refered issues mention no group convolution with customized conv layer. I follow the api doc and make sure that \"Input channels and filters must both be divisible by groups\". channels 256 and filters 256 is divisible by 4. the forward path can be runned without any problem, the backward path causes errors. this issue is a novel one.", "I found the buildin conv2d has the same problem. so the code to reproduce the issue can be further simplified as the following code\r\n\r\n```python\r\nimport tensorflow as tf;\r\n\r\ninputs = tf.random.normal(shape = (4,224,224,256));\r\nconv2d = tf.keras.layers.Conv2D(256, (3,3), groups = 4, padding = 'same');\r\nwith tf.GradientTape() as tape:\r\n  outputs = conv2d(inputs);\r\n# NOTE: error occurs here\r\ngrads = tape.gradient(outputs, conv2d.trainable_variables);\r\n```", "@sanatmpa1 Was able to replicate the issue on Colab using TF v2.7.0 and tf-nightly, please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/0485d198b6761f103fdbde50421e302c/53180.ipynb) for reference .Thank you!", "@breadbread1984,\r\n\r\nThe reported issue mostly looks like Keras related one, So can you please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues),To know more see;[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) you ", "have transferred the issue here https://github.com/keras-team/keras/issues/15713", "Thank you @breadbread1984, In this case, Can you close this issue here, as it will be followed up in `Keras` repo?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53180\">No</a>\n"]}, {"number": 53178, "title": "TF:TRT Fix shape value collection", "body": "TRT has a special API to specify shape value inputs, therefore TF-TRT uses a boolean mask (`is_shape_value_`) that defines which input tensors correspond to shape inputs. \r\n\r\nWhether a tensor is a shape tensor (from TRT's point of view) can be only determined once the TRT network is constructed (or equivalently, the TRT engine is loaded). If the network construction (or engine loading during deserialization) fails, then the input mask might not be correctly initialized, which can lead to an error in the subsequent memory copy. This PR fixes the initialization of the `is_shape_value_` mask to avoid this error.\r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.", "comments": ["@tfeher Can you please resolve conflicts? Thanks!", "@tfeher  #53213 is merged. Can you update the code and the PR description? Please also squash the commits.", "@bixia1 conflicts are resolved, please have another look."]}, {"number": 53177, "title": "Should use CMAKE_CURRENT_*_DIR", "body": null, "comments": []}, {"number": 53176, "title": "TypeError: '<' not supported between instances of 'WhileBodyFuncGraph' and 'FuncGraph'", "body": "Hello,\r\n\r\nI have ran tensorflow 2.1 and python 3.8 on a tensorflow project. I got the fp;;\r\n\r\n\r\n\r\n> Start training loop... 0%|          | 0/16153 [00:00<?, ?it/s]2021-11-23 17:42:25.980547: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2) 2021-11-23 17:42:33.055750: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll 2021-11-23 17:42:33.387709: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll Traceback (most recent call last): File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 350, in _flatten_module leaves = nest.flatten_with_tuple_paths( File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 1425, in flatten_with_tuple_paths flatten(structure, expand_composites=expand_composites))) File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 341, in flatten return _pywrap_utils.Flatten(structure, expand_composites) TypeError: '<' not supported between instances of 'WhileBodyFuncGraph' and 'FuncGraph' \r\n> \r\n> The above exception was the direct cause of the following exception: Traceback (most recent call last): File \"code2seq.py\", line 29, in <module> model.train() File \"C:\\modelrunner.py\", line 129, in train gradients = tape.gradient(loss, self.model.trainable_variables) File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 175, in trainable_variables return tuple( File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 390, in _flatten_module for subvalue in subvalues: File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 390, in _flatten_module for subvalue in subvalues: File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 390, in _flatten_module for subvalue in subvalues: File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\module\\module.py\", line 353, in _flatten_module six.raise_from( File \"<string>\", line 3, in raise_from ValueError: Error processing property '_dropout_mask_cache' of <ContextValueCache at 0x2675729b820> 0%|          | 0/16153 [00:10<?, ?it/s]\r\n\r\n\r\n", "comments": ["Hi! @Avra2 ! \r\nCould you please update the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks! "]}, {"number": 53175, "title": "Protobuf header files not copied with -install_headers- bazel target", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: CentOS Linux Release 8.0.1905\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6 release\r\n- Python version: 3.7.8\r\n- Installed using virtualenv? pip? conda?: None, installed libtensorflow_cc.so and libtensorflow_framework.so from source. \r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nI would like to compile `libtensorflow_framework.so` and `libtensorflow_cc.so` and link them with my own custom code. \r\nFor this, I first compiled the two targets above using bazel as:\r\n\r\n```\r\nbazel --output_user_root=/raid/projects/bazel build --jobs=8 --config=opt //tensorflow:libtensorflow_cc.so --noincompatible_do_not_split_linking_cmdline --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --verbose_failures\r\n```\r\n\r\n```\r\nbazel --output_user_root=/raid/projects/bazel build --jobs=8 --config=opt //tensorflow:libtensorflow_framework.so --noincompatible_do_not_split_linking_cmdline --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --verbose_failures\r\n```\r\n\r\nHowever, I still needed the include files so I executed:\r\n\r\n```\r\nbazel --output_user_root=/raid/projects/bazel build --jobs=8 --config=opt //tensorflow:install_headers --noincompatible_do_not_split_linking_cmdline --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --verbose_failures\r\n```\r\n\r\nwhich created the required include directory within bazel_bin. At this point, I added the generated include folder to the include path to compile my custom code (which uses tensorflow):\r\n\r\n```\r\n-I$(HOME)/projects/tensorflow/bazel-bin/tensorflow/include\r\n```\r\n\r\nThen I hit `make` and I got the following error:\r\n\r\n```\r\n$(HOME)/projects/tensorflow/bazel-bin/tensorflow/include/tensorflow/core/framework/types.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory\r\n   10 | #include <google/protobuf/port_def.inc>\r\n```\r\n\r\nI believe that the inc header files are not copied with the install_headers target. The confusing part is that pip_packages/setup.py seems to have the copy. Maybe it just doesn't have the com_google_protobuf folder in tensorflow at that point. Any suggestions for a workaround? I tried to compile my own version of protobuf (3.11.4) but when I include the header files from those I see conflicting definitions.  \r\n\r\nAlso, if I install the libraries using pip/conda, I am able to link those libraries with my code. I just want to have an alternative which doesn't require pip/python.\r\n", "comments": ["Found the include files in `tensorflow/bazel-bin/tensorflow/include/src`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53175\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53175\">No</a>\n"]}, {"number": 53174, "title": "training stuck in first epoch at random batch # while using rmsprop as optimizer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): >= 2.5.0\r\n- Python version:  >= 3.6\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2 / 8.1\r\n- GPU model and memory: RTX8000 48GB\r\n\r\n**Describe the current behavior**\r\ntraining stuck at first epoch at random batch # while using rmsprop as optimizer. Same code is working with RTX3090.\r\n\r\n**Describe the expected behavior**\r\ncontinue training till end of the defined epoch\r\n\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n\r\n", "comments": ["@makermotion ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "Also please take a look at this links [1](https://stackoverflow.com/questions/53471855/why-does-keras-halt-at-the-first-epoch-when-i-attempt-to-train-it-using-fit-gene) [2](https://stackoverflow.com/questions/59808666/stuck-in-the-first-epoch-when-training-the-cnn-lstm-using-keras) and [3](https://stackoverflow.com/questions/66900639/tensorflow-stuck-on-first-epoch) with the similar error.Please upgrade to latest tf v2.7 and let us know if the issue still exists.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53174\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53174\">No</a>\n"]}, {"number": 53173, "title": "cmake: Use real source path for flatbuffers-flatc", "body": "This to able to override the source path for flatbuffers-flatc with the\r\nCMake parameter FETCHCONTENT_SOURCE_DIR_FLATBUFFERS.", "comments": ["@terryheo Can you please review this PR ? Thanks!"]}, {"number": 53172, "title": "Update documentation of the tf.nn.softmax function", "body": "This PR addresses the issue #53112 \r\n\r\nAs discussed, the tf.nn.softmax documentation states that:\r\n\r\n> This function performs the equivalent of\r\n> ```\r\n> softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)\r\n> ```\r\n\r\nHowever the above computation is wrong for tensors with rank greater than 1.\r\n\r\nI belive that the correct description would be:\r\n```\r\nsoftmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)\r\n```", "comments": ["Hi, I noticed that the line is longer than 80 characters, any suggestion on how to rewrite it in a two-line format?\r\n\r\nP.S. I also tried to run tf_doctest.py (as indicated here: https://www.tensorflow.org/community/contribute/docs_ref) but it throws \"AttributeError: module 'tensorflow.python.ops.logging_ops' has no attribute 'enable_interactive_logging\" ", "@T-Almeida Can you please address PyLint errors? Thanks!", "Hi @gbaned, I made two changes in order to fix the PyLint erros, see if its okey"]}, {"number": 53171, "title": "[TF-TRT] Add Grappler optimizers to converter", "body": "cc @bixia1 @tfeher @DEKHTIARJonathan\r\n\r\nThis mitigates one of the problems seen in https://github.com/tensorflow/tensorrt/issues/255 and makes that BERT Large model (and others) convert successfully.\r\n\r\n- In the previous set of optimizers, `constfold` was duplicating constants unnecessarily due to some control dependencies. `dependency` and `common_subgraph_elimination` do a great job at removing this duplication.\r\n- I also noticed that having a `constfold` before _and_ after `layout` was unnecessary, and in all the models I've tested, equivalent to just having the second one.\r\n\r\nI haven't measured any significant performance impact but the size of converted transformer models tends to be reduced by 50% with this change.\r\n\r\n_Note: I had to change the tests slightly because the dependency optimizer removes the `ReadVariableOp` and keeps a constant `v1` whereas constant folding alone optimizes `ReadVariableOp` to a constant with the value of `v1`. This  also depends on the order of the Grappler passes.", "comments": ["@Nyrio There are a few test fail due to TRTEngines built differently, can you please make sure that tests pass with TF1 and TF2 ?\r\n\r\nHere are the failing test:\r\n    tensorflow/python/compiler/tensorrt/test:base_test\tFAILED\r\n    tensorflow/python/compiler/tensorrt/test:quantization_test\tFAILED\r\n    tensorflow/python/compiler/tensorrt/test:reshape_transpose_test\tFAILED", "@Nyrio Thanks for the fix. Please squash the commits and I will approve it.", "@bixia1 Squashed.\r\nThe CI failure in the oneDNN build appears unrelated.", "this test still fails //third_party/tensorflow/python/compiler/tensorrt/test:gpu_base_test\r\n\r\nConstInputTest.testTfTrt_OfflineConversion_DynamicEngine_FP16_NoCalibration_ImplicitBatch\r\n 0:00:00.200\r\nTraceback (most recent call last):\r\n  File \"/build/work/d3efda52e05638903d137374a91789e33061/google3/runfiles/google3/third_party/tensorflow/python/framework/test_util.py\", line 3468, in assertDictEqual\r\n    super().assertDictEqual(a, b, msg)\r\n  File \"/build/work/d3efda52e05638903d137374a91789e33061/google3/runfiles/google3/third_party/py/absl/testing/absltest.py\", line 1780, in assertDictEqual\r\n    raise self.failureException('\\n'.join(message))\r\nAssertionError: {'TRTEngineOp_0': {'input_0', '^incompatible'}, 'TRTEngineOp_1': {'^incompatible', 'incompatible1'}, 'c': set(), 'incompatible': {'input_0', '^c'}, 'incompatible1': {'TRTEngineOp_0'}, 'input_0': set(), 'output_0': {'TRTEngineOp_1'}} != {'TRTEngineOp_0': {'incompatible1'}, 'TRTEngineOp_1': {'input_0'}, 'incompatible1': {'TRTEngineOp_1'}, 'input_0': set(), 'output_0': {'TRTEngineOp_0'}} (\r\nexpected:\r\n[('TRTEngineOp_0', {'input_0', '^incompatible'}), ('TRTEngineOp_1', {'^incompatible', 'incompatible1'}), ('c', set()), ('incompatible', {'input_0', '^c'}), ('incompatible1', {'TRTEngineOp_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_1'})]\r\nvs actual:\r\n[('TRTEngineOp_0', {'incompatible1'}), ('TRTEngineOp_1', {'input_0'}), ('incompatible1', {'TRTEngineOp_1'}), ('input_0', set()), ('output_0', {'TRTEngineOp_0'})])\r\nrepr() of differing entries:\r\n'TRTEngineOp_0': {'input_0', '^incompatible'} != {'incompatible1'}\r\n'TRTEngineOp_1': {'^incompatible', 'incompatible1'} != {'input_0'}\r\n'incompatible1': {'TRTEngineOp_0'} != {'TRTEngineOp_1'}\r\n'output_0': {'TRTEngineOp_1'} != {'TRTEngineOp_0'}\r\n\r\nMissing entries:\r\n'c': set()\r\n'incompatible': {'input_0', '^c'}\r\n\r\n", " @bixia1 The dependency optimizer added in this PR doesn't mix well with the v1 integration tests that expect redundant dependencies, so I added some code to emulate the transitive reduction of the control dependencies on the expected graph. Also, the new optimizers optimize constants with the same value or nodes that aren't used by any output, so I had to do a few changes here and there.\r\n\r\nI could use some feedback on the change in `tf_trt_integration_test_base.py`.\r\nI'll squash later (also, the CI pylint job is suddenly failing on previous changes for a \"line too long\" though the line seems fine to me, i don't see why).", "I made the proposed changes in tests. I'm still quite puzzled with the pylint false positive in CI. It's a \"line too long\" error but the line is actually far less than 80 characters long and I can't reproduce it when I run pylint locally...", "This test still fails:\r\ntensorflow/python/compiler/tensorrt/test:gpu_tf_function_test\r\n\r\nThe waymo test mentioned above still fails.\r\n", "@Nyrio Can you please check @bixia1's comments and keep us posted ? Thanks!", "@gbaned @bixia1 I fixed the test mentioned above."]}, {"number": 53170, "title": "Rename the TFLite library convert_type to tfl_convert_type as it conf\u2026", "body": "Rename the TFLite library convert_type to tfl_convert_type as it conflicts with the similarly named library in Tensorflow", "comments": ["Hi, \r\nWanted to ask if I need to do anything else to get this patch reviewed ? \r\nThanks\r\nAaron ", "Could you perhaps expand the description to say what issues you ran into? E.g., there are many libraries with the same name, but they are inside different projects/folders, textually name spacing them does not seem very appealing. I'll also add someone from tflite team for review.", "Thanks for your reply. I am trying to re-use the TFlite Flatbuffer to MLIR Dialect function using tflite::FlatBufferToMlir() in an external project by linking to the tensorflow libraries. My external project is using a Cmake build system. \r\n\r\nIn doing so I end up adding a number of libraries to my build system. It seems I end up with dependencies from both convert_type (tflite) and convert_type (tensorflow) which I cannot link to easily since they have the same name. \r\n\r\nRenaming to tfl_convert_type for the tflite version allows me to end up with a solution where I can link them, in the simplest way possible. ", "Hi @karimnosseir is there a chance for a review of this pull request please ?", "It looks the way you are adding the files doesn't take full path in consideration. I am afraid this change is not future proof for other similar scenarios, and we can't guarantee that the files doesn't get renamed again - which might cause collision on your end again.\r\n\r\nMay be you should perhaps update your build/script to take full path so you avoid any collisions.\r\n\r\nThanks", "I understand but in my cmake file when I tried your suggestion, and I give the full paths then I get cyclic errors in the linker for some reason, and the solution fails to link:\r\n\r\ne.g.\r\nCannot generate a safe runtime search path for target unit_tests because\r\n  there is a cycle in the constraint graph:\r\n\r\nThere may be some trick I am missing. The alternative may be for me to rename the library as part of the build system, but that seems a bit fragile. "]}, {"number": 53169, "title": "filter dataset too slow .", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.6\r\n- Are you willing to contribute it (Yes/No): if need I want \r\n\r\n\r\nI must have filter dataset, so I must write ```map.filter.batch```, but map batch is too slow than batch map, like:\r\n```\r\nds = tf.data.TFRecordDataset(\"a.txt\")\r\nds = ds.map()\r\nds = ds.filter()\r\nds = ds.batch()\r\nit = ds.get_one_shot_iterator()\r\ngn = it.get_next()\r\n```\r\nso I want:\r\n```\r\nds = tf.data.TFRecordDataset(\"a.txt\")\r\nds = ds.batch()\r\nds = ds.map() // parallel batch map\r\nds = ds.unbatch() // unbatch\r\nds = ds.filter() // filter\r\nds = ds.batch() // rebatch \r\nit = ds.get_one_shot_iterator()\r\ngn = it.get_next()\r\n```\r\n\r\nbut I get : \r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 181, in <module>\r\n    sess.run(train_data_reader(\"\"));\r\n  File \"main.py\", line 169, in train_data_reader\r\n    train_iterator = train_data_set.make_one_shot_iterator()\r\nAttributeError: 'PrefetchDataset' object has no attribute 'make_one_shot_iterator'\r\n```\r\n\r\nI want a best profromance filter dataset .", "comments": ["![image](https://user-images.githubusercontent.com/33950866/143202509-f7d0dfa0-c5a2-4e71-b926-5d6acbe95e98.png)\r\nI calculated the time spent on the data set pipeline, and I got the unbatch and filter cost for a long time, so I want to get a parallel ```unbatch``` and ```filter``` function.\r\nHow to parallel unbatch and filter ?", "Hi @zhaozheng09! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. You can refer latest documents on [ tf.data](https://www.tensorflow.org/api_docs/python/tf/data)  from 2.6/2.7 version or [use migration document](https://www.tensorflow.org/guide/migrate)  to proceed further. Thanks!", "> Hi @zhaozheng09! It seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. You can refer latest documents on [ tf.data](https://www.tensorflow.org/api_docs/python/tf/data) from 2.6/2.7 version or [use migration document](https://www.tensorflow.org/guide/migrate) to proceed further. Thanks!\r\n\r\nI change my issue 1.15->2.6 ...", "Hi @chunduriv! Could you please look at this issue?", "@zhaozheng09, In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\t", "Data confidentiality, re-fake data is too complicated", "@zhaozheng09, If you are unable to provide necessary information, it would be difficult for us to debug the issue.\r\n\r\nKindly open this issue in [Tensorflow forum](https://discuss.tensorflow.org/) as there is a larger community to support. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53169\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53169\">No</a>\n"]}, {"number": 53168, "title": "Failed precondition: Attempting to use uninitialized", "body": "My code:\r\n\r\n```\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    # input_his: num_samples:216,9,9,144\r\n    input_hsi = tf.placeholder(tf.float32, [None, x_train.shape[1], x_train.shape[2], x_train.shape[3], 1],\r\n                               name='input_hsi')\r\n    labels_hsi = tf.placeholder(tf.float32, [None, nb_classes], name='labels_hsi')\r\n    is_train = tf.placeholder(tf.bool, shape=[], name='is_train')\r\n    learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')\r\n    keep_prob = tf.placeholder(tf.float32, shape=[], name='keep_prob') \r\n\r\n   ##################\r\n    ## # Wrong here ###\r\n    logits, prob = aucn_model.build_model(input_hsi, nb_classes, is_train=is_train, keep_prob=keep_prob) # Wrong here\r\n    #################\r\n\r\n    pred = tf.argmax(prob, 1)\r\n    tf.add_to_collection('pred', pred)\r\n```\r\n\r\nIn function aucn_model.build_model\r\n\r\n```\r\ndef build_model(input_images, num_output, is_train, keep_prob):\r\n    k = 36\r\n    first_channel = 64\r\n    t_channel = 48\r\n\r\n    print(input_images.get_shape())\r\n\r\n    x1_0 = first_conv(input_images, channel=first_channel)\r\n\r\n    # build spectral blocks\r\n    print('the input shape of spectral blocks is: ', x1_0.get_shape())\r\n    x1, x_1_transit_feature = loop_block(x1_0, channels_per_layer=k, kernel_size=(1, 1, 7), layer_num=2,\r\n                                         is_train=is_train, block_name='spectral_block', loop_num=1)\r\n\r\n    # transition layer\r\n    print('the output shape of spectral blocks is: ', x1.get_shape())\r\n    x1 = bn_relu(x1, is_train=is_train)\r\n    tran1_var = conv_var(kernel_size=(1, 1, x1.get_shape()[3]), in_channels=x1.get_shape()[4], out_channels=t_channel,\r\n                         init_method='msra', name='first_transition')\r\n    tran1 = tf.nn.conv3d(x1, tran1_var, [1, 1, 1, 1, 1], padding='VALID')\r\n\r\n    tran1 = bn_relu(tran1, is_train=is_train)\r\n    print(tran1.get_shape())\r\n    tran2 = Reshape((tran1.get_shape()[1], tran1.get_shape()[2], tran1.get_shape()[4], 1))(tran1)\r\n\r\n    print(tran2.get_shape())\r\n    tran2_var = conv_var(kernel_size=(3, 3, t_channel), in_channels=1, out_channels=first_channel,\r\n                         init_method='msra', name='second_transition')\r\n    x2_0 = tf.nn.conv3d(tran2, tran2_var, [1, 1, 1, 1, 1], padding='VALID')\r\n\r\n    print('the input of spatial block:', x2_0.get_shape())\r\n    # build spatial blocks\r\n    x2, x_2_transit_feature = loop_block(x2_0, channels_per_layer=k, kernel_size=(3, 1, 1), layer_num=2,\r\n                                         is_train=is_train, block_name='spatial_block_1', loop_num=1)\r\n\r\n    print('the output of spatial block:', x2.get_shape())\r\n    x3, x_3_transit_feature = loop_block(x2_0, channels_per_layer=k, kernel_size=(1, 3, 1), layer_num=2,\r\n                                         is_train=is_train, block_name='spatial_block_2', loop_num=1)\r\n    x4 = tf.concat([x2, x3], axis=4)\r\n\r\n    # Classifier block\r\n\r\n    pool1 = tf.nn.avg_pool3d(x4, ksize=[1, x4.get_shape()[1], x4.get_shape()[2], 1, 1],\r\n                             strides=[1, 1, 1, 1, 1], padding='VALID')\r\n    print(pool1.get_shape())\r\n\r\n ### WRONG  HERE ####\r\n    flatten = tf.layers.flatten(pool1)\r\n# ---when changed to below it works---\r\n    other_dim = pool1.get_shape()[1]*pool1.get_shape()[2]*pool1.get_shape()[3]*pool1.get_shape()[4]\r\n    flatten = Reshape((other_dim,))(pool1)\r\n#######################\r\n\r\n\r\n    #print(tf.__version__)\r\n    #flatten = tf.contrib.layers.flatten(pool1)\r\n    print(flatten.get_shape())\r\n    # flatten = tf.nn.dropout(flatten, keep_prob=keep_prob)\r\n    wfc = tf.get_variable(name='FC_W', shape=[flatten.get_shape()[1], num_output],\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n    bfc = tf.get_variable(name='FC_b', initializer=tf.constant(0.0, shape=[num_output]))\r\n\r\n    logits = tf.matmul(flatten, wfc) + bfc\r\n    print(logits.get_shape())\r\n    prob = tf.nn.softmax(logits)\r\n\r\n    return logits, prob\r\n```\r\nWhen I change to this code it works\r\n```\r\n\r\nflatten = tf.layers.flatten(pool1) # it doesnt work\r\n\r\nother_dim = pool1.get_shape()[1]*pool1.get_shape()[2]*pool1.get_shape()[3]*pool1.get_shape()[4]\r\nflatten = Reshape((other_dim,))(pool1) # it works\r\n```\r\n_\r\n\r\n**System information**\r\ncolab,TF1.15\r\n\r\ni just confused that why collapsed in making computational graph informed error \u2018uninitialized\u2019,i even not try to excute there.", "comments": ["@ponytailGHJ We see that you are using `TF v1.15` which is `out of support window` .Could you please upgrade to latest `TF v2.7.0 `and let us know if the issue still persists ? You may refer  [this](https://www.tensorflow.org/guide/migrate?hl=en#migrate-from-tensorflow-1x-to-tensorflow-2) to Migrate from TensorFlow 1.x to TensorFlow 2 .Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53168\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53168\">No</a>\n", "> @ponytailGHJ We see that you are using `TF v1.15` which is `out of support window` .Could you please upgrade to latest `TF v2.7.0 `and let us know if the issue still persists ? You may refer [this](https://www.tensorflow.org/guide/migrate?hl=en#migrate-from-tensorflow-1x-to-tensorflow-2) to Migrate from TensorFlow 1.x to TensorFlow 2 .Thank you!\r\n\r\nin fact i have solved it by using reshape instead of tf.layers.flatten,\r\ni just wonder why it doesnt work in tf 1.15", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53168\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53168\">No</a>\n", "@ponytailGHJ Thank you for the update! Could you please move  this ticket to closed status  as you have found one workaround as per the [comment](https://github.com/tensorflow/tensorflow/issues/53168#issuecomment-976580770) ?\r\nAs **TF v1.15** is not actively supported ,for further queries please post this issue in [TF Forum](https://discuss.tensorflow.org/) where there is a larger community to get you the right help.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53168\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53168\">No</a>\n"]}, {"number": 53167, "title": "Correct use of StringRef and ArrayRef", "body": "Current code takes a reference to transient objects which go out of scope and get overwritten. This renders the references taken invalid and leads to incorrect operation and possible SIGSEGV.\r\nThe objects referred to by StringRef and ArrayRef need to have their own independent lifetime.\r\nFixes https://github.com/tensorflow/tensorflow/issues/53166", "comments": ["Build of master was broken when the checks were run. The failure comes from that reason and is not due to this commit."]}, {"number": 53166, "title": "Unit tests //tensorflow/core/ir/... fail or crash depending on optimization level", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 8.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): git HEAD\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nTests fail with errors like\r\n\r\n<stdin>:4:22: error: custom op 'tfg.AddV2' attribute '_mlir_device' occurs more than once in the attribute list\r\n    %AddV2, %ctl_0 = AddV2(%placeholder, %placeholder_1) device(\"GPU\") assigned_device(\"TPU\") {_mlir_device = \"GPU\", some_attribute = \"some attr!\"} : (tensor<*xi32>, tensor<*xi32>) -> (tensor<*xi32>)\r\n                     ^\r\nFileCheck error: '<stdin>' is empty.\r\nFileCheck command line:  /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/llvm-project/llvm/FileCheck /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nAll tests pass\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing): Correct improper use of ArrayRef and StringRef\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=\"\"  --remote_cache_proxy=\"\" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --copt=-O0 --copt=-ggdb --verbose_failures -- //tensorflow/core/ir/tests:ops.mlir.test\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n================================================================================\r\n==================== Test output for //tensorflow/core/ir/tests:ops.mlir.test:\r\n-- Testing: 1 tests, 1 workers --\r\nFAIL: MLIR tests :: ops.mlir (1 of 1)\r\n******************** TEST 'MLIR tests :: ops.mlir' FAILED ********************\r\nScript:\r\n--\r\n: 'RUN: at line 1';   /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/tfg-opt-no-passes /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir | /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/tfg-opt-no-passes | /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/llvm-project/llvm/FileCheck /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir\r\n--\r\nExit Code: 2\r\n\r\nCommand Output (stderr):\r\n--\r\n<stdin>:4:22: error: custom op 'tfg.AddV2' attribute '_mlir_device' occurs more than once in the attribute list\r\n    %AddV2, %ctl_0 = AddV2(%placeholder, %placeholder_1) device(\"GPU\") assigned_device(\"TPU\") {_mlir_device = \"GPU\", some_attribute = \"some attr!\"} : (tensor<*xi32>, tensor<*xi32>) -> (tensor<*xi32>)\r\n                     ^\r\nFileCheck error: '<stdin>' is empty.\r\nFileCheck command line:  /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/llvm-project/llvm/FileCheck /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir\r\n\r\n--\r\n\r\n********************\r\n********************\r\nFailed Tests (1):\r\n  MLIR tests :: ops.mlir\r\n\r\n\r\nTesting Time: 0.11s\r\n  Failed: 1\r\n================================================================================\r\n\r\n", "comments": ["@cfRod @nSircombe ", "@elfringham ,\r\nThe issue will move to closed status once the PR is merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53166\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53166\">No</a>\n"]}, {"number": 53164, "title": "Tensorflow build fails with \"Multiple matches are not allowed unless one is unambiguously more specialized\" on linux_s390x", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Redhat 8.1 on s390x\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Tensorflow build\r\n- TensorFlow version: v2.7.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): 3.7.0\r\n- GCC/Compiler version (if compiling from source): 8.3\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nTensorflow build fails with \"Multiple matches are not allowed unless one is unambiguously more specialized\" on linux_x390x\"\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`bazel --host_jvm_args=\"-Xms8g\" --host_jvm_args=\"-Xmx8g\" build --define=tensorflow_mkldnn_contraction_kernel=0 --copt=-Wno-maybe-uninitialized  --copt=-mzvector --copt=-funroll-loops --copt=-march=z14 --color=yes --curses=yes -s --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nfailed with \"Multiple matches are not allowed unless one is unambiguously more specialized\" on linux_x390x\"\r\n\r\n**Any other info / logs**\r\n```\r\nBuilding TENSORFLOW.....\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /root/public/git/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /root/public/git/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /root/public/git/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /root/public/git/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:nogcp in file /root/public/git/tensorflow/.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:linux in file /root/public/git/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /root/public/git/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Build option --define has changed, discarding analysis cache.\r\nERROR: /root/public/git/tensorflow/tensorflow/tools/pip_package/BUILD:182:10: Illegal ambiguous match on configurable attribute \"data\" in //tensorflow/tools/pip_package:licenses:\r\n//tensorflow:linux_s390x\r\n//tensorflow:no_gcp_support\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: /root/public/git/tensorflow/tensorflow/tools/pip_package/BUILD:182:10: Illegal ambiguous match on configurable attribute \"data\" in //tensorflow/tools/pip_package:licenses:\r\n//tensorflow:linux_s390x\r\n//tensorflow:no_gcp_support\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nINFO: Elapsed time: 0.777s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (4 packages loaded, 274 targets co\\\r\nnfigured)\r\nCleaned up the artifacts\r\nCleaned up the artifacts\r\n```\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@potula-chandra Could you please have a look at the [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest [microsoft visual c++ redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads) from here.\r\nSee the [tested build configurations](https://www.tensorflow.org/install/source_windows#tested_build_configurations)\r\nAlso, please follow the instructions  to install from [Tensorflow website](https://www.tensorflow.org/install/source).\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167, #42367,#42197,#45435\r\nCould you please refer to the common [errors](https://www.tensorflow.org/install/errors) and let us know if it helps ? Thanks!", "@sushreebarsa I am able to build tensorflow 2.7.0 on IBM LinuxONE platform s390x  with some recommendations made [here](https://github.com/tensorflow/tensorflow/issues/51770#issuecomment-911554418) and [here](https://github.com/tensorflow/tensorflow/issues/22023)\r\n\r\nBut when I'm installing the whl file I get the following error:\r\n```\r\n[root@ git]# pip install ./tensorflow-2.7.0-cp38-cp38-linux_s390x.whl\r\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\r\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\r\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\r\nProcessing ./tensorflow-2.7.0-cp38-cp38-linux_s390x.whl\r\nCollecting absl-py>=0.4.0\r\n  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\r\nCollecting gast<0.5.0,>=0.2.1\r\n  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\r\nCollecting wrapt>=1.11.0\r\n  Using cached wrapt-1.13.3.tar.gz (48 kB)\r\n  Preparing metadata (setup.py) ... done\r\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib64/python3.8/site-packages (from tensorflow==2.7.0) (2.9.0)\r\nRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.7.0) (3.13.0)\r\nCollecting astunparse>=1.6.0\r\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\nCollecting tensorflow-estimator<2.8,~=2.7.0rc0\r\n  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\r\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.7.0) (3.10.0.0)\r\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.15.0)\r\nCollecting termcolor>=1.1.0\r\n  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\r\n  Preparing metadata (setup.py) ... done\r\nRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib64/python3.8/site-packages (from tensorflow==2.7.0) (1.18.5)\r\nCollecting opt-einsum>=2.3.2\r\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\nERROR: Could not find a version that satisfies the requirement tensorflow-io-gcs-filesystem>=0.21.0 (from tensorflow) (from versions: none)\r\nERROR: No matching distribution found for tensorflow-io-gcs-filesystem>=0.21.0\r\n```\r\nHence now I would like to build tensorflow `2.7.0` with options `--config=noaws --config=nogcp --config=nohdfs --config=nonccl`", "I am not sure why it is throwing the following error. Pypi build https://pypi.org/project/tensorflow-io-gcs-filesystem/ is there\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow-io-gcs-filesystem>=0.21.0 (from tensorflow) (from versions: none)\r\nERROR: No matching distribution found for tensorflow-io-gcs-filesystem>=0.21.0\r\n```\r\n\r\nWill check for more details about the error. Thanks", "Could be an issue with Bazel version as reported [here](https://stackoverflow.com/questions/49825590/tensorflow-bazel-building-fails) for a similar error\r\n\r\nCan you pleas try with \r\nBazel version (if compiling from source): 3.7.2\r\n\r\n", "@jvishnuvardhan thank you for the update.  Couple of things here:\r\n1. Fix requires to build Tensorflow 2.7.0 when provided options `--config=noaws --config=nogcp --config=nohdfs --config=nonccl`\r\n2. Even if we are able build Tensorflow 2.7.0 with some tweaks as mentioned in my earlier comment https://github.com/tensorflow/tensorflow/issues/53164#issuecomment-976522393, gcs is still posing as runtime dependency while installing the whl file and that needs to be looked at.\r\n\r\nIn the mean time I will give a try with bazel 3.7.2.", "@jvishnuvardhan I have got chance to try out using bazel 3.7.2 and problem still remains the same:\r\n\r\n```\r\nBuilding TENSORFLOW.....\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /root/test-bazel/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /root/test-bazel/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /root/test-bazel/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/lib/python3.8/site-packages --python_path=/usr/bin/python3.8\r\nINFO: Reading rc options for 'build' from /root/test-bazel/tensorflow/.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /root/test-bazel/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /root/test-bazel/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:noaws in file /root/test-bazel/tensorflow/.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file /root/test-bazel/tensorflow/.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file /root/test-bazel/tensorflow/.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:nonccl in file /root/test-bazel/tensorflow/.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:linux in file /root/test-bazel/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /root/test-bazel/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nERROR: /root/test-bazel/tensorflow/tensorflow/tools/pip_package/BUILD:182:10: Illegal ambiguous match on configurable attribute \"data\" in //tensorflow/tools/pip_package:licenses:\r\n//tensorflow:linux_s390x\r\n//tensorflow:no_gcp_support\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nINFO: Repository absl_py instantiated at:\r\n  /root/test-bazel/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /root/test-bazel/tensorflow/tensorflow/workspace2.bzl:1089:21: in workspace\r\n  /root/test-bazel/tensorflow/tensorflow/workspace2.bzl:497:20: in _tf_repositories\r\n  /root/test-bazel/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /root/test-bazel/tensorflow/third_party/repo.bzl:66:35: in <toplevel>\r\nINFO: Repository gast_archive instantiated at:\r\n  /root/test-bazel/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /root/test-bazel/tensorflow/tensorflow/workspace2.bzl:1089:21: in workspace\r\n  /root/test-bazel/tensorflow/tensorflow/workspace2.bzl:438:20: in _tf_repositories\r\n  /root/test-bazel/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /root/test-bazel/tensorflow/third_party/repo.bzl:66:35: in <toplevel>\r\nINFO: Repository opt_einsum_archive instantiated at:\r\n  /root/test-bazel/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /root/test-bazel/tensorflow/tensorflow/workspace2.bzl:1089:21: in workspace\r\n  /root/test-bazel/tensorflow/tensorflow/workspace2.bzl:485:20: in _tf_repositories\r\n  /root/test-bazel/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /root/test-bazel/tensorflow/third_party/repo.bzl:66:35: in <toplevel>\r\nINFO: Repository enum34_archive instantiated at:\r\n  /root/test-bazel/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /root/test-bazel/tensorflow/tensorflow/workspace2.bzl:1089:21: in workspace\r\n  /root/test-bazel/tensorflow/tensorflow/workspace2.bzl:514:20: in _tf_repositories\r\n  /root/test-bazel/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /root/test-bazel/tensorflow/third_party/repo.bzl:66:35: in <toplevel>\r\nINFO: Repository rules_java instantiated at:\r\n  /root/test-bazel/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /root/test-bazel/tensorflow/tensorflow/workspace0.bzl:120:20: in workspace\r\n  /root/.cache/bazel/_bazel_root/8b56e020b41c9a7c4ef50a1eafd78aa1/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps\r\n  /root/.cache/bazel/_bazel_root/8b56e020b41c9a7c4ef50a1eafd78aa1/external/com_google_protobuf/protobuf_deps.bzl:34:21: in protobuf_deps\r\nRepository rule http_archive defined at:\r\n  /root/.cache/bazel/_bazel_root/8b56e020b41c9a7c4ef50a1eafd78aa1/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nINFO: Repository rules_proto instantiated at:\r\n  /root/test-bazel/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /root/test-bazel/tensorflow/tensorflow/workspace0.bzl:120:20: in workspace\r\n  /root/.cache/bazel/_bazel_root/8b56e020b41c9a7c4ef50a1eafd78aa1/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps\r\n  /root/.cache/bazel/_bazel_root/8b56e020b41c9a7c4ef50a1eafd78aa1/external/com_google_protobuf/protobuf_deps.bzl:42:21: in protobuf_deps\r\nRepository rule http_archive defined at:\r\n  /root/.cache/bazel/_bazel_root/8b56e020b41c9a7c4ef50a1eafd78aa1/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: /root/test-bazel/tensorflow/tensorflow/tools/pip_package/BUILD:182:10: Illegal ambiguous match on configurable attribute \"data\" in //tensorflow/tools/pip_package:licenses:\r\n//tensorflow:linux_s390x\r\n//tensorflow:no_gcp_support\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nINFO: Elapsed time: 166.586s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (138 packages loaded, 2775 targets\\\r\n configured)\r\n    currently loading: @bazel_tools//tools/jdk ... (2 packages)\r\nCleaned up the artifacts\r\nCleaned up the artifacts\r\n```", "Even with main branch code issue reproducible \r\n\r\n```\r\nDownload Tensorflow source code.....\r\nperl: warning: Setting locale failed.\r\nperl: warning: Please check that your locale settings:\r\n\tLANGUAGE = (unset),\r\n\tLC_ALL = (unset),\r\n\tLC_CTYPE = \"UTF-8\",\r\n\tLANG = \"en_US.UTF-8\"\r\n    are supported and installed on your system.\r\nperl: warning: Falling back to a fallback locale (\"en_US.UTF-8\").\r\nYou have bazel 3.7.2- (@non-git) installed.\r\nPlease upgrade your bazel installation to version 4.2.2 or higher to build TensorFlow!\r\n\r\nBuilding TENSORFLOW.....\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /root/test-tf-main/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /root/test-tf-main/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /root/test-tf-main/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /root/test-tf-main/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:noaws in file /root/test-tf-main/tensorflow/.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file /root/test-tf-main/tensorflow/.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file /root/test-tf-main/tensorflow/.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:nonccl in file /root/test-tf-main/tensorflow/.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:linux in file /root/test-tf-main/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /root/test-tf-main/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1596824487 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /root/test-tf-main/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /root/test-tf-main/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  /root/.cache/bazel/_bazel_root/d060c4c0d9c9dceb29ac14538fe78b96/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /root/.cache/bazel/_bazel_root/d060c4c0d9c9dceb29ac14538fe78b96/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/322055148b47dccb76bc03ad010f16c1e3a94817.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nERROR: /root/test-tf-main/tensorflow/tensorflow/core/common_runtime/BUILD:1864:16: Illegal ambiguous match on configurable attribute \"deps\" in //tensorflow/core/common_runtime:core_cpu_internal:\r\n//tensorflow:linux_s390x\r\n//tensorflow:no_gcp_support\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: /root/test-tf-main/tensorflow/tensorflow/core/common_runtime/BUILD:1864:16: Illegal ambiguous match on configurable attribute \"deps\" in //tensorflow/core/common_runtime:core_cpu_internal:\r\n//tensorflow:linux_s390x\r\n//tensorflow:no_gcp_support\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nINFO: Elapsed time: 1.136s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (35 packages loaded, 2966 targets \\\r\nconfigured)\r\n    Fetching @icu; fetching\r\n    Fetching ...al/icu; Extracting /root/.cache/bazel/_bazel_root/d060c4c0d9c9\\\r\ndceb29ac14538fe78b96/external/icu/temp12689593740590110713/release-64-2.zip\r\nCleaned up the artifacts\r\nCleaned up the artifacts\r\n```", "@jvishnuvardhan I used the following command to build Tensorflow 2.7.0 : \r\n\r\n`GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=True bazel --host_jvm_args=\"-Xms8g\" --host_jvm_args=\"-Xmx8g\" build --define=tensorflow_mkldnn_contraction_kernel=0 --copt=-Wno-maybe-uninitialized --copt=-mzvector --copt=-funroll-loops --copt=-march=z14 --color=yes --curses=yes -s --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow/tools/pip_package:build_pip_package\r\n`\r\n\r\nwith below patch changes : \r\n```\r\ndiff --git a/tensorflow/tools/pip_package/BUILD b/tensorflow/tools/pip_package/BUILD\r\nindex c8001b9f5cc..ae003566a0c 100644\r\n--- a/tensorflow/tools/pip_package/BUILD\r\n+++ b/tensorflow/tools/pip_package/BUILD\r\n@@ -238,7 +238,6 @@ filegroup(\r\n         \"//tensorflow:ios\": [],\r\n         \"//tensorflow:linux_s390x\": [],\r\n         \"//tensorflow:windows\": [],\r\n-        \"//tensorflow:no_gcp_support\": [],\r\n         \"//conditions:default\": [\r\n             \"@com_github_googlecloudplatform_google_cloud_cpp//:LICENSE\",\r\n         ],\r\ndiff --git a/tensorflow/core/platform/default/build_config.bzl b/tensorflow/core/platform/default/build_config.bzl\r\nindex 50a50c70aa2..6182e2ea86f 100644\r\n--- a/tensorflow/core/platform/default/build_config.bzl\r\n+++ b/tensorflow/core/platform/default/build_config.bzl\r\n@@ -659,7 +659,6 @@ def tf_additional_core_deps():\r\n         clean_dep(\"//tensorflow:android\"): [],\r\n         clean_dep(\"//tensorflow:ios\"): [],\r\n         clean_dep(\"//tensorflow:linux_s390x\"): [],\r\n-        clean_dep(\"//tensorflow:no_gcp_support\"): [],\r\n         \"//conditions:default\": [\r\n             \"//tensorflow/core/platform/cloud:gcs_file_system\",\r\n         ],\r\n```\r\nBuild successful and I am able to try basic things on the Tensorflow (like import and run a use cases etc).\r\n\r\nAs I am not a bazel expert and I want to make sure if some one can confirm if the above changes are reasonable while they do not impact any other platforms, then I can send a PR.\r\n\r\nPlease let me know !", "@jvishnuvardhan Similar to code change proposal made here : https://github.com/tensorflow/tensorflow/pull/48525", "@mihaimaruseac  @sushreebarsa @jvishnuvardhan will you be able to update on the comment https://github.com/tensorflow/tensorflow/issues/53164#issuecomment-1083006737 ? Thanks", "PR been merged so closing the issue. Thank you @mihaimaruseac and others for your support !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53164\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53164\">No</a>\n"]}, {"number": 53163, "title": "model convert successfully, but invoke fail.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installation (pip package or built from source):\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\n\r\n### 2. [Code](url)\r\n[tf-awake-1109crnn-81-int8-2.6.tflite.zip](https://github.com/tensorflow/tensorflow/files/7585973/tf-awake-1109crnn-81-int8-2.6.tflite.zip)\r\n\r\n\r\n### 3. Failure after conversion\r\nmodel convert successfully. but invoke fail.\r\nCONV_2D exception: xa_nnlib_hifi5/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym8sxasym8s_asym8s_circ.c  \r\n", "comments": []}, {"number": 53162, "title": "gpu floor mod .", "body": null, "comments": ["What is the CL description?", "@sanjoy, IIRC we generate MLIR-based GPU kernels for component wise ops. Is MLIR used for FloorMod? I would prefer to generate a GPU kernel instead of adding more hand-written ones.", "1. Can mlir cover all scenes? Are there scenes that mlir cannot cover?\r\n2. The optimal solution is to call the implementation of the open source library, but when the open source library is not available, do we need to hand-write the operator for this function?", "@zhaozheng09 Can you please resolve conflicts? Thanks!", "So the motivation here is to add support for floor mod on GPU? Can you provide some background what motivates this?\r\n\r\nFor context: We are about to add this operation as a jitted operation, which means it will be compiled on first invocation. Does that address your need?", "> * Can mlir cover all scenes? Are there scenes that mlir cannot cover?\r\n> * The optimal solution is to call the implementation of the open source library, but when the open source library is not available, do we need to hand-write the operator for this function?\r\n\r\n\r\n\r\n> So the motivation here is to add support for floor mod on GPU? Can you provide some background what motivates this?\r\n> \r\n> For context: We are about to add this operation as a jitted operation, which means it will be compiled on first invocation. Does that address your need?\r\n\r\nmotivation: TF don't have GPU floormod.\r\njitted operation: \r\n1. Are there scenes that jit op cannot satisfy?\r\n2. If jit op can satisfy all scenarios, can all tf op be deleted like **+, -, *, /** ?\r\n3. Is there anyone who has to close jit due to the negative effects of JIT (for example: dynamic shape), and thus must use the GPU version of the floor mod?", "We have now enabled the FloorMod MLIR generated JIT kernel by default. See e688b1e5dc87c67860bae7d477ae555ff5c9d0c9\r\n\r\nI will try to answer your questions:\r\n1. It works for all data types that the op is registered for on CPU, except int32 (but for int32 it still uses the special GPU kernel that is registered in cwise_op_floor_mod.cc which uses host memory).\r\n\r\n2. For most ops we don't have jitted kernels, but kernels which are compiled ahead of time. FloorMod would work with AOT compilation as well, but we try to avoid increasing the binary size. But yes, quite a few Eigen based GPU kernels could potentially be deleted by now.\r\n\r\n3. Our generated code supports dynamic rank and dynamic sizes (with the same limitations as the Eigen based kernels, so broadcasting works up to rank 5 after the shapes have been simplified to smaller ranks when possible - this is a bit complicated to explain). So it needs to compile only once per data type.\r\n\r\nI hope the new MLIR generated kernel fits your needs :)", "> We have now enabled the FloorMod MLIR generated JIT kernel by default. See [e688b1e](https://github.com/tensorflow/tensorflow/commit/e688b1e5dc87c67860bae7d477ae555ff5c9d0c9)\r\n> \r\n> I will try to answer your questions:\r\n> \r\n> 1. It works for all data types that the op is registered for on CPU, except int32 (but for int32 it still uses the special GPU kernel that is registered in cwise_op_floor_mod.cc which uses host memory).\r\n> 2. For most ops we don't have jitted kernels, but kernels which are compiled ahead of time. FloorMod would work with AOT compilation as well, but we try to avoid increasing the binary size. But yes, quite a few Eigen based GPU kernels could potentially be deleted by now.\r\n> 3. Our generated code supports dynamic rank and dynamic sizes (with the same limitations as the Eigen based kernels, so broadcasting works up to rank 5 after the shapes have been simplified to smaller ranks when possible - this is a bit complicated to explain). So it needs to compile only once per data type.\r\n> \r\n> I hope the new MLIR generated kernel fits your needs :)\r\n\r\nThank you for your professional and comprehensive answer, and I will close this pr.\r\nBy the way, how did you make XLA or MILR support dynamic shapes? Are there any related articles? I want to learn\uff0c thanks:)", "So first, a small disappointment: I had to revert my change because it caused problems on Windows (still need to figure out why, and how to fix them).\r\n\r\n> Thank you for your professional and comprehensive answer, and I will close this pr. By the way, how did you make XLA or MILR support dynamic shapes? Are there any related articles? I want to learn\uff0c thanks:)\r\n\r\nI don't think we have articles about this. But the basic idea for elementwise ops is that for unary ops we reshape it to 1D, then we just need code that works on a dynamic 1D tensor. For binary ops it is harder, because it needs to support broadcasting. Here we do shape specialization. We generate code for ranks between 0 and 5, and the runtime decides which one is picked. To make it possibly work for bigger ranks as well, as a first step, we reshape to shapes with minimum rank such that the broadcasting result would still be the same. We can merge dimensions if there is no change in what is broadcasted. E.g. (1, 1, 5, 5) tensor on left hand side with (3, 3, 5, 5) tensor on right hand side can be simplified to (1, 25) lhs and (9, 25) rhs. In fact we also get rid of leading 1 dimensions, so lhs shape becomes (25).", "> So first, a small disappointment: I had to revert my change because it caused problems on Windows (still need to figure out why, and how to fix them).\r\n> \r\n> > Thank you for your professional and comprehensive answer, and I will close this pr. By the way, how did you make XLA or MILR support dynamic shapes? Are there any related articles? I want to learn\uff0c thanks:)\r\n> \r\n> I don't think we have articles about this. But the basic idea for elementwise ops is that for unary ops we reshape it to 1D, then we just need code that works on a dynamic 1D tensor. For binary ops it is harder, because it needs to support broadcasting. Here we do shape specialization. We generate code for ranks between 0 and 5, and the runtime decides which one is picked. To make it possibly work for bigger ranks as well, as a first step, we reshape to shapes with minimum rank such that the broadcasting result would still be the same. We can merge dimensions if there is no change in what is broadcasted. E.g. (1, 1, 5, 5) tensor on left hand side with (3, 3, 5, 5) tensor on right hand side can be simplified to (1, 25) lhs and (9, 25) rhs. In fact we also get rid of leading 1 dimensions, so lhs shape becomes (25).\r\n\r\nthx~", "So by now I have tested the MLIR generated FloorMod kernel with ahead of time compilation on windows, and that worked. So the problem is not with the kernel itself, but with the JIT mode (we haven't triggered the problems yet with the other JIT kernels because they are with data types that are probably almost not used). I will keep you updated.", "Sorry, I forgot to comment after we have re-enabled the MLIR generated FloorMod kernel. It happened in commit ddb8fe0f9fc24a67a2a76a6b9f11174556c33938"]}]