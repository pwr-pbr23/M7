[{"number": 48630, "title": "TPU ops are compiled by default despite of if_libtpu or if_with_tpu_support configurations", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): UBUNTU18\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): SOURCE\r\n- TensorFlow version: TF2.4.1\r\n- Python version:  PY3.6\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):  3.1\r\n- GCC/Compiler version (if compiling from source): 7.5\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the problem**\r\nTPU ops are compiled by default on any platform other than ``chromiumos`` despite of if_libtpu or if_with_tpu_support configurations\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nfollow standard build from source procedure\r\nhttps://www.tensorflow.org/install/source \r\n\r\n**Any other info / logs**\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD#L575\r\n\r\n\u2018\u2019\u2018\r\ncc_library(\r\n    name = \"ops\",\r\n    visibility = [\"//visibility:public\"],\r\n    deps = [\r\n        \":user_ops_op_lib\",\r\n        \"//tensorflow/c/kernels:bitcast_op_lib\",\r\n        \"//tensorflow/c/kernels:histogram_summary_op_lib\",\r\n        \"//tensorflow/c/kernels:merge_summary_op_lib\",\r\n        \"//tensorflow/c/kernels:summary_op_lib\",\r\n        \"//tensorflow/core/ops:ops\",\r\n    ] + if_chromiumos(\r\n        [],\r\n        # Non-tpu platforms don't need tpu dependency.\r\n        [\r\n            \"//tensorflow/core/tpu/ops\",\r\n        ],\r\n    ) + if_tensorrt([\r\n        \"//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_op_lib\",\r\n        \"//tensorflow/compiler/tf2tensorrt:trt_op_libs\",\r\n    ]) + if_libtpu(\r\n        if_false = [\"//tensorflow/compiler/mlir/tensorflow:mlir_passthrough_op\"],\r\n        if_true = [],\r\n    ),\r\n)\r\n\u2019\u2018\u2019\r\n", "comments": ["@kevint324 Thanks for the report! This is working as intended, because TPU ops are also used in the current Cloud TPU product and we want to support this out of the box in TensorFlow. `//tensorflow/core/tpu/ops` also only contains the op definitions and not any kernel implementations, so should be very lightweight.\r\n\r\nThe `with_tpu_support` and `if_libtpu` are there to disable the building of the actual kernels by default, since they won't be used by the vast majority of TF users.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48630\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48630\">No</a>\n", "Got it. Thanks for the reply."]}, {"number": 48628, "title": "[r2.5 port][ROCm] Port PR 47851 to r2.5", "body": "/cc @mihaimaruseac @angerson\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/47851", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48628) for more info**.\n\n<!-- need_author_consent -->", "@deven-amd Can you please sign CLA. Thanks!", "@googlebot I consent"]}, {"number": 48627, "title": "Change \"Simple Audio Recognition\" to right link", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\n\r\nThe link of \"Simple Audio Recognition\" in  tensorflow/lite/micro/examples/micro_speech/train/README.md is not right anymore, it jumps to https://www.tensorflow.org/tutorials/sequences/audio_recognition before, but now is https://www.tensorflow.org/tutorials/audio/simple_audio .\r\n\r\nSo need to update.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["related PR has been approved and will be merged soon. Thank you "]}, {"number": 48626, "title": "Change \"Simple Audio Recognition\" to right link", "body": "1. Change the link of \" Simple Audio Recognition\" from \"https://www.tensorflow.org/tutorials/sequences/audio_recognition\" to \"https://www.tensorflow.org/tutorials/audio/simple_audio\"\r\n\r\nresolves #48627", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48626) for more info**.\n\n<!-- need_sender_cla -->", "@wenwu-glagle Can you please sign CLA. Thanks!", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48626) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48626) for more info**.\n\n<!-- need_sender_cla -->", "@gbaned I have one CLA, it's email is sww807574364@gmail.com, and I have add it in my count according [set email on git commits](https://docs.github.com/en/github/setting-up-and-managing-your-github-user-account/setting-your-commit-email-address)\r\nbut when I see https://github.com/tensorflow/tensorflow/pull/48626.patch, it's email is just the old  one like below:\r\n`From 5ba9344d27dc08bc68c98204c4a0cdcb9b486269 Mon Sep 17 00:00:00 2001\r\nFrom: wenwu <57759804+wenwu-glagle@users.noreply.github.com>\r\nDate: Tue, 20 Apr 2021 09:46:02 +0800\r\nSubject: [PATCH] Change \"Simple Audio Recognition\" to right link`\r\n\r\nI think it may be because I have checked \"Keep my email address private\" option before.\r\n\r\nSo I have discheck the  \"Keep my email address private\" in my settings now, but still can not pass the CLA check .\r\n\r\nCould you help me with this problem? Or can someone manually checking the CLA themselves since [\"It\u2019s okay to accept a contribution that you are certain is covered by a CLA, even if the automatic verification failed for some reason.\"](https://opensource.google/docs/cla/#wrong-email)", "@wenwu-glagle  Can you please make sure to use same GitHub username and email-id associated with it.", "@googlebot I signed it!", "@gbaned oh, thank you, I have the wrong  GitHub username before, and after change it, I can pass the check. "]}, {"number": 48625, "title": "Pastor", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48625) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 48623, "title": "[CherryPick:r2.5]Use the standard lib ast.unparse instead of astunparse, available in Python 3.9+.", "body": "PiperOrigin-RevId: 367434930\nChange-Id: Ieab1c5b76e54f00986ea4e6467714b1ee71199d4", "comments": ["Confirming, I listed the wrong CL number, sorry for the runaround - we should revert this PR.", "Closing as it uses the wrong CL"]}, {"number": 48622, "title": "2.5.0-rc1 cherry-pick request: Update tensorboard dependency to 2.5.x", "body": "TensorBoard release: https://pypi.org/project/tensorboard/2.5.0/\r\n\r\nPiperOrigin-RevId: 369275616\r\nChange-Id: I4355e3861c493aa8693ca1e0ea1d96695ed87a53", "comments": []}, {"number": 48621, "title": "Update mobilenet_v3.py", "body": "The documentation on the web, pulls from the docstring for the module.\r\nThis module uses an assignment to BASE_DOCSTRING to hold the documentation, which is therefore hidden from the publishing tools.\r\nI've suggested a comment indicating where the docstring can be found\r\nThis is one way to fix the issue. \r\nThe correct fix would be to move the contents of BASE_DOCSTRING to the module level. I have not done that fix, because I don't understand why this method passed code review.", "comments": ["Thanks for the PR, but we found this change is not required, and we will close it. Thanks!"]}, {"number": 48620, "title": "hard_sigmoid doc", "body": "The description wasn't detailed enough. Added a tiny bit more explanation and a reference.\r\nI don't know if this is the proper formatting of the url.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48620) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 48619, "title": "Add -mlongcalls to the Xtensa compiler flags.", "body": "This is the fix suggested by @kpraving and fixes #48516\r\n\r\nManually confirmed by restoring the resize_bilinear kernel and checking that the Xtensa build command from #48516 passes.\r\n\r\nWhile the additional `-mlongcalls` is only currently needed for the Vision P6, we have applied it uniformly to all Xtensa builds since the latency impact is minimal.\r\n\r\nThe latency change for the keyword benchmark is as follows:\r\n\r\nFusion F1:\r\n```\r\n make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_keyword_benchmark -j8\r\n```\r\n\r\nbefore:\r\n```\r\nInitializeKeywordRunner took 177927 ticks (177 ms).\r\n\r\nKeywordRunNIerations(1) took 34251 ticks (34 ms)\r\n```\r\n\r\nafter:\r\n```\r\nInitializeKeywordRunner took 178575 ticks (178 ms).\r\n\r\nKeywordRunNIerations(1) took 34333 ticks (34 ms)\r\n```\r\n\r\nHifimini:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG run_keyword_benchmark -j8\r\n```\r\n\r\nbefore:\r\n```\r\nInitializeKeywordRunner took 1403928 ticks (1403 ms).\r\n\r\nKeywordRunNIerations(1) took 90231 ticks (90 ms)\r\n```\r\n\r\nafter:\r\n```\r\nInitializeKeywordRunner took 1405400 ticks (1405 ms).\r\n\r\nKeywordRunNIerations(1) took 90252 ticks (90 ms)\r\n```\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48618, "title": "[Cherrypick:2.5]Switch absl to `lts_2021_03_24` LTS branch", "body": "PiperOrigin-RevId: 366869196\nChange-Id: Id47f326debed35e71d6df6f069fd1d50e2054a45", "comments": []}, {"number": 48617, "title": "Port RESIZE_BILINEAR kernel and tests from lite to micro", "body": "This is PR three out of three in delivering #48537. This PR ports the resize_bilinear kernel and the corresponding tests to micro, as well as adds resize bilinear to the micro build. @advaitjain ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@advaitjain now that #48616 is merged this is ready for review"]}, {"number": 48616, "title": "Refactor resize_bilinear reference op", "body": "PR two out of three in delivering #48537. This PR breaks out the resize bilinear reference op into its own header file.", "comments": []}, {"number": 48615, "title": "TF/XLA ignores tf.config set_visible_devices API and initializes all GPUs in the system", "body": "This leads to many superfluous memory allocations, possibly leading to OOM.\r\n\r\nReproduction instructions for the duplicate memory allocations when running with XLA on a multiGPU host.\r\nUse https://github.com/horovod/horovod/blob/master/examples/tensorflow2/tensorflow2_mnist.py as test.py\r\nthis test uses the \u201cset_visible_devices\u201d TF API\r\n\r\nwhen running with\r\nTF_XLA_FLAGS=\"--tf_xla_auto_jit=1 --tf_xla_enable_xla_devices=true\"  python3 test.py\r\nmemory is allocated on the visible GPU only. This is the expected behavior, observed with TF 2.3\r\n\r\nwhen running with\r\nTF_XLA_FLAGS=\"--tf_xla_auto_jit=1 --tf_xla_enable_xla_devices=false\"  python3 test.py\r\nmemory is allocated on all GPUs in the system. This behavior does not honor the tf.config set_visible_devices API, observed with > TF2.4\r\n \r\nThis issues is similar to the closed horovod issue https://github.com/horovod/horovod/issues/2548.\r\nThat issue was closed prematurely, as the presumed fix was not compatible with distributed strategy.\r\n", "comments": ["Fixed in https://github.com/tensorflow/tensorflow/commit/eaa0c588cf44e0c436560738ec361cbc4861cb2d", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48615\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48615\">No</a>\n", "> Fixed in [eaa0c58](https://github.com/tensorflow/tensorflow/commit/eaa0c588cf44e0c436560738ec361cbc4861cb2d)\r\n\r\nI confirm it fixes the issue. Thanks!"]}, {"number": 48613, "title": "Why XLA leads to much more cuMemcpyHtoDAsync calls?", "body": "I did some profiling shows that, for a resnet50 inference task, (V100 batch=64), xla jit will have 5000 more cuMemcpyHtoDAsync calls in average. For some research purpose, I would like to decrease the number of cuMemcpyHtoDAsync calls.\r\n\r\nI notice that there is hlo memory scheduler in XLA that might do the same thing as described in Grapper's page: \"Memory optimizer - Analyzes the graph to inspect the peak memory usage for each operation and inserts CPU-GPU memory copy operations for swapping GPU memory to CPU to reduce the peak memory usage.\" \r\n\r\nDoes hlo memory scheduler minize the peak memory in the same way which leads to more  cuMemcpyHtoDAsync calls? If true, how to disable such optimization?\r\n\r\nWould really appreciate any ideas on this.\r\n\r\n", "comments": ["I assume those extra memory transfers are related to JIT optimization and they would only run once during the first warmup run.\r\nAfter the first batch there should be less memory transfers.", "@kevint324 Greate thanks for the hint. I just tried and result did show that most memory transfers happends at first warmup run. \r\n\r\nJust out of curiousity, do you have any idea why XLA Jit optimization need so many memory transfers? \r\n", "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/xla/service\r\n\r\nThere are quite some hlo and llo optimization passes during the XLA setup stage.  ", "@kevint324 Thanks! I'll take a look into it."]}, {"number": 48611, "title": "E tensorflow/core/grappler/clusters/utils.cc:87] Failed to get device properties, error code: 3", "body": "### System information\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 18.04)**:\r\n-   **TensorFlow installed from (source or binary)**:source\r\n-   **TensorFlow version (use command below)**:1.15.4\r\n-   **Keras version()**:2.3.1\r\n-   **Python version**:3.6.3\r\n-   **GCC/Compiler version (if compiling from source)**:7.5\r\n-   **CUDA/cuDNN version**:1.11.0 / 8.0.4\r\n-   **GPU model and memory**:RTX3090 24GB\r\n\r\n\r\n### Describe the problem\r\nthe code I run no problem as I use RTX2070 with tensorflow1.13 keras2.2.4 and cuda10.0,but when I transfer my code to the new machine which has RTX3090,I tried to enable the oldest code and model,which lead to error when use `normalization` and **stop at the tf.TF_SessionRun_wrapper()** and can not dive into the deeper code.\r\n\r\n- segmentation code traceback(by manual track)\r\n1 \r\n/home/dell/anaconda3/envs/dxjc/lib/python3.6/site-packages/efficientnet/model.py \r\nline 390 \r\n**x = layers.BatchNormalization(axis=bn_axis, name='stem_bn')(x)**\r\n2 \r\n/home/dell/anaconda3/envs/dxjc/lib/python3.6/site-packages/keras/layers/convolutional.py\r\nline 156\r\n3 \r\n/home/dell/anaconda3/envs/dxjc/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\r\nline 1858\r\n4\r\n/home/dell/anaconda3/envs/dxjc/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\r\nline 292\r\n5\r\n/home/dell/anaconda3/envs/dxjc/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\r\nline 278\r\n6\r\n/home/dell/anaconda3/envs/dxjc/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\r\nline 198\r\n7\r\n/home/dell/anaconda3/envs/dxjc/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\r\nline 955\r\nline 1091\r\nline 1318\r\nline 1441 \r\n**return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\n                                            fetch_list, target_list,\r\n                                            run_metadata)**\r\n\r\n- detection traceback(by manual track)\r\n1\r\n/home/dell/anaconda3/envs/dx/lib/python3.6/site-packages/keras_applications/mobilenet_v2.py\r\n**x = layers.BatchNormalization(axis=channel_axis,epsilon=1e-3,momentum=0.999,name='bn_Conv1')(x)**\r\n2\r\n/home/dell/anaconda3/envs/dx/lib/python3.6/site-packages/keras/engine/base_layer.py\r\nline 451\r\n3\r\n/home/dell/anaconda3/envs/dx/lib/python3.6/site-packages/keras/layers/normalization.py \r\nline 183\r\nnormed_training, mean, variance = K.normalize_batch_in_training(\r\n            inputs, self.gamma, self.beta, reduction_axes,\r\n            epsilon=self.epsilon)\r\n4\r\n/home/dell/anaconda3/envs/dx/lib/python3.6/site-packages/keras/layers/normalization.py \r\nline 296\r\n_LOCAL_DEVICES = get_session().list_devices()\r\nline 503\r\n5\r\n/home/dell/anaconda3/envs/dx/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\r\nline 903\r\n6\r\n/home/dell/anaconda3/envs/dxjc/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\r\nline 955\r\nline 1091\r\nline 1318\r\nline 1441 \r\n**return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\n                                            fetch_list, target_list,\r\n                                            run_metadata)**\r\n\r\n### Source code / logs\r\n```python\r\nE tensorflow/core/grappler/clusters/utils.cc:87] Failed to get device properties, error code: 3\r\n```", "comments": ["@LeopoldACC ,\r\n\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48611\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48611\">No</a>\n"]}, {"number": 48610, "title": "Check if all dimensions in output are non-zero", "body": "See #48589 ", "comments": ["@irvifa \r\nYes, I'm working on that.", "@irvifa \r\nMade requested additions.\r\n", "@gbaned \r\nRequesting to review this one. Thanks!", "@chenmoneygithub \r\nThanks for the review! I have made the requested changes. ", "@chenmoneygithub \r\nThanks for the approval!", "@chenmoneygithub \r\nThe build tests failed. I will fix the errors and then request a review.\r\nThanks.", "@chenmoneygithub \r\nRequesting review again. I have fixed the syntactic bugs that were present. Using `conv_utils.output_length` function, which does not return `TensorShape` instance for easier debugging.", "@chenmoneygithub\r\nMade requested changes", "@chenmoneygithub \r\nMade requested changes", "@chenmoneygithub \r\nRequesting review.", "@chenmoneygithub \r\nMade requested changes. Requesting review.", "@AdityaKane2001 can you please check sanity build failures ?", "@rthadur \nYes", "@rthadur\r\n```python\r\n  def _check_invalid_dimension(self, dimension, idx):\r\n    \"\"\"Checks if output has all positive dimensions\"\"\"\r\n    output_dimension = conv_utils.conv_output_length(dimension,\r\n                                                     self.kernel_size[idx],\r\n                                                     self.padding,\r\n                                                     self.strides[idx],\r\n                                                     dilation = self.dilation_rate[idx]) #line 323\r\n```\r\nIn `convolutional.py` the line 323 gives sanity error. If I delete the spaces before `dilation` then it may give another sanity error. Should I disable pylint long line check for this line only? Or is there some other solution?\r\n\r\ncc @chenmoneygithub @gbaned ", "@AdityaKane2001 please try below , thank you \r\n\r\n```\r\ndef _check_invalid_dimension(self, dimension, idx):\r\n    \"\"\"Checks if output has all positive dimensions.\"\"\"\r\n    output_dimension = conv_utils.conv_output_length(\r\n        dimension,\r\n        self.kernel_size[idx],\r\n        self.padding,\r\n        self.strides[idx],\r\n        dilation=self.dilation_rate[idx])\r\n```", "Thanks\r\n", "@rthadur \r\nThe last commit should fix the build errors. Could you please run the tests again? Thanks\r\n\r\ncc @chenmoneygithub  @gbaned ", "@rthadur\r\nThanks for the review", "@rthadur \r\nI believe the windows bazel build error is persisting for all of the recent PRs. It is also unrelated to the PR and can be ignored\r\n", "@rthadur \r\nI just synced the repo to my fork, and I saw that the PR was rolled back by tensorflower-gardener. Can you please explain what's happening? Commit [here](https://github.com/tensorflow/tensorflow/commit/0174bcb337e2c8e7d6782b60b094ea70460ad071#diff-83c641ba1ee058bf5b2afe821cedf89f177973095960a14739ef12a9575416b3)."]}, {"number": 48609, "title": "Bug in ragged version of tf.losses.SparseCategoricalCrossentropy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Debian Buster**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v1.12.1-55105-ga7116dd3913 2.6.0-dev20210418**\r\n- Python version: **3.7.10**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nThe ragged version of `tf.losses.SparseCategoricalCrossentropy()` fails when the inner shape (the number of classes in the prediction) is not ragged (which is the default if the predictions are generated by a Dense layer).\r\n\r\nIn other words, while the following works,\r\n```python\r\ny_true = tf.ragged.constant([[0, 1], [2]])\r\ny_pred = tf.ragged.constant([[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]], dtype=tf.float32)\r\nprint(y_true.shape, y_pred.shape)\r\n>>> (2, None) (2, None, None)\r\nprint(tf.losses.SparseCategoricalCrossentropy()(y_true, y_pred))\r\n```\r\nthe following code fails:\r\n```python\r\ny_true = tf.ragged.constant([[0, 1], [2]])\r\ny_pred = tf.ragged.constant([[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]], ragged_rank=1, dtype=tf.float32)\r\nprint(y_true.shape, y_pred.shape)\r\n>>> (2, None) (2, None, 3)\r\nprint(tf.losses.SparseCategoricalCrossentropy()(y_true, y_pred))\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe computation should not crash.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1k8POHBqlHn4Q5_7GUuaINdmX4F-u3Ktb?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nAdding @pedro-r-marques who wrote the code.", "comments": ["Adding @pedro-r-marques who wrote the code.", "Was able to reproduce the issue in TF 2.4.1 and nightly versions. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/17a0a4e7dd419bf42f096ccb9db1fca1/raggedsparsecategoricalcrossentropybug.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48609\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48609\">No</a>\n"]}, {"number": 48608, "title": "Keras plot_model: AttributeError: 'ResourceVariable' object has no attribute '_keras_history'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n\r\nThe following things probably don't matter:\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 11.2/8.1\r\n- GPU model and memory: RTX3070, 8GB\r\n\r\n**Describe the current behavior**\r\nError: `AttributeError: 'ResourceVariable' object has no attribute '_keras_history'`\r\n\r\n**Describe the expected behavior**\r\nPlot is saved to 'model.png'\r\n\r\n**Standalone code to reproduce the issue**\r\nCopy into colab or standalone file:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model, Input\r\nfrom tensorflow.keras.layers import Add\r\n\r\n\r\nclass MyModel(Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n\r\n        self.bias = self.add_weight(shape=(10, 64, 64, 256))\r\n        self.add = Add()\r\n\r\n    def call(self, x):\r\n        return self.add([x, self.bias])\r\n\r\n\r\nif __name__ == '__main__':\r\n    mm = MyModel()\r\n\r\n    x = Input(shape=(64, 64, 256), batch_size=10, name='Input')\r\n    m = Model(inputs=[x], outputs=mm.call(x))\r\n    tf.keras.utils.plot_model(m)\r\n\r\n```\r\n\r\n**Other info / logs** \r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 22, in <module>\r\n    tf.keras.utils.plot_model(m)\r\n  File \"/home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/keras/utils/vis_utils.py\", line 322, in plot_model\r\n    dot = model_to_dot(\r\n  File \"/home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/keras/utils/vis_utils.py\", line 235, in model_to_dot\r\n    for inbound_layer in nest.flatten(node.inbound_layers):\r\n  File \"/home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/node.py\", line 258, in inbound_layers\r\n    inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\r\n  File \"/home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 867, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 867, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/node.py\", line 258, in <lambda>\r\n    inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\r\nAttributeError: 'ResourceVariable' object has no attribute '_keras_history'\r\n```\r\n", "comments": ["@benjs \r\nThe code shared works on 1.x, please refer to the (does not throw any error)[gist here](https://colab.research.google.com/gist/Saduf2019/efa031a154c8e6d526efe730e892d904/untitled589.ipynb).\r\n\r\nYou can find same error reported and resolved as mentioned above here:\r\n[link](https://stackoverflow.com/questions/44889187/attributeerror-tensor-object-has-no-attribute-keras-history), [link1](https://github.com/keras-team/keras-contrib/issues/554)", "isn't ` m = Model(inputs=[x], outputs=mm(x))` ?", "Thank you for your answers. My issue has been resolved here: [https://stackoverflow.com/questions/67159270/keras-layer-channel-wise-multiplication-of-scalar-and-graph-plotting](https://stackoverflow.com/questions/67159270/keras-layer-channel-wise-multiplication-of-scalar-and-graph-plotting)\r\n\r\nAdding weights to a model probably doesnt work with plotting. ", "@benjs \r\nPlease move this issue to closed status as its resolved.", "Isn't it a bug, that `keras.utils.plot_model` cannot plot such models?\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48608\">No</a>\n"]}, {"number": 48607, "title": "Old makefile target in person dection example README", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Debian Linux\r\n- TensorFlow installed from source\r\n- Tensorflow version  7e55a20\r\n- Target platform: host\r\n\r\n**Describe the problem**\r\n\r\nThe README documentation for the \"Run the tests on a development machine\" of the [person_detection example README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/README.md#run-the-tests-on-a-development-machine) specifies a wrong make target. This PR shows the incorrect target and what it ought to be corrected to: #48594\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nI was attempting to follow the instructions at [tensorflow/lite/micro/examples/person_detection/README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/README.md#run-the-tests-on-a-development-machine) for running person detection test on a development machine.\r\n\r\nThe second step is:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_person_detection_test\r\n```\r\n\r\nThis fails with the message\r\n\r\n```\r\nmake: *** No rule to make target 'test_person_detection_test'.  Stop.\r\n```\r\n", "comments": ["@alanvgreen,\r\nCan you please confirm if we can close this issue as [the respective PR](https://github.com/tensorflow/tensorflow/pull/48594) has been merged? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48606, "title": "Enhance GreedyMemoryPlanner::PrintMemoryPlan() format", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: all\r\n- TensorFlow installed from source\r\n- Tensorflow version: All\r\n- Target platform: All\r\n\r\n**Describe the problem**\r\n\r\nWhile using the output of `GreedyMemoryPlanner::PrintMemoryPlan()` to better understand model memory use, I found myself tweaking the output to provide information in a more readily understandable way.\r\n\r\nI have a PR that collects these changes: #48595 As noted there, the changes are:\r\n\r\nPer-buffer info line improvements:\r\n\r\n(a) Reduces text quantity to make it easier to scan for information.\r\n(b) Moves size to near front line, to make it simpler for the eye to\r\nfind this information.\r\n(c) Includes ordinal letter used in per-time display below to make it\r\nsimpler to cross-reference.\r\n\r\nPer time improvements:\r\n\r\n(a) Includes tick number. This is useful in larger models for cross\r\nreferencing to the per-buffer information and also for helping to\r\ndetermine the actual operation being executed at that time.\r\n(b) Includes total memory use of buffers, which helps to more clearly\r\nidentify memory bottlenecks.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48606\">No</a>\n"]}, {"number": 48603, "title": "Unsupported data type 13 in tensor", "body": "Hi everyone,\r\n I recently converted my custom object detection model from TensorFlow '1.15.0' to TensorFlow lite so I can implement it on a raspberry PI 3 model B. But, when I tried to test it I got this error 'Unsupported data type 13 in tensor'. \r\n Can anyone help me understand this error?\r\nps:   I am using the latest TF Lite runtime version 2.5", "comments": ["Could you make sure that the installed TF version on the raspberry Pi board is 2.5.0 or beyonds? The resource tensor is supported since TF 2.5.", "By the way, are you using TFLite micro product or TFLite product?", "@abattery Thank you for replying. \r\nI am using the 2.5 version as it is showing :\r\n![xx](https://user-images.githubusercontent.com/62913708/115170161-e072a080-a0b7-11eb-8271-bac138fabaaf.PNG)\r\n\r\nand to answer your question, I am using the TFLite product. ", "Please share simple stand alone code such that we can replicate the issue faced or if possible share a colab gist. If possible, please share your converted tflite model file.", "\r\nhere's a link to my converted Tflite model :\r\nhttps://drive.google.com/drive/folders/1bNewhxBiKqv4dgaI_I0Tj0LZoc-DDwLp?usp=sharing\r\n", "I have confirmed that the above model works well with the recent TF version. It is hard to reproduce your issue on my end.\r\n\r\nHow about using the TF nightly version at your machine?", "My recommendation is that, instead of the tflite_runtime package, could you try using the recent TensorFlow package? To run resources including TensorArray through the TF select option, the TF kernels are required, which are not included in the tflite_runtime pip package yet.", "Okay I will try what you suggested. Can you please provide me the exact versions that you used to test my model?", "I verified with the today's tf-nightly version.", "I did some research and unfortunately I wasn't able to install TF2.5 on the raspberry. so , I tried with the TF 2.4 version and I am still getting the same error.\r\n ", "TF 2.4 does not support resource types. You need to install TF 2.5 or beyonds. Please refer to the following document to install TensorFlow version on your board:\r\n\r\nhttps://www.tensorflow.org/install/pip#3.-install-the-tensorflow-pip-package", "Also, as I said earlier, please make sure to use the `tf.lite.Interpreter` instead of `tflite_runtime.interpreter.Interpreter` in the TF 2.5 or beyonds.", "@HRania, As mentioned in above [comment](https://github.com/tensorflow/tensorflow/issues/48603#issuecomment-822921829), please upgrade to `TFv2.5` or `TFv2.6`. Also please refer [this link](https://www.tensorflow.org/lite/api_docs/python/tf/lite/Interpreter?hl=en) for more information on `tf.lite.Interpreter`, it helps. Thanks!", "Yes, it worked with the `TFv2.5`. Thank you all", "@HRania, Glad the issue is resolved for you, please feel free to move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48603\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48603\">No</a>\n"]}, {"number": 48602, "title": "Atrous/Dilated Convolutions cannot dynamically Compute Shape on TPU", "body": "**System information**\r\n- Colab Pro TPU Environment\r\n- TF 2.4.1\r\n- TPU v2-8\r\n\r\n**Describe the current behavior**\r\ntf.keras.layers.Conv2D(filters = 32, kernel_size=3, strides = 1, use_bias = False, dilation_rate = 2) does not compile on TPU. \r\n\r\nBuilding a custom layer using tf.nn.atrous_conv2d results in the same error. \r\n\r\n**Describe the expected behavior**\r\ntf.nn.atrous_conv2d is a supported op based on this [list](https://cloud.google.com/tpu/docs/tensorflow-ops) of supported ops. \r\n\r\n**Standalone code to reproduce the issue**\r\nBelow notebook builds minimal model and trains on mnist. Works when dilation rate is not used for the conv2d. Otherwise generates the error. \r\n\r\n[colab notebook](https://colab.research.google.com/drive/1Hl5IZ9rnK6g58j0MSC3rGqTwrsXkrfHN?usp=sharing)\r\n\r\n**Error code from Notebook**\r\n```\r\nInvalidArgumentError: 9 root error(s) found.\r\n  (0) Invalid argument: {{function_node __inference_train_function_16413}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.107 = f32[2,2,32,12,12,32]{5,4,3,2,1,0} reshape(f32[<=128,12,12,32]{3,2,1,0} %convolution.106), metadata={op_type=\"BatchToSpaceND\" op_name=\"sequential_5/conv2d_10/Conv2D/BatchToSpaceND\"}. \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_7203248111506234482/_5}}]]\r\n\t [[TPUReplicate/_compile/_757783574996934175/_4/_300]]\r\n  (1) Invalid argument: {{function_node __inference_train_function_16413}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.107 = f32[2,2,32,12,12,32]{5,4,3,2,1,0} reshape(f32[<=128,12,12,32]{3,2,1,0} %convolution.106), metadata={op_type=\"BatchToSpaceND\" op_name=\"sequential_5/conv2d_10/Conv2D/BatchToSpaceND\"}. \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_7203248111506234482/_5}}]]\r\n\t [[TPUReplicate/_compile/_757783574996934175/_4/_336]]\r\n  (2) Invalid argument: {{function_node __inference_train_function_16413}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.107 = f32[2,2,32,12,12,32]{5,4,3,2,1,0} reshape(f32[<=128,12,12,32]{3,2,1,0} %convolution.106), metadata={op_type=\"BatchToSpaceND\" op_name=\"sequential_5/conv2d_10/Conv2D/BatchToSpaceND\"}. \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_7203248111506234482/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_7203248111506234482/_5/_279]]\r\n  (3) Invalid argument: {{function_node __inference_train_function_16413}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.107 = f32[2,2,32,12,12,32]{5,4,3,2,1,0} reshape(f32[<=128,12,12,32]{3,2,1,0} %convolution.106), metadata={op_type=\"BatchToSpaceND\" op_name=\"sequential_5/conv2d_10/Conv2D/BatchToSpaceND\"}. \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_7203248111506234482/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_7203248111506234482/_5/_267]]\r\n  (4) Invalid argument: {{function_node __inference_train_function_16413}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.107 = f32[2,2,32,12,12,32]{5,4,3,2,1,0} reshape(f32[<=128,12,12,32]{3,2,1,0} %convolution.106), metadata={op_type=\"BatchToSpaceND\" op_name=\"sequential_5/conv2d_10/Conv2D/BatchToSpaceND\"}. \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_7203248111506234482/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_7203248111506234482/_5/_291]]\r\n  (5) ... [truncated]\r\n```", "comments": ["@hyang0129 \r\n\r\nSince similar issue is already being tracking in issues [#1110](https://github.com/tensorpack/tensorpack/issues/1110) [,#28788](https://github.com/tensorflow/tensorflow/issues/28788), [#26797](https://github.com/tensorflow/tensorflow/issues/26797), [#29542](https://github.com/tensorflow/tensorflow/issues/29542) .Could you please let us know if it helps.\r\n\r\nTo avoid duplicates can you please close this and subscribe/follow that issues? Thanks!\r\n\r\n\r\n\r\n", "> @hyang0129\r\n> \r\n> Since similar issue is already being tracking in issues [#1110](https://github.com/tensorpack/tensorpack/issues/1110) [,#28788](https://github.com/tensorflow/tensorflow/issues/28788), [#26797](https://github.com/tensorflow/tensorflow/issues/26797), [#29542](https://github.com/tensorflow/tensorflow/issues/29542) .Could you please let us know if it helps.\r\n> \r\n> To avoid duplicates can you please close this and subscribe/follow that issues? Thanks!\r\n\r\nThis is not a duplicate of the issues you mentioned. Those issues deal with GPU hardware accelerators. This issue deals with TPU hardware accelerators. \r\n\r\nYou can reproduce the issue by running the colab notebook posted above, using TF 2.4.1 and a TPU v2-8. \r\n\r\nThis issue has not been resolved for TPU systems. ", "@UsharaniPagadala this is not a duplicate issue. This is specific to TPU hardware accelerators. Also, those issues that you listed have been resolved, but we still get the error above in TF 2.4.1 on a TPU. ", "@ymodak,\r\nI was able to reproduce the error with [TF v2.4.1](https://colab.research.google.com/gist/amahendrakar/1bd46aca07368f2057087de77774cbe7/48602.ipynb#scrollTo=CXtFIzZ0zD9s). \r\n\r\nWhereas, with [TF v2.5.0rc1](https://colab.research.google.com/gist/amahendrakar/6fc93724ae3b0f5836684ecbe8ad308e/48602-2-5.ipynb#scrollTo=CXtFIzZ0zD9s) and TF-nightly, the error changes to \r\n```\r\nNotFoundError: Op type not registered 'XlaSetDynamicDimensionSize' in binary running on n-70c139a5-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```\r\n\r\nPlease check the linked gist for reference. Thanks!", "If anyone has a quick fix for this please let me know ASAP. I've got a deadline to hit and the latest I could start training on imagenet and get results in time would be end of April. For now, I'm substituting the dilated convs with non dilated convs. ", "Sorry for the slow response.\r\n\r\nThe root issue is that your dataset doesn't have static batch shape, and that leads to dynamic shape inside TPU XLA which is problematic.\r\n\r\nYou can simply use `drop_remainder=True` when calling the batch function to produce a `fixed` shaped batch, e.g.:\r\n\r\n```python\r\ndata = data.batch(256, drop_remainder=True)\r\n```\r\n\r\nAnd that would give you the output shape below:\r\n```\r\n<MapDataset shapes: ((256, 28, 28, 1), (256,)), types: (tf.float32, tf.int64)>\r\n```\r\n\r\nWith that, you should be able to train properly.", "oh. That is... not anywhere in the documentation that I could find.\r\n\r\nCan this be added somewhere or can we update the error generated when calling batch_to_space?\r\n\r\nSince batch to space is used by pretty much every convolution and this doesn't seem to be a common issue, so long as people drop their remainder. Maybe we can add a comment to the error saying something like \"Please ensure you have a static batch shape by batching your data and using drop_remainder = True\". ", "https://github.com/tensorflow/tensorflow/commit/6b746d6631b9dce36303ecac26b70fef9a9d8cea\r\n\r\nThe commit should make the documentation on dataset batching a bit more clear.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48602\">No</a>\n"]}, {"number": 48600, "title": "NameError: name 'wait' is not defined when executing tf.distribute.MirroredStrategy()", "body": "\r\n**System information*\r\n- TensorFlow version : 2.6.0\r\n- Python version: 3.8\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA version:  11.2\r\n\r\nI want to train the model using TensorFlow's MirroredStrategy on several GPUs. I've used `strategy = tf.distribute.MirroredStrategy()` many times before but this time when I executed it, I got an unusually long error and the mechanism became stuck for an extended period of time.\r\n\r\n**Traceback (most recent call last)**\r\n`File \"/research/dept8/gds/anafees/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\r\n    self.run()\r\n  File \"/research/dept8/gds/anafees/anaconda3/lib/python3.8/threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/research/dept8/gds/anafees/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 519, in _handle_workers\r\n    cls._wait_for_updates(current_sentinels, change_notifier)\r\n  File \"/research/dept8/gds/anafees/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 499, in _wait_for_updates\r\n    wait(sentinels, timeout=timeout)\r\nNameError: name 'wait' is not defined`\r\nNote: It works flawlessly on a single GPU but causes the above problem on two GPUs.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48600\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48600\">No</a>\n", "I reinstalled TensorFlow and now everything is fine."]}, {"number": 48599, "title": "Resnet50 pretrained model for fine tuning, the model is not convergence", "body": "Hi,\r\n\r\nI'm using pretrained Resnet50 model for my own data's training.  The model is not convergence even the train accuracy looks good shown by the log, and validation loss and accuracy is not improved during the training phase.\r\n\r\nI also test the trained model on training and val set, the accuracy is very pool(see below) \r\n\r\nAnd I try the tensorflow 1.15.0 and 2.4.0 different version, the problem is the same. Then I just change to VGG model, it works fine(no convergence problem). So could help on this issue?\r\n\r\nMy code is :\r\n`   \r\n\r\n    base_model = tf.keras.applications.ResNet50(include_top=False)\r\n    base_model.trainable = False\r\n\r\n    model = tf.keras.models.Sequential([\r\n        base_model,\r\n        #tf.keras.layers.Conv2D(filters=num_cat, kernel_size=1),\r\n        tf.keras.layers.GlobalAveragePooling2D(),\r\n        tf.keras.layers.Dense(units=num_cat)\r\n    ])\r\n    model.summary()\r\n    optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\r\n    loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\r\n    eval_func = tf.keras.metrics.CategoricalAccuracy()\r\n\r\n    model.compile(\r\n        optimizer=optimizer,\r\n        loss=loss_func,\r\n        metrics=[eval_func]\r\n    )\r\n    history = model.fit(train_ds, epochs=10, validation_data=val_ds)\r\n    model.save_weights('./checkpoints/final')`\r\n\r\nAnd the training log is:\r\n\r\n>\r\nTrain on 156 steps, validate on 39 steps\r\nEpoch 1/10\r\n156/156 [==============================] - 183s 1s/step - loss: 1.8214 - categorical_accuracy: 0.5048 - val_loss: 3.5411 - val_categorical_accuracy: 0.0386\r\nEpoch 2/10\r\n156/156 [==============================] - 33s 211ms/step - loss: 0.4409 - categorical_accuracy: 0.8931 - val_loss: 3.6663 - val_categorical_accuracy: 0.0386\r\nEpoch 3/10\r\n156/156 [==============================] - 34s 219ms/step - loss: 0.2582 - categorical_accuracy: 0.9365 - val_loss: 3.8821 - val_categorical_accuracy: 0.0386\r\nEpoch 4/10\r\n156/156 [==============================] - 32s 203ms/step - loss: 0.1666 - categorical_accuracy: 0.9550 - val_loss: 3.9013 - val_categorical_accuracy: 0.0386\r\nEpoch 5/10\r\n156/156 [==============================] - 31s 201ms/step - loss: 0.1212 - categorical_accuracy: 0.9630 - val_loss: 4.2440 - val_categorical_accuracy: 0.0386\r\nEpoch 6/10\r\n156/156 [==============================] - 31s 201ms/step - loss: 0.0826 - categorical_accuracy: 0.9759 - val_loss: 4.2431 - val_categorical_accuracy: 0.0386\r\nEpoch 7/10\r\n156/156 [==============================] - 31s 198ms/step - loss: 0.0648 - categorical_accuracy: 0.9807 - val_loss: 4.3009 - val_categorical_accuracy: 0.0514\r\nEpoch 8/10\r\n156/156 [==============================] - 32s 205ms/step - loss: 0.0573 - categorical_accuracy: 0.9823 - val_loss: 4.3420 - val_categorical_accuracy: 0.0386\r\nEpoch 9/10\r\n156/156 [==============================] - 31s 196ms/step - loss: 0.0548 - categorical_accuracy: 0.9839 - val_loss: 4.4843 - val_categorical_accuracy: 0.0386\r\nEpoch 10/10\r\n156/156 [==============================] - 31s 200ms/step - loss: 0.0478 - categorical_accuracy: 0.9887 - val_loss: 4.7390 - val_categorical_accuracy: 0.0386\r\n\r\nRun inference on training data and validation data:\r\n156/156 [==============================] - 35s 227ms/step - loss: 4.7357 - categorical_accuracy: 0.0386\r\n39/39 [==============================] - 11s 279ms/step - loss: 4.7326 - categorical_accuracy: 0.0386\r\nTrain Loss: 4.735676199961931; Train Acc: 0.03858520835638046\r\nVal Loss: 4.732603843395527; Val Acc: 0.03858520835638046\r\n\r\n", "comments": ["@gganduu \r\n\r\nThis issue is more suitable for TensorFlow Models repo. Could you Please post it on Tensorflow Models repo from [here.](https://github.com/tensorflow/models/issues) Thanks!\r\n\r\n\r\n\r\n", "@UsharaniPagadala\uff0c already posted.", "@gganduu \r\nTo avoid duplicates can you please close this and subscribe/follow that issue posted in TensorFlow Models repo ? \r\nThanks!", "Sure, thank @UsharaniPagadala ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48599\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48599\">No</a>\n"]}, {"number": 48598, "title": "Failed to build `xla_client:libxla_computation_client` undeclared inclusion", "body": "\r\n**System information**\r\n- OS Platform and Distribution: OS: Ubuntu 20.04.2 LTS (x86_64)\r\n- TensorFlow installed from (source or binary): 00d31f1d50c\r\n- TensorFlow version: 00d31f1d50c\r\n- Python version: 3.8.5\r\n- Installed using: conda\r\n- Bazel version (if compiling from source): 3.7.2 (1608224243)\r\n- GCC/Compiler version (if compiling from source):  c++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version: cuda-11.2\r\n- GPU model and memory: GeForce RTX 3090 / 24267MiB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nFailed to build `xla_client:libxla_computation_client.so ` at the middle with `this rule is missing dependency declarations for the following files included by 'buffer_reuse_pass.cc': 'bazel-out/.../Dialect/mhlo/IR/lhlo_ops_structs.h.inc'`\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n\r\n(xla) \u2718-8 ~/Documents/github/pytorch/xla/third_party/tensorflow [:00d31f1d50c|\u20263] \r\n19:54 $ bazel build -s --define framework_shared_object=false -c opt --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --cxxopt=-std=c++14 --cxxopt=-Wno-c++11-narrowing --cxxopt=-DXLA_CUDA=1 --config=cuda //tensorflow/compiler/xla/xla_client:libxla_computation_client.so\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=211\r\nINFO: Reading rc options for 'build' from /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=@org_tensorflow//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=@org_tensorflow//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:linux in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/compiler/xla/xla_client:libxla_computation_client.so (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nSUBCOMMAND: # //tensorflow/cc:client_session [action 'Compiling tensorflow/cc/client/client_session.cc', configuration: 56c9a992f14816a9d59321cd4c3b0fb4f53a5114659e8a55ae567caceaaa3e8d, execution platform: @local_execution_config_platform//:platform]\r\n(cd /home/tyoc213/.cache/bazel/_bazel_tyoc213/19011bf2b17a0b6da5215ad1a05b9611/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-11.2/lib64: \\\r\n    PATH=/home/tyoc213/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/usr/local/cuda-11.2/bin:/home/tyoc213/miniconda3/envs/xla/lib:/home/tyoc213/miniconda3/envs/xla/include:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/home/tyoc213/go/bin:/home/tyoc213/.nvm/versions/node/v15.6.0/bin:/home/tyoc213/.deta/bin:/home/tyoc213/miniconda3/envs/xla/bin:/home/tyoc213/miniconda3/condabin:/home/tyoc213/.rvm/gems/ruby-2.7.0/bin:/home/tyoc213/.rvm/gems/ruby-2.7.0@global/bin:/home/tyoc213/.rvm/rubies/ruby-2.7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/tyoc213/.rvm/bin:/home/tyoc213/.rvm/bin:/usr/local/go/bin:/home/tyoc213/go/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_NEED_CUDA=1 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/cc/_objs/client_session/client_session.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/cc/_objs/client_session/client_session.pic.o' -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/k8-opt/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/k8-opt/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/k8-opt/bin/external/aws-checksums -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_headers_virtual -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cublas/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cublas/include -isystem external/local_config_cuda/cuda/cufft/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cufft/include -isystem external/local_config_cuda/cuda/curand/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/curand/include -isystem external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/k8-opt/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/k8-opt/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/k8-opt/bin/external/aws-checksums/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' '-D_GLIBCXX_USE_CXX11_ABI=0' '-std=c++14' -Wno-c++11-narrowing '-DXLA_CUDA=1' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' -msse3 -DTENSORFLOW_MONOLITHIC_BUILD -pthread -c tensorflow/cc/client/client_session.cc -o bazel-out/k8-opt/bin/tensorflow/cc/_objs/client_session/client_session.pic.o)\r\n\r\n...................\r\n\r\nERROR: /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/compiler/mlir/tools/kernel_gen/transforms/BUILD:75:11: undeclared inclusion(s) in rule '//tensorflow/compiler/mlir/tools/kernel_gen/transforms:passes':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/mlir/tools/kernel_gen/transforms/buffer_reuse_pass.cc':\r\n  'bazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'\r\n\r\nTarget //tensorflow/compiler/xla/xla_client:libxla_computation_client.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1719.611s, Critical Path: 267.51s\r\nINFO: 799 processes: 5 internal, 794 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\nor if I don't use -s\r\n```\r\nERROR: /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/compiler/mlir/hlo/BUILD:556:11: undeclared inclusion(s) in rule '//tensorflow/compiler/mlir/hlo:hlo_dialect_registration':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/init.cc':\r\n  'bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'\r\nTarget //tensorflow/compiler/xla/xla_client:libxla_computation_client.so failed to build\r\n\r\n```\r\n**Any other info / logs**\r\n\r\n[full.log](https://github.com/tensorflow/tensorflow/files/6330345/full.log)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/6330346/tf_env.txt)\r\n\r\n", "comments": ["https://github.com/pytorch/xla/issues/2896#issuecomment-824581049", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48598\">No</a>\n"]}, {"number": 48596, "title": "micro: Show memory plan if preprocessor define set", "body": "Changes MicroAllocator to call GreedyMemoryPlanner::PrintMemoryPlan when\r\nthe TF_LITE_SHOW_MEMORY_USE preprocessor symbol is defined. The\r\nmotivation is to make it possible for developers to show the memory plan\r\nwithout needing to modify TfLM source code.\r\n\r\nTF_LITE_SHOW_MEMORY_USE is a new preprocessor symbol. It is not used\r\nelsewhere.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Created issue #48605"]}, {"number": 48595, "title": "micro: Printed memory plan layout changes", "body": "Reformats output of GreedyMemoryPlanner:PrintMemoryPlan() to make it\r\nsimpler to interpret.\r\n\r\nPer-buffer info line improvements:\r\n\r\n(a) Reduces text quantity to make it easier to scan for information.\r\n(b) Moves size to near front line, to make it simpler for the eye to\r\n    find this information.\r\n(c) Includes ordinal letter used in per-time display below to make it\r\n    simpler to cross-reference.\r\n\r\nPer time improvements:\r\n\r\n(a) Includes tick number. This is useful in larger models for cross\r\n    referencing to the per-buffer information and also for helping to\r\n    determine the actual operation being executed at that time.\r\n(b) Includes total memory use of buffers, which helps to more clearly\r\n    identify memory bottlenecks.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Sample output from PrintMemoryPlan()\r\n\r\n```\r\n0 (id=0): size=256, offset=2304, first_used=27 last_used=28\r\n1 (id=1): size=16, offset=0, first_used=28 last_used=29\r\n2 (id=2): size=16, offset=16, first_used=29 last_used=30\r\n3 (id=3): size=18432, offset=0, first_used=0 last_used=1\r\n4 (id=4): size=4608, offset=4608, first_used=19 last_used=20\r\n5 (id=5): size=4608, offset=0, first_used=20 last_used=21\r\n6 (id=6): size=4608, offset=4608, first_used=21 last_used=22\r\n7 (id=7): size=4608, offset=0, first_used=22 last_used=23\r\n8 (id=8): size=1152, offset=4608, first_used=23 last_used=24\r\n9 (id=9): size=2304, offset=0, first_used=24 last_used=25\r\na (id=10): size=2304, offset=2304, first_used=25 last_used=26\r\nb (id=11): size=2304, offset=0, first_used=26 last_used=27\r\nc (id=12): size=18432, offset=36864, first_used=1 last_used=2\r\nd (id=13): size=36864, offset=0, first_used=2 last_used=3\r\ne (id=14): size=9216, offset=36864, first_used=3 last_used=4\r\nf (id=15): size=18432, offset=0, first_used=4 last_used=5\r\ng (id=16): size=18432, offset=18432, first_used=5 last_used=6\r\nh (id=17): size=18432, offset=0, first_used=6 last_used=7\r\ni (id=18): size=4608, offset=18432, first_used=7 last_used=8\r\nj (id=19): size=9216, offset=0, first_used=8 last_used=9\r\nk (id=20): size=9216, offset=9216, first_used=9 last_used=10\r\nl (id=21): size=9216, offset=0, first_used=10 last_used=11\r\nm (id=22): size=2304, offset=9216, first_used=11 last_used=12\r\nn (id=23): size=4608, offset=0, first_used=12 last_used=13\r\no (id=24): size=4608, offset=4608, first_used=13 last_used=14\r\np (id=25): size=4608, offset=0, first_used=14 last_used=15\r\nq (id=26): size=4608, offset=4608, first_used=15 last_used=16\r\nr (id=27): size=4608, offset=0, first_used=16 last_used=17\r\ns (id=28): size=4608, offset=4608, first_used=17 last_used=18\r\nt (id=29): size=4608, offset=0, first_used=18 last_used=19\r\nu (id=30): size=16, offset=0, first_used=30 last_used=30\r\nv (id=31): size=9216, offset=18432, first_used=0 last_used=0\r\n   0: 33333333333333333333333333vvvvvvvvvvvvvv........................................ (27k)\r\n   1: 33333333333333333333333333...........................ccccccccccccccccccccccccccc (36k)\r\n   2: dddddddddddddddddddddddddddddddddddddddddddddddddddddccccccccccccccccccccccccccc (54k)\r\n   3: dddddddddddddddddddddddddddddddddddddddddddddddddddddeeeeeeeeeeeee.............. (45k)\r\n   4: ffffffffffffffffffffffffff...........................eeeeeeeeeeeee.............. (27k)\r\n   5: ffffffffffffffffffffffffffggggggggggggggggggggggggggg........................... (36k)\r\n   6: hhhhhhhhhhhhhhhhhhhhhhhhhhggggggggggggggggggggggggggg........................... (36k)\r\n   7: hhhhhhhhhhhhhhhhhhhhhhhhhhiiiiiii............................................... (23k)\r\n   8: jjjjjjjjjjjjj.............iiiiiii............................................... (14k)\r\n   9: jjjjjjjjjjjjjkkkkkkkkkkkkk...................................................... (18k)\r\n  10: lllllllllllllkkkkkkkkkkkkk...................................................... (18k)\r\n  11: lllllllllllllmmm................................................................ (12k)\r\n  12: nnnnnn.......mmm................................................................ (7k)\r\n  13: nnnnnnooooooo................................................................... (9k)\r\n  14: ppppppooooooo................................................................... (9k)\r\n  15: ppppppqqqqqqq................................................................... (9k)\r\n  16: rrrrrrqqqqqqq................................................................... (9k)\r\n  17: rrrrrrsssssss................................................................... (9k)\r\n  18: ttttttsssssss................................................................... (9k)\r\n  19: tttttt4444444................................................................... (9k)\r\n  20: 5555554444444................................................................... (9k)\r\n  21: 5555556666666................................................................... (9k)\r\n  22: 7777776666666................................................................... (9k)\r\n  23: 77777788........................................................................ (6k)\r\n  24: 999...88........................................................................ (4k)\r\n  25: 999aaa.......................................................................... (5k)\r\n  26: bbbaaa.......................................................................... (5k)\r\n  27: bbb............................................................................. (3k)\r\n  28: ................................................................................ (1k)\r\n  29: ................................................................................ (1k)\r\n  30: ................................................................................ (1k)\r\n```", "Created issue #48606 for discussion.", "@alanvgreen  Can you please check @advaitjain's comments and keep us posted ? Thanks!", "Apologies for the slow response - will have time to address in the next few days.", "Done. Please Take a Look.\r\n\r\nI had been using a #ifndef TF_LITE_STRIP_ERROR_STRINGS to avoid an undefined variable error in the case that MicroPrintf was stripped from the code. To avoid that, I implemented '%c' on MicroPrintf(). This increases debug build code size by several bytes on x86.", "I have pushed a commit to fix the bazel build. This PR should be ready to go now."]}, {"number": 48594, "title": "micro: Update person detection readme", "body": "The README points to a Makefile target that no longer exists\r\n(test_person_detection_test).\r\n\r\nThis commit changes the README file to instruct the developer to use the\r\ntest_person_detection_test_int8 target instead.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Added #48607 for discussion"]}, {"number": 48593, "title": "No rule to make target 'install' [cmake]", "body": "Hello!\r\nWhile compiling the minimal.cpp example of TensorFlow Lite with cmake on Yocto platform, we are having error as follows:\r\n\r\n```\r\nLog data follows:\r\n| DEBUG: Executing shell function do_install\r\n| NOTE: make -j 4 DESTDIR=/home/student/fsl-release-bsp/build-analytics-tflite/tmp/work/cortexa9hf-neon-poky-linux-gnueabi/minimal/1.0-r0/image install\r\n| ERROR: oe_runmake failed\r\n| make: *** No rule to make target 'install'.  Stop.\r\n| ERROR: Function failed: do_install (log file is located at /home/student/fsl-release-bsp/build-analytics-tflite/tmp/work/cortexa9hf-neon-poky-linux-gnueabi/minimal/1.0-r0/temp/log.do_install.2654)\r\nERROR: Task (/home/student/fsl-release-bsp/sources/meta-analytics-tflite/recipes-hello/patch/minimal.bb:do_install) failed with exit code '1'\r\nNOTE: Tasks Summary: Attempted 1787 tasks of which 1786 didn't need to be rerun and 1 failed.\r\n```\r\nOn the install line of our CMakeLists.txt file the idea is not clear in our mind about \"TARGETS\". If you brighten up our mind it would be perfect.\r\n\r\n- The CMakeLists.txt file is as follows:\r\n\r\n```\r\ncmake_minimum_required(VERSION 3.6)\r\n\r\nproject(patch)\r\n\r\nadd_executable(minimal minimal.cc)\r\n\r\ntarget_include_directories(minimal PUBLIC /home/student/fsl-release-bsp/build-analytics-tflite/tmp/sysroots/analytics/usr/include/tensorflow/contrib/lite/tools/make/downloads/flatbuffers/include)\r\n\r\ntarget_link_libraries(minimal tensorflow-lite pthread ${CMAKE_DL_LIBS})\r\n\r\ninstall(TARGETS patch DESTINATION git)\r\n```\r\nYour guidance and clarifications are important for us.\r\n\r\nThank you in advance !", "comments": ["@terryheo could you take a look?", "> @terryheo could you take a look?\r\n\r\nWe fixed our fault on install part as `install(TARGETS minimal DESTINATION git)`. However, still we are getting the same error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48593\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48593\">No</a>\n"]}]