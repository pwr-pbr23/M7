[{"number": 39679, "title": "tf.keras.callbacks.ModelCheckpoint only saves weights although save_weights_only=False", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, although very minimal\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04.5 LTS \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nPlain pip install\r\n- TensorFlow version (use command below):\r\n2.2.0\r\n- Python version:\r\n3.7.3\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A, reproducible with CPU\r\n- GPU model and memory:\r\nN/A\r\n\r\n\r\n**Describe the current behavior**\r\ntf.keras.callbacks.ModelCheckpoint only saves weights for a subclassed model, although save_weights_only=False.\r\n\r\n**Describe the expected behavior**\r\ntf.keras.callbacks.ModelCheckpoint saves the full model as a tf.SavedModel.\r\n\r\nThis behaviour is expected based on the API docs:\r\n>  save_weights_only | if True, then only the model's weights will be saved (model.save_weights(filepath)), else **the full model is saved (model.save(filepath))**.`\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\r\n\r\nAnd model.save:\r\n> save_format | Either 'tf' or 'h5', indicating whether to save the model to **Tensorflow SavedModel** or HDF5. **Defaults to 'tf'** in TF 2.X, and 'h5' in TF 1.X.`\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#save\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n````\r\nfrom pathlib import Path\r\nfrom typing import Optional\r\n\r\nimport os\r\nimport tempfile\r\nimport tensorflow as tf\r\n\r\n_TENSOR = tf.ones((10, 1, 1, 3))\r\n\r\n\r\nclass SomeModel(tf.keras.models.Model):\r\n    def call(\r\n        self,\r\n        inputs: tf.Tensor,\r\n        training: Optional[bool] = None,\r\n        mask: Optional[tf.Tensor] = None,\r\n    ) -> tf.Tensor:\r\n        return inputs\r\n\r\n\r\ndef _get_model() -> SomeModel:\r\n    model = SomeModel()\r\n    model.compile()\r\n    model.fit(x=_TENSOR, y=_TENSOR)\r\n    return model\r\n\r\n\r\ndef _unexpected_failure() -> None:\r\n    \"\"\"\r\n    The minimal working example to reproduce the bug.\r\n    tf.keras.callbacks.ModelCheckpoint has a bug: it only saves weights, while the expected\r\n    behaviour is that it saves using tf.SavedModel.\r\n    \"\"\"\r\n    model = _get_model()\r\n    with tempfile.TemporaryDirectory() as save_dir:\r\n        model_save_dir = os.path.join(save_dir, \"model\")\r\n        model.fit(\r\n            x=_TENSOR,\r\n            y=_TENSOR,\r\n            callbacks=[\r\n                tf.keras.callbacks.ModelCheckpoint(\r\n                    model_save_dir, save_weights_only=False\r\n                )\r\n            ],\r\n        )\r\n\r\n        # We can see that only the weights are saved. This prints e.g.:\r\n        # /tmp/tmpvuciirc6/model.data-00000-of-00001 contains: []\r\n        # /tmp/tmpvuciirc6/model.index contains: []\r\n        # /tmp/tmpvuciirc6/checkpoint contains: []\r\n        print(_list_dir_content(Path(save_dir)))\r\n\r\n        # Restoring throws an error:\r\n        # Traceback (most recent call last):\r\n        #  ............\r\n        #   File \"[...]/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 189, in load_model\r\n        #     loader_impl.parse_saved_model(filepath)\r\n        #   File \"[...]/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 113, in parse_saved_model\r\n        #     constants.SAVED_MODEL_FILENAME_PB))\r\n        # OSError: SavedModel file does not exist at: /tmp/tmpvuciirc6/{saved_model.pbtxt|saved_model.pb}\r\n        saved_model = tf.keras.models.load_model(save_dir, compile=False)\r\n\r\n\r\ndef _ok() -> None:\r\n    \"\"\"\r\n    This shows that the same model can be saved as a tf.SavedModel just fine.\r\n    model.save() works fine: it saves as a tf.SavedModel.\r\n    \"\"\"\r\n    model = _get_model()\r\n    with tempfile.TemporaryDirectory() as save_dir:\r\n        model_save_dir = os.path.join(save_dir, \"model\")\r\n        model.save(model_save_dir)\r\n\r\n        # We can load the model again and run inference:\r\n        saved_model = tf.keras.models.load_model(model_save_dir, compile=False)\r\n        # We can run inference from the loaded model.\r\n        saved_model(_TENSOR)\r\n\r\n        # We can see that the model is saved as a tf.SavedModel. This print e.g.:\r\n        # /tmp/tmp3f6w2q5i/model_regular_save contains: [PosixPath('/tmp/tmp3f6w2q5i/model/variables'), PosixPath('/tmp/tmp3f6w2q5i/model/assets'), PosixPath('/tmp/tmp3f6w2q5i/model/saved_model.pb')]\r\n        print(_list_dir_content(Path(save_dir)))\r\n\r\n\r\nclass ModelCheckpointWorkAround(tf.keras.callbacks.ModelCheckpoint):\r\n    def set_model(self, model: tf.keras.models.Model) -> None:\r\n        # Work around, so that the if at\r\n        # https://github.com/tensorflow/tensorflow/blob/1186e3f2098793952aa82bf356dfe51b967fb26c/tensorflow/python/keras/callbacks.py#L1189\r\n        # is skipped, so that self.save_weights_only remains False.\r\n        self.model = model\r\n\r\n\r\ndef _workaround() -> None:\r\n    \"\"\"\r\n    A workaround that shows how the bug may be fixed.\r\n    Saving and restoring using ModelCheckpointWorkAround works fine, the if at\r\n    https://github.com/tensorflow/tensorflow/blob/1186e3f2098793952aa82bf356dfe51b967fb26c/tensorflow/python/keras/callbacks.py#L1189\r\n    is circumvented.\r\n    \"\"\"\r\n    model = _get_model()\r\n    with tempfile.TemporaryDirectory() as save_dir:\r\n        model_save_dir = os.path.join(save_dir, \"model\")\r\n        model.fit(\r\n            x=_TENSOR,\r\n            y=_TENSOR,\r\n            callbacks=[\r\n                ModelCheckpointWorkAround(model_save_dir, save_weights_only=False)\r\n            ],\r\n        )\r\n\r\n        # We can see the model is saved as a tf.SavedModel. This prints e.g.:\r\n        # /tmp/tmpxmdhsfi_/model contains: [PosixPath('/tmp/tmpxmdhsfi_/model/variables'), PosixPath('/tmp/tmpxmdhsfi_/model/assets'), PosixPath('/tmp/tmpxmdhsfi_/model/saved_model.pb')]\r\n        print(_list_dir_content(Path(save_dir)))\r\n\r\n        # Running inference from a restored model works fine:\r\n        saved_model = tf.keras.models.load_model(model_save_dir, compile=False)\r\n        saved_model(_TENSOR)\r\n\r\n\r\ndef _list_dir_content(path: Path) -> str:\r\n    return \"\\n\".join(\r\n        [f\"{elem} contains: {list(elem.glob('*'))}\" for elem in path.glob(\"*\")]\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    _ok()\r\n    _workaround()\r\n    _unexpected_failure()\r\n````\r\n\r\n**Other info / logs**\r\nI **suspect** the if block at https://github.com/tensorflow/tensorflow/blob/1186e3f2098793952aa82bf356dfe51b967fb26c/tensorflow/python/keras/callbacks.py#L1189 can simply be removed, and it is a remnant from the past when tf.SavedModel did not exist yet, and H5 was the only option. Perhaps the author, @fchollet can confirm?\r\n\r\nMy suspicion is based on the fact that the provided work around works and also based on the error message here: https://github.com/tensorflow/tensorflow/blob/35b03590f6bcd5869355ce8a0c3a995fd23ecdd7/tensorflow/python/keras/saving/save.py#L123", "comments": ["I have tried in colab with TF verison 2.2 and was able to reproduce the issue.Please,find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/a9cc4ee8dfa095dbf4792c24d2e5b3a2/untitled912.ipynb).Thanks!", "Gents has this been fixed please :( ... !!!", "Issue still exists in TF 2.6 as well. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/446a5b3a9bbaafd3dba457d148202479/nighly.ipynb).Thanks!", "Also happening on TF 2.4.1", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39679\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39679\">No</a>\n"]}, {"number": 39678, "title": "Fix a bug that the output of `tf.sparse.fill_empty_rows` may not be ordered", "body": "This is a PR from JIZHI Team & TaiJi AI platform in Tencent.\r\n\r\nThis pull request is for tf 2.2.0, to fix a bug that the `sp_ordered_output ` of `tf.sparse.fill_empty_rows` is not ordered when the sp_input is unordered and all rows of the sp_input are not empty.\r\n\r\nReferring to the [docs](https://www.tensorflow.org/api_docs/python/tf/sparse/fill_empty_rows), the output of `tf.sparse.fill_empty_rows` named `sp_ordered_output` is ordered. e.g.\r\n```python\r\nsp_ids = tf.SparseTensor(indices=[[1, 2], [1, 3], [0, 1], [0, 3]],\r\n                         values=[2, 1, 1, 1],\r\n                         dense_shape=[3, 4])\r\nsp_ids, _ = tf.sparse.fill_empty_rows(sp_ids, default_value=0)\r\nprint(sp_ids)\r\n```\r\nOutput:\r\n```\r\nSparseTensor(\r\n  indices=tf.Tensor(\r\n  [[0 1]\r\n   [0 3]\r\n   [1 2]\r\n   [1 3]\r\n   [2 0]], shape=(5, 2), dtype=int64), \r\n  values=tf.Tensor([1 1 2 1 0], shape=(5,), dtype=int32), \r\n  dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)\r\n)\r\n```\r\n\r\nHowever, the `sp_ordered_output` is not ordered when the sp_input is unordered and all rows of the sp_input are not empty. e.g.\r\n```python\r\nsp_ids = tf.SparseTensor(indices=[[1, 2], [1, 3], [0, 1], [0, 3]],\r\n                         values=[2, 1, 1, 1],\r\n                         dense_shape=[2, 4])\r\nsp_ids, _ = tf.sparse.fill_empty_rows(sp_ids, default_value=0)\r\nprint(sp_ids)\r\n```\r\nOutput:\r\n```\r\nSparseTensor(\r\n  indices=tf.Tensor(\r\n  [[1 2]\r\n   [1 3]\r\n   [0 1]\r\n   [0 3]], shape=(4, 2), dtype=int64), \r\n  values=tf.Tensor([2 1 1 1], shape=(4,), dtype=int32), \r\n  dense_shape=tf.Tensor([2 4], shape=(2,), dtype=int64)\r\n)\r\n```\r\n\r\nSo we add a judgment for whether the `sp_input` is ordered, and turns to make a new ordered output tensor instead of returning the original one.\r\n\r\nThank you for your time on reviewing this PR.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39678) for more info**.\n\n<!-- need_sender_cla -->", "@firejq Thank you for your contribution. Can you please sign CLA? Thanks!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39678) for more info**.\n\n<!-- ok -->", "@ebrevdo Could you have a look please?\r\nThanks!", "@ebrevdo Can you please review this PR ? Thanks!", "Hi, @gbaned, @ebrevdo . \r\nCould you please have a look at the reason for `import/copybara` checking fail?  I would like to fix if there is something wrong in my PR.", "> Hi, @gbaned, @ebrevdo .\r\n> Could you please have a look at the reason for `import/copybara` checking fail? I would like to fix if there is something wrong in my PR.\r\n\r\n@firejq  It is successfully completed now. Thank you.", "@gbaned Could you help to merge this PR? Thanks!", "> @gbaned Could you help to merge this PR? Thanks!\r\n\r\n@firejq This is only waiting for an internal approval, it will be merged soon."]}, {"number": 39677, "title": "[TF 2.2 API docs] tf.keras.applications.resnet_v2.preprocess_input docs Returns paragraph is misleading", "body": "[https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/preprocess_input](url)\r\nThe documentation says in the 'Returns' paragraph that\r\n> The images are converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset, without scaling. \r\n\r\nHowever, according to [the function definition](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/applications/resnet_v2.py#L125-L139) it's not true anymore  since the **mode parameter** can't be set and it is always equal to **'tf'**.\r\n\r\nTherefore, this docs part must be corrected to \r\n>will scale pixels between -1 and 1,  sample-wise\r\n\r\n\r\n", "comments": ["@RaibekTussupbekov This was already resolved. Please check the page [here](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/preprocess_input).\r\n\r\nPlease verify once and close the issue. Thanks!"]}, {"number": 39676, "title": "form keras part recurrent_dropout causes a exploding loses ", "body": "recurrent_dropout promotes exploding losses\r\nhttps://github.com/fchollet/deep-learning-with-python-notebooks/issues/127\r\nhttps://github.com/fchollet/deep-learning-with-python-notebooks/issues/52\r\n\r\nsame problem with  Tensorflow :  2.0.0 and  Keras :  **2.2.4-tf**\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/35724#issuecomment-582712923\r\n\r\nwhen recurrent_dropout is zero there is no problem\r\n", "comments": ["@birolkuyumcu \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@Saduf2019 \r\nif you check these links there is no only me \r\nand also known at see this comment\r\nhttps://github.com/tensorflow/tensorflow/issues/35724#issuecomment-582712923\r\n\r\n> \"I confirmed numerous times again, that recurrent_dropout promotes exploding gradients (nan loss & weights) \"\r\n\r\ni use \r\nsame problem with Tensorflow : 2.0.0 and Keras : 2.2.4-tf\r\nwith prebuilt tensorflow pip package under windows\r\n", "any solution regarding the issue? I'm using Windows 10 64 bit OS. \r\nTensorflow: 1.15.0 and Keras: 2.2.4-tf ", "> \r\n> \r\n> any solution regarding the issue? I'm using Windows 10 64 bit OS.\r\n> Tensorflow: 1.15.0 and Keras: 2.2.4-tf\r\n\r\ndont use  recurrent_dropout for TF 2.0", "i am try wit TF 2.2 at colab \r\nrecurrent_dropout  does not  cause a expanding losses. \r\nSo for TF 2.2 at colab  there is no problem,", "ohhh... ok mate... thanks... let me check....", "![rnn](https://user-images.githubusercontent.com/65279631/82457786-ab408580-9ad7-11ea-85f6-c94aff997b25.PNG)\r\nSame again....\r\nI'm using TF 2.2 at colab", "\r\n![resim](https://user-images.githubusercontent.com/8981828/82466086-a6280a00-9ac8-11ea-93e0-c99434f0b151.png)\r\n\r\nstrange !\r\nthis my case \r\ntf 2.2.0 , colab\r\n\r\n\r\n", "yeah!!! really strange!!!! ", "@birolkuyumcu @Reeshad-Khan Can you please share a standalone code to reproduce the issue? I know `dropout` was an issue in `TF2.0` but not anymore. Thank you.\r\n", "https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb\r\ncell 28\r\n", "Hi \r\ni write standalone code to reproduce the issue\r\ntested with TF 2.2 at colab\r\nproblem related to GRU and timesteps value\r\nhttps://gist.github.com/birolkuyumcu/b0ab305dab5a6f1ed3f8c4f59e954a1f\r\n", "@birolkuyumcu I think this was resolved already. Please check the [gist](https://colab.research.google.com/gist/jvishnuvardhan/cbe0be8152adfa6680d1665c0adbba1e/gru_recurrent_dropout_bug.ipynb) with `tf-nightly`. I had checked on local also.\r\n\r\nPlease verify once and close the issue. If you have any further support type questions, please post them in Stackoverflow as there is a large community to help. GitHub is mainly for tracking bugs. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39676\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39676\">No</a>\n", "possibly related?\r\nhttps://github.com/fchollet/deep-learning-with-python-notebooks/issues/151"]}, {"number": 39675, "title": "Infinite Training", "body": "Code is : \r\n```\r\nimport tensorflow as tf\r\n\r\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\r\n    rescale=1./255,\r\n    shear_range=0.2,\r\n    zoom_range=0.2,\r\n    horizontal_flip=True, validation_split=0.2)\r\n\r\ntrain_generator = datagen.flow_from_directory(\r\n    directory=\"/content/dataset/data\",\r\n    target_size=(224, 224),\r\n    color_mode=\"rgb\",\r\n    batch_size=2,\r\n    class_mode=\"binary\",\r\n    seed=42,\r\n    subset='training'\r\n)\r\n\r\nvalid_generator = datagen.flow_from_directory(\r\n    directory=\"/content/dataset/data\",\r\n    target_size=(224, 224),\r\n    color_mode=\"rgb\",\r\n    batch_size=2,\r\n    class_mode=\"binary\",\r\n    seed=42, subset='validation'\r\n)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Conv2D(2, 5, activation='relu', input_shape=(224, 224, 3)),\r\n  tf.keras.layers.Flatten(),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit_generator(train_generator, epochs=1)\r\n```\r\n\r\nOutput is \r\n```\r\n799/Unknown - 231s 289ms/step - loss: 0.0250 - accuracy: 0.9981\r\n```\r\n\r\nIssue:\r\nThe model is training infinite it isn't stopping, I also stop running the cell and run the evaluation in test data, it is evaluating for infinite.  I have a total of 15 images in two classes. \r\n\r\nI am using tensorflow version `2.2.0` and keras version `2.3.0-tf` and running the code on google colab. Thanks.", "comments": ["@Shubhamai \r\n\r\nWill it be possible to share data to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "I am experiencing the same thing. I wasn't sure, if it is an unexpected behaviour, because using steps_per_epoch in fit method solves the issue. \r\ntensorflow.__version__ : 2.1.0", "I am not sure why this happened before @ravikyram .When I was making another colab notebook to share your code and run the code, everything works perfectly, I check the versions and they were the same. Anyway, the issue is now solved for me and Thanks for the assistance \ud83d\ude42.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39675\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39675\">No</a>\n"]}, {"number": 39674, "title": "Cloud TPU not connecting", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to run a custom script in Google cloud TPUs using TPUStratagy.\r\nWhenever I try to start the TPUs to work it is hung up on connect to the cluster and not proceeding to the next step\r\n\r\n2020-05-19 09:43:27.869431: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-19 09:43:27.875081: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\r\n2020-05-19 09:43:27.875376: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4630540 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-19 09:43:27.875476: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-05-19 09:43:27.882332: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> node-1:8470}\r\n2020-05-19 09:43:27.882369: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:43986}\r\n\r\nAfter this, it is not going any further or showing any error msgs.\r\n\r\n*Code to connect to TPU*\r\n\r\n`\r\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://node-1')\r\ntf.config.experimental_connect_to_cluster(cluster_resolver)\r\ntf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\r\n`\r\n\r\nInstance name (VM) : instance - 1\r\nZone : us-central1-b\r\nTPU name = node-1\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Instead of:\r\n```\r\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://node-1')\r\n````\r\n\r\nCan you run:\r\n```\r\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='node-1')\r\n```\r\n\r\nIf you're going to use `grpc://` you need to provide it a gprc endpoint like `grpc://10.0.0.2:8470` where `10.0.0.2` would be the TPU internal IP address.", "It was an internal TPU problem, I solved it by creating a new Google Cloud project."]}, {"number": 39673, "title": "AllocateTensors fails in hello world example", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Source\r\n- Tensorflow version (commit SHA if source): 32165792a\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Host\r\n\r\n**Describe the problem**\r\nI try to run the hello world example as described on page 92 of the TinyML book but it fails.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nBuilding the binary\r\n`make -f tensorflow/lite/micro/tools/make/Makefile hello_world`\r\nRunning the binary\r\n`tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world`\r\nI get back\r\n```\r\nFailed to allocate memory. Requested: 400, available 264, missing: 136\r\nFailed to allocate memory for node_and_registrations.\r\nAllocateTensors() failed\r\n[1]    9967 segmentation fault (core dumped)  tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world\r\n```", "comments": ["Hi Colin,\r\n\r\nI had the same issue. I'm using Ubuntu 18.04 on aarch64. \r\n\r\nI'm not sure if this solution fixes the \"real\" root of the problem but it helped.\r\n\r\nIn the file tensorflow/lite/micro/examples/hello_world/main_functions.cc:\r\n\r\nYou have to increase your reserved memory block.\r\n\r\noriginal line 38:\r\n`constexpr int kTensorArenaSize = 2 * 1024;`\r\nnew line 38: \r\n`constexpr int kTensorArenaSize = 3 * 1024;` \r\n\r\nI'm really not sure why this is a problem on a custom pc. I built this example with the command \r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS-nn nucleo_f207zg\" generate_hello_world_mbed_project\r\n\r\nfor my STM development board and it worked with the 2048 bytes. \r\n\r\nAs mentioned in the book defining a tensor arena could be trail and error.\r\n\r\nCheers \r\nPhilipp\r\n", "@muskedunder It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39673\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39673\">No</a>\n"]}, {"number": 39672, "title": "tflite_convert with --experimental_new_converter=True produce wrong result", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): tf-nightly 2.2.0.dev20200408, master version also has this problem\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ntflite_convert --saved_model_dir=./SavedModelLight_0012 --output_file=model_v12_24.tflite --experimental_new_converter=True\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-05-19 15:33:51.065585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-05-19 15:33:52.879630: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-05-19 15:33:52.886456: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-05-19 15:33:52.886489: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: GPU003\r\n2020-05-19 15:33:52.886495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: GPU003\r\n2020-05-19 15:33:52.886528: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.39.0\r\n2020-05-19 15:33:52.886572: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.39.0\r\n2020-05-19 15:33:52.886577: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.39.0\r\n2020-05-19 15:33:52.886747: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-05-19 15:33:52.891806: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3191935000 Hz\r\n2020-05-19 15:33:52.892139: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xac00be0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-19 15:33:52.892151: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-05-19 15:33:59.546331: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-05-19 15:33:59.546416: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-05-19 15:33:59.605104: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:802] Optimization results for grappler item: graph_to_optimize\r\n2020-05-19 15:33:59.605149: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:804]   function_optimizer: Graph size after: 1554 nodes (1425), 2396 edges (2267), time = 35.24ms.\r\n2020-05-19 15:33:59.605157: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:804]   function_optimizer: function_optimizer did nothing. time = 0.916ms.\r\nI0519 15:34:00.563315 139800252675840 lite.py:672] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n[generator.zip](https://github.com/tensorflow/tensorflow/files/4649090/generator.zip)\r\n[tflite.zip](https://github.com/tensorflow/tensorflow/files/4649097/tflite.zip)\r\n\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results\r\nsaved model output vs tflite output\r\n![image](https://user-images.githubusercontent.com/38289304/82303562-aef7dd80-99ed-11ea-9d0f-f15d6bde1c57.png)\r\n\r\nas you can see, tflite's output result has a weird texture, if I set --experimental_new_converter=False them I can get correct output, but I can't have dynamic shape.", "comments": ["@zerollzeng This is interesting. Can you please provide a standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan  do you mean the inference code of saved_model and tflite?", "let me just put some pre-processing code and post-processing code here, which should help how to get the output with the model I uploaded\r\n``` python\r\n        import numpy as np\r\n        from imageio import imwrite\r\n        from PIL import Image\r\n        import tensorflow as tf\r\n\r\n        imported = tf.saved_model.load(m_path)\r\n        f = imported.signatures[\"serving_default\"]\r\n\r\n        img = Image.open(img_path).convert(\"RGB\")\r\n        width, height = img.size\r\n        scale_ratio = height / 800\r\n        new_size = (int(width / scale_ratio), 800)\r\n        input_img = img.resize(new_size)\r\n        img = np.array(input_img)\r\n        #core part\r\n        img = np.expand_dims(img, 0).astype(np.float32) / 127.5 - 1\r\n        out = f(tf.constant(img))['output_1']\r\n        out = ((out.numpy().squeeze() + 1) * 127.5).astype(np.uint8)\r\n\r\n        imwrite(out_path, out)\r\n``` ", "@zerollzeng  Could you provide a link to a [colab jupyter notebook](https://colab.research.google.com/) where you perform an end-to-end conversion to a TFLite Model and run inference/prediction on some images using the Saved model (using your above comment) and the TFLite model (as given here: [end-to-end_mobilenet_conversion](https://www.tensorflow.org/lite/convert/python_api#end-to-end_mobilenet_conversion_))? [Optionally: You can plot the original image, SavedModel prediction and TFLite model prediction side by side for a few images in different jupyter notebook cells for comparison. This would make it much easier for us to debug the issue.]\r\n\r\nNote:\r\n- The preferred way of converting models is using the [Python API](https://www.tensorflow.org/lite/convert/python_api).. You can load a saved a model into the TFLiteConverter Python API using `.from_saved_model()` as given [here](https://www.tensorflow.org/lite/convert/python_api#converting_a_savedmodel_)\r\n- In TF2, `--experimental_new_converter` is set to `True` by default. You don't have to provide this flag.\r\n", "Sorry it's been too long from I am working on this issue, and I gave up run my model on mobile devices finally."]}, {"number": 39671, "title": "Update \"Custom Training\" section for `Model.train_step`", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/customization/custom_training\r\nhttps://www.tensorflow.org/tutorials/customization/custom_training_walkthrough\r\n\r\n## Description of issue (what needs changing):\r\nThe tutorial for custom training makes no mention of the `train_step` and related methods that have been added to `keras.Model`.  \r\n\r\nOne of these tutorials should have a section and example on using a custom `train_step` method.\r\n", "comments": ["I don't think we need to update the entire site, but seems like a useful thing to add to the Keras guide, at least: https://www.tensorflow.org/guide/keras/overview\r\n", "sorry I wasn't very specific, with update I mainly meant \"add a section (and example) to\"; I've updated the issue to make that clearer", "@ngc92,\r\nPlease take a look at [this guide](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example) to customize what happens in the `model.fit()` function and let us know if this is what you're looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "yes, that looks good\r\nmaybe \r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#train_step\r\nand\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#test_step\r\nshould link to that page", "Closing issue since this is explained in the [Customize what happens in Model.fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example) guide. Feel free to put in a PR if you think it would help to link to the guide from the api docs."]}, {"number": 39670, "title": "ops.executing_eagerly_outside_functions AssertionError when using train_on_batch with MirroredStrategy and disable_eager_execution", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Quadro RTX\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"` v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n\r\n**Describe the current behavior**\r\nMulti-GPU training (MirrorStrategy) with Keras train_on_batch API and disable_eager_execution() causes the following AssertionError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"mini_test.py\", line 27, in <module>\r\n    model.train_on_batch(x, y)\r\n  File \"/home/ubuntu/keras-examples/venv-keras-examples/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 1050, in train_on_batch\r\n    output_loss_metrics=self._output_loss_metrics)\r\n  File \"/home/ubuntu/keras-examples/venv-keras-examples/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py\", line 316, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/home/ubuntu/keras-examples/venv-keras-examples/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py\", line 250, in _process_single_batch\r\n    with backend.eager_learning_phase_scope(1 if training else 0), \\\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/home/ubuntu/keras-examples/venv-keras-examples/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 456, in eager_learning_phase_scope\r\n    assert ops.executing_eagerly_outside_functions()\r\nAssertionError\r\n```\r\n**Describe the expected behavior**\r\nError only occurs when using train_on_batch, MirrorStrategy and disable_eager_execution all together at the same time. \r\n* Turn off MirrorStrategy or disable_eager_execution will make the error disappear. \r\n* Replace model.train_on_batch() by model.fit() will also make the error go away\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\ndisable_eager_execution()\r\n\r\nstrategy = tf.distribute.MirroredStrategy(\r\n    devices=[\"/gpu:0\", \"/gpu:1\"])\r\n\r\nx = np.zeros((32, 28, 28))\r\ny = np.zeros((32,))\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10),\r\n    ])\r\n\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.SGD(),\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        metrics=['accuracy'])\r\n\r\nfor i in range (2):\r\n    model.train_on_batch(x, y)\r\n\r\nprint(tf.executing_eagerly())\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nReplace model.train_on_batch by model.fit (the code below) will make the error go away. Comment out `disable_eager_execution()` or `with strategy.scope():` will also make the error disappear. \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\ndisable_eager_execution()\r\n\r\nstrategy = tf.distribute.MirroredStrategy(\r\n    devices=[\"/gpu:0\", \"/gpu:1\"])\r\n\r\nwith strategy.scope():\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n    x_train, x_test = x_train / 255.0, x_test / 255.0\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(512)\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(64, activation='relu'),       \r\n        tf.keras.layers.Dense(10),\r\n    ])\r\n\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.SGD(),\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        metrics=['accuracy'])\r\n\r\nmodel.fit(\r\n    train_dataset,\r\n    epochs=2\r\n)\r\n\r\nprint(tf.executing_eagerly())\r\n```", "comments": ["@chuanli11 \r\nI ran the code shared above on tf 2.2 please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/0d7560831307c0fd1082231651123fef/untitled187.ipynb) and confirm if it replicates the issue reported else please provide complete code such that we could replicate the issue or share a colab gist with the error for us to analyse.", "@Saduf2019 Thanks for looking into this. One need to choose GPU accelerator in the runtime setting to reproduce the error.\r\n* This is the [gist](https://colab.research.google.com/drive/1GHl4xSQCidFNZOgwkC5FPm52y6voiweq?usp=sharing) that reproduces the error. \r\n* This is the [gist](https://colab.research.google.com/drive/1StQwZ8S9BBTtbiVDvHxcEO7f-lRn4Zzn?usp=sharing) that runs successfully without `disable_eager_execution`\r\n* This is the [gist](https://colab.research.google.com/drive/1ijlB3cQp_HVTXYxbcemgwUVqjNMH84m8?usp=sharing) that runs successfully without `MirrorStrategy`\r\n* This is the [gist](https://colab.research.google.com/drive/19mCtWGkO6FNEEFaOI6KD5WYr1yEoiHI5?usp=sharing) that runs successfully with `model.fit` + `disable_eager_execution` + `MirrorStrategy`", "Hi,\r\nI am using a GAN model on environment Tensorflow 2.0 + CUDA 10.0 + cuDNN 7.6.5\r\nand I have encountered the same issue\r\nbut neither two suggestions solve my problem\r\n\r\nBefore using `MirroredStrategy`\r\nmy code can work fine under both eager execution and lazy execution\r\n\r\nAfter I add `MirroredStrategy`\r\n\r\n- with eager execution\r\nit causes following error on the line with `train_on_batch`\r\n`TypeError: Variable is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.`\r\n\r\n- disable eager execution\r\nit causes `AssertionError` on the line with `train_on_batch`\r\n\r\nI've try your second suggestion\r\nbut replacing `model.train_on_batch()` by `model.fit()` doesn't work for me\r\nit still causes `AssertionError`", "@y31101996, thanks for sharing the information. \r\nFor the record, I wasn't proposing suggestions. My examples are extra context and I hope someone can educate me what is going on here.\r\n\r\n", "I am able to replicate the error gist shared, please find it [here](https://colab.sandbox.google.com/gist/Saduf2019/195a7e232c7591da1d3e2304aed56239/untitled190.ipynb)", "If you are using Distribution Strategies, it is best to use the full TFv2 functionality and enable eager mode. In particular, using train_on_batch with Dist Strat and eager disabled is not supported.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39670\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39670\">No</a>\n"]}, {"number": 39669, "title": "How to hook cuda api in the latest tensorflow?", "body": "I wanted to reimplement cuMemGetInfo and cuMemAlloc for gpu resource limitations, and it worked before, but recently it doesn't work any more:\r\n```\r\nCUresult cuMemGetInfo(size_t *free, size_t *total) {\r\n  size_t used = 0;\r\n\r\n//  if (g_vcuda_config.enable) {\r\n//    atomic_action(pid_path, get_used_gpu_memory, (void *)&used);\r\n\r\n      *total = UP_LIMIT;\r\n      *free = UP_LIMIT - USED;\r\n\r\n    return CUDA_SUCCESS;\r\n//  }\r\n\r\n//  return CUDA_ENTRY_CALL(cuda_library_entry, cuMemGetInfo_v2, free, total);\r\n}\r\n```\r\n\r\nthen I compile the file using gcc:\r\n`gcc -shared -fPIC malloc_hook.cpp -o malloc_hook.so`\r\n\r\nand load my library using LD_PRELOAD, it still gives the unlimited gpu memory.\r\n\r\ntensorflow environment:\r\ndocker tensorflow/tensorflow:latest-gpu-py3\r\n\r\nAnd run some inference benchmark:\r\n```\r\nroot@cac2936adfe8:/# python3 /test/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model=resnet50\r\n2020-05-19 06:41:22.733408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-05-19 06:41:22.734860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\n2020-05-19 06:41:23.324689: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-05-19 06:41:23.335171: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394360000 Hz\r\n2020-05-19 06:41:23.335310: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5109470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-19 06:41:23.335324: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-05-19 06:41:23.338012: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-05-19 06:41:23.470965: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x50f82a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-05-19 06:41:23.471014: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-05-19 06:41:23.472144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:af:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-05-19 06:41:23.472212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-05-19 06:41:23.472267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-19 06:41:23.474753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-05-19 06:41:23.475185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-05-19 06:41:23.478005: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-05-19 06:41:23.479674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-05-19 06:41:23.479734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-19 06:41:23.481353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-05-19 06:41:23.481398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-05-19 06:41:23.851131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-05-19 06:41:23.851172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-05-19 06:41:23.851197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-05-19 06:41:23.852244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10236 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:af:00.0, compute capability: 7.5)\r\nTensorFlow:  2.1\r\nModel:       resnet50\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  64 global\r\n             64 per device\r\nNum batches: 100\r\nNum epochs:  0.00\r\nDevices:     ['/gpu:0']\r\nNUMA bind:   False\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\nGenerating training model\r\nWARNING:tensorflow:From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:134: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.keras.layers.Conv2D` instead.\r\nW0519 06:41:23.879678 139849542941568 deprecation.py:323] From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:134: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.keras.layers.Conv2D` instead.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.__call__` method instead.\r\nW0519 06:41:23.881723 139849542941568 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.__call__` method instead.\r\nWARNING:tensorflow:From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:266: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.MaxPooling2D instead.\r\nW0519 06:41:23.907096 139849542941568 deprecation.py:323] From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:266: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.MaxPooling2D instead.\r\nInitializing graph\r\nWARNING:tensorflow:From /test/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:2267: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\nW0519 06:41:25.853999 139849542941568 deprecation.py:323] From /test/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:2267: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\n2020-05-19 06:41:26.146340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:af:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-05-19 06:41:26.146396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-05-19 06:41:26.146406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-19 06:41:26.146423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-05-19 06:41:26.146433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-05-19 06:41:26.146442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-05-19 06:41:26.146453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-05-19 06:41:26.146460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-19 06:41:26.147285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-05-19 06:41:26.147317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-05-19 06:41:26.147326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-05-19 06:41:26.147332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-05-19 06:41:26.148206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10236 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:af:00.0, compute capability: 7.5)\r\nINFO:tensorflow:Running local_init_op.\r\nI0519 06:41:27.420840 139849542941568 session_manager.py:504] Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nI0519 06:41:27.491935 139849542941568 session_manager.py:507] Done running local_init_op.\r\nRunning warm up\r\n2020-05-19 06:41:28.953542: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-19 06:41:29.262221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\nDone warm up\r\n```\r\nIt still gives the correct gpu memory, not the UP_LIMIT that i set.\r\nCould anyone please help?Thanks.", "comments": ["When I use some simple samples like use LD_PRELOAD to implement rand():\r\n```#include<stdio.h>\r\n#include<stdlib.h>\r\n#include<time.h>\r\nint main(){\r\n    srand(time(NULL));\r\n    int i =10;\r\n    while(i--)printf(\"%d\\n\",rand()%100);\r\n    return 0;\r\n}\r\n```\r\n\r\n```\r\n//unrandom.c\r\nint rand(){\r\n    return 42;//the most random number in the universe\r\n}\r\n```\r\n\r\nAfter setting LD_PRELOAD, the \"unrandom\" program does give 42 every time, so it's probably not a docker issue.\r\n", "@hyc3z you can try this.  it's work  https://github.com/phrb/intro-cuda/blob/master/src/cuda-samples/7_CUDALibraries/cuHook/libcuhook.cpp  \r\n\r\n> // We need to interpose dlsym since anyone using dlopen+dlsym to get the CUDA driver symbols will\r\n// bypass the hooking mechanism (this includes the CUDA runtime). Its tricky though, since if we\r\n// replace the real dlsym with ours, we can't dlsym() the real dlsym. To get around that, call the\r\n// 'private' libc interface called __libc_dlsym to get the real dlsym.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> I wanted to reimplement cuMemGetInfo and cuMemAlloc for gpu resource limitations, and it worked before, but recently it doesn't work any more:\r\n> \r\n> ```\r\n> CUresult cuMemGetInfo(size_t *free, size_t *total) {\r\n>   size_t used = 0;\r\n> \r\n> //  if (g_vcuda_config.enable) {\r\n> //    atomic_action(pid_path, get_used_gpu_memory, (void *)&used);\r\n> \r\n>       *total = UP_LIMIT;\r\n>       *free = UP_LIMIT - USED;\r\n> \r\n>     return CUDA_SUCCESS;\r\n> //  }\r\n> \r\n> //  return CUDA_ENTRY_CALL(cuda_library_entry, cuMemGetInfo_v2, free, total);\r\n> }\r\n> ```\r\n> \r\n> then I compile the file using gcc:\r\n> `gcc -shared -fPIC malloc_hook.cpp -o malloc_hook.so`\r\n> \r\n> and load my library using LD_PRELOAD, it still gives the unlimited gpu memory.\r\n> \r\n> tensorflow environment:\r\n> docker tensorflow/tensorflow:latest-gpu-py3\r\n> \r\n> And run some inference benchmark:\r\n> \r\n> ```\r\n> root@cac2936adfe8:/# python3 /test/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model=resnet50\r\n> 2020-05-19 06:41:22.733408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n> 2020-05-19 06:41:22.734860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n> WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> non-resource variables are not supported in the long term\r\n> 2020-05-19 06:41:23.324689: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n> 2020-05-19 06:41:23.335171: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394360000 Hz\r\n> 2020-05-19 06:41:23.335310: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5109470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-05-19 06:41:23.335324: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> 2020-05-19 06:41:23.338012: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n> 2020-05-19 06:41:23.470965: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x50f82a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-05-19 06:41:23.471014: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n> 2020-05-19 06:41:23.472144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> pciBusID: 0000:af:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\n> coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n> 2020-05-19 06:41:23.472212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n> 2020-05-19 06:41:23.472267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2020-05-19 06:41:23.474753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n> 2020-05-19 06:41:23.475185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n> 2020-05-19 06:41:23.478005: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-05-19 06:41:23.479674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-05-19 06:41:23.479734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-05-19 06:41:23.481353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> 2020-05-19 06:41:23.481398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n> 2020-05-19 06:41:23.851131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-05-19 06:41:23.851172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n> 2020-05-19 06:41:23.851197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n> 2020-05-19 06:41:23.852244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10236 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:af:00.0, compute capability: 7.5)\r\n> TensorFlow:  2.1\r\n> Model:       resnet50\r\n> Dataset:     imagenet (synthetic)\r\n> Mode:        training\r\n> SingleSess:  False\r\n> Batch size:  64 global\r\n>              64 per device\r\n> Num batches: 100\r\n> Num epochs:  0.00\r\n> Devices:     ['/gpu:0']\r\n> NUMA bind:   False\r\n> Data format: NCHW\r\n> Optimizer:   sgd\r\n> Variables:   parameter_server\r\n> ==========\r\n> Generating training model\r\n> WARNING:tensorflow:From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:134: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use `tf.keras.layers.Conv2D` instead.\r\n> W0519 06:41:23.879678 139849542941568 deprecation.py:323] From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:134: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use `tf.keras.layers.Conv2D` instead.\r\n> WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Please use `layer.__call__` method instead.\r\n> W0519 06:41:23.881723 139849542941568 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Please use `layer.__call__` method instead.\r\n> WARNING:tensorflow:From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:266: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use keras.layers.MaxPooling2D instead.\r\n> W0519 06:41:23.907096 139849542941568 deprecation.py:323] From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:266: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use keras.layers.MaxPooling2D instead.\r\n> Initializing graph\r\n> WARNING:tensorflow:From /test/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:2267: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Please switch to tf.train.MonitoredTrainingSession\r\n> W0519 06:41:25.853999 139849542941568 deprecation.py:323] From /test/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:2267: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Please switch to tf.train.MonitoredTrainingSession\r\n> 2020-05-19 06:41:26.146340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> pciBusID: 0000:af:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\n> coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n> 2020-05-19 06:41:26.146396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n> 2020-05-19 06:41:26.146406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2020-05-19 06:41:26.146423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n> 2020-05-19 06:41:26.146433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n> 2020-05-19 06:41:26.146442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-05-19 06:41:26.146453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-05-19 06:41:26.146460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-05-19 06:41:26.147285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> 2020-05-19 06:41:26.147317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-05-19 06:41:26.147326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n> 2020-05-19 06:41:26.147332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n> 2020-05-19 06:41:26.148206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10236 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:af:00.0, compute capability: 7.5)\r\n> INFO:tensorflow:Running local_init_op.\r\n> I0519 06:41:27.420840 139849542941568 session_manager.py:504] Running local_init_op.\r\n> INFO:tensorflow:Done running local_init_op.\r\n> I0519 06:41:27.491935 139849542941568 session_manager.py:507] Done running local_init_op.\r\n> Running warm up\r\n> 2020-05-19 06:41:28.953542: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2020-05-19 06:41:29.262221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> Done warm up\r\n> ```\r\n> \r\n> It still gives the correct gpu memory, not the UP_LIMIT that i set.\r\n> Could anyone please help?Thanks.\r\n\r\nany update yet?"]}, {"number": 39668, "title": "Faulty error message: \"InternalError: Failed copying input tensor from\" on tf.concat", "body": "The following code:\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n \r\nt1 = tf.constant([])\r\nprint(t1)\r\nt2 = tf.constant([\"hello\", \"world\"])\r\nprint(t2)\r\nt3 = tf.concat([t1, t2], axis=0)\r\nprint(t3)\r\n``` \r\nGives the following on my installation:\r\n```\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\ntf.Tensor([], shape=(0,), dtype=float32)\r\ntf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n~/omp/m5/plots_server.py in <module>\r\n      6 t2 = tf.constant([\"hello\", \"world\"])\r\n      7 print(t2)\r\n----> 8 t3 = tf.concat([t1, t2], axis=0)\r\n      9 print(t3)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in concat(values, axis, name)\r\n   1604           dtype=dtypes.int32).get_shape().assert_has_rank(0)\r\n   1605       return identity(values[0], name=name)\r\n-> 1606   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n   1607 \r\n   1608 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in concat_v2(values, axis, name)\r\n   1179         pass  # Add nodes to the TensorFlow graph.\r\n   1180     except _core._NotOkStatusException as e:\r\n-> 1181       _ops.raise_from_not_ok_status(e, name)\r\n   1182   # Add nodes to the TensorFlow graph.\r\n   1183   if not isinstance(values, (list, tuple)):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6651   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6652   # pylint: disable=protected-access\r\n-> 6653   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6654   # pylint: enable=protected-access\r\n   6655 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run ConcatV2: Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0. [Op:ConcatV2] name: concat\r\n```\r\nOn collab I get the correct output:\r\n```\r\nv2.2.0-0-g2b96f3662b 2.2.0\r\ntf.Tensor([], shape=(0,), dtype=float32)\r\ntf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-17-fcff1beaaddc> in <module>()\r\n      6 t2 = tf.constant([\"hello\", \"world\"])\r\n      7 print(t2)\r\n----> 8 t3 = tf.concat([t1, t2], axis=0)\r\n      9 print(t3)\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: cannot compute ConcatV2 as input #1(zero-based) was expected to be a float tensor but is a string tensor [Op:ConcatV2] name: concat\r\n```\r\n\r\n**System information**\r\n ```\r\nlsb_release -a \r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 18.04.4 LTS\r\nRelease:\t18.04\r\nCodename:\tbionic\r\n```\r\nTensorflow installed - a docker container:\r\n```\r\nFROM tensorflow/tensorflow:2.2.0-gpu-jupyter\r\n...\r\n```\r\nCPU: Intel(R) Xeon(R) CPU D-1518 @ 2.20GHz\r\nGPU:  NVIDIA Corporation GP104 [GeForce GTX 1070 Ti] (rev a1)\r\n\r\nThe code is obviously incorrect, but the seem-to-be faulty error message makes it very difficult to spot the problem.\r\n", "comments": ["@rani-pinchuk,\r\nThe difference in the error log is due to the TensorFlow version. \r\n\r\nOn running the code with [TensorFlow CPU](https://colab.research.google.com/gist/amahendrakar/500f35afe24c6e913761de3b6713cff5/39668-2-2.ipynb) the error is `InvalidArgumentError: cannot compute ConcatV2 as input #1(zero-based) was expected to be a float tensor but is a string tensor [Op:ConcatV2] name: concat`\r\n\r\nWhereas on running the code with [TensorFlow GPU](https://colab.research.google.com/gist/amahendrakar/693da38e1d229c34154514f4769dc216/39668-2-2-gpu.ipynb#scrollTo=B0kwwUbXwzDU) the error is `InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run ConcatV2: Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0. [Op:ConcatV2] name: concat`.\r\n\r\nPlease check the linked Colab gist for the complete code. Thanks!", "Thanks @amahendrakar. I see that if I run this with CPU I get a different error from the one I get when running it with GPU (on the same version v2.2.0-0-g2b96f3662b 2.2.0). So it seems that the difference in versions in my original entry is irrelevant. \r\nI wonder if this difference in the error messages between CPU and GPU is documented somewhere. ", "@rani-pinchuk While the error messages seem to be different, looks like it does makes sense:\r\n1. On CPU, the error happens when Concat op happens where you cannot concat two different dtype tensors.\r\n2. On GPU, before Concat happens, data has to be transferred to GPU first. And the error happens here because string data is not supported on GPU.", "@yongtang Thanks for the explanation. This is clear now. "]}, {"number": 39667, "title": "New converter in tf 2.2 does not propely convert PReLU operation", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): TF Google Colaboratory\r\n- TensorFlow version (or github SHA if from source): TF 2.2\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\ndef bilinear_resize(x, rsize):\r\n  return tf.compat.v1.image.resize_bilinear(x, [rsize,rsize], align_corners=True)\r\n\r\nmodel=load_model('/content/slim-net-157-0.02.hdf5',compile=False)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nopen(\"model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n1511172 \r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://github.com/anilsathyan7/Portrait-Segmentation/blob/master/models/slim_seg_512/slim-net-157-0.02.hdf5\r\n```\r\nThe result is same for any model in general with PReLU operation\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n-  It splits PReLU into Neg, ReLU and Mul layers\r\n-  I need to run it using tflite gpu delegate which supports PReLU operation\r\n\r\n**Any other info / logs**\r\n\r\nIt produces correct results using old converter i.e \r\nwith option **converter.experimental_new_converter=False**\r\n", "comments": ["@anilsathyan7 \r\nPlease provide complete stand alone code such that we could replicate the issue reported above, the code shared produces different error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/88e6e8e9140b3707d09f78248d0bc280/untitled187.ipynb)", "Here is the updated gist:https://colab.research.google.com/gist/anilsathyan7/7187a94a6c5f146f952a8f1149d287a4/untitled187.ipynb\r\n\r\n**With new converter(default):-**\r\nPReLU is split into Neg, ReLU and Mul\r\n![prelu_split](https://user-images.githubusercontent.com/1130185/82318550-ac958380-99ed-11ea-8637-c3fb2a008ea9.png)\r\n\r\n**With old converter:-**\r\nWe are able to convert PReLU properly\r\n![prelu_expected](https://user-images.githubusercontent.com/1130185/82319001-6bea3a00-99ee-11ea-8f76-d8ea23778b5b.png)\r\n\r\n", "I am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/5daf3e01cb271ba7dc1fd8622a9f554e/untitled191.ipynb)", "How can we fix this problem?", "The issue is now **resolved** with the latest TensorFlow version:\r\n1. TF tf_nightly (2.3.0.dev20200610)\r\n```\r\n!pip install tf_nightly\r\n```\r\n2. TF 2.2 latest version (2.2.0-rc4)\r\n```\r\n!pip install tensorflow==2.2.0-rc4\r\n```\r\n"]}, {"number": 39666, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@rekhat1023 \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose)\r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "Closing as duplicate and no issue template filled\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\nJust to sample over 100 similar issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n\r\nPlease make sure you do a search in the future.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39666\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39666\">No</a>\n"]}, {"number": 39665, "title": "Why is TF 2.2 much slower than TF 2.0?", "body": "Followup to [original](https://github.com/tensorflow/tensorflow/issues/33487); I've benched various configurations per code [here](https://stackoverflow.com/questions/58441514/why-is-tensorflow-2-much-slower-than-tensorflow-1#answer-58653636), and found \"Large model on Large data\" to perform much worse in TF 2.2 than in TF 2.0 or TF 1.14.0. Code for that particular case, and result plots, below.\r\n\r\nWhy is this the case? To me, this is a clear dealbreaker for upgrading.\r\n\r\n<hr>\r\n\r\n**Test plots**:\r\n\r\n![image](https://user-images.githubusercontent.com/16495490/82284722-bba71200-99aa-11ea-9877-933e45d5d945.png)\r\n\r\nPer above, Graph and Eager are **1.56x** and **1.97x** slower than their TF1 counterparts, respectively. \r\n\r\n<hr>\r\n\r\n<details>\r\n  <summary><b>Test code</b></summary>\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport random\r\nimport matplotlib.pyplot as plt\r\nfrom termcolor import cprint\r\nfrom time import time\r\n\r\nfrom tensorflow.keras.layers import Input, Dense, Conv1D\r\nfrom tensorflow.keras.layers import Dropout, GlobalAveragePooling1D\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\nimport tensorflow.keras.backend as K\r\n\r\n# tf.compat.v1.disable_eager_execution()\r\n\r\n###############################################################################\r\ndef reset_seeds(reset_graph_with_backend=None, verbose=1):\r\n    if reset_graph_with_backend is not None:\r\n        K = reset_graph_with_backend\r\n        K.clear_session()\r\n        tf.compat.v1.reset_default_graph()\r\n        if verbose:\r\n            print(\"KERAS AND TENSORFLOW GRAPHS RESET\")\r\n\r\n    np.random.seed(1)\r\n    random.seed(2)\r\n    tf.compat.v1.set_random_seed(3)\r\n    if verbose:\r\n        print(\"RANDOM SEEDS RESET\")\r\n\r\nprint(\"TF version: {}\".format(tf.__version__))\r\nreset_seeds()\r\n\r\ndef timeit(func, iterations, *args, _verbose=0, **kwargs):\r\n    times = []\r\n    t0 = time()\r\n    for _ in range(iterations):\r\n        t1 = time()\r\n        func(*args, **kwargs)\r\n        times.append(time() - t1)\r\n        print(end='.'*int(_verbose))\r\n    print(\"Time/iter: %.4f sec\" % ((time() - t0) / iterations))\r\n    plt.stem(times)\r\n    plt.ylim(0, 15)\r\n\r\n###############################################################################\r\ndef make_model_large(batch_shape):\r\n    ipt   = Input(batch_shape=batch_shape)\r\n    x     = Conv1D(64,  400, strides=4, padding='valid')(ipt)\r\n    x     = Conv1D(128, 200, strides=1, padding='valid')(x)\r\n    for _ in range(40):\r\n        x = Conv1D(256,  12, strides=1, padding='same')(x)\r\n    x     = Conv1D(512,  20, strides=2, padding='valid')(x)\r\n    x     = Conv1D(1028, 10, strides=2, padding='valid')(x)\r\n    x     = Conv1D(256,   1, strides=1, padding='valid')(x)\r\n    x     = GlobalAveragePooling1D()(x)\r\n    x     = Dense(256, activation='relu')(x)\r\n    x     = Dropout(0.5)(x)\r\n    x     = Dense(128, activation='relu')(x)\r\n    x     = Dense(64,  activation='relu')(x)\r\n    out   = Dense(1,   activation='sigmoid')(x)\r\n    model = Model(ipt, out)\r\n    model.compile(Adam(lr=1e-4), 'binary_crossentropy')\r\n    return model\r\n\r\ndef make_data(batch_shape):\r\n    return np.random.randn(*batch_shape), \\\r\n           np.random.randint(0, 2, (batch_shape[0], 1))\r\n\r\ndef test_all():\r\n    for model_fn, model_name, iters in zip(make_model_fns, model_names,\r\n                                           iterations):\r\n        for batch_shape, shape_name in zip(batch_shapes, shape_names):\r\n            reset_seeds(reset_graph_with_backend=K)\r\n            data = make_data(batch_shape)\r\n            model = model_fn(batch_shape)\r\n\r\n            model.train_on_batch(*data)\r\n            timeit(model.train_on_batch, iters, *data, _verbose=1)\r\n            \r\n            cprint(\">> {}, {} done <<\\n\".format(model_name, shape_name), 'blue')\r\n            del model\r\n###############################################################################\r\n\r\nbatch_shape_large  = (32, 14000, 30)\r\nbatch_shape = batch_shape_large\r\nmodel_fn = make_model_large\r\n\r\nbatch_shapes = batch_shape_large,\r\nmake_model_fns = make_model_large,\r\niterations = [200]\r\nshape_names = [\"Large data\"]\r\nmodel_names = [\"Large model\"]\r\n\r\ntest_all()\r\n```\r\n\r\n</details>", "comments": ["@OverLordGoldDragon,\r\nRunning the given code on Google Colab takes around 400 seconds per iteration.\r\n\r\nCould you please provide a minimal code sample to reproduce the issue reported here. Thanks!", "Can you compare tf-nightly? So we can test master code", "@amahendrakar Did you use the GPU backend? Runs fine for me", "@mihaimaruseac tf-nightly plots for 100 iterations (200 is lengthy); the Eager numbers are nicer per being biased by the initial 'warmup' - but strangely enough, Graph is worse now, though this might be due to me taking less time between successive tests this time around (I did Eager first). Either way, still much slower than TF 2.0 (and btw, this isn't new to 2.2; 2.1 was the same deal).\r\n\r\n![image](https://user-images.githubusercontent.com/16495490/82385682-b518a900-9a43-11ea-9491-be4585623a02.png)\r\n\r\n\r\n\r\n", "Was able to reproduce the issue. Please check the below plots and linked gist for the respective TensorFlow versions.\r\n- [TF v1.14](https://colab.research.google.com/gist/amahendrakar/ade7deaed275faae234cb39c0d5aceb0/39665-1-14.ipynb#scrollTo=LSQqiFqAhgoR) and [TF v1.15](https://colab.research.google.com/gist/amahendrakar/998c99bd90902986e596c3953b861046/39665-1-15.ipynb#scrollTo=LSQqiFqAhgoR)\r\n![1 x](https://user-images.githubusercontent.com/57165142/82440116-7291b380-9ab9-11ea-851c-7be010479c1d.png)\r\n\r\n- [TF v2.0 eager mode](https://colab.research.google.com/gist/amahendrakar/0a152d6ebe5f840592c57c3c5c3eea22/39665-2-0.ipynb#scrollTo=tvE7GOv8ZhJ6&line=1&uniqifier=1) and [TF v2.0 graph mode](https://colab.research.google.com/gist/amahendrakar/70e5667b5061af28af14c28a65e3bd75/39665-2-0-graph.ipynb#scrollTo=tvE7GOv8ZhJ6&line=95&uniqifier=1)\r\n![2 0](https://user-images.githubusercontent.com/57165142/82440294-b684b880-9ab9-11ea-848a-25f42333fa9b.png)\r\n\r\n- [TF v2.1 eager mode](https://colab.research.google.com/gist/amahendrakar/aeb204c3cb4071c423b197f2bbc4baa0/39665-2-1.ipynb#scrollTo=LSQqiFqAhgoR) and [TF v2.1 graph mode](https://colab.research.google.com/gist/amahendrakar/139a8bb3b52a43746bb6a9ec67311ef4/39665-2-1-graph.ipynb#scrollTo=LSQqiFqAhgoR)\r\n![2 1](https://user-images.githubusercontent.com/57165142/82440481-02376200-9aba-11ea-9076-fb9a45ad105a.png)\r\n\r\n- [TF v2.2 eager mode](https://colab.research.google.com/gist/amahendrakar/04d2152837786cc6b689241fd6b72dba/39665-2-2.ipynb#scrollTo=LSQqiFqAhgoR) and [TF v2.2 graph mode](https://colab.research.google.com/gist/amahendrakar/158879ba379abe8df65292f3360bac07/39665-2-2-graph.ipynb#scrollTo=LSQqiFqAhgoR)\r\n![2 2](https://user-images.githubusercontent.com/57165142/82440654-52162900-9aba-11ea-8ab9-6a0d1016fa7a.png)\r\n\r\n- [TF nightly eager mode](https://colab.research.google.com/gist/amahendrakar/48357c72d4bfa45bf87b0776733dc42a/39665-tf-nightly.ipynb#scrollTo=LSQqiFqAhgoR) and [TF nightly graph mode](https://colab.research.google.com/gist/amahendrakar/c4f351a921d3c547d0f5a20e2fec1de7/39665-nightly-graph.ipynb#scrollTo=LSQqiFqAhgoR)\r\n![2 3](https://user-images.githubusercontent.com/57165142/82440764-7e31aa00-9aba-11ea-8e91-46509015eadc.png)\r\n\r\nThanks!", "@amahendrakar Thanks for the encompassing testing. TF2.0-Graph is unusually fast per these benchmarks, and TF1.14.0 is unusually slow; I wonder if the difference in GPU's is responsible - my benchmarks are based on my local GTX 1070. I'll look into it.", "Regarding my plots, turns out my GPU's been overheating lately, thus the rising stem plots.\r\n\r\nHowever, the problem isn't fully solved; [followup](https://github.com/tensorflow/tensorflow/issues/42429). ", "Hi @OverLordGoldDragon ! \r\nWe are checking to see whether you still need help in this issue . Have you tried in latest version (TF 2.7 ) yet?", "Been resolved since 2.3, but also haven't checked since - at any rate I'll mark it resolved.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 39664, "title": "tflite_interpreter.invoke() give error for 2 input tensors", "body": "project description:\r\nauto image caption model building using LSTM and ann. and convert  model into tflite model.\r\n \r\nproblem:\r\nso, in this model we need to give 2 inputs one for image another one for text.\r\ntflite model was created.\r\nwhen i checking tflite model through input, `tflite_interpreter.invoke()` arise error.\r\n\r\ni attached my ipynb link.kindly go through it.\r\nadvance thanks for every one.\r\n[Notebook](https://github.com/mahesh2996/image_caption)\r\n\r\n", "comments": ["@ mahesh2996 \r\nCan you please let us know the tensorflow version on which this issue is faced.\r\n\r\n\r\n", "> @ mahesh2996\r\n> Can you please let us know the tensorflow version on which this issue is faced.\r\n\r\ntensorflow version:2.2.0\r\nThank you very much for your help.", "@mahesh2996 \r\nI ran the notebook shared but face an error, please refer to this [gist here](https://colab.sandbox.google.com/gist/Saduf2019/3c28a3a85c9c221e2e2f61720c70f4c9/untitled198.ipynb), please share all dependencies to reproduce the error faced or a colab gist if possible.", "> @mahesh2996\r\n> I ran the notebook shared but face an error, please refer to this [gist here](https://colab.sandbox.google.com/gist/Saduf2019/3c28a3a85c9c221e2e2f61720c70f4c9/untitled198.ipynb), please share all dependencies to reproduce the error faced or a colab gist if possible.\r\n\r\nImage caption:Gist Link\r\nhttps://gist.github.com/mahesh2996/bbc9a0730bae276c94fbe7187029e928", "> > @mahesh2996\r\n> > I ran the notebook shared but face an error, please refer to this [gist here](https://colab.sandbox.google.com/gist/Saduf2019/3c28a3a85c9c221e2e2f61720c70f4c9/untitled198.ipynb), please share all dependencies to reproduce the error faced or a colab gist if possible.\r\n> \r\n> Image caption:Gist Link\r\n> https://gist.github.com/mahesh2996/bbc9a0730bae276c94fbe7187029e928\r\n\r\ndata set:[https://www.kaggle.com/keenwarrior/small-flicker-data-for-image-captioning/version/1](url)\r\n\r\n", "@mahesh2996\r\nI am unable to access the links shared, Could you please share a gist of your complete code along with the errors faced.", "> @mahesh2996\r\n> I am unable to access the links shared, Could you please share a gist of your complete code along with the errors faced.\r\n\r\ni tried gist.but not able to do.sorry for that.\r\nIn this notebook all cells are executed perfectly except the error one.[Notebook](https://github.com/mahesh2996/image_caption)\r\ni used this data set:[https://www.kaggle.com/keenwarrior/small-flicker-data-for-image-captioning/version/1](url)\r\n\r\n\r\n\r\n", "@mahesh2996\r\nLike the earlier gist shared i am unable to access this code, can you please paste the code here so i could try to replicate the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39663, "title": "When using tf.keras.Model.predict(), the `batch_size` makes the results different", "body": "The code below is different:\r\nimages = np.array([cv2.imread('a.jpg'), cv2.imread('b.jpg')]) / 255.\r\nmodel = tf.keras.applications.xception.Xception()\r\nmodel.predict(images,  **batch_size**=1)\r\n\r\nand:\r\nmodel = tf.keras.applications.xception.Xception()\r\nmodel.predict(images,  **batch_size**=2)\r\n", "comments": ["I have change the BatchNormlization()(x, training=False)", "It seems that the inputs of the model are different, because `images` tensor is generated randomly.", "I have change the dscription, the images is same, the results is different", "@mengjiexu \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Also, please incluse your Tensorflow version?Thanks!", "I mean with two same images(can be any images), same xception network, when predict with batch_size = 1 and batch_size=2, the result is different.", "I have tried in colab with TF version 2.2.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/d4c498f25ce24fec03c5e9cab1aa8e0c/untitled914.ipynb)Is the the expected behavior?.Thanks!", "@mengjiexu \r\n\r\nAny update on this issue please. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39663\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39663\">No</a>\n"]}, {"number": 39662, "title": "Resize images before feeding for deep tensorflow model", "body": "Hello everyone.\r\n I tested any size of input image for ssd detection models, it don\u2019t effect in timing prediction and all the results are same and correct.\r\nIn your opinion, inside the frozen graph model has resizer node or operation for this task on GPU?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 39661, "title": "Unaligned 64-bit access causes crash on Hexagon platforms", "body": "We've discovered the cause of a TF Lite Micro crash on some Qualcomm Hexagon platforms, and need to figure out a good solution. Here are the details:\r\n\r\nQuote from tensorflow/lite/micro/micro_allocator.cc:327 :\r\n\r\n```\r\n\r\n      // Note that the zero_point field in the FlatBuffers schema is a 64-bit\r\n\r\n      // integer, but the zero_point field in the TfLiteQuantizationParams struct\r\n\r\n      // is a 32-bit integer.\r\n\r\n      result->params.zero_point =\r\n\r\n          static_cast<int32_t>(src_quantization->zero_point()->Get(0));\r\n\r\n```\r\n\r\nAs stated in the comment, the zero_point is a 64bit value inside flatbuffer\u2019s schema.\r\n\r\nBut when hexagon DSP tries to read a 64bit value, the address needs to be 64bit aligned. This is a hardware instruction level limitation of our DSP.\r\n\r\nSo, when the execution fell into IndirectHelper::Read(), hexagon-sim crashed.\r\n\r\nQuote from tensorflow/lite/micro/tools/make/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:104 :\r\n\r\n```\r\n\r\n>>  static return_type Read(const uint8_t *p, uoffset_t i) {\r\n\r\n      return EndianScalar((reinterpret_cast<const T *>(p))[i]);\r\n\r\n    }\r\n\r\n```\r\n\r\nThis pointer \u201cp\u201d is not guaranteed to be 8 byte aligned. So when I am not lucky, the linker will give it a bad location. Hence, random crash.\r\n\r\n ", "comments": ["After evaluating options, I've opted to add alignas(8) to our serialized flatbuffers.  The entire flatbuffer is internally aligned properly (for example, 64-bit datatypes are aligned to 8 byte boundaries).\r\n\r\nI have submitted a change to update our in-tree example and benchmark models to align the flatbuffer array to 8 bytes.  I am working to update alignment in models which are downloaded at compile time.", "@petewarden  \r\nPlease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39661\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39661\">No</a>\n"]}, {"number": 39660, "title": "test1", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39660\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39660\">No</a>\n"]}, {"number": 39659, "title": "[XLA:GPU] Handle Batch Group Convolutions for GPU backend", "body": "**Notation Summary/Notes:**\r\n- For the purposes of this explanation, a layout `[N, H, W, ..., C]` is expressed as `[N, C]` ignoring the spatial dimension.\r\n- `lhs (*) rhs -> out` convolution operation\r\n-`lhs (fg*) rhs -> out` feature grouped convolution where `fg` is feature groups\r\n-`lhs (bg*) rhs -> out` feature grouped convolution where `bg` is batch groups\r\n- `lhs` and `rhs` are the inputs and kernels/filters for the convolution respectively.\r\n- Canonical form of feature grouped convolutions\r\n`[N, C_i] (fg*) [C_i/fg, C_o] -> [N, C_o]`\r\n-  `C_i, C_o` are input features and output features respectively.\r\n\r\n**Description:**\r\nSemantics of a batch group convolution was defined as follows in TF 1.x (ignoring the spatial dimensions):\r\n`[N, C] (bg*) [C, bg] -> [N/bg, bg]` where `bg` is the `batch-group-count`. These semantics allowed backward depthwise filter convolutions to be mapped to forward batch grouped convolutions in the tf2xla bridge. In order to thunk these convolutions to cudnn backward feature grouped convs, a pass _DepthwiseConvolutionConverter_ was introduced in this [PR](https://github.com/tensorflow/tensorflow/pull/31597).\r\n\r\nNow, however, batch group convolution semantics have changed. They are now defined as\r\n`[N, C_i] (bg*) [C_i, C_o] -> [N/bg, C_o]` , `bg` being the batch group count. It can be mathematically shown that backward filter grouped convolutions can be evaluated using batch-group-convolutions (see proof at the end of this description). Hence, tf2xla now pipes all grouped backward filter through batch-group-convolutions.\r\n\r\nAs a consequence of the above, almost all backward filter grouped convolutions are now transformed to ungrouped forward convolutions via the pass _ConvolutionGroupConverter_ which then gets thunked to cuDNN forward convolutions. This can cause performance regressions in networks such as ResNext which have grouped convolutions. We have seen earlier that thunking the backward filter group convolutions to forward convolutions in Mobilenet had caused regressions. This regression can now extend to all networks with grouped convolutions.\r\n\r\n_DepthwiseConvolutionConverter_ was designed based on the earlier definition of batch-group-convolutions and assumed that only depthwise filter convolutions used batch-group-convolutions in tf2xla. That assumption is no longer valid and hence the incompatibility. Now all backward filter grouped convolutions can trigger this pass producing incorrect results.\r\n\r\nTo solve this issue, I made changes to `gpu_conv_rewriter` while turning off _DepthwiseConvolutionConverter_ and _ConvolutionGroupConverter_  such that all batch grouped convolutions (which are actually backward filter grouped convs) get rewritten into the correct custom call that thunks to cudnn backward filter APIs.\r\n\r\nSome more details:\r\nWhen batch grouped conv is intercepted by _MatchBackwardFilter_, the forward convolution looks like this:\r\n`[C_i, N] (bg*) [N, C_o] -> [C_i/bg, C_o]`\r\nCudnn backward filter API expects lhs to be `[N, C_i]` and rhs to be `[N, C_o]`. So all we need to do at this stage to thunk to the correct cudnn API is to swap the batch and channel dimension. \r\nHowever, if a batch group conv cannot be thunked to a cudnn backward filter conv (when padding_high is negative), the above batch group forward convolution needs to be transformed to feature grouped convolution that can be then thunked to a cudnn forward conv.  This transformation requires 2 reshapes and a transpose to generate the following conv:\r\n`[C_i/fg, fg*N] (fg*) [N, C_o] -> [C_i/fg, C_o]` where `fg = bg`\r\n\r\nI qualified my changes by validating the functionality using tensorflow/compiler/tests/depthwise_conv_op_test.py, tensorflow/python/kernel_tests/conv_ops_test.py,\r\ntensorflow/compiler/xla/tests/\\*conv\\*test.cc\r\n`run_hlo_module` on the hlo provided by Adrian from Google which initially reproduced the issue.\r\n\r\n**Concise proof of how backward filter feature group convs are mapped to forward batch group convs:**\r\nConvolving a transformed input with gradient will produce the grad filter. For a convolution that has feature groups, the convolution looks like this:\r\n`[C_i/fg, fg*N] (fg*) [N, C_o] -> [C_i/fg, C_o]`\r\nwhere `[C_i/fg, fg*N]` is the transformed input data tensor (details can be found in [conv_grad_ops.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_grad_ops.h))\r\nNow the cannonical form of a batch grouped convolution is \r\n`[N, C_i] (bg*) [C_i, C_o] -> [N/bg, C_o]`\r\n\r\nNow our goal is the produce filter gradients of dimensions `[C_i/fg, C_o]`. Instead of using feature groups convolutions, if we use batch group convolutions, that goal can be easily achieved by just swapping the batch dimension and the feature dimension in the input and having `bg = fg`. No layout change required. The input still needs to be dilated though as described in [conv_grad_ops.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_grad_ops.h))\r\nThe batch group convolution would look like this: \r\n`[C_i, N] (bg*) [N, C_o] -> [C_i/bg, C_o]`", "comments": ["@akuegel Hi Adrian... I have created this PR to handle the batch_group_conv/feature grouped filter grad conv changes. I have qualified this PR based on the tests I have mentioned in the details. However, I need your help to run the performance benchmarks. There may be other issues I might have missed due to lack of a relevant TF2 model to run.  Hopefully we can identify and fix them using the CI.", "@akuegel, @joker-eph, is there anything needed from our end to advance the review of this PR?", "@bas-aarts I'm just auto-subscribed on PR which touch \" tensorflow/compiler/...\" but I am not knowledgeable enough to review this.\r\nAdding @sanjoy in case he can redirect by adding someone else here?", "Adrian is the best person to review this, let me ping him in case this PR got lost in the mail.", "@akuegel Thanks Adrian for looking into this. Can you please share any performance numbers that you found based on this change? May be looking at the numbers with/without this change would be useful in analyzing the regression caused by the bridge change and hopefully confirm that this PR has fixed the issue.", "I ran our benchmarks, but as expected there were no visible changes. We only had one benchmark which used grouped convolutions, and that benchmark got broken by TF 2.0, so we don't run it anymore.", "> I ran our benchmarks, but as expected there were no visible changes. We only had one benchmark which used grouped convolutions, and that benchmark got broken by TF 2.0, so we don't run it anymore.\r\n\r\nI see...the only ones affected would the ones with grouped conv like ResNext or MobileNet. It would be interesting to know their performance with TF 2.x based on these changes. But it seems there aren't any working models available for now. "]}, {"number": 39658, "title": "TFLite quantized maximum/minimum ops still do basic comparison even with different input tensor scales", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 18.04.1-Ubuntu\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: from source, git hash 491d6e42ce118a0c0156ce93eca67541069c617f (note bug is not specific to this hash)\r\n- **TensorFlow version (use command below)**: 2.1.0\r\n- **Python version**: 3.6.9\r\n- **Bazel version (if compiling from source)**: 3.0.0\r\n- **GCC/Compiler version (if compiling from source)**:  7.5.0\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI found tensorflow lite quantized element-wise maximum/minimum ops has issue when its input tensors have different scales. It supposed to do rescaling on one of its input tensor before doing the max/min selection. but from the source code and my experiment it seems to do the direct comparison.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nHere's my synthetic tflite .mlir of my quantized uint8 deepspeech model:\r\n\r\nmodule attributes {tfl.description = \"TOCO Converted.\", tfl.schema_version = 3 : i32} {\r\n  func @main(%arg0: tensor<16x2048x!quant.uniform<u8:f32, 0.24592778086662292>>) -> tensor<16x2048x!quant.uniform<u8:f32, 0.078431375324726104>> attributes {tf.entry_function = {inputs = \"input_tensor\", outputs = \"Minimum\"}} {\r\n    %0 = \"tfl.pseudo_qconst\"() {qtype = tensor<!quant.uniform<u8<1:255>:f32, 0.078431375324726104>>, value = dense<-1> : tensor<i8>} : () -> tensor<!quant.uniform<u8<1:255>:f32, 0.078431375324726104>>\r\n    %1 = \"tfl.minimum\"(%arg0, %0) : (tensor<16x2048x!quant.uniform<u8:f32, 0.24592778086662292>>, tensor<!quant.uniform<u8<1:255>:f32, 0.078431375324726104>>) -> tensor<16x2048x!quant.uniform<u8:f32, 0.078431375324726104>>\r\n    return %1 : tensor<16x2048x!quant.uniform<u8:f32, 0.078431375324726104>>\r\n  }\r\n}\r\n\r\ninput (rhs input to minimum op) and output of this graph are the same (since rhs input to minimum is constant 255):\r\n[[15  1  0 ...  3  1  0]\r\n [14  4  0 ...  1  7  0]\r\n [ 8  1  0 ...  0  0  0]\r\n ...\r\n [ 0  0  0 ...  0  0  0]\r\n [ 0  0  0 ...  0  8  0]\r\n [ 0  3  9 ...  0  1  0]]\r\n\r\nBut since output tensor scale is 0.07843, and lhs input tensor scale is 0.24592. shouldn't output be rescaled in this case? My predicted result, which takes this into consideration, would be:\r\n[[47  3  0 ...  9  3  0]\r\n [44 13  0 ...  3 22  0]\r\n [25  3  0 ...  0  0  0]\r\n ...\r\n [ 0  0  0 ...  0  0  0]\r\n [ 0  0  0 ...  0 25  0]\r\n [ 0  9 28 ...  0  3  0]]\r\n\r\nAnd I take a look at tensorflow lite reference kernel implementation at tensorflow/lite/kernels/maximum_minimum.cc. Not surprisingly, it's doing the comparison directly without any rescale. Seems like tfl quantized maximum/minimum ops are assuming input tensors always have same scales?\r\n", "comments": ["@renjie-liu could you check this?", "Hi,\r\n\r\ncan you try with the new mlir converter?\r\n\r\nWe're ensuring the operands have the same scale.", "Hi,\r\n\r\nAppreciate for your reply.\r\n\r\nI'm using flatbuffer_translate --tflite-flatbuffer-to-mlir to convert my .tflite to tflite .mlir, with the git hash I specified above (dated at May 7th). Do you mean there's recent hash that happen to fix this problem? Or there's other tool I should use to do this conversion?", "I mean using the new converter to convert the tflite model directly, if you're using the python TFLiteConverter API, that should be set to true on default for tf2.2", "thanks. I guess that mean I need to regenerate my tflite model with more recent hash."]}, {"number": 39657, "title": "test", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": []}, {"number": 39656, "title": "Eager execution in TF 2.2.0rc2", "body": "Tensorflow 2.2.0rc2\r\nWindows 10\r\nPython 3.6.8(Anaconda)\r\nCuDNN 7.6.5 CUDA 10.1\r\nNvidia 1060 6Gb\r\nI have a model based on Keras\r\n\r\n> import tensorflow.keras as keras\r\n\r\nwhen try \r\n```\r\nmodel_details=model.fit(\r\n    data_train,\r\n    box_train,    \r\n    batch_size=128,epochs=100,shuffle=True,validation_split=0.1,callbacks=[lr_scheduler,lr_reducer,checkpoint],verbose=1)\r\n```\r\n\r\ngot error\r\n`InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]`\r\n\r\n\r\nbut when disable Eager , everything is working?\r\n\r\n`tf.compat.v1.disable_eager_execution()`", "comments": ["Why are you using 2.2.0rc2? Have you tried reproducing this using the proper 2.2.0 version?\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v2.2.0", "@vladimircape,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nAlso, please upgrade to the stable version of TensorFlow v2.2 and let us know if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39655, "title": "Update saved_model README.md", "body": "Updated python usage for TF 2.X version.\r\nFixes #39638", "comments": ["Since this is deprecated, I think in the readme it should point to the proper way of being used, I don't think using the compatibility mode is the proper way.\r\n\r\nAnd in TF 2.0 as I understand you are not supposed to use tf.Session anymore not even with compatibility mode, as I understand, the compatibility mode is just meant for retro-compatibility to quick fix old codes, but if you are starting with a new code you are not supposed to use those methods.\r\n\r\nAm I wrong?"]}, {"number": 39654, "title": "How to speed up text generation in TensorFlow reference example notebook?", "body": "The tensorflow official example for text generation (https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb) runs in a loop as defined below. The text generation feels slow, and according to NVTOP only uses a fraction of the available GPU resources (15-20%).\r\n\r\n```\r\ndef generate_text(model, start_string):\r\n  # Evaluation step (generating text using the learned model)\r\n\r\n  # Number of characters to generate\r\n  num_generate = 1000\r\n\r\n  # Converting our start string to numbers (vectorizing)\r\n  input_eval = [char2idx[s] for s in start_string]\r\n  input_eval = tf.expand_dims(input_eval, 0)\r\n\r\n  # Empty string to store our results\r\n  text_generated = []\r\n\r\n  # Low temperatures results in more predictable text.\r\n  # Higher temperatures results in more surprising text.\r\n  # Experiment to find the best setting.\r\n  temperature = 1.0\r\n\r\n  # Here batch size == 1\r\n  model.reset_states()\r\n  for i in range(num_generate):\r\n      predictions = model(input_eval)\r\n      # remove the batch dimension\r\n      predictions = tf.squeeze(predictions, 0)\r\n\r\n      # using a categorical distribution to predict the character returned by the model\r\n      predictions = predictions / temperature\r\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n\r\n      # We pass the predicted character as the next input to the model\r\n      # along with the previous hidden state\r\n      input_eval = tf.expand_dims([predicted_id], 0)\r\n\r\n      text_generated.append(idx2char[predicted_id])\r\n\r\n  return (start_string + ''.join(text_generated))\r\n```\r\n\r\nDo you have any suggestions on how I can speed this up? Or parallelize it by generating multiple examples at the same time? A quick look at cprofiler shows that 90% of the time is spent on the single line predictions = model(input_eval), so this is where we'd most likely find a speedup. Would appreciate any advice, and happy to submit a PR if I'm able to speed it up! \r\n\r\n**System information**\r\n- I am running the TensorFlow reference text generation example:  https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb\r\n- Tested on Debian and Google Colab (with GPU support)\r\n- TensorFlow installed from (source or binary): Binary\r\n-TensorFlow version: v2.1.0-rc2-17\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.243\r\n- GPU model and memory: NVidia Tesla T4\r\n\r\n**Describe the current behavior**\r\nText generation works fine, but feels slow. Using NVTOP it shows only 15% GPU utilization on average.\r\n\r\n**Describe the expected behavior**\r\nHoping to speed up text generation by better leveraging the GPU\r\n\r\n**Standalone code to reproduce the issue**\r\nThis issue can be replicated by running the standard TensorFlow text generation tutorial on Google Colaboratory with GPU\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\n![Screen Shot 2020-05-18 at 10 20 17 AM](https://user-images.githubusercontent.com/6510818/82244078-7f65aa00-98f5-11ea-95b0-87f1f5ab89fb.png)\r\n", "comments": ["@zredlined \r\nCan you please share simple stand alone code to replicate the issue or if possible share a colab gist for us to a analyse the error", "@Saduf2019 It is not an error, the code just does not efficiently leverage the GPU by default and I'm hoping to find some advice on speeding it up. You can run the colab here:\r\n\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb\r\n\r\nThe line I'm hoping to speed up is in the generate_text() function above: \r\n      predictions = model(input_eval)\r\n", "@jvishnuvardhan It seems to me the challenge is getting parallelization for the GPU, while maintaining statefulness of the LSTM to predict the next character in the sentence. \r\n\r\nPerhaps I can batch several lines to generate at once into the model.predict() while maintaining individual LSTM state per line in the batch? Or load multiple models as workers? Any suggestions or pseudocode would be much appreciated!", "Any suggestions here? It would be acceptable to generate multiple texts simultaneously to more effectively use the GPU. Any insights would be appreciated", "@zredlined Try batching several lines to generate at once into the model.predict() while maintaining individual LSTM state per line in the batch and let us know if it speeds up on no.", "@gowthamkpr thanks! I can't figure out how to maintain LSTM state per line in the batch. The model.predict() appears to just update a single LSTM state after processing each line in the batch. Any suggestions on how to do this?", "I'm working on this, for other reasons, but I'll try to fix this at the same time.\r\nIt may take a little while to land, but  wrapping that in a `tf.function`, and batching the inputs should give a good speedup.", "@zredlined \r\nCould you please check on tf 2,4,1 and let us know if you still face this issue.", "I got the `tf.function` implementation working in that tutorial.\r\n\r\nThe `tf.function` only runs one step at a time, so it's still not ideal. \r\n\r\nIn this commit I fixed NMT-with attention to `tf.function` compile the whole loop, with batched-inputs. That should be even faster.\r\n\r\nhttps://github.com/tensorflow/docs/commit/9e18593b7ee19058b651626d0e96a552ac7b4733\r\n\r\nThat commit got rolled-back because of 2.4/2.5 incompatibilities, but I'm planning to resubmit it as soon as tf 2.5 is released.\r\n"]}, {"number": 39653, "title": "[Tf 2.x] Can't return variables as tf.keras.Model outputs", "body": "**System information**\r\n- Reproduced in Colab, currently with Tensorflow 2.2\r\n- Reproduced in Debian Buster, Tensorflow 2.1, CPU, built from source.\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport tensorflow as tf\r\n\r\ninputs = tf.keras.Input([])\r\nvar = tf.Variable(3.0)\r\n\r\n# OK\r\ntf.keras.Model(inputs=inputs, outputs=[inputs*var])\r\n\r\n# AttributeError exception\r\ntf.keras.Model(inputs=inputs, outputs=[inputs*var,var])\r\n# The same happens with variants like `tf.identity(var)`\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo exception raised\r\n\r\n**Standalone code to reproduce the issue**\r\nCode [https://colab.research.google.com/drive/1EBXU51kVb6zrf1gA4iHcJInzkixrXk4l?usp=sharing](https://colab.research.google.com/drive/1EBXU51kVb6zrf1gA4iHcJInzkixrXk4l?usp=sharing)\r\n\r\n**Other info / logs**\r\nSame as: https://github.com/keras-team/keras/issues/12673, but that was on the wrong repo, I think.\r\n", "comments": ["I have tried in colab with TF version 2.1.0, 2.2.0, nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/13aaca4ee43e1d7ee23143e4cf8420db/untitled907.ipynb).Thanks!", "We do not allow setting Variables as model outputs. That said, you should be able to cast the Variable as a tensor using `tf.identity`, and the tensor can then serve as the output. \r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/identity", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39653\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39653\">No</a>\n", "Sorry, but it seems that it can't be used as output not even with `var.value()` or `tf.identity(var)`.\r\nTell me if I'm wrong."]}, {"number": 39652, "title": "Tensorflow containers are missing from Docker Hub", "body": "Your docker pages point at \r\n\r\nhttps://hub.docker.com/r/tensorflow/tensorflow \r\n\r\nToday that returns \"404\" Oops! Page not found.", "comments": ["It works fine now. I am guessing this was a temporary glitch.", "@pauljohn32,\r\nI am able to access the link you have posted. Could you please try again and check if it works. Thanks!", "Agree. Works now. I tried to access at exact moment the offerings were being replaced, I think."]}, {"number": 39651, "title": "Checkpoint is not work properly", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1909 Home\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\ntf.save_weights is not work properly\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/save_and_load#saved_model%EC%9D%84_%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0\r\n\r\nWhen I follow the tutorial of tensorflow checkpoint section, It does not make *.ckpt file but makes .ckpt folder!!!!\r\n\r\n**Describe the expected behavior**\r\nIt need make .ckpt files.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport cv2\r\nimport random\r\nimport os\r\n\r\n\r\n\r\n\r\nnp.random.seed(1)\r\nrandom.seed(1)\r\ntf.random.set_seed(1)\r\n\r\ncheckpoint_path = 'checkpoint\\\\cp-{epoch:04d}.ckpt'\r\ncheckpoint_dir = os.path.dirname(checkpoint_path)\r\n\r\n\r\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\r\n\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\n\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(512, activation='relu'),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dropout(0.02, seed=1),\r\n    tf.keras.layers.Dense(254, activation='relu'),\r\n    tf.keras.layers.Dropout(0.02, seed=1),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dropout(0.01, seed=1),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(32, activation='sigmoid'),\r\n    tf.keras.layers.Dense(10)\r\n])\r\n\r\n\r\n\r\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_path,\r\n    verbose=1,\r\n    save_weight_only=True,\r\n    period=2\r\n)\r\n\r\n\r\nmodel.save_weights(checkpoint_path.format(epoch=0))\r\n\r\nepochs = 10\r\n\r\nmodel.compile(\r\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n    metrics=['accuracy']\r\n)\r\n\r\n\r\nmodel.fit(\r\n    train_dataset.shuffle(int(len(train_images)+500), seed=1).batch(256),\r\n    epochs=epochs,\r\n    callbacks=[checkpoint_callback],\r\n    verbose=1\r\n)\r\n\r\n\r\nloss, acc = model.evaluate(test_images,  test_labels, verbose=2)\r\nprint(\"Untrained model, accuracy: {:5.2f}%\".format(100*acc))\r\n\r\n```", "comments": ["@shwoghk14 \r\nPlease share simple stand alone code for us to replicate the issue faced, or share a colab gist with the error for us to analyse the issue faced", "> @shwoghk14\r\n> Please share simple stand alone code for us to replicate the issue faced, or share a colab gist with the error for us to analyse the issue faced\r\n\r\n@Saduf2019 \r\n\r\nI added my code below that takes weird actions.\r\n\r\nThanks.", "@shwoghk14 \r\nI ran the code shared , please refer to [this gist](https://colab.sandbox.google.com/gist/Saduf2019/bcbf7c557ffd275a7a57fc3bbc5d65ba/untitled191.ipynb) and let us know if it confirms your issue", "> @shwoghk14\r\n> I ran the code shared , please refer to [this gist](https://colab.sandbox.google.com/gist/Saduf2019/bcbf7c557ffd275a7a57fc3bbc5d65ba/untitled191.ipynb) and let us know if it confirms your issue\r\n\r\n@Saduf2019 \r\nYes! If you see the /content where saving location of checkpoint, you can see the folders but not .ckpt files.\r\nIt's odds!\r\n\r\nWhy tensorflow makes folders? not files? \r\nIs it a bug? or my mistakes?", "@shwoghk14 I have changed your code very little and it is working as expected. Please verify once and let me know if this is what you are expecting.  Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "> @shwoghk14 I have changed your code very little and it is working as expected. Please verify once and let me know if this is what you are expecting. Thanks!\r\n> \r\n> Please close the issue if this was resolved for you. Thanks!\r\n\r\n@jvishnuvardhan Where can I see the code which was eidted?\r\nDo I need update my tensorflow module from github?\r\nThanks.", "@shwoghk14 Sorry, forgot to attach. [Here](https://colab.research.google.com/gist/jvishnuvardhan/a650c2493304930e9e3d5d43496ccdaa/39651.ipynb) is the gist. \r\n\r\nI ran with `TF2.1` and `tf-nightly`. You can run with `TF2.1`, don't need to change (but suggested as newer versions are has better performance). Thanks!", "> @shwoghk14 Sorry, forgot to attach. [Here](https://colab.research.google.com/gist/jvishnuvardhan/a650c2493304930e9e3d5d43496ccdaa/39651.ipynb) is the gist.\r\n> \r\n> I ran with `TF2.1` and `tf-nightly`. You can run with `TF2.1`, don't need to change (but suggested as newer versions are has better performance). Thanks!\r\n\r\n@jvishnuvardhan I ran the code on my computer but it occurs a error.\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: checkpoint/cp-0001.ckpt\\variables; No such file or directory\r\n```\r\n\r\nIt is why I instead use \\\\\\\\ but / in  checkpoint_path.\r\nAnyway I ran your code on gist and on my local computer(use \\\\\\\\ in path) not changed behaviors.\r\nIt makes folders not files.\r\n\r\nMy OS is Windows 10 not linux or else.\r\n![result](https://user-images.githubusercontent.com/48680511/83960791-d42e8c00-a8c7-11ea-93ef-a538ff48878c.jpg)\r\n", "I will check on my windows laptop and let you know the solution. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@jvishnuvardhan \r\nIs it not done? The tensorflow bot says this issue is closed soon.", "@shwoghk14 bot will not close this. I will check it on my windows laptop later today. Thanks!", "@shwoghk14 I ran it in my Windows10 laptop. It works without any error. Please check the log.\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!\r\n\r\n```\r\nC:\\Users\\XXXXX>python C:\\Users\\XXXXX\\Downloads\\GitHub_models\\checkpoints.py\r\n2020-07-07 17:25:22.093668: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-07-07 17:25:22.103596: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1dff5494650 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-07 17:25:22.109377: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-07 17:25:23.270684: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 376320000 exceeds 10% of free system memory.\r\n2020-07-07 17:25:23.662429: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 376320000 exceeds 10% of free system memory.\r\nEpoch 1/10\r\n234/235 [============================>.] - ETA: 0s - loss: 0.3782 - accuracy: 0.9316\r\nEpoch 00001: saving model to checkpoint/cp-0001.ckpt\r\n2020-07-07 17:25:29.053866: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From c:\\users\\XXXXX\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n235/235 [==============================] - 5s 21ms/step - loss: 0.3778 - accuracy: 0.9316\r\n2020-07-07 17:25:30.085141: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 376320000 exceeds 10% of free system memory.\r\nEpoch 2/10\r\n234/235 [============================>.] - ETA: 0s - loss: 0.1064 - accuracy: 0.9751\r\nEpoch 00002: saving model to checkpoint/cp-0002.ckpt\r\n235/235 [==============================] - 5s 21ms/step - loss: 0.1064 - accuracy: 0.9751\r\n2020-07-07 17:25:35.396674: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 376320000 exceeds 10% of free system memory.\r\nEpoch 3/10\r\n234/235 [============================>.] - ETA: 0s - loss: 0.0686 - accuracy: 0.9824\r\nEpoch 00003: saving model to checkpoint/cp-0003.ckpt\r\n235/235 [==============================] - 5s 21ms/step - loss: 0.0686 - accuracy: 0.9824\r\n2020-07-07 17:25:40.746641: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 376320000 exceeds 10% of free system memory.\r\nEpoch 4/10\r\n233/235 [============================>.] - ETA: 0s - loss: 0.0492 - accuracy: 0.9870\r\nEpoch 00004: saving model to checkpoint/cp-0004.ckpt\r\n235/235 [==============================] - 5s 21ms/step - loss: 0.0494 - accuracy: 0.9870\r\nEpoch 5/10\r\n234/235 [============================>.] - ETA: 0s - loss: 0.0373 - accuracy: 0.9898\r\nEpoch 00005: saving model to checkpoint/cp-0005.ckpt\r\n235/235 [==============================] - 5s 22ms/step - loss: 0.0374 - accuracy: 0.9898\r\nEpoch 6/10\r\n235/235 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9912\r\nEpoch 00006: saving model to checkpoint/cp-0006.ckpt\r\n235/235 [==============================] - 5s 23ms/step - loss: 0.0318 - accuracy: 0.9912\r\nEpoch 7/10\r\n234/235 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.9928\r\nEpoch 00007: saving model to checkpoint/cp-0007.ckpt\r\n235/235 [==============================] - 5s 22ms/step - loss: 0.0257 - accuracy: 0.9928\r\nEpoch 8/10\r\n234/235 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9936\r\nEpoch 00008: saving model to checkpoint/cp-0008.ckpt\r\n235/235 [==============================] - 5s 22ms/step - loss: 0.0238 - accuracy: 0.9937\r\nEpoch 9/10\r\n235/235 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9944\r\nEpoch 00009: saving model to checkpoint/cp-0009.ckpt\r\n235/235 [==============================] - 5s 21ms/step - loss: 0.0212 - accuracy: 0.9944\r\nEpoch 10/10\r\n233/235 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9951\r\nEpoch 00010: saving model to checkpoint/cp-0010.ckpt\r\n235/235 [==============================] - 5s 22ms/step - loss: 0.0181 - accuracy: 0.9951\r\n313/313 - 1s - loss: 0.0891 - accuracy: 0.9787\r\nUntrained model, accuracy: 97.87%\r\n```", "@janapativishnu Hello.\r\nThe error is not a point.\r\nMy issue is the abnormal behavior of checkpoint.\r\nI has known the checkpoint makes .ckpt file.\r\nBut my code makes .ckpt folder.\r\nThat is a point.", "@shwoghk14 It is expected. Code makes .ckpt folder and inside the folder there will be three things (`saved_model.pb`, and two folders `assets` and `variables`).\r\n\r\nI used this path (instead of your's)\r\n```\r\n#checkpoint_path = 'checkpoint\\\\cp-{epoch:04d}.ckpt'\r\ncheckpoint_path = '/checkpoint/cp-{epoch:04d}.ckpt'\r\ncheckpoint_dir = os.path.dirname(checkpoint_path)\r\n```\r\nPlease check the checkpoint directory.\r\n\r\n![GitHub](https://user-images.githubusercontent.com/15189960/86864306-c9f7dd00-c081-11ea-8b6f-eb0a4e502dc6.png)\r\n\r\n\r\n\r\n\r\n", "@shwoghk14 I just tried your path also. It works similar to my path shown in the last comment. Similarly, each of the .ckpt directory has one file and 2 folders. Thanks!", "@janapativishnu Yes, I also got that. but, I need a cp-0001.ckpt file, not cp-0001.ckpt folder.\r\nMake a folder is expected behavior?\r\n\r\nbut I doing this code which on below URL makes cp-0001.ckpt file.\r\nhttps://www.tensorflow.org/tutorials/keras/save_and_load#saved_model%EC%9D%84_%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0\r\n\r\nThanks.", "@janapativishnu I'm sorry. My mistakes to say cp-0001.ckpt file.\r\n\r\nThe tutorial makes cp-0005.ckpt.index, cp-0005.ckpt.data-00000-of-00002, cp-0005.ckpt.data-00001-of-00002, cp-0005.ckpt.index\r\n\r\nI want those files which above four.\r\nBut my code makes, you know, .ckpt folders. \r\nIt's weird.\r\n\r\nMy code is not different with tutorial code, but why It makes folder?\r\n\r\n\r\nthis is tutorial code.\r\n```\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nprint(tf.version.VERSION)\r\n\r\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\r\n\r\ntrain_labels = train_labels[:1000]\r\ntest_labels = test_labels[:1000]\r\n\r\ntrain_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\r\ntest_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0\r\n\r\n# Define a simple sequential model\r\ndef create_model():\r\n  model = tf.keras.models.Sequential([\r\n    keras.layers.Dense(512, activation='relu', input_shape=(784,)),\r\n    keras.layers.Dropout(0.2),\r\n    keras.layers.Dense(10)\r\n  ])\r\n\r\n  model.compile(optimizer='adam',\r\n                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                metrics=['accuracy'])\r\n\r\n  return model\r\n\r\n# Create a basic model instance\r\nmodel = create_model()\r\n\r\n# Display the model's architecture\r\nmodel.summary()\r\n\r\n# Include the epoch in the file name (uses `str.format`)\r\ncheckpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\r\ncheckpoint_dir = os.path.dirname(checkpoint_path)\r\n\r\n# Create a callback that saves the model's weights every 5 epochs\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_path,\r\n    verbose=1,\r\n    save_weights_only=True,\r\n    period=5)\r\n\r\n# Create a new model instance\r\nmodel = create_model()\r\n\r\n# Save the weights using the `checkpoint_path` format\r\nmodel.save_weights(checkpoint_path.format(epoch=0))\r\n\r\n# Train the model with the new callback\r\nmodel.fit(train_images,\r\n          train_labels,\r\n          epochs=50,\r\n          callbacks=[cp_callback],\r\n          validation_data=(test_images,test_labels),\r\n          verbose=0)\r\n\r\n# This may generate warnings related to saving the state of the optimizer.\r\n# These warnings (and similar warnings throughout this notebook)\r\n# are in place to discourage outdated usage, and can be ignored.\r\n\r\nmodel = create_model()\r\n\r\n# Evaluate the model\r\nloss, acc = model.evaluate(test_images,  test_labels, verbose=2)\r\nprint(\"Untrained model, accuracy: {:5.2f}%\".format(100*acc))\r\n\r\n```\r\n<img width=\"777\" alt=\"\uc8fc\uc11d 2020-07-08 111412\" src=\"https://user-images.githubusercontent.com/48680511/86866584-33bcc900-c10c-11ea-9dbd-f4e727ff2fb4.png\">\r\n\r\nThanks.", "Change save_weight_only=True, to **save_weights_only**=True (weights with s) in tf.keras.callbacks.ModelCheckpoint and it should create files instead of folders\r\n\r\nRef : https://abstack.pk/post/tensorflow-create-ckpt-files-instead-of-folders", "@abStack-pk  HI,\r\nI hadn't know that has missing letter 's'.\r\nThanks, My question is solved.\r\n\r\nBest regards.", "Closing this issue as this was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39651\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39651\">No</a>\n"]}, {"number": 39650, "title": "Allow single element padding_values to be broadcasted to a structure for tf.data.Dataset.padded_batch", "body": "This PR is a follow up based on comment in https://github.com/tensorflow/tensorflow/issues/35900#issuecomment-576969545\r\n\r\nIn #35900, the issue was raised where if padding_values is a single elment\r\nand the dataset has a structured shape, then tf.data.Dataset.padded_batch\r\nwill return and error.\r\n\r\nThis PR adds the support to \"broadcast\" padding_values to match the same\r\nstructure as the dataset, which could be convenient in many cases (avoid\r\nrepeat the padding_values into the stucture manually).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["Thanks @jsimsa. The PR has been updated with docstring expanded with a new code example. Please take a look and let me know if there are any issues.", "@jsimsa Thanks for the review. The PR has been updated with docstring adjusted to args part. Please take a look."]}]