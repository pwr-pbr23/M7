[{"number": 16141, "title": "FIX Typo", "body": "fix typo", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16140, "title": "Fix typo", "body": "fix typo", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16139, "title": "Segmentation fault when running optimization step with 3d convolution", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux (4.14.13-1 linux kernel version)\r\n- **TensorFlow installed from (source or binary)**: source (using the package here: https://www.archlinux.org/packages/community/x86_64/python-tensorflow-cuda/)\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.1\r\n- **CUDA/cuDNN version**: 9.1.85-1/7.0.5-2\r\n- **GPU model and memory**: NVidia Quadro K4200, 4028MiB\r\n- **Exact command to reproduce**: `python test.py`\r\nNote that the same code also fails in a Ubuntu docker container (Dockerfile attached).\r\n\r\n### Describe the problem\r\nI set up a computation graph with a 3d convolution. I can evaluate the result of this graph, but when I attempt to optimize the parameters of the graph (`train_step.run(feed_dict={x: sample, y_: label})`), tensorflow segfaults.\r\n\r\nIn a jupyterlab notebook running on Ubuntu, if I run the same code, the notebook hangs indefinitely at the same line. In both cases, the last line of the program is never run - \"ran train step\" is never printed.\r\n\r\nI also tried running this on my CPU with `os.environ['CUDA_VISIBLE_DEVICES'] = '-1'`. I get the same segfault.\r\n\r\nThe segfault goes away if I do any of the following:\r\n- Remove the 3d convolution\r\n- Reduce the input size significantly (e.g. 100x smaller to 1 x 41 x 96 x 128 x 1)\r\n- Reduce the kernel size significantly\r\n\r\n### Source code / logs\r\nMinimal example code (test.py):\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nsample = np.zeros((1, 41, 960, 1280, 1))\r\nlabel = np.zeros((1,))\r\n\r\nrc_kernel = np.ones((31,))\r\n\r\nx = tf.placeholder(tf.float64, shape=[None, 41, 960, 1280, 1])\r\ny_ = tf.placeholder(tf.float64, shape=[None])\r\n\r\nW_conv_r = tf.Variable(rc_kernel.reshape((1, -1, 1, 1, 1)))\r\nh_blur = tf.nn.conv3d(x, W_conv_r, [1, 1, 1, 1, 1], \"VALID\")\r\n\r\nh_sum = tf.reduce_sum(tf.reduce_sum(tf.reduce_sum(h_blur, axis=3), axis=2), axis=1)\r\ny = tf.sigmoid(h_sum)\r\n\r\nsq_err = (y - y_) ** 2\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(0.1).minimize(sq_err)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    E = sq_err.eval(feed_dict={x: sample, y_: label})\r\n    print(f'E = {E}')\r\n    train_step.run(feed_dict={x: sample, y_: label})  # fails here\r\n    print('ran train step')\r\n```\r\n\r\nDockerfile:\r\n[Dockerfile.txt](https://github.com/tensorflow/tensorflow/files/1633128/Dockerfile.txt)", "comments": ["This definitely seems to be a bug. It seems to cause an infinite loop in an Eigen routine when I trace it in gdb. The stack gets exhausted, thus causing the segmentation fault. If I set the 960 to 96 and 1280 to 128 (making the tensor smaller), it works fine. So it may be some sort of deeply recursive implementation that just gets intractable with big tensors. @mjanusz, since you implemented the Conv3D code, do you have any thoughts on this? \r\n", "@benoitsteiner, any other thoughts... here is a sampling of the eigen function stack entries that are so prevelant\r\n\r\n```\r\n#30817 0x00007fffd55898fb in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<double, long, Eigen::internal::TensorContractionSubMapper<double, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 2, true, false, 0, Eigen::MakePointer>, 4, 2, 0, false, false>, Eigen::internal::gemm_pack_rhs<double, long, Eigen::internal::TensorContractionSubMapper<double, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 2, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<double, double, long, Eigen::internal::blas_data_mapper<double, long, 0, 0>, 4, 4, false, false>, Eigen::internal::TensorContractionInputMapper<double, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 2, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<double, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 2, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<double, long, 0, 0> >::pack_lhs(long, long) ()\r\n   from /usr/local/google/home/aselle/tensorflow3/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#30818 0x00007fffd5588a49 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<double, long, Eigen::internal::TensorContractionSubMapper<double, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 2, true, false, 0, Eigen::MakePointer>, 4, 2, 0, false, false>, Eigen::internal::gemm_pack_rhs<double, long, Eigen::internal::TensorContractionSubMapper<double, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 2, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<double, double, long, Eigen::internal::blas_data_mapper<double, long, 0, 0>, 4, 4, false, false>, Eigen::internal::TensorContractionInputMapper<double, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 2, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<double, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 2, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<double, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool) ()\r\n   from /usr/local/google/home/aselle/tensorflow3/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\n```\r\n", "Could this bug be related to the following issue?\r\nhttps://github.com/tensorflow/tensorflow/issues/14807", "@amathuri looks related to me. I must have missed that when searching for a solution to this problem.\r\n\r\nIf it's helpful to anyone else, I did end up coming up with a workaround for my particular case. My architecture essentially has a separable 3D convolution, so I only need to convolve in one dimension at a time. My workaround was to use tf.nn.conv2d() instead, reshaping the z dimension into the batch dimension beforehand (so the shape is (depth, height, width, channels)). This lets me convolve in the x and y dimensions. Convolving in the z dimension is a little trickier, but I did it by swapping the x and z dimensions with tf.transpose(), convolving in x, then swapping the dimensions back.\r\n\r\nUnfortunately, this method prevents batching, so it may not be helpful for #14807. However, I may try something like this:\r\nhttps://stackoverflow.com/questions/45987156/tensorflow-average-gradients-over-several-batches\r\nto do the batching instead.", "Pinging @benoitsteiner", "@benoitsteiner , could you please take a look.", "I could replicate this issue in [2.8](https://colab.sandbox.google.com/gist/mohantym/f7b0f5294fcbf44bc0632ebddb897eb7/github_16139.ipynb#scrollTo=8cIAhnfez68P) version using compatibility mode.", "@mohantym did you reproduce using TF v2 APIs?", "Hi @mihaimaruseac ! I replicated this issue using compat.v1 mode .I have not tried in v2 APIs yet. ", "We no longer support v1 APIs.", "Hi @cericson ! 1.x versions are not supported any more . Can you please create a new issue with the 2.x code base?", "So I refactored the original code a bit (see below) to run it in tensorflow 2.8.0 with no issue. The bug may have been resolved already, or at least it doesn't resurface with the way I'm using the v2 API.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsample = np.zeros((1, 41, 960, 1280, 1))\r\nlabel = np.zeros((1,))\r\nkernel = tf.Variable(np.ones((1, 31, 1, 1, 1)))\r\n\r\nwith tf.GradientTape() as g:\r\n    h_blur = tf.nn.conv3d(sample, kernel, [1, 1, 1, 1, 1], \"VALID\")\r\n    h_sum = tf.reduce_sum(tf.reduce_sum(tf.reduce_sum(h_blur, axis=3), axis=2), axis=1)\r\n    y = tf.sigmoid(h_sum)\r\n    sq_err = (y - label) ** 2\r\n\r\ngrad = g.gradient(sq_err, kernel)\r\noptimizer = tf.optimizers.SGD()\r\noptimizer.apply_gradients([(grad, kernel)])\r\nprint('ran train step')\r\n```", "@cericson ! This issue is getting resolved with shape (1, 41, 96, 128, 1)  or (1, 41, 400, 800, 1) as you pointed out earlier but it is  replicating with the shape (1, 41, 960, 1280, 1). Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/f2201df9a33c41ac62ddf3bca8be4549/github_16139_2-8.ipynb#scrollTo=Fq_5wbxlch_m) for reference.", "FWIW I still can't replicate with tf2.8 on my cluster - it runs fine even with the shape (1, 41, 960, 1280, 1). I haven't used much colab before - could it just be running out of memory? I'm fine with closing this issue if that makes sense.\r\n![image](https://user-images.githubusercontent.com/6691014/162476625-3d5df436-0394-4568-aa1a-6741228214ad.png)\r\n", "Thanks @cericson for confirmation ! **It is not replicating in [GPU mode](https://colab.sandbox.google.com/gist/mohantym/f7b0f5294fcbf44bc0632ebddb897eb7/github_16139.ipynb#scrollTo=3UXCw2yM0DZX)**(Rather replicating in CPU mode)  as you pointed out. Closing this for now as it is not replicating in GPU mode. Feel free to re-open if you need further assistance.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16139\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16139\">No</a>\n"]}, {"number": 16138, "title": "Build fails with Visual Studio 2017", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.5.0-dev20180103\r\nhttps://github.com/tensorflow/tensorflow/commit/25d275280dfb163674f81c7681c2c1d34545a155\r\n- **Python version**: \r\n3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.0 with cuDNN 7\r\n- **GPU model and memory**:\r\nGeForce GTX 1060 6GB\r\n- **Exact command to reproduce**:\r\nOpen Visual Studio x64 Native Tools Command Prompt with admin rights. \r\nusing MSBuild 15.5.180.51428,\r\n\r\n```\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.12/swig.exe ^\r\n-DPYTHON_EXECUTABLE=C:\\Users\\csemp\\AppData\\Local\\Programs\\Python\\Python35\\python.exe ^\r\n-DPYTHON_LIBRARIES=C:\\Users\\csemp\\AppData\\Local\\Programs\\Python\\Python35\\libs\\python35.lib ^\r\n-Dtensorflow_ENABLE_GPU=ON ^\r\n-DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\" ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX ^\r\n-Dtensorflow_BUILD_CC_EXAMPLE=OFF\r\n\r\nset PreferredToolArchitecture=x64\r\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\Bin\\amd64\\MSBuild.exe\" /m:2 /p:Configuration=Release tf_python_build_pip_package.vcxproj /v:diag > diag.log\r\n```\r\n\r\n### Describe the problem\r\nThe build fails with the error \r\n```\r\n 133>CustomBuild: (TargetId:8893)\r\n                     CMake Error at tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Release.cmake:222 (message): (TaskId:3323)\r\n                       Error generating (TaskId:3323)\r\n                       C:/Users/csemp/dev/tensorflowbuild/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/Release/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj (TaskId:3323)\r\n                      (TaskId:3323)\r\n                      (TaskId:3323)\r\n```\r\nIt also has the error \r\n```\r\n(ClCompile target) -> \r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file C:\\Users\\csemp\\dev\\tensorflowbuild2\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc) [C:\\Users\\csemp\\dev\\tensorflowbuild2\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2100: illegal indirection (compiling source file C:\\Users\\csemp\\dev\\tensorflowbuild2\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc) [C:\\Users\\csemp\\dev\\tensorflowbuild2\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n```\r\nthe latter of which can be fixed by a simple change, as noted in https://github.com/tensorflow/tensorflow/issues/15925#issuecomment-356275963 .\r\nCreating this new issue per @gunan's request\r\nhttps://github.com/tensorflow/tensorflow/issues/14691#issuecomment-356846982\r\n\r\n### Source code / logs\r\nThe log file is way too big to post apparently. I'm open to suggestions\r\n\r\n", "comments": ["NVCC can fail because of:\r\n\r\n- Compiler not supported:\r\nUse older c++17/14 compiler; you can download it from visual studio installer and after that change the CUDA_HOST_COMPILER path (per CMake); \r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Professional/VC/Tools/MSVC/14.11.25503/bin as example\r\nSee: https://devtalk.nvidia.com/default/topic/1027299/cuda-9-failed-to-support-the-latest-visual-studio-2017-version-15-5/\r\n\r\n- whitespaces in include-path (shouldn't be your issue)\r\n\r\nDidn't look into what causes the Iterator error, but you can use use something like this instead of lower_bound\r\n\r\n```\r\nIterator::Iterator(SparseColumnIterable* iter, int64 example_idx)\r\n\t: iter_(iter), example_idx_(example_idx), end_(iter->ix_.dimension(0)) {\r\n\tIndicesRowIterator start = IndicesRowIterator(iter, 0);\r\n\tconst IndicesRowIterator end = IndicesRowIterator(iter, end_);\r\n\tsize_t count = std::distance(start, end);\r\n\r\n\twhile (0 < count)\r\n\t{\t// divide and conquer, find half that contains answer\r\n\t\tsize_t count2 = count >> 1; // TRANSITION, VSO#433486\r\n\t\tconst IndicesRowIterator _Mid = std::next(start, count2);\r\n\t\tif (_Mid.row_idx() < example_idx)\r\n\t\t{\t// try top half\r\n\t\t\tstart = _Next_iter(_Mid);\r\n\t\t\tcount -= count2 + 1;\r\n\t\t}\r\n\t\telse\r\n\t\t{\r\n\t\t\tcount = count2;\r\n\t\t}\r\n\t}\r\n\r\n\tcur_ = next_ = start.row_idx();\r\n\tUpdateNext();\r\n}\r\n```\r\n", "The Problem with lower_bound is solved, see #16048 and #15925", "@DrPingy have you been able to confirm that the fix https://github.com/tensorflow/tensorflow/issues/15925#issuecomment-356275963 doesn't mess up the build on Linux? If so, I can go ahead and create a PR", "We can certainly try running presubmits on a potential PR to see if the linux builds hold up.\r\nJust let me know when the PR is ready and I can quickly kick off the presubmits.", "@gunan followup to #14691", "@CarltonSemple : I'm working on Windows and cannot say anything about Linux build.\r\nBut the additional const is in current source.", "For a workaround on issue pertaining to the \"adjust_contrast_op_gpu.cu.cc\" error (and many more Eigen::half-related errors afterwards), do try out these steps: \r\n\r\n1. [Post-CMake] Make proper amendments to all targeted CUDA capabilities in \"..\\tensorflow\\contrib\\cmake\\CMakeLists.txt\" (fixing \"cuda_config.h\" after CMake as suggested in [here](https://github.com/tensorflow/tensorflow/issues/14691#issuecomment-357742568) won't ensure targeted CUDA capabilities get implemented because the content will eventually be overwritten with default settings amidst building. On the other hand, default settings won't produce the said error, at least not on my configurations (TF1.5.0-rc1, AVX2, CUDA9.1, CUDNN7, VS2017-v140). The error only cropped up when I add CUDA capability to include 6.1 (GTX1080).\r\n\r\n2. [Pre-MSBuild] Comment out \"#define EIGEN_HAS_CUDA_FP16\" in \"..\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc\" and \"..\\tensorflow\\contrib\\cmake\\build\\external\\eigen_archive\\Eigen\\Core\". Comment out conditions in \"..\\tensorflow\\contrib\\cmake\\build\\external\\eigen_archive\\Eigen\\src\\Core\\arch\\CUDA\\Half.h\" only when \"defined(EIGEN_HAS_CUDA_FP16)\" is absent (since we have undefined the macro earlier), effectively forcing the function to return the latter of two definitions. This would of course, render fp16 unusable. Then again for consumer grade GTX like mine, this yield no harm, owing to the fact that its fp16 performance is crippled.\r\n\r\nIn addition, there is a linkage issue pertaining to \"proto_text.vcxproj\" where it seems to be linking to the wrong \"libprotobuf.lib\". A quick remedy is to overwrite the content of \"..\\tensorflow\\contrib\\cmake\\build\\protobuf\\src\\protobuf\\Release\" with the one in \"..\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\protobuf\\Release\", granted \"-Dtensorflow_ENABLE_GRPC_SUPPORT=ON\" or else the correct \"libprotobuf.lib\" won't be available).", "@jstumpin: Can you clarify \"only when \"defined(EIGEN_HAS_CUDA_FP16)\" is absent\" with an example please? \r\nI'm not sure what I should comment.\r\nThanks!", "\"defined(EIGEN_HAS_CUDA_FP16)\" is present (no action needed):\r\n```\r\n#if defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_CUDA_ARCH) && EIGEN_CUDA_ARCH >= 300\r\n  __half tmp_ff = __float2half(ff);\r\n  return *(__half_raw*)&tmp_ff;\r\n#elif defined(EIGEN_HAS_FP16_C)\r\n  __half_raw h;\r\n  h.x = _cvtss_sh(ff, 0);\r\n  return h;\r\n#else\r\n```\r\n\r\n\"defined(EIGEN_HAS_CUDA_FP16)\" is absent:\r\n```\r\nEIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half exp(const half& a) {\r\n//#if EIGEN_CUDACC_VER >= 80000 && defined EIGEN_CUDA_ARCH && EIGEN_CUDA_ARCH >= 530\r\n//  return half(hexp(a));\r\n//#else\r\n   return half(::expf(float(a)));\r\n//#endif\r\n}\r\n```\r\n", "@jstumpin Much appreciated!", "The last time I checked, I was able to successfully build with windows 2017.\r\nCan anyone confirm, or report any issues if you saw any?", "Is this issue resolved?\r\nI think when I last checked the build worked fine on windows GPU, but just wanted to verify.", "I tried on window 10, visual studio 2017, GPU NVIDIA GTX 1070 but I got the following error\r\n\r\nBUILD FAILED\r\n\"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (1)->\r\nC:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj (32,3): error MSB4019:The imported project \"c:/Micorsoft.Cpp.Default.props\" \r\nwas not found. confirm that the path in the <import> declaration is correct, and that file exists on disk.", "Nagging Assignee @gunan: It has been 63 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 78 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Support for cmake is dropped, as it was proving to be too brittle.\r\nMoving forward, this is the documented way to build tensorflow on windows:\r\nhttps://www.tensorflow.org/install/install_sources_windows"]}, {"number": 16137, "title": "download_dependencies.sh fails for r1.5 and master.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.5\r\n- **Python version**:  3.6 - Anaconda\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.1 or 4.8.4 (tried both)\r\n- **CUDA/cuDNN version**: CUDA 8.0 and cuDNN 6.0.21 or cuDNN 5.0.5 (tried both)\r\n- **GPU model and memory**: 4x NVIDIA GTX Titan X 12GB\r\n- **Exact command to reproduce**: `sh ./tensorflow/contrib/makefile/download_dependencies.sh`\r\n\r\n### Describe the problem\r\nI have put together a script to build a distribution of TensorFlow for C++ development. Part of this script invokes `tensorflow/contrib/makefile/download_dependencies.sh`. However, this fails with the following output(truncated, error is the same for each subsequent download):\r\n\r\n`\r\ndownloading https://bitbucket.org/eigen/eigen/get/034b6c3e1017.tar.gz\r\n./tensorflow/contrib/makefile/download_dependencies.sh: 61: ./tensorflow/contrib/makefile/download_dependencies.sh: [\r\n[: not found\r\n./tensorflow/contrib/makefile/download_dependencies.sh: 63: ./tensorflow/contrib/makefile/download_dependencies.sh: [\r\n[: not found\r\ndownloading https://mirror.bazel.build/github.com/google/gemmlowp/archive/010bb3e71a26ca1d0884a167081d092b43563996.zi\r\np\r\n./tensorflow/contrib/makefile/download_dependencies.sh: 61: ./tensorflow/contrib/makefile/download_dependencies.sh: [\r\n[: not found\r\n./tensorflow/contrib/makefile/download_dependencies.sh: 63: ./tensorflow/contrib/makefile/download_dependencies.sh: [\r\n[: not found\r\ndownloading https://github.com/google/googletest/archive/release-1.8.0.tar.gz\r\n./tensorflow/contrib/makefile/download_dependencies.sh: 61: ./tensorflow/contrib/makefile/download_dependencies.sh: [\r\n[: not found\r\n./tensorflow/contrib/makefile/download_dependencies.sh: 63: ./tensorflow/contrib/makefile/download_dependencies.sh: [\r\n[: not found\r\ndownloading https://mirror.bazel.build/github.com/google/nsync/archive/8502189abfa44c249c01c2cad64e6ed660a9a668.tar.g\r\nz\r\n./tensorflow/contrib/makefile/download_dependencies.sh: 61: ./tensorflow/contrib/makefile/download_dependencies.sh: [\r\n[: not found\r\n./tensorflow/contrib/makefile/download_dependencies.sh: 63: ./tensorflow/contrib/makefile/download_dependencies.sh: [\r\n[: not found\r\ndownloading https://mirror.bazel.build/github.com/google/protobuf/archive/b04e5cba356212e4e8c66c61bbe0c3a20537c5b9.tar.gz\r\n`\r\nIt appears that maybe the download URL's are out of date. If this is indeed a bug, it should be reproducible on Linux with the attached script(which should be sufficiently commented.).\r\n\r\n### Source code / logs\r\nSee attached script.\r\n[build_tf.txt](https://github.com/tensorflow/tensorflow/files/1632802/build_tf.txt)\r\n\r\n", "comments": ["My apologies. This turned out to be a shell issue on the server I was using."]}, {"number": 16136, "title": "Error while using cuda-9.1, libcublas.so.8.0: cannot open shared object file: No such file or directory", "body": "Hi,\r\nI am having the import problem. I installed cuda 9.1 and set the path as suggested https://stackoverflow.com/questions/36159194/tensorflow-libcudart-so-7-5-cannot-open-shared-object-file-no-such-file-or-di\r\nShould i install cuda 8.0 for resolving this problem?\r\nIssue :tensorflow not supporting cuda version greater than 8.\r\n\r\nThe error is:\r\n\r\n            import tensorflow\r\n            Traceback (most recent call last):\r\n            File \"/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in\r\n            from tensorflow.python.pywrap_tensorflow_internal import *\r\n            File \"/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in\r\n            _pywrap_tensorflow_internal = swig_import_helper()\r\n            File \"/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n            _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n            File \"/home/honeypot/tensorflow/lib/python3.5/imp.py\", line 242, in load_module\r\n            return load_dynamic(name, filename, file)\r\n            File \"/home/honeypot/tensorflow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n            return _load(spec)\r\n            ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"\", line 1, in\r\nFile \"/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/init.py\", line 24, in\r\nfrom tensorflow.python import *\r\nFile \"/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/init.py\", line 49, in\r\nfrom tensorflow.python import pywrap_tensorflow\r\nFile \"/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in\r\nraise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\nFile \"/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in\r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in\r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"/home/honeypot/tensorflow/lib/python3.5/imp.py\", line 242, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"/home/honeypot/tensorflow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\nreturn _load(spec)\r\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nFailed to load the native TensorFlow runtime", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Where is the issue template?I have updated the comment and written the issue.", "@Prajwal1997 \r\nHe means\r\n> Have I written custom code\r\nOS Platform and Distribution\r\nTensorFlow installed from\r\nTensorFlow version\r\nBazel version\r\nCUDA/cuDNN version\r\nGPU model and memory\r\nExact command to reproduce\r\n\r\nTo solve this problem(ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory), you can try\r\n```\r\nsudo vim /etc/profile\r\nexport PATH=/usr/local/cuda/bin:$PATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\r\nsource /etc/profile\r\n```\r\nIf you haven't install cuda8, install it", "I have already added the relevant path as you mentioned and installed cuda 9. Does tensorflow support cuda 9? It looks like tensorflow doesn't support cuda 9!", "I have the same problems. I have added `/usr/local/cuda/lib64` to `$LD_LIBRARY_PATH`. The same import error message pops up when I `import tensorflow`.   \r\nAfter I make a soft link`sudo ln -s libcublas.so.9.0 libcublas.so.8.0`, Another ImportError message:\r\n>ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory   \r\n\r\nSo I try to make a softlink for all `*.9.0`\r\n```bash\r\nfor i in $(ls *9.0)\r\ndo\r\necho $i\r\nsudo ln -s $PWD/$i $PWD/${i/9.0/8.0}\r\ndone\r\n```\r\nAfter that , ImportError comes again\r\n>ImportError: libcudnn.so.6: cannot open shared object file: No such file or directory\r\n\r\nI solved the problem by the same way`sudo ln -s libcudnn.so.7 libcudnn.so.6`.  At last, I can import tensorflow without any error message. \r\n\r\nBut when I try to run the code below in hello.py:\r\n```python\r\nimport tensorflow as tf\r\n\r\nhello_constant = tf.constant('Hello World!')\r\n\r\nwith tf.Session() as sess:\r\n    output = sess.run(hello_constant)\r\n    print(output)\r\n```\r\nIt collapsed with the following message:\r\n```\r\n2018-01-17 17:01:19.184972: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-17 17:01:19.256650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-01-17 17:01:19.256927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 750 Ti major: 5 minor: 0 memoryClockRate(GHz): 1.0845\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1.95GiB freeMemory: 1.45GiB\r\n2018-01-17 17:01:19.256955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n*** stack smashing detected ***: <unknown> terminated\r\n[1]    31692 abort (core dumped)  python hello.py\r\n```\r\n\r\nI think tensorflow is not compatible with CUDA 9.\r\n\r\nSome helpful info:\r\nUbuntu 17.10, Python 2.7.14, conda 4.3.29, gcc (Ubuntu 6.4.0-8ubuntu1) 6.4.0 20171010, tensorflow-gpu==1.4.0\r\n```\r\n(tf) \u279c  ~ nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n```\r\nInstall cudnn by downloading cudnn-9.0-linux-x64-v7.tgz.\r\n\r\n\r\n \r\n\r\n\r\n\r\n\r\n", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "Yes, we have upgraded to `cuda9` at version `1.5.0` if memory serves. It looks like you have an issue with your CUDA installation. Please try to re-install.", "@drpngx \r\n```\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n```\r\nit can be solved .", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@bringtree yes, that works if it's there. Closing since it's an issue with the so location in most cases.", "yes the path was an issue and it was solved.Thanks anyways @drpngx ", "> @Prajwal1997\r\n> He means\r\n> \r\n> > Have I written custom code\r\n> > OS Platform and Distribution\r\n> > TensorFlow installed from\r\n> > TensorFlow version\r\n> > Bazel version\r\n> > CUDA/cuDNN version\r\n> > GPU model and memory\r\n> > Exact command to reproduce\r\n> \r\n> To solve this problem(ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory), you can try\r\n> \r\n> ```\r\n> sudo vim /etc/profile\r\n> export PATH=/usr/local/cuda/bin:$PATH\r\n> export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\r\n> source /etc/profile\r\n> ```\r\n> If you haven't install cuda8, install it\r\n\r\noh,it works~lol"]}, {"number": 16135, "title": "Distributed Tensorflow  using MPI", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\nI have tried stackflow and Google group discussion forum but could  get any reply or comment\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRedhat 7.4\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nfrom source with MPI\r\n- **TensorFlow version (use command below)**:\r\n1.41\r\n- **Python version**: \r\n2.7.14\r\n- **Bazel version (if compiling from source)**:\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC 6.0\r\n- **CUDA/cuDNN version**:\r\n8.0/6.5\r\n- **GPU model and memory**:\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K20Xm         Off  | 00000000:08:00.0 Off |                    0 |\r\n| N/A   34C    P0    61W / 235W |      0MiB /  5699MiB |     72%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI am using the following  script to launch distributed computing.\r\n\r\n\r\n#! /bin/bash\r\n\r\nmodule load openmpi/3.0.0-gnu\r\n\r\nhost=$(hostname -s)\r\nif [[ $host == \"node06\" ]]; then\r\n        echo \"statring Node 6\"\r\n        python tf_dis_2.py --job_name=\"ps\" --task_index=0\r\nelif [[ $host == \"node07\" ]]; then\r\n        echo \"starting Node 7 as worker\"\r\n        python tf_dis_2.py --job_name=\"worker\" --task_index=0\r\nelif [[ $host == \"node08\" ]]; then\r\n        echo \"starting Node 8 as worker\"\r\n        python tf_dis_2.py --job_name=\"worker\" --task_index=1\r\nfi\r\n\r\n-----\r\n\r\nI am running it on slurm  with three nodes.\r\n\r\nsrun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh\r\n\r\nI am using MPI instead of GPRC.\r\n\r\nI am getting the following message:\r\n\r\n---------------------------------------------------\r\nsrun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh\r\nstatring Node 6\r\nstarting Node 8 as worker\r\nstarting Node 7 as worker\r\n2018-01-15 11:34:59.961617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 5.57GiB freeMemory: 5.49GiB\r\n2018-01-15 11:34:59.961674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)\r\nE0115 11:35:00.020327488   36133 ev_epoll1_linux.c:1051]     grpc epoll fd: 22\r\n2018-01-15 11:35:00.026716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}\r\n2018-01-15 11:35:00.026760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> localhost:2224}\r\n2018-01-15 11:35:00.029261: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2224\r\n2018-01-15 11:35:00.439045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 5.57GiB freeMemory: 5.49GiB\r\n2018-01-15 11:35:00.439124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)\r\nE0115 11:35:00.497022377   13701 ev_epoll1_linux.c:1051]     grpc epoll fd: 22\r\n2018-01-15 11:35:00.503585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2018-01-15 11:35:00.503622: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> node08:2224}\r\n2018-01-15 11:35:00.505803: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\r\n2018-01-15 11:33:39.681311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 5.57GiB freeMemory: 5.49GiB\r\n2018-01-15 11:33:39.681375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)\r\nE0115 11:33:39.739196190   46236 ev_epoll1_linux.c:1051]     grpc epoll fd: 22\r\n2018-01-15 11:33:39.745655: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}\r\n2018-01-15 11:33:39.745697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> node08:2224}\r\n2018-01-15 11:33:39.747692: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\r\nAbid Malik\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nVariables initialized ...\r\nTraceback (most recent call last):\r\n  File \"tf_dis_2.py\", line 102, in <module>\r\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=\"/tmp/train_logs\",global_step=global_step,init_op=init_op)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 336, in __init__\r\n    self._verify_setup()\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 885, in _verify_setup\r\n    \"their device set: %s\" % op)\r\nValueError: When using replicas, all Variables must have their device set: name: \"weights/Variable\"\r\nop: \"VariableV2\"\r\nattr {\r\n  key: \"container\"\r\n  value {\r\n    s: \"\"\r\n  }\r\n}\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"shape\"\r\n  value {\r\n    shape {\r\n      dim {\r\n        size: 784\r\n      }\r\n      dim {\r\n        size: 100\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"shared_name\"\r\n  value {\r\n    s: \"\"\r\n  }\r\n}\r\n\r\n2018-01-15 11:33:41.719083: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: Endpoint read failed\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nVariables initialized ...\r\nTraceback (most recent call last):\r\n  File \"tf_dis_2.py\", line 114, in <module>\r\n    with sv.prepare_or_wait_for_session(server.target) as sess:\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 708, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 273, in prepare_session\r\n    config=config)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 205, in _restore_checkpoint\r\n    saver.restore(sess, ckpt.model_checkpoint_path)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1666, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: Endpoint read failed\r\nsrun: error: node08: task 2: Exited with exit code 1\r\nsrun: error: node07: task 1: Exited with exit code 1\r\n---------------------------------------------------------------------------------\r\n\r\nWhy is it crashing? I have been trying to solve this for the last three weeks by putting it on different forums and groups. However, could not get any reply. I would be grateful if someone can guide me. I apologize in advance if this is not the right forum.\r\n\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\n``\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport sys\r\nimport time\r\n\r\n\r\nprint(\"Abid Malik\")\r\n\r\n\r\nparameter_servers = [\"node06:2222\"]\r\nworkers = [\"node07:2223\",\"node08:2224\"]\r\ncluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\r\n\r\n\r\n\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n\r\n\r\n\r\n\r\nserver = tf.train.Server(\r\n    cluster,\r\n    job_name=FLAGS.job_name,\r\n    task_index=FLAGS.task_index)\r\n\r\n\r\nbatch_size = 100\r\nlearning_rate = 0.0005\r\ntraining_epochs = 20\r\nlogs_path = \"/tmp/mnist/1\"\r\n\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\nif FLAGS.job_name == \"ps\":\r\n    server.join()\r\nelif FLAGS.job_name == \"worker\":\r\n\r\n        with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,cluster=cluster)):\r\n              \r\n                global_step = tf.get_variable('global_step',[],initializer = tf.constant_initializer(0), trainable = False)\r\n\r\n              \r\n        with tf.name_scope('input'):\r\n              \r\n                  x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\r\n               \r\n                  y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\r\n\r\n                \r\n        tf.set_random_seed(1)\r\n        with tf.name_scope(\"weights\"):\r\n                        W1 = tf.Variable(tf.random_normal([784, 100]))\r\n                        W2 = tf.Variable(tf.random_normal([100, 10]))\r\n\r\n               \r\n        with tf.name_scope(\"biases\"):\r\n                        b1 = tf.Variable(tf.zeros([100]))\r\n                        b2 = tf.Variable(tf.zeros([10]))\r\n\r\n               \r\n        with tf.name_scope(\"softmax\"):\r\n                        # y is our prediction\r\n                        z2 = tf.add(tf.matmul(x,W1),b1)\r\n                        a2 = tf.nn.sigmoid(z2)\r\n                        z3 = tf.add(tf.matmul(a2,W2),b2)\r\n                        y  = tf.nn.softmax(z3)\r\n\r\n               \r\n        with tf.name_scope('cross_entropy'):\r\n                        # this is our cost\r\n                        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n\r\n             \r\n        with tf.name_scope('train'):\r\n                       \r\n                                                                                                                                                                                                                                                                \r\n\r\ngrad_op = tf.train.GradientDescentOptimizer(learning_rate)\r\n                        train_op = grad_op.minimize(cross_entropy, global_step=global_step)\r\n\r\n\r\n        with tf.name_scope('Accuracy'):\r\n                        # accuracy\r\n                        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\n                        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n   \r\n        tf.summary.scalar(\"cost\", cross_entropy)\r\n        tf.summary.scalar(\"accuracy\", accuracy)\r\n\r\n        saver = tf.train.Saver()\r\n       \r\n        summary_op = tf.summary.merge_all()\r\n        init_op = tf.global_variables_initializer()\r\n        print(\"Variables initialized ...\")\r\n\r\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=\"/tmp/train_logs\",global_step=global_step,init_op=init_op)\r\n\r\n\r\n        begin_time = time.time()\r\n        frequency = 100\r\n        with sv.prepare_or_wait_for_session(server.target) as sess:\r\n                # create log writer object (this will log on every machine)\r\n                writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\r\n\r\n                # perform training cycles\r\n                start_time = time.time()\r\n                for epoch in range(training_epochs):\r\n\r\n                        # number of batches in one epoch\r\n                        batch_count = int(mnist.train.num_examples/batch_size)\r\n\r\n                        count = 0\r\n                        for i in range(batch_count):\r\n                                batch_x, batch_y = mnist.train.next_batch(batch_size)\r\n\r\n                                # perform the operations we defined earlier on batch\r\n                                _, cost, summary, step = sess.run([train_op, cross_entropy, summary_op, global_step], feed_dict={x: batch_x, y_: batch_y})\r\n                                writer.add_summary(summary, step)\r\n\r\n                                count += 1\r\n                                if count % frequency == 0 or i+1 == batch_count:\r\n                                        elapsed_time = time.time() - start_time\r\n                                        start_time = time.time()\r\n                                        print(\"Step: %d,\" % (step+1),\r\n                                                                \" Epoch: %2d,\" % (epoch+1),\r\n                                                                \" Batch: %3d of %3d,\" % (i+1, batch_count),\r\n                                                                \" Cost: %.4f,\" % cost,\r\n                                                                \" AvgTime: %3.2fms\" % float(elapsed_time*1000/frequency))\r\n                                        count = 0\r\n\r\n\r\n                print(\"Test-Accuracy: %2.2f\" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\r\n                print(\"Total Time: %3.2fs\" % float(time.time() - begin_time))\r\n                print(\"Final Cost: %.4f\" % cost)\r\n\r\n        sv.stop()\r\n        print(\"done\")\r\n                                                                                                                                                                                                                                                                 \r\n\r\n``", "comments": ["@mrry, can you give any advice. I've never used MPI and don't know who's maintaining this aspect. It seems like the error is saying the model needs to be modified so that variable names match so replication can work properly.", "MPI support is supplled via contrib library:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/mpi\r\nThe contact is @jbedorf ", "Actually there is nothing MPI specific here. As the protocol is not specified in the ` tf.train.Server` call it will just take the default `grpc` implementation. ", "This error seems relevant:\r\n\r\n> ValueError: When using replicas, all Variables must have their device set: name: \"weights/Variable\"\r\n\r\nIt looks like the scope of the `with tf.device(...):` block, defined on this line:\r\n\r\n```python\r\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,cluster=cluster)):\r\n```\r\n\r\n...does not contain all of the variable creations. The `tf.train.Supervisor` code requires that an explicit device be set for each variable in your model. To resolve the problem, ensure that all of the `tf.Variable()` calls in your program are inside the scope of the `with tf.device(...):` block.", "Is there a distributed tensorflow demo with mpi\uff1f", "    with sv.prepare_or_wait_for_session(server.target) as sess:\r\n            # create log writer object (this will log on every machine)\r\n            writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\r\n\r\nyou can try to delete the config :tf.get_default_graph()"]}, {"number": 16134, "title": "Added training parameter to batch_normalization", "body": "According to the docs the batch_normalization layer does not work properly if the parameter is not set correctly.", "comments": ["Can one of the admins verify this patch?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@PiotrCzapla could you address my comments so we can merge this?", "@rmlarsen thanks for making me fix this style issues. It make sense to keep the examples clean.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16133, "title": "Add training parameter to dropout to make it work", "body": "I think that without this parameter set dropout is disabled all the time. At least this is what I read in the documentation, besides adding this improves training.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed the development agreement.", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@PiotrCzapla any update?", "@rmlarsen, I've removed the redundant if statement around dropout, and added parenthesis as requested. Maybe we can squash this two commits?", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@PiotrCzapla please fix the linter errors: https://source.cloud.google.com/results/invocations/2b1cc07a-4044-4ef2-a26e-a5f31a80316d/log"]}, {"number": 16132, "title": "Bug while printing parameters and gradients", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (anaconda)\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: using CPU\r\n- **GPU model and memory**: using CPU\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nThe model is very simple, I do digits classification with MNIST. There is only one parameter matrix W, no bias and no non-linearities. The model show convergence since the loss is decreasing. I checked predictions and accuracy but I do not copy paste useless code here. If I print the parameters before and after training they are the same, however, it shouldn't be the case. Moreover, the gradient of the loss w.r.t. parameters are zero but again it shouldn't be the case since the model converges so there should be a non-zero gradient. I cannot explain why and my implementation seems correct, that's why I am posting my code here.\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.set_random_seed(42)\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nmnist = input_data.read_data_sets('data/', one_hot=True)\r\n\r\nx = tf.placeholder(tf.float32, shape=(None, 784))\r\ny = tf.placeholder(tf.float32, shape=(None, 10))\r\n\r\nW = tf.get_variable('W0', (784, 10))\r\npred = tf.matmul(x, W)\r\nloss = tf.reduce_sum((y - pred) ** 2)\r\ngrads = tf.gradients(loss, W)\r\ntrain_step = tf.train.AdamOptimizer().minimize(loss)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nprint(sess.run(W))\r\n\r\n>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605\r\n  -0.01042821]\r\n [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129\r\n  -0.0796528 ]\r\n [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736\r\n  -0.04312544]\r\n ...\r\n [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174\r\n   0.06578781]\r\n [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339\r\n  -0.02337921]\r\n [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324\r\n  -0.02436799]]\r\n\r\nfor _ in range(1000):\r\n    x_mb, y_mb = mnist.train.next_batch(32)\r\n    loss_, _ = sess.run([loss, train_step], {x: x_mb, y: y_mb})\r\n    print('loss: {:2.5}'.format(loss_))\r\n\r\n>>> I won't print uselss log here but the loss is decreasing\r\n\r\nprint(sess.run(W))\r\n\r\n>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605\r\n  -0.01042821]\r\n [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129\r\n  -0.0796528 ]\r\n [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736\r\n  -0.04312544]\r\n ...\r\n [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174\r\n   0.06578781]\r\n [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339\r\n  -0.02337921]\r\n [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324\r\n  -0.02436799]]\r\n\r\nprint(sess.run(grads, {x: x_mb, y: y_mb}))\r\n\r\n>>> [array([[0., 0., 0., ..., 0., 0., 0.],\r\n       [0., 0., 0., ..., 0., 0., 0.],\r\n       [0., 0., 0., ..., 0., 0., 0.],\r\n       ...,\r\n       [0., 0., 0., ..., 0., 0., 0.],\r\n       [0., 0., 0., ..., 0., 0., 0.],\r\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]\r\n\r\n```\r\n", "comments": ["This sort of question is better asked on stackoverflow.\r\n\r\nIt appears that your W values are changing, but not in the small regions shown by the abbreviated python output.    Try\r\n     sum_w = tf.reduce_sum(W)\r\n     ...\r\n     print(sess.run(sum_w))\r\n"]}, {"number": 16131, "title": "Update contrib/HVX readme", "body": "I updated the README because of some imprecisions and to add clarifications of what I think will guide the users more appropriately.\r\n\r\nFirst, the very simple \"quick start guide\" doesn't work, there's no \"-X\" option (at least publicly) and so you always need to have the SDK installed manually.\r\n\r\nApart from that, some clarifications and rewording were done to help the users understand what's happening.\r\n\r\n/cc @satok16 ", "comments": ["Can one of the admins verify this patch?", "@petewarden Could you please take a look? "]}, {"number": 16130, "title": "Fix broken python3 build", "body": "Currently building tensorflow master branch with python3 fails with following error message.\r\n```\r\nERROR: ${BAZEL_CACHE}/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/code_gen.py failed (Exit 1).\r\n```\r\nIt seems that the 3 newly added `third_party/*.BUILD` scripts from https://github.com/tensorflow/tensorflow/pull/15955/commits/4080654c8f03ec34f2822c14db5fd8b75f63d569 are missing `srcs_version = \"PY2AND3\"` part, which all the other py_library modules have.\r\n\r\nI'm using bazel 0.5.4 on linux ubuntu 16.04 to build the current master branch.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 16129, "title": "allow 'None' as batch size for TimeFreqLSTMCell", "body": "Currently it is not allowed to have a variable batch size in TimeFreqLSTMCell, as the size is casted to an int internally.\r\nThis patch fixes this by omitting the int cast.\r\n\r\nTested it in an audio event detection framework without problems.", "comments": ["Can one of the admins verify this patch?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@progwolff please rebase to resolve the conflicts.", "@rmlarsen done.\r\n\r\n@ebrevdo Isn't `array_ops.shape` right here?", "@ebrevdo using array_ops.shape seems fine to me. What did you have in mind?", "\"x.shape[0].value or array_ops.shape(..)\"\n\npreserves more static shape info than just \"array_ops.shape(...)\"\n\nOn Wed, Feb 7, 2018 at 2:54 PM, Rasmus Munk Larsen <notifications@github.com\n> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> using array_ops.shape seems fine to\n> me. What did you have in mind?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16129#issuecomment-363939171>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzbAgndvVaiKkK0ZwqaQcS0rzh02ks5tSimngaJpZM4ReNU->\n> .\n>\n", "@ebrevdo Yes, we do want to capture that. I initially tought that ```x.shape[0].value or array_ops.shape(..)``` was pseudocode, but I guess the (ugly) way Python handles None with binary logical operators makes this work. Sigh.", "yes; it'll use dynamic shape if the batch size is statically 0; but that's\nan exceedingly rare case so we do this for the succinctness :)\n\nOn Wed, Feb 7, 2018 at 3:10 PM, Rasmus Munk Larsen <notifications@github.com\n> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Yes, we do want to capture that. I\n> initially tought that x.shape[0].value or array_ops.shape(..) was\n> pseudocode, but I guess the (ugly) way Python handles None with binary\n> logical operators makes this work. Sigh.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16129#issuecomment-363942789>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimynV9NtP6cw7LuZf_BG-eBvryOfBks5tSi1IgaJpZM4ReNU->\n> .\n>\n", "The requested change is implemented. Good catch!\r\n\r\nMany thanks to both of you!", "Fixed linter error."]}, {"number": 16128, "title": "Improvement Proposal - tf.alphas_like (merging tf.ones_like and tf.zeros_like into one)", "body": "Hello dear tensorflowers,\r\n\r\nIn a research project, I encountered the need to create tensors of the same shape than any other tensor with a custom value (not just 0 or 1), could be Boolean, Floats, Integers and so on.\r\n\r\nThe functions prototypes are the following and will be implemented in [ tensorflow/python/ops/array_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py): \r\n- **tf.alphas** (shape, alpha_value, name=None)\r\n- **tf.alphas_like** (tensor, alpha_value, name=None, optimize=True)\r\n\r\nThe code is not created from scratch. It is **highly** inspired by the functions tf.ones, tf.zeros for tf.alphas and by tf.zeros_like, tf.ones_like for tf.alphas_like.\r\n\r\nThe code use the latest implementation and has been designed to work with *eager_mode*.\r\nThe number of modification is relatively small, thus I am relatively confident on the robustness of the new implementation (largely based on the existing one).\r\n\r\nThe idea is to reproduce and merge the functions while enabling to set any custom value in the tensor:\r\n\r\n- **tf.alphas merges:**\r\n  - tf.ones\r\n  - tf.zeros\r\n- **tf.alphas_like merges:**\r\n  - tf.zeros_like\r\n  - tf.ones_like\r\n\r\n### How is the API Working ?\r\n\r\nMy new functions take a parameter _alpha_value_ and fill the tensor with this value. This allows me to run such a script:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.constant([\r\n    [\r\n        [4, 5, 6],\r\n        [1, 2, 3]\r\n    ],\r\n    [\r\n        [4, 5, 6],\r\n        [1, 2, 3]\r\n    ]\r\n])\r\n\r\nb1 = tf.alphas_like(a, 0.5431)\r\nb2 = tf.alphas_like(a, 5)\r\nb3 = tf.alphas_like(a, -5)\r\nb4 = tf.alphas_like(a, True)\r\n\r\nwith tf.Session() as sess:\r\n    _b1, _b2, _b3, _b4 = sess.run([b1, b2, b3, b4])\r\n    \r\nprint(\"b1:\", _b1)\r\nprint(\"b2:\", _b2)\r\nprint(\"b3:\", _b3)\r\nprint(\"b4:\", _b4)\r\n\r\n############### OUTPUTS ###############\r\n\r\n>>> b1: [\r\n  [\r\n    [ 0.5431  0.5431  0.5431]\r\n    [ 0.5431  0.5431  0.5431]\r\n  ]\r\n  [\r\n    [ 0.5431  0.5431  0.5431]\r\n    [ 0.5431  0.5431  0.5431]\r\n  ]\r\n]\r\n\r\n>>> b2: [\r\n  [\r\n    [5 5 5]\r\n    [5 5 5]\r\n  ]\r\n  [\r\n    [5 5 5]\r\n    [5 5 5]\r\n  ]\r\n]\r\n\r\n>>> b3: [\r\n  [\r\n    [-5 -5 -5]\r\n    [-5 -5 -5]\r\n  ]\r\n  [\r\n    [-5 -5 -5]\r\n    [-5 -5 -5]\r\n  ]\r\n]\r\n\r\n>>> b4: [\r\n  [\r\n    [ True  True  True]\r\n    [ True  True  True]\r\n  ]\r\n  [\r\n    [ True  True  True]\r\n    [ True  True  True]\r\n  ]\r\n]\r\n```\r\n\r\n### How can you help ?\r\n\r\nBefore submitting a PR, I would like to know a few things:\r\n  - Is it something that would be any kind of interest and worth a PR?\r\n  - Are the names I have chosen (tf.alphas and tf.alphas_like) okay with everyone ?\r\n  - In my PR, should I delete the implementation of tf.zeros_likes and tf.ones_likes and replace them as an alias of my new function which basically does the same job, just in a more flexible way ?\r\n\r\nThanks for your time and attention,\r\n\r\nBest Regards,\r\n\r\nJonathan", "comments": ["Sounds good, however those workarounds might work well:\r\n```python\r\nb1 = tf.ones_like(a, dtype=tf.float32) * 0.5431\r\nb2 = tf.ones_like(a, dtype=tf.int32) * 5\r\nb4 = tf.ones_like(a, dtype=tf.bool)\r\n```", "To be honest, I didn't tought about doing this... Shame on me ...\r\n\r\nOn the other hand, it's quite illogical to have the same functionality (ones_like and zeros_like) in two different functions to do basically the same operation that could be wrapped into one.\r\n\r\nThis will ease the code-base maintainance effort, only maintaining one function instead of two.\r\n\r\nYou tell me, if it's worth the effort cleaning my work and writing the unittest, I will submit the PR", "After your remark @facaiy, I tried to run a simple benchmark test to measure any difference in speed,\r\nhere is what I find.\r\n\r\nEach run has been executed 5 times and execution time averaged:\r\n- The method working today (by @facaiy) : [47.5s, 47.5s, 48.1s, 47.6s; 47.8s] => **Average Time: 47.7 secs**\r\n\r\n- The method I implemented \"alphas_like\": [25.0s, 25.0s, 25.0s, 24.9s, 25.0s] => **Average Time: 25 secs**\r\n\r\n**My method is almost twice as fast !**\r\n\r\n### Code used to produce this numbers:\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport time\r\n\r\na  = tf.ones(shape=(64,255,255,3))\r\n\r\nmethod1 = tf.ones_like(a) * 2.55\r\nmethod2 = tf.alphas_like(a, alpha_value=2.55)\r\n\r\n\r\nfor _ in range(5):\r\n    TIC = time.time()\r\n    for __ in range(100):\r\n\r\n        with tf.Session() as sess:\r\n            tmp = sess.run(method1)\r\n\r\n    TOC = time.time()\r\n\r\n    print(\"Execution with method 1 took: %sms\" % str((TOC-TIC)*1000))\r\n\r\nprint(\"\\n######################################\\n\")\r\n\r\nfor _ in range(5):\r\n    TIC = time.time()\r\n    for __ in range(100):\r\n\r\n        with tf.Session() as sess:\r\n            tmp = sess.run(method2)\r\n\r\n    TOC = time.time()\r\n\r\n    print(\"Execution with method 2 took: %sms\" % str((TOC-TIC)*1000))\r\n```\r\n\r\n### Which gives me these results:\r\n\r\n```\r\nExecution with method 1 took: 47542.79899597168ms\r\nExecution with method 1 took: 47539.61968421936ms\r\nExecution with method 1 took: 48089.28418159485ms\r\nExecution with method 1 took: 47630.205392837524ms\r\nExecution with method 1 took: 47837.78142929077ms\r\n\r\n######################################\r\n\r\nExecution with method 2 took: 25034.541845321655ms\r\nExecution with method 2 took: 25057.311058044434ms\r\nExecution with method 2 took: 25051.66268348694ms\r\nExecution with method 2 took: 24936.548471450806ms\r\nExecution with method 2 took: 24955.832958221436ms\r\n```\r\n", "Yes, do you try tf.fill? It should be more efficient, I think. ", "It's the underlying implementation of these methods", "@martinwicke would this be a welcome contribution?", "`ones_like` and `zeros_like` come from the eponymous [numpy](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.zeros_like.html) functions, and we will not be able to get rid of them, even if we have alphas_like (though of course they can share implementation).\r\n\r\nGiven the simple workarounds (in particular, `tf.fill`), and the fact that these rarely sit in performance critical path, how important is this? I'd like to avoid adding stuff we'll have to maintain.", "Up to me, we have Code duplication in the functions the functions: tf.zeros/ones and tf.zeros_like/ones.like.\n\nMore than just allowing a more generic behavior, I would think it is of good practice to have one single set of functions that maintains the core logic, the other ones would be some sort of proxies to maintain backward compatibility and numpy similarity.\n\nIt would be a lot more easy to maintain only one function that two. And in the execution or the user, it doesn't change anything.", "Sure. Go for it.", "Closing as this is resolved\r\n\r\n\r\n\r\n"]}, {"number": 16127, "title": "fix default parameters for TimeFreqLSTMCell, fixes #16100", "body": "Resolve #16100 \r\n\r\nThe default parameters for TimeFreqLSTMCell lead to a division by `None`, which throws an exception.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Signed the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "The fix looks good. +1 to merge"]}, {"number": 16126, "title": "Error \tLNK1181\tcannot open input file 'To.obj' ConsoleApplication1", "body": "The tensorflow Library built in \r\nwindows 10; Visual Studio 2015(Update 3);Python 3.5, \r\nwith CMake, \r\nnow I try to run an example and I get this error, \r\nI don't find any To.obj in my Tensorflow build folder.\r\nthis is my code (with name: ConsoleApplication1): \r\n`// matmul.cpp\r\n#define _ITERATOR_DEBUG_LEVEL 0  \r\n\r\n#include <vector>\r\n#include <eigen/Dense>\r\n\r\n#include \"matmul.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n\r\n\r\nusing namespace tensorflow;\r\n\r\n// Build a computation graph that takes a tensor of shape [?, 2] and\r\n// multiplies it by a hard-coded matrix.\r\nGraphDef CreateGraphDef()\r\n{\r\n\tScope root = Scope::NewRootScope();\r\n\r\n\tauto X = ops::Placeholder(root.WithOpName(\"x\"), DT_FLOAT,\r\n\t\tops::Placeholder::Shape({ -1, 2 }));\r\n\tauto A = ops::Const(root, { { 3.f, 2.f },{ -1.f, 0.f } });\r\n\r\n\tauto Y = ops::MatMul(root.WithOpName(\"y\"), A, X,\r\n\t\tops::MatMul::TransposeB(true));\r\n\r\n\tGraphDef def;\r\n\tTF_CHECK_OK(root.ToGraphDef(&def));\r\n\r\n\treturn def;\r\n}\r\n\r\nint main()\r\n{\r\n\tGraphDef graph_def = CreateGraphDef();\r\n\r\n\t// Start up the session\r\n\tSessionOptions options;\r\n\tstd::unique_ptr<Session> session(NewSession(options));\r\n\tTF_CHECK_OK(session->Create(graph_def));\r\n\r\n\t// Define some data.  This needs to be converted to an Eigen Tensor to be\r\n\t// fed into the placeholder.  Note that this will be broken up into two\r\n\t// separate vectors of length 2: [1, 2] and [3, 4], which will separately\r\n\t// be multiplied by the matrix.\r\n\tstd::vector<float> data = { 1, 2, 3, 4 };\r\n\tauto mapped_X_ = Eigen::TensorMap<Eigen::Tensor<float, 2, Eigen::RowMajor>>\r\n\t\t(&data[0], 2, 2);\r\n\tauto eigen_X_ = Eigen::Tensor<float, 2, Eigen::RowMajor>(mapped_X_);\r\n\r\n\tTensor X_(DT_FLOAT, TensorShape({ 2, 2 }));\r\n\tX_.tensor<float, 2>() = eigen_X_;\r\n\r\n\tstd::vector<Tensor> outputs;\r\n\tTF_CHECK_OK(session->Run({ { \"x\", X_ } }, { \"y\" }, {}, &outputs));\r\n\r\n\t// Get the result and print it out\r\n\tTensor Y_ = outputs[0];\r\n\tstd::cout << Y_.tensor<float, 2>() << std::endl;\r\n\r\n\tsession->Close();\r\n}`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 16125, "title": "Disable stacktrace_handler_test becase stack trace isn't generated on Windows", "body": "Fix http://ci.tensorflow.org/job/tf-master-win-bzl/2259/console", "comments": ["Testing at http://ci.tensorflow.org/view/TF%20pull%20requests/job/tensorflow-pr-win-bazel/55/console", "Thanks for catching this, sorry for the simple mistake.\r\nAny recommendations on how to generate stacktraces on windows?", "You're welcome! But I have no idea how to generate stack trace on Windows.."]}, {"number": 16124, "title": "How can I batch images of arbitrary sizes in tensorflow?", "body": "I want to realize arbitrary inputs that I can batch them in one batch.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Please go to stackoverflow, this is not the place to discuss about this.", "@DEKHTIARJonathan OK then", "Please close the issue"]}, {"number": 16123, "title": "add rolling window batch operation for tf.data.Dataset ", "body": "Resolve #15044.\r\n\r\n### implementation\r\n\r\nThe PR proposes a `slide` method for Dataset: Groups elements in fixed size blocks by passing a \"sliding window\" over Dataset. It behaves like `batch`, in fact, `batch(n) == slide(n, n)`.\r\n\r\n~I failed to move c++ implementation from `core` to `contrib`. Any help will be appreciated.~\r\n\r\n\r\n### how to test\r\n\r\n+ [x] add test case.\r\n+ [x] pass all tests.", "comments": ["Can one of the admins verify this patch?", "Thanks for review, I have resolved all the comments except of padding.\r\n\r\nIf necessary, how about to support padding like  [` tf.data.Dataset.padded_batch`](https://www.tensorflow.org/versions/master/api_docs/python/tf/data/Dataset#padded_batch)?", "Moreover, I find it a little ambiguous for padding.\r\nFor example:\r\n\r\n### Example 1\r\n```python\r\ndataset = [1, 2, 3, 4, 5, 6, 7]\r\nwindow_size = 4\r\nstride = 2\r\npadding_values = 0\r\n```\r\nWhich one of the behaviors below do we expect?\r\n+ a:\r\n```\r\n[1, 2, 3, 4]\r\n    [3, 4, 5, 6]\r\n         [5, 6, 7, 0]\r\n```\r\n+ b:\r\n```\r\n[1, 2, 3, 4]\r\n    [3, 4, 5, 6]\r\n         [5, 6, 7, 0]\r\n             [7, 0, 0, 0]\r\n```\r\n\r\n### Example 2\r\n\r\n```python\r\ndataset = [1, 2, 3, 4, 5, 6, 7]\r\nwindow_size = 5\r\nstride = 2\r\npadding_values = 0\r\n```\r\nWhich one of the behaviors below do we expect?\r\n+ a:\r\n```\r\n[1, 2, 3, 4, 5]\r\n    [3, 4, 5, 6, 7]\r\n```\r\n+ b:\r\n```\r\n[1, 2, 3, 4, 5]\r\n    [3, 4, 5, 6, 7]\r\n        [5, 6, 7, 0, 0]\r\n```\r\n+ c:\r\n```\r\n[1, 2, 3, 4, 5]\r\n    [3, 4, 5, 6, 7]\r\n        [5, 6, 7, 0, 0]\r\n            [7, 0, 0, 0, 0]\r\n```\r\n", "I have checked those failed logs: build broken. And those tests run well on my work machine. Seems unrelated?", "Thanks, I'll try to fix the problem when I get back from holiday.\r\n\r\n```CPP\r\ntensorflow/core/kernels/data/slide_dataset_op.cc:208:14: error: 'tensorflow::Status tensorflow::{anonymous}::SlideDatasetOp::Dataset::Iterator::RestoreInternal(tensorflow::OpKernelContext*, tensorflow::IteratorStateReader*)' marked override, but does not override\r\n       Status RestoreInternal(OpKernelContext* ctx,\r\n              ^\r\nIn file included from ./tensorflow/core/framework/tensor_shape.h:23:0,\r\n                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,\r\n                 from tensorflow/core/kernels/data/slide_dataset_op.cc:15:\r\ntensorflow/core/kernels/data/slide_dataset_op.cc: In member function 'tensorflow::Status tensorflow::{anonymous}::SlideDatasetOp::Dataset::Iterator::RestoreInternal(tensorflow::OpKernelContext*, tensorflow::IteratorStateReader*)':\r\ntensorflow/core/kernels/data/slide_dataset_op.cc:212:68: error: no matching function for call to 'tensorflow::{anonymous}::SlideDatasetOp::Dataset::Iterator::RestoreParent(tensorflow::OpKernelContext*&, tensorflow::IteratorStateReader*&, std::unique_ptr<tensorflow::IteratorBase>&)'\r\n           TF_RETURN_IF_ERROR(RestoreParent(ctx, reader, input_impl_));\r\n                                                                    ^\r\n```", "Hi, I have merged the latest codes from upstream, and fixed the broken test (at least for Linux CPU). Dose anyone help run all tests again? Thanks. cc @rmlarsen @mrry ", "@drpngx Hi, could you help me run all tests again? 7 days passed, and other reviewers seem so busy to reply. Thanks.", "Thank @drpngx for your help, all tests passed.\r\n\r\n@mrry @rmlarsen I think all comments have been addressed, and could you take a look? Thank you.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks all for the help! ", "Woohoo!\n\nOn Wed, Mar 7, 2018, 2:28 PM Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> Thanks all for the help!\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16123#issuecomment-371307108>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbQLzaR8DNTO1jk7urSTJ24zTEuxnks5tcF7qgaJpZM4Rd-0O>\n> .\n>\n", "Hi, I have issues getting this working\r\n\r\nAttributeError: module 'tensorflow.python.ops.gen_dataset_ops' has no attribute 'slide_dataset'\r\n\r\nany suggestions greatly appreciated, thank you", "@rombin Could you collect all necessary information and open a issue to report the problem? ", "I think I have an issue with installation. I have installed tensorflow 1.6 via Conda. But I do not see the sliding.py file in the contrib/data/python/ops folder. Do I somehow need even newer version? I notice you merged 2 days ago??\n\n\n\n\n> On 17 Mar 2018, at 23:01, Yan Facai (\u989c\u53d1\u624d) <notifications@github.com> wrote:\n> \n> @rombin <https://github.com/rombin> Could you collect all necessary information and open a issue to report the problem?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/pull/16123#issuecomment-373959082>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AiaZ4s2_pZkXop7EuDuoHMCnK1bOUIuuks5tfZW5gaJpZM4Rd-0O>.\n> \n\n", "Yes, I think the op will be released in the next version, and you can also install [tf-nightly](https://github.com/tensorflow/tensorflow#installation) to take a try.", "tried tf-nightly, works like charm, thank you\n\n\n> On 18 Mar 2018, at 00:25, Yan Facai (\u989c\u53d1\u624d) <notifications@github.com> wrote:\n> \n> Yes, I think the op will be released in the next version, and you can also install tf-nightly <https://github.com/tensorflow/tensorflow#installation> to take a try.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/pull/16123#issuecomment-373962914>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AiaZ4s1P5doPCjlZ1c27S-HslrwxvpHmks5tfalwgaJpZM4Rd-0O>.\n> \n\n"]}, {"number": 16122, "title": "fix typo", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16121, "title": "Enable some passes for graph_transform on Windows", "body": "Don't know why but the following passes are disabled on Windows:\r\n\r\n* quantize_weights\r\n* quantize_nodes\r\n* round_weights\r\n\r\nThis patch re-enabled them. This should fix #11351.\r\n\r\nRegarding the original commit disabled the passes on Windows, `git blame` gives the commit:\r\n\r\n```\r\ncommit d1ba01f81d8fa1d0171ba9ce871599063d5c7eb9\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Wed Feb 1 18:13:33 2017 -0800\r\n\r\n    Merge changes from github.\r\n    Change: 146316196\r\n```\r\n\r\nDoes anyone know what the commit message means?\r\n\r\nI built it on Windows 7 x64 and ran it for my tiny MNIST model. Looks fine...\r\n**So I am abusing CI to test it.**\r\n", "comments": ["Can one of the admins verify this patch?", "@petewarden sorry to bother. But since you are the owner of #11351, any suggestions?", "Sorry for the slow reply on this one, but thanks for the fix! The background is that the gemmlowp library used to fail to compile on Windows so rules that used them were disabled, but this has been fixed recently, so since the CI builds work this is good to go.", "CI seems failed at other places. Anything I could help?", "The failure seems unrelated. Rerunning..."]}, {"number": 16120, "title": "Utility classes for writing Java source code from a C++ process (part 3)", "body": "Part 3 (and last) of pull request #14094 that has been split into several commits.\r\n\r\nThis part features Java output streams, based on the previously-commited `SourceWriter`, to output Java code with a stream-like API. After this PR, everything will be setup to write Java code from the generator so we could start focusing again on the real thing: ops generation.\r\n\r\nCC: @asimshankar \r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Karl and I discussed some changes offline, he'll be making those changes soon.", "And here you go, please @asimshankar when you'll have a chance to take a look. Like we discussed I merged everything into the SourceWriter classes, mainly for simplicity.", "Hi @asimshankar, just a friendly little ping, in case you haven't noticed my last commit, thank you!", "@asimshankar no I don\u2019t! ;)", "Hi @asimshankar , do you have any visibility when this can be reviewed and (hopefully) merged? Thanks!", "Hi @asimshankar, I don't know if you can help here but it seems that the merge has been blocked somehow, thanks!", "@karllessard : I'm seeing some compilation failures on Mac. For example:\r\n\r\n```\r\n)\r\nIn file included from ./tensorflow/java/src/gen/cc/source_writer.h:26:0,\r\n                 from tensorflow/java/src/gen/cc/source_writer.cc:20:\r\n./tensorflow/java/src/gen/cc/java_defs.h: In member function 'tensorflow::java::Method& tensorflow::java::Method::add_arguments(const std::vector<tensorflow::java::Variable>&)':\r\n./tensorflow/java/src/gen/cc/java_defs.h:226:68: error: no matching function for call to 'std::vector<tensorflow::java::Variable>::insert(std::vector<tensorflow::java::Variable>::const_iterator, std::vector<tensorflow::java::Variable>::const_iterator, std::vector<tensorflow::java::Variable>::const_iterator)'\r\n     arguments_.insert(arguments_.cend(), args.cbegin(), args.cend());\r\n                                                                    ^\r\n./tensorflow/java/src/gen/cc/java_defs.h:226:68: note: candidates are:\r\nIn file included from /usr/include/c++/4.8/vector:69:0,\r\n                 from /usr/include/c++/4.8/bits/random.h:34,\r\n                 from /usr/include/c++/4.8/random:50,\r\n                 from /usr/include/c++/4.8/bits/stl_algo.h:65,\r\n                 from /usr/include/c++/4.8/algorithm:62,\r\n                 from tensorflow/java/src/gen/cc/source_writer.cc:17:\r\n/usr/include/c++/4.8/bits/vector.tcc:107:5: note: std::vector<_Tp, _Alloc>::iterator std::vector<_Tp, _Alloc>::insert(std::vector<_Tp, _Alloc>::iterator, const value_type&) [with _Tp = tensorflow::java::Variable; _Alloc = std::allocator<tensorflow::java::Variable>; std::vector<_Tp, _Alloc>::iterator = __gnu_cxx::__normal_iterator<tensorflow::java::Variable*, std::vector<tensorflow::java::Variable> >; typename std::_Vector_base<_Tp, _Alloc>::pointer = tensorflow::java::Variable*; std::vector<_Tp, _Alloc>::value_type = tensorflow::java::Variable]\r\n     vector<_Tp, _Alloc>::\r\n     ^\r\n/usr/include/c++/4.8/bits/vector.tcc:107:5: note:   candidate expects 2 arguments, 3 provided\r\nIn file included from /usr/include/c++/4.8/vector:64:0,\r\n                 from /usr/include/c++/4.8/bits/random.h:34,\r\n                 from /usr/include/c++/4.8/random:50,\r\n                 from /usr/include/c++/4.8/bits/stl_algo.h:65,\r\n                 from /usr/include/c++/4.8/algorithm:62,\r\n                 from tensorflow/java/src/gen/cc/source_writer.cc:17:\r\n/usr/include/c++/4.8/bits/stl_vector.h:988:7: note: std::vector<_Tp, _Alloc>::iterator std::vector<_Tp, _Alloc>::insert(std::vector<_Tp, _Alloc>::iterator, std::vector<_Tp, _Alloc>::value_type&&) [with _Tp = tensorflow::java::Variable; _Alloc = std::allocator<tensorflow::java::Variable>; std::vector<_Tp, _Alloc>::iterator = __gnu_cxx::__normal_iterator<tensorflow::java::Variable*, std::vector<tensorflow::java::Variable> >; typename std::_Vector_base<_Tp, _Alloc>::pointer = tensorflow::java::Variable*; std::vector<_Tp, _Alloc>::value_type = tensorflow::java::Variable]\r\n       insert(iterator __position, value_type&& __x)\r\n       ^\r\n/usr/include/c++/4.8/bits/stl_vector.h:988:7: note:   candidate expects 2 arguments, 3 provided\r\n/usr/include/c++/4.8/bits/stl_vector.h:1005:7: note: void std::vector<_Tp, _Alloc>::insert(std::vector<_Tp, _Alloc>::iterator, std::initializer_list<_Tp>) [with _Tp = tensorflow::java::Variable; _Alloc = std::allocator<tensorflow::java::Variable>; std::vector<_Tp, _Alloc>::iterator = __gnu_cxx::__normal_iterator<tensorflow::java::Variable*, std::vector<tensorflow::java::Variable> >; typename std::_Vector_base<_Tp, _Alloc>::pointer = tensorflow::java::Variable*]\r\n       insert(iterator __position, initializer_list<value_type> __l)\r\n       ^\r\n/usr/include/c++/4.8/bits/stl_vector.h:1005:7: note:   candidate expects 2 arguments, 3 provided\r\n/usr/include/c++/4.8/bits/stl_vector.h:1023:7: note: void std::vector<_Tp, _Alloc>::insert(std::vector<_Tp, _Alloc>::iterator, std::vector<_Tp, _Alloc>::size_type, const value_type&) [with _Tp = tensorflow::java::Variable; _Alloc = std::allocator<tensorflow::java::Variable>; std::vector<_Tp, _Alloc>::iterator = __gnu_cxx::__normal_iterator<tensorflow::java::Variable*, std::vector<tensorflow::java::Variable> >; typename std::_Vector_base<_Tp, _Alloc>::pointer = tensorflow::java::Variable*; std::vector<_Tp, _Alloc>::size_type = long unsigned int; std::vector<_Tp, _Alloc>::value_type = tensorflow::java::Variable]\r\n       insert(iterator __position, size_type __n, const value_type& __x)\r\n       ^\r\n/usr/include/c++/4.8/bits/stl_vector.h:1023:7: note:   no known conversion for argument 1 from 'std::vector<tensorflow::java::Variable>::const_iterator {aka __gnu_cxx::__normal_iterator<const tensorflow::java::Variable*, std::vector<tensorflow::java::Variable> >}' to 'std::vector<tensorflow::java::Variable>::iterator {aka __gnu_cxx::__normal_iterator<tensorflow::java::Variable*, std::vector<tensorflow::java::Variable> >}'\r\n/usr/include/c++/4.8/bits/stl_vector.h:1044:9: note: template<class _InputIterator, class> void std::vector<_Tp, _Alloc>::insert(std::vector<_Tp, _Alloc>::iterator, _InputIterator, _InputIterator) [with _InputIterator = _InputIterator; <template-parameter-2-2> = <template-parameter-1-2>; _Tp = tensorflow::java::Variable; _Alloc = std::allocator<tensorflow::java::Variable>]\r\n         insert(iterator __position, _InputIterator __first,\r\n         ^\r\n/usr/include/c++/4.8/bits/stl_vector.h:1044:9: note:   template argument deduction/substitution failed:\r\nIn file included from ./tensorflow/java/src/gen/cc/source_writer.h:26:0,\r\n                 from tensorflow/java/src/gen/cc/source_writer.cc:20:\r\n./tensorflow/java/src/gen/cc/java_defs.h:226:68: note:   cannot convert '((tensorflow::java::Method*)this)->tensorflow::java::Method::arguments_.std::vector<_Tp, _Alloc>::cend<tensorflow::java::Variable, std::allocator<tensorflow::java::Variable> >()' (type 'std::vector<tensorflow::java::Variable>::const_iterator {aka __gnu_cxx::__normal_iterator<const tensorflow::java::Variable*, std::vector<tensorflow::java::Variable> >}') to type 'std::vector<tensorflow::java::Variable>::iterator {aka __gnu_cxx::__normal_iterator<tensorflow::java::Variable*, std::vector<tensorflow::java::Variable> >}'\r\n     arguments_.insert(arguments_.cend(), args.cbegin(), args.cend());\r\n                                                                    ^\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/java/BUILD:308:1: Couldn't build file tensorflow/java/_objs/source_writer_test/tensorflow/java/src/gen/cc/source_writer_test.o: C++ compilation of rule '//tensorflow/java:source_writer_test' failed (Exit 1): cr\r\n```\r\n", "Oups, sorry, I assumed too quick that it was unrelated again, I\u2019ll check it out", "Here @asimshankar, basically I replaced all uses of `std::vector` and `std::deque` by `std::list`, which seems more permissive. I could reproduce before the problem on a Mac with C++ 4.2 and it now seems happy with it. Can we remerge the code please?", "@karllessard : Couldn't you have kept the vectors? I suspect the build was fixed by the fact that you removed the `add_arguments` function instead of the move to a `std::list`. In fact, I think it would have also been fixed by changing `add_arguments`'s implementation to use `begin()` and `end()` instead of `cbegin()` and `cend()`.\r\n\r\nI'm kicking off another round of tests, but if the simpler fix of just removing the `add_arguments` function works with the previous `vector` and `deque`s then that might be preferable.", "> but if the simpler fix of just removing the add_arguments function works with the previous vector and deques then that might be preferable.\r\n\r\n@asimshankar : Well that's what I thought at first but on my Mac, the compiler was complaining about the `std::deque<Type> supertypes_`, trying to compute the `sizeof(Type)` at construct time for some reason, which of course failed since this field is a member of `Type` itself. Replacing it by a `list` fixed my problem and I thought that, since they are more convenient than `vector` for finding elements etc. and optimization of this generator is not critical, why not applying it everywhere.\r\n\r\n>  In fact, I think it would have also been fixed by changing add_arguments's implementation to use begin() and end() instead of cbegin() and cend().\r\n\r\nI simply removed the method because while preparing the next PR, I figured out that I don't need it anymore.", "Sounds good, thanks."]}, {"number": 16119, "title": "Created dense_to_sparse in contrib.layers", "body": "Added `dense_to_sparse`. This does the conversion of dense labels into sparse ones to be passed into the core ctc_loss function. Addresses feature request https://github.com/tensorflow/tensorflow/issues/15985", "comments": ["Can one of the admins verify this patch?", "@selcouthlyBlue yes. I meant that instead of having two loss functions, one which converts some arguments and calls the other,  we could have a simple utility function to convert the argument and a single loss function. This feels to me like a smaller API surface.", "@selcouthlyBlue Please resolve conflict.", "I forgot that I was pulling from my copy of the repository. My bad. Resolving the conflicts now.", "Resolved the conflicts. Test please.", "@selcouthlyBlue Thanks!", "It's good to go "]}, {"number": 16118, "title": "Minor improvements to TFRecord format docs", "body": "The TFRecord format documentation mentions that hashes are computed using a CRC32, but doesn't mention the polynomial used. I added that detail, so the documentation is now sufficient for a developer trying to write a parser / writer for (uncompressed) TFRecord files.", "comments": ["Can one of the admins verify this patch?", "@rmlarsen Thank you for the review!", "Thanks for the improvement to the docs!"]}, {"number": 16117, "title": "Add nsync lib dep. to cc_library rule android_tensorflow_lib_selective_registration", "body": "The nsync lib dep is missed in rule \"android_tensorflow_lib_selective_registration\"\r\n\r\nThis pr adds it.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16116, "title": "Implement Scale Operate?? ", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code: N/A**:\r\n- **OS Platform and Distribution: openSUSE Leap 42.3**:\r\n- **TensorFlow installed from: binary**:\r\n- **TensorFlow version: tensorflow_gpu-1.4.0**:\r\n- **Python version: 2.7.13**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version: 8.0/6.0**:\r\n- **GPU model and memory: 11GB**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIn Caffe, Scale layer could do this:\r\ntop = alpha\u2217bottom+beta\r\nwhere bottom is the input, top is the output, alpha and beta are the learnable params.\r\nIn Tensorflow, what operate could implement above layer?\r\nThanks, Help a lot.", "comments": ["Do you mean batch normalization?", "I have the same question, the beta (bias) has been implemented as tf.nn.bias_add and it support NCHW or NHWC format, but I didn't find the alpha op. So right now I constuct my own scale layer in python, which seems a little slower. ", "@facaiy , it is not a batch normalization operate, it just scale the inputs without norm the inputs", "@boluoweifenda , I solve this problem like this:\r\n```\r\nwith tf.variable_scope(\"scale\", reuse=tf.AUTO_REUSE):\r\n    alpha= tf.get_variable(name='alpha', shape=(channel_num,), trainable=True)\r\n    beta = tf.get_variable(name='beta', shape=(channel_num,), trainable=True)\r\n    output = gamma * input + beta\r\n```\r\nThe channel_num is the channel number of feature map. The Scale layer in Caffe, is just operate the channel. I think it is solving the problem.", "@ly-atdawn [`tf.layers.dense`](https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/dense)?\r\n> This layer implements the operation: outputs = activation(inputs * kernel + bias)", "You can look the Scale layer for detail, it is not same with \"tf.layers.dense\"........", "Oh, the multiplication operation is Hadamard product, not matrix product, right?\r\nhttp://caffe.berkeleyvision.org/doxygen/classcaffe_1_1ScaleLayer.html#details", "@ly-atdawn  Yes, but I find this is even slower than the fused-batchnormalization  (NCHW format)", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf this is a feature request, please create a new issue by providing all the details asked in [this template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks !\r\n"]}, {"number": 16115, "title": "Gradients w.r.t. eigenvalues/eigenvectors ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.1\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see code\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nHi,\r\n\r\nI'm having issues with evaluating the gradients of eigenvalues/eigenvectors with respect to the underlying matrix. Tensorflow evaluates the gradients, but these are not correct when compared to the true gradients.\r\n\r\nWe are using tf.self_adjoint_eig to evaluate the spectral decomposition for the tensorflow variable input. We have initialised this with a symmetric matrix to satisfy the self adjoint operator property. We wish to take the derivative of individual eigenvalues or eigenvector with respect to the original matrix (note for eigenvectors we take one element from one eigenvector, e.g. element 2 in eigenvector 1 to avoid the issue of gradient aggregation for now).\r\n\r\nThe methodology for evaluating true gradients of eigenvalues and vectors of a symmetric real matrix can be found in the paper \"On differentiating Eigenvalues and Eigenvectors\" by Magnus (1985), Theorem 1 eqn (6) + (7). We evaluated using our input matrix the gradients under this paper and compared it to tensorflow evaluated gradients and the gradients from finite difference approximation. For the eigenvalues, the gradients are similar (identical on diagonal entries, off by a factor of 2 on off-diagonal elements), however the eigenvectors are off by quite a bit outside the diagonal entries. To start, define a matrix A as (excuse the matrix output formatting from Python)\r\n\r\nA=[[-3 -2  4]\r\n      [-2  1  1]\r\n      [ 4  1  5]]\r\n\r\nusing np.linalg.eig and tf.self_adjoint_eig on A (I simply initialised a variable with A and computed the gradient for the tf implementation)\r\n\r\nTensorflow eigenvalues: [-5.43071561  1.76904987  6.66166575]\r\nPython eigenvalues: [-5.43071561  6.66166575  1.76904987]\r\n\r\nI now wish to evaluate the gradient of eigenvalue 1 (-5.43071561) w.r.t. A. \r\n\r\nAnalytical gradient:\r\n[[ 0.75896178  0.28555906 -0.31842553]\r\n [ 0.28555906  0.10744148 -0.11980748]\r\n [-0.31842553 -0.11980748  0.13359674]]\r\n\r\nTensorflow gradient:\r\n[[[ 0.75896178,  0.        ,  0.        ],\r\n   [ 0.57111812,  0.10744148,  0.        ],\r\n  [-0.63685107, -0.23961495,  0.13359674]]]\r\n\r\nThe diagonal entries are the same but the off-diagonal entries are clearly off by a factor of 2.  Now we try and evaluate gradients for the eigenvectors.\r\n\r\nTensorflow eigenvectors:\r\n[[-0.87118413 -0.31452619 -0.37697678]\r\n [-0.32778267  0.94426587 -0.0303395 ]\r\n [ 0.36550888  0.09713517 -0.92572567]]\r\nPython eigenvectors:\r\n[[ 0.87118413  0.37697678 -0.31452619]\r\n [ 0.32778267  0.0303395   0.94426587]\r\n [-0.36550888  0.92572567  0.09713517]]\r\n\r\nWe try and find the gradient of the eigenvector 1 element 2 (+/-0.32778267). We expect the tensorflow gradient to the equivalent to the analytical gradient (after taking into account the sign difference).\r\n\r\nAnalytical gradient:\r\n[[ 0.03511309 -0.10795607 -0.01312188]\r\n [ 0.01321128 -0.04061843 -0.0049371 ]\r\n [-0.01473184  0.04529341  0.00550534]]\r\n\r\nTensorflow gradient:\r\n[[[-0.03511309,  0.        ,  0.        ],\r\n   [ 0.09474478,  0.04061843,  0.        ],\r\n   [ 0.02785372, -0.04035631, -0.00550534]]]\r\n\r\nBesides the entries being different between the tensorflow evaluated and real gradients, one issue is that tensorflow only returns the lower triangle of the gradient. Despite A being symmetric, the gradient matrix is not symmetric (as per the true gradient) and so the upper right triangle shouldn't be zeros. I believe this to be a bug, however I understand there may be reasons for the differences. Thank you for reading!\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nHere is a script that reproduces the above.\r\n\r\n[eigen_decomp_examplev2.py.zip](https://github.com/tensorflow/tensorflow/files/1546631/eigen_decomp_examplev2.py.zip)\r\n", "comments": ["@rmlarsen, have you experience with any of this code?", "If the input matrix A is symmetric, changing A_ij, will also change A_ji, so we need to multiple the off-diagonal elements by 2. For more details including results on the generalized eigenvalue problem see this [wiki article on Eigenvalue perturbation](https://en.wikipedia.org/wiki/Eigenvalue_perturbation).", "Closing this issue due to staleness. Please check with the latest version of TensorFlow. Feel free to reopen if issue still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=16115\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=16115\">No</a>\n", "The described problem still exists in TensorFlow 2.0. For example, the first result posted by @mingu6 can be reproduced with TensorFlow 2.0 with the below code.\r\n\r\nIs there any new insight into the problem?\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom numpy.linalg import eig\r\nfrom numpy.linalg import pinv\r\n\r\n#create upper triangular matrix\r\nA_vec = np.array([-3,-2,4,-2,1,1,4,1,5])\r\nA = np.reshape(A_vec,[3,3])\r\ne = eig(A)\r\n#extract eigenvalues and eigenvectors\r\ne_val = e[0]\r\ne_vec = e[1]\r\n\r\n#tensorflow variables\r\nA_tf = tf.Variable(tf.constant(A,dtype=tf.float64))\r\ne_tf = tf.linalg.eigh(A_tf)\r\n#extract eigenvalues and eigenvectors\r\ne_val_tf = e_tf[0]\r\ne_vec_tf = e_tf[1]\r\n\r\n#print A\r\nprint(\"A:\")\r\nprint(A)\r\n\r\n#show eigenvalues for Tensorflow and R\r\nprint(\"Tensorflow eigenvalues:\",tf.keras.backend.eval(e_val_tf))\r\nprint(\"Python eigenvalues:\",e_val)\r\n\r\n############# Eigenvector element gradient #############\r\n\r\nA_tf = tf.Variable(tf.constant(A,dtype=tf.float64))\r\nwith tf.GradientTape() as g:\r\n    g.watch(A_tf)\r\n\r\n    e_val_tf, e_vec_tf = tf.linalg.eigh(A_tf)\r\n    #extract eigenvalues and eigenvectors\r\n    e_val_0 = e_val_tf[0]\r\n\r\neval_grad_tf = g.gradient(e_val_0, A_tf) #Tensorflow gradient of first eigenvalue w.r.t. A\r\n\r\n#analytical gradient\r\neigenvec = e_vec[:,0]\r\n\r\n#analytical deriv from analytical gradient of eigenvalue from Magnus (1985), \"On differentiating\r\n#Eigenvalues and Eigenvectors\", Theorem 1 eqn (6)\r\neval_grad_analytical = np.outer(eigenvec,eigenvec)\r\n\r\nprint(\"Analytical gradient:\")\r\nprint(eval_grad_analytical)\r\nprint(\"Tensorflow gradient:\")\r\nprint(tf.keras.backend.eval(eval_grad_tf))\r\n```"]}, {"number": 16114, "title": "Update maxout.py", "body": "Specify the final number of features in the maxout axis", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@carltonwang please rebase and push again.", "sorry, i do not know what is rebase", "Rebase means to put your changes on top of the current tip of the branch.\r\n\r\nProbably something like that:\r\n```\r\ngit remote add origin git@github.com:cartonwang/tensorflow.git\r\ngit remote add tfhead https://github.com/tensorflow/tensorflow.git\r\ngit fetch tfhead && git rebase tfhead/master\r\ngit push -u origin master\r\n```"]}, {"number": 16113, "title": "Propagate the error string of GIF processing for decode_gif", "body": "This fix tries to improve the error thrown by `decode_gif` to include the error string generated by GIF processing.\r\n\r\nPreviously, the error was not very indicative as the error string\r\nreturned by GIF processing was hidden:\r\n```\r\n..........\r\nInvalidArgumentError (see above for traceback): Invalid GIF data, size 2091369\r\n\t [[Node: DecodeGif = DecodeGif[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile)]]\r\n```\r\n\r\nThis fix propagate the error string (`can't process optimized gif`) to be part of the `InvalidArgumentError`:\r\n```\r\nInvalidArgumentError (see above for traceback): Invalid GIF data (size 2091369), can't process optimized gif\r\n         [[Node: DecodeGif = DecodeGif[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile)]]\r\n```\r\n\r\nThis fix fixes #15838.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @rmlarsen for the review. A unit test `testOptimizedGifErrorString` has been added to image_ops_test.py. Please take a look and let me know if there are any issues."]}, {"number": 16112, "title": "Define gradient for tf.linspace and make it work with higher-rank tensors, not just scalars.", "body": "I needed to feed-forward through `tf.linspace`, but it seems that it does not have gradient defined.\r\n\r\nI don't know how to define gradient for existing op, but I've implemented my own version of `tf.linspace` in python using tensorflow with so that automatically defined gradient.\r\n```python\r\n            def linspace(start, end, num):\r\n                range = end - start\r\n                num_steps = num - 1\r\n                h = range / num_steps\r\n\r\n                def cond(ta, x, k):\r\n                    return tf.less(x, end)\r\n\r\n                def body(ta, x, k):\r\n                    x = x + h\r\n                    ta = ta.write(k, x)\r\n                    return ta, x, k+1\r\n\r\n                k = tf.constant(0)\r\n                ta = tf.TensorArray(dtype=tf.float32, size=num)\r\n                ta = ta.write(k, start)\r\n                ta = tf.while_loop(cond, body, [ta, start, k+1])[0]\r\n                return ta.stack()\r\n```\r\n\r\nOne more feature I can suggest adding is improve to `linspace` so it would work with higher-rank tensors.\r\nThe function I wrote is also very short and simple, but is interesting as a generalization of `linspace`.\r\n```python\r\n            def linspace_vectors(start, end, num):\r\n                cnct = tf.concat([start, end], 1)\r\n                seq = tf.map_fn(\r\n                    lambda row_i: linspace(row_i[0], row_i[1], num), cnct)\r\n                splits = tf.split(seq, num, 1)\r\n                return tf.stack(splits)\r\n```\r\nThe function is taken from my project and returns 3-rank tensor with shapes [num, r, 1]. Inputs are 2-rank tensors with shapes [r, 1].\r\nSo that I linspaced vectors-columns, not just scalars as `tf.linspace` do.\r\nWhat do you think? Is it worth adding?\r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: pip nightly build\r\nTensorFlow version: 1.4.1\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I think it's still an issue. And I have update some fields requested", "To write the gradient of a tensorflow op you can write a gradient op in C (see the implementation of linspace) or python and then use RegisterGradient to register it as the gradient of another op. This discussion should move to a pull request if you want to contribute code."]}]