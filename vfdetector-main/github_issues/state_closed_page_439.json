[{"number": 40689, "title": "[INTEL MKL] Fix conv_ops_test and remapper_test", "body": "This PR addresses the Windows build issue with the previous PR\r\n    https://github.com/tensorflow/tensorflow/pull/39548    (approved/merged then revoked)\r\n\r\nThis PR fixes two C++ test failures related to MKL ops.\r\n1. conv_ops_test          // MklConvOp does not support EXPLICIT padding\r\n2. remapper_test // Fusion of MKL Conv and Mkl FusedBatchNorm is not supported\r\n\r\nTest has been done in both Windows & Linux systems.", "comments": ["@gbaned  Hi, I noticed that this PR has not been merged for a while. Please let me know if you need anything from my side. Thanks!  -GZ"]}, {"number": 40688, "title": "C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04 -- building inside `Dockerfile` with `FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04`\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version: 2.0.0 (extracted from _TF_MAX_BAZEL)\r\n- GCC/Compiler version: 7.4.0\r\n- CUDA/cuDNN version: 10.1 / 7 \r\n- GPU model and memory: tested on Titan XP and RTX 2070 8GB \r\n\r\n\r\n**Describe the problem**\r\n\r\nBuild fails with\r\n```ERROR: /usr/local/src/tensorflow/tensorflow/python/BUILD:437:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1)\r\nESC[0mESC[91mtensorflow/python/lib/core/bfloat16.cc: In function 'bool tensorflow::{anonymous}::Initialize()':\r\ntensorflow/python/lib/core/bfloat16.cc:636:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const c\r\nhar [6], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int\r\n*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:640:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const c\r\nhar [10], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int\r\n*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:643:77: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const c\r\nhar [5], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n   if (!register_ufunc(\"less\", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {\r\n\r\n                                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:647:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:651:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:655:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\nESC[0mESC[91mTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nESC[0mESC[91mERROR: /usr/local/src/tensorflow/tensorflow/tools/pip_package/BUILD:62:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1)\r\nESC[0mESC[91mINFO: Elapsed time: 1828.057s, Critical Path: 881.14s\r\nINFO: 13824 processes: 13824 local.\r\nESC[0mESC[91mFAILED: Build did NOT complete successfully\r\nESC[0mESC[91mFAILED: Build did NOT complete successfully\r\nESC[0mESC[91mCommand exited with non-zero status 1\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nReproducible with the following `Dockerfile`\r\n\r\n```\r\nFROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\r\n\r\n# Install system packages\r\nENV DEBIAN_FRONTEND noninteractive\r\nRUN apt-get update -y \\\r\n  && apt-get install -y --no-install-recommends apt-utils \\\r\n  && apt-get install -y \\\r\n    build-essential \\\r\n    checkinstall \\\r\n    cmake \\\r\n    curl \\\r\n    g++ \\\r\n    gcc \\\r\n    git \\\r\n    locales \\\r\n    perl \\\r\n    pkg-config \\\r\n    protobuf-compiler \\\r\n    python3-dev \\\r\n    rsync \\\r\n    software-properties-common \\\r\n    unzip \\\r\n    wget \\\r\n    zip \\\r\n    zlib1g-dev \\\r\n  && apt-get clean\r\n\r\n# UTF-8\r\nRUN localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8\r\nENV LANG en_US.utf8\r\n\r\n# Setup pip\r\nRUN wget -q -O /tmp/get-pip.py --no-check-certificate https://bootstrap.pypa.io/get-pip.py \\\r\n  && python3 /tmp/get-pip.py \\\r\n  && pip3 install -U pip \\\r\n  && rm /tmp/get-pip.py\r\n# Some TF tools expect a \"python\" binary\r\nRUN ln -s $(which python3) /usr/local/bin/python\r\n\r\n# /etc/ld.so.conf.d/nvidia.conf point to /usr/local/nvidia which seems to be missing, point to the cuda directory install for libraries\r\nRUN cd /usr/local && ln -s cuda nvidia\r\nARG CTO_CUDA_VERSION=\"10.1\"\r\nARG CTO_CUDA_PRIMEVERSION=\"10.0\"\r\nARG CTO_CUDA_APT=\"cuda-npp-${CTO_CUDA_VERSION} cuda-cublas-${CTO_CUDA_PRIMEVERSION} cuda-cufft-${CTO_CUDA_VERSION} cuda-libraries-${CTO_CUDA_VERSION} cuda-npp-dev-${CTO_CUDA_VERSION} cuda-cublas-dev-${CTO_CUDA_PRIMEVERSION} cuda-cufft-dev-${CTO_CUDA_VERSION} cuda-libraries-dev-${CTO_CUDA_VERSION}\"\r\nRUN apt-get install -y --no-install-recommends \\\r\n  time ${CTO_CUDA_APT} \\\r\n  && apt-get clean\r\n\r\nENV LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\"\r\n\r\n# Install Python tools \r\nRUN pip3 install -U \\\r\n  mock \\\r\n  numpy \\\r\n  setuptools \\\r\n  six \\\r\n  wheel \\\r\n  && pip3 install 'future>=0.17.1' \\\r\n  && pip3 install -U keras_applications --no-deps \\\r\n  && pip3 install -U keras_preprocessing --no-deps \\\r\n  && rm -rf /root/.cache/pip\r\n\r\n## Download & Building TensorFlow from source\r\nARG LATEST_BAZELISK=1.5.0\r\nARG CTO_TENSORFLOW_VERSION=\"2.2.0\"\r\nRUN curl -s -Lo /usr/local/bin/bazel https://github.com/bazelbuild/bazelisk/releases/download/v${LATEST_BAZELISK}/bazelisk-linux-amd64 \\\r\n  && chmod +x /usr/local/bin/bazel \\\r\n  && mkdir -p /usr/local/src \\\r\n  && cd /usr/local/src \\\r\n  && wget -q --no-check-certificate https://github.com/tensorflow/tensorflow/archive/v${CTO_TENSORFLOW_VERSION}.tar.gz \\\r\n  && tar xfz v${CTO_TENSORFLOW_VERSION}.tar.gz \\\r\n  && mv tensorflow-${CTO_TENSORFLOW_VERSION} tensorflow \\\r\n  && rm v${CTO_TENSORFLOW_VERSION}.tar.gz \\\r\n  && cd /usr/local/src/tensorflow \\\r\n  && fgrep _TF_MAX_BAZEL configure.py | grep '=' | perl -ne 'print $1 if (m%\\=\\s+.([\\d\\.]+).$+%)' > .bazelversion\r\nRUN cd /usr/local/src/tensorflow \\\r\n  && TF_CUDA_CLANG=0 TF_CUDA_VERSION=${CTO_CUDA_VERSION} TF_CUDNN_VERSION=7 TF_DOWNLOAD_CLANG=0 TF_DOWNLOAD_MKL=0 TF_ENABLE_XLA=0 TF_NEED_AWS=0 TF_NEED_COMPUTECPP=0 TF_NEED_CUDA=1 TF_NEED_GCP=0 TF_NEED_GDR=0 TF_NEED_HDFS=0 TF_NEED_JEMALLOC=1 TF_NEED_KAFKA=0 TF_NEED_MKL=0 TF_NEED_MPI=0 TF_NEED_OPENCL=0 TF_NEED_OPENCL_SYCL=0 TF_NEED_ROCM=0 TF_NEED_S3=0 TF_NEED_TENSORRT=0 TF_NEED_VERBS=0 TF_SET_ANDROID_WORKSPACE=0 TF_CUDA_COMPUTE_CAPABILITIES=\"5.3,6.0,6.1,6.2,7.0,7.2,7.5\" GCC_HOST_COMPILER_PATH=$(which gcc) CC_OPT_FLAGS=\"-march=native\" PYTHON_BIN_PATH=$(which python) PYTHON_LIB_PATH=\"$(python -c 'import site; print(site.getsitepackages()[0])')\" ./configure\r\nRUN cd /usr/local/src/tensorflow \\\r\n  && time bazel build --verbose_failures --config=opt --config=v2 --config=cuda //tensorflow/tools/pip_package:build_pip_package \\\r\n  && time ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \\\r\n  && time pip3 install /tmp/tensorflow_pkg/tensorflow-*.whl\r\n\r\nCMD bash\r\n```\r\n\r\nBuilt using `docker build --tag cto:test .`\r\n\r\nNote tested with CUDA 10.1, 10.0 and 10.2.\r\nAlso occurs with TF 1.15.3\r\n\r\n\r\n\r\n**Any other info / logs**\r\nI can provide the full build log if requested (91MB)\r\n\r\n```Step 18/20 : RUN cd /usr/local/src/tensorflow   && TF_CUDA_CLANG=0 TF_CUDA_VERSION=${CTO_CUDA_VERSION} TF_CUDNN_VERSION=7 TF_DOWNLOAD_CLANG=0 TF_DOWNLOAD_MKL=0 TF_ENABLE_XLA=0 TF_NEED_AWS=0 TF_NEED_COMPUTECPP=0 TF_NEED_CUDA=1 TF_NEED_GCP=0 TF_NEED_GDR=0 TF_NEED_HDFS=0 TF_NEED_JEMALLOC=1 TF_NEED_KAFKA=0 TF_NEED_MKL=0 TF_NEED_MPI=0 TF_NEED_OPENCL=0 TF_NEED_OPENCL_SYCL=0 TF_NEED_ROCM=0 TF_NEED_S3=0 TF_NEED_TENSORRT=0 TF_NEED_VERBS=0 TF_SET_ANDROID_WORKSPACE=0 TF_CUDA_COMPUTE_CAPABILITIES=\"5.3,6.0,6.1,6.2,7.0,7.2,7.5\" GCC_HOST_COMPILER_PATH=$(which gcc) CC_OPT_FLAGS=\"-march=native\" PYTHON_BIN_PATH=$(which python) PYTHON_LIB_PATH=\"$(python -c 'import site; print(site.getsitepackages()[0])')\" ./configure\r\n ---> Running in 9690386205a5\r\n2020/06/22 14:11:17 Downloading https://releases.bazel.build/2.0.0/release/bazel-2.0.0-linux-x86_64...\r\nExtracting Bazel installation...\r\nYou have bazel 2.0.0 installed.\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda-10.1/lib64\r\n    /usr/local/cuda-10.1/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\n\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\nRemoving intermediate container 9690386205a5\r\n ---> 8910acc4d9c5\r\nStep 19/20 : RUN cd /usr/local/src/tensorflow   && time bazel build --verbose_failures --config=opt --config=v2 --config=cuda //tensorflow/tools/pip_package:build_pip_package   && time ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg   && time pip3 install /tmp/tensorflow_pkg/tensorflow-*.whl\r\n ---> Running in 3b0267b1209d\r\nESC[91mStarting local Bazel server and connecting to it...\r\nESC[0mESC[91mWARNING: The following configs were expanded more than once: [v2, cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nESC[0mESC[91mINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nESC[0mESC[91mINFO: Reading rc options for 'build' from /usr/local/src/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /usr/local/src/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /usr/local/src/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages --python_path=/usr/local/bin/python --action_env TF_CUDA_VERSION=10.1 --action_env TF_CUDNN_VERSION=7 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.3,6.0,6.1,6.2,7.0,7.2,7.5 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/extras/CUPTI/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /usr/local/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nESC[0mESC[91mINFO: Found applicable config definition build:cuda in file /usr/local/src/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /usr/local/src/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:opt in file /usr/local/src/tensorflow/.tf_configure.bazelrc: --copt=-march=native --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:v2 in file /usr/local/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file /usr/local/src/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /usr/local/src/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:linux in file /usr/local/src/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /usr/local/src/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nESC[0mESC[91mLoading: \r\nESC[0mESC[91mLoading: 0 packages loaded\r\nESC[0mESC[91mLoading: 0 packages loaded\r\nESC[0mESC[91mLoading: 0 packages loaded\r\nESC[0mESC[91mDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nESC[0mESC[91mDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /root/.cache/bazel/_bazel_root/bbcc73fcc5c2b01ab08b6bcf7c29e42e/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - /root/.cache/bazel/_bazel_root/bbcc73fcc5c2b01ab08b6bcf7c29e42e/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - /usr/local/src/tensorflow/WORKSPACE:37:1\r\nESC[0mESC[91mLoading: 0 packages loaded\r\nESC[0mESC[91mLoading: 0 packages loaded\r\nESC[0mESC[91mLoading: 0 packages loaded\r\nESC[0mESC[91mLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nESC[0mESC[91mDEBUG: /root/.cache/bazel/_bazel_root/bbcc73fcc5c2b01ab08b6bcf7c29e42e/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:5: \r\n[...]```\r\n\r\n\r\n", "comments": ["Possibly duplicate of #40654? I'm also seeing the same issue with `v2.2.0` and GCC 7.5.0.", "> Possibly duplicate of #40654? I'm also seeing the same issue with `v2.2.0` and GCC 7.5.0.\r\n\r\nThank you :) \r\nI looked at the PR, and am integrating this change into the `Dockerfile`:\r\n`&& perl -pi.bak -e 's%, CompareUFunc%, (PyUFuncGenericFunction) CompareUFunc%g' tensorflow/python/lib/core/bfloat16.cc \\`\r\nright before the `./configure` step.\r\n\r\nWill report if this fixes the build", "Confirming that this solves the `build` issue (for 2.20 and 10.1):\r\n\r\n```\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\n```\r\n\r\nGoing to check for 2.20 and 10.2 then 1.15.3 and 10.2", "In our testing, we found that this issue breaks building from source for TF 1.15.x and 2.x.\r\n\r\nThe issue comes from source build being incompatible with numpy `1.19.0` which has a breaking ABI change (numpy/numpy#15355) and was released 2 days ago.\r\n\r\nFixing numpy to pre 1.19.0 fixes the issue:\r\n```\r\npip install numpy<1.19.0\r\n```", "Thank you, will force `numpy<1.19.0` for the time being.\r\n\r\nAlso confirming 2.20 and 10.2 compiles with the `PyUFuncGenericFunction` fix\r\n\r\n", "Confirming successful compilation on 2.20 and 10.2 with `numpy<1.19.0`.\r\n\r\nOkay to close the issue.\r\n\r\nI have different problems with 1.15.3 and `nvlink` (with 10.0 and 10.1) but if I can not resolve, I will open a different ticket.", "@mmartial ,\r\n\r\nOn behalf #40654 thank you for investigation !\r\n", "> Okay to close the issue.\r\n\r\nMarking the issue as closed, as it is resolved. Please feel free to re-open the issue if required. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40688\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40688\">No</a>\n", "Hi, @mmartial. I also run into the same issue. I've downgraded numpy to 1.18.5 but it did not fix the problem. Here's the error message I received\r\n\r\n> tensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}\u2019\r\ntensorflow/python/lib/core/bfloat16.cc:640:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [10], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}\u2019\r\ntensorflow/python/lib/core/bfloat16.cc:643:77: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [5], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n   if (!register_ufunc(\"less\", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {\r\n                                                                             ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}\u2019\r\ntensorflow/python/lib/core/bfloat16.cc:647:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}\u2019\r\ntensorflow/python/lib/core/bfloat16.cc:651:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}\u2019\r\ntensorflow/python/lib/core/bfloat16.cc:655:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}\u2019\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/aptx4869/github/tensorflow/tensorflow/tools/pip_package/BUILD:62:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1)\r\nINFO: Elapsed time: 24.977s, Critical Path: 13.97s\r\nINFO: 2 processes: 2 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nIt seems related to `PyUFuncGenericFunction` which you mentioned to \"fix\". How should I do it?\r\n\r\nHere's my environment information:\r\n\r\nUbuntu: 18.04\r\nTF: r2.2 (trying to build from source but failed)\r\nCUDA: 10.2\r\nCuDNN: 7.6.5\r\npython: 3.7.7\r\nBazel: 2.0.0\r\n\r\nAnd here's the output of `pip list` in case you need:\r\n\r\ncertifi             2020.6.20\r\ndecorator           4.4.0\r\nfuture              0.18.2\r\nh5py                2.10.0\r\nKeras-Applications  1.0.8\r\nKeras-Preprocessing 1.1.2\r\nmock                4.0.2\r\nnumpy               1.18.5\r\npip                 20.1.1\r\nsetuptools          47.3.1.post20200622\r\nsix                 1.15.0\r\nwheel               0.34.2", "> > tensorflow/python/lib/core/bfloat16.cc:655:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], , const std::array<int, 3>&)\u2019\r\n> > compare_types)) {\r\n\r\n@xlnwel Looking at the above, I wonder: did you use both the PR (or the Perl command) and the `numpy<1.19.0` ?\r\n\r\nTo make it work, I had to use either of those.\r\n\r\nI am putting below the updated `Dockerfile` hoping it works for you:\r\n```\r\nARG CTO_CUDA_VERSION=\"10.2\"\r\nFROM nvidia/cuda:${CTO_CUDA_VERSION}-cudnn7-devel-ubuntu18.04\r\nARG CTO_CUDA_VERSION=\"10.2\"\r\n\r\n# Install system packages\r\nENV DEBIAN_FRONTEND noninteractive\r\nRUN apt-get update -y \\\r\n  && apt-get install -y --no-install-recommends apt-utils \\\r\n  && apt-get install -y \\\r\n    build-essential \\\r\n    checkinstall \\\r\n    cmake \\\r\n    curl \\\r\n    g++ \\\r\n    gcc \\\r\n    git \\\r\n    locales \\\r\n    perl \\\r\n    pkg-config \\\r\n    protobuf-compiler \\\r\n    python3-dev \\\r\n    rsync \\\r\n    software-properties-common \\\r\n    unzip \\\r\n    wget \\\r\n    zip \\\r\n    zlib1g-dev \\\r\n  && apt-get clean\r\n\r\n# UTF-8\r\nRUN localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8\r\nENV LANG en_US.utf8\r\n\r\n# Setup pip\r\nRUN wget -q -O /tmp/get-pip.py --no-check-certificate https://bootstrap.pypa.io/get-pip.py \\\r\n  && python3 /tmp/get-pip.py \\\r\n  && pip3 install -U pip \\\r\n  && rm /tmp/get-pip.py\r\n# Some TF tools expect a \"python\" binary\r\nRUN ln -s $(which python3) /usr/local/bin/python\r\n\r\n# /etc/ld.so.conf.d/nvidia.conf point to /usr/local/nvidia which seems to be missing, point to the cuda directory install for libraries\r\nRUN cd /usr/local && ln -s cuda nvidia\r\nARG CTO_CUDA_PRIMEVERSION=\"10.0\"\r\nARG CTO_CUDA_APT=\"cuda-npp-${CTO_CUDA_VERSION} cuda-cublas-${CTO_CUDA_PRIMEVERSION} cuda-cufft-${CTO_CUDA_VERSION} cuda-libraries-${CTO_CUDA_VERSION} cuda-npp-dev-${CTO_CUDA_VERSION} cuda-cublas-dev-${CTO_CUDA_PRIMEVERSION} cuda-cufft-dev-${CTO_CUDA_VERSION} cuda-libraries-dev-${CTO_CUDA_VERSION}\"\r\nRUN echo ${CTO_CUDA_APT}\r\nRUN apt-get install -y --no-install-recommends \\\r\n  time ${CTO_CUDA_APT} \\\r\n  && apt-get clean\r\n\r\n# Install TensorRT. Requires that libcudnn7 is installed\r\n#RUN apt-get install -y --no-install-recommends \\\r\n#  libnvinfer6=6.0.1-1+cuda${CTO_CUDA_VERSION} \\\r\n#  libnvinfer-dev=6.0.1-1+cuda${CTO_CUDA_VERSION} \\\r\n#  libnvinfer-plugin6=6.0.1-1+cuda${CTO_CUDA_VERSION} \\\r\n#  && apt-get clean\r\n\r\nENV LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\"\r\n\r\n# Install Python tools \r\nRUN pip3 install -U \\\r\n  mock \\\r\n  'numpy<1.19.0' \\\r\n  setuptools \\\r\n  six \\\r\n  wheel \\\r\n  && pip3 install 'future>=0.17.1' \\\r\n  && pip3 install -U keras_applications --no-deps \\\r\n  && pip3 install -U keras_preprocessing --no-deps \\\r\n  && rm -rf /root/.cache/pip\r\n\r\n## Download & Building TensorFlow from source\r\nARG LATEST_BAZELISK=1.5.0\r\nARG CTO_TENSORFLOW_VERSION=\"2.2.0\"\r\nRUN curl -s -Lo /usr/local/bin/bazel https://github.com/bazelbuild/bazelisk/releases/download/v${LATEST_BAZELISK}/bazelisk-linux-amd64 \\\r\n  && chmod +x /usr/local/bin/bazel \\\r\n  && mkdir -p /usr/local/src \\\r\n  && cd /usr/local/src \\\r\n  && wget -q --no-check-certificate https://github.com/tensorflow/tensorflow/archive/v${CTO_TENSORFLOW_VERSION}.tar.gz \\\r\n  && tar xfz v${CTO_TENSORFLOW_VERSION}.tar.gz \\\r\n  && mv tensorflow-${CTO_TENSORFLOW_VERSION} tensorflow \\\r\n  && rm v${CTO_TENSORFLOW_VERSION}.tar.gz \\\r\n  && cd /usr/local/src/tensorflow \\\r\n  && fgrep _TF_MAX_BAZEL configure.py | grep '=' | perl -ne 'print $1 if (m%\\=\\s+.([\\d\\.]+).$+%)' > .bazelversion\r\nRUN cd /usr/local/src/tensorflow \\\r\n  && TF_CUDA_CLANG=0 TF_CUDA_VERSION=${CTO_CUDA_VERSION} TF_CUDNN_VERSION=7 TF_DOWNLOAD_CLANG=0 TF_DOWNLOAD_MKL=0 TF_ENABLE_XLA=0 TF_NEED_AWS=0 TF_NEED_COMPUTECPP=0 TF_NEED_CUDA=1 TF_NEED_GCP=0 TF_NEED_GDR=0 TF_NEED_HDFS=0 TF_NEED_JEMALLOC=1 TF_NEED_KAFKA=0 TF_NEED_MKL=0 TF_NEED_MPI=0 TF_NEED_OPENCL=0 TF_NEED_OPENCL_SYCL=0 TF_NEED_ROCM=0 TF_NEED_S3=0 TF_NEED_TENSORRT=0 TF_NEED_VERBS=0 TF_SET_ANDROID_WORKSPACE=0 TF_CUDA_COMPUTE_CAPABILITIES=\"5.3,6.0,6.1,6.2,7.0,7.2,7.5\" GCC_HOST_COMPILER_PATH=$(which gcc) CC_OPT_FLAGS=\"-march=native\" PYTHON_BIN_PATH=$(which python) PYTHON_LIB_PATH=\"$(python -c 'import site; print(site.getsitepackages()[0])')\" ./configure\r\nRUN cd /usr/local/src/tensorflow \\\r\n  && time bazel build --verbose_failures --config=opt --config=v2 --config=cuda //tensorflow/tools/pip_package:build_pip_package \\\r\n  && time ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \\\r\n  && time pip3 install /tmp/tensorflow_pkg/tensorflow-*.whl\r\n\r\nCMD bash\r\n```\r\n\r\n", "Hi @mmartial. I build TF2.2 following the [official guide](https://www.tensorflow.org/install/source) without Dockerfile. Do you mean I should execute `fgrep _TF_MAX_BAZEL configure.py | grep '=' | perl -ne 'print $1 if (m%\\=\\s+.([\\d\\.]+).$+%)' > .bazelversion` before `bazel build`? I've tried it and then found `bazel build //tensorflow/tools/pip_package:build_pip_package` seemed to have no effect at all.", "> Hi @mmartial. I build TF2.2 following the [official guide](https://www.tensorflow.org/install/source) without Dockerfile. Do you mean I should execute `fgrep _TF_MAX_BAZEL configure.py | grep '=' | perl -ne 'print $1 if (m%\\=\\s+.([\\d\\.]+).$+%)' > .bazelversion` before `bazel build`? I've tried it and then found `bazel build //tensorflow/tools/pip_package:build_pip_package` seemed to have no effect at all.\r\n\r\nNo, I was referring to https://github.com/tensorflow/tensorflow/issues/40688#issuecomment-647791594\r\nWhen I saw your error, I saw the `PyUFuncGenericFunction` which was fixed by that call.\r\n\r\nNote that simply using `'numpy<1.19.0'` in my pip install was sufficient to solve this issue.", "Unfortunately it does not work for me. Maybe I have to open another issue.", "@amahendrakar this is still an issue on r2.3; i just tried to build tf branch r2.3 on my ubuntu system and ran into the same issue; the perl rewrite works, we should just fix the code to do a proper static cast.  @penpornk who's closest to this code?", "@ebrevdo This is Python glue code so it probably belongs to TF Core folks. But the fixes are simple enough. I can do it. ", "It seems @chsigg has already fixed this in https://github.com/tensorflow/tensorflow/commit/75ea0b31477d6ba9e990e296bbbd8ca4e7eebadf recently (Jun 26, 2020) by adding an overload function. I tried compiling with the latest code from master and didn't get the error anymore. \r\n\r\n(It's too late to patch this into releases 2.2.0 and 2.3.0 now, so this issue will be fixed in release 2.4.0.)", "Thank you for the update!\n\nOn Tue, Aug 4, 2020 at 5:40 PM Penporn Koanantakool <\nnotifications@github.com> wrote:\n\n> It seems @chsigg <https://github.com/chsigg> has already fixed this in\n> 75ea0b3\n> <https://github.com/tensorflow/tensorflow/commit/75ea0b31477d6ba9e990e296bbbd8ca4e7eebadf>\n> recently (Jun 26, 2020) by adding an overload function. I tried compiling\n> with the latest code from master and didn't get the error anymore.\n>\n> (It's too late to patch this into releases 2.2.0 and 2.3.0 now, so this\n> issue will be fixed in release 2.4.0.)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40688#issuecomment-668902870>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG73UXJSXZ36KTGUOW3R7CTBTANCNFSM4OE7JZYQ>\n> .\n>\n", "I will take this opportunity to update another part of the `perl`-glue (for the bazel version): since in 2.3.0 it looks like the max bazel version is now set to 3.99 (while the lastest bazel release is 3.4.1), I added the following version checking function to my 2.3.0 build (still in testing)\r\n\r\n<pre>\r\nARG LATEST_BAZEL=3.4.1\r\n[...]\r\n  && fgrep _TF_MAX_BAZEL configure.py | grep '=' | perl -ne '$lb=\"'${LATEST_BAZEL}'\";$brv=$1 if (m%\\=\\s+.([\\d\\.]+).$+%); sub numit{@g=split(m%\\.%,$_[0]);return(1000000*$g[0]+1000*$g[1]+$g[2]);}; if (&numit($brv) > &numit($lb)) { print \"$lb\" } else {print \"$brv\"};' > .bazelversion \\\r\n  && bazel clean \\\r\n[...]\r\n</pre>", "Remember to run **bazel clean** after downgrading numpy. I downloaded numpy 1.18 and it worked.", "Fixed by aafe25d", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40688\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40688\">No</a>\n", "Confirmed r2.2 source incompatible with Numpy version 1.18.5 and 1.19.0. Downgrade numpy < 1.18.5 will resolve the issues.\r\n\r\n`pip install numpy<1.18.5`"]}, {"number": 40687, "title": "tf.keras.activations.relu doesn't support fp16 through mixed precision for threshold greater than 0", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\ntf.keras.activations.relu doesn't support fp16 for threshold greater than 0. The documentation says it should honor the input dtype but defaults to float32 through floatx\r\n**Describe the expected behavior**\r\nShould cast to input dtype instead of floatx\r\n\r\n\r\nAddressing issue in pr #40685\r\n", "comments": ["@vishalsubbiah \r\nThe pr is in merged status, please let us know if we could move this to closed status.", "@Saduf2019  it can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40687\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40687\">No</a>\n"]}, {"number": 40686, "title": "saved_model_cli convert tensorrt adds unknown inputs to serving signature", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.4 \r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nsaved_model_cli convert \\\r\n    --dir /tmp/bert \\\r\n    --output_dir /tmp/bert-fp16 \\\r\n    --tag_set serve \\\r\n    tensorrt --precision_mode FP16\r\n```\r\nresults in the 100s additional inputs in the converted model `serving_default` signature.  \r\n\r\n```\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['input_mask'] tensor_info:\r\n        dtype: DT_INT32\r\n        shape: (-1, -1)\r\n        name: serving_default_input_mask:0\r\n    inputs['input_type_ids'] tensor_info:\r\n        dtype: DT_INT32\r\n        shape: (-1, -1)\r\n        name: serving_default_input_type_ids:0\r\n    inputs['input_word_ids'] tensor_info:\r\n        dtype: DT_INT32\r\n        shape: (-1, -1)\r\n        name: serving_default_input_word_ids:0\r\n    inputs['unknown'] tensor_info:\r\n        dtype: DT_RESOURCE\r\n        shape: ()\r\n        name: serving_default_unknown:0\r\n    inputs['unknown_0'] tensor_info:\r\n        dtype: DT_RESOURCE\r\n        shape: ()\r\n        name: serving_default_unknown_0:0\r\n    inputs['unknown_1'] tensor_info:\r\n        dtype: DT_RESOURCE\r\n        shape: ()\r\n        name: serving_default_unknown_1:0\r\n    inputs['unknown_10'] tensor_info:\r\n        dtype: DT_RESOURCE\r\n        shape: ()\r\n        name: serving_default_unknown_10:0\r\n    inputs['unknown_100'] tensor_info:\r\n        dtype: DT_RESOURCE\r\n        shape: ()\r\n        name: serving_default_unknown_100:0\r\n    inputs['unknown_101'] tensor_info:\r\n        dtype: DT_RESOURCE\r\n        shape: ()\r\n        name: serving_default_unknown_101:0\r\n    inputs['unknown_102'] tensor_info:\r\n        dtype: DT_RESOURCE\r\n        shape: ()\r\n        name: serving_default_unknown_102:0\r\n...\r\n```\r\n\r\n**Describe the expected behavior**\r\nConverted signature corresponds to the original SavedModel\r\n\r\n```\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['input_mask'] tensor_info:\r\n        dtype: DT_INT32\r\n        shape: (-1, -1)\r\n        name: serving_default_input_mask:0\r\n    inputs['input_type_ids'] tensor_info:\r\n        dtype: DT_INT32\r\n        shape: (-1, -1)\r\n        name: serving_default_input_type_ids:0\r\n    inputs['input_word_ids'] tensor_info:\r\n        dtype: DT_INT32\r\n        shape: (-1, -1)\r\n        name: serving_default_input_word_ids:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n...\r\n```\r\n**Standalone code to reproduce the issue**\r\nTry converting BERT from TF Hub\r\nhttps://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2", "comments": ["@ievgen-goichuk-rft \r\n\r\nThis issue is more suitable for TensorFlow Serving repo. Please post it on Serving repo from [here.](https://github.com/tensorflow/serving/issues) Thanks!", "OK. I have raised a https://github.com/tensorflow/serving/issues/1675 .", "@ievgen-goichuk-rft \r\n\r\nCan we close the issue here and track the issue in serving repo. Please, confirm.Thanks!", "@ravikyram Well, they closed an issue in tensorflow/serving saying I need to raise it either here or in tensorflow/tensorrt. And a similar issue was raised in there https://github.com/tensorflow/tensorrt/issues/202  but nobody from ternsorrt has confirmed so far it is their work.", "@ievgen-goichuk-rft  It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version 2.5 or 2.4.1 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40686\">No</a>\n"]}, {"number": 40685, "title": "supporting x dtype instead of only float 32 in K.relu", "body": "K.relu casts to floatx() by default when threshold is greater than 0. This fails when one sets mixed precision through `tf.keras.mixed_precision.experimental.set_policy` to `mixed_float16` or just pass an input of dtype float16\r\n\r\nChanging it to follow the documentation to use the inputs dtype instead.\r\n\r\n#40687\r\n cc @Saduf2019", "comments": ["@qlzh727  fixed the tests. Can you review again please?"]}, {"number": 40684, "title": "changed the Eigen Third Party library commit for bf16 ( TF2.3 )", "body": "changed the Eigen Third Party library commit for bf16 support ( TF2.3 )", "comments": ["The changes are in https://github.com/tensorflow/tensorflow/commit/8cf97846290cf7d8b95256fe3123abaaa8c8e553. Closing this PR now. :)"]}, {"number": 40683, "title": "Tensorflow will not recognize GPU", "body": "An truing to get TensorFlow to recognize that there is a GPU installed on the PC.  \r\nWindows10 Pro 64bit version\r\nNvidia GTX1660 TI with latest drivers \r\nTensorflow - 2.3.0-dev20200615\r\nCUDA v10.0\r\ncudnn 7.6.5.32\r\n\r\n`print(device_lib.list_local_devices())`\r\n```\r\nprint(device_lib.list_local_devices())\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 4283633287383166880\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 5791820499534504026\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 15245658627817172966\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n]\r\n```\r\nso system sees that there isa GPU installed, however \r\n\r\n```\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\nNum GPUs Available:  0\r\n```\r\nTrying to get a model to train using the GPU to complete faster - my current training takes 5 hours per epoch.....please help as its for a college project and i really need the speed of the GPU to help.\r\n", "comments": ["@Cillinc,\r\nAs per [this](https://github.com/tensorflow/tensorflow/issues/32967) similar issue installing the GPU package of TensorFlow fixed the issue. \r\n\r\nCould you please install TensorFlow using the command `pip install tf-nightly-gpu` and let us know if it works. Thanks!", "```\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\nNum GPUs Available:  1\r\n```\r\n\r\ntanks guys for the help - it's worknig now after :\r\n`pip install tf-nightly-gpu `"]}, {"number": 40682, "title": "Add Map Ops", "body": "Adding map ops and kernels.", "comments": []}, {"number": 40681, "title": "TFLite Int8 Quantization silently failing, resulting in np.float32 input dtypes in c++ library", "body": "Earlier reported on stackoverflow without feedback\r\n\r\n[https://stackoverflow.com/questions/59253566/quantized-input-and-output-with-tflite-in-tensorflow-2-0-0](https://stackoverflow.com/questions/59253566/quantized-input-and-output-with-tflite-in-tensorflow-2-0-0)\r\n\r\nThere appears to be an issue where a fully int8 quantized model still is detected by the c++ TFLite library interpreter to be a Float32 type.  This has a downstream issue when creating quantized models for microcontrollers, as the floating point data types are not fully supported.\r\n\r\nIt is documented [that an error should be thrown](https://www.tensorflow.org/lite/performance/post_training_integer_quant) when quantization fails, yet quantizing to uint8, for example using the following python completes without an error\r\n```python\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\nmodel_quant_int8_tflite = converter.convert()\r\nquantized_size_int8 = open(MODEL_QUANT_INT8_TFLITE, 'wb').write(model_quant_int8_tflite)\r\n```\r\n\r\nInspecting the resultant TFLite model with a simple iterative printing loop, \r\n```python\r\nprint(interpreter.get_input_details())\r\nprint(interpreter.get_output_details())\r\n\r\nfor l in interpreter.get_tensor_details():\r\n  print(f'Name: {l[\"name\"]}')\r\n  print(f' Index: {l[\"index\"]}')\r\n  print(f' Shape: {l[\"shape\"]}')\r\n  print(f' DType: {l[\"dtype\"]}')\r\n```\r\nshows that the layers are correctly converted to int8 quantization, however the input layer still has a dtype of numpy.float32\r\n```\r\n[{'name': 'conv2d_input', 'index': 23, 'shape': array([ 1, 49, 40,  1], dtype=int32), 'shape_signature': array([ 1, 49, 40,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n[{'name': 'Identity', 'index': 24, 'shape': array([ 1, 14], dtype=int32), 'shape_signature': array([ 1, 14], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\nName: conv2d_input_int8\r\n Index: 0\r\n Shape: [ 1 49 40  1]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/dense/BiasAdd/ReadVariableOp\r\n Index: 1\r\n Shape: [64]\r\n DType: <class 'numpy.int32'>\r\nName: sequential/dense_1/BiasAdd/ReadVariableOp\r\n Index: 2\r\n Shape: [14]\r\n DType: <class 'numpy.int32'>\r\nName: sequential/flatten/Const\r\n Index: 3\r\n Shape: [2]\r\n DType: <class 'numpy.int32'>\r\nName: sequential/batch_normalization/FusedBatchNormV3;sequential/conv2d/BiasAdd/ReadVariableOp;sequential/conv2d/BiasAdd;sequential/conv2d/Conv2D/ReadVariableOp;sequential/conv2d/Conv2D\r\n Index: 4\r\n Shape: [24]\r\n DType: <class 'numpy.int32'>\r\nName: sequential/batch_normalization_1/FusedBatchNormV3;sequential/conv2d_1/BiasAdd/ReadVariableOp;sequential/conv2d_1/BiasAdd;sequential/batch_normalization_2/FusedBatchNormV3;sequential/conv2d_2/BiasAdd/ReadVariableOp;sequential/conv2d_2/BiasAdd;sequential/conv2d_2/Conv2D/ReadVariableOp;sequential/conv2d_2/Conv2D;sequential/conv2d_1/Conv2D/ReadVariableOp;sequential/conv2d_1/Conv2D\r\n Index: 5\r\n Shape: [48]\r\n DType: <class 'numpy.int32'>\r\nName: sequential/activation_2/Relu;sequential/batch_normalization_2/FusedBatchNormV3;sequential/conv2d_2/BiasAdd/ReadVariableOp;sequential/conv2d_2/BiasAdd;sequential/conv2d_2/Conv2D/ReadVariableOp;sequential/conv2d_2/Conv2D\r\n Index: 6\r\n Shape: [48]\r\n DType: <class 'numpy.int32'>\r\nName: sequential/dense/MatMul\r\n Index: 7\r\n Shape: [ 64 384]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/dense_1/MatMul\r\n Index: 8\r\n Shape: [14 64]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/batch_normalization/FusedBatchNormV3;sequential/conv2d/BiasAdd/ReadVariableOp;sequential/conv2d/BiasAdd;sequential/conv2d/Conv2D/ReadVariableOp;sequential/conv2d/Conv2D1\r\n Index: 9\r\n Shape: [24  3  3  1]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/batch_normalization_1/FusedBatchNormV3;sequential/conv2d_1/BiasAdd/ReadVariableOp;sequential/conv2d_1/BiasAdd;sequential/conv2d_1/Conv2D/ReadVariableOp;sequential/conv2d_1/Conv2D\r\n Index: 10\r\n Shape: [48  3  3 24]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/batch_normalization_2/FusedBatchNormV3;sequential/conv2d_2/BiasAdd/ReadVariableOp;sequential/conv2d_2/BiasAdd;sequential/conv2d_2/Conv2D/ReadVariableOp;sequential/conv2d_2/Conv2D\r\n Index: 11\r\n Shape: [48  3  3 48]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/batch_normalization/FusedBatchNormV3;sequential/conv2d/BiasAdd/ReadVariableOp;sequential/conv2d/BiasAdd;sequential/conv2d/Conv2D/ReadVariableOp;sequential/conv2d/Conv2D2\r\n Index: 12\r\n Shape: [ 1 49 40 24]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/max_pooling2d/MaxPool\r\n Index: 13\r\n Shape: [ 1 12 20 24]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/activation/Relu\r\n Index: 14\r\n Shape: [ 1 12 20 24]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/batch_normalization_1/FusedBatchNormV3;sequential/conv2d_1/BiasAdd/ReadVariableOp;sequential/conv2d_1/BiasAdd;sequential/batch_normalization_2/FusedBatchNormV3;sequential/conv2d_2/BiasAdd/ReadVariableOp;sequential/conv2d_2/BiasAdd;sequential/conv2d_2/Conv2D/ReadVariableOp;sequential/conv2d_2/Conv2D;sequential/conv2d_1/Conv2D/ReadVariableOp;sequential/conv2d_1/Conv2D1\r\n Index: 15\r\n Shape: [ 1 12 20 48]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/max_pooling2d_1/MaxPool\r\n Index: 16\r\n Shape: [ 1  3 10 48]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/activation_1/Relu\r\n Index: 17\r\n Shape: [ 1  3 10 48]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/activation_2/Relu;sequential/batch_normalization_2/FusedBatchNormV3;sequential/conv2d_2/BiasAdd/ReadVariableOp;sequential/conv2d_2/BiasAdd;sequential/conv2d_2/Conv2D/ReadVariableOp;sequential/conv2d_2/Conv2D1\r\n Index: 18\r\n Shape: [ 1  1  8 48]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/flatten/Reshape\r\n Index: 19\r\n Shape: [  1 384]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/activation_3/Relu;sequential/dense/BiasAdd\r\n Index: 20\r\n Shape: [ 1 64]\r\n DType: <class 'numpy.int8'>\r\nName: sequential/dense_1/BiasAdd\r\n Index: 21\r\n Shape: [ 1 14]\r\n DType: <class 'numpy.int8'>\r\nName: Identity_int8\r\n Index: 22\r\n Shape: [ 1 14]\r\n DType: <class 'numpy.int8'>\r\nName: conv2d_input\r\n Index: 23\r\n Shape: [ 1 49 40  1]\r\n DType: <class 'numpy.float32'>\r\nName: Identity\r\n Index: 24\r\n Shape: [ 1 14]\r\n DType: <class 'numpy.float32'>\r\n```\r\n\r\nThis results in the interpreter failing while validating the input tensor, as the model_input->type != kTfLiteUInt8.\r\n```c++\r\n  // Get information about the memory area to use for the model's input.\r\n  model_input = interpreter->input(0);\r\n  if ((model_input->dims->size != 4) || (model_input->dims->data[0] != 1) ||\r\n      (model_input->dims->data[1] != kFeatureSliceCount) ||\r\n      (model_input->dims->data[2] != kFeatureSliceSize) ||\r\n      (model_input->type != kTfLiteUInt8)) {\r\n    error_reporter->Report(\"Bad input tensor parameters in model\");\r\n    return;\r\n  }\r\n```\r\nMay we please get a developer to comment on the likely causes of failed int8 quantization?  ", "comments": ["@victorromeo  can you share which tensorflow version you're using? You can print it in python by running\r\n`print(tf.__version__)`\r\n\r\nThe `inference_input_type` and `inference_output_type` is not supported in TensorFlow 2.2 and will be available soon in TensorFlow 2.3 (yet to be released). However, it is available in our nightly builds. To unblock yourself and use this feature right now, you can run the following commands in your colab/python script:\r\n```\r\n!pip uninstall tensorflow\r\n!pip install tf-nightly\r\nimport tensorflow as tf\r\n`print(tf.__version__)` # This should output: 2.3.0.dev20200622\r\n```", "Certainly.\r\n\r\nI'm using a Docker latest-gpu-jupyter image with Tensorflow 2.2.0\r\n```\r\ntensorflow/tensorflow:latest-gpu-jupyter\r\n```\r\nThis provides access to my local GPU (Num GPUs Available:  1)", "The conversion using 2.3.0 has produced a minor improvement, as it appears that int8 quantization is implemented on input and output layers.  I'm now finding that when the interpreter is run I get a QUANTIZE error now on layer index 11.\r\n\r\n```c++\r\nfor (size_t i = 0; i < subgraph_->operators()->size(); ++i) {\r\n    auto* node = &(node_and_registrations_[i].node);\r\n    auto* registration = node_and_registrations_[i].registration;\r\n\r\n    if (registration->invoke) {\r\n      TfLiteStatus invoke_status = registration->invoke(&context_, node);\r\n      if (invoke_status == kTfLiteError) {\r\n        TF_LITE_REPORT_ERROR(  // invoke_status == kTfLiteError, when i == 11 and OpNameFromRegistration(registration) == QUANTIZE\r\n            error_reporter_,  \r\n            \"Node %s (number %d) failed to invoke with status %d\",\r\n            OpNameFromRegistration(registration), i, invoke_status);\r\n        return kTfLiteError;\r\n      } else if (invoke_status != kTfLiteOk) {\r\n        return invoke_status;\r\n      }\r\n    }\r\n  }\r\n```\r\nThe tflite tensor details can be inspected to produce a simple output as follows.  I'm unsure where the QUANTIZE step is however, as it doesn't appear in the tensor_details list.  There does however appear to be a `numpy.Int32` layer there which hasn't been converted to Int8, which indicates that quantization is still failing as an operation is not yet supported.  Given this is targeting an Arduino Nano 33 BLE Sense device, I cannot run SELECT_TF_OPS.  Any thoughts on how to proceed? \r\n```\r\n[{'name': 'conv2d_input', 'index': 0, 'shape': array([ 1, 49, 40,  1], dtype=int32), 'shape_signature': array([-1, 49, 40,  1], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.10217524319887161, -128), 'quantization_parameters': {'scales': array([0.10217524], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n[{'name': 'Identity', 'index': 22, 'shape': array([ 1, 14], dtype=int32), 'shape_signature': array([-1, 14], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\nLayer: 0 conv2d_input [ 1 49 40  1] <class 'numpy.int8'>\r\nLayer: 1 sequential/dense/BiasAdd/ReadVariableOp/resource [64] <class 'numpy.int32'>\r\nLayer: 2 sequential/dense_1/BiasAdd/ReadVariableOp/resource [14] <class 'numpy.int32'>\r\nLayer: 3 sequential/flatten/Const [2] <class 'numpy.int32'>\r\nLayer: 4 sequential/activation_2/Relu;sequential/batch_normalization_2/FusedBatchNormV3;sequential/conv2d_2/BiasAdd/ReadVariableOp/resource;sequential/conv2d_2/BiasAdd;sequential/conv2d_2/Conv2D [48] <class 'numpy.int32'>\r\nLayer: 5 sequential/dense/MatMul [ 64 384] <class 'numpy.int8'>\r\nLayer: 6 sequential/dense_1/MatMul [14 64] <class 'numpy.int8'>\r\nLayer: 7 sequential/conv2d/Conv2D [24  3  3  1] <class 'numpy.int8'>\r\nLayer: 8 sequential/conv2d_1/Conv2D [48  3  3 24] <class 'numpy.int8'>\r\nLayer: 9 sequential/conv2d_2/Conv2D [48  3  3 48] <class 'numpy.int8'>\r\nLayer: 10 sequential/batch_normalization/FusedBatchNormV3;sequential/conv2d/BiasAdd/ReadVariableOp/resource;sequential/conv2d/BiasAdd [24] <class 'numpy.int32'>\r\nLayer: 11 sequential/batch_normalization_1/FusedBatchNormV3;sequential/conv2d_1/BiasAdd/ReadVariableOp/resource;sequential/conv2d_1/BiasAdd [48] <class 'numpy.int32'>\r\nLayer: 12 sequential/batch_normalization/FusedBatchNormV3;sequential/conv2d/BiasAdd/ReadVariableOp/resource;sequential/conv2d/BiasAdd;sequential/conv2d/Conv2D [ 1 49 40 24] <class 'numpy.int8'>\r\nLayer: 13 sequential/max_pooling2d/MaxPool [ 1 12 20 24] <class 'numpy.int8'>\r\nLayer: 14 sequential/activation/Relu [ 1 12 20 24] <class 'numpy.int8'>\r\nLayer: 15 sequential/batch_normalization_1/FusedBatchNormV3;sequential/conv2d_1/BiasAdd/ReadVariableOp/resource;sequential/conv2d_1/BiasAdd;sequential/conv2d_2/Conv2D;sequential/conv2d_1/Conv2D [ 1 12 20 48] <class 'numpy.int8'>\r\nLayer: 16 sequential/max_pooling2d_1/MaxPool [ 1  3 10 48] <class 'numpy.int8'>\r\nLayer: 17 sequential/activation_1/Relu [ 1  3 10 48] <class 'numpy.int8'>\r\nLayer: 18 sequential/activation_2/Relu;sequential/batch_normalization_2/FusedBatchNormV3;sequential/conv2d_2/BiasAdd/ReadVariableOp/resource;sequential/conv2d_2/BiasAdd;sequential/conv2d_2/Conv2D1 [ 1  1  8 48] <class 'numpy.int8'>\r\nLayer: 19 sequential/flatten/Reshape [  1 384] <class 'numpy.int8'>\r\nLayer: 20 sequential/activation_3/Relu;sequential/dense/BiasAdd [ 1 64] <class 'numpy.int8'>\r\nLayer: 21 sequential/dense_1/BiasAdd [ 1 14] <class 'numpy.int8'>\r\nLayer: 22 Identity [ 1 14] <class 'numpy.int8'>\r\n```", "@victorromeo Could you share the code you run which caused the QUANTIZE error? Along with the error details?\r\n\r\nFor an int8 quantized model, the biases are always of type int32 - this is expected.\r\nWhy? Let's say that an op output =  weight * op input + bias. The product of two int8 operations (weight and op input) and an addition requires a int32 register. Hence, the range of values that the bias term would have is also in the int32 range. You can read more about it [https://www.tensorflow.org/lite/performance/quantization_spec]\r\n\r\n\r\nThe should be no quantize and dequantize op in the input and output layer.\r\nWhy? When you int8 quantize the model it changes as follows:\r\nInitial Model: [float32] --> [quantize] --> [int8] -->.. [int8]....[int8] --> [dequantize] -> [float32]\r\nFinal Model:                                                 [int8] -->.. [int8]....[int8] \r\n\r\nThe float input, input quantize op (float32-->int8), output quantize op (int8-->float32) and float output are REMOVED. The user is now expected to *manually* quantize the inputs to int8 and and dequantize the outputs from int8. Refer to the \"Inference\" section in the first comment in this github issue to learn how you can manually quantize and dequantize the inputs and outputs. [https://github.com/tensorflow/tensorflow/issues/38285] *Note: This is in Python, you may need to write a c++ equivalent*", "@MeghnaNatraj Thanks for your valuable reply.  \r\n\r\nThe model I'm quantizing is available in the following Gist [here](https://gist.github.com/victorromeo/9b7ff6d0ca8cf6283e446b3d8a7808bf) and the training and quantization is [here](https://gist.github.com/victorromeo/144fb97b1b90a6ec38fa3396b5527923).  The QUANTIZE error is created when using Default optimizations and int8 inference types.\r\n\r\nI have found that the `2.3.0.dev20200622` revision has resolved the quantization error, when I selected the correct conversion arguments.\r\n\r\nAs I'm trying to make a drop in alternate model to TinyML using SB-CNN for the 'micro_speech' example, I'm planning on using the TFLite flat buffer created using OPTIMIZE_FOR_LATENCY an int8 quantization (Size = 66992 bytes) (Accuracy = 0.9420).\r\n\r\nI note however that micro_speech assumes an input inference type of int8 and an output inference type of uint8.  Is this considered hybrid quantization?  Assuming the developer is responsible for input quantization and output dequantization, has this policy changed since TFLite 1.x?\r\n\r\nI also noted yesterday in my Jupyter logs that TFLite conversion reported that some quantization was skipped as there were insufficient weights.  Given the target to drop to Arduino with int8 quantization, is there some way this conversion can be forced? If not, won't this cause the TFLite flat buffer to fail on the device?\r\n\r\n", "When I use \r\n```\r\n    converter.representative_dataset = representative_dataset\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_type = tf.int8\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.uint8\r\n```\r\n\r\nThe QUANTIZE error is raised, as the TFLite library doesn't have an implementation for int8 -> uint8 conversion.\r\n\r\n![Screenshot from 2020-06-24 12-35-28](https://user-images.githubusercontent.com/3838993/85491680-4cf74d00-b617-11ea-80e3-76dca8becbd4.png)\r\n\r\nSo, I'm going to revert to using the following then, perform manual dequantization on the final output result, before evaluating. \r\n```\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n```", "@victorromeo is it possible to update your progress and let us know what issue you're facing right now so we can look into it?", "@victorromeo \r\nPlease update a s per above comment.", "Understood.  Update to be provided early this week.", "Using the tf-nightly release 2_4_0_dev20200712, tflite conversion was successfully implemented and quantized.  Inference is running, albeit slowly (5-7 seconds).\r\n\r\n```python\r\n!pip uninstall tensorflow\r\n!pip install tf-nightly\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\n...\r\n\r\ndef representative_dataset():\r\n  for i in range(len(x_test)):\r\n    yield([x_test[i:i+1,:,:,:]])\r\n...\r\n\r\ntf_version = tf.__version__.replace('.','_').replace('-','_')\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/model_sbcnn_yn')\r\n\r\nconverter.representative_dataset = representative_dataset\r\n\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\ntflite_model = converter.convert()\r\n\r\nwith open(f'model_sbcnn_yn_{tf_version}_int8.tflite', 'wb') as f:\r\n  bytes_written = f.write(tflite_model)\r\n  print(bytes_written)\r\n```\r\n\r\nI can confirm, I am no longer getting float32 dtypes in the input and output tensors."]}, {"number": 40680, "title": "Segmentation fault when running TFLite's benchmark_model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin Kernel Version 19.5.0(for model conversion, Linux 4.9.140-tegra for running the benchmark tool)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Installed TFLite benchmark_model tool using the instructions given here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark.\r\n- TensorFlow version (use command below): N/A\r\n- Python version: N/A\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI converted emotion_ferplus (https://github.com/onnx/models/tree/master/vision/body_analysis/emotion_ferplus) ONNX model to TF using onnx-tensorflow(https://github.com/onnx/onnx-tensorflow) and then converted the TF model to TFLite using tflite_convert.\r\n```\r\n$ onnx-tf convert -i model.onnx -o emotion.pb\r\n$ tflite_convert --enable_v1_converter --graph_def_file=emotion.pb --output_file=emotion.tflite --output_format=TFLITE --input_shape=1,1,64,64 --input_arrays=Input3 --output_arrays=Plus692_Output_0 --inference_type=FLOAT --input_data_type=FLOAT\r\n```\r\nThen I ran benchmark_model tool on the tflite model I got from the previous step.\r\n```\r\nonnxruntime@onnxruntime-desktop:~/Documents/tensorflow$ bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=emotion.tflite --num_runs=10 --num_threads=4\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nMin num runs: [10]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [0]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [emotion.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nInput value ranges: []\r\nInput layer values files: []\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [0]\r\nMax profiling buffer entries: [1024]\r\nCSV File to export profiling data to: []\r\nEnable platform-wide tracing: [0]\r\n#threads used for CPU inference: [0]\r\nMax number of delegated partitions : [0]\r\nMin nodes per partition : [0]\r\nExternal delegate path : []\r\nExternal delegate options : []\r\nUse gpu : [0]\r\nUse xnnpack : [0]\r\nLoaded model ../models/tflite/emotion.tflite\r\nThe input model file size (MB): 35.0461\r\nInitialized session in 1.645ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\nSegmentation fault (core dumped)\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\nbenchmark tool should run without crashing and give model performance metrics like the following:\r\n```\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=10 first=101249 curr=46906 min=46491 max=101249 avg=52839.8 std=16163\r\n\r\nRunning benchmark for at least 10 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=22 first=46543 curr=46554 min=46473 max=49668 avg=46957.8 std=627\r\n\r\nInference timings in us: Init: 218485, First inference: 101249, Warmup (avg): 52839.8, Inference (avg): 46957.8\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=1.23047 overall=28.8906\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi Can you attach the generated TF lite model? Thanks!", "> Hi Can you attach the generated TF lite model? Thanks!\r\n\r\nHow do you suggest I do that? I can't attach a TFLite model here. Also, if you follow the steps mentioned above, you can get the TFLite model.", "The dialog box I am typing in now says \"Attach files by dragging and dropping\":\r\n\r\n<img width=\"530\" alt=\"Screen Shot 2020-06-23 at 7 48 18 AM\" src=\"https://user-images.githubusercontent.com/1891418/85418755-0675f580-b526-11ea-99e9-7551fa77677f.png\">\r\n\r\n\r\nI think this should work for the model.", "You can't attach a TFLite model here(unsupported file type).", "I tried creating a zip and attaching it, but the zipped version is 31 MB and there is a 10 MB limit here.", "Ah I see. I created the `emotion.pb` file by using `onnx-tf` released version with TF 1.15. I was unable to convert this tflite_convert. What version of TF did you use for TF->TFL conversion? Can you execute the instructions in the template so we can get version information?", "```\r\n(onnxrt_env) prroy-Mac:emotion_ferplus prroy$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.15.0-92-g5d80e1e8e6 1.15.2\r\n(onnxrt_env) prroy-Mac:emotion_ferplus prroy$ tflite_convert --enable_v1_converter --graph_def_file=emotion.pb --output_file=emotion.tflite --output_format=TFLITE --input_shape=1,1,64,64 --input_arrays=Input3 --output_arrays=Plus692_Output_0 --inference_type=FLOAT --input_data_type=FLOAT\r\n2020-06-24 13:49:23.852168: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-24 13:49:23.863606: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbd4d6ecc90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-24 13:49:23.863622: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-24 13:49:24.062042: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-06-24 13:49:24.062143: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-24 13:49:24.500949: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-06-24 13:49:24.500974: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 175 nodes (-86), 174 edges (-56), time = 243.756ms.\r\n2020-06-24 13:49:24.500979: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 175 nodes (0), 174 edges (0), time = 86.759ms.\r\n```", "thanks! I was able to successfully convert the model. When I visualized it with `tensorflow/lite/tools:visualize` I see that the model has int64 inputs to the ADD op:\r\n\r\n<img width=\"1396\" alt=\"Screen Shot 2020-06-24 at 2 34 08 PM\" src=\"https://user-images.githubusercontent.com/1891418/85630269-33aacc80-b628-11ea-9e03-f57bfba880cc.png\">\r\n\r\nThis is not supported in TF Lite, so when I run`benchmark_model` I get this error:\r\n\r\n```\r\nERROR: Inputs and outputs not all float|uint8|int16 types.\r\nERROR: Node number 3 (ADD) failed to invoke.\r\n```\r\n\r\nI converted the model successfully with `--target_ops=SELECT_TF_OPS` and then ran with `tensorflow/lite/tools/benchmark:benchmark_model_plus_flex`:\r\n\r\n```\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\nINFO: TfLiteFlexDelegate delegate: 102 nodes delegated out of 102 nodes with 1 partitions.\r\n\r\nThe input model file size (MB): 35.0607\r\nInitialized session in 34.002ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=16 first=51470 curr=29156 min=28983 max=51470 avg=31982 std=5605\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=29166 curr=29189 min=28529 max=29906 avg=28935.9 std=255\r\n\r\nInference timings in us: Init: 34002, First inference: 51470, Warmup (avg): 31982, Inference (avg): 28935.9\r\n```\r\n\r\n\r\n\r\nBy the way, I think it is best in this case to use the same version of `benchmark_model` as you used for the ONNX conversion. Apparently their toolchain requires TF 1.15, so I did a `git checkout` to release 1.15.2 before building and running benchmark_model. Perhaps there is a way to tell ONNX that the largest int type should be `int32`? I am not very familiar with the tool. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40680\">No</a>\n", "> thanks! I was able to successfully convert the model. When I visualized it with `tensorflow/lite/tools:visualize` I see that the model has int64 inputs to the ADD op:\r\n> \r\n> <img alt=\"Screen Shot 2020-06-24 at 2 34 08 PM\" width=\"1396\" src=\"https://user-images.githubusercontent.com/1891418/85630269-33aacc80-b628-11ea-9e03-f57bfba880cc.png\">\r\n> \r\n> This is not supported in TF Lite, so when I run`benchmark_model` I get this error:\r\n> \r\n> ```\r\n> ERROR: Inputs and outputs not all float|uint8|int16 types.\r\n> ERROR: Node number 3 (ADD) failed to invoke.\r\n> ```\r\n> \r\n> I converted the model successfully with `--target_ops=SELECT_TF_OPS` and then ran with `tensorflow/lite/tools/benchmark:benchmark_model_plus_flex`:\r\n> \r\n> ```\r\n> INFO: Created TensorFlow Lite delegate for select TF ops.\r\n> INFO: TfLiteFlexDelegate delegate: 102 nodes delegated out of 102 nodes with 1 partitions.\r\n> \r\n> The input model file size (MB): 35.0607\r\n> Initialized session in 34.002ms.\r\n> Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n> count=16 first=51470 curr=29156 min=28983 max=51470 avg=31982 std=5605\r\n> \r\n> Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n> count=50 first=29166 curr=29189 min=28529 max=29906 avg=28935.9 std=255\r\n> \r\n> Inference timings in us: Init: 34002, First inference: 51470, Warmup (avg): 31982, Inference (avg): 28935.9\r\n> ```\r\n> \r\n> By the way, I think it is best in this case to use the same version of `benchmark_model` as you used for the ONNX conversion. Apparently their toolchain requires TF 1.15, so I did a `git checkout` to release 1.15.2 before building and running benchmark_model. Perhaps there is a way to tell ONNX that the largest int type should be `int32`? I am not very familiar with the tool.\r\n\r\nI didn't get that error when running benchmark_model. The tool itself crashed with segmentation fault. I can retry everything on latest TF version.", "I upgraded TF version and saw the error:\r\nERROR: tensorflow/lite/kernels/add.cc:326 Type INT64 is unsupported by op Add.\r\nERROR: Node number 5 (ADD) failed to invoke.\r\n\r\nShouldn't tflite_convert fail in such cases?\r\n\r\n", "> I upgraded TF version and saw the error:\r\n> ERROR: tensorflow/lite/kernels/add.cc:326 Type INT64 is unsupported by op Add.\r\n> ERROR: Node number 5 (ADD) failed to invoke.\r\n> \r\n> Shouldn't tflite_convert fail in such cases?\r\n\r\nDid you solve it, I have the same problem.", "> > I upgraded TF version and saw the error:\r\n> > ERROR: tensorflow/lite/kernels/add.cc:326 Type INT64 is unsupported by op Add.\r\n> > ERROR: Node number 5 (ADD) failed to invoke.\r\n> > Shouldn't tflite_convert fail in such cases?\r\n> \r\n> Did you solve it, I have the same problem.\r\n\r\nNo, it didn't work for me."]}, {"number": 40679, "title": "Python 3.6? 3.8? Catalina install, R error 951, 951, 2290, 2649", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): nope\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.5 on MacBook Pro (Retina, 13-inch, Early 2015) 2.7 GHz Dual-Core Intel Core i5 8 GB 1867 MHz DDR3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version:  3.8.3 \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nBelow is the full code. When I run the 489 line the following error appears.\r\nI have installed Python from terminal using  your guide [here](https://www.tensorflow.org/install/pip). I have installed KERAS for R and it installed python 3.6 in its own bin, all the files are there. \r\n\r\nI'm a bit confused as I'm not sure which version I should use here and where. I would start with a fresh laptop install if needed to make this work. I have similar issues on Windows tho (another issue already discussed).\r\n\r\nAny idea? Error message below here\r\n\r\nEpoch 1/50\r\n7/7 [==============================] - ETA: 0s - loss: 0.5326 - categorical_accuracy: 0.7778\r\n Error in py_call_impl(callable, dots$args, dots$keywords) : \r\n  ValueError: in user code:\r\n\r\n    /Users/axeldrioli/Library/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:941 test_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /Users/axeldrioli/Library/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /Users/axeldrioli/Library/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /Users/axeldrioli/Library/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /Users/axeldrioli/Library/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages/tensorflow/python/keras/e \r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n---\r\n  title: \"Detection, Extraction and Classification of Bird and Bat Vocalizations in R\"\r\nauthor: \"Francois Fabianek, Jean Marchal\"\r\ndate: \"2018 October 30th\"\r\noutput:\r\n  rmarkdown::html_vignette:\r\n  fig_caption: yes\r\nnumber_sections: yes\r\ntoc: yes\r\ntoc_depth: 4\r\nrmarkdown::pdf_document:\r\n  fig_caption: yes\r\nnumber_sections: yes\r\ntoc: yes\r\ntoc_depth: 4\r\nvignette: >\r\n  %\\VignetteIndexEntry{Tutorial: Detection, Extraction and Classification of Bird and Bat Vocalizations in R} \r\n%\\VignetteEncoding{UTF-8}\r\n%\\VignetteEngine{knitr::knitr} \r\n---\r\n  \r\n  _______\r\n_______\r\n\r\n# Load the necessary packages\r\n\r\nMake sure you have the latest version of the packages:\r\n  \r\n  ```{r install_packages, message=FALSE, eval=FALSE}\r\ninstall.packages(\"bioacoustics\")\r\n# The bioacoustics package may also be installed from GitHub using devtools as follows:\r\ndevtools::install_github(\"wavx/bioacoustics\", build_vignettes = TRUE)\r\ninstall.packages(\"warbleR\")\r\ninstall.packages(\"dplyr\")\r\ninstall.packages(\"magrittr\")\r\ninstall.packages(\"randomForest\")\r\n```\r\n\r\n```{r load_packages, message=FALSE, eval=FALSE}\r\n# Load the packages\r\nlibrary(warbleR)\r\nlibrary(bioacoustics)\r\nlibrary(dplyr)\r\nlibrary(tools)\r\nlibrary(randomForest)\r\nlibrary(magrittr)\r\n```\r\n\r\n\r\n___________________\r\n\r\n# Load audio files\r\n\r\nWe can use the `quer_xc()` function from **warbleR** to download bird vocalizations from Xeno-Canto: <https://www.xeno-canto.org/>\r\n  \r\n  We are going to choose calls from *Catharus bicknelli*, songs from *Passerella iliaca* and *Setophaga magnolia* recorded in the United States and Canada.\r\n\r\nWe will filter only \"A\" quality recordings, then, pick up only the first nine, and merge all the metadata into a single \"df\" data frame. This data frame will be used to download MP3 files in your working directory directly from Xeno-Canto with the `quer_xc()` function:\r\n  \r\n  ```{r xeno1, message=FALSE, eval=FALSE}\r\ndf1 = quer_xc(qword ='Catharus bicknelli type:call cnt:\"United States\"', download = FALSE)\r\ndf1 = df1[df1$Vocalization_type==\"call\",]\r\ndf1 = df1[df1$Quality==\"A\",]\r\ndf1 = df1[1:9,]\r\n\r\ndf2 = quer_xc(qword ='Setophaga magnolia type:song cnt:\"Canada\"', download = FALSE)\r\ndf2 = df2[df2$Quality==\"A\",]\r\ndf2 = df2[1:9,]\r\n\r\ndf3 = quer_xc(qword ='Passerella iliaca type:song cnt:\"Canada\"', download = FALSE)\r\ndf3 = df3[df3$Vocalization_type==\"song\",]\r\ndf3 = df3[df3$Quality %in% c(\"A\", \"B\"),]\r\ndf3 = df3[1:9,]\r\ndf3 <- df3 %>%\r\n  select(-(\"Other_species8\"))\r\ndf3\r\ndf = rbind(df1,df2,df3)\r\nrm(df1,df2,df3)\r\n```\r\n\r\n```{r xeno4, eval=FALSE}\r\n# Visualize your data frame\r\nView(df)\r\n# We will work in the R temp directory\r\nwd <- tempdir()\r\n# Create a data directory if it does not exist\r\ndata_dir <- file.path(wd, \"data\")\r\nif(!dir.exists(data_dir))\r\n  dir.create(data_dir)\r\n# Download the MP3 files into your data directory\r\nquer_xc(X = df, download = TRUE, path = data_dir)\r\n\r\n\r\n```\r\n\r\nNow that we have recordings stored in the data directory, we can read one of them to look at its structure and content.\r\n\r\n```{r read_audio, eval=TRUE}\r\n\r\nCATBIC <- read_audio(file.path(data_dir, \"Catharus-bicknelli-54866.mp3\"))\r\nCATBIC\r\n\r\n```\r\n\r\nWe can see that the MP3 file has been converted into a Wave object with 7551360 samples, a duration of 157.32 seconds, a sampling rate of 48000 Hz, a bit depth of 16 bits, and that it contains one channel (mono, stereo being two channels).\r\n\r\nRemember that you just have to divide the number of samples by the sampling rate to retrieve the duration (s) of an audio file.\r\n\r\n___________________\r\n\r\n# Extract GUANO metadata\r\n\r\n[GUANO](https://guano-md.org/) stands for the \"Grand Unified Acoustic Notation Ontology\" and means to be a universal, extensible, open metadata format for bat (ultrasonic) and non-bat (audible, infrasonic) acoustic recordings [more here](https://github.com/riggsd/guano-r). GUANO format is now embeded directly in the WAV files generated from most Pettersson, Wildlife Acoustics, Titley Scientific acoustic recorders. It is possible to extract the metadata and GUANO embedded in the WAV file by using the `metadata()` function.\r\n\r\n```{r metadata,eval=FALSE}\r\nmetadata(CATBIC)\r\n```\r\n<br>\r\n\r\n\r\nNow that we have explored a WAV file, we will use the Fast Fourrier Transform (FFT) to compute a frequency-time representation of the recording, called a spectrogram. A spectrogram being the representation of the spectrum of frequencies in a recording as they vary through time. This representation, although not optimal, is still commonly used to detect animal vocalizations and extract acoustic features useful for classification with the purpose of animal identification.\r\n___________________\r\n\r\n# Plot audio files\r\n\r\nThere are several options to display animal vocalizations in audio files with R. You can use both `spectro()` or `fspec()` functions to generate spectrograms with **bioacoustics**. `fspec()` generates only a matrix of the spectrogram, and thus has to be used with the `image()` function to display the spectrogram. It is also possible to use the `spectro()` function in **Seewave**.  \r\n\r\nNext, we will search manually and display an audio event (here, a bird vocalization) from a recording of *Catharus bicknelli*. To display a Region Of Interest (ROI) of the recording we will use temporal and frequency filters. Lets start with a temporal slice from 1 to 10 secs and a FFT size of 512 sample\r\n\r\n```{r spectro0, eval=FALSE}\r\n# Set plot margins to 0\r\npar(mar = c(0, 0, 0, 0), oma = c(0, 0, 0, 0))\r\n# Display with spectro()\r\nticks <- c(from = 1000, to = 20000, by = 1000) # frequency tick marks from 1 to \r\n# 20 kHz, and steps at each 1 kHz\r\ntemp_slice <- c(from = 1, to = 10) # in seconds\r\nspectro(CATBIC, tlim = temp_slice, FFT_size = 512, ticks_y = ticks)\r\n```  \r\n\r\nLets display spectrograms with various time / frequency limits (with `tlim=` and `flim=` arguments). You can also play with other arguments in `spectro()` and `fspec()` functions such as the percent of overlap between two FFTs (with `FFT_overlap=`) and various FFT resolutions (with `FFT_size=`). Note that the arguments are briefly explained in the documentation of each function:  \r\n  \r\n```{r help, eval=FALSE}\r\n# Access the arguments of the spectro function\r\n?spectro\r\n?fspec\r\n```\r\n\r\nFirst, lets shorten the temporal axis from 2 to 3.5 secs to work on a shorter time window and compare the outputs from `spectro()` and `fspec()`  functions. Note that spectrograms can also be generated automatically while using the detection functions from **bioacoustics**. We will explore that in details in section 4.1.\r\n\r\n```{r spectro1, eval=FALSE}\r\n# Set plot margins to 0\r\npar(mar = c(0, 0, 0, 0), oma = c(0, 0, 0, 0))\r\n# Display the spectrogram with spectro()\r\nticks <- c(from = 1000, to = 20000, by = 1000) # frequency tick marks from 1 to \r\n# 20 kHz, 1 kHz steps\r\ntemp_slice <- c(from = 2, to = 3.5) # in seconds\r\nspectro(CATBIC, tlim = temp_slice, FFT_size = 512, ticks_y = ticks)\r\n# fspec() gives you the spectrogram matrix with energy values (dB)\r\nspec_mx <- fspec(CATBIC, tlim = temp_slice, FFT_size = 512, rotate = TRUE)\r\n# You can display the spectrogram with image()\r\nimage(spec_mx, xaxt = \"n\", yaxt = \"n\") \r\n```\r\n\r\nThe tick marks on the (frequency) y-axis were defined in the `spectro()` function starting from 1 to 20 kHz with an interval at each 1 kHz. The FFT size was 512 samples with an overlap between two FFT windows set by default at 0.875. Now try these settings: FTT size = 256, 1024 and 2048; FFT overlap = 0.3, 0.6, 0.9...\r\n\r\nAnother interesting thing to perform with the fspec outputs, is to implement your own set of filters. Lets try to reduce the background noise from a spectrogram with a narrower time and frequency bandwidth:\r\n\r\n```{r filter, eval=FALSE}\r\ntemp_slice <- c(from = 2.5, to = 3.5)\r\nfreq_slice <- c(from = 1500, to = 20000)\r\nspec_o <- fspec(CATBIC, tlim = temp_slice, flim = freq_slice, FFT_size = 512, rotate = TRUE)\r\n## min and max (range) dB intensity\r\nrange(spec_o) # -120 (min) to 0 dB (max)\r\n# Note that the tolerance of your recorders depends on the number of bits. \r\n# 16-bit recorders offer only around -96 dB tolerance and sound pressure above\r\n# this level is clipped to 0 dB.\r\n## Let's try a filter by mean + sd intensity\r\nspec_f <- fspec(CATBIC, tlim = temp_slice, flim = freq_slice, FFT_size = 512, rotate = TRUE)\r\nspec_f[spec_f < mean(spec_f) + sd(spec_f)] <- -120\r\n# Works well with high intensity audio events, but leads to\r\n# false negatives (missed events) otherwise.\r\npar(mar = c(0, 0, 0, 0), oma = c(0, 0, 0, 0))\r\nimage(spec_o, xaxt=\"n\", yaxt=\"n\")\r\nimage(spec_f, xaxt=\"n\", yaxt=\"n\")\r\n```\r\n\r\n___________________\r\n\r\n# The use of filters to detect and extract audio events\r\n\r\nThe functions used to detect and extract audio events in a recording also rely on \"generic\" filters based on frequency and duration, along with other \"specific\" filters. Let's take a quick look at the generic filters available in the `threshold_detection()` and `blob_detection()` functions:\r\n\r\n  * High Pass (HPF) and Low Pass filters (LPF) can be employed to reduce the amount of unwanted noise in the recording or to track particular audio events within a narrower frequency bandwidth than the recording sampling rate. Frequencies below the HPF and above the LPF cutoff are greatly attenuated. These frequency filters can be set using the `HPF=` and `LPF=` arguments in the `threshold_detection()` and `blob_detection()` functions.\r\n  * Minimum and maximum duration of an audio event, and a minimum time between two audio events also help reduce the amount of unwanted noise, or track a particular audio event within a narrower temporal window. These temporal filters can be set using the `min_dur=`, `max_dur=`, and `TBE=` arguments in both `threshold_detection()` and `blob_detection()` functions.\r\n  * Other set of filters are specific to each detection function and will be defined while working with these functions on bird vocalizations.\r\n\r\n___________________\r\n\r\n## Detect and extract audio events in a recording\r\n\r\n### Threshold detection\r\n\r\nLet's start with the `threshold_detection()` function on a recording containing calls from *Catharus bicknelli*. This function is an amplitude threshold detector that picks up audio events above the Signal to Noise Ratio (SNR). It combines several algorithms for detection, filtering and audio feature extraction. We will play with the arguments of this function to understand their implication in the detection and extraction of audio events (here, calls of *Catharus bicknelli*).  \r\n\r\n```{r threshold_help, eval=FALSE}\r\n# Access the arguments of the threshold_detection function\r\n?threshold_detection\r\n```\r\n\r\n```{r threshold1, eval=FALSE}\r\n# Set each argument according to the targeted audio events\r\nTD <- threshold_detection(\r\n  CATBIC, # Either a path to an audio file (see ?read_audio), or a Wave object\r\n  threshold = 12, # 12 dB SNR sensitivity for the detection algorithm\r\n  time_exp = 1, # Time expansion factor of 1. Only needed for bat recordings.\r\n  min_dur = 140, # Minimum duration threshold of 140 milliseconds (ms)\r\n  max_dur = 440, # Maximum duration threshold of 440 ms\r\n  min_TBE = 10, # Minimum time window between two audio events of 10 milliseconds\r\n  max_TBE = 5000, # Maximum time window between two audio events, here 5 seconds\r\n  EDG = 0.996, # Temporal masking with Exponential Decay Gain from 0 to 1\r\n  LPF = 10000, # Low-Pass Filter of 10 kHz\r\n  HPF = 1000, # High-Pass Filter of 1 kHz\r\n  FFT_size = 256, # Size of the Fast Fourrier Transform (FFT) window\r\n  FFT_overlap = 0.875, # Percentage of overlap between two FFT windows\r\n  \r\n  start_thr = 25, # 25 dB threshold at the start of the audio event\r\n  end_thr = 30, # 30 dB threshold at the end of the audio event\r\n  SNR_thr = 10, # 10 dB SNR threshold at which the extraction of the audio event stops\r\n  angle_thr = 45, # 45\u00b0 of angle at which the extraction of the audio event stops\r\n  duration_thr = 440, # Noise estimation is resumed after 440 ms\r\n  NWS = 1000, # Time window length of 1 s used for background noise estimation\r\n  KPE = 1e-05, # Process Error parameter of the Kalman filter (for smoothing)\r\n  KME = 1e-04, # Measurement Error parameter of the Kalman filter (for smoothing)\r\n  settings = FALSE, #  Save on a list the above parameters set with this function\r\n  acoustic_feat = TRUE, # Extracts the acoustic and signal quality parameters \r\n  metadata = FALSE, # Extracts on a list the metadata embedded with the Wave file\r\n  spectro_dir = file.path(tempdir(), \"Spectros\"), # Directory where to save the spectrograms\r\n  time_scale = 1, # Time resolution of 2 ms for spectrogram display\r\n  ticks = TRUE # Tick marks and their intervals are drawn on the y-axis (frequencies) \r\n) \r\n# Get the number of extracted audio events\r\nnrow(TD$data$event_data)\r\n```\r\n\r\nLet the HTML page open with the 57 spectrograms (each representing an extracted audio event). These settings will be our benchmark for the number of audio events that can be extracted with the `threshold_detection()` function. In the following exercise, you will try to reach or beat this number by exploring different combinations of parameters for each argument of the function.\r\n\r\n```{r threshold2, eval=FALSE}\r\n# Let's try various settings, starting with 1024 FFT size instead of 256.\r\nTD <- threshold_detection(\r\n  CATBIC, threshold = 12, time_exp = 1, min_dur = 140, max_dur = 440, \r\n  min_TBE = 10, max_TBE = 5000, EDG = 0.996, LPF = 10000, HPF = 1000, \r\n  FFT_size = 1024, FFT_overlap = 0.875, start_thr = 25, end_thr = 30, \r\n  SNR_thr = 10, angle_thr = 45, duration_thr = 440, NWS = 1000, \r\n  KPE = 1e-05, KME = 1e-04, settings = FALSE, acoustic_feat = TRUE,\r\n  metadata = FALSE, spectro_dir = file.path(tempdir(), \"Spectros\"), time_scale = 1, \r\n  ticks = c(1000, 10000, 1000) # Tick marks from 1 to 10 kHz with 1 kHz interval\r\n) \r\n# Take a look at the spectrograms and compare them with the previous extraction.\r\nnrow(TD$data$event_data) # Only three audio events!\r\n```\r\n\r\nWe will play with various detection thresholds: end_thr, SNR_thr, angle_thr, KPE and KME parameters. Try to reach 66 spectrograms extracted with a contour (*i.e.*, Kalman curve) that best matches the audio event (answer below). The FFT size will be set at 256 samples.\r\n\r\n```{r threshold3, eval=FALSE}\r\nCATBIC <- read_audio(file.path(data_dir, \"Catharus-bicknelli-54866.mp3\"))\r\nTD <- threshold_detection(\r\n  CATBIC, threshold = 12, time_exp = 1, min_dur = 140, max_dur = 440, min_TBE = 10, \r\n  max_TBE = Inf, EDG = 0.996, LPF = 10000, HPF = 1000, FFT_size = 256, FFT_overlap = 0.875, \r\n  start_thr = 22, end_thr = 30, SNR_thr = 10, angle_thr = 125, duration_thr = 440, NWS = 1000,\r\n  KPE = 1e-05, KME = 1e-05, settings = FALSE, acoustic_feat = TRUE, metadata = FALSE\r\n)\r\n```\r\n\r\nLets take a look at the extracted audio features. Note that all the features are described and explained in the package vignette (`vignette(\"bioacoustics\")`).\r\n\r\n```{r features1, eval=FALSE}\r\n# Acoustic features are stored in a data frame called event_data,\r\n# stored by order of detection.\r\nView(TD$data$event_data) # Contains the filename and the time of detection in the \r\n                         # recording, and 26 extracted features.\r\n```\r\n\r\nThe location (in number of samples) of the audio event in the recording is saved in a list. \r\n\r\n```{r features2, eval=FALSE}\r\n# Start and end of the 5th extracted audio event (in samples)\r\nc(TD$data$event_start[[5]], TD$data$event_end[[5]])\r\n# Remember you just have to divide by the sample rate to retrieve the time (s)\r\nc(TD$data$event_start[[5]], TD$data$event_end[[5]]) / slot(CATBIC, \"samp.rate\")\r\n```\r\n\r\nThe amplitude (dB) and frequency (Hz) tracks (or bins) are also saved in a list. These can be used to build your own acoustic features.\r\n\r\n```{r features3, eval=FALSE}\r\npar(mar = c(1,1, 1, 1), oma = c(1, 1, 1, 1))\r\n# Amplitude track of the 5th audio event\r\nplot(TD$data$amp_track[[5]], type = \"l\")\r\n# Frequency track of the 5th audio event\r\nplot(TD$data$freq_track[[5]], type = \"l\")\r\n```\r\n\r\nThe whole energy and frequency content can also be used to classify audio events instead of using acoustic features that may result in a loss of information. We will get there soon, but first, lets discover another detection function, here applied on echolocation calls of bats.\r\n\r\n\r\n### Blob detection\r\n\r\n### The `blob_detection()` function will be used on a recording containing 10 bat echolocation calls from the *Myotis* genus. This function combines several image processing, filtering and image feature extraction. A blur and contrast boost is applied after mean background subtraction to increase the SNR of the audio event. The blob detection algorithm is applied on the processed spectrogram to detect the ROI (i.e., each preprocessed audio event). The blob detector simultaneously labels the connected FFT values and their contours in the spectrogram. Labelling is done in a single pass over the spectrogram, while contour points are revisited more than once and up to four times (see [Chang et al., 2004](https://www.iis.sinica.edu.tw/papers/fchang/1362-F.pdf)). We will play with the arguments of this function to extract bat echolocation calls.\r\n\r\n```{r blob0, eval=FALSE}\r\n### Access the arguments of the blob_detection function\r\n?blob_detection\r\n```\r\n\r\n```{r blob1, eval=FALSE}\r\n# Use the bat recording stored in the package\r\ndata(myotis)\r\n# Set each argument according to the targeted audio events\r\nBD <- blob_detection(\r\n  myotis, # Either a path to an audio file (see ?read_audio), or a Wave object\r\n  time_exp = 10, # Time expansion factor of 10 for time expanded recordings.\r\n  min_dur = 1.5, # Minimum duration threshold of 1.5 milliseconds (ms)\r\n  max_dur = 80, # Maximum duration threshold of 80 ms\r\n  min_area = 40, # minimum number of 40 pixels in the blob\r\n  min_TBE = 20, # Minimum time window between two audio events of 20 milliseconds\r\n  EDG = 0.996, # Temporal masking with Exponential Decay Gain from 0 to 1\r\n  LPF = slot(myotis, \"samp.rate\") * 10 / 2, # Low-Pass Filter at the Nyquist frequency\r\n  HPF = 16000, # High-Pass Filter of 16 kHz\r\n  FFT_size = 256, # Size of the Fast Fourrier Transform (FFT) window\r\n  FFT_overlap = 0.875, # Percentage of overlap between two FFT windows\r\n  blur = 2, # Gaussian smoothing function with a factor of 2 for blurring the spectrogram\r\n  bg_substract = 20, # Foreground extraction with a mean filter applied on the spectrogram\r\n  contrast_boost = 20, # Edge contrast enhancement filter of the spectrogram contour\r\n  settings = FALSE, #  Save on a list the above parameters set with this function\r\n  acoustic_feat = TRUE, # Extracts the acoustic and signal quality parameters \r\n  metadata = FALSE, # Extracts on a list the metadata embedded with the Wave file\r\n  spectro_dir = file.path(tempdir(), \"Spectros\"), # HTML page with spectrograms by order of detection \r\n  time_scale = 0.1, # Time resolution of 2 ms for spectrogram display\r\n  ticks = TRUE # Tick marks and their intervals are drawn on y-axis (frequencies)\r\n) \r\n# Get the number of extracted audio events\r\nnrow(BD$data$event_data)\r\n```\r\n\r\nDo not close the HTML page and tune the FFT size at 512. Lets play with the blur, contrast boost and background subtraction parameters to retrieve a number of 10 extracted echolocation calls.\r\n\r\n```{r blob2, eval=FALSE}\r\n# Lets try various settings, starting with 512 FFT size instead of 256\r\nBD <- blob_detection(\r\n  myotis, time_exp = 10, FFT_size = 512, settings = FALSE, acoustic_feat = TRUE,\r\n  metadata = FALSE, spectro_dir = file.path(tempdir(), \"Spectros\"), time_scale = 0.1, ticks = TRUE\r\n) \r\n# Take a look at the spectrograms and compare them with the previous extraction.\r\nnrow(BD$data$event_data) # Only 6 audio events!\r\n```\r\n\r\nLets take a look at the extracted audio features. All the features are described and explained in the package vignette.\r\n\r\n```{r blobfeat1, eval=FALSE}\r\n# Acoustic features\r\nhead(BD$data)\r\n```\r\n\r\nThis data frame is, for now, the only available set of acoustic features with the `blob_detection()` function. However, it combines well with the `fspec()` to make image analysis.\r\n\r\nNow that we have played with both detection functions with bird and bat vocalizations, lets go back to birds to explore batch analysis (*i.e.*, with several recordings) and audio event classification.\r\n\r\n___________________\r\n\r\n## Batch analysis and classification\r\n\r\nIn this section, we will learn how to analyze several recordings at the same time and train a simple classifier (with training set) that will be used to classify new data (*i.e.*, the test set).\r\n\r\nWe will work with 27 recordings of *Catharus-bicknelli* (*n* = 9), *Passerella iliaca* (*n* = 9), and *Setophaga-magnolia* (*n* = 9). We will split the extracted audio events in a 70 % training set (called \"Train\") and 30 % test set (called \"Test\").\r\n\r\nOur target audio events are calls of *Catharus-bicknelli*. We will use the threshold detector previously configured for this species (see section 4.1.1).\r\n\r\n```{r classification1, eval=FALSE}\r\n# Get the filepath for each MP3 file\r\nfiles <- dir(data_dir, recursive = TRUE, full.names = TRUE, pattern = \"[.]mp3$\")\r\n# Detect and extract audio events\r\nTDs <- setNames(\r\n  lapply(\r\n    files,\r\n    threshold_detection,\r\n    threshold = 12, min_dur = 140, max_dur = 440, min_TBE = 50, max_TBE = Inf,\r\n    LPF = 8000, HPF = 1500, FFT_size = 256, start_thr = 30, end_thr = 20, \r\n    SNR_thr = 10, angle_thr = 125, duration_thr = 400, spectro_dir = NULL,\r\n    NWS = 2000, KPE = 0.00001, time_scale = 2, EDG = 0.996\r\n  ),\r\n  basename(file_path_sans_ext(files))\r\n)\r\n# Keep only files with data in it\r\nTDs <- TDs[lapply(TDs, function(x) length(x$data)) > 0]\r\n# Keep the extracted feature and merge in a single data frame for further analysis\r\nEvent_data <- do.call(\"rbind\", c(lapply(TDs, function(x) x$data$event_data), list(stringsAsFactors = FALSE)))\r\nnrow(Event_data) # 355 audio events extracted\r\n# Compute the number of extracted CATBIC calls\r\nsum(startsWith(Event_data$filename, \"Cat\"))\r\n# Add a \"Class\" column: \"CATBIC\" vs. other species of birds \"OTHERS\"\r\nclasses <- as.factor(ifelse(startsWith(Event_data$filename, \"Cat\"), \"CATBIC\", \"OTHERS\"))\r\nEvent_data <- cbind(data.frame(Class = classes), Event_data)\r\n# Get rid of the filename and time in the recording\r\nEvent_data$filename <- Event_data$starting_time <- NULL\r\n```\r\n\r\nWe now have the necessary dataset to train a classifier: we will train a Random Forest on the training set and validate the results on the test set.\r\n\r\n```{r classification2, eval=FALSE}\r\n# Split the data in 60% Training / 40% Test sets\r\ntrain <- sample(1:nrow(Event_data), round(nrow(Event_data) * .6))\r\nTrain <- Event_data[train,]\r\ntest <- setdiff(1:nrow(Event_data), train)\r\nTest <- Event_data[test,]\r\n# Train a random forest classifier\r\nset.seed(666)\r\nrf <- randomForest(Class ~ duration + freq_max_amp + freq_max + freq_min +\r\n                     bandwidth + freq_start + freq_center + freq_end +\r\n                     freq_knee + fc + freq_bw_knee_fc + bin_max_amp + \r\n                     pc_freq_max_amp + pc_freq_max + pc_freq_min +\r\n                     pc_knee + temp_bw_knee_fc + slope + kalman_slope +\r\n                     curve_neg + curve_pos_start + curve_pos_end + \r\n                     mid_offset + smoothness + snr + hd + smoothness,\r\n                   data = Train, importance = FALSE, proximity = FALSE,\r\n                   replace = TRUE, ntree = 4000, mtry = 4)\r\n# Look at the confusion matrix of the training set\r\nrf$confusion # looks good, but...\r\n# Let's make predictions with our classifier on a test set\r\ntable(Test[,1], predict(rf, Test[,-1], type = \"response\")) # not bad!\r\n# To look at the predictions \r\nhead(predict(rf, Test[,-1], type = \"prob\"))\r\n```\r\n\r\nWe are now able to use this simple, but proven robust, classifier to detect new calls of your target species.\r\n\r\n\r\n___________________\r\n\r\n# Deep learning classification with the R interface to Keras\r\n\r\nWe will use Keras in R which requires to install several packages in [Python](https://www.python.org/downloads/)\r\nGuidelines to install Keras properly in R are available [here](https://keras.rstudio.com/)\r\n\r\nLets now explore a ConvNet approach available on Keras. We will follow the approach of [Hatami et al. (2017)](https://arxiv.org/pdf/1710.00886.pdf) to analyze time series as images with 2D ConvNets. The difference is that we will only perform max pooling at the last layer before activation and add batch normalization with dropouts at each layer.\r\n\r\n```{r keras1, eval=FALSE}\r\ndevtools::install_github(\"rstudio/keras\")\r\n# Run if keras is installed on your machine\r\nlibrary(keras)\r\n# Build the training set\r\nY_train <- to_categorical(as.integer(Train[,1]) - 1) # One hot encoding\r\n# X as matrix\r\nX_train <- as.matrix(Train[,-1])\r\n# Build the test set\r\nY_test <- to_categorical(as.integer(Test[,1]) - 1)\r\nY_test <- Y_test[,-1]\r\nX_test <- as.matrix(Test[,-1])\r\n# Build the sequential model\r\nmod0 <- keras_model_sequential()\r\nmod0 %>%\r\n  # Input shape layer = c(samples, rows, cols, channels)\r\n  layer_reshape(input_shape=ncol(X_train),target_shape=c(1,1,ncol(X_train))) %>% \r\n  # First conv 2d layer with 128 neurons, kernel size of 8 x 8 and stride of 1 x 1\r\n  layer_conv_2d(128, c(8,8), c(1,1), padding='same') %>%\r\n  layer_batch_normalization() %>%\r\n  layer_activation(\"relu\") %>%\r\n  layer_dropout(0.2) %>%\r\n  # Second conv 2d layer with 256 neurons, kernel size of 5 x 5 and stride of 1 x 1\r\n  layer_conv_2d(256, c(5,5), c(1,1), padding='same') %>%\r\n  layer_batch_normalization() %>%\r\n  layer_activation(\"relu\") %>%\r\n  layer_dropout(0.2) %>%\r\n  # Third conv 2d layer with 128 neurons, kernel size of 3 x 3 and stride of 1 x 1\r\n  layer_conv_2d(128, c(3,3), c(1,1), padding='same') %>%\r\n  layer_batch_normalization() %>%\r\n  layer_activation(\"relu\") %>%\r\n  layer_dropout(0.2) %>%\r\n  # Average pooling layer\r\n  layer_global_average_pooling_2d() %>%\r\n  # Activation output layer with 2 classes\r\n  layer_dense(units = ncol(Y_train),  activation='softmax')\r\n# Model compile\r\nmod0 %>% compile(loss = 'categorical_crossentropy',\r\n                 optimizer = \"adam\",\r\n                 metrics = \"categorical_accuracy\")\r\n# Add a callback to reduce the learning rate when reaching the plateau\r\nreduce_lr <- callback_reduce_lr_on_plateau(monitor = 'loss', factor = 0.5,\r\n                                           patience = 50, min_lr = 0.0001)\r\n# Start learning\r\nmod0 %>% fit(X_train, Y_train, batch_size = 32, epochs = 50,\r\n             validation_data = list(X_test, Y_test),\r\n             verbose = 1, callbacks = reduce_lr)\r\n# Score on the test set\r\nscore <- mod0 %>% evaluate(X_test, Y_test, batch_size = 32)\r\nscore\r\n```\r\n\r\nLets work a bit with the output to build a confusion matrix and use the predict function on the test set.\r\n\r\n```{r keras2, eval=FALSE}\r\n# Look at predictions and build a confusion matrix\r\nPred <- as.factor(predict_classes(mod0, X_test, batch_size = 32, verbose = 1))\r\ntable(Y_test[,2], Pred)\r\n# To look at the prediction values \r\nProb <- round(predict_proba(mod0, X_test, batch_size = 32, verbose = 1), 2)\r\n```\r\n\r\n\r\nWe obtained a val_loss < 0.2 and val_categorical_accuracy > 0.94 which is acceptable, but not better than the simplest RF approach we used in section 3.2.\r\nUsing only 26 acoustic features as model inputs instead of the whole spectrogram content (energy and frequency distribution, and harmonics) probably reduced the performances of the CNN model.\r\n\r\nThis tutorial is now complete. Comments and feedback are welcome:\r\n  \r\n  Francois: francois.fabianek@wavx.ca  \r\nJean: jean.marchal@wavx.ca  \r\n[www.wavx.ca](https://www.wavx.ca)\r\n\r\n\r\n_______\r\n_______\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@spatialaudiolabs,\r\nLooks like this issue is a duplicate of [#40099](https://github.com/tensorflow/tensorflow/issues/40099). Can you please close this issue since it is already being tracked there. Thanks!", "Hi Amahendrakar\r\n\r\nHere I am running on macOS and the output error is actually different. Shouldn't they be treated separately?", "@spatialaudiolabs,\r\nPlease take a look at these comments from similar issues and let us know if it helps.\r\n\r\n[Link 1](https://github.com/rstudio/keras/issues/693#issuecomment-470892009), [Link 2](https://github.com/rstudio/keras/issues/33#issuecomment-306918034)\r\n\r\n Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40678, "title": "Tangent Batching for regular ops ", "body": "Tangent Batching for regular ops can be done by vectorizing `_jvp_dispath`.\r\n\r\n(cc @allenlavoie)", "comments": ["I'd say input tangents should have shape `[batch] + input_shape`. So for `input_shape = []` (scalar), input tangents would have shape `[batch]`. (And output tangents then have shape `[batch] + output_shape`.) I think that's your first case? I don't see how it's awkward; like you say, that lets you just slice the input to get back to the un-batched case.\r\n\r\nAs for where the logic goes, pushing it down a little might simplify things. I agree that we shouldn't modify `_jvp_helper`, but maybe we can make an optional wrapper around it? That way batching doesn't get mixed up in the shape relaxation logic. So `_jvp_relaxed_shapes` and `_jvp_exact_shapes` would both reference the batching wrapper rather than the existing `_jvp_helper`. It does mean we'll need to plumb the option through a bit deeper.", "Looks like there are some lint errors (Ubuntu Sanity). Can you fix them?\r\n\r\nThere's some background/instruction here: https://www.tensorflow.org/community/contribute/code_style", "@abhichou4 Can you please fix build failures ? Thanks!", "@abhichou4 Can you please check @allenlavoie's comments and keep us posted. Thanks!", "Thanks for the review and for being patient! @allenlavoie @gbaned "]}, {"number": 40677, "title": "Unsupported ops when converting keyword spotting model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin Kernel Version 19.5.0\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.15.2\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ntflite_convert --enable_v1_converter --graph_def_file=tf5/DS_CNN_L.pb --output_file=tflite/DS_CNN_L.tflite --output_format=TFLITE --input_shape=1 --input_arrays=wav_data --output_arrays=labels_softmax --inference_type=FLOAT --input_data_type=STRING\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-06-22 19:10:58.286058: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-22 19:10:58.298962: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f998703d820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-22 19:10:58.298980: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-22 19:10:58.334024: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-06-22 19:10:58.334124: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-22 19:10:58.366223: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-06-22 19:10:58.366256: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 125 nodes (-57), 125 edges (-57), time = 15.329ms.\r\n2020-06-22 19:10:58.366261: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 125 nodes (0), 125 edges (0), time = 6.637ms.\r\nTraceback (most recent call last):\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 515, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 511, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 199, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 983, in convert\r\n    **converter_kwargs)\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-06-22 19:11:00.826624: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodeWav\r\n2020-06-22 19:11:00.826669: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: AudioSpectrogram\r\n2020-06-22 19:11:00.826680: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Mfcc\r\n2020-06-22 19:11:00.828343: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 98 operators, 181 arrays (0 quantized)\r\n2020-06-22 19:11:00.829492: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 98 operators, 181 arrays (0 quantized)\r\n2020-06-22 19:11:00.833039: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 19 operators, 46 arrays (0 quantized)\r\n2020-06-22 19:11:00.833242: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 19 operators, 46 arrays (0 quantized)\r\n2020-06-22 19:11:00.833372: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 19 operators, 46 arrays (0 quantized)\r\n2020-06-22 19:11:00.833621: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2020-06-22 19:11:00.833699: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 410704\r\n2020-06-22 19:11:00.834066: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, RESHAPE, SOFTMAX, SQUEEZE. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, DecodeWav, Mfcc.\r\nTraceback (most recent call last):\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, RESHAPE, SOFTMAX, SQUEEZE. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, DecodeWav, Mfcc.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://github.com/ARM-software/ML-KWS-for-MCU/tree/master/Pretrained_models/DS_CNN\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@thaink could you take a look?", "Let me add these ops.", "Hi prabhat00155,\r\nYou are using a too old version of Tensorflow.\r\nThose ops are supported with flex delegate. Please check it with Tensorflow v2.2.", "Thanks, I have updated my TF version. When I run summarize_graph on the model, I get shape=[]. How do I pass this when using tflite_convert?\r\n```\r\n(onnxrt_env) prroy-Mac:tensorflow prroy$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=DS_CNN_L.pb \r\nFound 1 possible inputs: (name=wav_data, type=string(7), shape=[]) \r\nNo variables spotted.\r\nFound 1 possible outputs: (name=labels_softmax, op=Softmax) \r\nFound 422850 (422.85k) const parameters, 0 (0) variable parameters, and 0 control_edges\r\nOp types used: 70 Const, 57 Identity, 12 BiasAdd, 11 FusedBatchNorm, 11 Relu, 6 Conv2D, 5 DepthwiseConv2dNative, 2 Reshape, 1 AudioSpectrogram, 1 MatMul, 1 Mfcc, 1 Placeholder, 1 DecodeWav, 1 AvgPool, 1 Softmax, 1 Squeeze\r\n```\r\n", "@abattery Do we need to pass --input_shapes in case of string?", "`shape=[]` means scalar type. You can put one string value via TFLite inference API. \r\n\r\nE.g.,\r\n```\r\nmodel_interpreter.set_tensor(input_details[0]['index'],\r\n                               np.array(['foo'], dtype=np.string_))\r\n```\r\n\r\nFor the inference API, you can refer to the following link: https://www.tensorflow.org/lite/guide/inference", "Thanks, I was able to convert the model to TFLite. Can I use benchmark_model tool on this model?\r\nI built benchmark_model with ` --define=tflite_convert_with_select_tf_ops=true`.\r\n```\r\nbazel build -c opt tensorflow/lite/tools/benchmark:benchmark_model --define=tflite_convert_with_select_tf_ops=true\r\n```\r\nHowever, I get the following error when running the tool on the converted model:\r\n```\r\nbazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=../models/tflite/wave_model.tflite --num_runs=10 --num_threads=4 --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS --define=tflite_convert_with_select_tf_ops=true\r\n```\r\n```\r\nLoaded model ../models/tflite/wave_model.tflite\r\nERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nERROR: Node number 0 (FlexDecodeWav) failed to prepare.\r\n\r\nFailed to allocate tensors!\r\nBenchmarking failed.\r\n```", "Could you try \"bazel build --config=monolithic -c opt tensorflow/lite/tools/benchmark:benchmark_model_plus_flex\" instead?", "[output.txt](https://github.com/tensorflow/tensorflow/files/4873069/output.txt)\r\nThanks, I got some error when running the model with benchmark_model_plus_flex. I have attached the output log file.\r\n```\r\n2020-07-04 16:17:28.726122: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at decode_wav_op.cc:55 : Invalid argument: Header mismatch: Expected RIFF but found we'r\r\nERROR: Header mismatch: Expected RIFF but found we'r\r\n\t (while executing 'DecodeWav' via Eager)\r\nERROR: Node number 19 (TfLiteFlexDelegate) failed to invoke.\r\n```", "@prabhat00155 please make sure that your inputs are valid to both the TFLite corresponding TF model. According to the error message, TF's DecodeWav op could not handle your input.", "@prabhat00155 \r\nPlease update as per above comment.", "I am running the benchmark_model_plus_flex tool like this:\r\n```\r\nbazel-bin/tensorflow/lite/tools/benchmark/benchmark_model_plus_flex --graph=wave_model.tflite --num_runs=10 --num_threads=4\r\n```\r\nI am not passing any input, the benchmark tool generates random input and then repeatedly runs the model for specified number of runs.", "The wave model could not handle any random data generated from benchmark model since the model needs a valid wav input.", "In that case, is there a way to pass valid data using the benchmark model app? ", "The benchmark tool always uses random input. I think you have to write your own inference code."]}, {"number": 40676, "title": "quantization not yet supported for op %", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):tf-nightly2.3\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\nconventer=tf.lite.TFLiteConverter.from_keras_model(inference)\r\nconventer.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconventer.representative_dataset = representative_data_gen\r\nconventer.experimental_new_converter=True\r\nconventer.target_spec.supported_ops=[tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconventer.inference_input_type = tf.uint8  # or tf.uint8\r\nconventer.inference_output_type = tf.uint8  # or tf.uint8\r\ntflitemodel=conventer.convert()\r\nopen(\"./centernet.tflite\",\"wb\").write(tflitemodel)\r\n\r\n\r\n**The output from the converter invocation**\r\nquantization not yet supported for op % \r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\npost-traiing quantization for uint8 model \r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\ngot an error:quantization not yet supported for op %\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you please attach your model/ share google colab code to debug further? Thanks!", "@ymodak  I think  it was in my topk op\uff0c the code  is here\uff1a\r\n```python\r\n    def __topK(scores, K):\r\n        B, H, W, C = scores.shape\r\n        scores = tf.reshape(scores, shape=(B, -1))\r\n        topk_scores, topk_inds = tf.math.top_k(input=scores, k=K, sorted=True)\r\n        topk_clses = topk_inds % C\r\n        topk_xs = tf.cast(topk_inds // C % W, tf.float32)\r\n        topk_ys = tf.cast(topk_inds // C // W, tf.float32)\r\n        topk_inds = tf.cast(topk_ys * tf.cast(W, tf.float32) + topk_xs, tf.int32)\r\n        return topk_scores, topk_inds, topk_clses, topk_ys, topk_xs\r\n```", "Thanks for your response. Are you able to convert your model correctly if you remove`%` operations from the code above?", "@ymodak   yes without % it can convert  success", "If still using `%` op in your code try setting custom_op flag true and convert again.\r\n`python\r\nconverter.allow_custom_ops = True\r\n`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40675, "title": "[doc] Fix broken figures in tiled_layout.md", "body": "This pr changed the figures in a style that https://stackoverflow.com/a/12118349/5163915 suggests so that  they can be shown correctly on github.\r\n\r\nThank you for your time on reviewing this pr.", "comments": ["@lamberta \r\nThis is a fix of xla doc. Could you have a look at this? Thank you!"]}, {"number": 40674, "title": "Replace deprecated numpy function calls", "body": "This PR switches to using `numpy.ndarray.tobytes()` instead of `numpy.ndarray.tostring()` which has been deprecated in Numpy 1.19 (https://github.com/numpy/numpy/pull/15867) and replaces `numpy.asscalar` with `numpy.ndarray.item()`.\r\n\r\nThese are non-functional changes that only remove deprecation warnings in newer versions of numpy.", "comments": []}, {"number": 40673, "title": "@Lu1352 Can you please open a new issue with a simple standalone code to reproduce the error? Thanks!", "body": "@Lu1352 Can you please open a new issue with a simple standalone code to reproduce the error? Thanks!\r\n\r\n_Originally posted by @jvishnuvardhan in https://github.com/tensorflow/tensorflow/issues/35750#issuecomment-642966950_\r\n\r\nHere you go:\r\n```\r\n#!/usr/bin/python3\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\n(train_images, train_answers), (test_images, test_answers) = tf.keras.datasets.mnist.load_data()\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nmodelfile=\"saved.model\"\r\nif os.path.exists(modelfile):\r\n   model = tf.keras.models.load_model(modelfile)\r\nelse:\r\n   model = tf.keras.models.Sequential([\r\n     tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n     tf.keras.layers.Dense(128, activation='relu'),\r\n     tf.keras.layers.Dropout(0.2),\r\n     tf.keras.layers.Dense(10)\r\n   ])\r\n   loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n   model.compile(optimizer='adam', loss=loss_fn)\r\n   model.fit(train_images, train_answers, epochs=1)\r\n   model.save(modelfile)\r\nmodel.summary()\r\n```\r\n\r\nRun it twice to reproduce the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"./bug.py\", line 11, in <module>\r\n    model = tf.keras.models.load_model(modelfile)\r\n  File \"/home/dbw/.local/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 146, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/dbw/.local/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 244, in load_model_from_hdf5\r\n    sample_weight_mode=sample_weight_mode)\r\n  File \"/home/dbw/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/dbw/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 300, in compile\r\n    self.loss, self.output_names)\r\n  File \"/home/dbw/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 1094, in prepare_loss_functions\r\n    generic_utils.check_for_unexpected_keys('loss', loss, output_names)\r\n  File \"/home/dbw/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 592, in check_for_unexpected_keys\r\n    expected_values))\r\nValueError: Unknown entries in loss dictionary: ['class_name', 'config']. Only expected following keys: ['dense_1']\r\n```", "comments": ["Sorry.  I went back to check the tf version and found I was accidentally using 1.14.  This issue does not occur with tf 2.2."]}, {"number": 40672, "title": "Optimizing scatter_nd_* for complex tensors", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, as noted in #40605 , the `scatter_nd_*` functions are extremely slow.\r\n\r\nHowever, this is even more extreme when used on complex tensors. This [colab notebook](https://colab.research.google.com/drive/1omAKl8vcqd2TBVbXEnVGvey8Urmcp-kH?usp=sharing) illustrates this fact.\r\n\r\nA simple hack consisting in treating separately real and imaginary parts makes us gain a factor 20 on computation time.\r\n\r\nI guess at least this hack could be implemented at the python level of the op, a part on which I am definitely willing to contribute.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone using complex tensors, so I would say people working in sound processing and MRI, but this is surely not exhaustive.\r\n\r\n**Any Other info.**\r\n\r\nI don't know if this is due to eager execution or not. I don't necessarily know how to best profile these kind of issues in graph mode.\r\n\r\nI think even if this is eager-related it still deserves a fix.", "comments": ["For the latest nightly release (2.4.0dev-20200708), it seems that this problem is not present anymore (see the [same colab](https://colab.research.google.com/drive/1omAKl8vcqd2TBVbXEnVGvey8Urmcp-kH?usp=sharing)).\r\n\r\n[This PR](https://github.com/tensorflow/tensorflow/issues/40577#issuecomment-646138225) solved a problem  related to tf 2.3, and maybe some other issue with `scatter_nd_*` was solved in tf 2.3.\r\n\r\nClosing this since it's not relevant anymore."]}, {"number": 40671, "title": "[TF 1.15] unexpected segment faults for seemingly \"reasonable and correct\" codes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos7  (but libtensorflow.so is built in official docker image tensorflow/serving:1.15.0-devel-gpu)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 1080Ti 11178MiB\r\n\r\n**Describe the current behavior**\r\nIt is by-product of tensorflow serving. In official docker image (tensorflow/serving:1.15.0-devel-gpu) , we build tensorflow serving 1.15 based on tensorflow 1.15. Everything is OK for tensorflow serving mode, and it has been deployed in our product environment. However, we meet the bottleneck of inter-process for current tensorflow serving, hence we consider using direct local GPU infer.  \r\n\r\nLuckily tensorflow is already included in tensorflow serving.  In tensorflow folder we build tensorflow_cc. using following command:\r\n\r\n```shell\r\ncd ./tensorflow\r\nexport  TF_NEED_CUDA=1\r\nexport  TF_NEED_S3=1\r\nexport  TF_CUDA_COMPUTE_CAPABILITIES=\"3.5,5.2,6.1\"\r\nexport  TF_NEED_GCP=1\r\nexport  TF_NEED_JEMALLOC=0\r\nexport  TF_NEED_HDFS=0\r\nexport  TF_NEED_OPENCL=0\r\nexport  TF_NEED_MKL=0\r\nexport  TF_NEED_VERBS=0\r\nexport  TF_NEED_MPI=0\r\nexport  TF_DOWNLOAD_MKL=0\r\nexport  TF_NEED_GDR=0\r\nexport  TF_ENABLE_XLA=0\r\nexport  TF_CUDA_CLANG=0\r\nexport  TF_NEED_OPENCL_SYCL=0\r\nexport  GCC_HOST_COMPILER_PATH=/usr/bin/gcc\r\nexport  PYTHON_BIN_PATH=/usr/bin/python\r\nexport  PYTHON_LIB_PATH=/usr/lib/python2.7/site-packages/\r\nexport  CC_OPT_FLAGS=\"-march=native\"\r\nbazel build -c opt --config=cuda --copt=-mavx --verbose_failures tensorflow:libtensorflow_cc.so\r\n```\r\n\r\nBuild is OK as:\r\n```\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc:    \r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc:                                            \r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy.cc:                                   \r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++                                                                                             \r\nTarget //tensorflow:libtensorflow_cc.so up-to-date:                             \r\n  bazel-bin/tensorflow/libtensorflow_cc.so                                               \r\nINFO: Elapsed time: 1258.352s, Critical Path: 455.73s                                                                                                                  \r\nINFO: 7436 processes: 7436 local.                                                                                                                                                \r\nINFO: Build completed successfully, 11163 total actions  \r\n```\r\n\r\nThen I import following files into my project:\r\n```\r\n  libtensorflow_cc.so\r\n  libtensorflow_framework.so.1 so\r\n  c_api.h\r\n  tf_atrrtype.h\r\n  tf_datatype.h\r\n  tf_status.h\r\ntf_tensor.h\r\n```\r\nMy code calls only C api of tensorflow (which is tested in old cpu tensorflow 1.13 version, and main code logic is OK), and main code is below:\r\n```\r\n// init session\r\n    TF_Session * sess;\r\n    TF_SessionOptions* sess_opts = TF_NewSessionOptions();\r\n    if(proto_len > 0)\r\n    {\r\n        TF_SetConfig(sess_opts,(void*)options_proto, proto_len, status);\r\n\r\n        if(TF_GetCode(status) != TF_OK)\r\n        {\r\n            TF_DeleteSessionOptions(sess_opts);\r\n            return NULL;\r\n        }\r\n    }\r\n    sess = TF_NewSession(graph, sess_opts, status);\r\n\r\n// session infer (most crashes occurs here)\r\nTF_SessionRun(...)\r\n```\r\n\r\nSPECIAL NOTE: due to environment and other dependencies issues, my personal project  has to be built in local GPU centos (default glibc is old 2.17). In order to exclude the difference of gcc and glibc,   I build my project by using the same gcc 5.4.0 (the same as tensorflow/serving:1.15.0-devel-gpu) and the same glibc 2.23 (the same as tensorflow/serving:1.15.0-devel-gpu). Here is change of cmakelist.txt:\r\n```\r\nset(third_links ${third_links}\r\n    -Wl,--rpath=/myf12/Code/glibc-2.23/install/lib\r\n    -Wl,--dynamic-linker=/myf12/Code/glibc-2.23/install/lib/ld-2.23.so\r\n    -lrt\r\n    )\r\nlist(INSERT third_links 0 \"-L /myf12/Code/glibc-2.23/install/lib/\") \r\n```\r\nwhere third_links will be used as\r\n```\r\ntarget_link_libraries(xxx\t${third_links} )\r\n```\r\nBuild is ok and here is ldd info:\r\n```\r\nldd bin.debug/fst\r\n        linux-vdso.so.1 =>  (0x00007ffc51718000)\r\n        libdl.so.2 => /myf12/Code/glibc-2.23/install/lib/libdl.so.2 (0x00007f2c4f224000)\r\n        libtensorflow_cc_v1.15_avx.so => /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so (0x00007f2c11963000)\r\n        librt.so.1 => /myf12/Code/glibc-2.23/install/lib/librt.so.1 (0x00007f2c1175b000)\r\n        libjemalloc.so.2 => /mnt/lustre/cm/shared/global/src/misc/jemalloc/5.2.1/lib/libjemalloc.so.2 (0x00007f2c112cc000)\r\n        libpthread.so.0 => /myf12/Code/glibc-2.23/install/lib/libpthread.so.0 (0x00007f2c110af000)\r\n        libstdc++.so.6 => /mnt/lustre/cm/shared/global/src/dev/gcc/5.4.0/lib64/libstdc++.so.6 (0x00007f2c10d34000)\r\n        libm.so.6 => /myf12/Code/glibc-2.23/install/lib/libm.so.6 (0x00007f2c10a2e000)\r\n        libgcc_s.so.1 => /mnt/lustre/cm/shared/global/src/dev/gcc/5.4.0/lib64/libgcc_s.so.1 (0x00007f2c10817000)\r\n        libc.so.6 => /myf12/Code/glibc-2.23/install/lib/libc.so.6 (0x00007f2c10476000)\r\n        /myf12/Code/glibc-2.23/install/lib/ld-2.23.so => /lib64/ld-linux-x86-64.so.2 (0x00007f2c4f42a000)\r\n        libcusparse.so.10.0 => /mnt/lustre/cm/shared/global/src/dev/cuda/10.0/lib64/libcusparse.so.10.0 (0x00007f2c0ca0c000)\r\n        libcusolver.so.10.0 => /mnt/lustre/cm/shared/global/src/dev/cuda/10.0/lib64/libcusolver.so.10.0 (0x00007f2c04325000)\r\n        libnvinfer.so.5 => /mnt/lustre/cm/shared/global/src/machinelearning/tensorrt/TensorRT-5.0.2.6/lib/libnvinfer.so.5 (0x00007f2bfcec8000)\r\n        libnvinfer_plugin.so.5 => /mnt/lustre/cm/shared/global/src/machinelearning/tensorrt/TensorRT-5.0.2.6/lib/libnvinfer_plugin.so.5 (0x00007f2bfc995000)\r\n        libcublas.so.10.0 => /mnt/lustre/cm/shared/global/src/dev/cuda/10.0/lib64/libcublas.so.10.0 (0x00007f2bf83ff000)\r\n        libcudnn.so.7 => /mnt/lustre/cm/shared/global/src/dev/cudnn/7.5.1/lib64/libcudnn.so.7 (0x00007f2be2de0000)\r\n        libcufft.so.10.0 => /mnt/lustre/cm/shared/global/src/dev/cuda/10.0/lib64/libcufft.so.10.0 (0x00007f2bdc92b000)\r\n        libcurand.so.10.0 => /mnt/lustre/cm/shared/global/src/dev/cuda/10.0/lib64/libcurand.so.10.0 (0x00007f2bd87c4000)\r\n        libcudart.so.10.0 => /mnt/lustre/cm/shared/global/src/dev/cuda/10.0/lib64/libcudart.so.10.0 (0x00007f2bd854a000)\r\n        libgomp.so.1 => /mnt/lustre/cm/shared/global/src/dev/gcc/5.4.0/lib64/libgomp.so.1 (0x00007f2bd8327000)\r\n        libnvToolsExt.so.1 => /mnt/lustre/cm/shared/global/src/dev/cuda/10.0/lib64/libnvToolsExt.so.1 (0x00007f2bd811e000)\r\n\r\n```\r\n\r\nProgram crashes due to segment fault as below:\r\n```\r\n2020-06-22 12:03:56.005532: I external/org_tensorflow/tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.                       \r\n2020-06-22 12:03:56.007051: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0                                                                 \r\n2020-06-22 12:03:56.650162: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:                               \r\n2020-06-22 12:03:56.650296: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0                                                                                        \r\n2020-06-22 12:03:56.650307: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N                                                                                        \r\n2020-06-22 12:03:56.652890: F external/org_tensorflow/tensorflow/core/common_runtime/device.cc:28] Check failed: DeviceNameUtils::ParseFullName(name(), &parsed_name_) Invalid device name:\r\n```\r\n\r\ncore dump is\r\n```\r\n#0  0x00007f0482651298 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007f048265271a in __GI_abort () at abort.c:89\r\n#2  0x00007f048e307a34 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#3  0x00007f048dc2921f in tensorflow::Device::Device(tensorflow::Env*, tensorflow::DeviceAttributes const&) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#4  0x00007f048dc62e64 in tensorflow::LocalDevice::LocalDevice(tensorflow::SessionOptions const&, tensorflow::DeviceAttributes const&) ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#5  0x00007f048d4d9166 in tensorflow::BaseGPUDevice::BaseGPUDevice(tensorflow::SessionOptions const&, std::string const&, tensorflow::gtl::IntType<tensorflow::Bytes_tag_, long long>, tensorflow::DeviceLocality const&, tensorflow::gtl::IntType<tensorflow::TfGpuId_tag_, int>, std::string const&, tensorflow::Allocator*, tensorflow::Allocator*, bool, int) ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#6  0x00007f048d4e40c8 in tensorflow::GPUDeviceFactory::CreateGPUDevice(tensorflow::SessionOptions const&, std::string const&, tensorflow::gtl::IntType<tensorflow::Bytes_tag_, long long>, tensorflow::DeviceLocality const&, tensorflow::gtl::IntType<tensorflow::TfGpuId_tag_, int>, std::string const&, tensorflow::Allocator*, tensorflow::Allocator*) ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#7  0x00007f048d4dcc9e in tensorflow::BaseGPUDeviceFactory::CreateGPUDevice(tensorflow::SessionOptions const&, std::string const&, tensorflow::gtl::IntType<tensorflow::TfGpuId_tag_, int>, long long, tensorflow::DeviceLocality const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#8  0x00007f048d4e1f2a in tensorflow::BaseGPUDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#9  0x00007f048dc299c9 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#10 0x00007f048c709161 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#11 0x00007f048dca7380 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n```\r\n\r\nThen I fix \"bug\" in following line https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/core/common_runtime/gpu/gpu_device.cc#L1265\r\nfrom\r\n```\r\nconst string device_name =\r\n      strings::StrCat(name_prefix, \"/device:GPU:\", tf_gpu_id.value());\r\n```\r\n into\r\n```\r\nconst string device_name = name_prefix + \"/device:GPU:\" + std::to_string(tf_gpu_id.value());\r\n```\r\n\r\nPrevious crash is avoided (see new log \"Created TensorFlow device\" ) and rerun again. It crashes again in \"InferStatically\":\r\n```\r\n2020-06-22 12:08:22.185006: I external/org_tensorflow/tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.                       \r\n2020-06-22 12:08:22.186563: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1768] Adding visible gpu devices: 0                                                                 \r\n2020-06-22 12:08:23.047640: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:                                \r\n2020-06-22 12:08:23.047693: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0                                                                                        \r\n2020-06-22 12:08:23.047705: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N                                                                                        \r\n2020-06-22 12:08:23.050308: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8164 MB memory) ->\r\n physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)                                                                                                    \r\n                                                                                                                                                                                                           \r\n                                                                                                                                                                                                           \r\n^[[A^[[ASegmentation fault (core dumped)    \r\n\r\n#0  0x00007fa69ce6ce96 in std::_Hashtable<tensorflow::NodeDef const*, std::pair<tensorflow::NodeDef const* const, tensorflow::grappler::NodeState>, std::allocator<std::pair<tensorflow::NodeDef const* cons\r\nt, tensorflow::grappler::NodeState> >, std::__detail::_Select1st, std::equal_to<tensorflow::NodeDef const*>, std::hash<tensorflow::NodeDef const*>, std::__detail::_Mod_range_hashing, std::__detail::_Defau\r\nlt_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::_M_find_before_node(unsigned long, tensorflow::NodeDef const* const&, unsigned long) const [clo\r\nne .isra.856] () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n\r\n#1  0x00007fa69ce76041 in tensorflow::grappler::VirtualScheduler::GetNodeStateOrCreateIt(tensorflow::NodeDef const*) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so         \r\n#2  0x00007fa69ce78d9b in tensorflow::grappler::VirtualScheduler::Init(tensorflow::grappler::GrapplerItem const*) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so            \r\n#3  0x00007fa69ce569df in tensorflow::grappler::AnalyticalCostEstimator::PredictCosts(tensorflow::GraphDef const&, tensorflow::RunMetadata*, tensorflow::grappler::Costs*) const ()                        \r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#4  0x00007fa69ce4f37f in tensorflow::grappler::VirtualCluster::Run(tensorflow::grappler::GrapplerItem const&, tensorflow::RunMetadata*) ()                                                                \r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#5  0x00007fa69cdddb5a in tensorflow::grappler::GraphMemory::InferStatically(std::unordered_map<std::string, tensorflow::DeviceProperties, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, tensorflow::DeviceProperties> > > const&) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so                                                     \r\n#6  0x00007fa69cdce427 in tensorflow::grappler::(anonymous namespace)::IdentifySwappingCandidates(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem*, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> >*, std::unordered_map<tensorflow::NodeDef*, tensorflow::grappler::(anonymous namespace)::SwapInfo, std::hash<tensorflow::NodeDef*>, std::equal_to<tensorflow::NodeDef*>, std::allocator<std::pair<tensorflow::NodeDef* const, tensorflow::grappler::(anonymous namespace)::SwapInfo> > >*) [clone .constprop.1020] ()                      \r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#7  0x00007fa69cdd091d in tensorflow::grappler::(anonymous namespace)::SwappingPass(tensorflow::RewriterConfig_MemOptType, tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem*, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> >*) [clone .constprop.1019] ()                                                                         \r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#8  0x00007fa69cdd3a97 in tensorflow::grappler::MemoryOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()                             \r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#9  0x00007fa69cd01e5a in tensorflow::grappler::MetaOptimizer::RunOptimizer(tensorflow::grappler::GraphOptimizer*, tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem*, tensorflow::GraphDef*, tensorflow::grappler::MetaOptimizer::GraphOptimizationResult*) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so                                                            \r\n#10 0x00007fa69cd03431 in tensorflow::grappler::MetaOptimizer::OptimizeGraph(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()                          \r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#11 0x00007fa69cd04c64 in tensorflow::grappler::MetaOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()                               \r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#12 0x00007fa69cd070af in tensorflow::grappler::RunMetaOptimizer(tensorflow::grappler::GrapplerItem const&, tensorflow::ConfigProto const&, tensorflow::DeviceBase*, tensorflow::grappler::Cluster*, tensorflow::GraphDef*) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#13 0x00007fa69ccf5fa8 in tensorflow::GraphExecutionState::OptimizeGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so                       \r\n#14 0x00007fa69ccf8361 in tensorflow::GraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::ClientGraph, std::default_delete<tensorflow::ClientGraph> >*) ()   \r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#15 0x00007fa69bd9e46a in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::string, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, long long*) ()                                                                        \r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#16 0x00007fa69bd9f9de in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys> >*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo> >*, tensorflow::DirectSession::RunStateArgs*) ()      \r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#17 0x00007fa69bda1d64 in tensorflow::DirectSession::GetOrCreateExecutors(absl::Span<std::string const>, absl::Span<std::string const>, absl::Span<std::string const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so                                                                          \r\n#18 0x00007fa69bda3648 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> >\r\n> const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so                                                                                                 \r\n#19 0x00007fa696f70911 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) [clone .constprop.628] ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#20 0x00007fa696f71179 in TF_SessionRun () from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so \r\n```\r\n\r\nThen I fix \"bug\" in following line https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/core/grappler/costs/graph_properties.cc#L2140\r\nfrom\r\n```\r\nfed_ports[tensor_id.node()].insert(tensor_id.index());\r\n```\r\n into:\r\n```\r\n      if (fed_ports.find(tensor_id.node()) == fed_ports.end())\r\n      {\r\n         std::unordered_set<int> ports = {tensor_id.index()};\r\n         fed_ports[tensor_id.node()] = ports;\r\n      }\r\n      else\r\n      {\r\n         fed_ports[tensor_id.node()].insert(tensor_id.index());\r\n      }\r\n```\r\n\r\nThis crash is fixed  and rerun again. It crashes again in \"GetNodeStateOrCreateIt\" .\r\n```\r\n#0  0x00007f348a9f4e46 in std::_Hashtable<tensorflow::NodeDef const*, std::pair<tensorflow::NodeDef const* const, tensorflow::grappler::NodeState>, std::allocator<std::pair<tensorflow::NodeDef const* const, tensorflow::grappler::NodeState> >, std::__detail::_Select1st, std::equal_to<tensorflow::NodeDef const*>, std::hash<tensorflow::NodeDef const*>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::_M_find_before_node(unsigned long, tensorflow::NodeDef const* const&, unsigned long) const [clone .isra.856] ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#1  0x00007f348a9fe1f5 in tensorflow::grappler::VirtualScheduler::GetNodeStateOrCreateIt(tensorflow::NodeDef const*) ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#2  0x00007f348aa00e5d in tensorflow::grappler::VirtualScheduler::Init(tensorflow::grappler::GrapplerItem const*) ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#3  0x00007f348a9de98f in tensorflow::grappler::AnalyticalCostEstimator::PredictCosts(tensorflow::GraphDef const&, tensorflow::RunMetadata*, tensorflow::grappler::Costs*) const ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#4  0x00007f348a9d732f in tensorflow::grappler::VirtualCluster::Run(tensorflow::grappler::GrapplerItem const&, tensorflow::RunMetadata*) ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n#5  0x00007f348a965b0a in tensorflow::grappler::GraphMemory::InferStatically(std::unordered_map<std::string, tensorflow::DeviceProperties, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, tensorflow::DeviceProperties> > > const&) ()\r\n   from /myf12/Code/asr-kernel/third/tensorflow/libtensorflow_cc_v1.15_avx.so\r\n\r\n```\r\n\r\nAfter quickly fixing 2 bugs, it is unexpected and impossible for me to see theses \"obvious bugs\" in tensorflow release 1.15.  I strongly suspect it relates to some build configuration or running configuration. \r\n\r\nCan you help check it? Thanks. \r\n", "comments": ["Could you please attach the c_api.h file you use, and complete .cc file?", "Also, can you try using 1.15.3 instead of 1.15.0?", "@xldrx @mihaimaruseac \r\nThank for your reply. \r\n\r\nFirst of all, I would like to share latest progress:\r\nIn order to exclude factors introduced by compilation and runtime, I move my whole project into offcial Tensorflow Docker environment (tensorflow/serving:1.15.0-devel-gpu Ubuntu 16.04.6 LTS ). It means tensorflow_cc.so and my project binary are both compiled in the same compilation enviornment. Then I run my program for simple tf infer, and its result is correct. There is no crash. \r\n\r\nThen I copy binary from Docker to my product environment (centos7), and use glibc 2.23 dependencies. Result is still correct as expectation.\r\n\r\nI suspect it may lead to some conclusion:\r\n- problem may be introduced by compilation time (previously my binary is compiled in centos with the same GCC 5.4 and glibc 2.23 as official tensroflow dev Docker)\r\n- runtime is OK becuase binary (compiled in tensorflow docker) can run successful in centos environment.\r\n- my binary is working and code should be correct\r\n\r\nIs there any comment for this?", "@xldrx \r\n\r\nPlease refer to my last comment that probably indicates my code is working.\r\nAnyway, I still provide my information as below.\r\n\r\nI attach all 5 unchanged files ( [c_api_tf_1.15.h.tar.gz](https://github.com/tensorflow/tensorflow/files/4841915/c_api_tf_1.15.h.tar.gz) ) relate to c_api.h for branch tensorflow 1.15:\r\n- tensorflow/c/tf_attrtype.h\r\n- tensorflow/c/tf_datatype.h\r\n- tensorflow/c/tf_status.h\r\n- tensorflow/c/tf_tensor.h\r\n- tensorflow/c/c_api.h\r\n\r\nFor business resaon, I can not paste complete code. Here is main code which covers every step for calling function TF_* :\r\n```c\r\n// get node info\r\nint tf_utils_get_node(TF_Output * node, TF_Graph * graph,const char * op_name, int index)\r\n{\r\n    node->oper = TF_GraphOperationByName(graph, op_name);\r\n    node->index = 0;\r\n    if(node->oper == NULL)\r\n    {\r\n        printf(\"cannot find %s\\n\", op_name);\r\n        return -1;\r\n    }\r\n    return 0;\r\n}\r\n\r\n\r\n// creaye session\r\nTF_Session * tf_utils_new_session(TF_Graph * graph, const char * options_proto, int proto_len, TF_Status * status)\r\n{\r\n    TF_Session * sess;\r\n    TF_SessionOptions* sess_opts = TF_NewSessionOptions();\r\n    if(proto_len > 0)\r\n    {\r\n        TF_SetConfig(sess_opts,(void*)options_proto, proto_len, status);\r\n\r\n        if(TF_GetCode(status) != TF_OK)\r\n        {\r\n            printf(\"Can't set session options %s\\n\", TF_Message(status));\r\n            TF_DeleteSessionOptions(sess_opts);\r\n            return NULL;\r\n        }\r\n    }\r\n\r\n    sess = TF_NewSession(graph, sess_opts, status);\r\n    TF_DeleteSessionOptions(sess_opts);\r\n    if(TF_GetCode(status) != TF_OK)\r\n    {\r\n        printf(\"Can't new session: %s\\n\", TF_Message(status));\r\n        return NULL;\r\n    }\r\n    return sess;\r\n}\r\n\r\nTF_Buffer * buffer; // init is ignored\r\nTF_Status *status = TF_NewStatus();\r\nTF_Graph * graph = TF_NewGraph();\r\nTF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();\r\nTF_GraphImportGraphDef(graph, buffer, opts, status);\r\nif(TF_GetCode(status) != TF_OK)\r\n{\r\n    printf(\"Can't import GraphDef:%s\\n\", TF_Message(status));\r\n    return NULL;\r\n}\r\nTF_DeleteImportGraphDefOptions(opts);\r\n\r\n// for loop to init TF_Output by getting operation by name\r\nTF_Output feeds[4], fetches[4];\r\n(for loop)\r\n{\r\n   if(tf_utils_get_node(&(feeds[0], graph, \"ac_input\", 0) != 0)\r\n   {\r\n      printf(\"error...\\n\");\r\n      return;\r\n   }\r\n}\r\n\r\n// create session\r\nTF_Session *sess = tf_utils_new_session(graph, NULL, 0, status);\r\n\r\n// loop to init tensor for input\r\nTF_Tensor *  input_ts_begin, input_ts, input_length_ts, input_flag, pre_state_ts;\r\nint begin_input_buf_size, input_buf_size;// init by model info\r\n(for loop)\r\n{\r\n    int64_t dims[] = {1, begin_input_buf_size};\r\n    input_ts_begin = TF_AllocateTensor(TF_FLOAT, dims, 2, begin_input_buf_size * sizeof(float));\r\n    \r\n    dims[1] = input_buf_size;\r\n    input_ts = TF_AllocateTensor(TF_FLOAT, dims, 2, input_buf_size * sizeof(float));\r\n    // ignore others\r\n}\r\n\r\n// start warmup\r\nint *input_length = TF_TensorData(input_length_ts);\r\nint *input_flag = TF_TensorData(input_flag);\r\n// create feed data\r\nint64_t dims[] = {1, 0};\r\nTF_Tensor *input_ts = NULL;\r\ndims[1] = in_cols * num_sample;\r\ninput_ts = TF_AllocateTensor(TF_FLOAT, dims, 2,  dims[1] * sizeof(float));\r\n*input_length = num_sample;\r\n*input_flag = 0;\r\nfloat* input_buf; //init by real input data\r\nmemcpy(TF_TensorData(input_ts), input_buf, sizeof(float)*num_sample);\r\n\r\nTF_Tensor* feedValues[] = {input_ts, input_length_ts, pre_state_ts, input_flag};\r\nTF_Tensor* fetchValues[2] = {0};\r\n// crash occurs here!!!\r\nTF_SessionRun(sess,\r\n        NULL, // Run options.\r\n        feeds, feedValues,  num_inputs, // Input tensors, input tensor values, number of inputs.\r\n        fetches, fetchValues, num_outputs, // Output tensors, output tensor values, number of outputs.\r\n        NULL, 0, // Target operations, number of targets.\r\n        NULL, // Run metadata.\r\n        status // Output status.\r\n        );\r\n\r\n```\r\n\r\nIn my previous comment, crash occurs in TF_SessionRun. Can you help double-check whether it is OK?\r\n\r\n", "You have to build your application using the same C/C++/python toolchain as the one TF was build with. That's why building in the TF container worked.\r\n\r\nThis requirement is because of [ABI compatiblity](https://learnlistt.com/list/understanding-c-abi-compatibility) issues.", "@mihaimaruseac  As I can figured out it in Docker which is acceptable for us, should I close it?", "I think so, this seems to be caused by ABI breakages and using Docker should go around those issues.", "@fangminyu Thank for the additional information. It seems it is safe to close this issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40671\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40671\">No</a>\n"]}, {"number": 40670, "title": "Initialization of rmsprop optimizer in tensorflow", "body": "In tensorflow RMSPROP optimizer can be used with tf.compat.v1.train.RMSPropOptimizer function as follows.\r\n\r\ntf.compat.v1.train.RMSPropOptimizer(\r\n    learning_rate, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False,\r\n    centered=False, name='RMSProp'\r\n)\r\nBut it doesn't convey how to initialize moving average of the squared gradient for each weight.  Can anyone tell me how does tensorflow initialize it?", "comments": []}, {"number": 40669, "title": "Windowing on multiple timeseries", "body": "Usecase:\r\nI have 360 day history of 1000 company stocks. My requirement is to create overlapping windows of the timeseries data for every company stock.\r\n\r\nCurrent Solution (none TF one):\r\nCurrently I have implemented this by grouping the data on 'company' and applying a function that creates multi-step input and multistep output using pd.shift() function. Looking to upgrade the code using Tensorflow2.2 preprocessing functions.\r\n\r\nPartial solutions tried:\r\nI have tried the \"window\" function of the tf.data.Dataset that comprises of a single company's history. And I have tried the group_by_reducer function to group the data using 'company' as key. But could not figure a complete solution that can replace my current solution.\r\n\r\nPlease let me know if any code snippets are required. Appreciate any help or pointers on this.\r\n\r\nRegards,\r\nJo\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 40668, "title": "ImportError: No module named '_pywrap_tensorflow'", "body": "I installed tensorflow in VS Code on Python 3.7.7 (32-bit) using the code :\r\n ```pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl```\r\nIt successfully got installed at first, but when i tried to run a small code to check if it works it gave me error. I have referred to the earlier mentions of this issue and i have Microsoft Visual C++ 2015 already installed. \r\nHere is the code i tried to run -\r\n```#This file has been made to practice tensors/numpy/pandas\r\nimport tensorflow as tf\r\nimport numpy as np\r\nr0t = tf.constant(4)\r\nprint(r0t)```\r\n\r\n**Here the error i'm facing -**\r\n\r\n```PS D:\\programs\\Python> python tensor.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensor.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Harshit Paliwal\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nPS `D:\\programs\\Python>```\r\nI also reffered to the GitHub link given in the last 4th line but the page is no more available.\r\nPlease provide me with a working solution to this problem.\r\nThanks\r\n", "comments": ["@harshitpaliwal1011 \r\n\r\nPlease, fill i[ssue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Hey, @ravikyram \r\nI'm new to the platform, thanks for guiding me.\r\n\r\nMy system specifications are as follows:\r\n* Operating System - Windows 10 Pro (x 64 bit architecture)\r\n* Processor - Intel Core i3-2330M CPU @ 2.20GHz\r\n* RAM - 8 GB\r\n* Language - Python (Version 3.7.7) 32 bit architecture\r\n* Editor Used - VS Code \r\n* Installed pip - Version 20.1.1\r\n* Installed tensorflow - Version Unknown (Latest)\r\n\r\nHere is the elaborated description of the problem i faced:\r\n* I tried to upgrade my pip version in order to be able to install tensorflow\r\n![image](https://user-images.githubusercontent.com/54016415/85295453-92a00280-b4bd-11ea-84d1-33bea77357a0.png)\r\n\r\n* When i tried to install tensorflow with `pip install tensorflow` command, it didn't work out.\r\n![image](https://user-images.githubusercontent.com/54016415/85295694-ea3e6e00-b4bd-11ea-93af-28e55d25ef2e.png)\r\n\r\n* After trying some of the listed solutions to install tensorflow, i then switched to a virtual environment and updated pip to be able to install tensorflow\r\n![image](https://user-images.githubusercontent.com/54016415/85296025-66d14c80-b4be-11ea-995e-0abc510be264.png)\r\n\r\n* After all this i was finally able to install tensorflow with the help of this command\r\n`pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl`\r\n\r\n* But now whenever i try to import tensorflow\r\n  For example this code:\r\n![image](https://user-images.githubusercontent.com/54016415/85296832-96348900-b4bf-11ea-9884-969bcdc104d5.png)\r\n\r\n   It gives the above mentioned error.\r\n![image](https://user-images.githubusercontent.com/54016415/85296632-52da1a80-b4bf-11ea-9bcf-dc6541f99c5e.png)\r\n![image](https://user-images.githubusercontent.com/54016415/85296665-5ff70980-b4bf-11ea-9e4c-8f3a696455d8.png)\r\n\r\n", "@harshitpaliwal1011 \r\n\r\nCan you try with Windows 7 or later (64-bit) (Python 3 only)\r\nMicrosoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019.\r\n\r\nI don't think Tensorflow support 32 bit architecture.Please, refer the issue #32315\r\n\r\nPlease, follow the instructions from [here.](https://www.tensorflow.org/install/source_windows).Thanks!\r\n", "@ravikyram \r\nThanks, I'll try installing python 64 bit and then tensorflow.", "@ravikyram \r\nUpdate, thanks for the help.\r\nI successfully installed Python 3.7.8 (64 bit) and then installed tensorflow \r\nImported tensorflow and ran a small code. It seems to be working fine now.\r\nHere's the code snippet and the output.\r\n* Code:\r\n![image](https://user-images.githubusercontent.com/54016415/85425273-9a7fa580-b596-11ea-8f4c-2b2938b33ed8.png)\r\n\r\n* Output:\r\n![image](https://user-images.githubusercontent.com/54016415/85425349-b3885680-b596-11ea-89c3-95c3b832f70a.png)\r\n", "@harshitpaliwal1011 \r\n\r\nPlease, close this thread , since your issue was resolved. Thanks!"]}, {"number": 40667, "title": "having impport error ..tensorflow and tflearn", "body": "having impport error ..tensorflow and tflearn\r\n\r\nAttributeError: module 'importlib._bootstrap' has no attribute 'SourceFileLoader'\r\n\r\ni  am experiencing same import error in tflearn as well as tensorflow ..when i comment out import tensorflow same error is been reported as shown below ...\r\nalso thse libraries where working just fine a couple of days before ...i had imported them without showing errors .\r\ni have just added python socketio and flask packages in virtual env\r\nusing python 3.6.10\r\ntflearn :0.3\r\ntensorflow :1.14\r\n![Screenshot (65)](https://user-images.githubusercontent.com/65727171/85283770-890d9f00-b4ab-11ea-9d42-2094a54d75a7.png)\r\n![Screenshot (64)](https://user-images.githubusercontent.com/65727171/85283781-8dd25300-b4ab-11ea-91d2-69381ab042b5.png)\r\n\r\n_Originally posted by @BV-SS in https://github.com/tensorflow/tensorflow/issues/38589#issuecomment-647464978_", "comments": ["@BV-SS,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "Also, please take a look at [this](https://stackoverflow.com/a/47278208) StackOverflow comment from a similar issue and let us know if it helps. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40667\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40667\">No</a>\n"]}, {"number": 40666, "title": "Very strange Reshape layer behavier in Keras", "body": "\r\n**System information**\r\n- win10\r\n- TensorFlow installed from pip\r\n- TensorFlow version (use command below): 1.15.0, cpu version\r\n- Python version: 3.6\r\n- Keras: 2.2.4\r\n\r\n\r\n**Describe the current behavior**\r\nwhile I create a net,like this:\r\n\r\n```\r\nprint(priorbox3.shape) #got (?, 38, 38, 3, 8)\r\npriorbox3_reshape = Reshape((38*38*3, 4))(priorbox3)\r\nprint(priorbox3_reshape.shape) #(?, 4332, 4)\r\n```\r\n\r\nIt runs successfully, but in fact 38 * 38 * 3 * 8 != 4332 * 4!\r\n\r\n if I set Reshape((38 * 38 * 3, 8)), I will get a mismatch error.\r\n\r\nYou can download the code and run:\r\n\r\n```\r\npython libfacemodel.py\r\n```\r\n\r\n[code.zip](https://github.com/tensorflow/tensorflow/files/4812938/code.zip)\r\n\r\n", "comments": ["@fire717 \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b773342f5175609abe9e1e9d36038a66/untitled238.ipynb)", "> @fire717\r\n> I ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b773342f5175609abe9e1e9d36038a66/untitled238.ipynb)\r\n\r\nthx for reply, in your gist link, i just saw two blocks,\r\n```\r\npip install tensorflow==1.14\r\n\r\nprint(priorbox3.shape) #got (?, 38, 38, 3, 8)\r\npriorbox3_reshape = Reshape((38*38*3, 4))(priorbox3)\r\nprint(priorbox3_reshape.shape) #(?, 4332, 4)\r\n```\r\n\r\nof course, you'll get priorbox3 not define error.\r\n\r\nI didn't saw the 3 python files in code.zip I uploaded here.\r\n", "I've paste the 3 files and meet the same error :\r\n[gist here](https://colab.research.google.com/gist/fire717/d0ea1f2528680e4af4b4021af6f55c88/untitled238.ipynb)", "I sovled this by rewrite the layer function: compute_output_shape. "]}, {"number": 40665, "title": "Cross-compile TFLite aarch64 with TF ops", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: Python3\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1/7\r\n- GPU model and memory: -\r\n\r\n\r\n\r\nI'm trying to cross-compile TFLite model with TF ops with tensorflow/lite/delegates/flex:delegate dep, and it failed on @icu//:icuuc\r\n\r\nBuild:\r\n`bazel build -c opt --config=elinux_aarch64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl --config=monolithic //tensorflow/lite/examples/myexample:myexample`\r\n\r\nMy .tf_configure.bazelrc:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\n#build --config=xla\r\nbuild --config=tensorrt\r\nbuild --action_env TF_CUDA_VERSION=\"10.1\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-10.1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"3.5,7.0\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/usr/include/x64_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/x86_64-linux-gnu-gcc-7\"\r\n#build --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_env=LD_LIBRARY_PATH\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nError:\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/icu/BUILD.bazel:33:1: C++ compilation of rule '@icu//:icuuc' failed (Exit 1)\r\nexternal/icu/icu4c/source/common/brkiter.cpp:55:1: error: no declaration matches 'icu_60::BreakIterator* icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char*, UErrorCode&)'\r\n BreakIterator::buildInstance(const Locale& loc, const char *type, UErrorCode &status)\r\n ^~~~~~~~~~~~~\r\nIn file included from external/icu/icu4c/source/common/unicode/rbbi.h:28,\r\n                 from external/icu/icu4c/source/common/brkiter.cpp:27:\r\n/usr/include/unicode/brkiter.h:619:27: note: candidate is: 'static icu_60::BreakIterator* icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char*, int32_t, UErrorCode&)'\r\n     static BreakIterator* buildInstance(const Locale& loc, const char *type, int32_t kind, UErrorCode& status);\r\n                           ^~~~~~~~~~~~~\r\n/usr/include/unicode/brkiter.h:102:20: note: 'class icu_60::BreakIterator' defined here\r\n class U_COMMON_API BreakIterator : public UObject {\r\n                    ^~~~~~~~~~~~~\r\nexternal/icu/icu4c/source/common/brkiter.cpp: In static member function 'static icu_60::BreakIterator* icu_60::BreakIterator::makeInstance(const icu_60::Locale&, int32_t, UErrorCode&)':\r\nexternal/icu/icu4c/source/common/brkiter.cpp:415:70: error: no matching function for call to 'icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char [9], UErrorCode&)'\r\n         result = BreakIterator::buildInstance(loc, \"grapheme\", status);\r\n                                                                      ^\r\nIn file included from external/icu/icu4c/source/common/unicode/rbbi.h:28,\r\n                 from external/icu/icu4c/source/common/brkiter.cpp:27:\r\n/usr/include/unicode/brkiter.h:619:27: note: candidate: 'static icu_60::BreakIterator* icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char*, int32_t, UErrorCode&)'\r\n     static BreakIterator* buildInstance(const Locale& loc, const char *type, int32_t kind, UErrorCode& status);\r\n                           ^~~~~~~~~~~~~\r\n/usr/include/unicode/brkiter.h:619:27: note:   candidate expects 4 arguments, 3 provided\r\nexternal/icu/icu4c/source/common/brkiter.cpp:418:66: error: no matching function for call to 'icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char [5], UErrorCode&)'\r\n         result = BreakIterator::buildInstance(loc, \"word\", status);\r\n                                                                  ^\r\nIn file included from external/icu/icu4c/source/common/unicode/rbbi.h:28,\r\n                 from external/icu/icu4c/source/common/brkiter.cpp:27:\r\n/usr/include/unicode/brkiter.h:619:27: note: candidate: 'static icu_60::BreakIterator* icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char*, int32_t, UErrorCode&)'\r\n     static BreakIterator* buildInstance(const Locale& loc, const char *type, int32_t kind, UErrorCode& status);\r\n                           ^~~~~~~~~~~~~\r\n/usr/include/unicode/brkiter.h:619:27: note:   candidate expects 4 arguments, 3 provided\r\nexternal/icu/icu4c/source/common/brkiter.cpp:431:66: error: no matching function for call to 'icu_60::BreakIterator::buildInstance(const icu_60::Locale&, char [32], UErrorCode&)'\r\n         result = BreakIterator::buildInstance(loc, lbType, status);\r\n                                                                  ^\r\nIn file included from external/icu/icu4c/source/common/unicode/rbbi.h:28,\r\n                 from external/icu/icu4c/source/common/brkiter.cpp:27:\r\n/usr/include/unicode/brkiter.h:619:27: note: candidate: 'static icu_60::BreakIterator* icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char*, int32_t, UErrorCode&)'\r\n     static BreakIterator* buildInstance(const Locale& loc, const char *type, int32_t kind, UErrorCode& status);\r\n                           ^~~~~~~~~~~~~\r\n/usr/include/unicode/brkiter.h:619:27: note:   candidate expects 4 arguments, 3 provided\r\nexternal/icu/icu4c/source/common/brkiter.cpp:434:70: error: no matching function for call to 'icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char [9], UErrorCode&)'\r\n         result = BreakIterator::buildInstance(loc, \"sentence\", status);\r\n                                                                      ^\r\nIn file included from external/icu/icu4c/source/common/unicode/rbbi.h:28,\r\n                 from external/icu/icu4c/source/common/brkiter.cpp:27:\r\n/usr/include/unicode/brkiter.h:619:27: note: candidate: 'static icu_60::BreakIterator* icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char*, int32_t, UErrorCode&)'\r\n     static BreakIterator* buildInstance(const Locale& loc, const char *type, int32_t kind, UErrorCode& status);\r\n                           ^~~~~~~~~~~~~\r\n/usr/include/unicode/brkiter.h:619:27: note:   candidate expects 4 arguments, 3 provided\r\nexternal/icu/icu4c/source/common/brkiter.cpp:451:67: error: no matching function for call to 'icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char [6], UErrorCode&)'\r\n         result = BreakIterator::buildInstance(loc, \"title\", status);\r\n                                                                   ^\r\nIn file included from external/icu/icu4c/source/common/unicode/rbbi.h:28,\r\n                 from external/icu/icu4c/source/common/brkiter.cpp:27:\r\n/usr/include/unicode/brkiter.h:619:27: note: candidate: 'static icu_60::BreakIterator* icu_60::BreakIterator::buildInstance(const icu_60::Locale&, const char*, int32_t, UErrorCode&)'\r\n     static BreakIterator* buildInstance(const Locale& loc, const char *type, int32_t kind, UErrorCode& status);\r\n                           ^~~~~~~~~~~~~\r\n/usr/include/unicode/brkiter.h:619:27: note:   candidate expects 4 arguments, 3 provided\r\nTarget //tensorflow/lite/examples/myexample:myexample failed to build\r\n\r\n```", "comments": ["Since I don't have an access to your //tensorflow/lite/examples/myexample:myexample, I can't verify but the following command works for me. So the building flex:delegate doesn't have an issue.\r\n\r\n```\r\n$ bazel build -c opt --cxxopt=--std=c++14 --config=noaws --config=nogcp --config=nohdfs \\\r\n  --config=nonccl --config=monolithic tensorflow/lite/delegates/flex:delegate\r\n```\r\n\r\nThis is my .tf_configure.bazelrc. I wonder why you have \"build --config=tensorrt\".\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/buildtools/current/sitecustomize\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild --action_env PYTHONPATH=\"/usr/local/buildtools/current/sitecustomize\"\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```", "> Since I don't have an access to your //tensorflow/lite/examples/myexample:myexample, I can't verify but the following command works for me. So the building flex:delegate doesn't have an issue.\r\n> \r\n> ```\r\n> $ bazel build -c opt --cxxopt=--std=c++14 --config=noaws --config=nogcp --config=nohdfs \\\r\n>   --config=nonccl --config=monolithic tensorflow/lite/delegates/flex:delegate\r\n> ```\r\n> \r\n> This is my .tf_configure.bazelrc. I wonder why you have \"build --config=tensorrt\".\r\n> \r\n> ```\r\n> build --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\n> build --action_env PYTHON_LIB_PATH=\"/usr/local/buildtools/current/sitecustomize\"\r\n> build --python_path=\"/usr/bin/python3\"\r\n> build --action_env PYTHONPATH=\"/usr/local/buildtools/current/sitecustomize\"\r\n> build:opt --copt=-march=native\r\n> build:opt --copt=-Wno-sign-compare\r\n> build:opt --host_copt=-march=native\r\n> build:opt --define with_default_optimizations=true\r\n> test --flaky_test_attempts=3\r\n> test --test_size_filters=small,medium\r\n> test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\n> test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\n> test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\n> test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\n> build --action_env TF_CONFIGURE_IOS=\"0\"\r\n> ```\r\n\r\n\r\nWhen i'm trying to build flex delegate, i have the same error. And as i can see, in your command there aren't cross-compiling option for aarch64 --config=elinux_aarch64\r\n\r\nMy cross-compiling build:\r\n`bazel build --config=elinux_aarch64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl --config=monolithic tensorflow/lite/delegates/flex:delegate`\r\n\r\nError:\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/icu/BUILD.bazel:33:1: C++ compilation of rule '@icu//:icuuc' failed (Exit 1)\r\nexternal/icu/icu4c/source/common/uniset_props.cpp: In member function 'icu_60::UnicodeSet& icu_60::UnicodeSet::applyIntPropertyValue(UProperty, int32_t, UErrorCode&)':\r\nexternal/icu/icu4c/source/common/uniset_props.cpp:819:31: error: 'u_getBinaryPropertySet' was not declared in this scope\r\n             const USet *set = u_getBinaryPropertySet(prop, &ec);\r\n                               ^~~~~~~~~~~~~~~~~~~~~~\r\nexternal/icu/icu4c/source/common/uniset_props.cpp:819:31: note: suggested alternative: 'u_hasBinaryProperty_60'\r\n             const USet *set = u_getBinaryPropertySet(prop, &ec);\r\n                               ^~~~~~~~~~~~~~~~~~~~~~\r\n                               u_hasBinaryProperty_60\r\nTarget //tensorflow/lite/delegates/flex:delegate failed to build\r\n\r\n```\r\n\r\nP.S. I tried to remove \"build --config=tensorrt\" from bazelrc, but it didn't help", "Didn't you miss \" --cxxopt=--std=c++14\" ?", "yep, sry, i tried \r\n`bazel build --config=elinux_aarch64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl --config=monolithic --cxxopt=--std=c++14 tensorflow/lite/delegates/flex:delegate\r\n`\r\ntoo, but got same error", "I suspect some parameter in your  .tf_configure.bazelrc\r\nCould you try to run configure again? Please do not enable anything.\r\n```\r\nexport TF_ENABLE_XLA=0; ./configure\r\n```", "Same error, but maybe i need to add that without --config=elinux_aarch64 its building fine", "Could you share which hash tag of master branch you are using?", "r2.3, and now i just tried r2.2 - same thing", "I'm fine with r2.3.\r\nCould you share bazel ourput with \"-s\" option? It might worth to try to build it with Docker image to eliminate other external dependency. ", "Thank you for help! In CPU Docker image its fine, before i tried GPU Docker image", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40665\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40665\">No</a>\n"]}, {"number": 40664, "title": "Tensorflow import issues", "body": "i can't import tensorflow in python\r\nbecause it says 'ModuleNotFoundError: No module named 'tensorflow.contrib''\r\ntensorflow version is 2.x\r\nwhat should i do?\r\nwindows 8.1\r\ncomplete nooob to ml and ai\r\n(i am new to github)", "comments": ["please help :/\r\n", "@codey-code,\r\nSeems like you are trying to run code meant for TF 1.x on TF 2.x. \r\n\r\n`tensorflow.contrib` has been deprecation in TensorFlow 2.x. For more information, please take a look at [this guide](https://www.tensorflow.org/guide/migrate#a_note_on_slim_contriblayers). Thanks!", "a question: we can't import tensorflow 2.x ?\r\ni just used \"import tensorflow\"?", "anyways.... what version of tensorflow should i use?", "> a question: we can't import tensorflow 2.x ?\r\n\r\nPlease run the below code to check if TensorFlow was installed successfully. \r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n```\r\n\r\n\r\n> anyways.... what version of tensorflow should i use?\r\n\r\nYou can try running the code with TF v1.15.3. But I'd suggest you to migrate the code to TF 2.x since TF 1.x is not actively supported. Thanks!", "well it din't work\r\nimport tensorflow as tf\r\nprint(tf.__version__)", "What was the output? 'It didn't work\" does not give us actionable information and we cannot help you by guessing what the error was.\r\n\r\nAlso, when posting code, please use markdown formatting around it to make it readable. A readable and well written issue request gets solved ~10 times faster than one who is not like that, according to studies.", "sorry @mihaimaruseac here is the error\r\n```\r\n 'ModuleNotFoundError: No module named 'tensorflow.contrib''\r\n```\r\ni can't get through this to even check its version", "@codey-code,\r\nCould you please run the below commands and paste the output here.\r\n\r\n- `python3 --version`\r\n\r\n- `pip3 freeze`\r\n\r\nAlso, try creating a Python virtual environment and check if you are facing the same issue. Thanks!", "`pip list` should display versions for all installed packages. `tf.contrib` no longer exists since TF 2.0.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40664\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40664\">No</a>\n"]}, {"number": 40663, "title": "Coral edge TPU Classification coordinates ", "body": "Hi,\r\nI am running my object classification using Raspberry pi 4, model B, coral edge TPU. I am using this command to classify the image. \r\n\r\n    \u2018model.classify_with_image(frame, threshold=args[\u201cconfidence\u201d])\u2019\r\n\r\nIt works perfectly but It does not give me coordinates like \r\n\r\n    \u2018model.detect_with_image()\u2019 \r\n\r\nIs there any way I can get the coordinates?\r\n\r\nFrom the official documentation: \r\n\r\n    detection: \r\n    detect_with_image(img, threshold=0.1, top_k=3, keep_aspect_ratio=False, relative_coord=True, resample=0)\r\n    classification: \r\n    classify_with_image(img, threshold=0.1, top_k=3, resample=0)\r\n\r\n", "comments": ["The image classification model will predict the probability of an image representing a particular class.\r\nThus you can expect output as an array of probabilities between 0 and 1.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 40662, "title": "build", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["what is this question???", "@f18298335152h \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nRequest you to share colab link or simple standalone code to reproduce the issue.It helps us in localizing the issue faster.Thanks!"]}, {"number": 40661, "title": "build failed with build_all_android.sh as \"make: *** No rule to make target '/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX1', needed by '/home/jason/workspace/algo/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/android_armeabi-v7a/tensorflow/core/kernels/cast_op_impl_bool.o'.  Stop. make: *** Waiting for unfinished jobs....", "body": "Hi Guys,\r\nI need to generate tensorflow static library for 32bit or 64 bit Arm for qsee platform. This is my first time to use tensorflow. I am trying to use \"./tensorflow/contrib/makefile/build_all_android.sh \" to generate such static library. Unfortunetly, it build failed.\r\n\r\nBelow are the information I am using, and problem are also described. Do you have any idea about it. Appreciated very much ^ ^.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.5\r\n- Python version: Python 2.7.17\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source): android-ndk-r12b\r\n\r\n\r\n**Describe the problem**\r\nCan not build successful with script build_all_android.sh\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbuild command used: ./tensorflow/contrib/makefile/build_all_android.sh \r\n\r\n\r\n**Any other info / logs**\r\n**make: *** No rule to make target '/home/jason/workspace/algo/tf/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX1', needed by '/home/jason/workspace/algo/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/android_armeabi-v7a/tensorflow/core/kernels/cast_op_impl_bool.o'.  Stop.\r\nmake: *** Waiting for unfinished jobs....**\r\n\r\n", "comments": ["Hi guys,\r\n\r\nIs there any other way that can generate static libraryy tensoflowlite.a. ^ ^. or someone can share the experience of it. I've been trying many ways such as bazel build, still meet many errors. \r\n\r\nshould the tensorflow version be consistent with bazel version or android NDK version or GCC version? ^^", "Please, check these.\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_rpi\r\nhttps://www.tensorflow.org/lite/guide/build_arm64\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/make\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40661\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40661\">No</a>\n"]}, {"number": 40660, "title": "Getting Error while tranning rasa model.(rasa train)", "body": "Traceback (most recent call last):\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\AKSHU\\Anaconda3\\envs\\rasa\\Scripts\\rasa.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\rasa\\__main__.py\", line 82, in main\r\n    set_log_level(log_level)\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\rasa\\utils\\common.py\", line 71, in set_log_level\r\n    update_tensorflow_log_level()\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\rasa\\utils\\common.py\", line 112, in update_tensorflow_log_level\r\n    import tensorflow as tf\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\akshu\\anaconda3\\envs\\rasa\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@Akshusharma7\r\n\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from a similar issue and let us know if it helps. Thanks!", "Hi @Saduf2019 yeah i have went through the comment and i have uninstalled tensorflow==2.2.0 and installed tensorflow== 2.0.0. So now i am getting below error:\r\n\r\nrasa.core.policies.ensemble.InvalidPolicyConfig: Module for policy 'MemoizationPolicy' could not be loaded. Please make sure the name is a valid policy.\r\ncould you pls comment how to fixed it.", "@Akshusharma7 \r\nCan you share the complete stand alone indented code such that we can replicate the issue faced or if possible share colab gist with the error for us to analyse.\r\n\r\na per the error can you refer to [this link](https://github.com/RasaHQ/rasa/issues/3021) and let us know if it helps resolve the issue", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40660\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40660\">No</a>\n"]}]