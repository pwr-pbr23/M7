[{"number": 1510, "title": "Delete useless directory", "body": "", "comments": ["Merged.\n"]}, {"number": 1509, "title": "Fix python3 b", "body": "", "comments": []}, {"number": 1508, "title": "Fix python3 breakage (old-style exception block)", "body": "", "comments": ["#1507 does the same?\n", "Ah. I'll merge that.\n\nOn Mon, Mar 14, 2016 at 9:46 PM Vijay Vasudevan notifications@github.com\nwrote:\n\n> #1507 https://github.com/tensorflow/tensorflow/pull/1507 does the same?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub:\n> https://github.com/tensorflow/tensorflow/pull/1508#issuecomment-196655060\n", "Merged #1507 instead.\n"]}, {"number": 1507, "title": "quick python3 fix", "body": "use as instead of comma for python3 compatibility\n", "comments": ["Can one of the admins verify this patch?\n", "apparently only python 2.6 and above though.\nhttp://stackoverflow.com/questions/12682558/how-to-write-an-exception-catching-code-works-in-python2-4-to-python3\n", "Thanks! Just did the same without seeing yours. I'll merge yours (tests are in #1508). We'll need more fixed though.\n", "@martinwicke ya, I fixed one other simple one with the b missing in front of the string.  A lot of the others require diving into the C++ part and/or server/clients and haven't gone deep into them yet.\nThanks for merging mine and hopefully you don't mind I stepped on your path again.  :)\n", "No problem. Always nice to have backup. :)\n"]}, {"number": 1506, "title": "ImportError: No module named tensorboard", "body": "``` bash\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nImportError: No module named tensorboard\n```\n\nI noticed that this issue is related to #1199 \n\nIt occurs after upgraded to v0.7.1 from v0.6. Seems `pip` did not overwrite `/usr/local/bin/tensorboard` when installing `tensorflow`. And the following `import` statement in the old script cause the error:\n\n``` python\nfrom tensorflow.tensorboard.tensorboard import main\n```\n", "comments": ["I don't think this is the same as #1199, and I'm not sure why pip upgrade doesn't overwrite /usr/local/bin/tensorboard -- it might be a permissions issue.  Uninstalling the pip, removing /usr/local/bin/tensorboard and then reinstalling should solve the problem.\n", "Yes, removing the script and reinstalling can solve the problem. I installed the package with `sudo`, so it doesn't feel like a permission issue. Anyway, this might not be a `tensorflow` issue (:\n"]}, {"number": 1505, "title": "typos fix and ign temp files in gitignore", "body": "Just wanted to fix a simple typo in the documentation and noticed that some temporary files (e.g. .ipynb checkpoints and Mac DS_Store files for indexing) were not listed in the gitignore, yet, so that I just added them as well.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 1504, "title": "Disable run_and_gather_logs_test", "body": "The test is broken until proper dependencies are declared in workspace.bzl.\n", "comments": ["Jenkins, test this please.\n", "@ebrevdo, please re-enable the test once the dependency is there.\n"]}, {"number": 1503, "title": "[Feature Request] Graph embedding (DeepWalk, LINE, ...)", "body": "Hello,\n\nRecently algorithms such as DeepWalk or LINE, by applying word embedding techniques to graph, have opened a way to apply deep learning to graph analysis. I wonder if there is any future plan for Tensorflow to support graph / network analysis in general and graph embedding in specific? \n", "comments": ["I imagine we'd welcome contributions to https://github.com/tensorflow/models with graph embedding models. If there are any specific features missing from core TensorFlow that would enable these models to be expressed efficiently, please let us know!\n", "Adding contributions welcome in case someone feels like contributing a graph model to tensorflow/models.\n", "I'd like to implement deepwalk since it's related to my study.\n", "Can I implement DeepWalk algorithm for tensorflow framework and ask for a pull request?", "@suiyuan2009 @vsuriya93 TensorFlow itself is the wrong place for such an algorithm, but https://github.com/tensorflow/models would work.", "@vsuriya93 I'm not working on this currently, just go ahead!", "Thanks.. But @suiyuan2009 should I create a graph representation module then?", "@vsuriya93 , yes, I suggest Forward Star (Reverse Star) since you can allocate a continuous memory space before adding edge, if you know the graph size first. You can define your own input pipelines to load data to your graph tensors. You may need a path generator op. Graph is hard to partition, which may limits input size."]}, {"number": 1502, "title": "BatchNorm kernel?", "body": "Hello,\nAdding batch norm to a model results in a significant slowdown (using tf.nn.batch_normalize).\n\nI was wondering why a dedicated kernel was removed in favour of a symbolic approach?\n(seemingly unrelated commit in question: https://github.com/tensorflow/tensorflow/commit/03869e6e9cfac85d3e0f87ded01f4dd8de10ed67)\n\nWas this a design decisions to not want these \"one off\" style optimizations/kernels?\n\nHaving a faster kernel, (like cudnn #714, or something like what torch does) would be great if its not against tensorflows design. I will look into it if it is not.\n\nThank you.\n", "comments": ["Yeah, we've been fusing graph ops manually for performance, it'd be great to understand the rationale for going the other way.\n", "It looks like two commits were inadvertently squashed. Here's the intended description for the batch norm parts of that commit:\n\n> General batchnorm op, suitable to both global normalization for convolutions and per-depth normalization of fully-connected layers. It also supports arbitrary layouts and orderings of the axes.\n> \n> Nothing deep in these essentially 4 lines of code, but note:\n> - The ops are ordered in a way that makes the computation much more efficient (thx Sergey). None of the existing wrappers that didn't use the fused kernel did that, and as a result were almost 2x slower according to the benchmark.\n> - The benchmark confirms that this implementation is competitive with the handcrafted kernel.\n> - I confirmed on inception that the step time or accuracy are not affected.\n> - Added documentation and tests showing how the op can be used for arbitrary layouts.\n> - The change in API compared with batch_norm_with_global_normalization makes it possible to not have a scale tensor when it's not used, which caused some confusion and unnecessary additional saved variables in the past.\n\n@vincentvanhoucke is working on the related issue https://github.com/tensorflow/tensorflow/issues/1122 and might have more to add.\n", "If you're seeing a significant regression from the non-fused kernel, I'd be interested in hearing about it. Note that there were two changes:\n- removing the fused kernel, which did not affect performance on any benchmark I could run. In particular, this one:\n  https://github.com/tensorflow/tensorflow/blob/0249729a26b2cd3bdbbc5880f8d634a56860b0fd/tensorflow/python/ops/batch_norm_benchmark.py\n- replacing the two-pass moment accumulation algorithm with a one-pass algorithm. This was a tad slower on GPU, and faster on CPU.\n\nOn balance, those two changes gave us a lot more flexibility:\n- the one-pass moment accumulation removes a long-lived doubling of the memory footprint of the activations.\n- it makes it possible to compute moments over long sequences more efficiently.\n- the removal of the fused kernel generalized the computation to many more data shapes and layouts, and will enable for example batch-major and depth-major for convolutions.\n\nWe could recreate a fused op that implements the more flexible shapes if it significantly improved performance. It's also possible that a fused kernel for the one-pass moment computation wold be much more efficient. We need to know which is causing regressions first if there are any.\n", "Thank you all so much for the information. There is no regression, it just seemed slower than I would have guessed from experience in other frameworks. No cross library benchmarks were done.\n\nAs for the benchmark, the python version is indeed the fastest on the workloads I care about (CNN).\n\nIf I get a free moment I will look into a faster kernel and check performance myself.\n", "I'm trying out `batch_norm_benchmark.py` with NCHW layout and the performance seems to drop significantly.\nI might not test it in the right way: I ran `run_graph()` with `shape=[8,128,128,32],axes=[0,1,2]`(NHWC) and then \n`shape=[8,32,128,128], axes=[0,2,3]`(NCHW), on GPU, with `mode=py`. I saw a 30x performance difference.\nWill this be fixed?\n", "Hmm I actually just ran into this locally on my own cnn models. (Didn't think about that benchmark though.) I did a quick profile with cuda and found `reduce_sum`'s being the big killers in performance (Slowdown's occur in the `batch_normalize` function as well as the `moments` calculation.)\n", "@benoitsteiner, a few reports of very poor reduction performance for some layouts (see benchmark above). Can you triage or investigate?\n", "@ppwwyyxx @lukemetz if you could confirm which version you're using as well. I know that a lot of work has gone into improving reductions recently, so if you're not using the head of the tree I'd love to get confirmation that this is still an issue.\n", "@vincentvanhoucke I updated yesterday morning (63409bd23facad471973b110df998782c0e19c06).\nFurther on this point, the naive transpose batch norm transpose is a lot faster than just batch norm on NCHW.\n", "I compiled just now. And use the following main() function in `batch_norm_benchmark.py`:\n\n``` python\ndef main(unused_argv):\n  print(\"Forward convolution (lower layers).\")\n  shape = [8, 128, 128, 32]\n  axes = [0, 1, 2]\n  t2 = run_graph(\"gpu\", shape, axes, 10, \"py\", True, False, 50)\n  shape = [8, 32, 128, 128]\n  axes = [0, 2, 3]\n  t2 = run_graph(\"gpu\", shape, axes, 10, \"py\", True, False, 50)\n```\n\noutput:\n\n```\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.019783 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.365370 secs\n```\n", "In the first case (reduction axes = [0, 1, 2]) we have an optimized implementation leverages the gpu fairly well. In the second case (reduction axes = [0, 2, 3]), we use a basic approach that currently leverages few cuda threads to perform the reduction. This is why the second case is much slower. I'll see what can be done to improve the second case.\n", "I think batch normalization in cudnnv4 uses NCHW layout, and I suppose that will be fast. Not sure if you have plan to integrate those into tensorflow?\n", "@benoitsteiner Friendly ping to update this bug with the latest status, thanks!\n", "Using the timeline module I see that I have very slow batch normalization. One example of \"moments/sufficient_statistics/mean_ss\" took 413.262ms and \"gradients/batchnorm/add_1_grad/Sum_1\" took 413.357ms. I feel like this is a lot slower than I've seen before. As a sanity check, are these along the same order of magnitude you all are seeing in your benchmarks? This is with a Titan X.\n", "Is this at the head of the tree? We recently changed:\n          mean, variance = nn.moments(inputs, axis, shift=moving_mean)\nto:\n          shift = math_ops.add(moving_mean, 0)\n          mean, variance = nn.moments(inputs, axis, shift=shift)\nto fix a race condition, and this slowed things a tiny bit in our benchmarks.\nSince you're seeing a slow Sum, I wonder if it's related. What happens if you roll back that change?\n", "This is using the v0.10 release and not compiled from scratch at the head. Here is a script I built for a quick benchmark around just computing the moments (very non-rigorous, but hopefully helpful):\n\n```\nimport tensorflow as tf\nimport datetime\n\na = tf.ones([64, 256, 256, 32])\nmean, variance = tf.nn.moments(a, [0, 1, 2])\n\nwith tf.Session() as sess:\n    start = datetime.datetime.now()\n    result = sess.run([mean, variance])\n    end = datetime.datetime.now()\n\nprint(end - start)\n```\n\nOutput on CPU: 0:00:01.318907\nOutput on GPU: 0:00:00.651986\n\nThis is basically BN after my first convolution; perhaps these times are actually reasonable? It is a fairly large matrix.\n", "Just as a followup, in my timeline the Sum op seems really expensive throughout my graph (mostly related to BN and gradients) and is blocking for most of the other computation. Is this expected behavior? \n\n<img width=\"768\" alt=\"timeline\" src=\"https://cloud.githubusercontent.com/assets/1364252/18144914/eccdc898-6f7d-11e6-912c-9a3b687001bb.png\">\n", "That looks over the top to me. Punting so someone who can actually help.\n", "If I recall correctly, the 0.10 _release candidate_ binary had a performance regression in some of the reduction kernels (on certain GPUs). I believe this is fixed now at HEAD, and will be in the actual _release_ which is due any day now.\n\nCan you try with the nightly build and confirm whether this is still a problem?\n", "Wow. Night and day difference. Using the nightly build I gained about a 3X speedup and all the long summations in the timeline have disappeared. Thank you!\n", "That's a relief! Thanks for confirming our suspicions :).\n", "Batch norm on NCHW layout is still over 10x slower (after half a year) as discussed above. It is mentioned in #1759 that there will be a cudnn integration. Do we have a time frame about that?\n", "It's in the master branch already, named `nn.fused_batch_norm`. Have a look at #4919 and #4899.\n"]}, {"number": 1501, "title": "ImportError: libcudart.so.7.5: cannot open shared object file", "body": "For bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Fedora 23\n\nIf installed from sources, provide the commit hash:\nb44a68232087284eccd55c6031e7fc3ebe5663ab\n### Steps to reproduce\n1.  ./configure with GPU support in /usr/local/cuda \n2.  build tests with --config=cuda option\n   3.\n### What have you tried?\n1.  building tests without --config=cuda has no problem\n2.  made sure libcudart.so.7.5 exists in /usr/local/cuda/lib64 (as a soft link to libcudart.so.7.5.18)\n3.  ./configure with /usr/local/cuda/lib64 (says file not found)\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nhttp://pastebin.com/xKrvzawQ\n", "comments": ["You might be missing the [shell environment variables](https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#test-the-tensorflow-installation):\n\n```\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64\"\nexport CUDA_HOME=/usr/local/cuda\n```\n\nThese are used during runtime (versus compile time) cuda shared library lookup.\n", "Thanks, I missed the CUDA_HOME variable, but I'm now getting this error.\n\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n--------------------------------------------------------------\ntensorflow/core/common_runtime_gpu_gpu_stream_util_test: error while loading shared libraries: libcudart.so.7.5: cannot open shared object file: No such file or directory\n", "I also have /usr/local/cuda/lib64 in PYTHONPATH\n", "New issue came up while trying to fix this.  Will open a new issue and close this.\n", "Where is the new issue posted ?\n", "I was getting similar error: \"ImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory\")\n\nI just did @moonboots answer and restared my ubuntu 16.04 and everything has been working well yet.\n\nhttp://www.deviantpics.com/images/2016/10/24/cuda%20radi.png\n", "I ran into it on Ubuntu14.04 trying to install Pycuda and I think Moonboots answer is correct; it worked for me, and I realise that the crucial point is about environment variables ( I mainly make executables ), \r\nThese are used during runtime (versus compile time)", "Just noticed this.  The new issue is #1701 and #2053 solved it.", "Hi there,Had similar trying to get PyCuda going, and took a suggestion from elsewhere to be careful about paths; I use Ubuntu, and this sort of did the trickexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64\"export CUDA_HOME=/usr/local/cudain bashrcI was using Nsight to compile, and forgot that it uses internal paths\u00a0Hope that helps!-----chemelnucfin <notifications@github.com> wrote: -----To: tensorflow/tensorflow <tensorflow@noreply.github.com>From: chemelnucfin <notifications@github.com>Date: 02/07/2017 01:06AMCc: cjfpainter <chris.painter@zigzag.co.uk>, Comment <comment@noreply.github.com>Subject: Re: [tensorflow/tensorflow] ImportError: libcudart.so.7.5: cannot open shared object file (#1501)Just noticed this.  The new issue is #1701 and #2053 solved it.\n\n\u2014You are receiving this because you commented.Reply to this email directly, view it on GitHub, or mute the thread.", "Hi, I am using Ubuntu14.04 with pycharm , but the  environment variables setting didn't work for me.\r\n\r\nWeird is that I can run it in a terminal, but not in pycharm.", "Still experiencing the same on Ubuntu 16.04. Frustrating.", "> Hi, I am using Ubuntu14.04 with pycharm , but the environment variables setting didn't work for me.\r\n> \r\n> Weird is that I can run it in a terminal, but not in pycharm.\r\n\r\nIn pycharm, I tried to revise the path variable in run configuration. And it works.", "> > Hi, I am using Ubuntu14.04 with pycharm , but the environment variables setting didn't work for me.\r\n> > Weird is that I can run it in a terminal, but not in pycharm.\r\n> \r\n> In pycharm, I tried to revise the path variable in run configuration. And it works.\r\n\r\nThat's true. Thank you. I should also update earlier here. Sorry."]}, {"number": 1500, "title": "beam search in translation model", "body": "Do we have plan to support beam search in the decoding process of translation model, since the raw paper shows better result with beam search?\n", "comments": ["Does the [`tf.contrib.ctc.ctc_beam_search_decoder()`](https://github.com/tensorflow/tensorflow/blob/b9e9c9a12dbd2946d44c76abdd603eedf6eedb9a/tensorflow/contrib/ctc/ctc_ops.py#L183) help here?\n", "@mrry thanks for your quick response, I have no ideas about how to apply ctc_beam_search_decoder to the attention-based LSTM decoding process of the seq2seq model used in translation model in tutorial, since the interface looks different, @ebrevdo could you help double confirm?\n", "Nope - ctc_beam_search_decoer is only meant to be used with the ctc_loss and not more general seq2seq models.  Sorry!  You'll have to write your own beam search for now.\n", "@mrry @ebrevdo thanks for your help, I just found this feature had been discussed before #654 , I will close this issue.\n"]}, {"number": 1499, "title": "'Math Processing Error' in web docs", "body": "See; tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html  --> Training section. \"Math Processing Error\" appears in multiple places in page. Used OSX / Safari, Firefox, and Chrome.  Screen shot attached.\n![screen shot 2016-03-14 at 3 15 14 pm](https://cloud.githubusercontent.com/assets/351870/13761735/a3f2a3b6-e9f7-11e5-9373-5b7eb37fd534.png)\n", "comments": ["It looks like the encoded apostrophes are causing problems for rendering on OS X. Perhaps replacing them with `^\\prime` would work?\n\nAssigning to @martinwicke since he knows the website generator better than I do.\n", "Closing this as a duplicate of https://github.com/tensorflow/tensorflow/issues/1377.\nWe're working on a fix and it should be in soon - as a workaround for now, you can right click on the equation and select the SVG renderer.\n"]}, {"number": 1498, "title": "bazel build nvcc: nvcc fatal : Unsupported gpu architecture 'compute_21'", "body": "After I executed the command\n`bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`\n\nI get an error like \n`ERROR: /home/rajarshee/tensorflow/tensorflow/core/kernels/BUILD:517:1: error while parsing .d file: /home/rajarshee/.cache/bazel/_bazel_rajarshee/0d043bf46cad9f31127eb8d06453610d/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/adjust_contrast_op_gpu/tensorflow/core/kernels/adjust_contrast_op_gpu.cu.d (No such file or directory).\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc fatal   : Unsupported gpu architecture 'compute_21'\nTarget //tensorflow/cc:tutorials_example_trainer failed to build`\n\nI provided 2.1 gpu compute capability in ./configure earlier.\n", "comments": ["Thanks to @zheng-xq, we figured out that `compute_21` is not a valid architecture (instead it is a virtual architecture). We need to provide a way to pass a compute capability of **2.0** to `nvcc`. Currently our build scripts do not support that, but you can hack in support by editing the [crosstool wrapper](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc). After you rung `./configure`, replace the loop on [L253--256](https://github.com/tensorflow/tensorflow/blob/e4add493f1020c0eb986aba21c266d9e6e6f4182/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc#L253) with the following:\n\n``` python\nnvccopts += r'-gencode=arch=compute_20,\\\"code=sm_21,compute_20\\\" '\n```\n", "While the previous error is no more appearing, a new incompatibility with my gcc version has occured. please note that i have both 5.2 and 4.9 versions of gcc and my cuda located at /usr/local/cuda has the gcc already linked to /usr/bin/gcc-4.9\n\n`error -- unsupported GNU version! gcc versions later than 4.9 are not supported!`\n", "@mrry \nAnd after I changed gcc to gcc-4.9 [here](https://github.com/tensorflow/tensorflow/blob/e4add493f1020c0eb986aba21c266d9e6e6f4182/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc#L47) the error was gone but this error came \n\n`9 errors detected in the compilation of \"/tmp/tmpxft_00005b89_00000000-7_adjust_contrast_op_gpu.cu.cpp1.ii\".\nERROR: /home/rajarshee/tensorflow/tensorflow/core/kernels/BUILD:517:1: output 'tensorflow/core/kernels/_objs/adjust_contrast_op_gpu/tensorflow/core/kernels/adjust_contrast_op_gpu.cu.o' was not created.\nERROR: /home/rajarshee/tensorflow/tensorflow/core/kernels/BUILD:517:1: not all outputs were created.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build`\n", "@rajarsheem, you'll have to post the errors here. In general, TF is tested on compute_35. While compute_20 may work for now, which I am not sure, there is no guarantee that it will keep working in the long term.\n", "@zheng-xq \n`INFO: Found 1 target...\nERROR: /home/rajarshee/tensorflow/tensorflow/core/kernels/BUILD:212:1: error while parsing .d file: /home/rajarshee/.cache/bazel/_bazel_rajarshee/0d043bf46cad9f31127eb8d06453610d/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/gather_op_gpu/tensorflow/core/kernels/gather_op_gpu.cu.d (No such file or directory).\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nIn file included from third_party/gpus/cuda/include/cuda_runtime.h:76:0,\n                 from <command-line>:0:\nthird_party/gpus/cuda/include/host_config.h:115:2: error: #error -- unsupported GNU version! gcc versions later than 4.9 are not supported!\n #error -- unsupported GNU version! gcc versions later than 4.9 are not supported!\n  ^\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 1.447s, Critical Path: 1.19s`\n", "> > unsupported GNU version! gcc versions later than 4.9 are not supported!\n\nThis problem is that your GCC and NVCC is not compatible. Which Cuda SDK\nare you using? Could you try the latest Cuda 7.5 and GCC 4.8, which are\nheavily tested.\n\nThanks.\n-XQ\n\nOn Mon, Mar 14, 2016 at 1:04 PM, Rajarshee Mitra notifications@github.com\nwrote:\n\n> @zheng-xq https://github.com/zheng-xq\n> INFO: Found 1 target...\n> ERROR: /home/rajarshee/tensorflow/tensorflow/core/kernels/BUILD:212:1:\n> error while parsing .d file:\n> /home/rajarshee/.cache/bazel/_bazel_rajarshee/0d043bf46cad9f31127eb8d06453610d/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/gather_op_gpu/tensorflow/core/kernels/gather_op_gpu.cu.d\n> (No such file or directory).\n> nvcc warning : option '--relaxed-constexpr' has been deprecated and\n> replaced by option '--expt-relaxed-constexpr'.\n> In file included from third_party/gpus/cuda/include/cuda_runtime.h:76:0,\n> from <command-line>:0:\n> third_party/gpus/cuda/include/host_config.h:115:2: error: #error --\n> unsupported GNU version! gcc versions later than 4.9 are not supported!\n> #error -- unsupported GNU version! gcc versions later than 4.9 are not\n> supported!\n> ^\n> Target //tensorflow/cc:tutorials_example_trainer failed to build\n> INFO: Elapsed time: 1.447s, Critical Path: 1.19s\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196498691\n> .\n", "@zheng-xq I am using Cuda 7.5 and two gcc versions installled - 4.9 and 5.2. the cuda located in /usr/local/cuda has it's gcc _linked_ to gcc-4.9.\n(gcc incompatibility earlier occured with theano and issue was solved after the linking)\n", "I don't think GCC 4.9 is officially supported by Cuda 7.5. Look at\ninclude/host_config.h\n\n#if defined(**GNUC**)\n#if **GNUC** > 4 || (**GNUC** == 4 && **GNUC_MINOR** > 9)\n#error -- unsupported GNU version! gcc versions later than 4.9 are not\nsupported!\n#endif /\\* **GNUC** > 4 || (**GNUC** == 4 && **GNUC_MINOR** > 9) */\n\nYou are better off with GCC 4.8 and use that instead.\n\nThanks.\n-XQ\n\nOn Mon, Mar 14, 2016 at 1:19 PM, Rajarshee Mitra notifications@github.com\nwrote:\n\n> I am using Cuda 7.5 and two gcc versions installled - 4.9 and 5.2. the\n> cuda located in /usr/local/cuda has it's gcc _linked_ to gcc-4.9. (the\n> gcc incompatibility issue earlier occured with theano and was solved after\n> the linking)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196502623\n> .\n", "@zheng-xq  upto 4.9 is supported.\n`#if **GNUC** > 4 || (**GNUC** == 4 && **GNUC_MINOR** > 9)\n\n#error -- unsupported GNU version! gcc versions later than 4.9 are not supported!`\n", "I stand corrected. It is a good idea to check which version of your GCC was\nactually used. A few things you can try:\n- You can add \"-s\" to your bazel to print out the crosstools command line.\n- Add \"--copt=--cuda_log\", which is passed to crosstools to print out the\n  nvcc command line.\n\nThat should be enough to see which version of the gcc was actually used.\n\nOn Mon, Mar 14, 2016 at 1:42 PM, Rajarshee Mitra notifications@github.com\nwrote:\n\n> @zheng-xq https://github.com/zheng-xq upto 4.9 is supported.\n> `#if _GNUC_ > 4 || (_GNUC_ == 4 && _GNUC_MINOR_ > 9)\n> \n> #error -- unsupported GNU version! gcc versions later than 4.9 are not\n> supported!`\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196512886\n> .\n", "@zheng-xq  May this be of help. \n\nhttp://paste.ubuntu.com/15387128/\n(note that default /usr/bin/gcc is actually the 5.2 one!)\n", "I think that might be the problem. The default gcc 5.2 is used instead. You\nhave a few options:\n- Symlink your default /usr/bin/gcc to gcc 4.9.\n- Modify\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc to\n  point to your gcc-4.9 location.\n\nOn Mon, Mar 14, 2016 at 2:18 PM, Rajarshee Mitra notifications@github.com\nwrote:\n\n> @zheng-xq https://github.com/zheng-xq May this be of help.\n> \n> http://paste.ubuntu.com/15387128/\n> (note that default /usr/bin/gcc is actually the 5.2 one!)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196524722\n> .\n", "@zheng-xq \nI changed all the variables below\nCPU_COMPILER,GCC_HOST_COMPILER_PATH and LLVM_HOST_COMPILER_PATH by replacing gcc with gcc-4.9. While the gcc issue seems to be vanished, this came:\n`5 errors detected in the compilation of \"/tmp/tmpxft_00005de0_00000000-7_sparse_tensor_dense_matmul_op_gpu.cu.cpp1.ii\".`\nFull output:\nhttp://paste.ubuntu.com/15387425/\n", "Unfortunately, Eigen starts to pick up warp shuffle capabilities recently,\nwhich requires compute_30 support.\n\nOn Mon, Mar 14, 2016 at 3:09 PM, Rajarshee Mitra notifications@github.com\nwrote:\n\n> @zheng-xq https://github.com/zheng-xq\n> I changed all the variables below\n> CPU_COMPILER,GCC_HOST_COMPILER_PATH and LLVM_HOST_COMPILER_PATH by\n> replacing gcc with gcc-4.9. While the gcc issue seems to be vanished, this\n> came:\n> 5 errors detected in the compilation of\n> \"/tmp/tmpxft_00005de0_00000000-7_sparse_tensor_dense_matmul_op_gpu.cu.cpp1.ii\".\n> Full output:\n> http://paste.ubuntu.com/15387425/\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196542169\n> .\n", "@benoitsteiner to comment on the Eigen request.\n\n@rajarsheem in this thread wants to support compute_20 device. However, since Eigen starts to use warp shuffling instructions, which is only available for compute_30 devices and later, it won't work.\n\nBenoit can tell you whether Eigen will support older devices without the warp shuffling or not. \n", "Official support for compute_20 device will be highly appreciated. \nFor now, let's count on @benoitsteiner ; may he throw some light.\n", "I'm having the same problem. Did someone find a solution to make Tensorflow work with compute capability 2.1 GPU's ?\n", "Eigen only supports compute 3.0 and above. Old GPUs tend to run slower than a decent CPU these days, so there is little incentive to remove this restriction at the moment.\n\n@rajarsheem @marcdumon: What are your use cases for GPUs that only support compute capabilities 2.1 ?\n", "Automatically closing because there was no response. Please reopen if it is still an issue.\n", "@benoitsteiner : Have witnessed that computations with my gpu (2.1) tend to be much faster than with just CPU when I used previously with Theano. Even that performance improvement can be highly beneficial.\n", "@rajarsheem I have fixed the Eigen reductions to remove the dependency on warp shuffles. The next time we update TensorFlow to pick up the latest version of Eigen it should be possible to run with older gpus.\n", "hey! @benoitsteiner When it could happen? =)\n", "@AlexJoz I checked that the code now compiles with architecture 2.0 and cuda 7.0. \n"]}, {"number": 1497, "title": "Hey, I wanted to know where can i find the implementation of the tensorflow with respect to devices like GPU. Basically I want to understand from source code level what changes are done inside the tensor flow to make it comptabile to GPU when launched. How  does the CUDA compile/run come into play when a session is launched ? Appreciate the help/pointers.", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["I would ask this question on StackOverflow, since this isn't a bug or feature request, thanks!\n"]}, {"number": 1496, "title": "fix crash for empty dev_set bucket", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1495, "title": "fix virtualenv pip testing", "body": "", "comments": ["@caisq how about this? IT is similar to what you had there some time ago. It should work.\n\nBTW: It may work completely without the second pip install if I do another thing I wanted to do ... one day ... for now this should work, right?\n"]}, {"number": 1494, "title": "Does the tensorflow support structured numpy array as inputs?", "body": " data = np.array([(1,2.), (2,3.)],dtype=[('foo', 'i4'),('bar', 'f4')])\n\nI want to feed this into\nx = tf.placeholder(\"float\", [None,2])\n\nWill the tensorflow automatically convert they datatypes?\n", "comments": ["At present, TensorFlow only supports primitive NumPy datatypes. If you need to represent a tensor of a tuple of values, you can instead use a (Python) tuple of tensors (see e.g. the [`tf.QueueBase`](https://www.tensorflow.org/versions/r0.7/api_docs/python/io_ops.html#QueueBase) documentation for an example).\n"]}, {"number": 1493, "title": "fixes #1423 - add check for tests to see if tensorflow was built with gpu", "body": "EDIT:\nThe changes worked on CPU, but I would like to test GPU just to make sure.  I opened an issue for GPU test failure #1501\n", "comments": ["Can one of the admins verify this patch?\n", "Yes, you cannot import tensorflow from anywhere inside tensorflow, except for inside tests.\n", "I see.  I couldn't import test before, but I guess \"test\" was already imported.  I've finished a fix, just running it through my CPU tests before uploading it (it takes about an hour now after rebase) :( .  I still have not figured out how to the GPU tests yet though.\n", "Jenkins, test this please.\n", "@martinwicke Hello, I saw that the GPU test was failing.  I apologize, I couldn't build with GPU to test on my machine.  It's a little strange that the CPU test can import test but not the GPU test.\n\nI'll try to resolve the testing locally, but I reinstalled everything from the beginning trying to fix it and followed the instructions.  Currently I can't even test the CPU, but I did built the wheel to install locally.  I can't build the GPU though.\n", "@martinwicke let me try setting up the virtualenv\n", "Any luck?\n", "Can one of the admins verify this patch?\n", "@martinwicke Sorry, nope.  Sometimes it's with protobuf, sometimes it's with cuda, sometimes it's with bazel, sometimes it's with blaze.  Kinda frustrating.  How do you guys deal with build config issues internally?\n", "Hm... I usually use virtualenv which makes this pretty straightforward. For maximum isolation from problems, you can try the ci_build scripts. They set up a docker container and install all the prerequisites inside. Then you can work inside there.\n\nYou do need the latest bazel, and the key is to have no incompatible protobuf and/or tensorflow installed. That's surpsingly hard to enforce, but virtualenv takes away most of the pain according to my experience.\n", "Do you want to update this PR? It seems it's a simple python syntax thing in test_util_test.\n", "@martinwicke I haven't figured out how to get tensorflow from source (especially with GPU).   For one thing, there seems to be a bazel issue upstream. https://github.com/bazelbuild/bazel/issues/1193\n\nThere was one issue that was fixed with the GPU using --genrule=standalone from #2053 though.\n\nI looked over the tests, it is a bit strange that the CPU tests pass, but not the GPU tests.  Perhaps you might know the reason for that and/or can help me fix it and test it quickly?  I feel bad holding you guys up on this too.\n", "@caisq: This fails only GPU_PIP, because tf.test is not available there, is that expected? It's possible we simply have to blacklist this test, but I may be missing something.\n", "Ping @caisq \n", "ping @caisq @martinwicke \n", "@vjv @martinwicke I did get testing working on gpu also last weekend.\nHowever I mostly can only work on tensorflow on weekends now.\n\nOn Tue, May 24, 2016, 12:59 PM Vijay Vasudevan notifications@github.com\nwrote:\n\n> ping @caisq https://github.com/caisq @martinwicke\n> https://github.com/martinwicke\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1493#issuecomment-221376159\n", "@caisq should we blacklist this test in the PIP build? Or should it be able to run?\n", "Closing due to inactivity, we can re-open if there's any more traction or debugging on this.  I suspect our testing team will at some point try to address these issues for load reasons.\n"]}, {"number": 1492, "title": "R0.7", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "(If you sign the CLA, comment here and we'll reopen)\n"]}, {"number": 1491, "title": "simple ci build improvements", "body": "- make the user inside docker sudoer without password\n- improve documentation\n", "comments": ["Can you rebase?\n", "@tensorflow-jenkins test this please\n", "@caisq does this readme look better to you? Should I merge it?\n"]}, {"number": 1490, "title": "Fix broken android example README link ", "body": "This PR fixes the link to the WORKSPACE file, highlights the relevant sections of that file, and recommends a faster command to initialize the protobuf submodule. I'm looking forward to getting this running on my phone! (I'm working through some ndk issues. I think bazel/protobuf depends on 32-bit r10e).\n", "comments": ["Can one of the admins verify this patch?\n", "Nice!  Can you squash the three commits into one?  I'll merge right after.\n", "Squashed, thanks!\n"]}, {"number": 1489, "title": "Fixing mac-pip builds", "body": "The --system-site-package option flag in PR 1472 broke Mac pip builds due to existing packages on the system. For example, see: http://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-mac-python2-copt_both/17/console\n\nGenerally speaking, the --system-site-package option makes the virtualenv environment depend on the packages on the Mac host. Removing the option gets rid of this dependency and the need to maintain the host packages, which will save work (especially if there are multiple Mac slaves). \n\nThe cost of installing the packages in the virtualenv should be minimal for both Mac and Linux, now that we cache the virtualenv folder by default. \n\nThe PR has been tested manually for both python2 and python3 on Mac. See:\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-mac-python2-copt_both/18/console\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-mac-python3-copt_both/4/console\n", "comments": ["And how is \"blaze test\" without pip.sh going to work?\n", "@jendap For Mac, \"blaze test\" should work as long as the correct versions of dependent python packages (numpy, scipy and sklearn) are installed on he system. I think the main problem with \"--system-site-packages\" is that if TensorFlow is already installed on the system, \"pip install\" in virtualenv from pip.sh will not update it, if the version number has not increased. Even the \"--force-reinstall\" option does not work. \n", "It would be great to have consistency with what bazel see and what virtualenv see.\n\nLooking at the output from your build 17 - it says \"Requirement already satisfied (use --upgrade to upgrade): tensorflow==0.7.1\". Have you tried adding \"--upgrade\" to pip install?\n\nIdeally it should upgrade tensorflow but not the other packages.\n", "@caisq This is what we need: `pip install --upgrade --no-deps --force-reinstall <packagename>` (see http://stackoverflow.com/a/27254355/5131748)\n", "Take a look at https://github.com/tensorflow/tensorflow/pull/1495. It does exactly what we need.\n", "Added the --upgrade suggested in #1495. Tested manually on mac, cpu and gpu slaves. Ready to merge. \n", "@tensorflow-jenkins test this please\n", "I merged #1495 so I'm closing this.\n"]}, {"number": 1488, "title": "ci_build: add cmake support", "body": "", "comments": ["@tensorflow-jenkins test this please\n", "The readme should be, imho, split by platform first. The interleaved ubuntu and windws is probably hard to read for both camps. But that is for some other PR.\n", "Can you rebase?\n", "Sure. Done.\n"]}, {"number": 1487, "title": "make xrange python 2 and 3 compatible in sdca_ops.py", "body": "use six.moves.range instead of xrange for python2/3 compatibility.\n\nBy the way, are there ways to use jenkins remotely to test on personal forks?  It takes a long time to run the entire test suite and renders my computer almost unusable for other tasks.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please.\n", "@vrv yup, those are the exact same tests on python3 that failed on mine too.  I just fixed an easy one.\n", "sounds good, thanks!\n\nas for remote testing -- unfortunately only admins can trigger tests right now.\n"]}, {"number": 1486, "title": "DocBug", "body": "This tutorial *_will how you *_how to run the example script on your own images, and will explain some of the options you have to help control the training process.\n\nfound at\nhttps://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html\n", "comments": ["It looks like the issue has been fixed in the code, probably pending a website update.\n", "Looks like this fix is now live, thanks!\n"]}, {"number": 1485, "title": "Fix the download of the notMNIST dataset.", "body": "It seems the Apache server at http://yaroslavvb.com is configured to\nallow access to only specific user agents. By passing a user agent\nstring like wget or Mozilla, it could be tricked into allowing the\nnotebook to download the dataset. It is an ugly hack for urllib, would\nhave been simpler if the notebook used the requests module :-)\n", "comments": ["Can one of the admins verify this patch?\n", "We're moving the data to a better host, stay tuned.\n", "It sounds like somebody enabled the filtering for a reason. And downloads from tensorflow could be that reason.\n\n:+1: for a different host\n", "The new URL:\n`url = 'http://commondatastorage.googleapis.com/books1000/'`\nLet me know if that one is still an issue. I'll update the container soon.\n", "The new URL works. I will close the PR. Thanks :)\n"]}, {"number": 1484, "title": "Add space before filename in the error message of maybe_download().", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1483, "title": "The error message in maybe_download was missing a space.", "body": "Add a space before concatenating filename in the error message of function maybe_download\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Signed the CLA with a different email address. Will make a new PR, closing this one. \n"]}, {"number": 1482, "title": "getting the dynamic shape of a Tensor as an integer", "body": "How can I get xshape as a int variable? Is this feature available?\n\nOf course if I evaluate, I will get the value of xshape, but the input is dynamic and I don't want to use the get_sum function in for loop, because it would break the graph.\n\nProblem: TypeError: range() integer end argument expected, got Tensor\n\n```\nimport tensorflow as tf\nimport numpy as np\n\n\nx = tf.placeholder(tf.float32, shape=[None])\nxshape = tf.placeholder(tf.float32, shape=[])\n\n\ndef get_sum(x, xshape):\n  sum = 0\n  for i in range(xshape):\n    sum += x[i]\n\ninit = tf.initialize_all_variables()\nsum = get_sum(x, xshape)\n\nwith tf.Session() as sess:\n  sess.run(init)\n  for i in range(100):\n    length = np.random.randint(0,10)\n    a = np.random.randint(0, 10, length)\n  print sess.run(sum,feed_dict={x:a, xshape:length})\n```\n", "comments": ["It's not clear to me why you want to get the value of xshape, which you are using as an input feed variable. Also why would you want to do a sum in a for loop like that?\nWhat are you trying to achieve?\n", "I want to do process on variable length tensors. In other words I need to do a computation for each element in a tensor with different values. Getting the shape as an integer is a first step.\n", "The only way to get the dynamic value of a tensor as a Python integer is to pass it to `sess.run()`. However, that doesn't really solve your problem, as you want to have data-dependent control flow in your graph. One way to do this is to upgrade to the latest nightly version of TensorFlow, and try the `tf.foldl()` or `tf.foldr()` functions, which operate like `scan()` in Theano to iterate over one dimension of a tensor.\n", "In my experience, if you need to do something 'iteratively' in tensorflow then you are most likely thinking about it wrong. Maybe you can formulate your task as a multiplication of matrices or something.\n", "Indeed, for this case it would be more appropriate to use `tf.reduce_sum()`. However, if you need to define a recurrent network, where the input to an op depends on its output in a previous iteration in some non-trivial way, then the support for iteration will be useful.\n", "Assigning @yuanbyu, who might have more information about the current status of the control flow ops.\n", "You could use fold to achieve what you wanted to do:\n\n```\nx = tf.placeholder(tf.float32, shape=[None])\ninit = tf.initialize_all_variables()\nsum = tf.foldl(lambda a, e: a + e, x, initializer=0.0)\nwith tf.Session() as sess:\n  sess.run(init)\n  for i in range(100):\n    length = np.random.randint(0,10)\n    a = np.random.randint(0, 10, length)\n  print sess.run(sum,feed_dict={x:a})\n```\n\nIf you want to split/unpack on a tensor with dynamic shape and do some computation on each subtensor, the combination of TensorArray and While() is the way to go.\n", "Closing this, since it looks like `tf.foldl()` is the way to go here.\n"]}, {"number": 1481, "title": "3D convnets", "body": "Hi All, do you have any plans to release code for tensor flow implementing 3D convnets?\n", "comments": ["Please use https://github.com/tensorflow/tensorflow/issues/150\n"]}]