[{"number": 16111, "title": "Implement Scale Operate?? ", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code: No**:\r\n- **OS Platform and Distribution: openSUSE Leap 42.3**:\r\n- **TensorFlow installed from: binary**:\r\n- **TensorFlow version: tensorflow_gpu-1.4.0**:\r\n- **Python version: 2.7.13**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version: 8.0/6.0**:\r\n- **GPU model and memory: 11GB**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIn Caffe, Scale layer could do this:\r\ntop = alpha\u2217bottom+beta\r\nwhere bottom is the input, top is the output, alpha and beta are the learnable params.\r\nIn Tensorflow, what operate could implement above layer?\r\nThanks, Help a lot.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code"]}, {"number": 16110, "title": "Using tf1.4 to restore a model from tf0.8, a NotFoundError appeared", "body": "When using tf1.4 to restore a model from tf0.8, I met a NotFoundError, the related code as flow:\r\n\r\nema = tf.train.ExponentialMovingAverage(1.0)\r\nsaver = tf.train.Saver(ema.variables_to_restore())\r\nmodel_checkpoint_path='./model_check_point/model-20160506.ckpt-500000'\r\nsaver.restore(sess, model_checkpoint_path)\r\n\r\nThe error as flow:\r\nNotFoundError (see above for traceback): Tensor name \"incept3a/in3_conv5x5_8/batch_norm/moments/Squeeze/ExponentialMovingAverage\" not found in checkpoint files ./model_check_point/model-20160506.ckpt-500000\r\n\t [[Node: save/RestoreV2_46 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_46/tensor_names, save/RestoreV2_46/shape_and_slices)]]\r\n\t [[Node: save/RestoreV2_315/_35 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_702_save/RestoreV2_315\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nHow can I solve this problem?\r\n\r\nThis is a project I download from the github, the code is not writen by me. I run it on ubuntu14.04,\r\nwith TF1.4.1 installed from source code, bazel0.8 , CUDA8.0, cudnn6.0, GTX1060 6G memory.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 16109, "title": "R1.4 restore a model from r0.8 encounter NotFoundError (see above for traceback): Tensor name ", "body": "When using tf1.4 to restore a model from tf0.8, I met a NotFoundError, the related code as flow:\r\n\r\nema = tf.train.ExponentialMovingAverage(1.0)\r\nsaver = tf.train.Saver(ema.variables_to_restore())\r\nmodel_checkpoint_path='./model_check_point/model-20160506.ckpt-500000'\r\nsaver.restore(sess, model_checkpoint_path)\r\n\r\nThe error as flow:\r\nNotFoundError (see above for traceback): Tensor name \"incept3a/in3_conv5x5_8/batch_norm/moments/Squeeze/ExponentialMovingAverage\" not found in checkpoint files ./model_check_point/model-20160506.ckpt-500000\r\n\t [[Node: save/RestoreV2_46 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_46/tensor_names, save/RestoreV2_46/shape_and_slices)]]\r\n\t [[Node: save/RestoreV2_315/_35 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_702_save/RestoreV2_315\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nHow can I solve this problem?", "comments": []}, {"number": 16108, "title": "No tf.metrics.true_negatives", "body": "**TensorFlow version**: 1.4.1\r\n\r\nIs there any particular reason for why there is no `tf.metrics.true_negatives` method? I know it's simple to calculate from other confusion metrics that are available, but I was wondering why the developers chose to let this one method out.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Seems like a good addition. @ispirmustafa can decide if it's a welcome contribution.", "There is [tf.metrics.true_negatives](https://www.tensorflow.org/versions/master/api_docs/python/tf/metrics/true_negatives)\r\n"]}, {"number": 16107, "title": "R1.5", "body": "just want to download the code as samples of how to access tensorflow using C code.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16106, "title": "Eager: Invalid placement of vars/consts depending on their types and not the tf.device", "body": "Hi,\r\nI'm currently testing eager execution on TF 1.5.0-rc1 (built it with XLA and CUDA enabled) and observe strange behavior: variables and constants get created either on GPU or CPU depending on their types, and not `with tf.device(...):` block. Moreover, on creation of int32 variable it fails completely. For example, when I run the following code\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nprint('TensorFlow version:', tf.__version__)\r\n\r\nwith tf.device('/gpu:0'):\r\n    A = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)\r\n    print('Const A is placed on:', A.device)\r\n\r\n    B = tf.constant([1, 2, 3], dtype=tf.int32)\r\n    print('Const B is placed on:', B.device)\r\n\r\n    C = tfe.Variable([1.0, 2.0, 3.0], dtype=tf.float32)\r\n    print('Variable C is placed on:', C.device)\r\n\r\n    D = tfe.Variable([1, 2, 3], dtype=tf.int32)\r\n    print('Variable D is placed on:', D.device)\r\n```\r\n\r\nI get the following output:\r\n\r\n```\r\nTensorFlow version: 1.5.0-rc1\r\n2018-01-14 01:16:06.385929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-01-14 01:16:06.386198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.91GiB freeMemory: 363.06MiB\r\n2018-01-14 01:16:06.386223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nConst A is placed on: /job:localhost/replica:0/task:0/device:GPU:0\r\nConst B is placed on: CPU:0\r\nVariable C is placed on: /job:localhost/replica:0/task:0/device:GPU:0\r\nTraceback (most recent call last):\r\n  File \"tf_bug.py\", line 18, in <module>\r\n    D = tfe.Variable([1, 2, 3], dtype=tf.int32)\r\n  File \"/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 277, in __init__\r\n    constraint=constraint)\r\n  File \"/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 422, in _init_from_args\r\n    graph_mode=self._in_graph_mode)\r\n  File \"/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 53, in _eager_safe_variable_handle\r\n    container=container)\r\n  File \"/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 396, in var_handle_op\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node VarHandleOp = VarHandleOp[container=\"eager-execution-2/\", dtype=DT_INT32, shape=[3], shared_name=\"11\"]()\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_HALF]\r\n  device='CPU'\r\n  device='XLA_GPU'\r\n  device='XLA_CPU'\r\n [Op:VarHandleOp] name: Variable/\r\n```\r\n\r\nAs you can see, the constants and variables get placed either on GPU:0 or CPU:0 despite all of them gathered inside the same `tf.device('/gpu:0')` block.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: No\r\nOS Platform and Distribution: Ubuntu 16.04.1\r\nTensorFlow installed from: the source code, tag r1.5.0-r1 without custom modifications\r\nBazel version: 0.7.0\r\nCUDA/cuDNN version: CUDA 9.0/ cuDNN 7.0.5\r\nGPU model and memory:  GeForce GTX 1080 Ti, 11Gb RAM", "Just in case, I've rebuilt TF without enabling XLA (but still with CUDA). No effect. The bug still shows up identically with the same stack trace.\r\n\r\nIt seems that the bug also has some side effects: some of the functions like tf.reverse and others sometimes won't work with GPU-placed tensors, ignoring `tf.device('/gpu:0')` section and throwing exception like\r\n\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Tensors on conflicting devices: cannot compute StridedSlice as input #0 was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu(), or transparently copied by using tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:StridedSlice] name: strided_slice/`", "@kpot : Thanks for the report.\r\n\r\nThis is admittedly confusing, but is \"working as intended\".\r\nThis is because `int32` tensors are placed in host memory. This hack comes from [here](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/eager/pywrap_tensor.cc#L287). You should not observe this behavior with other types.\r\n\r\nThe general idea is that many (though not all) GPU kernels require the tensor to be in host memory when operating on int32 inputs  (you noticed this in the [`Reverse`](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/kernels/reverse_op.cc#L331) operation, and the same holds for many other operations). When working with TensorFlow graphs (as opposed to eager execution), this is still the case, but the runtime copies tensors between devices if needed. Setting `tfe.DEVICE_PLACEMENT_SILENT` will achieve the same effect with eager execution. We didn't make automatic copying between devices default for eager execution as that can often lead to hard to diagnose performance problems (and tracing the precise copies that become bottlenecks can be a little involved). \r\n\r\nThis is something we need to clean up (CC @alextp @josh11b @agarwal-ashish @akshayka @allenlavoie ). In the interim, the mental model to work with is that the int32 type is almost always in host memory.\r\n\r\nMind if I ask how you ran into this error? Did you have a need to operate on int32 tensors on GPUs?", "@asimshankar Thank you for the detailed explanation! \r\n\r\nI ran into this error while trying to use `tf.nn.top_k` together with `tf.gather` to collect prioritized slices from a large tensor. Hence the int32 (indices). This is just a thing that my model requires before each training step. And I hoped it would be faster to do everything on the GPU side instead of dragging the data back and forth from CPU.", "@kpot - might I suggest alternatives for now?\r\n\r\nTwo options:\r\n\r\n- Use `int64` indices (which will be placed on GPU)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\nwith tf.device('/gpu:0'):\r\n  large_tensor = tf.constant([1, 2, 3, 4], dtype=tf.float32)\r\n  i64 = tf.constant([1, 2], dtype=tf.int64)\r\n# All operations run on the GPU and out will be placed on the GPU\r\n# The next line could be placed inside the \"with device\" scope,\r\n# but it can be placed outside as well - since all inputs are on\r\n# GPU, the op will execute on GPU\r\nout = tf.gather(large_tensor, tf.nn.top_k(i64, k=2).values)\r\n```\r\n\r\n- Pay the cost of copying the (hopefully small) indices tensor to GPU\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\nwith tf.device('/gpu:0'):\r\n  large_tensor = tf.constant([1, 2, 3, 4], dtype=tf.float32)\r\n  i32 = tf.constant([1, 2], dtype=tf.int32)\r\n# top_k will run on the CPU, but gather on the GPU,\r\n# so large_tensor will not move from GPU\r\nout = tf.gather(large_tensor, tf.nn.top_k(i32, k=2).values.gpu())\r\n```\r\n\r\nHope that helps.", "@asimshankar Thanks, your example with int64 works great.  However, when I modified my earlier example, changing `int32` with `int64`, it failed again (on the same TF 1.5.0-rc1). My code exactly:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nprint('TensorFlow version:', tf.__version__)\r\n\r\nwith tf.device('/gpu:0'):\r\n    D = tfe.Variable([1, 2, 3], dtype=tf.int64)\r\n    print('Variable D is placed on:', D.device)\r\n```\r\n\r\nit fails with the message\r\n```\r\nTensorFlow version: 1.5.0-rc1\r\n2018-01-17 23:42:46.460016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-01-17 23:42:46.460292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.91GiB freeMemory: 363.06MiB\r\n2018-01-17 23:42:46.460316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"tf_bug.py\", line 9, in <module>\r\n    D = tfe.Variable([1, 2, 3], dtype=tf.int64)\r\n  File \"/home/kpot/pyves/ib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 277, in __init__\r\n    constraint=constraint)\r\n  File \"/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 422, in _init_from_args\r\n    graph_mode=self._in_graph_mode)\r\n  File \"/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 53, in _eager_safe_variable_handle\r\n    container=container)\r\n  File \"/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 396, in var_handle_op\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node VarHandleOp = VarHandleOp[container=\"eager-execution-0/\", dtype=DT_INT64, shape=[3], shared_name=\"2\"]()\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_HALF]\r\n  device='CPU'\r\n [Op:VarHandleOp] name: Variable/\r\n```\r\nout of curiosity I've tried various `[u]intN`, none of them work as a dtype for the variable. It's not like I currently need such variables, just wanted to be thorough :)", "Taking a step back: In general, for any given TensorFlow operation there are multiple implementations (kernels) based on the device and set of types that the kernel implements support for.\r\n\r\nFor some operations, like `Gather`, there are GPU kernels that support int64 types.\r\nFor other operations, like `MatMul`, the int64 GPU kernel is not implemented yet.\r\n\r\nVariables are created using the `VarHandleOp`, and as you noticed, it doesn't implement a GPU kernel for the `DT_INT64` type yet. However, for this particular op, it seems like registering the int64 type for the kernels [here](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/core/kernels/resource_variable_ops.cc#L132) will suffice.\r\n\r\nLong story short: If you need `int64` variables, it should be trivial to add support for them. However, be warned that in general, GPU operations are not integer friendly so you will run into coverage issues there.", "@asimshankar While working with reinforcement learning I discovered that for some problems running on the same GPU not only the neural network, but the whole environment simulation, sometimes gives a big boost in performance (about 10x in my case). So now I often use TF more like \"a general computational framework for GPUs\" of which deep learning is only a part. And it doesn't disappoint. Hence the need for integer variables and perhaps some other peculiarities. A lot of data to track - a lot of indices. Unfortunately, writing and debugging such simulations can be painful sometimes, and eager execution looks very promising in that respect.\r\n\r\nI understand that writing a specialized kernel is a much better way to do such things. But for a rapid prototyping tools like TF are priceless. It doesn't matter if the performance suffers a little here and there if I still can get a decent speedup. Most importantly the things should be doable.", "@kpot - there is no disagreement about TensorFlow being useful as a general purpose library, and we're happy to receive PRs and/or work on addressing any gaps. \r\n\r\nFor the variable in particular, turns out we don't need to write a new kernel - simply register the existing kernel for the `int64` type [here](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/core/kernels/resource_variable_ops.cc#L132). For other operations (like `MatMul`) things may be more complicated.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Unfortunately there is nothing I can do here myself. I'm not a CUDA/TF developer and don't have the expertise necessary for the task. So for now I simply returned back to classical use of TF via building and executing computation graphs.\r\n\r\nFeel free to close the issue if there is nothing that can be done about it at this stage.", "@kpot : As a workaround, can you keep your integer variables on CPU? And furthermore, you can enable silent copying of tensors between CPU and GPU in a way that may cost performance (when the copies happen), but will work. Something like this:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\r\n\r\nwith tf.device('/cpu:0'):\r\n  D = tfe.Variable([1, 2, 3], dtype=tf.int32)\r\n```\r\n\r\nThis will be more similar to graph where the variable is actually placed in CPU memory and the contents are transparently transferred to GPU when needed. (See https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/contrib/eager/enable_eager_execution)\r\n\r\nDoes that work?\r\n\r\n(We sincerely appreciate your feedback and your help in exposing these issues so we can iron them out)\r\n", "I came across the same problem.", "Silent copying does not work for me when using `tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)` for me. I'm running tensorflow 1.12.0 in Arch Linux.\r\n\r\nAlso, using `with tf.device('/gpu:0')` explicitly works for some examples, but not all. For eg. when using `tf.layers.conv2d` in eager with silent device placement and enforcing `with tf.device('/gpu:0')` fails with:\r\n```\r\nNotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node {{node VarHandleOp}} = VarHandleOp[container=\"\", dtype=DT_INT32, shape=[3,3,4,64], shared_name=\"b11/conv2d/kernel_42\"]()\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_HALF]\r\n  device='CPU'\r\n  device='XLA_CPU'\r\n  device='XLA_GPU'\r\n [Op:VarHandleOp] name: b11/conv2d/kernel/\r\n```", "@rharish101 can you open a new issue with instructions to reproduce?\r\n\r\nI don't know why your conv kernel is trying to create a variable with a variant dtype; that is not supported.", "@alextp I sort of found out the issue. It was occurring when I was passing an `int` to the conv2d layer. Looking at the docs, I should have been passing a `float`. When I explicitly converted the input using `tf.to_float` before passing to the conv2d layer, it worked.\r\n\r\nHowever, instead of warning me about `int` being an unsupported dtype, it gave me this exception. Is this a bug?"]}, {"number": 16105, "title": "Fix crash in `tf.contrib.ffmpeg.decode_video`", "body": "This fix fixes the crash in `tf.contrib.ffmpeg.decode_video`. The reason for the crash was that, `decode_video` dumps the information about streaming etc. (as opposed to dump\r\nto stderr) info a file and read from it. As the loglevel was `error` the file was empty.\r\n\r\nThis fix addresses the issue.\r\n\r\nThe fix could be verifed by manually running\r\n```\r\nbazel test -s --config=opt --cache_test_results=no \\\r\n        //tensorflow/contrib/ffmpeg:decode_video_op_test\r\n```\r\n\r\nWith this fix, the above test works.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang  have updata tf1.6,but there is still error log\r\n######################################################\r\n\r\ndevice:GPU:0 with 4333 MB memory) -> physical GPU (device: 0, name: Tesla K20c, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2018-03-08 09:42:15.159004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 4333 MB memory) -> physical GPU (device: 1, name: Tesla K20m, pci bus id: 0000:81:00.0, compute capability: 3.5)\r\n2018-03-08 09:42:31.078200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0, 1\r\n2018-03-08 09:42:31.078609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 74 MB memory) -> physical GPU (device: 0, name: Tesla K20c, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2018-03-08 09:42:31.079489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 74 MB memory) -> physical GPU (device: 1, name: Tesla K20m, pci bus id: 0000:81:00.0, compute capability: 3.5)\r\n2018-03-08 09:44:24.215896: F tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc:400] Non-OK-status: ReadInfoFile(stderr_filename, width, height, frames) status: Unknown: Not enough video info returned by FFmpeg [127, 0, 640, 3]Could not read FFmpeg stderr file: /tmp/tmp_file_tensorflow_3_ukCr1I.err\r\n\r\n\u5df2\u653e\u5f03 (\u6838\u5fc3\u5df2\u8f6c\u50a8)\r\n", "@XuChunling If possible, can you provide the reproducible script and the video sample? Also, instead of continue discussion in a merged PR, can you open a new issue? You can ping or assign me in the new issue."]}, {"number": 16103, "title": "No OpKernel was registered to support Op 'AssignVariableOp' with DT_BFLOAT16", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch linux\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.5.0-rc1\r\n- **Python version**: NA (go bindings)\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.1\r\n- **CUDA/cuDNN version**: na (CPU)\r\n- **GPU model and memory**: na\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\n`AssignVariableOp` does not appear to appear to have a kernel for `DT_BFLOAT16`.\r\n\r\n\r\n### Source code / logs\r\n```\r\npackage main\r\n\r\nimport (\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n)\r\n\r\nfunc main() {\r\n\ts := op.NewScope()\r\n\tbfloat := op.Cast(s, op.Const(s, float32(0.1234)), tf.Bfloat16)\r\n\tvariable := op.VarHandleOp(s, tf.Bfloat16, tf.ScalarShape())\r\n\tinit := op.AssignVariableOp(s, variable, bfloat)\r\n\r\n\tgraph, err := s.Finalize()\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tsess, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\t_, err = sess.Run(nil, nil, []*tf.Operation{init})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n}\r\n```\r\n```\r\ngo run bfloat_demo.go \r\npanic: No OpKernel was registered to support Op 'AssignVariableOp' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; dtype in [DT_VARIANT]\r\n  device='CPU'; dtype in [DT_QINT32]\r\n  device='CPU'; dtype in [DT_QUINT8]\r\n  device='CPU'; dtype in [DT_QINT8]\r\n  device='CPU'; dtype in [DT_RESOURCE]\r\n  device='CPU'; dtype in [DT_STRING]\r\n  device='CPU'; dtype in [DT_BOOL]\r\n  device='CPU'; dtype in [DT_COMPLEX128]\r\n  device='CPU'; dtype in [DT_COMPLEX64]\r\n  device='CPU'; dtype in [DT_DOUBLE]\r\n  device='CPU'; dtype in [DT_FLOAT]\r\n  device='CPU'; dtype in [DT_HALF]\r\n  device='CPU'; dtype in [DT_INT8]\r\n  device='CPU'; dtype in [DT_UINT8]\r\n  device='CPU'; dtype in [DT_INT16]\r\n  device='CPU'; dtype in [DT_UINT16]\r\n  device='CPU'; dtype in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT64]\r\n\r\n\t [[Node: AssignVariableOp = AssignVariableOp[dtype=DT_BFLOAT16](VarHandleOp, Cast)]]\r\n\r\ngoroutine 1 [running]:\r\nmain.main()\r\n\t/home/isaac/go/src/github.com/is8ac/gotf/bfloat_demo.go:24 +0x250\r\nexit status 2\r\n```", "comments": ["I believe this is fixed from master. See this commit. \r\nhttps://github.com/tensorflow/tensorflow/commit/ccbd14b741e6efbe51769f0f1b9cb3719c42c23b\r\nClose this if building from master solves this problem for you.", "This solves the problem. Thank you."]}, {"number": 16102, "title": "[Bug]: Unable to update the batch_normalization layer moving_mean/moving_variance of keras ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: Tensorflow 1.4.1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **Python version**: Python 3.5\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: 1080 Ti\r\n- **Exact command to reproduce**: see below\r\n\r\nIt is following up this [issue](https://github.com/tensorflow/tensorflow/issues/15367#issuecomment-357088739) about training models by mixing Tensorflow with Keras using this style, exampled [here](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html). Basically, using Inception architectures in keras.applications.InceptionV3 and training using sess.run() with a placeholder K.learning_phase() to indicate the training/testing mode.\r\n\r\n_At beginning, I found the evaluation accuracy in testing mode (loaded back from a checkpoint) is different from the results when evaluation on the fly during training. But when I evaluated using training mode, it can give me reasonable but not good results. So I knew the problem happened in batch normalization._\r\n\r\n**I tried either using tf.keras and keras independent version (2.1.2). But both not work.**\r\n\r\nAfter some investigations, I found there should an issue of Keras of using batch normalization. I track the moving_mean and moving_variance and found they never update (maintaining the initial values, zero and one). In tf. keras, I notice its batchnorm inherits tf.layers.BatchNormalization (see [here](https://github.com/tensorflow/tensorflow/blob/ac8e67399d75edce6a9f94afaa2adb577035966e/tensorflow/python/keras/_impl/keras/layers/normalization.py#L26)). However, by checking the tutorial of this tf.layers.BatchNormalization,\r\n  _It required to add a **update_ops** in optimizer. But it never mentioned in Keras (which should be \r\n  clarified)._ \r\nI did not see Keras using update_ops anywhere. **So i believe if you want to fine-tune a keras predefined model in applications, you never be able to update your moving_mean and moving_variance for your new data**.\r\n\r\n**Note that I also tried keras independent version. It never worked. moving_mean and moving_variance are always not changed. I  tracked the value of batch_mean which is used to update moving_mean. batch_mean has values but not on moving_mean.**\r\n\r\nIn addition, I found moving_mean and moving_variance are in tf.trainable_variables() when using keras but not in tf.keras. I am not sure if this matters.\r\n\r\nHere is an example code\r\n\r\n```\r\nimage = keras.preprocessing.image\r\ndef preprocess_input(x):\r\n    # the same as keras.applications.inception_v3\r\n    with tf.name_scope('preprocess_input'):\r\n        x /= 255.\r\n        x -= 0.5\r\n        x *= 2.\r\n        return x\r\n\r\ndef get_main_network(name, input_tensor, use_weights=False):\r\n\r\n    processed_tensor = preprocess_input(input_tensor)\r\n\r\n    if name == 'inception':\r\n        base_model = keras.applications.InceptionV3(include_top=True,\r\n                                                    weights='imagenet' if use_weights else None,\r\n                                                    pooling='avg',\r\n                                                    input_tensor=processed_tensor)\r\n    \r\n    model = keras.models.Model(inputs=base_model.input, outputs=base_model.output)\r\n \r\n    return model\r\n\r\nimg_shape=[299,299]\r\nimg = tf.placeholder(tf.float32, shape=[None]+img_shape+[3])\r\n\r\nwith tf.name_scope('model'):\r\n    model = get_main_network('inception', input_tensor=img, use_weights=False)\r\n    output = model.output\r\n    logit = tf.cast(tf.argmax(output, axis=1), np.float32)\r\n\r\n# define loss\r\nwith tf.name_scope('cross_entropy'):\r\n    cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=output)  \r\n\r\n# define optimizer\r\nwith tf.name_scope('learning_rate'):\r\n    global_step = tf.Variable(0, name='global_step', trainable=False)\r\n    learning_rate = tf.train.exponential_decay(opt.learning_rate, global_step,\r\n                                        iter_epoch*opt.lr_decay_epoch, opt.lr_decay, staircase=True)\r\n    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) #.minimize(cross_entropy_loss, global_step=global_step)\r\n\r\n''' **This matters a lot but not working for independent keras version'**''\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    train_step = optimizer.minimize(cross_entropy_loss, global_step=global_step)\r\n\r\n''' Initialization '''\r\ninit_op = tf.variables_initializer([]) # a fake one, variables already initialized in keras\r\nsess.run(init_op)\r\n\r\nimg_path = 'elephant.jpg'\r\nimgs= image.load_img(img_path, target_size=img_shape)\r\nx = image.img_to_array(imgs)\r\nx = np.expand_dims(x, axis=0)\r\nsaver = tf.train.Saver(max_to_keep=20) # must be added in the end\r\nwith sess.as_default():\r\n    \r\n    feed_dict = {   \r\n                        img: x_batch,\r\n                        label: y_batch,\r\n                        K.learning_phase(): True\r\n                    }\r\n        _, loss = sess.run([train_step, \r\n                                    cross_entropy_loss, \r\n                                    ], feed_dict=feed_dict)\r\n\r\n```\r\n@tensorflowbutler responsed", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "Dear all, \r\nI have other example code shows keras's BatchNormalization doesn't work as following.\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\n#from tensorflow.python.keras import  backend\r\nfrom tensorflow import keras\r\nK = keras.backend\r\n\r\nfrom tensorflow.python.keras.layers import  Input\r\n\r\nfrom tensorflow.python.keras.layers import MaxPooling2D\r\nfrom tensorflow.python.keras.layers import BatchNormalization\r\nfrom tensorflow.python.keras.layers import Conv2D, Dense, Flatten\r\nfrom tensorflow.python.keras.layers import Activation\r\nfrom tensorflow.python.keras.models import Model\r\n\r\nprint(tf.__version__)\r\n# Load Data\r\nimport cifar10\r\n\r\nimg_size = 32\r\nnum_channels = 3\r\nnum_classes = 10\r\ncifar10.maybe_download_and_extract()\r\n\r\n\r\nclass_names = cifar10.load_class_names()\r\n\r\n\r\nimages_train, cls_train, labels_train = cifar10.load_training_data()\r\n\r\nimages_test, cls_test, labels_test = cifar10.load_test_data()\r\n\r\nprint(\"Size of:\")\r\nprint(\"- Training-set:\\t\\t{}\".format(len(images_train)))\r\nprint(\"- Test-set:\\t\\t{}\".format(len(images_test)))\r\n\r\n\r\n\r\nimg_size_cropped = 24\r\n#%%\r\ndef pre_process_image(image, training):\r\n    # This function takes a single image as input,\r\n    # and a boolean whether to build the training or testing graph.\r\n    \r\n    if training:\r\n        # For training, add the following to the TensorFlow graph.\r\n\r\n        # Randomly crop the input image.\r\n        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\r\n\r\n        # Randomly flip the image horizontally.\r\n        image = tf.image.random_flip_left_right(image)\r\n        \r\n        # Randomly adjust hue, contrast and saturation.\r\n        image = tf.image.random_hue(image, max_delta=0.05)\r\n        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\r\n        image = tf.image.random_brightness(image, max_delta=0.2)\r\n        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\r\n\r\n        # Some of these functions may overflow and result in pixel\r\n        # values beyond the [0, 1] range. It is unclear from the\r\n        # documentation of TensorFlow 0.10.0rc0 whether this is\r\n        # intended. A simple solution is to limit the range.\r\n\r\n        # Limit the image pixels between [0, 1] in case of overflow.\r\n        image = tf.minimum(image, 1.0)\r\n        image = tf.maximum(image, 0.0)\r\n    else:\r\n        # For training, add the following to the TensorFlow graph.\r\n\r\n        # Crop the input image around the centre so it is the same\r\n        # size as images that are randomly cropped during training.\r\n        image = tf.image.resize_image_with_crop_or_pad(image,\r\n                                                       target_height=img_size_cropped,\r\n                                                       target_width=img_size_cropped)\r\n\r\n    return image\r\n\r\ndef pre_process(images, training):\r\n    # Use TensorFlow to loop over all the input images and call\r\n    # the function above which takes a single image as input.\r\n    images = tf.map_fn(lambda image: pre_process_image(image, training), images)\r\n\r\n    return images\r\n\r\n#%%\r\ndef _bn_relu(input_layer):\r\n    \"\"\"Helper to build a BN -> relu block\r\n    \"\"\"\r\n    norm = BatchNormalization(axis=3)(input_layer)\r\n    return Activation(\"relu\")(norm)\r\n#%%\r\n\r\nx = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\r\n\r\ndistorted_images = pre_process(images=x, training=True)\r\n\r\ny_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\r\n\r\ny_true_cls = tf.argmax(y_true, axis=1)\r\n\r\ndef create_network(training):\r\n    # Wrap the neural network in the scope named 'network'.\r\n    # Create new variables during training, and re-use during testing.\r\n    with tf.variable_scope('network', reuse=not training):\r\n\r\n        images = x        \r\n        images = pre_process(images=images, training=training)\r\n        \r\n        inputs = Input(tensor=images)\r\n\r\n        net_ks = Conv2D(kernel_size=5, strides=1, filters=64, padding='same',\r\n             activation='linear', name='layer_conv1')(inputs)\r\n\r\n        net_ks = _bn_relu(net_ks)\r\n        net_ks = MaxPooling2D(pool_size=2, strides=2)(net_ks)\r\n\r\n\r\n        net_ks = Conv2D(kernel_size=5, strides=1, filters=64, padding='same',\r\n        activation='relu', name='layer_conv2')(net_ks) \r\n        net_ks = MaxPooling2D(pool_size=2, strides=2)(net_ks)\r\n\r\n        \r\n        net_ks = Flatten()(net_ks)\r\n        \r\n        net_ks = Dense(256, activation='relu', name='layer_fc1')(net_ks)\r\n        net_ks = Dense(128, activation='relu', name='layer_fc2')(net_ks)\r\n        preds_ks = Dense(num_classes, activation='linear')(net_ks)\r\n        \r\n        preds_softmax = tf.nn.softmax(preds_ks)\r\n        step1 = tf.cast(y_true, tf.float32) * tf.log(preds_softmax)\r\n        step2 = -tf.reduce_sum(step1, reduction_indices=[1])\r\n        loss = tf.reduce_mean(step2)       # loss\r\n        \r\n        model = Model(inputs=inputs, outputs=preds_ks)\r\n\r\n    return preds_softmax, loss, model\r\n\r\n#%% Create Neural Network for Training Phase\r\nglobal_step = tf.Variable(initial_value=0,\r\n                          name='global_step', trainable=False)\r\n\r\ny_pred, loss, model = create_network(training=True)\r\n\r\nmodel.summary()\r\n\r\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)\r\n#%% Create Neural Network for Test Phase / Inference\r\n#y_pred, _ , _= create_network(training=False)\r\ny_pred_cls = tf.argmax(y_pred, axis=1)\r\ncorrect_prediction = tf.equal(y_pred_cls, y_true_cls)\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\nsession = tf.Session()\r\nsession.run(tf.global_variables_initializer())\r\ntrain_batch_size = 64\r\n\r\ndef random_batch():\r\n    # Number of images in the training-set.\r\n    num_images = len(images_train)\r\n\r\n    # Create a random index.\r\n    idx = np.random.choice(num_images,\r\n                           size=train_batch_size,\r\n                           replace=False)\r\n\r\n    # Use the random index to select random images and labels.\r\n    x_batch = images_train[idx, :, :, :]\r\n    y_batch = labels_train[idx, :]\r\n\r\n    return x_batch, y_batch\r\n\r\ndef optimize(num_iterations):\r\n\r\n    for i in range(num_iterations):\r\n\r\n        x_batch, y_true_batch = random_batch()\r\n\r\n\r\n        feed_dict_train = {x: x_batch,\r\n                           y_true: y_true_batch, K.learning_phase(): 1}\r\n\r\n\r\n        i_global, _ = session.run([global_step, optimizer],\r\n                                  feed_dict=feed_dict_train)\r\n\r\n        # Print status to screen every 100 iterations (and last).\r\n        if (i_global % 100 == 0) or (i == num_iterations - 1):\r\n            # Calculate the accuracy on the training-batch.\r\n            feed_dict_train = {x: x_batch,\r\n                           y_true: y_true_batch, K.learning_phase(): 0}\r\n            batch_acc = session.run(accuracy,\r\n                                    feed_dict=feed_dict_train)\r\n\r\n            # Print status.\r\n            msg = \"Global Step: {0:>6}, Training Batch Accuracy(phase 0): {1:>6.1%}\"\r\n            print(msg.format(i_global, batch_acc))\r\n            \r\n            feed_dict_train = {x: x_batch,\r\n                           y_true: y_true_batch, K.learning_phase(): 1}\r\n            batch_acc = session.run(accuracy,\r\n                                    feed_dict=feed_dict_train)\r\n\r\n            # Print status.\r\n            msg = \"Global Step: {0:>6}, Training Batch Accuracy(phase 1): {1:>6.1%}\"\r\n            print(msg.format(i_global, batch_acc))  \r\n\r\n\r\n#%%\r\noptimize(num_iterations=10000)\r\n```\r\nPlease check this bug. Thanks a lot!!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Is it related to https://github.com/tensorflow/tensorflow/issues/17950?", "Check also https://github.com/tensorflow/tensorflow/issues/16455", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "> I did not see Keras using update_ops anywhere\r\n\r\nThat is by design. Global collections (and global states in general) are prone to various issues (like most global variables in software engineering) and should be avoided.\r\n\r\nYou can retrieve the updates created by your BatchNormalization layer via:\r\n\r\n```python\r\nupdates = layer.updates\r\n```\r\n\r\nAdditionally you can retrieve the aggregated updates of all stateful layers in a Keras model via:\r\n\r\n```python\r\nupdates = model.updates\r\n```\r\n\r\nIf you are writing your own custom training loops, you will have to run these updates as part of the call to `sess.run()`. But do note that tf.keras provides a built-in training loop in the form of `model.fit()`, as well as primitive methods for building your own custom training loops (in particular `model.train_on_batch()`  and `model.predict_on_batch()`). If you are using Keras you should generally never have to manually manage your model's updates.", "Hi @zizhaozhang and @ymcasky . In both your examples, can you try adding K.set_learning_phase(1) at the entry point of create_main_network() and see if it works?"]}, {"number": 16101, "title": "Add stream selection support for `tf.contrib.ffmpeg.decode_audio`", "body": "This fix tries to address the issue raised in #16073 where it was not possible to selectively decode a perticular stream with `tf.contrib.ffmpeg.decode_audio`.\r\nThis fix adds an additional attribute `stream` which could be used to specify the stream to decode (e.g., `stream=\"1\"`). By default `stream=\"\"` which leaves the decision to ffmpeg.\r\n\r\nThis fix fixes #16073.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["LGTM!", "Do you think it would be good for API symmetry to have the same extension for `tf.contrib.ffmpeg.decode_video` or is that excessive? I can't think of many video files with multiple streams but what do I know?", "@carlthome I think it makes sense to have stream selection support for `decode_video` as well. Let me  take a look.", "Actually there is a bug in `decode_video` that causes it to not run correctly. I have created a separate PR #16105 to fix it, as it is a bug (not a feature request).\r\n\r\nI will work on follow up PR for stream selection support for `decode_video` once #16105 is merged.", "@yongtang maybe I am missing this, but is there a way to get the number of available substreams so that users can iterate over all substreams?", "Similarly, being able to set a time offset and duration would be nice (i.e. ffmpeg `-ss 00:03:00` and `-t 60` for being able to only decode parts of longer movies and audio files, so you can work through a folder of files in reasonable segments, but maybe that is best to think about in a separate issue.", "@carlthome @faroit I think it might be possible to have another op (e.g., `video_info`) so that any video related information (e.g., number of streams, length, etc.) is accessible. I can work on creating another PR if this PR is merged.", "This looks great! Thanks for doing this. Unfortunately, merging this will break our internal build. I'm going to create an internal change that we can synchronize with this one. ", "@fredbertsch Thanks for the help! \ud83d\udc4d \ud83c\udf89 ", "Please have a look at the test failures, e.g.\r\n\r\nhttps://source.cloud.google.com/results/invocations/87ffa93c-e04d-463d-8b1d-5bd68486b076/targets/%2F%2Ftensorflow%2Fcontrib%2Flayers:layers_test/log\r\n\r\n```\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 1617, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 23 should be: 3\r\n\t [[Node: SparseToDense = SparseToDense[T=DT_INT64, Tindices=DT_INT64, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_to_sparse/Where, dense_to_sparse/GatherNd, dense_to_sparse/Shape, SparseToDense/default_value)]]\r\n```", "@rmlarsen I run though the test of `//tensorflow/contrib/layers:layers_test` but it passed. Is the failure related to another PR?", "The PR #16696 adds the stream selection support for `tf.contrib.ffmpeg.decode_video`."]}, {"number": 16100, "title": "Exception when not providing optional parameter frequency_skip in TimeFreqLSTMCell", "body": "### System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below\r\n- OS Platform and Distribution: \r\n- TensorFlow installed from: `pip3 install --user tensorflow-gpu`\r\n- TensorFlow version: 1.4.1\r\n- Python version: 3.5.2\r\n- CUDA: 8.0\r\n- GPU: NVidia Titan X\r\n\r\n### Describe the problem\r\n\r\nUsing a `TimeFreqLSTMCell` in a `dynamic_rnn` or `static_rcnn` without providing the optional parameter `frequency_skip` results in an exception:\r\n\r\n```\r\nTypeError: unsupported operand type(s) for /: 'int' and 'NoneType'\r\n```\r\n\r\nThe line which throws this exception is https://github.com/tensorflow/tensorflow/blob/8b78c23c161c9d0bec462d5f4c73f0fca413bc8b/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L474-L475\r\n`frequency_skip` has it's default value `None` here.\r\n\r\nMaybe the default should be changed to `1`?\r\n\r\n### Source code / logs\r\n\r\nSadly I am not allowed to share my full source code. However, this is how I create the RNN layers:\r\n\r\n```\r\nlstmcell = tf.contrib.rnn.TimeFreqLSTMCell(lstm_input.shape.as_list()[2], forget_bias = self.lstm_forget_bias, feature_size = lstm_input_rev.shape.as_list()[2])\r\n                \r\nstacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstmcell] * self.layers_lstm)\r\n                \r\nlstm_output, lstm_state = tf.nn.dynamic_rnn(stacked_lstm, lstm_input_rev, dtype=\"float32\", time_major=True)\r\n```", "comments": ["After reading the code, I agree that this is a bug. Don't know who knows this most, friendly ping @cy89 "]}, {"number": 16099, "title": "Make srcd in variable", "body": "", "comments": ["Can one of the admins verify this patch?", "Can you rebase? There are a lot of commits there for a nice 2-line cleanup.", "Jenkins, test this please."]}, {"number": 16098, "title": "Question about r1.5's default ptxas GPU back-end", "body": "From TF r1.5's release notes, it is said that \"GPU back-end now uses ptxas to compile generated PTX.\"  I have searched through the entire git repo of r1.5 code base with only finding that ptxas will be invoked in XLA flow.\r\nSince we are working on some XLA and GPU optimization stuffs, I just want to make sure the meaning of this sentence since I am a little bit puzzle about it and want to ensure our development flow will align with the community major direction.\r\n\r\nThanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 16097, "title": "terminate called after throwing an instance of 'std::bad_alloc'", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: 1.3\r\n- **Python version**: 2.7.13\r\n- **Bazel version**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0.61 / 6.0.21\r\n- **GPU model and memory**: Quadro P5000 16GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen I try to alloc a tf.constant variable as a [90000, 4096] matrix with float32 type. It seems that the graphics have enough memory. But I still got an memory error.\r\n\r\n### Source code / logs\r\nThe error information is as follows:\r\n\r\n```\r\nname: Quadro P5000\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:03:00.0\r\nTotal memory: 15.89GiB\r\nFree memory: 15.66GiB\r\n2018-01-13 19:18:10.558228: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x39a1500 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2018-01-13 19:18:10.559218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties:\r\nname: Quadro P5000\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:82:00.0\r\nTotal memory: 15.89GiB\r\nFree memory: 15.43GiB\r\n2018-01-13 19:18:10.559297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 0 and 1\r\n2018-01-13 19:18:10.559306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 1 and 0\r\n2018-01-13 19:18:10.559317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1\r\n2018-01-13 19:18:10.559321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y N\r\n2018-01-13 19:18:10.559324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   N Y\r\n2018-01-13 19:18:10.559333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P5000, pci bus id: 0000:03:00.0)\r\n2018-01-13 19:18:10.559338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Quadro P5000, pci bus id: 0000:82:00.0)\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\nAborted\r\n```\r\n\r\nThe test code is as follows:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\na = tf.constant(np.ones([90000, 4096], dtype = np.float32), dtype = tf.float32)\r\n\r\nsess = tf.Session()\r\nsess.run(a)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "`std::bad_alloc` indicates an error of out of *main* memory, i.e., where your CPU usually accesses. My guess is that the graph you created is too large because `a` is a large constant (1.4GiB).", "@netheril96 \r\n\r\nI'm sure that I have enough free main memory.\r\n\r\n```\r\n              total        used        free      shared  buff/cache   available\r\nMem:      263858420    25728860    19070132     2415448   219059428   234420312\r\nSwap:       4194300     1828184     2366116\r\n```", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Running that example uses a maximum 6.8 GB of CPU RAM for me using TensorFlow 1.5. Does the issue still occur on 1.5?\r\n\r\nAlso, try using binary search to find the maximum size tensor you can create, and see how much memory the process takes", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to a lack of response."]}, {"number": 16096, "title": "Address bad merge in Java install instructions", "body": "", "comments": ["I'll request a doc update asap. Not sure how this happened, I'll take a look.", "We cherrypicked https://github.com/tensorflow/tensorflow/pull/16056/commits/eece1521012959c10dfd928c56370a09fcad4777 which seems to have done this. @angersson any insight?"]}, {"number": 16095, "title": "Implement Scale Operate??", "body": "In Caffe, Scale layer could do this:\r\ntop=alpha\u2217bottom+beta\r\nwhere bottom is the input, top is the output, alpha and beta are the learnable params.\r\nIn Tensorflow, what operate could implement above layer?\r\nThanks, Help a lot", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", " This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 16094, "title": "Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss')", "body": "Have I written custom code: yes\r\nOS: Windows 8.1\r\nTensorflow installed from: conda\r\nTensorflow version: 1.4\r\n\r\n\r\nI've successfully converted a Tensor into a SparseTensor with this code:\r\n\r\n```\r\ndef dense_to_sparse(dense_tensor, out_type):\r\n    indices = tf.where(tf.not_equal(dense_tensor, tf.constant(0, dense_tensor.dtype)\r\n    values = tf.gather_nd(dense_tensor, indices)\r\n    shape = tf.shape(dense_tensor, out_type=out_type)\r\n    return tf.SparseTensor(indices, values, shape)\r\n```\r\n\r\nI want to try out using a `SparseTensor` converted from a dense one: \r\n\r\n```\r\ninput_layer = tf.placeholder(tf.float32, [None, 1596, 48])\r\ndense_labels = tf.placeholder(tf.int32)\r\nsparse_from_dense = dense_to_sparse(dense_lables, out_type=tf.int64)\r\ncell_fw = grid_rnn.Grid2LSTMCell(num_units=128)\r\ncell_bw = grid_rnn.Grid2LSTMCell(num_units=128)\r\nbidirectional_grid_rnn = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, input_layer, dtype=tf.float32)\r\noutputs = tf.reshape(bidirectional_grid_rnn[0], [-1, 256])\r\n\r\nW = tf.Variable(tf.truncated_normal([256, 80], stddev=0.1, dtype=tf.float32), name='W')\r\nb = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[80], name='b'))\r\n\r\nlogits = tf.matmul(outputs, W) + b\r\nlogits = tf.reshape(logits, [tf.shape(input_layer)[0], -1, 80])\r\nlogits = tf.transpose(logits, (1, 0, 2))\r\n\r\nloss = tf.nn.ctc_loss(inputs=logits, labels=sparse, sequence_length=320)\r\n```\r\n\r\nUnfortunately, when I do this, I encounter this error:\r\n\r\n`Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss') with input shapes: [?,?,80], [?,1], [?], [].`\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Updated the issue", "I already got it working thanks to the response in this SO [post](https://stackoverflow.com/questions/48239019/shape-must-be-rank-1-but-is-rank-0-for-ctcloss-op-ctcloss/48240880#48240880).", "hello, i meet the same problem where did you change?"]}, {"number": 16093, "title": "when will we have multi gpu support under eager mode? Pytorch has it.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Multiple GPUs are supported with eager execution in that you can use `with tf.device('/gpu:<N>')` blocks to place operations and tensors on different GPUs. For example, the following will execute one matmul on GPU0 and one on GPU1:\r\n\r\n```python\r\nwith tf.device('/gpu:0'):\r\n  tf.matmul([[1.]], [[2.]])\r\nwith tf.device('/gpu:1'):\r\n  tf.matmul([[1.]], [[2.]])\r\n```\r\nHowever, it will schedule them serially - so your bottleneck will likely be the Python interpreter's single thread feeding the GPUs.\r\n\r\nWe're working on APIs for more efficient use of multiple GPUs and overall cutting down the cost of initiating operations from the Python interpreter.\r\n\r\nI'm tempted to close out this issue since it is a bit open ended. Any significant APIs addressing this will be part of the release notes over the next few releases. Please let me know if you disagree.\r\n\r\nThanks!\r\n\r\n(CC @josh11b )", "@asimshankar When will we have a better version of multi-gpu support like pytorch? Where we can do pytorch nn.DataParallel kind of operation?", "@xiaoyunwu : Smoother multi-GPU support for eager is actively being worked on. However, I don't have a timeline to commit to at this point. We will surely make this clear in the release notes of the release where we feel it's in a reasonable state.", "@asimshankar any update? I am happy to test it out?\r\n", "@asimshankar any update? It has been a long while now. \r\n", "@xiaoyunwu - Unfortunately, no concrete update yet. However, the intent is to keep this in line with graph execution via the [distribution strategy APIs](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute) and integrate that with the `tf.keras` classes so that the same code works for both graph and eager execution.", "but pytorch is ahead of us for so long, we need to have some solution.\n\nXiaoyun\n\nOn Thu, Jul 12, 2018 at 7:26 AM, Asim Shankar <notifications@github.com>\nwrote:\n\n> @xiaoyunwu <https://github.com/xiaoyunwu> - Unfortunately, no concrete\n> update yet. However, the intent is to keep this in line with graph\n> execution via the distribution strategy APIs\n> <https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute>\n> and integrate that with the tf.keras classes so that the same code works\n> for both graph and eager execution.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16093#issuecomment-404340659>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFOFUa2AthRF1p2rhHGJUQhUK6kKGKE3ks5uFomlgaJpZM4RdFUg>\n> .\n>\n", "https://github.com/kashif/tf-keras-tutorial/blob/master/7-estimators-multi-gpus.ipynb\n\nIs this the sign of what we are heading?\n\nXiaoyun\n\nOn Thu, Jul 12, 2018 at 4:19 PM, Xiaoyun Wu <xiaoyun.wu@gmail.com> wrote:\n\n> but pytorch is ahead of us for so long, we need to have some solution.\n>\n> Xiaoyun\n>\n> On Thu, Jul 12, 2018 at 7:26 AM, Asim Shankar <notifications@github.com>\n> wrote:\n>\n>> @xiaoyunwu <https://github.com/xiaoyunwu> - Unfortunately, no concrete\n>> update yet. However, the intent is to keep this in line with graph\n>> execution via the distribution strategy APIs\n>> <https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute>\n>> and integrate that with the tf.keras classes so that the same code works\n>> for both graph and eager execution.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/16093#issuecomment-404340659>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AFOFUa2AthRF1p2rhHGJUQhUK6kKGKE3ks5uFomlgaJpZM4RdFUg>\n>> .\n>>\n>\n>\n", "That tutorial you referred to is nice - it does demonstrate the general idea that the exact same code (`tf.keras.Model` object) used with eager execution can be used to create an `Estimator` for distributed (multi-GPU or multi-node) training.\r\n\r\nWe are working on making that process much smoother, where say the `DistributionStrategy` to use can be provided directly to the `tf.keras.Model` object. Till that happens, the notebook you referred to is one possibility. Sound reasonable?", "Asim,\nany update on using multi gpu with eager mode?\nThanks.\n\nXiaoyun\n\nOn Sat, Jul 14, 2018 at 12:39 AM Asim Shankar <notifications@github.com>\nwrote:\n\n> That tutorial you referred to is nice - it does demonstrate the general\n> idea that the exact same code (tf.keras.Model object) used with eager\n> execution can be used to create an Estimator for distributed (multi-GPU\n> or multi-node) training.\n>\n> We are working on making that process much smoother, where say the\n> DistributionStrategy to use can be provided directly to the tf.keras.Model\n> object. Till that happens, the notebook you referred to is one possibility.\n> Sound reasonable?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16093#issuecomment-404886240>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFOFURD9fHXJiA5f5Ijcbvjgz2vijUgvks5uGM1PgaJpZM4RdFUg>\n> .\n>\n"]}, {"number": 16092, "title": "Branch 181814918", "body": "", "comments": []}, {"number": 16091, "title": "Failed to convert tf pb file (MTCNN model) to tflite format", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**:n/a\r\n- **Exact command to reproduce**: bazel run --config=opt --copt=-msse4.1 --copt=-msse4.2  //tensorflow/contrib/lite/toco:toco --  --input_file=/home/xxx/facenet/ks/onet.pb   --output_file=/home/xxx/facenet/ks/onet.tflite  --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=FLOAT   --input_shape=1,48,48,3   --input_array=onet/input --output_array=onet/prob1\r\n\r\n### Describe the problem\r\nI failed to covert my pb file to tflite format because some operations were not supported. Is there any workaround for the issue? \r\n\r\n### Source code / logs\r\n2018-01-13 09:47:41.439557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.439661: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.439687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.439736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.439767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.439786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.439824: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.439864: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.439884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.439940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.439970: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.439987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440027: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.440055: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.440073: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440107: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.440135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.440152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440208: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.440240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.440259: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.440333: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.440352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440392: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.440423: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.440442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440500: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.440533: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.440552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440596: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.440630: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.440650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440689: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.440720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.440739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440798: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.440828: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.440851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440886: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.440918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.440937: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.440987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.441018: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.441037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441090: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.441118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.441135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.441196: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.441213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441254: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Exp\r\n2018-01-13 09:47:41.441295: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.441324: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.441340: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441372: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.441400: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.441419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.441500: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.441518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441554: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform\r\n2018-01-13 09:47:41.441586: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2\r\n2018-01-13 09:47:41.441604: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441701: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441961: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.441990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.442071: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.442095: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.442120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.442142: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.443280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.443319: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.443386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.443411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.443433: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.443454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.443475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.443495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.443517: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign\r\n2018-01-13 09:47:41.446177: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 231 operators, 319 arrays (0 quantized)\r\n2018-01-13 09:47:41.449197: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 68 operators, 76 arrays (0 quantized)\r\n2018-01-13 09:47:41.449990: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 68 operators, 77 arrays (0 quantized)\r\n2018-01-13 09:47:41.450763: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 68 operators, 77 arrays (0 quantized)\r\n2018-01-13 09:47:41.451432: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n\r\n", "comments": ["Is this a training graph? If so, you'll need to freeze it before proceeding.", "Thanks for your reply. After I frozen it, it still failed with the following debug information.\r\n\r\nINFO: Running command line: bazel-bin/tensorfow/contrib/lite/toco/toco '--input_file=/work/tensorflow/tmp/onet_frozen_graph.pb' '--output_file=/work/tensorflow/tmp/onet_48.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=FLOAT' '--input_shape=1,48,48,3' '--input_array=input' '--output_arrays=prob1,conv6-2/conv6-2,conv6-3/conv6-3'\r\n2018-01-17 09:28:02.503519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Exp\r\n2018-01-17 09:28:02.504112: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 84 operators, 109 arrays (0 quantized)\r\n2018-01-17 09:28:02.505725: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 42 operators, 66 arrays (0 quantized)\r\n2018-01-17 09:28:02.506083: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 42 operators, 67 arrays (0 quantized)\r\n2018-01-17 09:28:02.506469: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 41 operators, 65 arrays (0 quantized)\r\n2018-01-17 09:28:02.506780: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 41 operators, 65 arrays (0 quantized)\r\n2018-01-17 09:28:02.507037: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 812544 bytes, theoretical optimal value: 812544 bytes.\r\n2018-01-17 09:28:02.507102: I tensorflow/contrib/lite/toco/toco_tooling.cc:273] Estimated count of arithmetic ops: 0.0263383 billion (note that a multiply-add is counted as 2 ops).\r\n2018-01-17 09:28:02.507214: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: DIV, Exp, NEG, Sub, TensorFlowMax, TensorFlowSum.\r\n\r\n", "Hi @kangshan0910 We are working on adding support for some of those ops. This is facenet, right?", "Thanks @andrehentz. This is one of the MTCNN models which include onet, pnet and rnet. ", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Quick update: Div, Exp and Sub are supported now. Neg, TensorflowMax and TensorflowSum will follow soon.\r\n", "@kangshan0910 Hi, I am trying to convert mtcnn to tflite as well but I got this error: https://github.com/tensorflow/tensorflow/issues/18195 \r\nwhat's your progress in this issue?", "Hi, \r\nI did almost the exact same thing to convert to a tflite file (speech model) but the output tflite file is zero bytes. Is there anything I might be missing in toco conversion? ", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe support for the missing op (DIV, Exp, NEG, Sub, TensorFlowMax, TensorFlowSum) is in place now.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Could you please try with a new nightly (pip install --upgrade tf-nightly) and let us know if that works as @andrehentz mentions, most of these ops have been added.", "Sorry for my late response, the pb file can be successfully converted now. Thank you very much for your support. ", "I also had this error while converting pnet.pb to tflite ,it suggested that max is not supported by tflite\r\ni just added --allow_custom_ops flag and the model is converted  ", "@Rob1datta Does the model converted with --allow_custom_ops flag work well?", "@kangshan0910 plz use my repo https://github.com/jiangxiluning/facenet_mtcnn_to_mobile ", "@jiangxiluning  Thank you for the github, I could convert the MTCNN model using it. \r\nLater, I wanted to to have variable size input for all the networks - (Pnet, Rnet and Onet). To achieve this I used tf-nightly package for conversion. \r\nWhile doing conversion I converted all the layers(cls_prob,bbox_pred,landmark_pred) one by one for Pnet, Rnet and Onet.\r\nIn the conversion step, I put \"--experimental_new_converter==True\".\r\nAfter converting to tflite I could use dynamic size input array for all the input networks by adding one line in interpreter phase as shown below:\r\n\r\n`from tensorflow.contrib.lite.python import interpreter`\r\n`model = Interpreter(model_path='cls_prob.tflite')`\r\n`input_details = model.get_input_details()`\r\n`req_input_size = (2, 128, 200, 1)`\r\n`model.resize_tensor_input(input_details[0]['index'], req_input_size)`\r\n`model.allocate_tensors()`\r\n", "@varshadhml  @jiangxiluning  can you please provide python code for how to use onet.tflite, pnet.tflite and rnet.tflite?"]}, {"number": 16090, "title": "MKL-DNN:  fix batchnorm unit test failures", "body": "Fix failures of all 9 fuse batchnorm test cases.\r\n\r\nhandle corner case (empty input tensors)\r\nhandle inference case properly - bwd bug related to fwd primitive creation (as a hint)\r\nrefactor - moving output tensor allocation to separate methods - to avoid duplicated code", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 16089, "title": "Disable flaky model_analyzer_test on windows.", "body": "PiperOrigin-RevId: 180599588", "comments": []}, {"number": 16088, "title": "Disable keras data_utils_test as it's flaky.", "body": "", "comments": ["Jenkins, test this please.", "If you also need to disable it in cmake builds, you will need to add it to the test blacklist in `tensorflow/contrib/cmake/tf_tests.cmake`", "@av8ramit is this still relevant?", "@m3bm3b I was having issues with this test as buildcop. If this is still occurring please feel free to merge. If not I can revert this.", "The flakiness is on windows, which is not addressed by this PR.\r\nI will close this for now, and we can disable it in cmake build if we see the flake again."]}, {"number": 16087, "title": "  tf.estimator.train_and_evaluate list  of eval_spec", "body": "Sometimes could be useful to evaluate on different clusters of the evaluation set with distinct metrics, summaries etc.\r\nDo you plan that this interface will accept a list of `eval_spec` in the future?", "comments": ["@martinwicke @ispirmustafa any hints about future plans?", "@xiejw could you please comment?", "Yes. it was planned. We are working on this. ", "+1\r\nTo compare train and validation metrics \"apples\" to \"apples\", I run two evaluations, one on the validation set and one on a fixed subset of training data. To do this I call the evaluate's in a CheckpointSaverListener passed to estimator.train(...., saving_listeners), but two EvalSpec's passed to train_and_evalute(..., eval_spec) would be cleaner. Alternatively, EvalSpec could take a list of input_fn's and corresponding name's. Whichever you think is easier/better for the api.", "Any updates on this issue? Can `tf.estimator.evaluate / tf.estimator.train_and_evaluate` get a list of `eval_spec`s yet? It'd be a very useful feature.\r\n", "> +1\r\n> To compare train and validation metrics \"apples\" to \"apples\", I run two evaluations, one on the validation set and one on a fixed subset of training data. To do this I call the evaluate's in a CheckpointSaverListener passed to estimator.train(...., saving_listeners), but two EvalSpec's passed to train_and_evalute(..., eval_spec) would be cleaner. Alternatively, EvalSpec could take a list of input_fn's and corresponding name's. Whichever you think is easier/better for the api.\r\n\r\nHI @markpwoodward could you please share the code to see how you are doing that? It is not very straightforward to me how we can add multiple datasets during evaluation (when we do it through TF's Estimator) and it's such a useful thing to do.\r\n\r\nThanks in advance!\r\n", "Here is an example of what I was talking about using estimator.train()\nhttps://github.com/markpwoodward/active_osl/blob/master/train.py\n\nBeing able to pass a list of eval_spec's to estimator.train_and_evaluate()\nwould clean this up considerably, removing the need for the\nCheckpointSaverListeners.\n\nBest,\nMark\n\nOn Thu, Dec 20, 2018 at 1:13 PM geopapa11 <notifications@github.com> wrote:\n\n> +1\n> To compare train and validation metrics \"apples\" to \"apples\", I run two\n> evaluations, one on the validation set and one on a fixed subset of\n> training data. To do this I call the evaluate's in a\n> CheckpointSaverListener passed to estimator.train(...., saving_listeners),\n> but two EvalSpec's passed to train_and_evalute(..., eval_spec) would be\n> cleaner. Alternatively, EvalSpec could take a list of input_fn's and\n> corresponding name's. Whichever you think is easier/better for the api.\n>\n> HI @markpwoodward <https://github.com/markpwoodward> could you please\n> share the code to see how you are doing that? It is not very\n> straightforward to me how we can add multiple datasets during evaluation\n> (when we do it through TF's Estimator) and it's such a useful thing to do.\n>\n> Thanks in advance!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16087#issuecomment-449136071>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGgTpVqUXMDjhKTO9bZnAfZyZqt86xD9ks5u6_2AgaJpZM4Rc-TX>\n> .\n>\n", "@markpwoodward thanks so much! This is immensely helpful! And yes I couldn't agree more. This added functionality in estimator.train_and_evaluate() would give so much more flexibility to test algo's performance on different datasets. Happy Holidays", "Hi, are there any updates on this ?", "Hi, \r\n\r\njust in case someone is still wondering about this, here is my workaround after some hours struggling:\r\n\r\nIn order to use `train_and_evaluate`, I include at `EvalSpec` definition a `tf.estimator.FeedFnHook` which contains a function that performs the evaluation over different datasets employing  `estimator.evaluate`. I give a different evaluation folder by the `name` parameter at `estimator.evaluate`. You will probably have to include a new `input_fn` function for `estimator.evaluate` but at least it kind of works\r\n\r\nHope is clearer in the following snippet:\r\n```python\r\ndef multi_eval_hook():\r\n  for validation_set in extra_validation_sets:\r\n    estimator.evaluate(input_fn=lambda:super_eval_fn(val_set.get(\"images\"), val_set.get(\"labels\")), \r\n                       steps=eval_steps, name=val_set[\"id\"])\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=lambda: train_fn,\r\n                                    max_steps=train_steps * train_epochs,\r\n                                    hooks=train_hooks)\r\neval_spec = tf.estimator.EvalSpec(input_fn=eval_fn,\r\n                                  steps=eval_steps, \r\n                                  hooks=[tf.estimator.FeedFnHook(multi_eval_hook)])\r\n```\r\n\r\n", "@PeterMcGor  Is it possible to get the name we pass in to `estimator.evaluate` inside of `model_fn`? I have a custom summary writer to log stuff into that directory. Any help is appreciated.", "I tried the workaround and I thought the evaluation was running forever. I then ran an empty multi eval hook with just logging inside.\r\n\r\n```\r\ndef multi_eval_hook():\r\n    logger.info(\"Starting additional evaluation\")\r\n    logger.info(\"Finished additional evaluation\")\r\n\r\neval_spec = tf.estimator.EvalSpec(lambda: valid_input_fn(params),\r\n                                  hooks=[tf.estimator.FeedFnHook(multi_eval_hook)], name=\"eval\")\r\n```\r\nThis produces \r\n```\r\n2020-04-14 11:27:03.000990: I saver.py:1284] Restoring parameters from experiments/custom_loss_normalized/loss_alpha_beta=0.75_0.75/model.ckpt-500\r\n2020-04-14 11:27:06.000067: I session_manager.py:504] Running local_init_op.\r\n2020-04-14 11:27:06.000375: I session_manager.py:507] Done running local_init_op.\r\n2020-04-14 11:27:08.000858: I train.py:78] Starting additional evaluation\r\n2020-04-14 11:27:08.000859: I train.py:83] Finished additional evaluation\r\n2020-04-14 11:27:18.000519: I train.py:78] Starting additional evaluation\r\n2020-04-14 11:27:18.000520: I train.py:83] Finished additional evaluation\r\n2020-04-14 11:27:19.000352: I train.py:78] Starting additional evaluation\r\n2020-04-14 11:27:19.000352: I train.py:83] Finished additional evaluation\r\n2020-04-14 11:27:19.000412: I train.py:78] Starting additional evaluation\r\n2020-04-14 11:27:19.000412: I train.py:83] Finished additional evaluation\r\n2020-04-14 11:27:19.000472: I train.py:78] Starting additional evaluation\r\n2020-04-14 11:27:19.000473: I train.py:83] Finished additional evaluation\r\n2020-04-14 11:27:19.000531: I train.py:78] Starting additional evaluation\r\n2020-04-14 11:27:19.000531: I train.py:83] Finished additional evaluation\r\n2020-04-14 11:27:19.000588: I train.py:78] Starting additional evaluation\r\n2020-04-14 11:27:19.000588: I train.py:83] Finished additional evaluation\r\n2020-04-14 11:27:19.000646: I train.py:78] Starting additional evaluation\r\n2020-04-14 11:27:19.000646: I train.py:83] Finished additional evaluation\r\n2020-04-14 11:27:19.000701: I train.py:78] Starting additional evaluation\r\n2020-04-14 11:27:19.000701: I train.py:83] Finished additional evaluation\r\n2020-04-14 11:27:19.000756: I train.py:78] Starting additional evaluation\r\n2020-04-14 11:27:19.000756: I train.py:83] Finished additional evaluation\r\n2020-04-14 11:27:20.000248: I evaluation.py:273] Inference Time : 18.71248s\r\n2020-04-14 11:27:20.000248: I evaluation.py:276] Finished evaluation at 2020-04-14-11:27:20\r\n```\r\n\r\nIt runs 10 times, instead of just once. I think it's called for each step, in mine I'm running one epoch for all validation data, but runs in 10 steps (or batches)", "Cool! It's true was slow, I didn't have so many datasets so it wast a big problem for me but this it is great! \r\n\r\nThank you!", "Hi. I tried @PeterMcGor proposal and the code inside multi_eval_hook() is executed three times. Any ideas how to solve this? It is actually evaluating the complete set three times, which increases a lot computation time."]}, {"number": 16086, "title": "[Intel MKL-DNN] fixes for several MKLDNN unit tests.", "body": "Current MKLDNN element wise (add) results in several unit test failure. A temporary workaround is provided by comment out the MKLDNN element wise (add) optimization. ", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 16085, "title": "Add unspecified dimensions (-1) support for noise_shape with tf.nn.dropout", "body": "This fix tries to address the issue raised in #16034 where it was not possible to have unspecified dimensions for `noise_shape` with `tf.nn.dropout`.\r\n\r\nThis fix adds the support so that it is possible to specify `noise_shape = [-1, 1, 1, -1]` instead of `noise_shape = [k, 1, 1, n]`.\r\n\r\nThis fix fixes #16034.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["I think it makes more sense to use \"None\" as the signal instead of -1; -1 typically means 'compute the unknowns based on the knowns', and there are two unknowns so the system is underdetermined.\r\n\r\nThere may be another way to make this code even simpler using tensor_shape.py utilities...", "(Specifically, I feel like using the merge_with function combined with Nones instead of -1 might be sufficient.)", "Thanks @vrv for the review. The PR has been updated to use `None` instead of `-1` for shapes. I tried to use `merge_with` though it seems that there is no flag to disable the compatibility check. So an error will be thrown:\r\n```\r\nValueError: Shapes (?, 1) and (1200, 3) are not compatible\r\n```\r\nIf I try to merge `[None, 1]` with `[1200, 3]`. For that I added additional logic to process the shape.\r\n\r\nPlease take a look and see if there are any issues.", "@vrv another look?", "Given the contract in the documentation that describes broadcasting compatibility, perhaps you could use one of either tf.broadcast_static_shape, or the shape helper: https://github.com/tensorflow/tensorflow/blob/b0d6bf342541d5665579b16f9c8af9022dd36536/tensorflow/python/framework/common_shapes.py#L566 ?\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> a = tf.TensorShape([None, 1])\r\n>>> b = tf.TensorShape([1200, 3])\r\n>>> tf.broadcast_static_shape(a, b)\r\nTensorShape([Dimension(1200), Dimension(3)])\r\n```\r\n\r\n(Trying to minimize the amount of duplicate code, if possible).\r\n\r\n", "Thanks @vrv. The broadcasting compatibility for dropout is the reverse of normal broadcasting compatibility. In other words, with\r\n```\r\n>>> a = tf.TensorShape([None, 1])\r\n>>> b = tf.TensorShape([1200, 3])\r\n```\r\nthe expected result is `[1200, 1]`.\r\n\r\nI think this specific shape processing might be used in other places in the future as well.\r\nMaybe I could add a method in common_shapes.py, e.g.,\r\n```\r\ndef as_shape_with_default(shape_x, shape_y): \r\n...\r\n```\r\nand calling the defined method from `nn_ops.py`?", "@yongtang I think it would be better for now to just use what you have; these broadcasting semantics aren't very common yet.", "Actually, I take that back, let's add it to nn_ops.py, because this function should probably also be used by the Dropout wrapper in python/layers/, which looks like it has the same noise_shape calculation that should ideally be shared.\r\n\r\n/cc @anj-s", "Thanks @vrv. The PR has been updated with noise function moved to `nn_ops.py` so that it could be used by layers. The except has also been restricted to `TypeError` and `ValueError` (in eager mode). Please take a look and let me know if there are any issues.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tensorflow-jenkins test this please"]}, {"number": 16084, "title": "Update download_dependencies.sh to prevent crash from 403", "body": "The eigen bitbucket seems to have changed causing the scrip to crash with a unrecognized archive error.\r\nChanging to grep -v mirror.bazel seems to fix this because otherwise we get a 403 forbidden error.", "comments": ["Can one of the admins verify this patch?", "@StevenHickson thanks for the fix."]}, {"number": 16083, "title": "Tensorflow and Bazle build in Docker Container on Windows", "body": "Hello, i'm trying to build the following Dockerfile:\r\n\r\n```\r\nFROM ubuntu:17.10\r\n\r\nADD https://bazel.build/bazel-release.pub.gpg /bazel-release.pub.gpg\r\nRUN apt-key add /bazel-release.pub.gpg && rm /bazel-release.pub.gpg\r\nRUN echo \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | tee /etc/apt/sources.list.d/bazel.list\r\n\r\nRUN apt-get update && apt-get install -y git build-essential bazel openjdk-8-jdk python3-dev python3-pip python3-numpy python3-wheel && ln --symbolic python3 /usr/bin/python\r\n\r\nRUN git clone --recursive --branch r1.4 --depth 1 --shallow-submodules https://github.com/tensorflow/tensorflow\r\nRUN cd /tensorflow && (echo \"\\n\\ny\\nn\\nn\\nn\\ny\\nn\\nn\\nn\\nn\\nn\\n\\n\" | ./configure)\r\nRUN cd /tensorflow && bazel build --config=opt //tensorflow/compiler/aot:tfcompile\r\n```\r\n\r\n\r\n\r\n\r\nIm getting the following Error on the last RUN operation:\r\n\r\n```\r\nStep 8/8 : RUN cd /tensorflow && bazel build --config=opt //tensorflow/compiler/aot:tfcompile\r\n ---> Running in ef36db7c22d1\r\n..................\r\nLoading:\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (1 packages loaded)\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (4 packages loaded)\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (5 packages loaded)\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (6 packages loaded)\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (6 packages loaded)\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (6 packages loaded)\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (6 packages loaded)\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (6 packages loaded)\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (19 packages loaded)\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (25 packages loaded)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):\r\n        File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD\", line 27\r\n                cc_library(name = \"syclrt\", srcs = [sycl_libr...\")], <3 more arguments>)\r\n        File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD\", line 30, in cc_library\r\n                sycl_library_path\r\nname 'sycl_library_path' is not defined\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (35 packages loaded)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nAnalyzing: target //tensorflow/compiler/aot:tfcompile (45 packages loaded)\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'\r\nERROR: /tensorflow/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'\r\nERROR: Analysis of target '//tensorflow/compiler/aot:tfcompile' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 18.318s\r\nFAILED: Build did NOT complete successfully (46 packages loaded)\r\n```\r\n\r\nMy Docker Version:\r\n```\r\nClient:\r\n Version:       17.12.0-ce\r\n API version:   1.35\r\n Go version:    go1.9.2\r\n Git commit:    c97c6d6\r\n Built: Wed Dec 27 20:05:22 2017\r\n OS/Arch:       windows/amd64\r\n\r\nServer:\r\n Engine:\r\n  Version:      17.12.0-ce\r\n  API version:  1.35 (minimum version 1.12)\r\n  Go version:   go1.9.2\r\n  Git commit:   c97c6d6\r\n  Built:        Wed Dec 27 20:12:29 2017\r\n  OS/Arch:      linux/amd64\r\n  Experimental: true\r\n```\r\n\r\n\r\nAny ideas why this doesn't work?\r\nThe Dockerfile works fine on linux.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 16082, "title": "Connect Apache Beam/Spark to TensorFlow (MonitoredTrainingSession) in a streaming manner?", "body": "### Describe the problem\r\nI have a lengthy question on [SO](https://stackoverflow.com/questions/47986410/optimal-data-streaming-and-processing-solution-for-enormous-datasets-into-tf-dat) about this. But in short, is there a way (or a best practice) to pipe big training datasets directly into a distributed setting (e.g. GKE),  especially if they are subjected to a heavy preprocessing? \r\nI'm basically reaching the limit of what can be sanely stored in TFRecords (they are verbose and heavy).\r\nThe closest issue was this one (https://github.com/tensorflow/tensorflow/issues/12903) and this guide (https://github.com/GoogleCloudPlatform/dataflow-prediction-example) but I do not see a healthy way to implement it (last one with a `@singleton` looks like a hack and not usable with the `tf.Dataset` or `MonitoredTrainingSession`).\r\n\r\nI believe this is a useful issue/feature request for a decent amount of Tensorflow users. ", "comments": ["There is a pending PR #14098 that adds a KafkaReader that might be related.", "@yongtang Yes! I'm subscribed to that PR, and already looked at the kernel (and it looks nice, well done!). Kafka would be a nice option to have.\r\nThe infrastructure, though, is scattered, and not well documented. For example, Is there a way to stream data directly from Apache Beam/Spark (or even Kafka) to a `tf.data.Dataset_from_generator()` without dumping data into `tfrecords` files? \r\nMaybe just a simple streaming serialization support a la Tensorflow Serving's `predict_pb2.PredictRequest()`  +  `CopyFrom(make_tensor_proto(SerializeToString())` can do the trick, as it works really well? (it can still be a `tf.Example`, `tf.SequenceExample` proto but in a more convenient online way)", "Adding @mrry for thoughts on best practices in this case.", "If you're using PySpark and wrapping your training step in an `RDD.mapPartitions()` transformation, you should be able to use `tf.data.Dataset.from_generator()` to convert the partition iterator into a generator of NumPy arrays. I dare say there might be a more efficient way to connect a Spark RDD to a `tf.data.Dataset` (e.g. with a custom reader dataset as @yongtang suggested), and we'd welcome contributions of that kind.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 16081, "title": "MKL-DNN: fix concat issue related to negative input concat_dim", "body": "For a negative concat_dim input, the actual concat_dim should be N + concat_dim with N\r\nbeing the dims of input tensors.\r\n\r\nThis PR fixes an issue of setting N properly.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}]