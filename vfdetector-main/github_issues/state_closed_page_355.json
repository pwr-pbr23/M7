[{"number": 43420, "title": "I encountered the following problems when installing the Go version of TensorFlow in the windows environment", "body": "I encountered the following problems when installing the Go version of TensorFlow in the windows environment\uff1a\r\n\r\ngo run tensorflow.go\r\nresult\uff1a\r\n..\\..\\..\\github.com\\tensorflow\\tensorflow\\tensorflow\\go\\saved_model.go:25:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any\r\nof:\r\n        C:\\Go\\src\\github.com\\tensorflow\\tensorflow\\tensorflow\\go\\core\\protobuf\\for_core_protos_go_proto (from $GOROOT)\r\n        H:\\project\\src\\github.com\\tensorflow\\tensorflow\\tensorflow\\go\\core\\protobuf\\for_core_protos_go_proto (from $GOPATH)\r\n\r\n------------------------\r\nI installed tensorflow(CPU) of C , the installation location is: C:\\Program Files\\tensorflow\\lib\r\n", "comments": ["https://tensorflow.google.cn/install/lang_go\r\nThe content of tensorflow.go is\uff1a\r\npackage main\r\n\r\nimport (\r\n    tf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n    \"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n    \"fmt\"\r\n)\r\n\r\nfunc main() {\r\n    // Construct a graph with an operation that produces a string constant.\r\n    s := op.NewScope()\r\n    c := op.Const(s, \"Hello from TensorFlow version \" + tf.Version())\r\n    graph, err := s.Finalize()\r\n    if err != nil {\r\n        panic(err)\r\n    }\r\n\r\n    // Execute the graph in a session.\r\n    sess, err := tf.NewSession(graph, nil)\r\n    if err != nil {\r\n        panic(err)\r\n    }\r\n    output, err := sess.Run(nil, []tf.Output{c}, nil)\r\n    if err != nil {\r\n        panic(err)\r\n    }\r\n    fmt.Println(output[0].Value())\r\n}", "@qinjinze,\r\nAs per the [documentation](https://www.tensorflow.org/install/lang_go#supported_platforms), TensorFlow for Go is supported on Linux and macOS.\r\n\r\nCould you please check if you are facing the same issue in a Linux machine? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43420\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43420\">No</a>\n"]}, {"number": 43419, "title": "invalid syntax error while importing tensorflow and keras", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.8.0\r\n- Python version:3.8.5\r\n- Installed using virtualenv? pip? conda?:virtualenv\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nTraceback (most recent call last):\r\n  File \"c:/Users/win10/Desktop/python projects/try.py\", line 5, in <module>\r\n    import keras\r\n  File \"C:\\Users\\win10\\tensor\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\r\n  File \"C:\\Users\\win10\\tensor\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\win10\\tensor\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\win10\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\win10\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 124\r\n    def TFE_ContextOptionsSetAsync(arg1, async):\r\n                                         ^\r\nSyntaxError: invalid syntax\r\n![Capture](https://user-images.githubusercontent.com/47914144/93744112-eaf6b580-fc0e-11ea-9add-ba65b39da2e3.JPG)\r\n![Capture](https://user-images.githubusercontent.com/47914144/93744138-f8ac3b00-fc0e-11ea-8af4-3fc8dae1565a.JPG)\r\n![Capture](https://user-images.githubusercontent.com/47914144/93744171-082b8400-fc0f-11ea-8ab4-f743631a702a.JPG)\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@mjvithesh \r\nIs there any particular reason for using such an old tf version when there are latest available, we have support for tf 1.15 and 2.x.\r\nI see you are using python 3.8 which is compatible from tf 2.2 version only.\r\nneed be you may follow [installation at this link.](https://www.tensorflow.org/install).\r\n", "I couldn't install the latest version.\nI tried to upgrade, but it is not upgrading to 2.2", "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow", "@mjvithesh \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/43289#issuecomment-694093326) and let us know.\r\nyou may also refer to: #42977 #43244 [can you please verify if your python/cpu is 64 bits] #33838, [link](https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip) , also please upgrade your pip version if you are unable to upgrade to 2.2, or try to down grade your python 64 bit and tf to 2.x version or 1.15", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43419\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43419\">No</a>\n"]}, {"number": 43418, "title": "Fixes #43200 - TensorBoard histogram breaks on unsupported weight dtypes", "body": "[43200](https://github.com/tensorflow/tensorflow/issues/43200)\r\n\r\nExplicit weight.dtype check was added into TensorBoard callback _log_weights.\r\nWeights with unsupported dtypes are skipped from summary histogram and warning is logged.\r\n\r\nOne thing to note is I wasn't able to finds explicit python list of supported dtypes, so had to hard-code one, which is not ideal. Left a comment in source code.\r\nSimilarly there is a list of unsupported dtypes hard-coded into test code.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43418) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43418) for more info**.\n\n<!-- ok -->", "@unsatcore Can you please check @omalleyt12's comments and keep us posted ? Thanks!", "@unsatcore  Can you please resolve conflicts? Thanks!", "Done", "@gbaned, @omalleyt12 let me know if you need any changes", "@unsatcore Can you please address Ubuntu Sanity errors? Thanks!", "@unsatcore  Can you please check @fchollet's comments and keep us posted ? Thanks!", "@unsatcore Any update on this PR? Please. Thanks!\r\n", "Closing due to lack of activity -- feel free to reopen the PR in the future (with recommended style fixes), this time targeting the new repo, `keras-team/keras`: https://github.com/keras-team/keras"]}, {"number": 43417, "title": "Fixes #42872: map_to_outputs_names always returns a copy", "body": "This PR always returns a copy to ensure that any change made to `struct` after calling this function does not propagate to the original `struct` which leads to undesired behavior.\r\nThe PR fixes #42872 (code snipped to reproduce the bug also present there)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43417) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it! ", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43417) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43417) for more info**.\n\n<!-- ok -->", "Is this merged?", "This PR is merged, hence closing this. Thank you."]}, {"number": 43416, "title": "Got Segmentation fault when calling tflite::Model::UnPack() ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):master\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):2.0.0\r\n- GCC/Compiler version (if compiling from source):7.5.0\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nPic.1:\r\n![image](https://user-images.githubusercontent.com/36526001/93737881-e3d3a580-fc16-11ea-8c06-4fb29ea06305.png)\r\nPic.2:\r\n![image](https://user-images.githubusercontent.com/36526001/93738189-bdfad080-fc17-11ea-8a54-bef065ec5ad7.png)\r\nPic.3:\r\n![image](https://user-images.githubusercontent.com/36526001/93738347-25b11b80-fc18-11ea-9b6b-235368499445.png)\r\n\r\nI'm doing benchmark on cartoongan model from tensorflow hub(https://tfhub.dev/sayakpaul/lite-model/cartoongan/int8/1). When I adding a line \"ModelT *modelT = model_->GetModel()->UnPack();\" to  tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc:712(Pic.1) for printing some debug info, I got a \"Segmentation fault\". I have found out that it happens when flatbuffers UnPacking zero_point in QuantizationParameters(Pic.2&3). \r\n\r\n**Describe the expected behavior**\r\nCalling tflite::Model::UnPack() should return tflite::ModelT\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@AssassinGQ \r\n\r\nCan you please share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Sorry  there are some mistakes in issue description, the version of bazel is 3.1.0.\r\n\r\nJust using this file to cover tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc on master and benching model in this link: https://tfhub.dev/sayakpaul/lite-model/cartoongan/int8/1 will  reproduce the issue.\r\n[benchmark_tflite_model.cc.txt](https://github.com/tensorflow/tensorflow/files/5253964/benchmark_tflite_model.cc.txt)\r\n\r\n", "@multiverse-tf \r\nHow is it going\uff1f", "@AssassinGQ  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest  stable Version of TF 2.6 and let us know if the issue still persists? Please refer to the [link](https://www.tensorflow.org/lite/convert),similar [issue](https://github.com/tensorflow/tensorflow/issues/40815) and let us know if it helps ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43416\">No</a>\n"]}, {"number": 43415, "title": "add support for mips64 platform", "body": "Since tensorflow is not supported on MIPS64 platform\uff0c use this patch\uff0cit can be build pass on MIPS64 platform\u3002\r\nand then\uff0c tensorflow can be supported on MIPS64 platform\uff01\r\n\r\nDear Sirs\uff0c\r\n   I have modified the code\uff1ause spaces instead of tabs and match the above indentation.", "comments": ["Dear Sir\uff0c\r\n  It seams this modifies will not affect oneDNN unit tests!\r\n  Please check it! Thank you very much!\r\n", "Dear Sir\uff0c\r\n  Please check this patch\uff1f if any pproblems\uff0c please tell me\uff0cthank you\uff01", "Please check the build failures. We need to make sure all tests pass before we can merge any changes.", "Dear Sirs,\r\n   I have checked the build fail:Windows Bazel GPU \u2014 Internal CI build failed!   \r\nfrom the fail log, it seams failed in x64 platform,  not mips64 platfrom. And, this patch is only for mips64 platform. I think the build fail is not caused by this patch, please help to check it! Thank you very much!\r\n\r\nthe fail log as below:\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(131): error C2143: syntax error: missing '}' before 'constant'\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(131): error C2059: syntax error: 'constant'\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(135): error C2143: syntax error: missing ';' before '}'\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(137): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(137): error C2737: 'ExperimentalCompile': 'constexpr' object must be initialized\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(137): error C2737: 'ExperimentalCompile': 'constexpr' object must be initialized\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(137): error C2146: syntax error: missing ';' before identifier 'ExperimentalCompile_MIN'\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(137): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(137): error C2065: 'NONE': undeclared identifier\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(138): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(138): error C2737: 'ExperimentalCompile': 'constexpr' object must be initialized\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(138): error C2086: 'const int ExperimentalCompile': redefinition\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(137): note: see declaration of 'ExperimentalCompile'\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(138): error C2737: 'ExperimentalCompile': 'constexpr' object must be initialized\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(138): error C2146: syntax error: missing ';' before identifier 'ExperimentalCompile_MAX'\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(138): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(139): error C2131: expression did not evaluate to a constant\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin\\tensorflow/core/protobuf/saved_object_graph.pb.h(139): note: failure was caused by non-constant arguments or reference to a non-constant symbol", "Also,the MacOS CPU Python3 \u2014 Internal CI build failed, failed logs as below.(I have not modified this code.)\r\n\r\nbazel-out/host/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:132:3: error: expected identifier\r\n  FALSE = 2,\r\n  ^\r\n/Applications/Xcode_10.3.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/boolean.h:85:17: note: expanded from macro 'FALSE'\r\n#define FALSE   0\r\n                ^\r\nIn file included from tensorflow/compiler/mlir/python/mlir.cc:29:\r\nIn file included from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:25:\r\nIn file included from ./tensorflow/cc/saved_model/bundle_v2.h:28:\r\nIn file included from bazel-out/host/bin/tensorflow/core/protobuf/meta_graph.pb.h:43:\r\nbazel-out/host/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:138:31: error: cannot initialize a variable of type 'const tensorflow::ExperimentalCompile' with an rvalue of ", "Dear Sirs,  \r\n  Please check my comments above, thank you very much!\r\nIt seams the fail test is not caused by my patch.\r\n\r\nBest Regards", "Triggering a new build", "Dear sir\r\n  Can this patch can be merged \uff1f\r\n\r\nBest Regards", "The automatic import failed. Let's try again. Please ping me in ~12-13 hours if `import/copybara` still says \"An error happened\""]}, {"number": 43414, "title": "Fix missing import", "body": "", "comments": []}, {"number": 43413, "title": "Fix tests broken during cherry-picks", "body": "", "comments": []}, {"number": 43412, "title": "Fix tests broken by merges", "body": "", "comments": []}, {"number": 43411, "title": "Fix tests broken by merge conflicts", "body": "", "comments": []}, {"number": 43410, "title": "Fix broken tests after cherrypicks", "body": "", "comments": []}, {"number": 43409, "title": "In load_model method: Input 'values' passed int64 expected int32", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution:  Linux Ubuntu 20.04 and kaggle\r\n\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version:3.7\r\n\r\n**Describe the current behavior**\r\nI have trained a model with custom loss and after save, I tried to load it but got this error\r\n\r\n**2 errors while building NodeDef 'tf_op_layer_stack_2/stack_2' using Op<name=Pack; signature=values:N*T -> output:T; attr=N:int,min=1; attr=T:type; attr=axis:int,default=0>:\r\nInput 'values' passed int64 expected int32\r\nInconsistent values for attr 'T' DT_INT32 vs. DT_INT64**\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should be loaded without any error\r\n\r\n**Standalone code to reproduce the issue**\r\nA minimalistic gist is [attached](https://gist.github.com/partha117/bbcd22f096c71d6f30858c4e560e9684).\r\n\r\n", "comments": ["@partha117,\r\nI was able to reproduce the error with TF v2.3.\r\n\r\nHowever, the error seems to be resolved with the latest TF-nightly. I was able to run the code without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/de5cf5d0b19a87f2f79b61642fec4ad0/43409-tf-nightly.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Yes, It has been solved thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43409\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43409\">No</a>\n"]}, {"number": 43408, "title": "Remove import that is not needed", "body": "", "comments": []}, {"number": 43407, "title": "Remove import that is not needed", "body": "", "comments": []}, {"number": 43406, "title": "Remove import that is not needed", "body": "", "comments": []}, {"number": 43405, "title": "Fix import path", "body": "", "comments": []}, {"number": 43404, "title": "Fix import path", "body": "", "comments": []}, {"number": 43403, "title": "Fix import path", "body": "", "comments": []}, {"number": 43402, "title": "Cast away a const in intermediate API", "body": "", "comments": []}, {"number": 43401, "title": "Cast away a const in intermediate API", "body": "", "comments": []}, {"number": 43400, "title": "Fix typo in macro", "body": "", "comments": []}, {"number": 43399, "title": "Fix typo in macro", "body": "", "comments": []}, {"number": 43398, "title": "Fix typo in macro", "body": "", "comments": []}, {"number": 43397, "title": "Hexagon delegate performance", "body": "Looking at http://ai-benchmark.com/ranking_detailed.html I note that hexagon delegate performance is practically identical on snapdragon 855 und snapdragon 865, even though the DSP on SD 865 is much more powerful according to specs. Why is that ?", "comments": ["Hi @aki65,\r\n\r\nI'm not familiar with the details about sd865 vs. sd855 w.r.t. DSP. Does such performance similarity also show up with larger models?\r\n\r\nBased on my limited knowledge about DSP, I guess it is possible that not all processing units on the sd865 DSP are utilized to run the inference, and therefore the theoretical computing capability isn't showcased here.\r\n\r\nNote, tflite hexagon delegate uses Qualcomm's the Hexagon NNlib to utilize the DSP. So, I think we might need to optimize this library to improve the inference latency. But to improve the throughput, it might be achieved by running with a pool of TFLite interpreters to do model inference simultaneously on the sd865 dsp.", "Not everything is more optimized on 865, so it comes down to the model. If it can be using any of the more optimized paths on the 865, otherwise can be the same."]}, {"number": 43396, "title": "Fix import path", "body": "", "comments": []}, {"number": 43395, "title": "Fix import path", "body": "", "comments": []}, {"number": 43394, "title": "Fix import path", "body": "", "comments": []}, {"number": 43393, "title": "GPUDelegate Produces Incorrect Result for reduce_sum", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux (Kernel version 5.8.10)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung s10e (Android 10)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.1-41975-g46b6537110 2.4.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.5.0\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: CUDA 11.0.3/cuDNN 8.0.2.39\r\n- GPU model and memory: 2080 Ti/11G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using GPU delegate on mobile with reduce_sum, the result is incorrect (the absolute difference is larger than 1e-1). By contrast, If GPU is not used the result is correct.\r\n\r\n**Describe the expected behavior**\r\nThe result difference should be relatively very small.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThe codes to generate the graph:\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef generate_buggy_graph(batch_size):\r\n    with tf.Graph().as_default() as graph, tf.compat.v1.Session(graph=graph) as session:\r\n        source = tf.compat.v1.placeholder(tf.float32, shape=[batch_size, 100])\r\n        target = tf.transpose(tf.reduce_sum(tf.transpose(source), axis=1))\r\n        # target = tf.reduce_sum(source, axis=0)\r\n\r\n    converter = tf.compat.v1.lite.TFLiteConverter.from_session(session, [source], [target])\r\n    with open(\"buggy_graph.tflite\", \"wb\") as writer:\r\n        writer.write(converter.convert())\r\n\r\n\r\ngenerate_buggy_graph(256)\r\n```\r\n\r\nTo push the generated model to the mobile I used the following:\r\n```shell\r\nadb push buggy_graph.tflite /storage/emulated/0/Android/data/com.example.mobilenn/files/models/BuggyGraph/buggy_graph.tflite\r\n```\r\n\r\nThe codes to run the graph on mobile:\r\n\r\nBuggyModel.java\r\n```java\r\npackage com.example.mobilenn.lite.models;\r\n\r\nimport android.util.Log;\r\n\r\nimport org.tensorflow.lite.Interpreter;\r\nimport org.tensorflow.lite.gpu.GpuDelegate;\r\nimport org.tensorflow.lite.nnapi.NnApiDelegate;\r\n\r\nimport java.io.File;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.ByteOrder;\r\nimport java.util.Locale;\r\n\r\npublic class BuggyModel {\r\n    public static void run(File basePath) {\r\n        Interpreter.Options interpreterOptions = new Interpreter.Options();\r\n//        interpreterOptions.addDelegate(new NnApiDelegate());\r\n        interpreterOptions.addDelegate(new GpuDelegate());\r\n        Interpreter interpreter = new Interpreter(\r\n                new File(basePath, \"buggy_graph.tflite\"),\r\n                interpreterOptions\r\n        );\r\n\r\n        ByteBuffer inputs = ByteBuffer\r\n                .allocateDirect(interpreter.getInputTensor(0).numBytes())\r\n                .order(ByteOrder.nativeOrder());\r\n        ByteBuffer outputs = ByteBuffer\r\n                .allocateDirect(interpreter.getOutputTensor(0).numBytes())\r\n                .order(ByteOrder.nativeOrder());\r\n\r\n        float[] stdAnswer = new float[100];\r\n        for (int batchId = 0; batchId < 256; batchId++) {\r\n            for (int channelId = 0; channelId < 100; channelId++) {\r\n                float value = (float) Math.random();\r\n                stdAnswer[channelId] += value;\r\n                inputs.putFloat(value);\r\n            }\r\n        }\r\n        interpreter.run(inputs, outputs);\r\n\r\n        for (int channelId = 0; channelId < 100; channelId++) {\r\n            Log.d(\"BuggyModel\", String.format(\r\n                    Locale.getDefault(),\r\n                    \"Channel %d: real: %.3f correct: %.3f diff: %.3f\",\r\n                    channelId,\r\n                    outputs.getFloat(channelId * Float.BYTES),\r\n                    stdAnswer[channelId],\r\n                    outputs.getFloat(channelId * Float.BYTES) - stdAnswer[channelId]\r\n            ));\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nMainActivity.java\r\n```java\r\npackage com.example.mobilenn;\r\n\r\nimport android.os.Bundle;\r\n\r\nimport androidx.appcompat.app.AppCompatActivity;\r\n\r\nimport com.example.mobilenn.lite.models.BuggyModel;\r\n\r\nimport java.io.File;\r\n\r\npublic class MainActivity extends AppCompatActivity {\r\n\r\n    @Override\r\n    protected void onCreate(Bundle savedInstanceState) {\r\n        super.onCreate(savedInstanceState);\r\n        setContentView(R.layout.activity_main);\r\n\r\n        BuggyModel.run(new File(getExternalFilesDir(\"models\"), \"BuggyGraph\"));\r\n    }\r\n\r\n}\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nWhen used with GPU delegate:\r\n```plain\r\n2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 0: real: 135.750 correct: 135.681 diff: 0.069\r\n2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 1: real: 131.625 correct: 131.705 diff: -0.080\r\n2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 2: real: 128.500 correct: 128.509 diff: -0.009\r\n2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 3: real: 127.500 correct: 127.556 diff: -0.056\r\n2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 4: real: 126.438 correct: 126.508 diff: -0.071\r\n2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 5: real: 130.125 correct: 130.037 diff: 0.088\r\n2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 6: real: 123.938 correct: 123.990 diff: -0.052\r\n2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 7: real: 128.625 correct: 128.717 diff: -0.092\r\n2020-09-20 18:32:39.363 24389-24389/com.example.mobilenn D/BuggyModel: Channel 8: real: 127.063 correct: 127.131 diff: -0.069\r\n2020-09-20 18:32:39.363 24389-24389/com.example.mobilenn D/BuggyModel: Channel 9: real: 122.500 correct: 122.633 diff: -0.133\r\n...\r\n```\r\n\r\nWhen used without any delegate or with NNAPI delegate:\r\n```plain\r\n2020-09-20 18:39:55.368 24973-24973/com.example.mobilenn D/BuggyModel: Channel 0: real: 135.449 correct: 135.449 diff: 0.000\r\n2020-09-20 18:39:55.368 24973-24973/com.example.mobilenn D/BuggyModel: Channel 1: real: 130.922 correct: 130.922 diff: 0.000\r\n2020-09-20 18:39:55.369 24973-24973/com.example.mobilenn D/BuggyModel: Channel 2: real: 128.056 correct: 128.056 diff: 0.000\r\n2020-09-20 18:39:55.369 24973-24973/com.example.mobilenn D/BuggyModel: Channel 3: real: 133.586 correct: 133.586 diff: 0.000\r\n2020-09-20 18:39:55.369 24973-24973/com.example.mobilenn D/BuggyModel: Channel 4: real: 139.664 correct: 139.664 diff: 0.000\r\n2020-09-20 18:39:55.370 24973-24973/com.example.mobilenn D/BuggyModel: Channel 5: real: 130.863 correct: 130.863 diff: 0.000\r\n2020-09-20 18:39:55.370 24973-24973/com.example.mobilenn D/BuggyModel: Channel 6: real: 127.332 correct: 127.332 diff: 0.000\r\n2020-09-20 18:39:55.370 24973-24973/com.example.mobilenn D/BuggyModel: Channel 7: real: 130.422 correct: 130.422 diff: 0.000\r\n2020-09-20 18:39:55.371 24973-24973/com.example.mobilenn D/BuggyModel: Channel 8: real: 130.867 correct: 130.867 diff: 0.000\r\n2020-09-20 18:39:55.371 24973-24973/com.example.mobilenn D/BuggyModel: Channel 9: real: 122.770 correct: 122.770 diff: 0.000\r\n...\r\n```", "comments": ["Seems like you're running things in FP16 and precision errors get accumulated.  If the said precision is not good enough for you, then consider running it at FP32.  There'll be always error if you go lower precision, and whether it works for your particular case or not depends on the model / application.  Feeding and adding up random numbers in a system with precision loss will of course bubble up the error.", "Thanks for the comments. But as you can see in the code, I used `tf.float32` instead of `tf.float16` and I believe Java float number is 4 bytes, which should be align with `tf.float32`. I know by doing some calculations the error will get accumulated when using floating numbers, but I still consider the relative error in magnitude of `1e-3` would be way too large to be practical for any model.", "How you create the model (and save the weights) is different from how you carry out the math with the GPU.  That option is set via `tensorflow/lite/delegates/gpu/java/src/main/java/org/tensorflow/lite/gpu/GpuDelegate.java`.  You can see that `precisionLossAllowed` is `true` by default, which is the thing that controls whether math should be carried out in FP32 or FP16.\r\n\r\nAlso, do not underestimate the error accumulation.  It can easily add up in reducers, and the range of FP16 is quite limiting  https://en.wikipedia.org/wiki/Half-precision_floating-point_format  and break your model especially if it's numerically unstable.  Again, rather than having a sum of random numbers with precision loss and claim that the entire runtime is unstable, I would suggest creating a full network, do a full training, and do an eval of whether the FP16 runtime still works for you or not with an eval dat set.", "I see your points but I has never never imagined the FP16 calculation will incur such large error. Anyways, the problem is dissolved when setting `precisionLossAllowed` to `true`. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43393\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43393\">No</a>\n"]}, {"number": 43392, "title": "Solve leftover from merge conflict", "body": "", "comments": []}, {"number": 43391, "title": "Solve leftover from merge conflict", "body": "", "comments": []}]