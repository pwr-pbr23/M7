[{"number": 576, "title": "Resource Exhausted Error MNIST Example with CPU and Python3 (3.4.3)", "body": "I am having a Resource Exhausted Error in MNIST demo model with CPU and Python3 (3.4.3) [I am using Ubuntu 14.04LTS VMWare with 3 GB over a Windows 10 System; PIP Installation, No VirtualEnv, No Docker]:\n\n$ python -m tensorflow.models.image.mnist.convolutional\n\ndavid@ubuntu:~$ \ndavid@ubuntu:~$ python3 -m tensorflow.models.image.mnist.convolutional\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 1\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 1\nInitialized!\nEpoch 0.00\nMinibatch loss: 12.053, learning rate: 0.010000\nMinibatch error: 90.6%\nW tensorflow/core/kernels/conv_ops.cc:178] Resource exhausted: OOM when allocating tensor with shapedim { size: 5000 } dim { size: 28 } dim { size: 28 } dim { size: 32 }\nW tensorflow/core/common_runtime/executor.cc:1076] 0x579eaa0 Compute status: Resource exhausted: OOM when allocating tensor with shapedim { size: 5000 } dim { size: 28 } dim { size: 28 } dim { size: 32 }\n     [[Node: Conv2D_2 = Conv2D[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Const, Variable/read)]]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 428, in _do_run\n    target_list)\ntensorflow.python.pywrap_tensorflow.StatusNotOK: Resource exhausted: OOM when allocating tensor with shapedim { size: 5000 } dim { size: 28 } dim { size: 28 } dim { size: 32 }\n     [[Node: Conv2D_2 = Conv2D[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Const, Variable/read)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 290, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 278, in main\n    error_rate(validation_prediction.eval(), validation_labels))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 460, in eval\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 2910, in _eval_using_default_session\n    return session.run(tensors, feed_dict)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 5000 } dim { size: 28 } dim { size: 28 } dim { size: 32 }\n     [[Node: Conv2D_2 = Conv2D[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Const, Variable/read)]]\nCaused by op 'Conv2D_2', defined at:\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 290, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 250, in main\n    validation_prediction = tf.nn.softmax(model(validation_data_node))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 188, in model\n    padding='SAME')\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 211, in conv2d\n    use_cudnn_on_gpu=use_cudnn_on_gpu, name=name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n\ndavid@ubuntu:~$ \ndavid@ubuntu:~$ \n", "comments": ["i am getting the exact same error when running this command in a docker container on windows 7 and the virtualbox host image.\n"]}, {"number": 575, "title": "558: Markdown to html, titles", "body": "Hi,\nMost of projects I have worked on were based on Jekyll, pigments of markdown to webpages and used title/layout based on .yml/css/layouts file.In other python projects, providing title on the link itself works.Both variants done as example on Contributing.md to seek inputs.thanks.\n\n```\n\n---\ntitle: Contributing to Spark\n\n---\n```\n\n```\njust getting started, Github has a [howto](https://help.github.com/articles/using-pull-requests/ \"How To Contribute Using Github\u201d)\n```\n\nHowever as this largely depends on project .js/.css files, from my quick look on this codebase, seems the tensorflow/python/framework/docs.py the .md content just gets transformed to body content, and likely library.title reflects the title of page.thanks.\n", "comments": ["Can one of the admins verify this patch?\n", "As @martinwicke mentioned in #558, this change has to be made internally in our website generation code.\n"]}, {"number": 574, "title": "Typo fix", "body": "Fix citation to Bahdanau et al. in sequence to sequence documentation\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 573, "title": "Fixed saver relative paths for `latest_checkpoint`", "body": "This would be cleaner if we made all paths listed in the \"latest\"\nfile relative to the its directory, allowing the removal of the\nadded `os.path.isabs` checks.\n\nThat would make the `os.join` in `saver.latest_checkpoint` much less\nsurprising.\n\nBut at least this way, there is no effect on currently working code.\n\nFixes #571\nChange-Id: I47d8536b9b2ed3dcc193d6e6b7f4573a4e22c9b3\n", "comments": ["Can one of the admins verify this patch?\n", "Let me try kicking off the test for this PR by saying some magic words\n", "test this please\n", "Looks generally fine, assigning to Sherry, the original reviewer.\n", "@tensorflow-jenkins, test this please.\n", "(Can you squash the commits if the tests are all passing?)\n", "Done. Rebased on master.\n", "@tensorflow-jenkins, test this please.\n", "Merged (trying a new workflow where I merge manually via rebasing rather than clicking the button for a better history).\n"]}, {"number": 572, "title": "Update README.md to remove stale install instructions, instead pointing to os_setup.md.", "body": "Change README.md to point to install instructions, since the install instructions were stale (mentioned 0.5.0), and are likely to continue to be stale with respect to os_setup.md.\n", "comments": ["LGTM\n"]}, {"number": 571, "title": "Saver / tf.train.latest_checkpoint doesn't work very well with relative paths", "body": "For example, if model is saved into a folder `test` by doing `saver.save(session, '/path/to/test')` - will result in the having `checkpoints` and `model-0` files. Where `checkpoints` file will have something like this:\n\n```\nmodel_checkpoint_path: \"test/model-0\"\nall_model_checkpoint_paths: \"test/model-0\"\n```\n\nWhich if you call now `tf.train.latest_checkpoint('test')` from the same directory, will result in the `None`.\n\nThis is due to `os.path.join(checkpoint_dir, ckpt.model_checkpoint_path)` which for relative path like above will result in the `test/test/model-0`.\n", "comments": ["Fixed by:\nChange-Id: I47d8536b9b2ed3dcc193d6e6b7f4573a4e22c9b3\n", "We abandoned the gerrit one -- if https://github.com/tensorflow/tensorflow/pull/573 looks good, we can merge\n", "This issue persists when you change the name of the run folder. It seems that the checkpoints/checkpoint file only keep the old path and tf fails when latest_checkpoint is used over this new folder.\n"]}, {"number": 570, "title": "Small fixes for Python 3 support", "body": "Question: what is the procedure to run the Python tests? The [documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md) does not mention them.\n", "comments": ["Can one of the admins verify this patch?\n", "Actually this is mostly a dupe of #568 and #569. Feel free to close this PR. I just added an entry in `.gitignore` for `__pycache__` folders on top of the other 2 PRs.\n", "BTW, is there no continuous integration for Python 3. This kind of regressions are prone to re-appear if the tensorflow core devs do not use Python 3 themselves.\n", "Yeah we just merged #568 -- do you want to update change or just send a new PR for the .gitignore?\n\nYup, we're working on continuous integration tests with python 3 to prevent these regressions.\n", "(re-open if you'd like)\n", "Ok thanks, sorry for the slow reaction.\n"]}, {"number": 569, "title": "Modified `coordinator.py`  to be Python 2/3 compatible", "body": "There was a line that was throwing an error when building for Python 3 (as in Issue #560), but which can be fixed in a way that is Python 2/3 compatible.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "#568 just did this and was a cleaner commit, so we pulled that one instead.  Thanks for the contribution though!\n"]}, {"number": 568, "title": "resolved syntax error on python3", "body": "Exception, -> Exception as, tested running tensorflow/models/image/mnist/convolutional.py on both python2.7 and python3.4 with GPU support.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed the CLA\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "LGTM\n"]}, {"number": 567, "title": "DO NOT MERGE - pull request builder test", "body": "- it should launch tensorflow-pull-requests-multijob\n  - that should launch android, cpu, gpu_pip and mac jobs\n  - android and gpu_pip should be successful, cpu test should\n    fail because of this commit and mac will fail because\n    master is failing\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "It works like a charm. Results are as expected:\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/1/\n\nCPU test have +1 test failed, Mac CPU have +1 failed test, gpu_pip and android demo app successful.\n\nNo \"Can one of the admins verify this patch?\" question here because I'm admin.\n\n@martinwicke @vrv please add tensorflow-jenkins github user as collaborator and the status should appear (with next build).\n", "@jendap Can the bot add a label like \"admin\" or \"maintaner\" when the PR was opened by developers with write access to this repo?\n", "@jendap: done, thanks!\n"]}, {"number": 566, "title": "suppressing \"I\" log messages in tensorflow", "body": "I am using TF 0.6.0 with python 2.7.10 on both ubuntu linux and osx 10.9.5.  TF base functionality is working fine.  The issue is logging - I want to suppress the following messages that are emitted whenever I construct the first tensorflow Session:\n\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 2\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 2\n\nThey do not appear to be transmitted through the normal python logging mechanism.  Any ideas?  Did I just miss something?\n\nI don't want to turn off all log messages, I just want TF to stop generating these two specific messages - and perhaps if you can tell me how to turn off all INFO messages from this particular (seemingly non-pythonic) source, that would be fine too.\n", "comments": ["Hi @davefairtex ,\nI am on latest tensor, python 2.7, and osx.On a quick look, these are logged using Google logging(logging.h) in local_device.cc/direct_session.cc from common_runtime module, and these logs can be required by some users.\nWhile invoking your application you could use flags like below to suppress as needed?\n\n```\n./your_application \u2014minloglevel=1 (such that INFO msgs are not logged; 0/1/2/3 acceptable values defaults to 0)\n./your_application \u2014logtostderr=0 (such that log msgs are suppressed in stderr; 0/1 acceptable values)\n```\n\nLogging levels for INFO, WARNING, ERROR, and FATAL are 0, 1, 2, and 3 respectively.\nThanks!\n", "Rekha-\n\nThanks much for your comment; now I at least have a clue where I should be looking.\n\nI don't have gflags installed.  I tried using an environment variable (which has been alleged to work in comments elsewhere), but it sadly did not work for me:\n\nub1%  GLOG_minloglevel=3 python\nPython 2.7.10 (v2.7.10:15c95b7d81dc, May 23 2015, 09:33:12) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow\n> > > tensorflow.Session()\n> > > I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4\n> > > I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 4\n> > > <tensorflow.python.client.session.Session object at 0x10693fb50>\n> > > import os\n> > > os.environ['GLOG_minloglevel']\n> > > '3'\n\nAny thoughts?  I also tried setting os.environ['GLOG_minloglevel'] = '3' within python directly prior to importing tensorflow, also no luck there.\n", "Hi @davefairtex ,\nFrom the logging referred to in tensorflow*, I observe the parameter re2::FLAGS_minloglevel that needs to be overridden. Set to 1 or greater to suppress info logs. I am not 100% sure it works as new checkins on re2 package, but Ideally it needs to be possible for user to override this parameter via command line. As a workaround, if you are stuck, you can pipeline current output to sed to remove unneeded log lines and get exact output you need.thanks.\n- https://github.com/google/re2/blob/master/util/logging.h\n- https://github.com/google/re2/blob/master/util/flags.h\n", "rekha- thanks for the update.  With your help I found the re2::FLAGS_minloglevel variable in the shared object, but I have no way of setting it directly from python.  (If I were using C++ I could totally do it - but I'm not).   I tried setting it using the python gflags interface:\n\nimport tensorflow\nimport gflags\ngflags.ADOPT_module_key_flags(tensorflow)\ngflags.FLAGS(['-minloglevel', '3'])\ntensorflow.Session()\n\nbut that still gives me the log messages.  Is this the correct way to set the tensorflow gflags?  (Is this actually a tensorflow module gflag?  It does show up in \"nm _pywrap_tensorflow.so\")\n\nIt would be awesome if the code had only one logging mechanism, controllable via python.\n\nI know I can use sed; it just seemed so disagreeable to do that.  I was hoping there was a three-line \"real\" way to fix the problem.\n", "Thanks @davefairtex  I agree its painful for its a simple act of suppressing and silencing logs.Was providing quick suggestions hoping simplest logical way works with flags :-) However looking more though there are various flavors of logging, this one just hits the tensorflow/core/platform/default/logging.h.For this case used verbose logging.thanks.\n", "https://github.com/tensorflow/tensorflow/pull/619\n", "\"And there was great rejoicing\" ...  thanks for the fix, look forward to the new version.  Its really nice to see that you guys are responsive about even minor things like this.\n", "@davefairtex What is the actual fix? Does your glfags python code from above now work for suppressing the \"I\" messages?\n\n> import tensorflow\n> import gflags\n> gflags.ADOPT_module_key_flags(tensorflow)\n> gflags.FLAGS(['-minloglevel', '3'])\n> tensorflow.Session()\n", "hi @fiannaca please refer https://github.com/tensorflow/tensorflow/pull/619 for this log issue.thanks!\n", "@rekhajoshm I should be more clear. I saw that pull request already. I'm just not sure what that means for how I can actually go about suppressing the \"I\" messages from TF. What are the actual steps now for suppressing log messages?\n", "Hi @fiannaca As stated in the #619 , logging has a different flow here and cannot be overridden by gflags/re2 atm. You can apply the patch for the #619 for your setup to avoid the logs. Another quick pull here https://github.com/tensorflow/tensorflow/pull/1229 to have conditional logging., in some time.thanks.\n", "I'm also confused about the status of this. Is this possible to achieve now with a released version, or is it yet to appear? \n\nIs there a way to accomplish it with r0.8?\n", "There's a related issue here: https://github.com/tensorflow/tensorflow/issues/1258\nThat issue is still open, so I suspects there's no way to change logging settings other than modifying the code and recompiling. For instance, here are instructions on how to change verbose logging level here -- http://stackoverflow.com/a/36505898/419116\n", "@vrv, @rekhajoshm: What's the status of this after #619? \n", "#619 just removed some unnecessary logging, our logging library still doesn't have the ability to control verbosity.  Still a contributions welcome at this point.\n", "It has been 11 months since my initial request: a way to stifle the flood of \"ALL IS STILL WELL AND OPERATING NORMALLY\" info messages that appear each every time tf starts up.\n\nSince a more general mechanism can't seem to make it onto anyone's list, can someone please just raise the default debugging level to WARN?  I don't care about INFO messages.  I suspect there is a vast silent majority that agrees with me.  Once our dev environments are configured properly, 99.9% of the problems we users face are in our own code, and the flood of (useless, to us) INFO messages proclaiming (yet again) that the normal initialization sequence worked properly just gets in the way of us trying to sort out the actual errors in our own stuff.\n\nThanks for listening to my rant.\n\nHere's an example of all the stuff I have to wade through every time I run tf (0.10.0):\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 970\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 3.94GiB\nFree memory: 3.88GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)\n", "I am in the same boat as @davefairtex . So far I've been using a workaround: a bash script wrapping the call to my python script, filtering out the unwanted messages. Let me share it here, in case it helps someone:\n\n``` bash\n#!/bin/bash\npython $* 3>&1 1>&2 2>&3 3>&- | grep -v ^I\\ | grep -v ^pciBusID | grep -v ^major: | grep -v ^name: |grep -v ^Total\\ memory:|grep -v ^Free\\ memory:\n```\n\nI called the script `tf.sh` and simpy preface python script calls with it. e.g. `tf.sh my_script.py --option option1 arg1`.\n\nA bit non-trivial as the naive approach using `|&` and then `grep -v` ends up buffering `STDOUT`, at least on my system.\n", "The more general tracking issue is https://github.com/tensorflow/tensorflow/issues/1258\nI like @rekhajoshm proposal to specify logging options as part of tf.ConfigProto given to Session\n", "Closing as duplicate of https://github.com/tensorflow/tensorflow/issues/1258, see comments there.\n", "Adding this just because I get redirected here way too often and still do not find the flag.\r\nTerminal: `export TF_CPP_MIN_LOG_LEVEL=2`\r\nPython: `tf.logging.set_verbosity(tf.logging.ERROR)`"]}, {"number": 565, "title": "Softmax Sampling Support", "body": "I couldn't find all the pieces to do scheduled softmax sampling (Bengio et al, 2015), is that supported in TF atm? Is there an TensorOp that can sample from the previous layer?\n\nThanks!\n", "comments": ["Take a look at this section:\n\nhttps://www.tensorflow.org/versions/master/api_docs/python/nn.html#candidate-samplers\n\nThe sampled softmax loss is here:\n\nhttps://github.com/tensorflow/tensorflow/blob/20723e2b3d58cc48b2a302f7ea9806c8a75fd18f/tensorflow/python/ops/nn.py#L778\n\nFor example used here:\n\nhttps://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/models/rnn/translate/seq2seq_model.py#L98\n", "I think that's a different \"sampled softmax\" ;) , the one I was referring to is to condition on a sample (either from the model, or from the ground truth; see Bengio et al, NIPS 2015), as opposed to the \"sampled softmax\" for estimating the partition function.... very confusing I know : )\n", "FYI, for those looking for a quick op for this, i wrote one here:\nmaster/tensorflow/core/kernels/gru_op.cc\n\nits under \"TokenSampleOp\", lots of code refactoring and generalization and tests needed before a patch can be submitted \"soon\"\n", "@ludimagister, @wchan: From the discussion above it seems like this was a resolved question, especially since #2093 is closed.  Please let me know if we should reopen!\n"]}, {"number": 564, "title": "Batchnorm gammas are all ones in the released Inception V3 model", "body": "Every batchnorm gamma in the inception V3 model (http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz) is identically ones.  Is this a mistake?\n", "comments": ["No. When using ReLUs (or linear) activations, because they are scale-invariant, the batchnorm gamma is redundant with the scale of the weights of the subsequent convolutional or fully-connected layer, so we don't train them.\n"]}, {"number": 563, "title": "Possible bug in seq2seq sequence_loss_by_example", "body": "Looking at the tensorflow/python/ops/seq2seq.py file of the sequence_loss_by_example function:\n\n```\n  if softmax_loss_function is None:\n    # TODO(lukaszkaiser): There is no SparseCrossEntropy in TensorFlow, so\n    # we need to first cast targets into a dense representation, and as\n    # SparseToDense does not accept batched inputs, we need to do this by\n    # re-indexing and re-sizing. When TensorFlow adds SparseCrossEntropy,\n    # rewrite this method.\n    indices = targets[i] + num_decoder_symbols * math_ops.range(batch_size)\n    with ops.device(\"/cpu:0\"):  # Sparse-to-dense must be on CPU for now.\n      dense = sparse_ops.sparse_to_dense(\n          indices, array_ops.expand_dims(length, 0), 1.0,\n          0.0)\n    target = array_ops.reshape(dense, [-1, num_decoder_symbols])\n    crossent = nn_ops.softmax_cross_entropy_with_logits(\n        logits[i], target, name=\"SequenceLoss/CrossEntropy{0}\".format(i))\n```\n\nIsn't this a bug? the softmax_cross_entropy_with_logits doesn't take in the weights as a parameter, and hence if your sequence is heavily padded, it will bias the optimization. however, the inference perplexity is done correctly later on due to the element wise product with weights, however the gradient won't have that information, or am i wrong?\n\nthanks!\n", "comments": ["- is there a zero one loss (for inference?), i can't find anything in the docs or thru grep... =\\\n", "I don't think there is a bug. Indeed, softmax_cross_entropy_with_logits does not take weights, and we're working on a loss function that will take integers directly. But in sequence_loss_by_example we do take weights into account later, in the following line:\n\n``` python\n  log_perp_list.append(crossent * weights[i])\n```\n\nDoes this help, does it look ok to you?\n\nLukasz\n", "so your claim is that because the:\n\n  log_perp_list.append(crossent \\* weights[i])\n\nexists, which scales the cross entropy loss, it will automatically scale the gradients? cool! didn't know TF was so smart, I thought the backprop starts off at the softmax_cross_entropy_with_logits OP.\n\nthanks! : )\n", "Yes - the backprop starts at the loss function you pass to tf.gradients, not anywhere else, so you can just scale like this. Merry Christmas :).\n", "@lukaszkaiser I am using tensorflow r-1.0 and I meet the same problem. When the padding size is big, which means there are many PAD_ID in the output sequence, the model would learned to predict almost all PAD_ID in output. Even I have already set the weights to 0. But when the padding_size is small, this problem disappear. So maybe there is really a bug not easy to find? "]}, {"number": 562, "title": "Install problems", "body": "I try to install tensorflow on Ubuntu 15\nuname -a\nLinux peter-linux 4.2.0-19-generic #23-Ubuntu SMP Wed Nov 11 11:39:30 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\npeter@peter-linux:~/tensorflow$ python\nPython 2.7.10 (default, Oct 14 2015, 16:09:02) \n[GCC 5.2.1 20151010] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > exit()\n> > > peter@peter-linux:~/tensorflow$ set|grep LD_LIBRARY\n> > > LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/lib/python2.7/dist-packages/tensorflow/python/:\n> > > peter@peter-linux:~/tensorflow$ set|grep CUDA\n> > > CUDA_HOME=/usr/local/cuda-7.0\n> > > ll /usr/local/cuda\n> > > lrwxrwxrwx 1 root root 8 nov 14 11:39 /usr/local/cuda -> cuda-7.0/\n> > > peter@peter-linux:~/tensorflow$ ls  /usr/local/cuda/lib64/libcudnn*\n> > > /usr/local/cuda/lib64/libcudnn.so      /usr/local/cuda/lib64/libcudnn.so.6.5.48  /usr/local/cuda/lib64/libcudnn.so.7.0.64\n> > > /usr/local/cuda/lib64/libcudnn.so.6.5  /usr/local/cuda/lib64/libcudnn.so.7.0     /usr/local/cuda/lib64/libcudnn_static.a\n> > > peter@peter-linux:~/tensorflow$ pip -V\n> > > pip 1.5.6 from /usr/lib/python2.7/dist-packages (python 2.7)\n> > > pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n> > > ......\n> > >  warning: no previously-included files matching '_.pyo' found anywhere in distribution\n> > >     warning: no previously-included files matching '_.pyd' found anywhere in distribution\n> > >     changing mode of /home/peter/.local/bin/f2py to 775\n> > > Successfully installed tensorflow six numpy\n> > > Cleaning up...\n> > >  python\n> > > Python 2.7.10 (default, Oct 14 2015, 16:09:02) \n> > > [GCC 5.2.1 20151010] on linux2\n> > > Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n> > > import tensorflow as tf\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"tensorflow/**init**.py\", line 4, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"tensorflow/tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"tensorflow/python/**init**.py\", line 13, in <module>\n> > >     from tensorflow.core.framework.graph_pb2 import *\n> > >   File \"tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n> > >     from google.protobuf import descriptor as _descriptor\n> > > ImportError: No module named protobuf\n\nor as it looks now\nDownloading/unpacking https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n  Downloading tensorflow-0.5.0-cp27-none-linux_x86_64.whl (50.5MB): 50.5MB downloaded\nRequirement already up-to-date: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages/six-1.10.0-py2.7.egg (from tensorflow==0.5.0)\nRequirement already up-to-date: numpy>=1.9.2 in /home/peter/.local/lib/python2.7/site-packages (from tensorflow==0.5.0)\nInstalling collected packages: tensorflow\nSuccessfully installed tensorflow\nCleaning up...\n(tensorflow)peter@peter-linux:~/tensorflow$ python\nPython 2.7.10 (default, Oct 14 2015, 16:09:02) \n[GCC 5.2.1 20151010] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow as tf\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"tensorflow/**init**.py\", line 4, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"tensorflow/tensorflow/python/**init**.py\", line 61, in <module>\n> > >     from tensorflow.python.util import compat\n> > > ImportError: cannot import name compat\n", "comments": ["Can you try installing the version 0.6.0 package and see if this problem persists?\n\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n", "Thanks, but I am giving up. I used to have a problem with my neural network on windows. Now with tensorflow I had one getting to run tensorflow and now my linux won't boot or shutdown anymore after doing some updates. For evenings and weekends I have been trying to revive it, digging through numerous fora and it only sets me back further. Linux is something not to work with to solve a problem, but if you like recompiling your kernel all day long without doing anything productive.\n", "Sorry to hear that! If you're used to running on Windows, I'd suggest trying the Docker packages, which should avoid any complications with your Linux configuration. Here are some [instructions](http://www.netinstructions.com/how-to-install-and-run-tensorflow-on-a-windows-pc/) that might be helpful.\n\nSince it sounds like the problem is related to your particular machine's configuration, I'm going to close this issue for now, but please feel free to reach out if you have other problems.\n"]}, {"number": 561, "title": "from tensorflow.models.rnn import linear causes error in python3", "body": " from tensorflow.models.rnn import linear\n  File \"/Users/Peace/Projects/venvs/allen/lib/python3.5/site-packages/tensorflow/models/rnn/linear.py\", line 25, in <module>\n    linear = tf.nn.linear\nAttributeError: module 'tensorflow.python.ops.nn' has no attribute 'linear'\n\nThis is using the 0.6 wheel in OS X\n", "comments": ["linear is defined in \ntensorflow-0.6.0/tensorflow/python/ops/rnn_cell.py\n", "@ebrevdo: I don't think `tf.nn.linear` is supposed to exist, but `models/rnn/linear.py` uses it.  Is this because we're missing a unit test?\n", "linear is deprecated and shouldn't be used by external users. I'll look\nthrough and see if we use it anywhere but internally in tf.nn.\nOn Feb 22, 2016 9:54 AM, \"Geoffrey Irving\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo: I don't think tf.nn.linear is\n> supposed to exist, but models/rnn/linear.py uses it. Is this because\n> we're missing a unit test?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/561#issuecomment-187292823\n> .\n", "Closing because folks shouldn't be using linear.  But let me know if this breaks any existing models in python3 and i'll reopen.\n", "@ebrevdo: could you please help reopen linear? I am trying to execute the code provided here by Alex (https://github.com/atpaino/deep-text-corrector), but I keep getting this error message: AttributeError: module 'tensorflow.python.ops.rnn_cell' has no attribute '_linear'.\r\n\r\n", "You can find a version of the function _linear in an old version of\ntensorflow (check the older version branches).  Copy and paste it into your\nown codebase and use it there.  We will not be making it public.\n\nOn Thu, Jul 13, 2017 at 11:49 AM, tle4336 <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo>: could you please help reopen\n> linear? I am trying to execute the code provided here by Alex (\n> https://github.com/atpaino/deep-text-corrector), but I keep getting this\n> error message: AttributeError: module 'tensorflow.python.ops.rnn_cell'\n> has no attribute '_linear'.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/561#issuecomment-315168289>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwVrC2RRl_NObvIBBqA8RK66jW3dks5sNmahgaJpZM4G4uY2>\n> .\n>\n"]}, {"number": 560, "title": "Syntax error when building with python3", "body": "i followed the instructions to install Tensorflow from sources. However, before finishing the installation of the wheel with pip3 a SyntaxError: invalid syntax comes up. The error doesn't abort the installation, but when i'm trying to import Tensorflow it shows this error:\n\n```\n >>> import tensorflow as tf\n I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library   libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 67, in <module>\n    from tensorflow.python.training import training as train\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/training.py\", line 141, in      <module>\n    from tensorflow.python.training.coordinator import Coordinator\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/coordinator.py\", line 201\n    except Exception, ex:\n                    ^\nSyntaxError: invalid syntax\n```\n\nwith python 2.7 it shows no such error and it works fine... it seems that the sources are not compatible with python3.4.\n\ni'm running on Ubuntu 14.04 with python 3.4.3 and GCC 4.8.4.\n", "comments": ["Yeah, we still don't yet have good testing for python 3, sadly.  Want to send us a pull request to fix?\n", "Pull request sent and tested running tensorflow/models/image/mnist/convolutional.py on both python2.7 and python3.4 with GPU support. It seems to work but it's my first pull so let me know.\n", "Thanks!  we merged it.\n"]}, {"number": 559, "title": "ImportError: numpy.core.multiarray failed to import", "body": "", "comments": ["When I ran into this, it was because I had another version of numpy installed on my machine in a different location -- I had to uninstall the other (older) version.\n", "thank you!\n", "Did anybody know how to fix it step by step? I faced the same problem (mac ox, python 2.7). And \n\n``` shell\npip install -U numpy\n```\n\ndidn't work. \n\nHere is what I found in my mac. \n- /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python -> numpy 1.8\n- /System/Library/Frameworks/Python.framework/Versions/2.6/Extras/lib/python -> numpy 1.6\n- ls /Library/Python/2.7/site-packages -> numpy 1.10.2 \n\nmy python (default) version 2.7.10 \n\nMany thanks \n", "I removed the older version of numpy on my system using rm.\n\n> > > import numpy as np\n> > > np.**path**\n> > > ['/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy']\n> > > exit()\n> > > sudo rm -rf /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy\n\nAfter this again tested:\n\n> > > import numpy as np\n> > > np.**path**\n> > > ['/usr/local/lib/python2.7/site-packages/numpy']\n> > > np.**version**\n> > > '1.10.1'\n> > > import tensorflow as tf\n\nNo issue after removing older version\n", "@se7en007  thank you so much! problem solved!\n", "@se7en007 I had been looking for an answer for two days. This problem caused me so much stress and frustration, but your answer finally fixed it! For OS X El Capitan users like me, you should first disable System Integrity Protection (rootless) as follows:\n1. Reboot your mac and hold Cmd+R at the startup chime.\n2. Chose Terminal>Utilities from the top menu bar.\n3. Type:\n   `csrutil disable; reboot`\n4. Now remove the system's numpy using the command:\n   `sudo rm -rf /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy`\n\nAnd voil\u00e1, it works.\n", "@wlsherica @ivarvanwooning Welcome.\n", "I encountered the same problem in Ubuntu 15.10. The `bazel build tensorflow_serving/...` says:\n\n```\nERROR: /amy/serving/tensorflow_serving/session_bundle/example/BUILD:34:1: Executing genrule //tensorflow_serving/session_bundle/example:half_plus_two failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nImportError: numpy.core.multiarray failed to import\nTraceback (most recent call last):\n  File \"/home/dp/.cache/bazel/_bazel_dp/3ace02c30acaa55abc93340b423e5df4/serving/bazel-out/host/bin/tensorflow_serving/session_bundle/example/export_half_plus_two.runfiles/tf_serving/tensorflow_serving/session_bundle/example/export_half_plus_two.py\", line 33, in <module>\n    import tensorflow as tf\n  File \"/home/dp/.cache/bazel/_bazel_dp/3ace02c30acaa55abc93340b423e5df4/serving/bazel-out/host/bin/tensorflow_serving/session_bundle/example/export_half_plus_two.runfiles/tf_serving/external/tf/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/dp/.cache/bazel/_bazel_dp/3ace02c30acaa55abc93340b423e5df4/serving/bazel-out/host/bin/tensorflow_serving/session_bundle/example/export_half_plus_two.runfiles/tf_serving/external/tf/tensorflow/python/__init__.py\", line 45, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/home/dp/.cache/bazel/_bazel_dp/3ace02c30acaa55abc93340b423e5df4/serving/bazel-out/host/bin/tensorflow_serving/session_bundle/example/export_half_plus_two.runfiles/tf_serving/external/tf/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/dp/.cache/bazel/_bazel_dp/3ace02c30acaa55abc93340b423e5df4/serving/bazel-out/host/bin/tensorflow_serving/session_bundle/example/export_half_plus_two.runfiles/tf_serving/external/tf/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: numpy.core.multiarray failed to import\n```\n\nI just have a single installation of numpy 1.11 at `/usr/local/lib/python2.7/dist-packages/numpy-1.11.0-py2.7-linux-x86_64.egg/numpy/`. From python terminal I can import the following without any error:\n\n```\nimport tensorflow\nimport numpy.core.multiarray\n```\n\n_Notice_: I installed NumPy from source to link it against OpenBLAS. The binary python is located at `/usr/bin/python' but the numpy is at '/usr/local/python2.7/dist-packages'. Is that the source of trouble?\n\n**Update 1***\nAfter removing the numpy compiled from source with linkage customization to openblas, re-install numpy from Ubuntu distro `sudo apt-get install python-numpy` and rebuild with bazel, the error above disappeared but I get a new mystical error\n\n```\nERROR: /amy/serving/tensorflow_serving/session_bundle/example/BUILD:34:1: Executing genrule //tensorflow_serving/session_bundle/example:half_plus_two failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 245.\n```\n", "I ran into this while building syntaxnet. The root issue was while I had NumPy installed in the virtualenv, I didn't have it installed in my 'base' system, and that's where SWIG was looking for the package.\n\nI did a quick search for ways to get SWIG to respect virtualenvs, but came up with nothing. I'd be interested to know if anyone learns how to fix this! In the meantime, I've just installed NumPy to the base system.\n", "Going back to Ubuntu 14.04 solves my problem. Maybe TF had instability issues with gcc 5.x.\n", "Thanks a lot.\nIt works when i deleted the old version of numpy.\n", "Just a shoutout to this thread! Thanks guys \ud83d\udc6f  OS X El Capitan, rm -rvf numpy path then pip install tensorflow\n", "@ivarvanwooning Thank you very much. It really works!\n", "I had the same problem.  I had installed python with homebrew and thus had two copies \nof numpy.  I just renamed the old version, could erase it also:\n\n$ sudo mv /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/ /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy_bk\n", "My solution is as follow:\n`$ import numpy as np\n$ np.**version** \n'1.8.0rc1'\n$ np.**path**\n['/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy']\n$ exit()\n\n$ sudo rm -rf /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy`\ndelete older version 1.8.0rc1\uff0cnewer version 1.11.2 will be used\n", "i had the same problem with Windows 10, numpy error under Python27 embedded in FreeCAD.   Installing various versions of numpy didn't help (newer versions gave slightly different error message for same issue).\r\n\r\nEventually i have solved it by fixing numpy to include a manifest pointing to required VC90 DLLs:\r\n\r\nProblem:\r\n`python -c \"import numpy\"`\r\n\r\nmake sure this is the 2.7 version of python that comes with FreeCAD, otherwise set it to the front of the PATH with\r\n`set Path=c:\\Program Files\\FreeCAD 0.16\\bin;%PATH%`\r\n\r\ngives error \r\n  \"Importing the multiarray numpy extension module failed.  Most likely you are trying to import a failed build of numpy.\" \r\nwith recent numpy version like 1.11 or 1.12.0rc2\r\nor \"ImportError: DLL load failed: The specified module could not be found.\"\r\n\r\nsolution:\r\nOpen Visual Studio Command Prompt as Administrator\r\n\r\n`cd \"c:\\Program Files\\FreeCAD 0.16\\bin\\Lib\\site-packages\\numpy\"\r\n`\r\nCreate a file called manifest.txt containing:\r\n\r\n`<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\r\n<assembly xmlns=\"urn:schemas-microsoft-com:asm.v1\" manifestVersion=\"1.0\">\r\n  <dependency>\r\n    <dependentAssembly>\r\n      <assemblyIdentity type=\"win32\" name=\"Microsoft.VC90.CRT\" version=\"9.0.30729.9247\" processorArchitecture=\"amd64\" publicKeyToken=\"1fc8b3b9a1e18e3b\"></assemblyIdentity>\r\n    </dependentAssembly>\r\n  </dependency>\r\n</assembly>\r\n`\r\nthen run:\r\n`FOR /r %f IN (*.pyd) DO mt.exe -manifest manifest.txt -outputresource:\"%f\";2\r\n`\r\n\r\nUpdates around 20 pyd file along these lines:\r\n`c:\\Program Files\\FreeCAD 0.16\\bin\\Lib\\site-packages\\numpy>mt.exe -manifest manifest -outputresource:\"c:\\Program Files\\FreeCAD 0.16\\bin\\Lib\\site-packages\\numpy\\core\\multiarray.pyd\";2\r\nMicrosoft (R) Manifest Tool version 6.3.9600.17336\r\nCopyright (c) Microsoft Corporation 2012.\r\nAll rights reserved.\r\n`\r\n\r\nNote this relies on the VC90 DLLs being present in C:\\Windows\\WinSxS\\amd64_microsoft.vc90.crt_1fc8b3b9a1e18e3b_9.0.30729.9247_none_08e394a1a83e212f\r\n\r\n", "$ conda install -c conda-forge numpy", "Just a note for others having this problem:\r\n\r\nIf you're inside a conda/virtualenv, make sure that your global python/conda installation doesn't have an older version inside. Deleting it promptly made keras/tensorflow work correctly in my case.\r\n\r\nFor windows you can find the \"global\" packages in\r\n`%APPDATA%\\..\\Local\\Continuum\\Anaconda\\Lib\\site-packages`", "Neither of these solutions worked for me since I INSTALLED IT WITH SYSTEM SITE PACKAGES. What I did to fix it is to just use mkvirtualenv to create a virtual environment and install tensorflow using ```pip install tensorflow.```", "I did \r\n`conda uninstall numpy`\r\nand then did what @UkiDLucas said\r\n`conda install -c conda-forge numpy`\r\nit works", "I have the same problem, installed numpy 1.13.1 with pip3 on Python 3.6.2\r\n\r\nAnyone know how I can fix it? ", "The same error came for me. The problem is that you might have created a file called numpy.py. This file might coincide with numpy library. So, delete that numpy.py file and problem gets solved ", "@ManojGuha \r\nI have checked through my folder, but there is no file called numpy.py\r\n\r\nI run the script on Jupyter, it somehow didn't give me the error message. So I just work on Jupyter now. ", "Here is the error message that i have when trying to import numpy from ipython. Any help please.\r\n\r\nImportError                               Traceback (most recent call last)\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\__init__.py in <module>()\r\n     15 try:\r\n---> 16     from . import multiarray\r\n     17 except ImportError as exc:\r\n\r\nImportError: cannot import name 'multiarray'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-0aa0b027fcb6> in <module>()\r\n----> 1 import numpy as np\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\__init__.py in <module>()\r\n    140         return loader(*packages, **options)\r\n    141\r\n--> 142     from . import add_newdocs\r\n    143     __all__ = ['add_newdocs',\r\n    144                'ModuleDeprecationWarning',\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\add_newdocs.py in <module>()\r\n     11 from __future__ import division, absolute_import, print_function\r\n     12\r\n---> 13 from numpy.lib import add_newdoc\r\n     14\r\n     15 ###############################################################################\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\__init__.py in <module>()\r\n      6 from numpy.version import version as __version__\r\n      7\r\n----> 8 from .type_check import *\r\n      9 from .index_tricks import *\r\n     10 from .function_base import *\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\type_check.py in <module>()\r\n      9            'common_type']\r\n     10\r\n---> 11 import numpy.core.numeric as _nx\r\n     12 from numpy.core.numeric import asarray, asanyarray, array, isnan, zeros\r\n     13 from .ufunclike import isneginf, isposinf\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\__init__.py in <module>()\r\n     24 Original error was: %s\r\n     25 \"\"\" % (exc,)\r\n---> 26     raise ImportError(msg)\r\n     27\r\n     28 for envkey in env_added:\r\n\r\nImportError:\r\nImporting the multiarray numpy extension module failed.  Most\r\nlikely you are trying to import a failed build of numpy.\r\nIf you're working with a numpy git repo, try `git clean -xdf` (removes all\r\nfiles not under version control).  Otherwise reinstall numpy.\r\n\r\nOriginal error was: cannot import name 'multiarray'\r\n", "I met the same problem. I am using python 3.6.\r\nI solved the problem by\r\n```bash\r\npip install --upgrade numpy\r\n```", "I met the same problem on centos.  The python version is 2.7\uff0cnumpy version is 1.15.0.\r\nI update the numpy version and the problem is solved.\r\n` sudo pip install --upgrade numpy`", "`sudo pip2 install --upgrade numpy` solves the problem.", "Solve with pip:\r\n`pip install --upgrade --force-reinstall numpy`", "I have this problem on windows and solve it by \uff1a\r\n(python35) D:\\Applications>pip install -U numpy", "The solution I've found to resolve the issue was to add the following line after \"import numpy\":\r\n`import numpy.core.multiarray as multiarray`", "I had this issues moments ago on windows 10 and i easily fixed it by continuously typing\r\n`pip uninstall numpy`\r\nin the anaconda3 console in my case, and confirming by pressing \"y\" and hitting enter until it returned that it can't uninstall it as it is not installed (in yellow), that got rid of all the versions for me,\r\nand then i reinstalled the latest version using\r\n`pip install -U numpy`\r\ni'm running python 3.7.1 and it automatically installs the lastest numpy (1.15.4 in my case).", "This may save people a lot of time.\r\n\r\nI fixed the problem via:\r\n\r\n    pip install --force-reinstall --no-cache-dir package-that-fails-importing-numpy\r\n\r\nMy hypothesis is a cached pip package with broken linkage to numpy. It might be caused by a bug in pip, and there might be more ways to fix the breakage.", "I had the same problem and have had similar problem with other modules before. I was running under virtualenv with python3 on Linux Ubuntu 18. First I deactivated my virtualenv for graphos and removed it, created a new and installed the requirements as the example below:\r\n\r\n`$ deactivate graphos`\r\n`$ rmvirtualenv graphos`\r\n`$ mkvirtualenv --python=/usr/bin/python3 graphos`\r\n`$ pip install -r requirements.txt --no-cache-dir`\r\n`$ python manage.py runserver`\r\n", "Uninstalling old numpy works for me. Thank you!", "Make sure you don't have `tempfile.py` somewhere in python path; which prevents import of standard library tempfile module.\r\n\r\nAlso make sure deleting `tempfile.pyc` if there it is.", "> I did\r\n> `conda uninstall numpy`\r\n> and then did what @UkiDLucas said\r\n> `conda install -c conda-forge numpy`\r\n> it works\r\n\r\nThanks a lot.. It worked", "Thanks a lot. It worked", "\r\nwhen i run the code, i gor this error message at the begining\r\n![1](https://user-images.githubusercontent.com/47709276/53696165-77057a00-3dea-11e9-8250-a9a268947d3e.png)\r\nlater it goes to this error state\r\n![untitled](https://user-images.githubusercontent.com/47709276/53696190-b46a0780-3dea-11e9-95ed-48c2eecb783c.png)\r\n\r\n\r\n", "> Hi Jasin.. Just write pip3 uninstall numpy.. It worked for me and the 2nd error is due to reason that numpy's compatible version library is not installed, when numpy will be imported after writing pip3 uninstall numpy, you will recieve no error..\r\n> [\u2026](#)\r\n> On Sun, Mar 3, 2019 at 7:03 PM jasinchandroth ***@***.***> wrote: when i run the code, i gor this error message at the begining [image: 1] <https://user-images.githubusercontent.com/47709276/53696165-77057a00-3dea-11e9-8250-a9a268947d3e.png> later it goes to this error state [image: untitled] <https://user-images.githubusercontent.com/47709276/53696190-b46a0780-3dea-11e9-95ed-48c2eecb783c.png> \u2014 You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#559 (comment)](https://github.com/tensorflow/tensorflow/issues/559#issuecomment-469025603)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/At1-_W6fNxURHuQoD66XRYpm2dMNBPMbks5vS9YigaJpZM4G4q5B> .\r\n\r\n\r\n\r\n> when i run the code, i gor this error message at the begining\r\n> ![1](https://user-images.githubusercontent.com/47709276/53696165-77057a00-3dea-11e9-8250-a9a268947d3e.png)\r\n> later it goes to this error state\r\n> ![untitled](https://user-images.githubusercontent.com/47709276/53696190-b46a0780-3dea-11e9-95ed-48c2eecb783c.png)\r\n\r\nas my python version is 3.7, still the pip3 is not getting recognize in commd prompt\r\nso i am not able to use pip3 uninstall  numpy code, for above problem ", "I solved the issue on Ubuntu by repeatedly uninstalling numpy with pip and doing 'rm' if pip uninstall didn't work. Apparently, there were many versions installed at the same time on my system.", "I had the same problem, and I fixed it by the following:\r\n1.Find the numpy installation package and delete it.\r\n2.pip install python ==1.15.2.\r\n\r\n", "> Yes Michal Gacka this problem happens when during the installation of different packages the numpy's incompatible version (w.rt opencv lib we have) is installed automatically, thus we need to uninstall the incompatible version to make it work.. That's great that your problem is solved.. Sorry for the late response of your 2nd mail..\r\n> [\u2026](#)\r\n> On Wed, Mar 6, 2019 at 5:21 PM Micha\u0142 Gacka ***@***.***> wrote: I solved the issue on Ubuntu by repeatedly uninstalling numpy with pip and doing 'rm' if pip uninstall didn't work. Apparently, there were many versions installed at the same time on my system. \u2014 You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#559 (comment)](https://github.com/tensorflow/tensorflow/issues/559#issuecomment-470086462)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/At1-_T9K-Ko1MuRpAxoikwE7m0adV6aoks5vT7LLgaJpZM4G4q5B> .\r\n\r\n\r\n\r\n\r\n> I solved the issue on Ubuntu by repeatedly uninstalling numpy with pip and doing 'rm' if pip uninstall didn't work. Apparently, there were many versions installed at the same time on my system.\r\n\r\nhi\r\ncan u please give the steps using 'rm' for solving my problem", "numpy==1.15 cause an error, and this error disappear by decrease the numpy version to 1.14.6. it's work for me ", "Kindly check whether you have installed the numpy package from pip. Because if you are running on conda evironment, then all packages need to be downloaded from there.\r\nPlease use the below mentioned statement for this purpose\r\n        conda install -c anaconda numpy\r\n\r\nAlso make sure that the numpy version supports the Python version you are using.", "this error seems to pop up randomly for me. i've had numpy installed and working fine for years and some days it errors out and others it doesn't. usually restarting my computer helps. i have absolutely no idea why.", "We had this error with\r\nnumpy 1.12.1\r\npyarrow 0.15.1\r\nTo solve, we updated our numpy version to 1.18.1.\r\nOur env was built with conda 4.6.14.", "Uninstall all the numpy versions you have installed. I had to repeat the following command like 5 times:\r\n\r\n`pip uninstall numpy`\r\n\r\nThen re-install torch, which will automatically install the correct numpy version as well\r\n\r\n`pip install torch==1.3.0`", "delete all the files in\r\n`C:\\Users\\username\\AppData\\Roaming\\Python\\Python37\\site-packages` \r\nthen everything starts working fine, in anaconda prompt\r\n", "Try This...\r\nRun this Code One by One...\r\n\r\npip uninstall numpy\r\npip install numpy==1.19.3", "@ncernek @avrajsri Do you know why numpy has to be upgrade to 1.18 or 1.19? I'm on numpy 1.14.6 and pyarrow 0.17.0, and I'm getting this error. I don't understand why pyarrow wouldn't be compatible with 1.14", "Hi!\r\ni am using windows 10 operating system, i have installed open cv by following \"pip install opencv-python\" and it is successfully installed\r\nbut when i execute \"import cv2\" on python interpreter it showing following error \r\n\r\n ** On entry to DGEBAL parameter number  3 had an illegal value\r\n ** On entry to DGEHRD  parameter number  2 had an illegal value\r\n ** On entry to DORGHR DORGQR parameter number  2 had an illegal value\r\n ** On entry to DHSEQR parameter number  4 had an illegal value\r\nTraceback (most recent call last):\r\n  File \"C:\\Python\\Python39\\lib\\site-packages\\numpy\\__init__.py\", line 305, in <module>\r\n    _win_os_check()\r\n  File \"C:\\Python\\Python39\\lib\\site-packages\\numpy\\__init__.py\", line 302, in _win_os_check\r\n    raise RuntimeError(msg.format(__file__)) from None\r\nRuntimeError: The current Numpy installation ('C:\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py') fails to pass a sanity check due to a bug in the windows runtime. See this issue for more information: https://tinyurl.com/y3dm3h86\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Python\\Python39\\lib\\site-packages\\cv2\\__init__.py\", line 5, in <module>\r\n    from .cv2 import *\r\nImportError: numpy.core.multiarray failed to import\r\n![image](https://user-images.githubusercontent.com/16851979/103077442-94c05680-45f1-11eb-8bec-a1dd7f850c30.png)\r\n\r\nPlease Help", "> Hi!\r\n> i am using windows 10 operating system, i have installed open cv by following \"pip install opencv-python\" and it is successfully installed\r\n> but when i execute \"import cv2\" on python interpreter it showing following error\r\n> \r\n> ** On entry to DGEBAL parameter number 3 had an illegal value\r\n> ** On entry to DGEHRD parameter number 2 had an illegal value\r\n> ** On entry to DORGHR DORGQR parameter number 2 had an illegal value\r\n> ** On entry to DHSEQR parameter number 4 had an illegal value\r\n> Traceback (most recent call last):\r\n> File \"C:\\Python\\Python39\\lib\\site-packages\\numpy__init__.py\", line 305, in\r\n> _win_os_check() File \"C:\\Python\\Python39\\lib\\site-packages\\numpy__init__.py\", line 302, in _win_os_check raise RuntimeError(msg.format(**file**)) from None RuntimeError: The current Numpy installation ('C:\\Python\\Python39\\lib\\site-packages\\numpy\\**init**.py') fails to pass a sanity check due to a bug in the windows runtime. See this issue for more information: https://tinyurl.com/y3dm3h86 Traceback (most recent call last): File \"\", line 1, in File \"C:\\Python\\Python39\\lib\\site-packages\\cv2__init__.py\", line 5, in\r\n> from .cv2 import *\r\n> ImportError: numpy.core.multiarray failed to import\r\n> ![image](https://user-images.githubusercontent.com/16851979/103077442-94c05680-45f1-11eb-8bec-a1dd7f850c30.png)\r\n> \r\n> Please Help\r\n\r\nRun this Code One by One...\r\n\r\npip uninstall numpy\r\npip install numpy==1.19.3\r\n\r\nThen try to import \"import cv2\" in cmd", "Thansk\r\nit works \ud83d\ude42\r\n\r\n________________________________\r\nFrom: Anand Vijay Rajsri <notifications@github.com>\r\nSent: Sunday, December 27, 2020 11:17 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: asfaz <asfanayaz@hotmail.com>; Comment <comment@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] ImportError: numpy.core.multiarray failed to import (#559)\r\n\r\n\r\nHi!\r\ni am using windows 10 operating system, i have installed open cv by following \"pip install opencv-python\" and it is successfully installed\r\nbut when i execute \"import cv2\" on python interpreter it showing following error\r\n\r\n** On entry to DGEBAL parameter number 3 had an illegal value\r\n** On entry to DGEHRD parameter number 2 had an illegal value\r\n** On entry to DORGHR DORGQR parameter number 2 had an illegal value\r\n** On entry to DHSEQR parameter number 4 had an illegal value\r\nTraceback (most recent call last):\r\nFile \"C:\\Python\\Python39\\lib\\site-packages\\numpy__init__.py\", line 305, in\r\nwin_os_check() File \"C:\\Python\\Python39\\lib\\site-packages\\numpy__init_.py\", line 302, in win_os_check raise RuntimeError(msg.format(file)) from None RuntimeError: The current Numpy installation ('C:\\Python\\Python39\\lib\\site-packages\\numpy*init*.py') fails to pass a sanity check due to a bug in the windows runtime. See this issue for more information: https://tinyurl.com/y3dm3h86 Traceback (most recent call last): File \"\", line 1, in File \"C:\\Python\\Python39\\lib\\site-packages\\cv2__init_.py\", line 5, in\r\nfrom .cv2 import *\r\nImportError: numpy.core.multiarray failed to import\r\n[image]<https://user-images.githubusercontent.com/16851979/103077442-94c05680-45f1-11eb-8bec-a1dd7f850c30.png>\r\n\r\nPlease Help\r\n\r\nRun this Code One by One...\r\n\r\npip uninstall numpy\r\npip install numpy==1.19.3\r\n\r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/559#issuecomment-751499786>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEASICYCGGQOTU2O35VWSPLSW5237ANCNFSM4BXCVZAQ>.\r\n"]}, {"number": 558, "title": "Web pages don't have titles", "body": "None of the web pages at tensorflow.org have titles.\n", "comments": ["https://github.com/tensorflow/tensorflow/pull/575\n", "Putting title annotations in the md like that won't work (unfortunately).\nWe have to fix this here.\nOn Sun, Dec 20, 2015 at 16:44 Rekha Joshi notifications@github.com wrote:\n\n> #575 https://github.com/tensorflow/tensorflow/pull/575\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/558#issuecomment-166168638\n> .\n", "@martinwicke: It looks like the pages have titles now.  Want to mark as fixed? \n", "Fixed.\n"]}, {"number": 557, "title": "Recursively copying elements from one graph to another", "body": "Allows for easy portability of elements (Variables and Ops) from one Tensorflow Graph to another. If called on a top-level root in a dataflow graph, automatically copies all required instances.\n\nProvides an API to retrieve the copied elements in the other graph, using a namespace.\n\nReference: https://codesachin.wordpress.com/2015/11/20/recursively-copying-elements-from-one-graph-to-another-in-tensorflow/\n", "comments": ["Just added the code, haven't changed any imports yet. Want to get general feedback before refining the code.\n", "Hi @srjoglekar246, this looks pretty cool, and thanks for going through the work to put this together!\n\nOne of the ideas we have been working on is a notion of \"Functions\" in the GraphDef, which would allow for re-usable components.  See [here](https://github.com/tensorflow/tensorflow/blob/d6357a5849db980df51d00d8a9ff874cda2faeb3/tensorflow/core/framework/function.proto#L14) for the proto definition.\n\nIt's not ready yet and still needs some work (we may not do this as a proto in the GraphDef) but the underlying idea would be something that would obviate the need for copying elements around with unique names: you could define a subgraph as a function with inputs and outputs, and then re-use them.  So instead of having multiple versions of the graph stamped out at once, each with unique names, you could have one named function that could be called, ported across graphs, etc.\n\nDo you feel that such a feature would accomplish your higher level goals?\n", "Aah yes. The whole idea was to enable reusability of dataflow-structures across Graph instances. If I am not wrong, your solution would need some careful handling of how sessions 'run' these portable functions. But if implemented right, it could save a lot of memory and accomplish what my code intends.\n", "Yeah, there's some more plumbing in the internal execution of graphs that specially handles functions.\n\nFor now, do you mind if we keep this pull request open but on the backburner, at least until we figure out whether functions may be an easier to use abstraction?\n\n(If you really want this checked in somewhere and know lots of others are using this, we've been intending to create a 'contrib' directory or repo where these types of utilities / functions could be placed).\n", "Yeah no problem!\nOn the other hand, a contrib directory for such scripts would be nice - especially for code that might be useful to a good audience as utilities (instead of being inside the main framework).\n", "Can one of the admins verify this patch?\n", "Ping @vrv\n", "We've considered adding a contrib directory but ownership and bug reports would be hard to manage -- probably needs to be a separate repo.  Adding @martinwicke since I think he's in the process of figuring this out.\n\nThere's been more progress over the past few weeks on functions.  I think it's close -- take a look at an example:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/function_test.py#L279  and let us know if that would sort of accomplish what you want\n", "@vrv With the experience of [opecv-contrib](https://github.com/Itseez/opencv_contrib) it is hard to maintain a quality level in a contributed repository and grant a decent review time of that PRs. One of the best community scaling effort is the Debian developer/maintainer process. We can implement a very light version of this here. Github lowered the entrance cost of new developers trough the fork-PR process but also let proliferation of sparse or short time frame contributions. We can find a way to reward middle and long term valid contributors or contributors groups to maintain some contrib modules in Tensorflow and to review and accept related PR and issue on this target modules. I don't know what will be the best way to to this with the actual Github management features for handling process of module Orphaning, contributor/group MIA, new module proposal acceptance and module removal. I think that Wikifing community rules a little bit, using labels, repositories, and submodules the process could be managed in some way.\n", "@vrv A separate repo would be nice, would even let users build a proper codebase for different algorithms implemented in tensorflow. Something like what sklearn is for scipy.\nAs far as the code goes, it pretty much does what I wanted to achieve, with a better interface for reusable functions across graphs. Is it coming out in the next version? The function class would also enable running of different algorithms within same environment - a nice bonus.\n", "@srjoglekar246, @bhack: we'll try to find something that works for contributions.\n\nAs for functions: I'm not entirely sure what the state of it is, but I wanted to solicit early feedback from you since it seems like what you originally were trying to do with this PR.  It's being actively worked on so I'm hoping it will be ready \"soon\".\n", "I guess once we can define reusable functions, we could do away with initialising ops as \"tf.add\", instead going for the method used here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/function_test.py#L99 . My only suggestion would be to make the system smarter with respect to argument types (like adding float types to int types should automatically return a float, assuming shapes are compatible). But I guess it won't be too easy, especially in C++.\n", "I'm afraid that creating a separate repo would lose the focus of the community as a whole.\n\nEdit: For example, I would like all implementations of new papers to have high quality documentation. I'm afraid that the the contrib repo would have a poor doc like some caffe PR has. Maybe if tensorflow has semi-offical implms of some paper this will disincentivize the main repo to make an official and better documented implem earlier than it would otherwise.\n", "Sorry for the long silence -- if you're still interested, I'd like to merge this into contrib. Can you move the file to tensorflow/contrib/copy_graph/python/util/copy.py?\n\nAlso, can you add a license header and the python3 from **future** imports, and can you add a test for this? \n", "Can one of the admins verify this patch?\n", "@martinwicke Will get it done by tomorrow. Any particular format/guideline for the unit test to be added?\n", "Just make sure that it tests the functionality you claim your functions provide.\n", "And can you modify the docstrings to match the tensorflow style guide (look at the \"writing documentation\" howto)?\n\nThanks!\n", "@martinwicke Could you take a look at the code (especially the BUILD and test files) and tell me if I am on the right track? \n", "@martinwicke Made the changes. Let me know if there's anything more to be modified.\n", "Ping @martinwicke \n", "Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "I've had some minor comments about python module things, but otherwise looks good.\n", "@martinwicke I made the changes, and fixed the bazel test errors. They all pass now. Have a look.\n", "Thanks! \n\nJenkins, test this please.\n", "@martinwicke Seems to work. Okay to be pushed in?\n", "Thanks!\n", "Hi @vrv ,\n\nI'm currently writing some high-level library based on tensorflow. I'm relying a lot on copying existing ops to achieve re-usability of data flow structures. So I'm very interested in the \"Functions\" idea you mentioned here. I'm wondering how this is going on.\nBecause I recently met some problems due to not copying op._control_flow_context, which makes gradients through tf.cond fail in copied subgraph. This problem also exists in this contribution.\n", "There's been some work on functions but it's still kind of primitive and I don't know how well it composes with control flow.\n\nWe have https://github.com/tensorflow/tensorflow/blob/5a566a7701381a5cf7f70fce397759483764e482/tensorflow/python/framework/function.py which isn't yet public (but until we seal the public interface it's still available to play around with), and it isn't getting that much love / attention unfortunately.  But if it proves useful, let us know and maybe we can at least make it public at some point.\n", "@vrv Thanks for the link. I had a look at functions and unfortunately that's not what I actually want. I can describe my high-level goals here. It's something like `theano.clone()` (related issues: #5479 #1070), which, in my view, can be seen as operation-level reuse rather than subgraph-level, which enables one to replace inputs of any operations in the graph. I guess this is also what the author of `tf.contrib.graph_editor` tries to achieve.\n"]}, {"number": 556, "title": "Tensorflow installation from local drive", "body": "Hi,\nI am trying to install Tensorflow in windows machine using docker. It's a corporate connection and I am not able to connect to daemon for installation. If there any way where I can download tensorflow in my local drive and execute it from docker?\n\n$ docker run -it b.gcr.io/tensorflow.tensorflow\nUnable to find image 'b.gcr.io/tensorflow.tensorflow:latest' locally\nError response from daemon: unable to ping registry endpoint https://b.gcr.io/v0/\nv2 ping attempt failed with error: Get https://b.gcr.io/v2/: dial tcp 74.125.68.\n82:443: connection refused\n\nPlease help.\n", "comments": ["Does your network has proxy?\n", "refer to [docker issue 868](https://github.com/docker/distribution/issues/868)\n", "Closing this, since it seems related to proxy settings that we can't reproduce. Let us know if you have any more problems.\n"]}, {"number": 555, "title": "typo on np.random.rand()", "body": "", "comments": []}, {"number": 554, "title": "Does the released Inception-v3 model only support forward pass with batch_size=1?", "body": "Hi all,\nI am new to TF. I am playing with Inception-v3 model and I found that I can only pass `batch_size=1` to the model. Here is my test code:\n\n``` python\nimport tensorflow.python.platform\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\nimport numpy as np\n\n\nwith tf.device(\"/cpu:0\"):\n    with gfile.FastGFile('/tmp/imagenet/classify_image_graph_def.pb', 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        _ = tf.import_graph_def(graph_def, name='')\n\nwith tf.Session() as sess:\n\n    array = np.random.rand(1, 299, 299, 3)\n    image_data = tf.convert_to_tensor(array, dtype=tf.float32).eval()\n\n    test_tensor = sess.graph.get_tensor_by_name('Mul:0')\n    print(test_tensor.get_shape())\n    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n    predictions = sess.run(softmax_tensor,\n                          {'Mul:0': image_data})\n```\n\nIf I change `array = np.random.rand(1, 299, 299, 3)` with `batch_size=2`, then TF would complain about the size mismatch.\n\nAm I doing something wrong? Any pointer would be of great help.\n", "comments": ["The classify_image_graph_def.pb is built explicitly to support a batch size of 1, and cannot support other batch sizes. But nice effort.\n\nSherry\n", "Does exist easy way to remove this limitation? \n", "In an already built graph, no. Sorry.\n\nOn Mon, Jan 18, 2016 at 9:17 AM \u0414\u043c\u0438\u0442\u0440\u0438\u0439 notifications@github.com wrote:\n\n> Does exist easy way to remove this limitation?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/554#issuecomment-172595552\n> .\n", "Those, because graph already built, it is impossible to cut out a part of saved graph and connect it with a new in \"inference\" stage?\n", "That would be a major headache. It's probably possible, but I wouldn't\nrecommend it. If any of the variables have to change shape for the\ndifferent batch size, your checkpoints become useless or you have to\nmanually add resize or reshape ops.\n\nOn Tue, Jan 19, 2016 at 9:33 AM \u0414\u043c\u0438\u0442\u0440\u0438\u0439 notifications@github.com wrote:\n\n> Those, because graph already built, it is impossible to cut out a part of\n> saved graph and connect it with a new in \"inference\" stage?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/554#issuecomment-172927407\n> .\n", "I also want to try Inception-V3 model to make a forward pass on images and extract features from intermediate layers. From where can I get the TensorFlow meta-graph file (i.e. classify_image_graph_def.pb)?\nAlso, does it contain pre-trained values for weights or biases or do I need to look for the corresponding checkpoint file (ckpt) in TensorFlow?\nFrom where, if exists, can I get the corresponding pre-trained Inception-V3 model?\n", "@sangramkapre http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\nYou could see https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py", "Hi @dmitryfisko, Have you find a way to solve this problem\uff1f Hmm, batch size =1 cost too much unnecessary time.\r\n@martinwicke Will the batch inference version of this pb model be released? ", "We will not release a new version of this old graph. I would check out tfhub.dev for various alternatives. "]}, {"number": 553, "title": "python: g != graph", "body": "when you create a new op with say an input with name \"g\", the auto-generated python files will conflict with \"g\" because it thinks the \"g\" is the graph passed through rather than the input g due to the name overload.\n", "comments": []}, {"number": 552, "title": "TF 0.6.0 slower than 0.5.0 with the cifar10 example", "body": "With the tensorflow/model/image/cifar10 example. While I run cifar10_train.py, \nIt used to be able to achieve around 400+ examples/sec with my single K40 GPU. Today I pulled the code and installed the latest version (0.6.0x), but it can only achieve 300 examples/sec or less sometimes. \n\nI see Alexnet benchmark performance gets improved, but this actually use case gets worse performance. \n", "comments": ["This issue is for the code at commit 8b5d9ed13f188662dde7cce75362cd2394bde40c. I haven't tried other earlier 0.6.0. but the old 0.5.0 is confirmed faster. \n", "This looks like a duplicate of issue 548: I'll take a look and confirm whether or not this is the case.\n", "I believe this is fixed: I am now getting about 600 samples per second with a GTX Titan. Please reopen if this is still a problem for you.\n"]}, {"number": 551, "title": "Lock contention in thread pool for CPU-based tensor operations", "body": "Running the December 15 Inception model, with either serial or parallel inference steps, is showing serious lock contention on the thread pool lock, with about 50% of the CPU being spent in futex operations:\n\n<img width=\"1296\" alt=\"jeremy_khan____projects_mldb\" src=\"https://cloud.githubusercontent.com/assets/112556/11910933/8f720e80-a5d3-11e5-9a03-33ad7985fe3f.png\">\n\nAs an experiment, replacing the mutex with a basic spinlock reduces latency by around 35% (whilst obviously increasing CPU usage due to all of the CPUs constantly spinning even with nothing to do).  Obviously a pure spinlock is not a viable solution, but it shows that there is some scope for improvement.\n\nThis leads to three related questions:\n1.  Would it be possible to allow the platform to provide its own thread pool implementation or factory?  The ThreadPool methods are virtual, so it would be possible to make it pluggable, except that the class is directly created wherever used.  In our case we have a separate thread pool that has a broadly compatible interface and we'd like to share with TensorFlow.\n2.  Would it make sense to reduce the granularity of the Eigen tensor operation multithreading?\n3.  If not, is there any interest in a lock free thread pool implementation?  Or is this something that somebody is already working on?\n", "comments": ["We are aware of the fact that there are cases where we shard the processing of the Eigen tensors too much, and plan to address this in the future. In the meantime, I'd be very happy to replace the current implementation of the ThreadPool class with a lock free implementation if you can contribute one. It would help speed things up quite a bit.\n", "As of change `ab02c5`, we consider the CPU scaling issue generally resolved, so I'm going to close this bug. Please file new issues with benchmarks and steps to reproduce if you still see any performance issues.\n"]}, {"number": 550, "title": "Translate.py - hard to detect convergence", "body": "In the Sequence to Sequence machine translation example - the training pipeline only outputs perplexity numbers which are hard to interpret. If it could also output some sample translations it would be much easier for people to measure convergence\n\nAnecdotally perplexities of around ~5 result in horrible translations - has anyone gotten it to work, and if so, what perplexities did you need to reach before you started getting reasonable translations?\n", "comments": ["I'm having the same experience - after about 9 hours of training and 15000 steps, the model has reached perplexity ~10, but the predictions are almost all _UNK and even after squashing all those probabilities to 0, the translations are really poor (with a tendency to output just a few random words when translating short sentences).\nAs a silver lining (depending on how you look at it), it does get the example \"Who is the president of the United States?\" right.\n", "@anjishnu I agree and I also would like to recommend showing a few examples translation for convergence or BLUE scores. A log file would be perfect.\n\n@juesato I think that there is a bug somewhere in the tutorial. I tried to make it work with a small subset and different configurations but I never got a plausible translation. I am trying to debug it. Until now no errors on the data and the script until the seq2seq model creation. Continuing...\n\nSee my issue #600\n", "Same here I trained during two days and nothing. Have you solve the problem @marcotrombetti ?\n", "To get reasonable translations you need to train a reasonably large network (say 3 layers with 1K nodes) for at least a few hundred thousand steps. Indeed, the perplexity numbers are just a guide to what's happening, but you can get decodings from the binary as well (or even add your own BLEU score computation using the same function). Did you try to re-run the tutorial setting? That was one decoding I got when training the tutorial before first release:\n\n```\nReading model parameters from /tmp/translate.ckpt-340000\n>  Who is the president of the United States?\n Qui est le pr\u00e9sident des \u00c9tats-Unis \n```\n\nWe should be able to get at least something like this, if not, there might be some new bug to correct. I'll start a run to check it, but it usually takes about a week to get to this point, so please, post here if you get some numbers earlier.\n", "I ran the tutorial model over the weekend + 1 day on a K40. It got to 193K steps (3 layers by 1024) and to reasonable perplexity and samples.\n\n```\nglobal step 193000 learning rate 0.0861 step-time 1.70 perplexity 4.17\n  eval: bucket 0 perplexity 5.02\n  eval: bucket 1 perplexity 3.27\n  eval: bucket 2 perplexity 3.51\n  eval: bucket 3 perplexity 4.82\n\nReading model parameters from /tmp/translate.ckpt-193000\n> Who is the president of the United States?\nQui est le pr\u00e9sident des \u00c9tats-Unis ?\n```\n\nSo I think it works as expected. Better results can be obtained if you tune the model, hopefully also after we close #640 or with more training time. But what it produces looks reasonable to me, so I'm closing this issue. It would be great if someone could confirm what I see -- please re-open if it does not work for you.\n", "I'm thinking we can solve this once and for all by simply printing some sample outputs at every checkpoint so it's easy people to see if it's starting to make sense? This way people can monitor their code without having to come here and find this issue to find out whether the model is working.\n", "I think it's not too hard now, just look for perplexity to go down to around 4, maybe 4.2 -- and the lower the better, esp. on more complex sentences. But indeed, it's a bit of a magic number and some examples might be good to have. Code contributions are welcome, just send your CL!\n", "It would also be helpful to output BLEU scores on the dev data set at each checkpoint.\n\nSomeone should train on the same data as this [Moses baseline](http://www.statmt.org/moses/?n=moses.baseline) and compare the final BLEU scores when decoding the test set (23.5 with Moses). That would be a great comparison point and sanity check; it should also train fast as the data is not that large. (Use the same tokenizer as given there instead of the tutorial tokenizer in data_utils.py). \nI've tried it myself, but a model of size 1024 with 3 layers doesn't fit in my 3GB GPU memory, and I haven't figured out yet how to get reasonable performance with a smaller model.\n", "The basically problem I'm observing is that while the model learns to translate \"Who is the president of the United States?\" it doesn't translate other very basic sentences well. Even though I had perplexity <5, basic translations such as \"What is my name?\" and \"How are you doing today?\" would not be translated well, and often have many OOV tokens (I haven't limited max vocab size at all). My concern is if perplexity isn't a good measure, for example, the model can game the cost function by frequently predicting OOV.\n\nI would post samples, but I'm out of the country without my computer - I'll be able to do this in a week.\n\n@lukaszkaiser could you post a few more sample translations? If your model seems robust, then maybe it's just a peculiarity on my end, but right now, I'm not convinced perplexity <5 means much. \n\nThank you for looking into this, really appreciate it.\n", "One thing that needs to be remembered is that neural translation is a young and active area of reseach -- it's not like there are established ways for everything. Let me comment on two points.\n\n(1) Perplexity vs BLEU. In a number of recent papers on neural translation many researchers report that perplexity correlates well with BLEU scores. Some papers even claim that perplexity correlates better with human judgement of translations than BLEU. But these are all just imperfect measures, sometimes imperfect in different ways. Looking at a few self-chosen samples is also an imperfect measure: the ones you consider might just be more outside of the training data (which comes from newspapers if I understand right), but not really harder. So in the end one needs to look at all of them to get a reasonable measurement and think hard what they mean.\n\n(2) OOV words. This is a well-known problem in neural translation models and a there are ongoing research efforts to deal with it. One way that seems to work well is having multiple UNK tokens and doing some replacements, see this paper: http://arxiv.org/abs/1410.8206 . In a basic model like the one in the tutorial the UNK is heavily over-represented in the training data which leads to bad decodings and a bit \"cheated\" perplexity, as you said. You can try to just decrease UNK probability in the training by using a few different UNKs, or the method from the above paper, or something else entirely.\n\nI'm writing about these points to illustrate one thing: this is an ongoing research effort. We constructed the tutorial to show how to get started with these kinds of models in TensorFlow, but all these research questions remain open. You can now see where the problems are and start working on your own solutions -- that's the intention!\n", "@lukaszkaiser Thanks for the response - I think that's a fair answer, and the explanations of these two challenges is helpful. I'm not sure I'll come back to this project, since there's other projects I'm working on, but I'll try digging deeper into your suggestions if I do.\n\nYour response clarified a lot for me - I'd suggest adding something along those lines to the tutorial page, or at least a mention along the lines of \"neural translation is still quickly improving and models are flawed - this model is only a starting point.\" I think it is easy to get the impression, as I and I'll venture others on this issue, based on the writing of the tutorial and the sample translation, that the model should just work out of the box.\n", "Just for completeness, I ran the queries you suggested through my model.\n\n```\n> What is your name ?\nQuelle est votre nom ?\n> What is my name ?\nQu ' est-ce que mon nom ?\n> How are you doing today ?\nComment _UNK aujourd\u2019hui ?\n```\n\nSo indeed - there is a strange _UNK there, but it's not bad, right? In fact quite funny in some ways - I think everyday queries are really a bit outside of the training data used there.\n", "Hm those results definitely look better than what I remember - I'll take a look back at my model and update this for completeness in a week.\n\nPoint regarding everyday questions absent in the training data is well-taken.\n", "> Code contributions are welcome, just send your CL!\n\nHaha, I'd love to. But given your CLA requirements I expect to be stuck in corporate purgatory for about 6-8 months before being denied permission. :)\n", "We made our process for accepting contributions as quick and painless as we could, and there are already many developers submitting their CLs. I suggest you try, and let me know personally if you have any trouble -- we want to make it as easy as possible for anyone to contribute!\n", "haha, no, I was referring to corporate policies at my employer, not of this project. I've seen the improvement since it began - it's quite impressive! \n", "@lukaszkaiser \nI have the following similar problem, suddenly my perplexity scores shot up from 3 digit numbers to 5+ digit numbers:\n\nhttp://stackoverflow.com/questions/35126954/how-can-i-know-if-the-epoch-point-is-reached-in-seq2seq-model\n", "@aliabbasjp It looks like your dev data is very different from your training data. Maybe there are many unknown words given the vocabulary size you picked. Try running with dev data that is closer to your training data, e.g., split 10% off your current training data and use it as dev data.  \n", "@markusdr No the training data is of the same domain and has similar words and I am not doing a translation but both pairs of sentences are very similar english sentences.\n", "I think the comment about the difference between your dev and train data came from your log output like this:\n\n```\nglobal step 374600 learning rate 0.0069 step-time 1.92 perplexity 1.02\n  eval: bucket 0 perplexity 137268.32\n```\n\nYour training perplexity is 1.02 -- the model is basically perfect on the data it receives for training. But your dev perplexity is enormous, the model does not work at all. How did it look in earlier epochs? I would suspect that there is some mismatch in something. Maybe the tokenization is different for train and dev? Maybe the sizes of the buckets from the original translation model are not appropriate for your use-case? If you share more details, I'll be happy to take a look and try to help (maybe better on the original bug).\n", "Some of the sampel outputs that I am getting are attached \n![sample-translations](https://cloud.githubusercontent.com/assets/5551707/16106680/15101370-33b4-11e6-8cb0-c206469f4141.png).\n\nThe results are not good. \n`global step 46300 learning rate 0.2499 step-time 0.89 perplexity 7.50\neval: bucket 0 perplexity 6.22\neval: bucket 1 perplexity 7.30\neval: bucket 2 perplexity 9.18\neval: bucket 3 perplexity 11.42`\n\nShould I train till perplexity goes further down?  and for the sentences in the development data, I see that for what ever input the translation is _UNK. what could be the reason for this?\n", "Yes, train more. From the seq2seq tutorial: \"one epoch (going through the training data once) takes about 340K steps with batch-size of 64. At this point the model can be used for translating English sentences to French\". You're at step 46K, that's not even 15% of the 340K described! (The UNK problem can be better remedied by improving tokenization, but when you train longer it becomes less of a problem in all cases.)\n", "It is difficult for me to reproduce the results in the tutorial. With all default parameters, my outputs are all _UNK, even at the 335800 steps.\n\n![image](https://cloud.githubusercontent.com/assets/22711053/20027603/8aeb77f2-a2ee-11e6-8009-64951300c8b1.png)\n\nMy perplexity has gone down to 2.15. However, the results are not good.\n\n> global step 335800 learning rate 0.3056 step-time 1.04 perplexity 2.15\n>   eval: bucket 0 perplexity 2.08\n>   eval: bucket 1 perplexity 1.87\n>   eval: bucket 2 perplexity 2.28\n>   eval: bucket 3 perplexity 2.73\n", "This is very strange, it does not happen to me. Also, your perplexity is very low, 2.15 is suspicious. It looks like you're maybe not using all training data, or maybe the data isn't tokenized right? These perplexities look way too low for the default setting.\n", "However, what happened is that I downloaded all the python source code from the tensorflow git project without any modification. The tokenizer is also the original basic_tokenizer in data_utils.py. \n\nI ran the command below as instructed on https://www.tensorflow.org/versions/r0.11/tutorials/seq2seq/index.html. \nI believe I exactly followed all the instructions in the tutorial, but I still cannot reproduce the results. Did I miss anything? \n\n```\npython translate.py\n  --data_dir [your_data_directory] --train_dir [checkpoints_directory]\n  --en_vocab_size=40000 --fr_vocab_size=40000\n```\n", "I have the same problem (all _UNK even after 3+ epochs of training); haven't found a solution yet. I opened a bug report in tensorflow/models where this example code now resides: https://github.com/tensorflow/models/issues/771", "I think that I may have a bug in the data_utils.py:\r\n\r\n`  if not normalize_digits:\r\n    return [vocabulary.get(w.decode(), UNK_ID) for w in words]\r\n  # Normalize digits by 0 before looking words up in the vocabulary.\r\n  return [vocabulary.get(_DIGIT_RE.sub(b\"0\", w).decode(), UNK_ID) for w in words]\r\n`\r\n\r\nw is a binary string and the vocabulary.get() would ALWAYS fail and result in an UNK. Adding w.decode() to convert it to a string seems to work. I am retraining the model right now, I get some gibberish, but at least no UNK. I am only at step 1500 so it will take a while!", "I think I may have found a bug in in data_utils.py:\r\n`  if not normalize_digits:\r\n    return [vocabulary.get(w.decode(), UNK_ID) for w in words]\r\n\r\n  return [vocabulary.get(_DIGIT_RE.sub(b\"0\", w).decode(), UNK_ID) for w in words]`\r\n\r\nUsing python 3.5.2, vocabulary.get(w) does not work when w is a binary string. vocabulary.get(w.decode()) works. I am retraining, at least I do not get the dreaded UNK...\r\n", "@julien888: Yes, I reported the same error and it's already been fixed:\r\nhttps://github.com/tensorflow/models/issues/771#issuecomment-269059667", "Recently got TensorFlow 1.0 and pulled recent copies of tutorial code and data. \r\n\r\nCommand line:\r\npython translate.py --data_dir {DATA_DIR} --train_dir {TRAIN_DIR} --en_vocab_size=40000 --fr_vocab_size=40000\r\n\r\nOld problem with getting mostly _UNK has resolved. However, perplexity is stuck around 9.5:\r\nglobal step 846400 learning rate 0.0001 step-time 0.55 perplexity 9.49\r\n\r\nDecode output bears some resemblance to French, but it's quite off:\r\n> Who is the president of the United States?\r\nLe Conseil de l\u2019Europe a adopt\u00e9 le rapport de M . _UNK .\r\n> The president will return on Thursday.\r\nLe gouvernement du Canada a publi\u00e9 un rapport sur les questions .", "I am very confused about my results. My network has only trained for 56,600 steps but my perplexities are far lower than many that are reported here. For example by bucket perplexities are 3.92, 2.77, 2.97, 3.91 and the total perplexity is something around 2.15\r\n\r\nI started the model training with\r\n\r\npython translate.py\r\n  --data_dir [your_data_directory] --train_dir [checkpoints_directory]\r\n  --en_vocab_size=40000 --fr_vocab_size=40000\r\n\r\nusing tensorflow 1.0 and python 3.5\r\n\r\nAlso when I try to translate things directly from the training set I often get nothing but _UNK's, for example:\r\n\r\n> What is light ?\r\n_UNK _UNK ?\r\n\r\nThe model also seems to really into the words spectrom\u00e9trie and TORONTO\r\n\r\n> Who is the president of the United States ?\r\nLa _UNK _UNK spectrom\u00e9trie _UNK ?\r\n> The hills are alive with the sound of music .\r\nLe _UNK est TORONTO la bonne forme TORONTO la musique .\r\n\r\nThere is also some strange stuff in my vocab files for example \r\n\"The\" appears in the vocab file as \"\u2028\u2028The\" where there are two boxes before it in the atom text editor and then I copy pasted it there there seem to be two invisable character that eat key presses as you try to arrow key over them. The same thing goes for \"the\" but there is only one box", "@lukaszkaiser do you still have the 193K checkpoint file? (/tmp/translate.ckpt-193000).", "Sorry, I don't have it any more.", "@lukaszkaiser No worries. Should i expect to get OK translations using 2 layers (256 units each) with enough training steps? Or is it a waste of time and I should restart training with the default 3 layers (1024 units each)?", "With the old seq2seq code, I'd say 2 layers of 256 will just not work. 3 layers of 1024 should be fine. But you should really check out the google/seq2seq repo (https://github.com/google/seq2seq). It's a newer and much improved version of TensorFlow seq2seq models. It's may look a bit more complex than the tutorial at first, but if you're after quality and better-tuned models, it'll be worth your while :).", "@lukaszkaiser Awesome suggestion thank you!", "Hi all,\r\ni run trainning with translate.py follow to https://www.tensorflow.org/tutorials/seq2seq with 3 layer of 512 units, but perplexity alway increase follow as:\r\nglobal step 104400 learning rate 0.2375 step-time 1.01 perplexity 1.00\r\n  eval: bucket 0 perplexity 52838.74\r\n  eval: bucket 1 perplexity 117227.88\r\n  eval: bucket 2 perplexity 445883.28\r\n  eval: bucket 3 perplexity 323676.55\r\nglobal step 104600 learning rate 0.2375 step-time 1.03 perplexity 1.01\r\n  eval: bucket 0 perplexity 36887.39\r\n  eval: bucket 1 perplexity 826537.03\r\n  eval: bucket 2 perplexity 525378.90\r\n  eval: bucket 3 perplexity 304065.98\r\nglobal step 104800 learning rate 0.2352 step-time 0.95 perplexity 1.00\r\n  eval: bucket 0 perplexity 27844.06\r\n  eval: bucket 1 perplexity 248171.42\r\n  eval: bucket 2 perplexity 550592.36\r\n  eval: bucket 3 perplexity 435554.41\r\nI do not know why? Please help me, I can not bug this problem.\r\n"]}, {"number": 549, "title": "Add a function unsorted_segment_max", "body": "We have unsorted_segment_sum and segment_max , so it seems natural to also provide unsorted_segment_max, which I happen to need.\n", "comments": []}, {"number": 548, "title": "Latest dev release actually slower than 0.5", "body": "After updating TensorFlow to the most recent source yesterday (I'm at b1cabed4e60015602dacd66ea39d419db50c3e1b), I've noticed that while GPU utilization frequently appears much higher in nividia-smi than in prior releases, my actual code is much slower. Some sequence to sequence models I was training began taking 3-4 times as long per step, despite GPU utilization hovering between 60 and 99%, which is much higher than I have observed in the past. As I have code for benchmarking fully connected feedforward networks on MNIST in various frameworks, I dusted that off and, again, slower. Previously, training a network with three hidden layers of 2,048 rectified linear units + dropout (input + hidden) took 1.78 seconds per epoch (averaged over 10 epochs)  when trained using vanilla SGD with momentum and a minibatch size of 256. That is now up to 65.2 seconds. This holds across different combinations of hidden layer sizes and minibatch sizes. On the other hand, convolutional net performance does not seem to be affected as when I run Soumith's convolutional net benchmarks, I get numbers close to what he originally reported using the same test setup.\n\nSo to summarize, I've been recompiling TensorFlow regularly (every few days since its release) and after the most recent compile noticed a quite substantial performance hit for vanilla fully connected feedforward and recurrent architectures, but not for convolutional networks. This is all with TensorFlow running on a Titan X with no other processes running and using the most recent versions of CUDA, cuDNN (well, I have v3 installed, not the release candidate for v4), cuBLAS, etc.\n", "comments": ["@nryant, we will look into this problem. But to make sure we are looking at\nthe same problem, could you provide some more reproducing details? It will\nbe very helpful if you can point us to a model either comes with TF, or\nwritten by you, that is confirmed to be slower on TF-0.6 and faster on\nTF-0.5.\n\nOn Fri, Dec 18, 2015 at 8:10 AM, Martin Wicke notifications@github.com\nwrote:\n\n> Assigned #548 https://github.com/tensorflow/tensorflow/issues/548 to\n> @zheng-xq https://github.com/zheng-xq.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/548#event-495975301.\n", "Over the weekend I can put up a repo on github containing code to reproduce the fully connected model I mentioned. I'll also try downgrading to a previous version to see if I can replicate the timings I recorded a few weeks ago. Would that be sufficient?\n", "That would be very helpful! Thanks.\n\nOn Fri, Dec 18, 2015 at 9:46 AM, nryant notifications@github.com wrote:\n\n> Over the weekend I can put up a repo on github containing code to\n> reproduce the fully connected model I mentioned. I'll also try downgrading\n> to a previous version to see if I can replicate the timings I recorded a\n> few weeks ago. Would that be sufficient?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/548#issuecomment-165850002\n> .\n", "We used to compile with our own version of Eigen. We started using the upstream version of Eigen last week, and it turns out that a couple optimizations are missing. I'll fix the upstream version as soon as possible to get back to where we were performance-wise.\n", "Benoit, is the correct tanh implementation upstream?\nOn Dec 21, 2015 12:17 PM, \"Benoit Steiner\" notifications@github.com wrote:\n\n> We used to compile with our own version of Eigen. We started using the\n> upstream version of Eigen last week, and it turns out that a couple\n> optimizations are missing. I'll fix the upstream version as soon as\n> possible to get back to where we were performance-wise.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/548#issuecomment-166438870\n> .\n", "The promised repo is at \n\nhttps://github.com/nryant/tensorflow_mnist_examples/tree/master\n\nand the benchmark located under the feedforward_bench/ directory therein. When run, feedforward_bench/bench.py will spit out to a file average time to complete an epoch (averaged over ten epochs) over MNIST for a fully connected feedforward model with 3 layers and varying layer sizes and minibatch sizes. I've included output from runs using the official 0.60 release and the more recent release cited upthread (see the .txt files). I've also included log files for each run containing device placement for the operations.\n\nAs you can see, the more recent release is FAR slower, though rather interestingly nvidia-smi shows much higher GPU utilization for it.\n", "is the fix upstream?\n", "I have cheked in several improvements to the upstream eigen repository such as [this one](https://bitbucket.org/eigen/eigen/commits/a0661a2bb16512dc4359c243db1dbeb9bc32b266). I have also updated the TensorFlow codebase to pull a recent version of Eigen that contains these improvements. This should fix the performance regression issue.\n", "As of d1b8333effdcb031e6e34a2835a2f1c877fdd79b, this performance issue appears resolved. See:\n\nhttps://github.com/nryant/tensorflow_mnist_examples/blob/master/feedforward_bench/d1b8333_timings.txt\n", "Thanks for verifying the fixes. I'm closing this issue.\n"]}, {"number": 547, "title": "Tensorflow MaxPool doesn't accept float64", "body": "I am using Tensorflow to train a CNN. I am currently basing my calculations on Float32 which is kind of default at the time of initialising variables.\n\nI guessed that by using float64as my dtype I can get more accurate results so I changed the initilization of my varibales as follows:\n\n`w1 = tf.Variable(tf.random_normal([5, 5, 3, 64], stddev=0.1, dtype=tf.float64))`\nBut I get following error in the maxpooloperation: I chacked the maxpooldocumentation and it accept value type as follow: `value: A 4-D Tensor with shape [batch, height, width, channels] and type float32, float64, qint8, quint8, qint32.`\n\nBut I get the following error. Is this a bug or I'm doing something wrong?\n\n`Input 'input' of 'MaxPool' Op has type float64 that does not match expected type of float32.`\n\nFull trace:\n\n``` python\nTraceback (most recent call last):\n  File \"/Users/hamedketabdar/LearningTensorFlow/CIFAR-Khodam/convolutional_network_batch_2d2c_clean_64f.py\", line 213, in <module>\n    pred = conv_net(x, weights, biases, keep_prob)\n  File \"/Users/hamedketabdar/LearningTensorFlow/CIFAR-Khodam/convolutional_network_batch_2d2c_clean_64f.py\", line 153, in conv_net\n    conv1 = max_pool(conv1, k=2) # Normally K=2\n  File \"/Users/hamedketabdar/LearningTensorFlow/CIFAR-Khodam/convolutional_network_batch_2d2c_clean_64f.py\", line 135, in max_pool\n    return tf.nn.max_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 235, in max_pool\n    name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 449, in _max_pool\n    strides=strides, padding=padding, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 430, in apply_op\n    (prefix, dtypes.as_dtype(input_arg.type).name))\nTypeError: Input 'input' of 'MaxPool' Op has type float64 that does not match expected type of float32.\n```\n\nstackoverflow link: http://stackoverflow.com/questions/34354126/tensorflow-maxpool-doesnt-accept-float64\n", "comments": ["It looks like the docs are wrong, in this case - the [registration](https://github.com/tensorflow/tensorflow/blob/a0f880c3432462f99c1ba046f0c4193675e7013f/tensorflow/core/ops/nn_ops.cc#L318) for the MaxPool op actually only accepts single-precision floats.\n", "Thanks, don't you think using float64 can enhance the overall performance? if yes, why not adding support for float64?\n", "The current implementation is pretty specialized to `float32`, so it wouldn't be completely trivial to switch it over. However, we'd be glad to take contributions on this if it would be useful.\n", "Hmm, tried briefly to do this, but hit the lack of `atomicAdd` on GPU for double.\n", "Stumbled upon some more errors in the documentation for AvgPool3D and MaxPool3D. Documentation says that these ops can be used with many tensor types, as can be seen [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/tf.nn.avg_pool3d.md) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/tf.nn.max_pool3d.md). But they are registered only for float32 as can be seen [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/pooling_ops_3d.cc#L124). Will send out a PR soon.\n", "@siddharth-agrawal: That would be awesome!  You'll likely need atomic add for doubles, but it's available now in `util/cuda_kernel_helper.h` as `CudaAtomicAdd`. \n", "@girving Oops. I think I didn't express myself right. I meant PR for the documentation errors. However, I found this while digging for the solution to this issue. So hopefully the PR you meant will come soon too. :)\n", "@siddharth-agrawal: If the documentation change requires changing the list of dtypes passed to `REGISTER_OP`, you'll break the backwards compatibility tests when doing so (those tests don't notice the lack of kernels).  If the double kernels are coming soon, it may be better to wait.  However, it's also possible (likely?) that I'm misunderstanding the changes, so please tell me if the previous makes no sense. \n", "@girving I was thinking of modifying the first two files I mentioned in my first comment on the thread and `api_docs/python/nn.md`. Do you mean the list passed in `core/ops/ops.pbtxt`? Is that used for registering the op? Also, could you tell me where the backward compatibility tests are? (sorry, newbie here)\n", "`api_docs/python/nn.md` is autogenerated from code: you can't modify it directly.  Specifically, it's autogenerated from `core/ops/nn_ops.cc`, `python/ops/nn.py`, and `python/ops/nn_ops.py`.\n", "Alright. I was looking at [this commit](https://github.com/tensorflow/tensorflow/commit/aa7c2472a70a26f4e57508b788eb9791abdbaee1), guess things have changed.\n", "@siddharth-agrawal: They're autogenerated but also checked in.  That commit also changed the code from which it was autogenerated (scroll down). \n", "@girving I ran into a problem while trying to solve this, let me know if the following makes sense.\n\nTo register `MaxPoolingGradOp` on the GPU for float64 we need to have `template class DnnPoolingGradOp<double>` in `kernels/pooling_ops_common.cc` as `MaxPoolingGradOp`'s `Compute()` uses it. But adding this gives a build error with the following message:\n`error: no matching function for call to 'perftools::gputools::Stream::ThenPoolForward(perftools::gputools::dnn::PoolingDescriptor&, perftools::gputools::dnn::BatchDescriptor&, perftools::gputools::DeviceMemory<double>&, perftools::gputools::dnn::BatchDescriptor&, perftools::gputools::DeviceMemory<double>*)'`\n\nThis feels like a dead end to me, as cudnn has come into the picture. Is there a way around this?\n", "It's surmountable, but I don't think we can accept pull requests that change the necessary code at the moment due to silly code organizational reasons.  @zheng-xq: I am I right that stream executor changes are difficult to accept from outside?\n", "Ah, okay. I will find something else to work on. Thanks for all the help.\n", "Here are the ops that are missing double versions.  I believe the only difficult ones to add are the max pool ops, as described in #547.  Most of them are very neural net specific, which is probably why they haven't been generalized yet: neural net specific applications rarely need double precision.  We'd be happy to accept PRs though!\n\n```\nAdjustContrastv2\nAllCandidateSampler\nAudioSummary\nCTCBeamSearchDecoder\nCTCGreedyDecoder\nCTCLoss\nComputeAccidentalHits\nDrawBoundingBoxes\nEditDistance\nExtractGlimpse\nFixedUnigramCandidateSampler\nHSVToRGB\nImageSummary\nInTopK\nLRN\nLRNGrad\nLearnedUnigramCandidateSampler\nLogUniformCandidateSampler\nMaxPool\nMaxPoolGrad\nMaxPoolGradWithArgmax\nMaxPoolWithArgmax\nNegTrain\nRGBToHSV\nSampleDistortedBoundingBox\nSparseMatMul\nStringToNumber\nTensorArrayConcat\nTensorArrayGrad\nTensorArrayPack\nTensorArrayRead\nTensorArraySize\nTensorArraySplit\nTensorArrayUnpack\nTensorArrayWrite\nThreadUnsafeUnigramCandidateSampler\nUniformCandidateSampler\n```\n", "Automatically closing due to lack of recent activity.", "what happen to this issue? it is still not fixed as fare as i can see"]}]