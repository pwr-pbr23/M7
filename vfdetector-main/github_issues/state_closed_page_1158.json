[{"number": 18465, "title": "Fix shape validation error with tf.nn.conv3d_transpose", "body": "This fix tries to address the issue raised in #18460. In `tf.nn.conv3d_transpose` when list or np array is passed, the validate of the output shape with filter shape uses `output_shape[4]` (channel). This will not work with `data_format='NCDHW'`.\r\n\r\nThis fix fixes the issue by replace `output_shape[4]` with `output_shape[axis]`.\r\n\r\nThis fix also adds a test case. Before this fix, the test case will fail.\r\n\r\nThis fix fixes #18460.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18464, "title": "Branch 192600256", "body": "Manually merged tensorflow/python/framework/dtypes_test.py ", "comments": ["@yifeif, seems that the python3 pip test is consistently breaking. From the history, it seems that cl/192504411 is the potential cause of it, and also the python3 pip test is not covered in presubmit for internal change.", "Left a comment on the CL. Yea we run a pip smoke test that detects dependency issues, but it doesn't run the tests.  @gunan do you think it's the time we run pip tests internally?", "Seems that the test is still broken, jsimsa@ is doing rollback for his change. Will do another pull once the rollback is done."]}, {"number": 18463, "title": "Op type not registered 'GatherV2' error when using libtensorflow maven artifacts", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: maven\r\n- **TensorFlow version (use command below)**: 1.7.0-rc1\r\n- **Java version**: 1.8.0_162\r\n- **GPU model and memory**: Tesla K80\r\n\r\n### Describe the problem\r\nWhen attempting to use the **libtensorflow** maven artifacts mentioned in the [GPU Support for Java](https://www.tensorflow.org/versions/master/install/install_java#gpu_support) page to load an exported graph, I received the error:\r\n\r\n```\r\nException in thread \"main\" org.tensorflow.TensorFlowException: Op type not registered 'GatherV2'\r\n\tat org.tensorflow.SavedModelBundle.load(Native Method)\r\n\tat org.tensorflow.SavedModelBundle.load(SavedModelBundle.java:39)\r\n\tat net.imagej.ops.experiments.filter.deconvolve.FlowdecGraph.load(FlowdecGraph.java:38)\r\n```\r\n\r\nPlease let me know if any other information would help, but the error seems pretty informative so I just wanted to check and see if support for newer operations like \"GatherV2\" were likely to make it into the java artifacts in the near future.\r\n\r\nIt may also help to note that the same code I was running works just fine when using either python or the non-gpu-enabled java artifact like this:\r\n\r\n```\r\n<dependency>\r\n         <groupId>org.tensorflow</groupId>\r\n         <artifactId>tensorflow</artifactId>\r\n         <version>1.7.0-rc1</version>\r\n</dependency>\r\n```\r\n\r\nBut switching that to these dependencies results in the error (with no other changes):\r\n\r\n```\r\n<dependency>\r\n\t<groupId>org.tensorflow</groupId>\r\n\t<artifactId>libtensorflow</artifactId>\r\n\t<version>1.7.0-rc1</version>\r\n</dependency>\r\n<dependency>\r\n\t<groupId>org.tensorflow</groupId>\r\n\t<artifactId>libtensorflow_jni_gpu</artifactId>\r\n\t<version>1.7.0-rc1</version>\r\n</dependency>\r\n```\r\n\r\nThanks for the help!\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nExact command to reproduce", "**Bazel version**: N/A\r\n**CUDA/cuDNN version**: N/A (This error occurs with and without a GPU and I see no way that CUDA lib versions apply -- I think it's solely related to missing ops in the java API)\r\n**Exact command to reproduce**:\r\n\r\n```python\r\n\r\n# Generate and export a graph with a \"GatherV2\" op in it\r\n\r\nimport tempfile\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntmp = tempfile.mkdtemp()\r\n!rm -rf $tmp\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(tmp)\r\n\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    x = tf.placeholder(tf.float32, shape=[5, 5])\r\n    y = tf.gather(x, tf.range(0, 5), axis=0)\r\n\r\n   # Running this a second time is what puts GatherV2 into the graph (does not happen w/o this line)\r\n    y = tf.gather(y, tf.range(0, 5), axis=1)\r\n    \r\nwith tf.Session(graph=g) as sess:\r\n    signature = tf.saved_model.signature_def_utils.build_signature_def(\r\n        inputs={'x': tf.saved_model.utils.build_tensor_info(x)},\r\n        outputs = {'y': tf.saved_model.utils.build_tensor_info(y)}\r\n    )\r\n\r\n    builder.add_meta_graph_and_variables(\r\n        sess, [tf.saved_model.tag_constants.SERVING],\r\n        signature_def_map={tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature}\r\n    )\r\n    \r\n    xv = np.arange(25).reshape(5, 5)\r\n    yv = sess.run(y, {x: xv})\r\n        \r\nbuilder.save(as_text=True)\r\n\r\n> INFO:tensorflow:SavedModel written to: b'/var/folders/vc/j01b02990c7g90mw6v_dqpsw0000gn/T/tmpx0od_epb/saved_model.pbtxt'\r\n```\r\n\r\nThen when importing this into java with the following maven dependencies:\r\n\r\n```xml\r\n<dependency>\r\n\t<groupId>org.tensorflow</groupId>\r\n\t<artifactId>libtensorflow</artifactId>\r\n\t<version>1.7.0</version>\r\n</dependency>\r\n<dependency>\r\n\t<groupId>org.tensorflow</groupId>\r\n\t<artifactId>libtensorflow_jni_gpu</artifactId>\r\n\t<version>1.7.0</version>\r\n</dependency> \r\n<dependency>\r\n\t<groupId>org.tensorflow</groupId>\r\n\t<artifactId>proto</artifactId>\r\n\t<version>1.7.0</version>\r\n</dependency>\r\n```\r\n\r\n```java\r\npublic class TFDebug {\r\n\tpublic static void main(String[] args) throws InvalidProtocolBufferException {\r\n\t\tString path = \"/var/folders/vc/j01b02990c7g90mw6v_dqpsw0000gn/T/tmpx0od_epb/\";\r\n\t\tSavedModelBundle model = SavedModelBundle.load(path, \"serve\");\r\n\t\tMetaGraphDef metaGraph = MetaGraphDef.parseFrom(model.metaGraphDef());\r\n\t}\r\n}\r\n// Running this results in:\r\nException in thread \"main\" org.tensorflow.TensorFlowException: Op type not registered 'GatherV2'\r\n\tat org.tensorflow.SavedModelBundle.load(Native Method)\r\n\tat org.tensorflow.SavedModelBundle.load(SavedModelBundle.java:39)\r\n\tat net.imagej.ops.experiments.filter.deconvolve.TFDebug.main(TFDebug.java:12)\r\n```\r\n\r\n", "@eric-czech : Is there more text around the exception which would provide more details of the error that you can share?\r\n\r\nThe Java API and the Python API have the exact same set of operations compiled in (they both depend on the same underlying native libraries), so it's not as if `GatherV2` should be excluded from the Java API (though some operations invoked by `tf.contrib` operations in Python may have to be explicitly loaded - see this, but `GatherV2` is decidedly a \"core\" operation).\r\n\r\nFWIW, I was unable to reproduce the problem. I used the Python script your provided to generate the saved model and then used the following `TFDebug.java`:\r\n\r\n```java\r\nimport java.util.Arrays;\r\nimport org.tensorflow.*;\r\nimport org.tensorflow.framework.MetaGraphDef;\r\n\r\npublic class TFDebug {\r\n  public static void main(String[] args) throws Exception {\r\n    String path = \"./saved_model\";\r\n    try (SavedModelBundle model = SavedModelBundle.load(path, \"serve\");\r\n        Tensor<Float> input = Tensors.create(new float[5][5])) {\r\n      try (Tensor<Float> output =\r\n          model\r\n              .session()\r\n              .runner()\r\n              .feed(\"Placeholder\", input)\r\n              .fetch(\"GatherV2\")\r\n              .run()\r\n              .get(0)\r\n              .expect(Float.class)) {\r\n        System.out.println(Arrays.deepToString(output.copyTo(new float[5][5])));\r\n      }\r\n      MetaGraphDef metaGraph = MetaGraphDef.parseFrom(model.metaGraphDef());\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWith the following `pom.xml`:\r\n\r\n```xml\r\n<project>\r\n    <modelVersion>4.0.0</modelVersion>\r\n    <groupId>org.myorg</groupId>\r\n    <artifactId>issue18463</artifactId>\r\n    <version>1.0-SNAPSHOT</version>\r\n    <properties>\r\n      <exec.mainClass>TFDebug</exec.mainClass>\r\n      <maven.compiler.source>1.7</maven.compiler.source>\r\n      <maven.compiler.target>1.7</maven.compiler.target>\r\n    </properties>\r\n    <dependencies>\r\n      <dependency>\r\n        <groupId>org.tensorflow</groupId>\r\n        <artifactId>libtensorflow</artifactId>\r\n        <version>1.7.0</version>\r\n      </dependency>\r\n      <dependency>\r\n        <groupId>org.tensorflow</groupId>\r\n        <artifactId>libtensorflow_jni_gpu</artifactId>\r\n        <version>1.7.0</version>\r\n      </dependency>\r\n      <dependency>\r\n        <groupId>org.tensorflow</groupId>\r\n        <artifactId>proto</artifactId>\r\n        <version>1.7.0</version>\r\n      </dependency>\r\n    </dependencies>\r\n</project>\r\n```\r\n\r\nI ran this with `mvn -q clean compile exec:java` and got the following:\r\n\r\n```\r\nmvn -q clean compile exec:java\r\n2018-04-14 02:00:37.765349: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-04-14 02:00:37.766017: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: ./saved_model\r\n2018-04-14 02:00:37.979333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-04-14 02:00:37.979768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\r\n2018-04-14 02:00:38.093554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-04-14 02:00:38.093843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: \r\nname: Quadro K620 major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 1.93GiB freeMemory: 759.38MiB\r\n2018-04-14 02:00:38.093887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1408] Ignoring visible gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0, compute capability: 5.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\r\n2018-04-14 02:00:38.093905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-14 02:00:38.545211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-14 02:00:38.545246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 \r\n2018-04-14 02:00:38.545254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N N \r\n2018-04-14 02:00:38.545259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   N N \r\n2018-04-14 02:00:38.545512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11374 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-04-14 02:00:38.663901: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.\r\n2018-04-14 02:00:38.663960: I tensorflow/cc/saved_model/loader.cc:171] The specified SavedModel has no variables; no checkpoints were restored.\r\n2018-04-14 02:00:38.663971: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.\r\n2018-04-14 02:00:38.664041: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 898045 microseconds.\r\n2018-04-14 02:00:38.669242: E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered\r\n[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]\r\n```\r\n\r\nWhat results do you get from running the same? ", "Hi @asimshankar ,\r\n\r\nSorry to have wasted your time on this but just knowing that it was working for you forced me to go back and consider other possibilities which ultimately led me to find that through a transitive dependency, an older version of the tensorflow artifacts was getting pulled into my maven build.  I excluded those and it works just as it did for you.\r\n\r\nThanks for the help though!  And apologies again for the false positive issue."]}, {"number": 18462, "title": "Add shape validation for tag input of StatsDataset", "body": "The tag field of the StatsDataset needs to be a scalar. However, there was no check in the shape function. This fix adds the check of the tag shape.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18461, "title": "Replace raw_input/input with six.moves.input for python 2/3", "body": "This fix is an enhancement to replace raw_input/input in python 2 and 3 with six.moves.input, which works in both python 2 and python 3.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18460, "title": "tf.nn.conv3d_transpose operation with data_format='NCDHW'", "body": "### System information\r\nIrrelevant.\r\n\r\n### Describe the problem\r\nIn implementation of \"tf.nn.conv3d_transpose\" operation with \"data_format='NCDHW'\",\r\nthe shape compatibility of \"output shape\" and \"kernel size\" check is not carried out correctly. (However, the check is fine in tf.nn.conv2d_transpose)\r\n\r\n\r\n### Source code / logs\r\n`    if isinstance(output_shape, (list, np.ndarray)):\r\n      # output_shape's shape should be == [5] if reached this point.\r\n      if not filter.get_shape()[3].is_compatible_with(output_shape[4]):\r\n        raise ValueError(\r\n            \"output_shape does not match filter's output channels, \"\r\n            \"{} != {}\".format(output_shape[4],\r\n                              filter.get_shape()[3]))`\r\n\r\nshould be changed to \r\n\r\n`    if isinstance(output_shape, (list, np.ndarray)):\r\n      # output_shape's shape should be == [5] if reached this point.\r\n      if not filter.get_shape()[3].is_compatible_with(output_shape[axis]):\r\n        raise ValueError(\r\n            \"output_shape does not match filter's output channels, \"\r\n            \"{} != {}\".format(output_shape[axis],\r\n                              filter.get_shape()[3]))`\r\n\r\nmore specifically, output_shape[4] to output_shape[axis].", "comments": ["Thanks @d-acharya. Created a PR #18465 and add a test case to address the issue."]}, {"number": 18459, "title": "crop_and_resize on gpu in eager mode fails", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nusing pip\r\n- **TensorFlow version (use command below)**:\r\n('v1.7.0-3-g024aecf414', '1.7.0')\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCuda 9, Cudnn 7\r\n- **GPU model and memory**:\r\nGeForce GTX 745, 4GB\r\n- **Exact command to reproduce**:\r\nimport tensorflow as tf, numpy as np\r\ntf.enable_eager_execution()\r\nimages = np.random.uniform(size=[1, 28, 28, 1]).astype('float32')\r\nwith tf.device('gpu:0'):\r\n  tf.image.crop_and_resize(images, boxes=[(0, 0, 0.9, 0.5)], box_ind=[0], crop_size=[20, 20])\r\n\r\n### Describe the problem\r\nWhen using `tf.image.crop_and_resize` in eager mode with the gpu, getting the error that says `box_index` has wrong values, where values are actually fine. \r\n\r\nSee the reproduction code above. Works without the gpu, or without eager mode. \r\nLooks similar to [https://github.com/tensorflow/tensorflow/issues/10618](url)\r\n\r\nError message:\r\n2018-04-12 14:44:57.137104: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-04-12 14:44:57.542554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-04-12 14:44:57.542970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: GeForce GTX 745 major: 5 minor: 0 memoryClockRate(GHz): 1.0325\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.92GiB\r\n2018-04-12 14:44:57.542987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-12 14:44:57.688951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-12 14:44:57.688973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \r\n2018-04-12 14:44:57.688982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \r\n2018-04-12 14:44:57.689139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3713 MB memory) -> physical GPU (device: 0, name: GeForce GTX 745, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_image_ops.py\", line 368, in crop_and_resize\r\n    extrapolation_value=extrapolation_value, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_image_ops.py\", line 396, in crop_and_resize_eager_fallback\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"/usr/local/lib/python2.7/dist-packages/six.py\", line 737, in raise_from\r\n    raise value\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: box_index has values outside [0, batch_size) [Op:CropAndResize]\r\n", "comments": ["I tried reproducing the error but couldn't do so.\r\n\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.7.0'\r\n>>> import numpy as np\r\n>>> tf.enable_eager_execution()\r\n>>> images = np.random.uniform(size=[1, 28, 28, 1]).astype('float32')\r\n>>> with tf.device('/gpu:0'):\r\n...     tf.image.crop_and_resize(images, boxes=[(0, 0, 0.9, 0.5)], box_ind=[0], crop_size=[20, 20])\r\n<tf.Tensor: id=4, shape=(1, 20, 20, 1), dtype=float32, numpy=\r\narray([[[[0.23771048],\r\n         [0.3173547 ],\r\n         [0.55291235],\r\n         [0.7272838 ],\r\n         [0.16079727],\r\n         [0.5161821 ],\r\n         [0.90516174],\r\n         [0.9034853 ],\r\n         [0.7911546 ],\r\n         [0.7401962 ],\r\n         [0.68701637],\r\n         [0.3191714 ],\r\n         [0.62902766],\r\n         [0.774474  ],\r\n         [0.11670986],\r\n         [0.4090613 ],\r\n         [0.73830366],\r\n         [0.9687719 ],\r\n         [0.69964784],\r\n         [0.5468331 ]],\r\n........", "It has been 35 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18458, "title": "Fix compilation of split with SYCL enabled", "body": "ERROR: /home/tomeu/opencl-prefix/source/tensorflow/tensorflow/core/kernels/BUILD:436:1: C++ compilation of rule '//tensorflow/core/kernels:split_lib' failed (Exit 1)\r\nIn file included from tensorflow/core/kernels/split_lib_cpu.cc:18:0:\r\n./tensorflow/core/kernels/split_lib.h:53:34: error: wrong number of template arguments (2, should be 3)\r\n struct Split<Eigen::SyclDevice, T> {\r\n                                  ^\r\n./tensorflow/core/kernels/split_lib.h:35:8: note: provided for \u2018template<class Device, class T, int NDims> struct tensorflow::functor::Split\u2019\r\n struct Split {\r\n        ^~~~~\r\n\r\nSigned-off-by: Tomeu Vizoso <tomeu.vizoso@collabora.com>\r\nFixes: 93f5dd54dab1 (\"Optimized non-aligned case of split and split_v on the first input dimension.\")", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Not sure what to do regarding the CLA, as tomeu.vizoso@collabora.com is part of my company's CLA group, and is also associated to this github account. This is the first time I contribute to a Google project from GitHub, but I have contributed to other projects via other means.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you try verifying the e-mail address is correct and sending a new pull request?"]}, {"number": 18457, "title": "Could change Eigen to other blas for matrix  operation", "body": "As for CPU Tensorflow, matrix operation use `eigen`. Could replace it with other blases, such as openBlas, atlas,  gotoBlas et al.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS :ubuntu\r\nTensorFlow installed from TF github\r\nTensorFlow version:1.2.0-rc1\r\nBazel version:0.5.0\r\nCUDA/cuDNN version: NO\r\nGPU model and memory: NO\r\nI just install CPU Tensorflow. Now, i want to test the performance of using other blases  for matrix operation instead of eigen.  I don't how to do. Could you please give me some ideas?", "For experimentation you may just want to change the kernel implementation to use another library. For example, change the [`MatMul` kernel](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/kernels/matmul_op.cc) implementation to use another library.\r\n\r\nThe [\"Adding an op\"](https://www.tensorflow.org/extend/adding_an_op) guide would be a useful read to provide background on ops and kernels.\r\n\r\nIf you get more sophisticated, you could try things like what is done for Intel MKL where [a graph rewrite pass](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/graph/mkl_layout_pass.cc#L49) updates the nodes in the graph so that MKL-enabled kernels are used.\r\n\r\nI don't believe that we (TensorFlow team) has any in-progress efforts for writing kernel implementations using the libraries you mentioned, but @tatianashp would have a more authoritative answer. \r\n\r\nHope that helps.\r\n\r\n(Closing this since this isn't a bug or feature request. Feel free to reopen if I misunderstood)", "`Adding an op` and `MKL-enable`  may be great. I will try.  Thanks for your attention. @asimshankar ", "We don't have  efforts adding support for BLAS libraries other than Eigen and MKL. [CPU performance guide](https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu)  gives instructions on how to build TensorFlow with MKL. It might be better to use the most recent released version of TensorFlow. 1.2 is quite old.", "ok. Thanks.", "We are trying to replace Eigen with our own modified BLAS on TensorFlow for CPU to use it for the matrix multiplication. Any ideas to solve this will be helpful. Thanks in advance.", "is there any progress about using BLAS on Tensorflow? @ruthmd "]}, {"number": 18456, "title": "Tensorflow failed when build with MSVC", "body": " System information\r\n\u2022\tHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nN/A\r\n\u2022\tOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows server 2016\r\n\u2022\tTensorFlow installed from (source or binary):\r\nSource\r\n\u2022\tTensorFlow version (use command below):\r\nMaster branch latest revison\r\n\u2022\tPython version:\r\nAnaconda 4.1.1 (Python 3.5 64-bit)\r\n\u2022\tBazel version (if compiling from source):\r\nN/A\r\n\u2022\tGCC/Compiler version (if compiling from source):\r\nVS2017 15.5.7\r\n\u2022\tCUDA/cuDNN version:\r\nNVidia CUDA Toolkit 8.0\r\nNVidia CUDNN 5.1\r\n\u2022\tGPU model and memory:\r\nN/A\r\n\u2022\tExact command to reproduce:\r\nN/A\r\n\r\n**Describe the problem:**\r\nTensorflow failed to build due to the error: No module named 'google'. This issue can be reproduced from master revision [90a3db9.](https://github.com/tensorflow/tensorflow/commit/90a3db9ff995634314227f0aacf4984d1eee752a) This should be tensorflow source issue, could you please help take a look at this? Thanks!\r\n\r\n**The failures like:**\r\nThe whole log file please see attachment.\r\n[log_x64_build.log](https://github.com/tensorflow/tensorflow/files/1902589/log_x64_build.log)\r\n\r\n 520>CustomBuild:\r\n         Traceback (most recent call last):\r\n           File \"D:/TensorFlow/build_x64/tf_python/tensorflow/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n             from tensorflow.python.util import tf_decorator\r\n           File \"D:\\TensorFlow\\build_x64\\tf_python\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n             from tensorflow.core.framework.graph_pb2 import *\r\n           File \"D:\\TensorFlow\\build_x64\\tf_python\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n             from google.protobuf import descriptor as _descriptor\r\n         ModuleNotFoundError: No module named 'google'\r\n   520>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [D:\\TensorFlow\\build_x64\\tf_python_api.vcxproj]\r\n\r\n**Repro steps:**\r\n\r\n1. git clone https://github.com/tensorflow/tensorflow D:\\Tensorflow\\src\r\n2. pushd D:\\Tensorflow\r\n3. set PreferredToolArchitecture=x64\r\n4. set rel=Release\r\n5. set CUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\cuda\"\r\n6. set PY=C:\\ProgramData\\Anaconda3\r\n7. set CL=/FS /permissive-\r\n8. cmake D:\\Tensorflow\\src\\tensorflow\\contrib\\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\\ProgramData\\Anaconda3\\python.exe -DPYTHON_LIBRARIES=C:\\ProgramData\\Anaconda3\\libs\\python36.lib -DSWIG_EXECUTABLE=D:\\Tensorflow\\swigwin-3.0.12\\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON\r\n9. MSBuild /m /p:Configuration=Release;Platform=x64 /p:WindowsTargetPlatformVersion=10.0.16299.0 tensorflow.sln /t:Rebuild\r\n", "comments": ["Module Google is not found. It is usually caused by the loss of the protobuf module. Rebuilt the specific project. From my experience,  error MSB6006 is usually caused by some losses of the files in the compiler. Especially when using VS2017, whose file structure is totally different from earlier version. Be careful. If succeed, it will be very nice to share to your experiences", "Hi @cy89, any update for this?", "Nagging Assignee @cy89: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Since the addition of create_python_api.py in 43f5b27, it does an import of tf code during the build of the tf_python_api project which means that you need to have at least protobuf and absl-py in the python path of the interpreter you specified when you ran cmake. \r\n\r\nIdeally (?), the local build of protobuf could be added to the pythonpath but a few additional steps need to be added to the protobuf build to get the locally built version to work, so as a workaround I just pip installed protobuf at a version compatible with the constraint in setup.py for the tf pip pkg on master. I also needed to pip install absl-py, which should probably be added to the list of deps (alongside numpy) for building the python package from source. Arguably, the full list of dependencies should already be in the python environment when building if parts of the package itself are going to be imported during the build, or those dependencies should be pulled, built and added to the pythonpath during the tf build.", "Thanks all for your help. I installed 'google' and 'abs1' modules and Tnesorflow no longer hit this issue.", "thanks for the explanation @tambu-j. I pip install protobuf and absl before doing the windows build. Also, I found, for GPU version build, cuda/bin and cudnn/bin needs to be in PATH, otherwise, will have errors like \r\n\r\n> File \"C:/agent/_work/5/b/tf_python/tensorflow/tools/api/generator/create_python_api.py\", line 26, in \r\n> <module>\r\n> 2018-05-11T02:49:37.9634660Z              from tensorflow.python.util import tf_decorator\r\n> 2018-05-11T02:49:37.9636047Z            File \"C:\\agent\\_work\\5\\b\\tf_python\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n> 2018-05-11T02:49:37.9636803Z              from tensorflow.python import pywrap_tensorflow\r\n> 2018-05-11T02:49:37.9638401Z            File \"C:\\agent\\_work\\5\\b\\tf_python\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n> 2018-05-11T02:49:37.9639141Z              self_check.preload_check()\r\n> 2018-05-11T02:49:37.9640148Z            File \"C:\\agent\\_work\\5\\b\\tf_python\\tensorflow\\python\\platform\\self_check.py\", line 97, in preload_check\r\n> 2018-05-11T02:49:37.9641098Z              % (build_info.cudnn_dll_name, build_info.cudnn_version_number))\r\n> 2018-05-11T02:49:37.9643318Z          ImportError: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn\r\n", "Nagging Assignee @cy89: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for the help, @StevenVerne and @tambu-j . I'll close, as it looks like this is resolved. "]}, {"number": 18455, "title": "eager mode fails with custom operator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes, I have written a custom tensorflow operator\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nbinary wheel\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n('v1.7.0-3-g024aecf414', '1.7.0')\r\n\r\n- **Python version**: \r\n\r\npython 2.7.12\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\nN/A\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nN/A\r\n\r\n- **GPU model and memory**:\r\n\r\nN/A\r\n\r\n- **Exact command to reproduce**:\r\n\r\nCustom operator is defined [here](https://github.com/ska-sa/montblanc/blob/f168870a9756f31c8639ff0c0219ce16af68e612/montblanc/impl/rime/tensorflow/rime_ops/phase_op_cpu.h#L44-L122) but I doubt you'll want to look through that.\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nfrom montblanc.impl.rime.tensorflow.tensorflow_ops import phase\r\n\r\nlm = np.random.random((10,2)).astype(np.float32)\r\nuvw = np.random.random((10,3,2)).astype(np.float32)\r\nfreq = np.linspace(.856e9, .856e8*2, 4).astype(np.float32)\r\n\r\n# Convert into tfe Variable\r\nlm, uvw, freq = (tfe.Variable(a) for a in (lm, uvw, freq))\r\nvalue = phase(lm, uvw, freq, CT=tf.complex64)\r\n```\r\n\r\nThis produces the following stack trace\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_tf_eager_grad.py\", line 65, in <module>\r\n    value = phase(lm, uvw, freq, CT=tf.complex64)\r\n  File \"<string>\", line 419, in phase\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 328, in apply_op\r\n    op_type_name, name, **keywords)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3277, in create_op\r\n    input_ops = set([t.op for t in inputs])\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 842, in op\r\n    raise AttributeError(\"op not supported for Eager Tensors.\")\r\nAttributeError: op not supported for Eager Tensors.\r\n```\r\n\r\n### Describe the problem\r\n\r\nEager mode does not work with a custom operator.\r\n\r\n### Source code / logs\r\n\r\nSee above", "comments": ["Thanks for the report, this was indeed an oversight. @akshaym has a fix for this that should be merged soon.", "This should be fixed with #18504.\r\n\r\nThanks for the report. ", "Thanks @akshaym. Will this go into the 1.8 release?", "@sjperkins : Alas, the fix was made after the 1.8 branch was cut, so this is not included there. It will be included in 1.9.\r\n"]}, {"number": 18454, "title": "Wrong behavior of tf.nn.conv2d when dilated rate>1 on CPU version of tensorflow.", "body": "# Required Info\r\n- **OS Platform and Distribution**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from**:binary\r\n- **Tensorflow Version**:1.6.0\r\n- **Python Version**:3.6.3\r\n- **Have I written custom code**:Yes\r\n- **Bazel version**:N/A\r\n- **CUDA/cuDNN version** : N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See Below\r\n\r\n# Problem description:\r\nWhen constucting the graph, i use tf.nn.conv2d with diated_rate > 1 and padding='VALID', the wrapper returns a correct shape. However when i feed an input in, and use sess.run to get the output, it turns out that tf.nn.ops is not using diliated conv at all. Then i run the exact commands on a machine with tensorflow_gpu, the gpu_versions output is correct. So i believe that i should be a bug.\r\n\r\n# Exact Commands:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ninputs = tf.get_variable(shape=[1, 7, 7, 1], dtype=tf.float32, name='inputs')\r\nfilters = tf.get_variable(shape=[3, 3, 1, 1], dtype=tf.float32, name='filters', initializer=tf.constant_initializer(\r\n    [\r\n        [1, 1, 1],\r\n        [1, 1, 1],\r\n        [1, 1, 1]\r\n    ]\r\n))\r\nconv = tf.nn.conv2d(input=inputs,\r\n                    filter=filters,\r\n                    strides=[1] + [1, 1] + [1],\r\n                    padding='VALID',\r\n                    use_cudnn_on_gpu=False,\r\n                    data_format=\"NHWC\",\r\n                    dilations=[1] + [2, 2] + [1],\r\n                    name='conv')\r\nprint(conv)\r\n\r\ninputs_v = np.zeros([1,7,7,1])\r\ninputs_v[:,0,:,:] = np.reshape([1,2,3,4,5,6,7], newshape=[1,7,1])\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nconv_v = sess.run(conv, feed_dict={inputs:inputs_v})\r\nprint(conv_v) \r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sorry for the missing fields in the template. I find that when using tf.nn.conv2d in jupyter notebook, there is a warning telling you that the current version dosen't support conv with dilations, but when runnning it directly with python script the warning disappears.  So i guess that you leave a dilation interface for compatibility with tensoflow-gpu, but dilation is actually not implemented in tensorflow-cpu?", "According to the changlelog : \r\nhttps://github.com/tensorflow/tensorflow/blob/r1.5/RELEASE.md#major-features-and-improvements\r\n\r\nConv2D, Conv2DBackpropInput, Conv2DBackpropFilter now supports arbitrary dilations with GPU and cuDNNv6 support."]}, {"number": 18453, "title": "Slim Train API doesn't provide control to user to extract loss and accuracy metrics", "body": "I am referring to [train_image_classifier.py][1] script provided by Tensorflow for image classification.\r\n\r\nScript uses slim.learning.train API to kick off the training and then it takes care for everything internally.\r\nThough it logs the metrics i.e. loss/accuracy but that's all internal to slim. I doesn't provide handle to get periodic updates of loss/accuracy in my script while training is going on.\r\nMy requirement is to extract loss and accuracy metrics after every n number of epochs, i.e. 5 and dump that into database for later analysis.\r\n\r\nBut because of high level abstraction of slim train API, It seems there is now way to get periodic updates loss/accuracy info during the training.\r\n\r\n\r\nIs it possible to tap the training process and get the loss/accuracy values after every given number of steps/epochs ?\r\nIf it's possible, please share the pseudo code for that.\r\n\r\nIt will be really helpful for me. \r\n\r\nThanks.\r\n\r\n\r\n  [1]: https://github.com/tensorflow/models/blob/master/research/slim/train_image_classifier.py", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks your reply.\r\nAdding the system info here.\r\n\r\nSystem information\r\n------------------\r\n\r\n - Have I written custom code - Yes\r\n - OS Platform CentOS - 7.2.1511\r\n - TensorFlow installed from - Binary\r\n - TensorFlow version - 1.3.0\r\n - Python version - 2.7\r\n - Bazel version - N/A\r\n - CUDA/cuDNN version - N/A\r\n - GPU model and memory - N/A\r\n - Exact command to reproduce - N/A", "@rohan100jain  - Kindly help me with your answer.", "Doesn't the log_every_n_steps flag give you what you need? It'll print out metrics periodically?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18452, "title": "Different pixel values for the same image when read using DecodeJpeg on x86_64 and arm64", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 \r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n- **Python version**: \r\n3.4\r\n- **Bazel version (if compiling from source)**:\r\n0.11.0\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0.5\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\ngiven below in the problem description\r\n\r\nI used `DecodeJpeg` method to read in images during training, I used it with `dct_method=\"\"`.\r\nIn order to use the same operation on Android, I compiled TensorFlow with DecodeJpeg using bazel.\r\nI compiled it for `arm64`.\r\n\r\nThe problem I'm facing is that, the pixel values from both the platforms are significantly different\r\n\r\nIn Python, I use the snippet below to read image,\r\n```\r\nwith tf.gfile.FastGFile(r'path\\to\\image.jpg', 'rb') as image_file:\r\n    image_bytes = image_file.read()\r\ndecode_jpeg_data = tf.placeholder(dtype=tf.string)\r\ndecode_jpeg = tf.image.decode_jpeg(decode_jpeg_data_f, channels=3)\r\nimage = sess.run(decode_jpeg , feed_dict={decode_jpeg_data: image_bytes })\r\n```\r\nI use [this ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java)script to execute the model on Android. \r\nI executed `DecodeJpeg` with `dct_method` set to `' '`, `INTEGER_FAST`, `INTEGER_ACCURATE`, on both desktop as well as Android.\r\nAll of them gave pixel values which were significantly different from what I got on a desktop, for the same `dct_method`\r\n\r\nFor instance, for the same image, at (100,100,0) the value on `x86_64` is `213`, while it is `204` on `arm64`.\r\nThe problem is that, the prediction label sometimes flips across the two platforms, for the same image.\r\n\r\nHow can I get the same behavior on both platforms?\r\nI had asked this on [StackOverflow](https://stackoverflow.com/questions/49748902/tensorflow-decodejpeg-method-gives-different-pixel-values-on-desktop-and-mobile)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This seems a bug to me. Thank you very much for reporting it. We will investigate into this issue.\r\n\r\nHow many percentage of pixels have different values on two platform, what is the average difference?\r\n \r\n\r\n", "Also, can you please try fancy_upscaling=false to see if it reproduced?", "@mingxingtan if you are interested.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18451, "title": "Try to fix windows libtensorflow", "body": "One more attempt to fix Windows libtensorflow build. Current error:\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\xmemory0(737): error C2280: 'std::unique_ptr<TF_Function,void (__cdecl *)(TF_Function *)>::unique_ptr(const std::unique_ptr<TF_Function,void (__cdecl *)(TF_Function *)> &)': attempting to reference a deleted function\r\n\r\nPrevious commits trying to fix this build:\r\nhttps://github.com/tensorflow/tensorflow/pull/18432#pullrequestreview-111414452\r\nhttps://github.com/tensorflow/tensorflow/pull/18442#pullrequestreview-111466504\r\n\r\n", "comments": []}, {"number": 18450, "title": "Tensorflow failed to compile", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux \r\n- **TensorFlow installed from (source or binary)**: Source from branch r1.7, did not compile\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.1\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:  ./configure && bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nWhile compiling tensorflow build failed. I just pulled the latest commit(92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5) from the r1.7 branch. I also updated bazel (from 0.11.1 to 0.12.0) and some other programs(should be irrelevant to tensorflow) beforehand.\r\n\r\n### Source code / logs\r\n`ERROR: /home/UserHome/.cache/bazel/_bazel_UserHome/ab33c8274551e1ea3125872a4c4e7db9/external/jpeg/BUILD:126:12: Illegal ambiguous match on configurable attribute \"deps\" in @jpeg//:jpeg:\r\n@jpeg//:k8\r\n@jpeg//:armeabi-v7a\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:\r\n\r\n/home/UserHome/.cache/bazel/_bazel_UserHome/ab33c8274551e1ea3125872a4c4e7db9/external/jpeg/BUILD:126:12: Illegal ambiguous match on configurable attribute \"deps\" in @jpeg//:jpeg:\r\n@jpeg//:k8\r\n@jpeg//:armeabi-v7a\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nINFO: Elapsed time: 1.086s\r\nFAILED: Build did NOT complete successfully (3 packages loaded)`\r\n\r\n**UPDATE:** The r1.8 branch compiles successfully. The 1.8 branch might have a fix? Or perhaps a dependency was updated and that fixed it?", "comments": ["try gcc 4~6 don't use gcc7", "I'm also facing build problems even with gcc-6.\r\n\r\n**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\nTensorFlow installed from (source or binary): Source from master branch (r1.7)\r\nTensorFlow version (use command below): 1.7.0\r\nPython version: 3.6.4\r\nBazel version (if compiling from source): 0.11.1\r\nGCC/Compiler version (if compiling from source): 6.4.1\r\nCUDA/cuDNN version: 9.1.85/7.1.2\r\nGPU model and memory: Nvidia GTX 960M, 4GB\r\nExact command to reproduce: `./configure && bazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\n**Logs:**\r\n```\r\nINFO: From Compiling tensorflow/contrib/reduce_slice_ops/kernels/reduce_slice_ops_gpu.cu.cc:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:970:16:   required by substitution of 'template<class _Tp, class _Arg, class> static std::true_type std::__do_is_direct_constructible_impl::__test(int) [with _Tp = std::tuple<int, int, int>; _Arg = const std::tuple<int, int, int>&; <template-parameter-1-3> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:981:40:   required from 'struct std::__is_direct_constructible_impl<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:143:8:   required from 'struct std::__and_<std::is_destructible<std::tuple<int, int, int> >, std::__is_direct_constructible_impl<std::tuple<int, int, int>, const std::tuple<int, int, int>&> >'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:985:8:   required from 'struct std::__is_direct_constructible_new_safe<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1067:8:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1115:8:   required from 'struct std::__is_constructible_impl<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1126:8:   required from 'struct std::is_constructible<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1138:8:   required from 'struct std::__is_copy_constructible_impl<std::tuple<int, int, int>, true>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1144:8:   required from 'struct std::is_copy_constructible<std::tuple<int, int, int> >'\r\n./tensorflow/compiler/xla/statusor.h:87:7:   required from 'class xla::StatusOr<std::tuple<int, int, int> >'\r\n./tensorflow/stream_executor/dnn.h:890:68:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:970:16:   required by substitution of 'template<class _Tp, class _Arg, class> static std::true_type std::__do_is_direct_constructible_impl::__test(int) [with _Tp = std::tuple<int, int, int>; _Arg = const std::tuple<int, int, int>&; <template-parameter-1-3> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:981:40:   required from 'struct std::__is_direct_constructible_impl<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:143:8:   required from 'struct std::__and_<std::is_destructible<std::tuple<int, int, int> >, std::__is_direct_constructible_impl<std::tuple<int, int, int>, const std::tuple<int, int, int>&> >'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:985:8:   required from 'struct std::__is_direct_constructible_new_safe<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1067:8:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1115:8:   required from 'struct std::__is_constructible_impl<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1126:8:   required from 'struct std::is_constructible<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1138:8:   required from 'struct std::__is_copy_constructible_impl<std::tuple<int, int, int>, true>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1144:8:   required from 'struct std::is_copy_constructible<std::tuple<int, int, int> >'\r\n./tensorflow/compiler/xla/statusor.h:87:7:   required from 'class xla::StatusOr<std::tuple<int, int, int> >'\r\n./tensorflow/stream_executor/dnn.h:890:68:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:662:419:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(const std::tuple<_Args1 ...>&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:970:16:   required by substitution of 'template<class _Tp, class _Arg, class> static std::true_type std::__do_is_direct_constructible_impl::__test(int) [with _Tp = std::tuple<int, int, int>; _Arg = const std::tuple<int, int, int>&; <template-parameter-1-3> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:981:40:   required from 'struct std::__is_direct_constructible_impl<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:143:8:   required from 'struct std::__and_<std::is_destructible<std::tuple<int, int, int> >, std::__is_direct_constructible_impl<std::tuple<int, int, int>, const std::tuple<int, int, int>&> >'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:985:8:   required from 'struct std::__is_direct_constructible_new_safe<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1067:8:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1115:8:   required from 'struct std::__is_constructible_impl<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1126:8:   required from 'struct std::is_constructible<std::tuple<int, int, int>, const std::tuple<int, int, int>&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1138:8:   required from 'struct std::__is_copy_constructible_impl<std::tuple<int, int, int>, true>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1144:8:   required from 'struct std::is_copy_constructible<std::tuple<int, int, int> >'\r\n./tensorflow/compiler/xla/statusor.h:87:7:   required from 'class xla::StatusOr<std::tuple<int, int, int> >'\r\n./tensorflow/stream_executor/dnn.h:890:68:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:970:16:   required by substitution of 'template<class _Tp, class _Arg, class> static std::true_type std::__do_is_direct_constructible_impl::__test(int) [with _Tp = std::tuple<int, int, int>; _Arg = std::tuple<int, int, int>&&; <template-parameter-1-3> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:981:40:   required from 'struct std::__is_direct_constructible_impl<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:143:8:   required from 'struct std::__and_<std::is_destructible<std::tuple<int, int, int> >, std::__is_direct_constructible_impl<std::tuple<int, int, int>, std::tuple<int, int, int>&&> >'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:985:8:   required from 'struct std::__is_direct_constructible_new_safe<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1067:8:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1115:8:   required from 'struct std::__is_constructible_impl<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1126:8:   required from 'struct std::is_constructible<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1156:8:   required from 'struct std::__is_move_constructible_impl<std::tuple<int, int, int>, true>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1162:8:   required from 'struct std::is_move_constructible<std::tuple<int, int, int> >'\r\n./tensorflow/compiler/xla/statusor.h:87:7:   required from 'class xla::StatusOr<std::tuple<int, int, int> >'\r\n./tensorflow/stream_executor/dnn.h:890:68:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:970:16:   required by substitution of 'template<class _Tp, class _Arg, class> static std::true_type std::__do_is_direct_constructible_impl::__test(int) [with _Tp = std::tuple<int, int, int>; _Arg = std::tuple<int, int, int>&&; <template-parameter-1-3> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:981:40:   required from 'struct std::__is_direct_constructible_impl<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:143:8:   required from 'struct std::__and_<std::is_destructible<std::tuple<int, int, int> >, std::__is_direct_constructible_impl<std::tuple<int, int, int>, std::tuple<int, int, int>&&> >'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:985:8:   required from 'struct std::__is_direct_constructible_new_safe<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1067:8:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1115:8:   required from 'struct std::__is_constructible_impl<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1126:8:   required from 'struct std::is_constructible<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1156:8:   required from 'struct std::__is_move_constructible_impl<std::tuple<int, int, int>, true>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1162:8:   required from 'struct std::is_move_constructible<std::tuple<int, int, int> >'\r\n./tensorflow/compiler/xla/statusor.h:87:7:   required from 'class xla::StatusOr<std::tuple<int, int, int> >'\r\n./tensorflow/stream_executor/dnn.h:890:68:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:686:422:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:970:16:   required by substitution of 'template<class _Tp, class _Arg, class> static std::true_type std::__do_is_direct_constructible_impl::__test(int) [with _Tp = std::tuple<int, int, int>; _Arg = std::tuple<int, int, int>&&; <template-parameter-1-3> = <missing>]'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:981:40:   required from 'struct std::__is_direct_constructible_impl<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:143:8:   required from 'struct std::__and_<std::is_destructible<std::tuple<int, int, int> >, std::__is_direct_constructible_impl<std::tuple<int, int, int>, std::tuple<int, int, int>&&> >'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:985:8:   required from 'struct std::__is_direct_constructible_new_safe<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1067:8:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1115:8:   required from 'struct std::__is_constructible_impl<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1126:8:   required from 'struct std::is_constructible<std::tuple<int, int, int>, std::tuple<int, int, int>&&>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1156:8:   required from 'struct std::__is_move_constructible_impl<std::tuple<int, int, int>, true>'\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1162:8:   required from 'struct std::is_move_constructible<std::tuple<int, int, int> >'\r\n./tensorflow/compiler/xla/statusor.h:87:7:   required from 'class xla::StatusOr<std::tuple<int, int, int> >'\r\n./tensorflow/stream_executor/dnn.h:890:68:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {tensorflow::Status}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {tensorflow::Status}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/compiler/xla/statusor.h:227:80:   required from 'xla::StatusOr<T>::StatusOr(tensorflow::Status&&) [with T = std::tuple<int, int, int>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {tensorflow::Status}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {tensorflow::Status}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {tensorflow::Status}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/compiler/xla/statusor.h:227:80:   required from 'xla::StatusOr<T>::StatusOr(tensorflow::Status&&) [with T = std::tuple<int, int, int>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {tensorflow::Status}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\nERROR: /mnt/Data/tensorflow/tensorflow/contrib/reduce_slice_ops/BUILD:14:1: output 'tensorflow/contrib/reduce_slice_ops/_objs/python/ops/_reduce_slice_ops_gpu/tensorflow/contrib/reduce_slice_ops/kernels/reduce_slice_ops_gpu.cu.pic.o' was not created\r\nERROR: /mnt/Data/tensorflow/tensorflow/contrib/reduce_slice_ops/BUILD:14:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 167.446s, Critical Path: 47.43s\r\nFAILED: Build did NOT complete successfully\r\n```", "gcc is not the problem(tried 4-6). It has worked with gcc 7.3.1 before as well. The only changes to programs related to tensorflow is the tensorflow commit itself and bazel.", "To corroborate, I just experienced the same problem/error as in the original post. I'd compiled tf 1.7 successfully previously (Ubuntu 14.04.4) but just did an OS upgrade (Ubuntu 14.04.5) and now I get the same error. Several packages were updated---I'm not sure what may have been the cause.", "Getting back to Moocow9m's issue. I just had the same problem. It was the bazel update from a couple of days ago. Try un-installing version 12 and get the 11.1 bazel deb file and install form that. I also had to add --action_env PATH=\"$PATH\" to keep it from forgetting where Python was (I'm using Anaconda3)\r\n\r\nI also have the following  cuda.conf in ld.so.conf.d\r\n/usr/local/cuda/lib64\r\n/usr/local/cuda/extras/CUPTI/lib64\r\n/usr/local/cuda/targets/x86_64-linux/lib/stubs", "@dbkinghorn It does compile with the older bazel build, however since version 0.12.0 can compile branch r1.8 I still think this might be a tensorflow issue in the r1.7 branch.", "I also can compile tf master with bazel 0.12. However, I'm still befuddled that the same tf commit (from branch 1.7) failed to compile after a suite of package updates. That doesn't necessarily mean it wasn't a tf issue in 1.7, just that it wasn't manifest until after a separate update (which I don't think changed bazel).", "Maybe this problem is caused by bazel. I could compile branch r1.6 with bazel 0.11.1. If I update bazel from 0.11.1 to 0.12.0, it will be failed.", "I just pulled the latest commits, and it builds fine.", "@rharish101 what version of bazel did you use? What is your OS?", "@LuBingtan , Yeah, must be the problem of bazel 0.12. I updated bazel from 0.12 to 0.11 and it seems fine now. \r\ntensorflow 1.5\r\nubuntu16\r\ncuda 8.0\r\ncudnn 5.1", "Had similar issue building TF 1.7 with Bazel 0.12.  Downgrading bazel to 0.11 seems to fix the issue", "Downgrading to Bazel 0.11.1 may fix this issue currently\r\n-- Tensorflow r1.7 with CUDA 9.1 and CuDNN7.1", "I confirm that downgrading bazel from 0.12.0 to 0.11.1 fixes the exact error. To downgrade, the only way I found is \r\n\r\n```sh\r\n$ sudo apt-get install -y --no-install-recommends bash-completion g++ zlib1g-dev\r\n$ curl -LO \"https://github.com/bazelbuild/bazel/releases/download/0.11.1/bazel_0.11.1-linux-x86_64.deb\" \r\n$ sudo dpkg -i bazel_*.deb\r\n```\r\n\r\nCopied and modified from [this bazel thread](https://github.com/bazelbuild/continuous-integration/issues/128).", "@Moocow9m Here are my specs:\r\nOS: Arch Linux\r\nPython version: 3.6.4\r\nBazel version (if compiling from source): 0.11.1\r\nCUDA/cuDNN version: 9.1.85/7.1.2\r\nGPU model and memory: Nvidia GTX 960M, 4GB", "The fix for this was merged into TF at https://github.com/tensorflow/tensorflow/pull/17508\r\n\r\nIt doesn't look like 1.7.0 has this fix, so it's failing with Bazel 0.12.0. cc @angersson @gunan ", "this [fix](https://github.com/tensorflow/tensorflow/issues/18450#issuecomment-381380000) by @zhanwenchen helped me ", "It's the problem caused by bazel, version 0.11 is fine with me.", "A better fix whould be to cherry-pick 15bda92cdb12bceb7c96404a3abafeed1a416651.\r\n```\r\ngit cherry-pick 15bda92cdb12bceb7c96404a3abafeed1a416651\r\n```", "I had exactly the same issue,  as soon as I switch from Bazel 0.12 to Bazel 0.11 the compilation went smoothly.    \r\nI followed the recommendation from zhanwenchen:\r\n $ sudo apt-get install -y --no-install-recommends bash-completion g++ zlib1g-dev\r\n$ curl -LO \"https://github.com/bazelbuild/bazel/releases/download/0.11.1/bazel_0.11.1-linux-x86_64.deb\" \r\n$ sudo dpkg -i bazel_*.deb", "I think this issue can be closed since #17508 has been merged to all branches and v1.7.1 contains the fix.", "--config=cuda_clang  mabe close this issue"]}, {"number": 18449, "title": "tf.contrib.summary to export the graph to tensorboard", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Very simple change from provided script\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.7.0-13-g99322a9 1.7.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.1 / 7.1\r\n- **GPU model and memory**: TitanXp 12Gb\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\n\r\nif tf.gfile.IsDirectory(\"testos\"):\r\n    tf.gfile.DeleteRecursively(\"testos\")\r\ntf.gfile.MakeDirs(\"testos\")\r\n\r\nglobal_step = tf.train.get_or_create_global_step()\r\nsummary_writer = tf.contrib.summary.create_file_writer(\r\n    \"./testos/test\", flush_millis=10000)\r\n\r\nwith summary_writer.as_default(), tf.contrib.summary.always_record_summaries():\r\n    x = tf.placeholder(tf.float32, [])\r\n    test = x > 5\r\n\r\n    res = tf.cond(test, true_fn=lambda: tf.constant(0), false_fn=lambda: tf.constant(2))\r\n\r\n    tf.contrib.summary.scalar(\"x\", x)\r\n    tf.contrib.summary.scalar(\"res\", res)\r\n\r\n    inc_op = global_step.assign(global_step + 1)\r\n\r\n    tf.contrib.summary.scalar(\"global_step\", global_step)\r\n\r\n    with tf.Session() as sess:\r\n        tf.global_variables_initializer().run()\r\n\r\n        tf.contrib.summary.initialize(graph=tf.get_default_graph())\r\n        summ = tf.contrib.summary.all_summary_ops()\r\n\r\n        for i in range(10):\r\n            plop, _sum, _g = sess.run([res, summ, inc_op], {x: i})\r\n```\r\n\r\n### Describe the problem\r\nI wanted to use summary with tf.cond. I then decided to use the tf.contrib.summary that makes it possible.\r\nThen based on the example given here https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/summary/summary.py\r\nI wrote the above code. (By the way the example does not show clearly that we need to stay in the `with summary_writer.as_default(), tf.contrib.summary.always_record_summaries():` scope when we write in the summary.)\r\n\r\nFor the scalar there is no problem, it appear correctly in tensorboard. But the graph does not appear. \r\nI get the following message:\r\n> Namespace hiearchy: Failed Adding Nodes\r\n\r\nHere is a screen shot:\r\n![screenshot from 2018-04-12 15-01-33](https://user-images.githubusercontent.com/10159876/38659030-807f6204-3e62-11e8-91df-ab29d7c24431.png)\r\n", "comments": ["@alextp It turns out we might need to take additional steps to use the graph to write the graph. Any ideas here? Note that the stack trace ends up being something like:\r\n\r\n```\r\nlocalhost/:80023 TypeError: Cannot read property 'Identity' of undefined\r\n    at hierarchy.js: parent.opHistogram[node.op] = (parent.opHistogram[node.op] || 0) + 1;\r\n    at createBaseFor\r\n    at baseForOwn\r\n    at createBaseEach\r\n    at Function.<anonymous>\r\n    at hierarchy.js: addNodes: _.each(graph.nodes, function (node, nodeName) {\r\n    at localhost/:73559\r\n    at Object.time (localhost/:79996)\r\n    at localhost/:80091\r\n```", "@jart I'm confused. Is the graph protocol buffer not correctly written? I don't know this js code well enough for the stack trace to make sense to me; can you explain what is happening?", "Hello @rayanelleuch , please check Stack Overflow for tf.contrib.summary usage. One example is:\r\nhttps://stackoverflow.com/questions/49083680/how-are-the-new-tf-contrib-summary-summaries-in-tensorflow-evaluated\r\nThanks.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18448, "title": "Replace all COMPILER_MSVC to _MSC_VER and _WIN32 accordingly", "body": "`-D` compile flag should be avoided as much as possible. Many Google projects like Chromium follow  (which uses CMake or GN) only define macro in header file like below and always include the header file to get the macro (IWYU).\r\n\r\n```cpp\r\n#ifdef _MSC_VER\r\n#define COMPILER_MSVC\r\n#endif\r\n```\r\n\r\nHowever, such rule is not easy to implement for Tensorflow as Bazel require strict header dependency. It is easier to just use `_MSC_VER` and `_WIN32` directly.\r\n\r\nOther stuff:\r\n\r\n- Remove `ARRAYSIZE` macro in `tensorflow/stream_executor/platform/port.h` (only used in `tensorflow/stream_executor/cuda/cuda_driver.cc`, which is now replaced with `TF_ARRAYSIZE` from `tensorflow/stream_executor/platform/macros.h`).", "comments": ["@mrry I fixed one more ARRAYSIZE -> TF_ARRAYSIZE. PTAL and re-trigger CI test.", "@meteorcloudy For some reason, I got the email notification that you pinged @tensorflow-jenkins for CI test, but your comment is missing in Github web and the CI test is not triggered.", "That's right.. I deleted the comment because I just realized TF is not using Jenkins to do presubmit anymore, I don't know how to trigger a retest. @mrry Can you do that?", "@meteorcloudy Sure: for future reference, you now have to apply the \"kokoro:force-run\" label to the PR to kick off the tests.", "@mrry Got it, thanks!", "Last commit fixes the remaining failing CI tests (\"GPU CC\", \"GPU Python3\" and \"XLA\"), caused by including non-existing header file. Sorry for the spam."]}, {"number": 18447, "title": "Numerically safe cross_entropy for tfd.Bernoulli?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```[python]\r\nimport tensorflow as tf\r\nfrom tensorflow import distributions as tfd\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef test_bernoulli_cross_entropy():\r\n    logits = tf.random_normal(shape=[3, 4])\r\n    labels = tf.cast(tf.random_normal(shape=[3, 4]) > 0.0, tf.float32)\r\n    p = tfd.Bernoulli(logits=logits)\r\n    q = tfd.Bernoulli(probs=labels)\r\n\r\n    ce1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=p.logits, labels=q.probs)\r\n    ce2 = q.cross_entropy(p)\r\n    print(ce1)\r\n    print(ce2)\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nv1.7.0-3-g024aecf414 1.7.0\r\n\r\n### Describe the problem\r\nThe Bernoulli in Tensorflow distributions does not seem to have a numerically safe cross entropy operation. See code above -- it should produce the same results as tf.nn.sigmoid_cross_entropy_with_logits, but produces NaN. If I set probs to be not exactly zero or one, then ce1 = ce2. So it seems to be a numerical stability issue.\r\n\r\nExample output:\r\n```\r\ntf.Tensor(\r\n[[0.88931024 1.4332782  0.47688866 0.48566538]\r\n [0.93064773 0.39325503 1.1308188  1.5703993 ]\r\n [0.09037975 0.1828385  0.39422417 0.839576  ]], shape=(3, 4), dtype=float32)\r\ntf.Tensor(\r\n[[nan nan nan nan]\r\n [nan nan nan nan]\r\n [nan nan nan nan]], shape=(3, 4), dtype=float32)\r\n```\r\n\r\n### Source code / logs\r\n```[python]\r\nimport tensorflow as tf\r\nfrom tensorflow import distributions as tfd\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef test_bernoulli_cross_entropy():\r\n    logits = tf.random_normal(shape=[3, 4])\r\n    labels = tf.cast(tf.random_normal(shape=[3, 4]) > 0.0, tf.float32)\r\n    p = tfd.Bernoulli(logits=logits)\r\n    q = tfd.Bernoulli(probs=labels)\r\n\r\n    ce1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=p.logits, labels=q.probs)\r\n    ce2 = q.cross_entropy(p)\r\n    print(ce1)\r\n    print(ce2)\r\n```\r\n", "comments": ["Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18446, "title": "Fix OdeIntTest", "body": "Fix OdeIntTest in GPU pip build. This effectively reverts https://github.com/tensorflow/tensorflow/commit/23d6c55ba22596f903696e0dba8037ad81470a39#diff-f541c00fcae53468339272b7c92a67a4.\r\n\r\nThe test is currently failing with:\r\ntensorflow/core/common_runtime/executor.cc:660] Executor failed to create kernel. Not found: No registered 'Square' OpKernel for GPU devices compatible with node ArithmeticOptimizer/odeint_4/interpolate_loop/interpolate/interp_evaluate/mul_square = Square[T=DT_COMPLEX64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](odeint_4/interpolate_loop/interpolate/interp_evaluate/Cast)\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]", "comments": []}, {"number": 18445, "title": "[MSVC] Remove -D__VERSION__ flag and implement tf_compiler_version", "body": "`__VERSION__` is only used in one place. Remove `-D__VERSION__` and implement `tf_compiler_version` for MSVC correctly.\r\n\r\nExample string returned by `tf_compiler_version` when using VS 2017 will be something like `MSVC 191025017`.", "comments": []}, {"number": 18444, "title": "Fix TensorRT static linkage problem", "body": "This PR fixes an issue with linkage manifested itself recently by linking static variables multiple times to different libraries.", "comments": ["@aaroey This should fix issues encountered in int8 calibration."]}, {"number": 18443, "title": "In Matmul, is FP16 x FP16 accumulated to FP16 or FP32?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.7\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.0\r\n- **GPU model and memory**:\r\nV100 16GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIt is not clear in documents that, in Matmul, FP16xFP16 is accumulated to FP16 or FP32. \r\nThis choice affects not only training accuracy, but also TensorCore computing performance on Volta GPU.\r\n\r\n", "comments": ["Nagging Assignee @jart: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thank you for the feedback on doc improvement. We'd welcome a contribution that makes our docs more clear on this matter.", "@reedwm do you know the answer?", "On GPUs, fp16 Matmul accumulation is done in fp32."]}, {"number": 18442, "title": "More build fixes", "body": "These fix typos in:\r\nhttps://github.com/tensorflow/tensorflow/pull/18432#pullrequestreview-111414452\r\nand\r\nhttps://github.com/tensorflow/tensorflow/pull/18439#pullrequestreview-111451143", "comments": []}, {"number": 18441, "title": "Something seems like a bug in r1.8 when building from source code?", "body": "I try to build tensorflow with gpu support from source.\r\n\r\n# **Basic information:**\r\n\r\n- OS Platform and Distribution : ubuntu 14.04\r\n- TensorFlow installed from : source code, branch r1.7\r\n- TensorFlow version : r1.7\r\n- Bazel version : 0.11.1\r\n- CUDA/cuDNN version: 8.0/6.0\r\n- GPU model and memory : Titan X\r\n- Exact command to reproduce:\r\n    `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package `\r\n\r\n# **My configuration:**\r\n```\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]:\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]:\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]:\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]:\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 8.0\r\n\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default\r\n is /usr/local/cuda]:\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 6.0\r\n\r\nPlease specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default\r\nis /usr/local/cuda]:/home/chongyang/RemoteSensingImage/cuda\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]:\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]:\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1,6.1,6.1,6.1]\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]:\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]:\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\nConfiguration finished\r\n```\r\n# **The error I encountered:**\r\n```\r\nERROR: /home/chongyang/RemoteSensingImage/tensorflow/tensorflow/stream_executor/BUILD:52:1: C++ compilation of\r\n rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1)\r\nIn file included from ./tensorflow/stream_executor/platform/port.h:21:0,\r\n                 from ./tensorflow/stream_executor/device_memory.h:30,\r\n                 from ./tensorflow/stream_executor/dnn.h:30,\r\n                 from ./tensorflow/stream_executor/cuda/cuda_dnn.h:22,\r\n                 from tensorflow/stream_executor/cuda/cuda_dnn.cc:16:\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In constructor 'perftools::gputools::cuda::CudnnRnnDescriptor::Cu\r\ndnnRnnDescriptor(perftools::gputools::cuda::CUDAExecutor*, cudnnHandle_t, int, int, int, cudnnRNNInputMode_t,\r\ncudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t, cudnnDataType_t, const perftools::gputools::dnn::Algori\r\nthmConfig&, float, tensorflow::uint64, perftools::gputools::ScratchAllocator*)':\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1188:29: error: 'class perftools::gputools::dnn::AlgorithmConfig' has no member named 'is_default'\r\n     CHECK(algorithm_config_.is_default())\r\n```\r\n\r\n# **Something seems like a bug in r1.7?**\r\nAs the error message show, 'class perftools::gputools::dnn::AlgorithmConfig' has no member named 'is_default'. Then I check the code in ` tensorflow/stream_executor/cuda/cuda_dnn.cc` and `tensorflow/stream_executor/dnn.h`\r\n\r\n```\r\n//tensorflow/stream_executor/dnn.h\r\nclass AlgorithmConfig {\r\n public:\r\n  AlgorithmConfig() {}\r\n  explicit AlgorithmConfig(AlgorithmDesc algorithm) : algorithm_(algorithm) {}\r\n  AlgorithmConfig(AlgorithmDesc algorithm, AlgorithmDesc algorithm_no_scratch)\r\n      : algorithm_(algorithm), algorithm_no_scratch_(algorithm_no_scratch) {}\r\n  AlgorithmDesc algorithm() const { return algorithm_; }\r\n  void set_algorithm(AlgorithmDesc val) { algorithm_ = val; }\r\n  AlgorithmDesc algorithm_no_scratch() const { return algorithm_no_scratch_; }\r\n  void set_algorithm_no_scratch(AlgorithmDesc val) {\r\n    algorithm_no_scratch_ = val;\r\n  }\r\n  bool operator==(const AlgorithmConfig& other) const {\r\n    return this->algorithm_ == other.algorithm_ &&\r\n           this->algorithm_no_scratch_ == other.algorithm_no_scratch_;\r\n  }\r\n  bool operator!=(const AlgorithmConfig& other) const {\r\n    return !(*this == other);\r\n  }\r\n  string ToString() const;\r\n\r\n private:\r\n  AlgorithmDesc algorithm_;\r\n  AlgorithmDesc algorithm_no_scratch_;\r\n};\r\n```\r\n```\r\n//tensorflow/stream_executor/cuda/cuda_dnn.cc\r\nCudnnRnnDescriptor(CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\r\n                     int num_layers, int hidden_size, int input_size,\r\n                     cudnnRNNInputMode_t input_mode,\r\n                     cudnnDirectionMode_t direction_mode,\r\n                     cudnnRNNMode_t rnn_mode, cudnnDataType_t data_type,\r\n                     cudnnDataType_t compute_type,\r\n                     const dnn::AlgorithmConfig& algorithm_config,\r\n                     float dropout, uint64 seed,\r\n                     ScratchAllocator* state_allocator)\r\n      : parent_(parent),\r\n        rnn_desc_(nullptr),\r\n        num_layers_(num_layers),\r\n        hidden_size_(hidden_size),\r\n        input_size_(input_size),\r\n        input_mode_(input_mode),\r\n        direction_mode_(direction_mode),\r\n        rnn_mode_(rnn_mode),\r\n        data_type_(data_type),\r\n        compute_type_(compute_type),\r\n       ///////////////////////////////////////////////////\r\n       ///// //algorithm_config_ is defined here!/////\r\n       ///////////////////////////////////////////////////\r\n        algorithm_config_(algorithm_config) {\r\n....\r\n       /////////////////////////////////////////////////////////////////////////////\r\n       //algorithm_config_ call function is_default(), which occurs the bug//\r\n        ////////////////////////////////////////////////////////////////////////////\r\n       CHECK(algorithm_config_.is_default())\r\n            << \"Non-default algorithm not supported for CUDA version < 6.0\";\r\n```\r\n\r\n**Actually, algorithm_config_ has no member named `is_default()`. Only `class AlgorithmDesc` has this function.**\r\n\r\nThanks for helping me to solve this problem and correct me!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I met the same problem\r\n\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: macOS Sierra 10.12.6\r\n- TensorFlow installed from: Source, r1.4\r\n- TensorFlow version: 1.4\r\n- Bazel version: 0.12.0-homebrew\r\n- CUDA/cuDNN version: 8.0/5.1\r\n- GPU model and memory: GTX 960, 4096 MB\r\n- Exact command to reproduce: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n>ERROR: .....tensorflow/tensorflow/stream_executor/BUILD:54:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1)\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1189:29: error: no member named 'is_default' in 'perftools::gputools::dnn::AlgorithmConfig'\r\n    CHECK(algorithm_config_.is_default())\r\n          ~~~~~~~~~~~~~~~~~ ^\r\n./tensorflow/core/platform/default/logging.h:98:26: note: expanded from macro 'CHECK'\r\n  if (TF_PREDICT_FALSE(!(condition))) \\\r\n                         ^~~~~~~~~\r\n./tensorflow/core/platform/macros.h:83:47: note: expanded from macro 'TF_PREDICT_FALSE'\r\n#define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))", "@zheng-xq do you have any idea what could be happening here?", "solved? the same error in r1.8", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I just encountered the issue.\r\n\r\n\r\n   * Have I written custom code: No\r\n   * OS Platform and Distribution: Ubuntu 16.04\r\n   * TensorFlow installed from: Source, 1.8\r\n   * TensorFlow version: 1.8\r\n   * Bazel version: 0.12.0\r\n   * CUDA/cuDNN version: 8.0/5.1\r\n   * GPU model and memory: GTX 1050\r\n   * Exact command to reproduce: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package", "@tensorflowbutler right, the issue is remained unsolved.", "For CUDNN < 6, https://github.com/tensorflow/tensorflow/commit/4f7943f7358fc69af62dc280c6f6ba549ebe2167 appears to have introduced a bug here:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1188\r\n\r\nBut it shouldn't get hit for CUDNN >= 6.\r\n@encore-zhou, is there any chance you have multiple cudnn versions installed and the v5 header is getting picked up instead of v6?", "I got the same error with TensorFlow r1.8, CUDA/cuDNN version 8.0/5.1. Does anyone know how to fix it? \r\n\r\n", "@orangetangerine @pLesur Have you guys figured out how to deal with this issue? ", "I ended up upgrading cuda and compiled the latest stable. Did you try upgrading cudnn to 6.x as mentioned in a previous comment?", "I finally upgraded cudnn to 7.x and get TF installed. There were some issues with my path previously. ", "Looks like this is resolved with cudnn upgrade. Closing the issue.", "@samikama Faced with exactly the same issue now, TF 1.8, CUDA 8.0.44 and CUDNN 8_v7.0.5 with Bazel 0.15. @centos6.9\r\nOn which cudnn version you were upgraded, please?", "maybe this problem is due to protoc's version , when i upgrade protoc version from 2.6.1 to 3.5.1\uff0cthis problem disappear", "@Vasik76 I believe TF supported cuda version is 9.0 and cuDNN is 7.0.5."]}, {"number": 18440, "title": "TenflowLite Crash", "body": "\r\ndevice : Galaxy Tab advanced 4\r\nversion : 6.0 \r\n\r\n04-12 11:37:19.164 1221-1221/? A/DEBUG: Build fingerprint: 'samsung/matisse10wifikx/matisse10wifikx:6.0.1/MMB29K/T536KXU1AQF1:user/release-keys'\r\n04-12 11:37:19.164 1221-1221/? A/DEBUG: Revision: '4'\r\n04-12 11:37:19.164 1221-1221/? A/DEBUG: ABI: 'arm'\r\n04-12 11:37:19.164 1221-1221/? A/DEBUG: pid: 27456, tid: 28023, name: Thread-24515  >>> me.visual.camp.mobileeye <<<\r\n04-12 11:37:19.164 1221-1221/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x6a9e1398\r\n04-12 11:37:19.174 1221-1221/? A/DEBUG:     r0 00000004  r1 00000020  r2 dec48050  r3 00000002\r\n04-12 11:37:19.174 1221-1221/? A/DEBUG:     r4 dec40010  r5 b8d7a6d0  r6 6a9e1398  r7 00000002\r\n04-12 11:37:19.174 1221-1221/? A/DEBUG:     r8 ebc9fe14  r9 ebc9fe04  sl dec40000  fp 00000004\r\n04-12 11:37:19.174 1221-1221/? A/DEBUG:     ip 00000000  sp d197f2f0  lr dec40008  pc f388cfee  cpsr a00f0030\r\n04-12 11:37:19.184 1221-1221/? A/DEBUG: backtrace:\r\n04-12 11:37:19.184 1221-1221/? A/DEBUG:     #00 pc 0005efee  /data/app/me.visual.camp.mobileeye-1/lib/arm/libtensorflowlite_jni.so\r\n04-12 11:37:19.184 1221-1221/? A/DEBUG:     #01 pc 0007922d  /data/app/me.visual.camp.mobileeye-1/lib/arm/libtensorflowlite_jni.so\r\n04-12 11:37:19.184 1221-1221/? A/DEBUG:     #02 pc 00007895  /data/app/me.visual.camp.mobileeye-1/lib/arm/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+980)\r\n04-12 11:37:19.184 1221-1221/? A/DEBUG:     #03 pc 005b1539  /data/app/me.visual.camp.mobileeye-1/oat/arm/base.odex (offset 0x431000)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "maybe you can do some debugging on your side to help us out here. The stack trace is not particularly helpful, unfortunately.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18439, "title": "Fix cudnn version test", "body": "Do not build tests under stream_executor/cuda directory if tensorflow_BUILD_CC_TESTS  is set to off. Otherwise, googletest is not included.\r\n", "comments": []}, {"number": 18438, "title": "Remove reference cycle checks from unit tests which touch uuid.uuid4()", "body": "Should fix the release builds. They're failing because uuid4() creates reference\r\ncycles in Python 2.7.9 (2.7.11+ are fine). ", "comments": []}, {"number": 18437, "title": "Converting TensorFlow frozen and inference model to lite fails with \"Check failed: array->has_shape\"", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS X High Sierra\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n'v1.7.0-rc1-1018-g7a0def60d4' 1.7.0-rc1\r\n- **Python version**: \r\n3.5.5\r\n- **Bazel version (if compiling from source)**:\r\n0.10.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\nApple LLVM version 9.1.0 (clang-902.0.39.1)\r\n- **CUDA/cuDNN version**:\r\nNA\r\n- **GPU model and memory**:\r\nNA\r\n- **Exact command to reproduce**:\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=frozen.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=frozen.lite \\\r\n--input_array=Inputs/Input \\\r\n--output_array=Outputs/Prediction \\\r\n--input_shape=1,28,28,1 \\\r\n--inference_type=FLOAT\r\n-**Error Message**\r\n F tensorflow/contrib/lite/toco/tooling_util.cc:831] Check failed: array->has_shape()\r\n\r\nI have created a TensorFlow model which classifies MNIST image data. The model is then frozen and optimized for inference. These models I can benchmark as well. But when converting to TensorFlow lite, the toco command fails with the above mentioned error message.\r\n\r\nI have summarized the graph as well for both frozen and inference model.\r\nThe frozen model has the same size of (1,28,28,1) but the inference has a size of (None). I am using a placeholder when creating the model for the Input.\r\n", "comments": ["@andrehentz could you take a look?", "That's a strange error. Could you make sure the input and output arrays have the correct names in your command line?", "The Input and Output arrays have the correct name. I'm using tf.gather in my graph which I guess might be  causing the problem as an unsupported node.", "It looks like this code has been changed recently so I can't pinpoint exactly where the CHECK is. It seems to be this line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/tooling_util.cc#L841\r\n\r\nThat indicates a problem with the model, or a unreasonable assumption by TOCO.", "Yeah that's what I was guessing. But I read the docs and it said tf.gather was not supported so I guess it won't be able to convert it.\r\n\r\nI just wanted to double check, does the input the lite models need to be a fixed number? e.g. the model is saved as Input [?, 28, 28, 1] but for lite conversion does it need to be [1,28,28,1]?\r\n\r\nThank you for looking into this issue.", "Support for tf.gather has been added recently.\r\n\r\nYou can resize the input tensor before inference. Call ResizeInputTensor().\r\n", "I have been having a similar issue.  All of the operations are supported, but conversion to `.tflite` fails because the input tensor has an input dimension of type `None` (to allow an arbitrary number of input datapoints)\r\n\r\nFrom the [tflite dev guide](https://www.tensorflow.org/versions/master/mobile/tflite/devguide), you can convert your model to FlatBuf like so:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nval = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])\r\nout = tf.identity(val, name=\"out\")\r\n\r\nwith tf.Session() as sess:\r\n  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])\r\n  open(\"converteds_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nHowever, if our input can be of arbitrary size, (e.g. our input `x` is of shape `[None, 784]`), then we get `Error: TypeError: __int__ returned non-int (type NoneType)`.\r\n\r\nI posted a [SO question](https://stackoverflow.com/questions/50028868/tensorflow-lite-toco-convert-for-arbitrary-sized-input-tensor), but have no answers yet. \r\n\r\nA M(not)WE could be:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\ngraph_mnist = tf.Graph()\r\n\r\nwith graph_mnist.as_default():\r\n    with tf.name_scope('data'):\r\n        inputs = tf.placeholder(tf.float32, [None, 784], name='inputs')\r\n        targets = tf.placeholder(tf.float32, [None, 10], name='targets')\r\n    with tf.name_scope('parameters'):\r\n        weights = tf.Variable(tf.zeros([784, 10]), name='weights')\r\n        biases = tf.Variable(tf.zeros([10]), name='biases')\r\n    with tf.name_scope('model'):\r\n        outputs = tf.matmul(inputs, weights) + biases\r\n    with tf.name_scope('error'):\r\n        error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets))\r\n    with tf.name_scope('train'):\r\n        train_step = tf.train.GradientDescentOptimizer(0.5).minimize(error)\r\n    with tf.name_scope('accuracy'):\r\n        accuracy = tf.reduce_mean(tf.cast(\r\n                tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1)), tf.float32))\r\n    with tf.name_scope('init'):\r\n        init_op = tf.global_variables_initializer()\r\n\r\nsess = tf.Session(graph=graph_mnist)\r\nsess.run(init_op)\r\n\r\nfor _ in range(200):\r\n    batch = mnist.train.next_batch(100)\r\n    _, batch_error = sess.run(\r\n        [train_step, error],\r\n        feed_dict={inputs: batch[0], targets: batch[1]})\r\n        \r\n        \r\ndef get_error_and_accuracy(xx, yy_):\r\n    err = 0\r\n    acc = 0\r\n    err += sess.run(error, feed_dict={inputs: xx, targets: yy_})\r\n    acc += sess.run(accuracy, feed_dict={inputs: xx, targets: yy_})\r\n    return err, acc\r\n\r\nprint('Train data: Error={0:.2f} Accuracy={1:.2f}'\r\n    .format(*get_error_and_accuracy(mnist.train.images, mnist.train.labels)))\r\nprint('Valid data: Error={0:.2f} Accuracy={1:.2f}'\r\n    .format(*get_error_and_accuracy(mnist.test.images, mnist.test.labels)))\r\n    \r\n    \r\ntflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [inputs], [outputs])\r\nopen(\"converteds_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nHow would one convert a model like this to the `.tflite` format?", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 75 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 90 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Wheest this seems like a separate issue. You should use a fixed batch size. In the future you should create a new issue.\r\n\r\nI think @andrehentz resolved the original issue, so closing this now."]}, {"number": 18436, "title": "Add GPU support for float16 batched matmul", "body": "- Uses cublasGemmBatchedEx introduced in CUDA 9.1.\r\n- Includes support for Tensor Op math.\r\n- Falls back to a loop over non-batched gemm calls on older CUDA\r\n  versions or GPU architectures.\r\n\r\nNote that //tensorflow/python/kernel_tests:batch_matmul_op_test previously passed only because it does not specify force_gpu=True and falls back to the CPU.\r\n\r\nNotifying @tfboyd", "comments": ["Also add @protoget ", "@benbarsdell Thanks for this change - could you please respond to @yzhwang's comments so that we can get to merging?", "Sorry for the delay. I'm working on addressing this now.", "@yzhwang does this look OK now? We could add autotuning in a later PR.", "I can not compile tensorflow with cuda 9.1 because of this. Shouldn't PERFTOOLS_GPUTOOLS_CUBLAS_WRAP macro be STREAM_EXECUTOR_CUBLAS_WRAP?\r\n\r\nPR #19210 will fix this.", "Thanks @achalshah20! This got overlooked in the automerge."]}]