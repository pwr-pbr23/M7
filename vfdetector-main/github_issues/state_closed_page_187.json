[{"number": 49129, "title": "How could I get middle layers output of TFLite ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like to know how to get the middle layers output of TFLite models. \r\nFor now, the method I used is:\r\n\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(input_details[\"index\"], test_image)\r\ninterpreter.invoke()\r\n\r\nlayer_output = interpreter.get_tensor(**index**), where index is the the index of the tensor\r\n\r\nIs that correct?\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who would like to access the middle layer outputs\r\n\r\n", "comments": ["If you want to get the intermediate tensors, you can use the experimental flag like the below one.\r\n\r\n```\r\ninterpreter = tf.lite.Interpreter(\r\n    model_path=\"test.tflite\",\r\n    experimental_preserve_all_tensors=True)\r\n# Run evaluation\r\ninterpreter.invoke()\r\n# Look at all tensors including intermediates.\r\nprint({\r\n    t['name']: interpreter.get_tensor(t['index'])\r\n    for t in interpreter.get_tensor_details()\r\n})\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49129\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49129\">No</a>\n", "> If you want to get the intermediate tensors, you can use the experimental flag like the below one.\r\n> \r\n> ```\r\n> interpreter = tf.lite.Interpreter(\r\n>     model_path=\"test.tflite\",\r\n>     experimental_preserve_all_tensors=True)\r\n> # Run evaluation\r\n> interpreter.invoke()\r\n> # Look at all tensors including intermediates.\r\n> print({\r\n>     t['name']: interpreter.get_tensor(t['index'])\r\n>     for t in interpreter.get_tensor_details()\r\n> })\r\n> ```\r\n\r\nI tried the \"experimental_preserve_all_tensors=True\" but got \"__init__() got an unexpected keyword argument 'experimental_preserve_all_tensors'\"\r\n\r\nIt seems that tf.lite.Interpreter() does not accept argument experimental_preserve_all_tensors.\r\nThis argument also does not appear in formal reference https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter\r\n", "This feature is newly enabled. Please try out with the tf-nightly version.", "FYI, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/interpreter.py#L312", "Thanks, may I know if I do not use \"experimental_preserve_all_tensors=True\", I can still get information of all tensors by calling \"interpreter.get_tensor(index)\". Where does those values comes from?"]}, {"number": 49128, "title": "Validate that a and b are proper sparse tensors", "body": "PiperOrigin-RevId: 373274848\nChange-Id: I3a665ac3a29dee9fb69bdf408a939330cb93ea75", "comments": []}, {"number": 49127, "title": "Question about `input_fn` in Tensorflow's multi worker estimator tutorial", "body": "\r\nIn this [tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator), function input_fn has a param named mode. I can't figure out how this param get passed to input_fn when training.\r\n```python\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\n\r\ndef input_fn(mode, input_context=None):\r\n  datasets, info = tfds.load(name='mnist',\r\n                                with_info=True,\r\n                                as_supervised=True)\r\n  mnist_dataset = (datasets['train'] if mode == tf.estimator.ModeKeys.TRAIN else\r\n                   datasets['test'])\r\n\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\n  if input_context:\r\n    mnist_dataset = mnist_dataset.shard(input_context.num_input_pipelines,\r\n                                        input_context.input_pipeline_id)\r\n  return mnist_dataset.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n```\r\n\r\nTo better clarify my question, see how this `input_fn` is passed as a function param. It is just passed without any wrapper that give `mode` a value.\r\n```python\r\ntf.estimator.train_and_evaluate(\r\n    classifier,\r\n    train_spec=tf.estimator.TrainSpec(input_fn=input_fn),\r\n    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)\r\n)\r\n```", "comments": ["@jiqiujia ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide the tensorflow version,the complete code and the dataset you are using. Thanks!", "@jiqiujia , `mode` is basically value from [tf.estimator.ModeKeys](https://www.tensorflow.org/api_docs/python/tf/estimator/ModeKeys), it can be between `TRAIN, EVAL and PREDICT`. \r\nThe value is passed to the input function when the [Train and evaluate](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator#train_and_evaluate_the_model) is called using [TrainSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/TrainSpec) or [EvalSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EvalSpec), for example if `tf.estimator.TrainSpec(input_fn=input_fn)` is used the mode will be taken as `TRAIN` and for `tf.estimator.EvalSpec(input_fn=input_fn)` the mode will be `EVAL`.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sachinprasadhs Sorry for my late response. It seems this is not documented? If not, please help me to point out.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@jiqiujia , As of now you can find only those information from different links provided in the comment above.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49127\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49127\">No</a>\n"]}, {"number": 49126, "title": "Ensure validation sticks in banded_triangular_solve_op", "body": "PiperOrigin-RevId: 373275480\nChange-Id: Id7717cf275b2d6fdb9441fbbe166d555182d2e79", "comments": []}, {"number": 49125, "title": "Keras Tensorboard callback not  generating profile with model.train_on_batch()", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):  pip\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nNo profile shown in Tensorboard.\r\n\r\n**Describe the expected behavior**\r\nShould see active profile. \r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n  .\r\n  '\r\n   model.compile(optimizer='adam', loss='mse', metrics=['mae'])\r\n   tboard_cb = tf.keras.callbacks.TensorBoard(log_dir = logs,\r\n                                                 histogram_freq = 1,\r\n                                                 update_freq = 'batch',\r\n                                                 write_images = True,\r\n                                                 write_graph = True,\r\n                                                 profile_batch = '0,3',\r\n                                                )\r\n    pbar_cb = tf.keras.callbacks.ProgbarLogger(count_mode='steps',\r\n                                stateful_metrics=None)\r\n    cblist = tf.keras.callbacks.CallbackList(callbacks=[tboard_cb, pbar_cb],\r\n                       model=model, add_history=True, add_progbar=True)\r\n   X_batch = ...\r\n   Y_batch = ...\r\n   for i in range(10):\r\n      loss = model.train_on_batch(X_batch, Y_batch)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Could you please provide the simple standalone code/ colab link to reproduce the issue at our end.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49125\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49125\">No</a>\n"]}, {"number": 49124, "title": "[tf.data][cherrypick] Fix snapshot segfault when using repeat and pre\u2026", "body": "\u2026fecth\r\n\r\nSimilar to #49121 on `r2.4`. Needed manual cherrypick due to refactoring\r\nafter `r2.5` branch cut", "comments": []}, {"number": 49123, "title": "Conv2D with dilation_rate>1 runs extremely slow on cpu", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes for the tf.keras.applications\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy Note 10 plus\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2 / 8.1.1\r\n- GPU model and memory: RTX 3080, 10GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n```\r\nibrary libcudart.so.11.0\r\nunknown 2.4.0\r\n```\r\n\r\n**Describe the current behavior**\r\n1. I modified tf.keras.applications.efficientNet.py's Conv2D operations from having dilation_rate=1 to dilation_rate=2.\r\n2. Converted the trained model into tflite format\r\n3. Ran inference on Android devices.\r\n4. It is running about 4 times slower on cpu compared to the baseline which has dilation_rate=1 conv. (GPU inference runs in similar speed)\r\n\r\n**Describe the expected behavior**\r\nExpected to be running in a similiar speed on cpu\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n# pylint: disable=invalid-name\r\n# pylint: disable=missing-docstring\r\n\"\"\"EfficientNet models for Keras.\r\n\r\nReference:\r\n  - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](\r\n      https://arxiv.org/abs/1905.11946) (ICML 2019)\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport copy\r\nimport math\r\n\r\nfrom tensorflow.python.keras import backend\r\nfrom tensorflow.python.keras.applications import imagenet_utils\r\nfrom tensorflow.python.keras.engine import training\r\nfrom tensorflow.python.keras.layers import VersionAwareLayers\r\nfrom tensorflow.python.keras.utils import data_utils\r\nfrom tensorflow.python.keras.utils import layer_utils\r\nfrom tensorflow.python.lib.io import file_io\r\nfrom tensorflow.python.util.tf_export import keras_export\r\n\r\n\r\nBASE_WEIGHTS_PATH = 'https://storage.googleapis.com/keras-applications/'\r\n\r\nWEIGHTS_HASHES = {\r\n    'b0': ('902e53a9f72be733fc0bcb005b3ebbac',\r\n           '50bc09e76180e00e4465e1a485ddc09d'),\r\n    'b1': ('1d254153d4ab51201f1646940f018540',\r\n           '74c4e6b3e1f6a1eea24c589628592432'),\r\n    'b2': ('b15cce36ff4dcbd00b6dd88e7857a6ad',\r\n           '111f8e2ac8aa800a7a99e3239f7bfb39'),\r\n    'b3': ('ffd1fdc53d0ce67064dc6a9c7960ede0',\r\n           'af6d107764bb5b1abb91932881670226'),\r\n    'b4': ('18c95ad55216b8f92d7e70b3a046e2fc',\r\n           'ebc24e6d6c33eaebbd558eafbeedf1ba'),\r\n    'b5': ('ace28f2a6363774853a83a0b21b9421a',\r\n           '38879255a25d3c92d5e44e04ae6cec6f'),\r\n    'b6': ('165f6e37dce68623721b423839de8be5',\r\n           '9ecce42647a20130c1f39a5d4cb75743'),\r\n    'b7': ('8c03f828fec3ef71311cd463b6759d99',\r\n           'cbcfe4450ddf6f3ad90b1b398090fe4a'),\r\n}\r\n\r\nDEFAULT_BLOCKS_ARGS = [{\r\n    'kernel_size': 3,\r\n    'repeats': 1,\r\n    'filters_in': 32,\r\n    'filters_out': 16,\r\n    'expand_ratio': 1,\r\n    'id_skip': True,\r\n    'strides': 1,\r\n    'se_ratio': 0.25\r\n}, {\r\n    'kernel_size': 3,\r\n    'repeats': 2,\r\n    'filters_in': 16,\r\n    'filters_out': 24,\r\n    'expand_ratio': 6,\r\n    'id_skip': True,\r\n    'strides': 2,\r\n    'se_ratio': 0.25\r\n}, {\r\n    'kernel_size': 5,\r\n    'repeats': 2,\r\n    'filters_in': 24,\r\n    'filters_out': 40,\r\n    'expand_ratio': 6,\r\n    'id_skip': True,\r\n    'strides': 2,\r\n    'se_ratio': 0.25\r\n}, {\r\n    'kernel_size': 3,\r\n    'repeats': 3,\r\n    'filters_in': 40,\r\n    'filters_out': 80,\r\n    'expand_ratio': 6,\r\n    'id_skip': True,\r\n    'strides': 2,\r\n    'se_ratio': 0.25\r\n}, {\r\n    'kernel_size': 5,\r\n    'repeats': 3,\r\n    'filters_in': 80,\r\n    'filters_out': 112,\r\n    'expand_ratio': 6,\r\n    'id_skip': True,\r\n    'strides': 1,\r\n    'se_ratio': 0.25\r\n}, {\r\n    'kernel_size': 5,\r\n    'repeats': 4,\r\n    'filters_in': 112,\r\n    'filters_out': 192,\r\n    'expand_ratio': 6,\r\n    'id_skip': True,\r\n    'strides': 2,\r\n    'se_ratio': 0.25\r\n}, {\r\n    'kernel_size': 3,\r\n    'repeats': 1,\r\n    'filters_in': 192,\r\n    'filters_out': 320,\r\n    'expand_ratio': 6,\r\n    'id_skip': True,\r\n    'strides': 1,\r\n    'se_ratio': 0.25\r\n}]\r\n\r\nCONV_KERNEL_INITIALIZER = {\r\n    'class_name': 'VarianceScaling',\r\n    'config': {\r\n        'scale': 2.0,\r\n        'mode': 'fan_out',\r\n        'distribution': 'truncated_normal'\r\n    }\r\n}\r\n\r\nDENSE_KERNEL_INITIALIZER = {\r\n    'class_name': 'VarianceScaling',\r\n    'config': {\r\n        'scale': 1. / 3.,\r\n        'mode': 'fan_out',\r\n        'distribution': 'uniform'\r\n    }\r\n}\r\n\r\nlayers = VersionAwareLayers()\r\n\r\nBASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\r\n\r\n  Reference:\r\n  - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](\r\n      https://arxiv.org/abs/1905.11946) (ICML 2019)\r\n\r\n  Optionally loads weights pre-trained on ImageNet.\r\n  Note that the data format convention used by the model is\r\n  the one specified in your Keras config at `~/.keras/keras.json`.\r\n  If you have never configured it, it defaults to `\"channels_last\"`.\r\n\r\n  Arguments:\r\n    include_top: Whether to include the fully-connected\r\n        layer at the top of the network. Defaults to True.\r\n    weights: One of `None` (random initialization),\r\n          'imagenet' (pre-training on ImageNet),\r\n          or the path to the weights file to be loaded. Defaults to 'imagenet'.\r\n    input_tensor: Optional Keras tensor\r\n        (i.e. output of `layers.Input()`)\r\n        to use as image input for the model.\r\n    input_shape: Optional shape tuple, only to be specified\r\n        if `include_top` is False.\r\n        It should have exactly 3 inputs channels.\r\n    pooling: Optional pooling mode for feature extraction\r\n        when `include_top` is `False`. Defaults to None.\r\n        - `None` means that the output of the model will be\r\n            the 4D tensor output of the\r\n            last convolutional layer.\r\n        - `avg` means that global average pooling\r\n            will be applied to the output of the\r\n            last convolutional layer, and thus\r\n            the output of the model will be a 2D tensor.\r\n        - `max` means that global max pooling will\r\n            be applied.\r\n    classes: Optional number of classes to classify images\r\n        into, only to be specified if `include_top` is True, and\r\n        if no `weights` argument is specified. Defaults to 1000 (number of\r\n        ImageNet classes).\r\n    classifier_activation: A `str` or callable. The activation function to use\r\n        on the \"top\" layer. Ignored unless `include_top=True`. Set\r\n        `classifier_activation=None` to return the logits of the \"top\" layer.\r\n        Defaults to 'softmax'.\r\n\r\n  Returns:\r\n    A `keras.Model` instance.\r\n\"\"\"\r\n\r\n\r\ndef EfficientNet(\r\n    width_coefficient,\r\n    depth_coefficient,\r\n    default_size,\r\n    dropout_rate=0.2,\r\n    drop_connect_rate=0.2,\r\n    depth_divisor=8,\r\n    activation='swish',\r\n    blocks_args='default',\r\n    model_name='efficientnet',\r\n    include_top=True,\r\n    weights='imagenet',\r\n    input_tensor=None,\r\n    input_shape=None,\r\n    pooling=None,\r\n    classes=1000,\r\n    classifier_activation='softmax'):\r\n  \"\"\"Instantiates the EfficientNet architecture using given scaling coefficients.\r\n\r\n  Reference:\r\n  - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](\r\n      https://arxiv.org/abs/1905.11946) (ICML 2019)\r\n\r\n  Optionally loads weights pre-trained on ImageNet.\r\n  Note that the data format convention used by the model is\r\n  the one specified in your Keras config at `~/.keras/keras.json`.\r\n\r\n  Arguments:\r\n    width_coefficient: float, scaling coefficient for network width.\r\n    depth_coefficient: float, scaling coefficient for network depth.\r\n    default_size: integer, default input image size.\r\n    dropout_rate: float, dropout rate before final classifier layer.\r\n    drop_connect_rate: float, dropout rate at skip connections.\r\n    depth_divisor: integer, a unit of network width.\r\n    activation: activation function.\r\n    blocks_args: list of dicts, parameters to construct block modules.\r\n    model_name: string, model name.\r\n    include_top: whether to include the fully-connected\r\n        layer at the top of the network.\r\n    weights: one of `None` (random initialization),\r\n          'imagenet' (pre-training on ImageNet),\r\n          or the path to the weights file to be loaded.\r\n    input_tensor: optional Keras tensor\r\n        (i.e. output of `layers.Input()`)\r\n        to use as image input for the model.\r\n    input_shape: optional shape tuple, only to be specified\r\n        if `include_top` is False.\r\n        It should have exactly 3 inputs channels.\r\n    pooling: optional pooling mode for feature extraction\r\n        when `include_top` is `False`.\r\n        - `None` means that the output of the model will be\r\n            the 4D tensor output of the\r\n            last convolutional layer.\r\n        - `avg` means that global average pooling\r\n            will be applied to the output of the\r\n            last convolutional layer, and thus\r\n            the output of the model will be a 2D tensor.\r\n        - `max` means that global max pooling will\r\n            be applied.\r\n    classes: optional number of classes to classify images\r\n        into, only to be specified if `include_top` is True, and\r\n        if no `weights` argument is specified.\r\n    classifier_activation: A `str` or callable. The activation function to use\r\n        on the \"top\" layer. Ignored unless `include_top=True`. Set\r\n        `classifier_activation=None` to return the logits of the \"top\" layer.\r\n\r\n  Returns:\r\n    A `keras.Model` instance.\r\n\r\n  Raises:\r\n    ValueError: in case of invalid argument for `weights`,\r\n      or invalid input shape.\r\n    ValueError: if `classifier_activation` is not `softmax` or `None` when\r\n      using a pretrained top layer.\r\n  \"\"\"\r\n  if blocks_args == 'default':\r\n    blocks_args = DEFAULT_BLOCKS_ARGS\r\n\r\n  if not (weights in {'imagenet', None} or file_io.file_exists_v2(weights)):\r\n    raise ValueError('The `weights` argument should be either '\r\n                     '`None` (random initialization), `imagenet` '\r\n                     '(pre-training on ImageNet), '\r\n                     'or the path to the weights file to be loaded.')\r\n\r\n  if weights == 'imagenet' and include_top and classes != 1000:\r\n    raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\r\n                     ' as true, `classes` should be 1000')\r\n\r\n  # Determine proper input shape\r\n  input_shape = imagenet_utils.obtain_input_shape(\r\n      input_shape,\r\n      default_size=default_size,\r\n      min_size=32,\r\n      data_format=backend.image_data_format(),\r\n      require_flatten=include_top,\r\n      weights=weights)\r\n\r\n  if input_tensor is None:\r\n    img_input = layers.Input(shape=input_shape)\r\n  else:\r\n    if not backend.is_keras_tensor(input_tensor):\r\n      img_input = layers.Input(tensor=input_tensor, shape=input_shape)\r\n    else:\r\n      img_input = input_tensor\r\n\r\n  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\r\n\r\n  def round_filters(filters, divisor=depth_divisor):\r\n    \"\"\"Round number of filters based on depth multiplier.\"\"\"\r\n    filters *= width_coefficient\r\n    new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\r\n    # Make sure that round down does not go down by more than 10%.\r\n    if new_filters < 0.9 * filters:\r\n      new_filters += divisor\r\n    return int(new_filters)\r\n\r\n  def round_repeats(repeats):\r\n    \"\"\"Round number of repeats based on depth multiplier.\"\"\"\r\n    return int(math.ceil(depth_coefficient * repeats))\r\n\r\n  # Build stem\r\n  x = img_input\r\n  x = layers.Rescaling(1. / 255.)(x)\r\n  x = layers.Normalization(axis=bn_axis)(x)\r\n\r\n  x = layers.ZeroPadding2D(\r\n      padding=imagenet_utils.correct_pad(x, 3),\r\n      name='stem_conv_pad')(x)\r\n  x = layers.Conv2D(\r\n      round_filters(32),\r\n      3,\r\n      strides=2,\r\n      padding='valid',\r\n      use_bias=False,\r\n      kernel_initializer=CONV_KERNEL_INITIALIZER,\r\n      name='stem_conv')(x)\r\n  x = layers.BatchNormalization(axis=bn_axis, name='stem_bn')(x)\r\n  x = layers.Activation(activation, name='stem_activation')(x)\r\n\r\n  # Build blocks\r\n  blocks_args = copy.deepcopy(blocks_args)\r\n\r\n  b = 0\r\n  blocks = float(sum(round_repeats(args['repeats']) for args in blocks_args))\r\n  for (i, args) in enumerate(blocks_args):\r\n    assert args['repeats'] > 0\r\n    # Update block input and output filters based on depth multiplier.\r\n    args['filters_in'] = round_filters(args['filters_in'])\r\n    args['filters_out'] = round_filters(args['filters_out'])\r\n\r\n    for j in range(round_repeats(args.pop('repeats'))):\r\n      # The first block needs to take care of stride and filter size increase.\r\n      if j > 0:\r\n        args['strides'] = 1\r\n        args['filters_in'] = args['filters_out']\r\n      x = block(\r\n          x,\r\n          activation,\r\n          drop_connect_rate * b / blocks,\r\n          name='block{}{}_'.format(i + 1, chr(j + 97)),\r\n          **args)\r\n      b += 1\r\n\r\n  # Build top\r\n  x = layers.Conv2D(\r\n      round_filters(1280),\r\n      1,\r\n      padding='same',\r\n      use_bias=False,\r\n      kernel_initializer=CONV_KERNEL_INITIALIZER,\r\n      name='top_conv')(x)\r\n  x = layers.BatchNormalization(axis=bn_axis, name='top_bn')(x)\r\n  x = layers.Activation(activation, name='top_activation')(x)\r\n  if include_top:\r\n    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\r\n    if dropout_rate > 0:\r\n      x = layers.Dropout(dropout_rate, name='top_dropout')(x)\r\n    imagenet_utils.validate_activation(classifier_activation, weights)\r\n    x = layers.Dense(\r\n        classes,\r\n        activation=classifier_activation,\r\n        kernel_initializer=DENSE_KERNEL_INITIALIZER,\r\n        name='predictions')(x)\r\n  else:\r\n    if pooling == 'avg':\r\n      x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\r\n    elif pooling == 'max':\r\n      x = layers.GlobalMaxPooling2D(name='max_pool')(x)\r\n\r\n  # Ensure that the model takes into account\r\n  # any potential predecessors of `input_tensor`.\r\n  if input_tensor is not None:\r\n    inputs = layer_utils.get_source_inputs(input_tensor)\r\n  else:\r\n    inputs = img_input\r\n\r\n  # Create model.\r\n  model = training.Model(inputs, x, name=model_name)\r\n\r\n  # Load weights.\r\n  if weights == 'imagenet':\r\n    if include_top:\r\n      file_suffix = '.h5'\r\n      file_hash = WEIGHTS_HASHES[model_name[-2:]][0]\r\n    else:\r\n      file_suffix = '_notop.h5'\r\n      file_hash = WEIGHTS_HASHES[model_name[-2:]][1]\r\n    file_name = model_name + file_suffix\r\n    weights_path = data_utils.get_file(\r\n        file_name,\r\n        BASE_WEIGHTS_PATH + file_name,\r\n        cache_subdir='models',\r\n        file_hash=file_hash)\r\n    model.load_weights(weights_path)\r\n  elif weights is not None:\r\n    model.load_weights(weights)\r\n  return model\r\n\r\n\r\ndef block(inputs,\r\n          activation='swish',\r\n          drop_rate=0.,\r\n          name='',\r\n          filters_in=32,\r\n          filters_out=16,\r\n          kernel_size=3,\r\n          strides=1,\r\n          expand_ratio=1,\r\n          se_ratio=0.,\r\n          id_skip=True):\r\n  \"\"\"An inverted residual block.\r\n\r\n  Arguments:\r\n      inputs: input tensor.\r\n      activation: activation function.\r\n      drop_rate: float between 0 and 1, fraction of the input units to drop.\r\n      name: string, block label.\r\n      filters_in: integer, the number of input filters.\r\n      filters_out: integer, the number of output filters.\r\n      kernel_size: integer, the dimension of the convolution window.\r\n      strides: integer, the stride of the convolution.\r\n      expand_ratio: integer, scaling coefficient for the input filters.\r\n      se_ratio: float between 0 and 1, fraction to squeeze the input filters.\r\n      id_skip: boolean.\r\n\r\n  Returns:\r\n      output tensor for the block.\r\n  \"\"\"\r\n  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\r\n\r\n  # Expansion phase\r\n  filters = filters_in * expand_ratio\r\n  if expand_ratio != 1:\r\n    x = layers.Conv2D(\r\n        filters,\r\n        1,\r\n        padding='same',\r\n        use_bias=False,\r\n        kernel_initializer=CONV_KERNEL_INITIALIZER,\r\n        name=name + 'expand_conv')(\r\n            inputs)\r\n    x = layers.BatchNormalization(axis=bn_axis, name=name + 'expand_bn')(x)\r\n    x = layers.Activation(activation, name=name + 'expand_activation')(x)\r\n  else:\r\n    x = inputs\r\n\r\n  # Depthwise Convolution\r\n  if strides == 2:\r\n    x = layers.ZeroPadding2D(\r\n        padding=imagenet_utils.correct_pad(x, kernel_size),\r\n        name=name + 'dwconv_pad')(x)\r\n    conv_pad = 'valid'\r\n  else:\r\n    conv_pad = 'same'\r\n  x = layers.DepthwiseConv2D(\r\n      kernel_size,\r\n      strides=strides,\r\n      padding=conv_pad,\r\n      use_bias=False,\r\n      depthwise_initializer=CONV_KERNEL_INITIALIZER,\r\n      name=name + 'dwconv')(x)\r\n  x = layers.BatchNormalization(axis=bn_axis, name=name + 'bn')(x)\r\n  x = layers.Activation(activation, name=name + 'activation')(x)\r\n\r\n  # Squeeze and Excitation phase\r\n  if 0 < se_ratio <= 1:\r\n    filters_se = max(1, int(filters_in * se_ratio))\r\n    se = layers.GlobalAveragePooling2D(name=name + 'se_squeeze')(x)\r\n    se = layers.Reshape((1, 1, filters), name=name + 'se_reshape')(se)\r\n    se = layers.Conv2D(\r\n        filters_se,\r\n        1,\r\n        padding='same',\r\n        activation=activation,\r\n        kernel_initializer=CONV_KERNEL_INITIALIZER,\r\n        name=name + 'se_reduce')(\r\n            se)\r\n    se = layers.Conv2D(\r\n        filters,\r\n        1,\r\n        padding='same',\r\n        activation='sigmoid',\r\n        kernel_initializer=CONV_KERNEL_INITIALIZER,\r\n        name=name + 'se_expand')(se)\r\n    x = layers.multiply([x, se], name=name + 'se_excite')\r\n\r\n  # Output phase\r\n  x = layers.Conv2D(\r\n      filters_out,\r\n      1,\r\n      padding='same',\r\n      use_bias=False,\r\n      kernel_initializer=CONV_KERNEL_INITIALIZER,\r\n      name=name + 'project_conv')(x)\r\n  x = layers.BatchNormalization(axis=bn_axis, name=name + 'project_bn')(x)\r\n  if id_skip and strides == 1 and filters_in == filters_out:\r\n    if drop_rate > 0:\r\n      x = layers.Dropout(\r\n          drop_rate, noise_shape=(None, 1, 1, 1), name=name + 'drop')(x)\r\n    x = layers.add([x, inputs], name=name + 'add')\r\n  return x\r\n\r\n\r\n@keras_export('keras.applications.efficientnet.EfficientNetB0',\r\n              'keras.applications.EfficientNetB0')\r\ndef EfficientNetB0(include_top=True,\r\n                   weights='imagenet',\r\n                   input_tensor=None,\r\n                   input_shape=None,\r\n                   pooling=None,\r\n                   classes=1000,\r\n                   classifier_activation='softmax',\r\n                   **kwargs):\r\n  return EfficientNet(\r\n      1.0,\r\n      1.0,\r\n      224,\r\n      0.2,\r\n      model_name='efficientnetb0',\r\n      include_top=include_top,\r\n      weights=weights,\r\n      input_tensor=input_tensor,\r\n      input_shape=input_shape,\r\n      pooling=pooling,\r\n      classes=classes,\r\n      classifier_activation=classifier_activation,\r\n      **kwargs)\r\n\r\n\r\n@keras_export('keras.applications.efficientnet.EfficientNetB1',\r\n              'keras.applications.EfficientNetB1')\r\ndef EfficientNetB1(include_top=True,\r\n                   weights='imagenet',\r\n                   input_tensor=None,\r\n                   input_shape=None,\r\n                   pooling=None,\r\n                   classes=1000,\r\n                   classifier_activation='softmax',\r\n                   **kwargs):\r\n  return EfficientNet(\r\n      1.0,\r\n      1.1,\r\n      240,\r\n      0.2,\r\n      model_name='efficientnetb1',\r\n      include_top=include_top,\r\n      weights=weights,\r\n      input_tensor=input_tensor,\r\n      input_shape=input_shape,\r\n      pooling=pooling,\r\n      classes=classes,\r\n      classifier_activation=classifier_activation,\r\n      **kwargs)\r\n\r\n\r\n@keras_export('keras.applications.efficientnet.EfficientNetB2',\r\n              'keras.applications.EfficientNetB2')\r\ndef EfficientNetB2(include_top=True,\r\n                   weights='imagenet',\r\n                   input_tensor=None,\r\n                   input_shape=None,\r\n                   pooling=None,\r\n                   classes=1000,\r\n                   classifier_activation='softmax',\r\n                   **kwargs):\r\n  return EfficientNet(\r\n      1.1,\r\n      1.2,\r\n      260,\r\n      0.3,\r\n      model_name='efficientnetb2',\r\n      include_top=include_top,\r\n      weights=weights,\r\n      input_tensor=input_tensor,\r\n      input_shape=input_shape,\r\n      pooling=pooling,\r\n      classes=classes,\r\n      classifier_activation=classifier_activation,\r\n      **kwargs)\r\n\r\n\r\n@keras_export('keras.applications.efficientnet.EfficientNetB3',\r\n              'keras.applications.EfficientNetB3')\r\ndef EfficientNetB3(include_top=True,\r\n                   weights='imagenet',\r\n                   input_tensor=None,\r\n                   input_shape=None,\r\n                   pooling=None,\r\n                   classes=1000,\r\n                   classifier_activation='softmax',\r\n                   **kwargs):\r\n  return EfficientNet(\r\n      1.2,\r\n      1.4,\r\n      300,\r\n      0.3,\r\n      model_name='efficientnetb3',\r\n      include_top=include_top,\r\n      weights=weights,\r\n      input_tensor=input_tensor,\r\n      input_shape=input_shape,\r\n      pooling=pooling,\r\n      classes=classes,\r\n      classifier_activation=classifier_activation,\r\n      **kwargs)\r\n\r\n\r\n@keras_export('keras.applications.efficientnet.EfficientNetB4',\r\n              'keras.applications.EfficientNetB4')\r\ndef EfficientNetB4(include_top=True,\r\n                   weights='imagenet',\r\n                   input_tensor=None,\r\n                   input_shape=None,\r\n                   pooling=None,\r\n                   classes=1000,\r\n                   classifier_activation='softmax',\r\n                   **kwargs):\r\n  return EfficientNet(\r\n      1.4,\r\n      1.8,\r\n      380,\r\n      0.4,\r\n      model_name='efficientnetb4',\r\n      include_top=include_top,\r\n      weights=weights,\r\n      input_tensor=input_tensor,\r\n      input_shape=input_shape,\r\n      pooling=pooling,\r\n      classes=classes,\r\n      classifier_activation=classifier_activation,\r\n      **kwargs)\r\n\r\n\r\n@keras_export('keras.applications.efficientnet.EfficientNetB5',\r\n              'keras.applications.EfficientNetB5')\r\ndef EfficientNetB5(include_top=True,\r\n                   weights='imagenet',\r\n                   input_tensor=None,\r\n                   input_shape=None,\r\n                   pooling=None,\r\n                   classes=1000,\r\n                   classifier_activation='softmax',\r\n                   **kwargs):\r\n  return EfficientNet(\r\n      1.6,\r\n      2.2,\r\n      456,\r\n      0.4,\r\n      model_name='efficientnetb5',\r\n      include_top=include_top,\r\n      weights=weights,\r\n      input_tensor=input_tensor,\r\n      input_shape=input_shape,\r\n      pooling=pooling,\r\n      classes=classes,\r\n      classifier_activation=classifier_activation,\r\n      **kwargs)\r\n\r\n\r\n@keras_export('keras.applications.efficientnet.EfficientNetB6',\r\n              'keras.applications.EfficientNetB6')\r\ndef EfficientNetB6(include_top=True,\r\n                   weights='imagenet',\r\n                   input_tensor=None,\r\n                   input_shape=None,\r\n                   pooling=None,\r\n                   classes=1000,\r\n                   classifier_activation='softmax',\r\n                   **kwargs):\r\n  return EfficientNet(\r\n      1.8,\r\n      2.6,\r\n      528,\r\n      0.5,\r\n      model_name='efficientnetb6',\r\n      include_top=include_top,\r\n      weights=weights,\r\n      input_tensor=input_tensor,\r\n      input_shape=input_shape,\r\n      pooling=pooling,\r\n      classes=classes,\r\n      classifier_activation=classifier_activation,\r\n      **kwargs)\r\n\r\n\r\n@keras_export('keras.applications.efficientnet.EfficientNetB7',\r\n              'keras.applications.EfficientNetB7')\r\ndef EfficientNetB7(include_top=True,\r\n                   weights='imagenet',\r\n                   input_tensor=None,\r\n                   input_shape=None,\r\n                   pooling=None,\r\n                   classes=1000,\r\n                   classifier_activation='softmax',\r\n                   **kwargs):\r\n  return EfficientNet(\r\n      2.0,\r\n      3.1,\r\n      600,\r\n      0.5,\r\n      model_name='efficientnetb7',\r\n      include_top=include_top,\r\n      weights=weights,\r\n      input_tensor=input_tensor,\r\n      input_shape=input_shape,\r\n      pooling=pooling,\r\n      classes=classes,\r\n      classifier_activation=classifier_activation,\r\n      **kwargs)\r\n\r\n\r\nEfficientNetB0.__doc__ = BASE_DOCSTRING.format(name='EfficientNetB0')\r\nEfficientNetB1.__doc__ = BASE_DOCSTRING.format(name='EfficientNetB1')\r\nEfficientNetB2.__doc__ = BASE_DOCSTRING.format(name='EfficientNetB2')\r\nEfficientNetB3.__doc__ = BASE_DOCSTRING.format(name='EfficientNetB3')\r\nEfficientNetB4.__doc__ = BASE_DOCSTRING.format(name='EfficientNetB4')\r\nEfficientNetB5.__doc__ = BASE_DOCSTRING.format(name='EfficientNetB5')\r\nEfficientNetB6.__doc__ = BASE_DOCSTRING.format(name='EfficientNetB6')\r\nEfficientNetB7.__doc__ = BASE_DOCSTRING.format(name='EfficientNetB7')\r\n\r\n\r\n@keras_export('keras.applications.efficientnet.preprocess_input')\r\ndef preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\r\n  return x\r\n\r\n\r\n@keras_export('keras.applications.efficientnet.decode_predictions')\r\ndef decode_predictions(preds, top=5):\r\n  return imagenet_utils.decode_predictions(preds, top=top)\r\n\r\n\r\ndecode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\r\n```\r\n\r\n\r\nIs there any reason why dilated convolution in tf.keras.layers.Conv2D runs extremely slower in cpu?\r\nThanks.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@renjie-liu could you take a look?", "Hi, can you attach the two tflite models? thanks!", "@renjie-liu \r\nHello, I am attaching two models\r\n\r\nThanks\r\n\r\nBtw, I may need to delete the link soon due to the project policy.", "Thanks a lot for sharing!\r\n\r\nBased on the two models, the dilated model seems simply to be larger.\r\n\r\nTake the first `conv` op for example:\r\n\r\nThe base has\r\ninput: 1x225x225x3\r\nweight:32x3x3x3\r\n**output:1x112x112x3**\r\n\r\nThe dilated has:\r\ninput: 1x225x225x3\r\nweight:32x3x3x3\r\n**output:1x221x221x3**\r\n\r\nDue to the output size is being ~4 times larger, causing the dilated model has more operations.\r\n\r\nSo I think this is intended.\r\n", "Oh I just missed that I also modified stride on conv2d.\r\nThanks for checking the model!"]}, {"number": 49122, "title": "Transpose Elimination for TF Dialect", "body": "Pull out distributed transposes from binary operators in canonicalization pass", "comments": ["@ksugama  Can you please resolve conflicts? Thanks!", "@gbaned Hi! Yes I just resolved the conflicts. Waiting for any build errors from Jenkins to show up, if any.", "@ksugama Can you please resolve conflicts? Thanks!"]}, {"number": 49121, "title": "<release 2.4>-<rc2> cherry-pick request: [tf.data] Fix snapshot segfault when using repeat and prefetch.", "body": "[tf.data] Fix snapshot segfault when using repeat and prefetch.\r\n\r\nFixes: https://github.com/tensorflow/tensorflow/issues/48903.\r\n\r\n`input_->MakeIterator` refs the dataset in\r\nhttps://github.com/tensorflow/tensorflow/blob/a9cf3a0e4b419630f0183b0cc4e48e0641a62721/tensorflow/core/framework/dataset.cc#L679\r\nSo we don't need to call `input_->Ref()`. Otherwise, if\r\n`SnapshotDatasetV2Op::Dataset::Iterator::Reader::Initialize` returns an error,\r\n`input_->Ref()` isn't called, but the destructor still calls `input_->Unref()`.\r\n\r\nIf `InitializeIterator` returns an error, the iterator_ needs to be reset to\r\nnullptr. Otherwise, if GetNextInternal is called a second time,\r\n`iterator_->GetNext` may dereference a null `input_impl_`.\r\n\r\n@ashahab", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49121) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 49120, "title": "Validate that a and b are proper sparse tensors", "body": "PiperOrigin-RevId: 373248068\nChange-Id: I0a2041a0747901b3f00387a6a3bce9bca6b0b3b1", "comments": []}, {"number": 49119, "title": "How does TFLite model and TFLite-quantized model converted?", "body": "### 1. System information\r\n Linux Ubuntu 16.04\r\n\r\n### 2. Question\r\nI downloaded two tflite models from https://www.tensorflow.org/lite/guide/hosted_models\r\n1. https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_0.75_224.tgz\r\n2.https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.75_224_quant.tgz\r\nThey are both Mobilenet_V1_0.75_224 but one if float version and one is quant version. \r\n\r\nWhen I checked the weights in first CONV (MobilenetV1/Conv2d_0/weights) node, I can notice that the first value in the float model is 0.15756385028362274 and the first value in the quantized model is 171. However, as the quantization information provided in quantized model, the float value is 0.018297843635082245 * (q - 153), which is 0.3293612003326416 for q == 171, which is not matched with float model.\r\n\r\nMay I know why is that? Does it mean that although two models are both from  Mobilenet_V1_0.75_224 but they are not converted from each other?\r\n\r\nAlso, even if two models weights are not matched, I can get nearly the same output classification when I tried several pictures. It seems that both models are correct but with different weights. Does it mean that two models are trained in different ways?\r\n", "comments": ["@daverim could you triage this issue?", "These are indeed different models, mobilenet_v1_0.75_224_quant.tgz was trained with quantization aware training for best possible accuracy and so has a slightly different structure", "> These are indeed different models, mobilenet_v1_0.75_224_quant.tgz was trained with quantization aware training for best possible accuracy and so has a slightly different structure\r\n\r\nThanks, in that case, is there will be any problem if I transferred back the mobilenet_v1_0.75_224_quant to non_quant version by using the quantization info provided in the tflite file? I tried with several inputs and the middle layer outputs getting bigger and bigger and lost track in the softmax layer at the last (the power becomes 10^6 level and exp() cannot handle that). I think that's because compared with normal mobilenet_v1_0.75_224, mobilenet_v1_0.75_224_quant does not have relu6 at the end of each conv, which constrain the output min/max. However, since the mobilenet_v1_0.75_224_quant provided the quantization min/max  at each layer, I think it should ensure that whatever the input is, the output would not exceed the min/max constrain, am I correct?\r\n\r\nOr is there another way I could do to convert the  mobilenet_v1_0.75_224_quant to its non_quant version?\r\n\r\nThanks!", "Closing this issue since above FR to cover this topic. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49119\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49119\">No</a>\n"]}, {"number": 49118, "title": "Fix heap OOB / undefined behavior in `RaggedTensorToTensor`", "body": "PiperOrigin-RevId: 373244623\nChange-Id: I2d6cbbc8c67b238a8815bf58097f7586d87c54f2", "comments": []}, {"number": 49117, "title": "Support all datatypes for Xtensa softmax implementation.", "body": " * This allows the micro_speech and person_detect examples to be usable with the optimized xtensa kernels.\r\n * enabled all the softmax kernel test cases for Xtensa (since we have a fallback to the reference kernels).\r\n * The keyword_benchmark specific variant is moved to its own .cc file since that allows the Xtensa linker to properly drop unsused symbols and ensures that the binary size for the keyword_benchmark is unchanged.\r\n    \r\nManually tested that the following tests pass:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=fusion_f1 OPTIMIZED_KERNEL_DIR=xtensa XTENSA_CORE=F1_190305_swupgrade test_person_detection_test_int8 -j8\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=fusion_f1 OPTIMIZED_KERNEL_DIR=xtensa XTENSA_CORE=F1_190305_swupgrade test_micro_speech_test -j8\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=fusion_f1 OPTIMIZED_KERNEL_DIR=xtensa XTENSA_CORE=F1_190305_swupgrade test_kernel_softmax_test -j8\r\n```\r\n\r\nConfirmed that the binary size for the keyword_benchmark is unchanged (relative to tip of tree):\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=fusion_f1 OPTIMIZED_KERNEL_DIR=xtensa XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nGives:\r\n```\r\n   text    data     bss     dec     hex filename\r\n   70368   41140   24856  136364   214ac tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nRelated bug: http://b/188581097", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 49116, "title": "Update version numbers for TensorFlow 2.5.0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 5 -> 5\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.5.0-rc3\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.5.0rc3\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 49115, "title": "Fix double \"/\" in include in test_helpers.h", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/49102", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 49114, "title": "Remove unused MicroInterpreter::tensor() API.", "body": "The implementation for this function was always allocating a persistent buffer which would mean that calling this function repeatedly would unexpectedly result in an error as a result of running out of space in the arena (basically a memory leak).\r\n\r\nAdditionally, it appears that the function was only being used to test ResetVariableTensor and that test case has also been removed with this change.\r\n\r\nhttp://b/187845286 will be used to add a unit test for ResetVaiableTensor.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> Do we have any way to test ResetVariableTensor once we remove its test here? Other than that looks good.\r\n\r\nNot immediately. I have filed a bug for that -- will figure out how we can test it in a bit.", "Got it. I suspect we can add a test along with subgraph support."]}, {"number": 49113, "title": "Patch mkl_serv_intel_cpu() to speed up MKL on AMD CPUs", "body": "Per https://www.agner.org/optimize/blog/read.php?i=49#1022 and https://danieldk.eu/Posts/2020-08-31-MKL-Zen.html patching `int mkl_serv_intel_cpu()` to return 1 will cause MKL to run significantly fast under AMD CPUs. The link provides background information.\r\n\r\nIs it possible to patch this out-of-the-box in the tensorflow-mkl package?\r\n\r\nNote: I am not associated with the authors of these blogs. I am just a TensorFlow end-user with an AMD processor.", "comments": ["@cowwoc Thank you share the info.\r\nBut the method is not official proposal and not verified fully.\r\nIt could make the special function to be improved for performance, but there would be potential risk of Tensorflow functions and performance.\r\n\r\nIntel and AMP CPU always have difference. It's possible to cheat MKL to speed up one function. But it's also possible to trigger MKL run hardware related code in wrong CPU, and make mistake/crash.\r\n\r\nSo, we don't recommend to use this method in official product.\r\n\r\nThank you!", "So, excuse me for pointing this out but... you work for Intel. There is a conflict of interest here.\r\n\r\nThe web is full of online reports of people using `MKL_DEBUG_CPU_TYPE=5` to fool MKL into thinking that it's running on an Intel CPU and performance improved by 30%-300% (e.g. https://www.reddit.com/r/MachineLearning/comments/f2pbvz/discussion_workaround_for_mkl_on_amd/)\r\n\r\nThe aforementioned workaround was disabled by Intel in a subsequent release but the principle remains the same.\r\n\r\nI propose patching this function to return a value that can be overridden using an environment variable (same concept as `MKL_DEBUG_CPU_TYPE=5`) and let people experiment with it. If enough people report that it works for them and is stable (which seems to have been the case in the past) then we can flip the default value.", "@cowwoc Yes, I'm working for Intel. :)\r\nThe MKL and Tensorflow-MKL are contributed by Intel.\r\nThey are only designed for and verified on Intel CPU.\r\nSo, other CPUs are out of our scope.\r\n\r\nIt's unknown the result of running on other CPUs.", "@cowwoc  Thank you for submitting the issue. The support for MKL in Tensforflow-mkl was removed late last year and is no longer supported as of TF2.4. (https://github.com/tensorflow/tensorflow/pull/43586). It has been replaced by oneDNN which is opensourced at https://github.com/oneapi-src/oneDNN. You can submit a patch to oneDNN on github.\r\n", "Thank you for the clarification. Closed in favor of https://github.com/oneapi-src/oneDNN/issues/1056.", "@cowwoc Thank you!"]}, {"number": 49112, "title": " tensorflow/core/grappler/costs/op_level_cost_estimator.cc:654] Check failed: iz % kz == 0 (256 vs. 0)Input channel 256 is not a multiple of filter channel 512.", "body": "I am implementing the following code i.e CBAM module in a network, however, I am getting the **following error:**\r\n\r\nF tensorflow/core/grappler/costs/op_level_cost_estimator.cc:654] Check failed: iz % kz == 0 (256 vs. 0)Input channel 256 is not a multiple of filter channel 512\r\n\r\nHere is the code of the CBAM block\r\n\r\n`import tensorflow as tf\r\n\r\ndef CBAM(input, reduction):\r\n    \"\"\"\r\n    @Convolutional Block Attention Module\r\n    \"\"\"\r\n\r\n    _, channel, width, height = input.get_shape()  # (B, W, H, C)\r\n\r\n    input = tf.transpose(input, perm=[0, 3, 2, 1]) \r\n    # channel attention\r\n    x_mean = tf.reduce_mean(input, axis=(1, 2), keepdims=True)   # (B, 1, 1, C)\r\n    x_mean = tf.layers.conv2d(x_mean, channel // reduction, 1, activation=tf.nn.relu, name='CA1', reuse=tf.AUTO_REUSE)  # (B, 1, 1, C // r)\r\n    x_mean = tf.layers.conv2d(x_mean, channel, 1, name='CA2', reuse=tf.AUTO_REUSE)   # (B, 1, 1, C)\r\n\r\n    x_max = tf.reduce_max(input, axis=(1, 2), keepdims=True)  # (B, 1, 1, C)\r\n    x_max = tf.layers.conv2d(x_max, channel // reduction, 1, activation=tf.nn.relu, name='CA1', reuse=tf.AUTO_REUSE)\r\n    # (B, 1, 1, C // r)\r\n    x_max = tf.layers.conv2d(x_max, channel, 1, name='CA2', reuse=tf.AUTO_REUSE)  # (B, 1, 1, C)\r\n\r\n    x = tf.add(x_mean, x_max)   # (B, 1, 1, C)\r\n    x = tf.nn.sigmoid(x)        # (B, 1, 1, C)\r\n    x = tf.multiply(input, x)   # (B, W, H, C)\r\n\r\n    # spatial attention\r\n    y_mean = tf.reduce_mean(x, axis=3, keepdims=True)  # (B, W, H, 1)\r\n    y_max = tf.reduce_max(x, axis=3, keepdims=True)  # (B, W, H, 1)\r\n    y = tf.concat([y_mean, y_max], axis=-1)     # (B, W, H, 2)\r\n    y = tf.layers.conv2d(y, 1, 7, padding='same', activation=tf.nn.sigmoid)    # (B, W, H, 1)\r\n    y = tf.multiply(x, y)  # (B, W, H, C)\r\n\r\n    y = tf.transpose(y, perm=[0, 3, 2, 1])\r\n    return y`\r\n\r\n\r\n**I have called CBAM module in-network as given below:**\r\n\r\n`layer17 = tf.layers.conv2d(layer16, 512, 3,\r\n                           padding=\"same\",\r\n                           activation=tf.nn.relu,\r\n                           dilation_rate=2,\r\n                           data_format=self._data_format,\r\n                           name=\"conv5/conv5_3\")\r\n\r\nlayer17=CBAM(layer17, 8)\r\nlayer18 = tf.layers.max_pooling2d(layer17, 2, 1,\r\n                                  padding=\"same\",\r\n                                  data_format=self._data_format)` \r\n", "comments": ["@Wmir \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version,stand alone code/colab gist to reproduce the issue faced].Thanks\r\n", "hi, UsharaniPagadala can you please elaborate a little bit more which template I need to fill? In the template section, i was asked three things that were not helpful for my scenario.", "@Wmir \r\nCould you please reopen the issue with the clear details and provide a colab gist along with the dependencies to analyse the issue reported if the issue still persist.Thanks"]}, {"number": 49111, "title": "[Pluggable Device] Adding more DEVICE_DEFAULT kernels.", "body": "The checkin extends the device default kernels to\r\n    - int32 operations\r\n    - certain resource operations\r\n    - TensorArray and TensorList operations\r\n\r\n@penpornk ", "comments": ["This PR was automatically rolled back last night because it broke some of our internal tests. I'll investigate."]}, {"number": 49110, "title": "[Pluggable Device] Adding more DEVICE_DEFAULT kernels.", "body": "The checkin extends the device default kernels to\r\n    - int32 operations\r\n    - certain resource operations\r\n    - TensorArray and TensorList operations", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49110) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 49108, "title": "[ROCm] Adding complex types to existing hipsparse functionality ", "body": "Added complex support for:\r\n- csrgemm\r\n- csrmm\r\n- csr2coo\r\n- coo2csr\r\n- csrmm2\r\n\r\n@cheshire for review.", "comments": ["@stevenireeves  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned \r\n\r\nThis was addressed in https://github.com/tensorflow/tensorflow/commit/061dec2ab7fd91f369598b2223ba388e5b10777d\r\n\r\nThe Ubuntu Sanity check on my PR was run a few hours before this commit was merged in.", "@gbaned I have merged the changes from 061dec2 to solve the pylint error in the Sanity Check. ", "@cheshire @gbaned Is this PR moving forward? ", "This was rolled back because the test dataset is too big and caused test failures for csr_sparse_matrix_ops_test.", "@akuegel, can you elaborate on what you mean? These changes should not have affected anything but on the ROCm side. What are the next steps so I can get these commits back in? ", "> @akuegel, can you elaborate on what you mean? These changes should not have affected anything but on the ROCm side. What are the next steps so I can get these commits back in? \n\nI agree with you, looking at your changes it also looked unlikely to me that they are causing the failures. I commented in the internal rollback change, maybe it gets rolled forward again. I didn't do the rollback, maybe I also misunderstood the reason. Just thought you should at least be notified that it was rolled back.", "@akuegel Thanks for the update, can you let me know if it is rolled forward again?", "I've rolled back the rollback."]}, {"number": 49107, "title": "TensorFlow won't detect GPU", "body": "Hi, I'm beginning to use TensorFlow for a project and I'm having some issues with it picking up my GPU. When I run print(tf.config.list_physical_devices('GPU')) it returns an empty list instead.\r\n\r\nI'm running it on TensorFlow 2.3.0, cudatoolkit 10.1 and my GPU is CUDA enabled.\r\n\r\nAny help is very much appreciated.\r\n\r\n\r\n", "comments": ["@Moronixo , Can you please let me know your cuDNN version and python version.", "My cuDNN version is 7.6.5 and python version is 3.8.", "@Moronixo Can you just try `print(tf.config.list_physical_devices())` sometimes if your GPU is `XLA_GPU` it won't list under GPU.", "@sachinprasadhs When I try that I get [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] returned", "@Moronixo , Can you tell me your OS,GPU and IDE which you are trying.", "@sachinprasadhs My OS is Windows 10, GPU is GeForce GTX 1050 Ti and my IDE is PyCharm.", "@Moronixo try with nvidia-smi on your Terminal to check that GPUs are visible.", "@sachinprasadhs I get this when I run nvidia-smi. ![nvidia_smi_capture](https://user-images.githubusercontent.com/84026099/117855266-36cf9b00-b282-11eb-854c-2e822dca5e3f.PNG)\r\n", "@Moronixo seems normal, can you try installing tensorflow-gpu using `pip install tensorflow-gpu==2.3.0`. If your gpu is still not listed then one of your installation might be corrupted you can download the latest driver for your gpu from [here](https://www.nvidia.com/Download/driverResults.aspx/175403/en-us) ,cuda 10.1 from [here](https://developer.nvidia.com/cuda-10.1-download-archive-update2?target_os=Windows&target_arch=x86_64&target_version=10), cuDNN from [here](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/production/10.1_20191031/cudnn-10.1-windows10-x64-v7.6.5.32.zip) and follow [this](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/) guide to setup again.", "@sachinprasadhs Installing tensorflow-gpu has made it show up,\r\n\r\n I've added the result below just in case it's the wrong thing. Thank you so much for your help!\r\n![clarify](https://user-images.githubusercontent.com/84026099/117961886-86f83d00-b316-11eb-8838-5a5f0d458d34.PNG)\r\n", "@Moronixo Yes, it's listing properly now, you can go ahead and close the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49107\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49107\">No</a>\n"]}, {"number": 49105, "title": "tensorflow.python.framework.errors_impl.NotFoundError: Key BeamSearchDecoderStep/multi_rnn_cell/cell_0_attention/attention_wrapper/lstm_cell_9/bias not found in checkpoint", "body": "**System information**\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.6.2\r\n\r\n**Problem**\r\nI am trying to upgrade the LAS model from tensorflow 1.8.0 version to 2.4.0. There is no problem in training the model, but in the testing phase, loading the model will show that there is a parameter not found. I printed the saved model file, there is a parameter named 1 in it. I don't understand where the problem is. I would be very grateful if you can answer my question!\r\n\r\n**Error Message**\r\n```\r\n2021-05-11 17:09:25.582423: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-05-11 17:09:25.582893: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nINFO:tensorflow:Using config: {'_model_dir': './data_kss/Kspon_dataset/model_test', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Building listener\r\n2021-05-11 17:09:33.837384: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-05-11 17:09:33.841199: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n2021-05-11 17:09:33.841721: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-05-11 17:09:33.850617: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-I630CDV\r\n2021-05-11 17:09:33.851304: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-I630CDV\r\n2021-05-11 17:09:33.852078: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-05-11 17:09:33.853485: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nINFO:tensorflow:Building speller\r\nWARNING:tensorflow:From C:\\Users\\yangrui\\Desktop\\PythonProject\\Korean_Speech\\las\\model.py:346: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2021-05-11T17:09:47Z\r\nINFO:tensorflow:Graph was finalized.\r\n2021-05-11 17:09:48.052221: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nINFO:tensorflow:Restoring parameters from ./data_kss/Kspon_dataset/model_test\\model.ckpt-0\r\n2021-05-11 17:09:48.135902: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n2021-05-11 17:09:48.321843: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at save_restore_v2_ops.cc:205 : Not found: Key BeamSearchDecoderStep/multi_rnn_cell/cell_0_attention/attention_wrapper/lstm_cell_9/bias not found in checkpoint\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1360, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1453, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key BeamSearchDecoderStep/multi_rnn_cell/cell_0_attention/attention_wrapper/lstm_cell_9/bias not found in checkpoint\r\n\t [[{{node save/RestoreV2}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1298, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 968, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1191, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1369, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1394, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key BeamSearchDecoderStep/multi_rnn_cell/cell_0_attention/attention_wrapper/lstm_cell_9/bias not found in checkpoint\r\n\t [[node save/RestoreV2 (defined at \\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:1647) ]]\r\n\r\nOriginal stack trace for 'save/RestoreV2':\r\n  File \"/Users/yangrui/Desktop/PythonProject/Korean_Speech/eval.py\", line 114, in <module>\r\n    main(args)\r\n  File \"/Users/yangrui/Desktop/PythonProject/Korean_Speech/eval.py\", line 85, in main\r\n    input_fn=lambda: input_fn(\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 467, in evaluate\r\n    name=name)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 510, in _actual_eval\r\n    return _evaluate()\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 499, in _evaluate\r\n    output_dir=self.eval_dir(name))\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1647, in _evaluate_run\r\n    config=self._session_config)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\evaluation.py\", line 269, in _evaluate_once\r\n    session_creator=session_creator, hooks=hooks) as session:\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1038, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 749, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1231, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1236, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 902, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 660, in create_session\r\n    self._scaffold.finalize()\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 235, in finalize\r\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 606, in _get_saver_or_default\r\n    saver = Saver(sharded=True, allow_empty=True)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 835, in __init__\r\n    self.build()\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 847, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 885, in _build\r\n    build_restore=build_restore)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 509, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 388, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 335, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 582, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1510, in restore_v2\r\n    name=name)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 750, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3536, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1990, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\", line 70, in get_tensor\r\n    self, compat.as_bytes(tensor_str))\r\nRuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1308, in restore\r\n    names_to_keys = object_graph_key_mapping(save_path)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1626, in object_graph_key_mapping\r\n    object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\", line 74, in get_tensor\r\n    error_translator(e)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\", line 35, in error_translator\r\n    raise errors_impl.NotFoundError(None, None, error_message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/yangrui/Desktop/PythonProject/Korean_Speech/eval.py\", line 114, in <module>\r\n    main(args)\r\n  File \"C:/Users/yangrui/Desktop/PythonProject/Korean_Speech/eval.py\", line 85, in main\r\n    input_fn=lambda: input_fn(\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 467, in evaluate\r\n    name=name)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 510, in _actual_eval\r\n    return _evaluate()\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 499, in _evaluate\r\n    output_dir=self.eval_dir(name))\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1647, in _evaluate_run\r\n    config=self._session_config)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\evaluation.py\", line 269, in _evaluate_once\r\n    session_creator=session_creator, hooks=hooks) as session:\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1038, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 749, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1231, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1236, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 902, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 669, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\session_manager.py\", line 295, in prepare_session\r\n    config=config)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\session_manager.py\", line 209, in _restore_checkpoint\r\n    saver.restore(sess, checkpoint_filename_with_path)\r\n  File \"C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1314, in restore\r\n    err, \"a Variable name or other graph key that is missing\")\r\ntensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\nKey BeamSearchDecoderStep/multi_rnn_cell/cell_0_attention/attention_wrapper/lstm_cell_9/bias not found in checkpoint\r\n\t [[node save/RestoreV2 (defined at \\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py:1647) ]]\r\n\r\nOriginal stack trace for 'save/RestoreV2':\r\n  File \"/Users/yangrui/Desktop/PythonProject/Korean_Speech/eval.py\", line 114, in <module>\r\n    main(args)\r\n  File \"/Users/yangrui/Desktop/PythonProject/Korean_Speech/eval.py\", line 85, in main\r\n    input_fn=lambda: input_fn(\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 467, in evaluate\r\n    name=name)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 510, in _actual_eval\r\n    return _evaluate()\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 499, in _evaluate\r\n    output_dir=self.eval_dir(name))\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1647, in _evaluate_run\r\n    config=self._session_config)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\evaluation.py\", line 269, in _evaluate_once\r\n    session_creator=session_creator, hooks=hooks) as session:\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1038, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 749, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1231, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1236, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 902, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 660, in create_session\r\n    self._scaffold.finalize()\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 235, in finalize\r\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 606, in _get_saver_or_default\r\n    saver = Saver(sharded=True, allow_empty=True)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 835, in __init__\r\n    self.build()\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 847, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 885, in _build\r\n    build_restore=build_restore)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 509, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 388, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 335, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 582, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1510, in restore_v2\r\n    name=name)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 750, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3536, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1990, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n**Part of my code:**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.util import nest\r\nimport tensorflow_addons as tfa\r\n\r\nfrom las.ops import lstm_cell\r\nfrom las.ops import pyramidal_bilstm\r\n# assert tf.executing_eagerly()\r\n__all__ = [\r\n    'listener',\r\n    'speller',\r\n]\r\n\r\n\r\n\"\"\"Reference: https://github.com/tensorflow/nmt/blob/master/nmt/gnmt_model.py\"\"\"\r\n\r\n\r\nclass AttentionMultiCell(tf.keras.layers.StackedRNNCells):\r\n# class AttentionMultiCell(tf.compat.v1.nn.rnn_cell.MultiRNNCell):\r\n    \"\"\"A MultiCell with attention style.\"\"\"\r\n\r\n    def __init__(self, attention_cell, cells, use_new_attention=False):\r\n        \"\"\"Creates a AttentionMultiCell.\r\n        Args:\r\n          attention_cell: An instance of AttentionWrapper.\r\n          cells: A list of RNNCell wrapped with AttentionInputWrapper.\r\n          use_new_attention: Whether to use the attention generated from current\r\n            step bottom layer's output. Default is False.\r\n        \"\"\"\r\n        cells = [attention_cell] + cells\r\n        self.use_new_attention = use_new_attention\r\n        super(AttentionMultiCell, self).__init__(\r\n            cells)\r\n\r\n    def __call__(self, inputs, state, training=False, scope=None):\r\n        \"\"\"Run the cell with bottom layer's attention copied to all upper layers.\"\"\"\r\n        if not nest.is_sequence(state):\r\n            raise ValueError(\r\n                \"Expected state to be a tuple of length %d, but received: %s\"\r\n                % (len(self.state_size), state))\r\n\r\n        with tf.compat.v1.variable_scope(scope or \"multi_rnn_cell\"):\r\n            new_states = []\r\n\r\n            with tf.compat.v1.variable_scope(\"cell_0_attention\"):\r\n                attention_cell = self.cells[0]\r\n                attention_state = state[0]\r\n                cur_inp, new_attention_state = attention_cell(\r\n                    inputs, attention_state)\r\n\r\n                new_states.append(new_attention_state)\r\n\r\n            for i in range(1, len(self.cells)):\r\n                with tf.compat.v1.variable_scope(\"cell_%d\" % i):\r\n\r\n                    cell = self.cells[i]\r\n                    cur_state = state[i]\r\n\r\n                    if self.use_new_attention:\r\n                        cur_inp = tf.concat(\r\n                            [cur_inp, new_attention_state.attention], -1)\r\n                    else:\r\n                        cur_inp = tf.concat(\r\n                            [cur_inp, attention_state.attention], -1)\r\n\r\n                    cur_inp, new_state = cell(cur_inp, cur_state)\r\n                    new_states.append(new_state)\r\n        return cur_inp, new_states\r\n\r\n\r\nclass CustomAttention(tfa.seq2seq.LuongAttention):\r\n    def __init__(self,\r\n                 num_units,\r\n                 memory,\r\n                 memory_sequence_length=None,\r\n                 scale=False,\r\n                 probability_fn=None,\r\n                 score_mask_value=None,\r\n                 dtype=None,\r\n                 name=\"CustomAttention\"):\r\n\r\n        super(CustomAttention, self).__init__(\r\n            num_units=num_units,\r\n            memory=memory,\r\n            memory_sequence_length=memory_sequence_length,\r\n            scale=scale,\r\n            probability_fn=probability_fn,\r\n            score_mask_value=score_mask_value,\r\n            dtype=dtype,\r\n            name=name)\r\n\r\n        self._query_layer = tf.compat.v1.layers.Dense(\r\n            num_units, name='query_layer', use_bias=False, dtype=dtype)\r\n\r\n        self._keys = tf.nn.relu(self.keys)\r\n\r\n    def __call__(self, query, state):\r\n        processed_query = tf.nn.relu(self.query_layer(query))\r\n\r\n        return super(CustomAttention, self).__call__(processed_query, state)\r\n\r\n\r\ndef listener(encoder_inputs,\r\n             source_sequence_length,\r\n             mode,\r\n             hparams):\r\n\r\n    if hparams['use_pyramidal']:\r\n        return pyramidal_bilstm(encoder_inputs, source_sequence_length, mode, hparams)\r\n    else:\r\n        forward_cell_list, backward_cell_list = [], []\r\n        for layer in range(hparams['num_layers']):\r\n            with tf.compat.v1.variable_scope('fw_cell_{}'.format(layer)):\r\n                cell = lstm_cell(hparams['num_units'], hparams['dropout'], mode)\r\n\r\n            forward_cell_list.append(cell)\r\n\r\n            with tf.compat.v1.variable_scope('bw_cell_{}'.format(layer)):\r\n                cell = lstm_cell(hparams['num_units'], hparams['dropout'], mode)\r\n\r\n            backward_cell_list.append(cell)\r\n\r\n        forward_cell = tf.keras.layers.StackedRNNCells(forward_cell_list)\r\n        backward_cell = tf.keras.layers.StackedRNNCells(backward_cell_list)\r\n\r\n        encoder_outputs, encoder_state = tf.keras.layers.Bidirectional(\r\n            forward_cell,\r\n            backward_cell,\r\n            encoder_inputs,\r\n            sequence_length=source_sequence_length,\r\n            dtype=tf.float32)\r\n        # outputs:[batch_size, max_time, forward_cell.output_size]\r\n        # [batch_size, max_time, hidden_size]\r\n\r\n        encoder_outputs = tf.concat(encoder_outputs, -1)\r\n\r\n        return (encoder_outputs, source_sequence_length), encoder_state\r\n\r\n\r\ndef attend(encoder_outputs,\r\n           source_sequence_length,\r\n           mode,\r\n           hparams):\r\n\r\n    memory = encoder_outputs\r\n\r\n    if hparams['attention_type'] == 'luong':\r\n        attention_fn = tfa.seq2seq.LuongAttention\r\n    elif hparams['attention_type'] == 'bahdanau':\r\n        attention_fn = tfa.seq2seq.BahdanauAttention\r\n    elif hparams['attention_type'] == 'custom':\r\n        attention_fn = CustomAttention\r\n\r\n    attention_mechanism = attention_fn(\r\n        hparams['num_units'], memory, source_sequence_length)\r\n\r\n    cell_list = []\r\n    for layer in range(hparams['num_layers']):\r\n\r\n        with tf.compat.v1.variable_scope('decoder_cell_'.format(layer)):\r\n            cell = lstm_cell(hparams['num_units'], hparams['dropout'], mode)\r\n\r\n        # cell = lstm_cell(hparams['num_units'], hparams['dropout'], mode)\r\n        cell_list.append(cell)\r\n\r\n    alignment_history = (mode != tf.estimator.ModeKeys.TRAIN)\r\n\r\n    if hparams['bottom_only']: # False\r\n        #  Only wrap the bottom layer with the attention mechanism.\r\n\r\n        attention_cell = cell_list.pop(0)\r\n        # attention_cell = tf.cast(attention_cell, dtype='float32')\r\n        # attention_mechanism = tf.cast(attention_mechanism, dtype='float32')\r\n        attention_cell = tfa.seq2seq.AttentionWrapper(\r\n            attention_cell, attention_mechanism,\r\n            attention_layer_size=hparams['attention_layer_size'],\r\n            alignment_history=alignment_history)\r\n\r\n        decoder_cell = AttentionMultiCell(attention_cell, cell_list)\r\n    else:\r\n        decoder_cell = tf.keras.layers.StackedRNNCells(cell_list)\r\n\r\n        decoder_cell = tfa.seq2seq.AttentionWrapper(\r\n            decoder_cell, attention_mechanism,\r\n            attention_layer_size=hparams['attention_layer_size'],\r\n            alignment_history=alignment_history)\r\n\r\n    return decoder_cell\r\n\r\n\r\ndef speller(encoder_outputs,\r\n            encoder_state,\r\n            decoder_inputs,\r\n            source_sequence_length,\r\n            target_sequence_length,\r\n            mode,\r\n            hparams):\r\n\r\n    batch_size = tf.shape(input=encoder_outputs)[0]\r\n    beam_width = hparams['beam_width']\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT and beam_width > 0:\r\n        encoder_outputs = tfa.seq2seq.tile_batch(\r\n            encoder_outputs, multiplier=beam_width)\r\n        source_sequence_length = tfa.seq2seq.tile_batch(\r\n            source_sequence_length, multiplier=beam_width)\r\n        encoder_state = tfa.seq2seq.tile_batch(\r\n            encoder_state, multiplier=beam_width)\r\n        batch_size = batch_size * beam_width\r\n\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL and beam_width > 0:\r\n        encoder_outputs = tfa.seq2seq.tile_batch(\r\n            encoder_outputs, multiplier=beam_width)\r\n        source_sequence_length = tfa.seq2seq.tile_batch(\r\n            source_sequence_length, multiplier=beam_width)\r\n        encoder_state = tfa.seq2seq.tile_batch(\r\n            encoder_state, multiplier=beam_width)\r\n        batch_size = batch_size * beam_width\r\n\r\n    def embedding_fn(ids):\r\n        # pass callable object to avoid OOM when using one-hot encoding\r\n        if hparams['embedding_size'] != 0:\r\n            target_embedding = tf.compat.v1.get_variable(\r\n                'target_embedding', [\r\n                    hparams['target_vocab_size'], hparams['embedding_size']],\r\n                dtype=tf.float32, initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\r\n\r\n            return tf.nn.embedding_lookup(params=target_embedding, ids=ids)\r\n        else:\r\n            return tf.one_hot(ids, hparams['target_vocab_size'])\r\n\r\n    decoder_cell = attend(\r\n        encoder_outputs, source_sequence_length, mode, hparams)\r\n\r\n    projection_layer = tf.keras.layers.Dense(\r\n        hparams['target_vocab_size'], use_bias=True, name='projection_layer')\r\n\r\n    if hparams['pass_hidden_state'] and hparams['bottom_only']:\r\n        initial_state = tuple(\r\n            zs.clone(cell_state=es)\r\n            if isinstance(zs, tfa.seq2seq.AttentionWrapperState) else es\r\n            for zs, es in zip(\r\n                decoder_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32), encoder_state))\r\n    else:\r\n        initial_state = decoder_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\r\n\r\n    maximum_iterations = None\r\n    if mode != tf.estimator.ModeKeys.TRAIN:\r\n        max_source_length = tf.reduce_max(input_tensor=source_sequence_length)\r\n        maximum_iterations = tf.cast(tf.round(tf.cast(\r\n            max_source_length, dtype=tf.float32) * hparams['decoding_length_factor']), dtype=tf.int32)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        decoder_inputs = embedding_fn(decoder_inputs)\r\n        decay_steps = hparams['decay_steps']\r\n        iter_num = tf.compat.v1.train.get_global_step()\r\n        inverse_probability = tf.compat.v1.train.polynomial_decay(\r\n            1.0, iter_num, decay_steps, 0.6)\r\n        sampling_probability = 1.0 - inverse_probability\r\n        if hparams['sampling_probability']:\r\n            helper = tfa.seq2seq.ScheduledEmbeddingTrainingSampler(\r\n                sampling_probability=sampling_probability,\r\n                embedding_fn=embedding_fn\r\n            )\r\n        else:\r\n            helper = tfa.seq2seq.TrainingSampler()\r\n\r\n        decoder = tfa.seq2seq.BasicDecoder(\r\n            cell=decoder_cell,\r\n            sampler=helper,\r\n            output_layer=projection_layer,\r\n            maximum_iterations=maximum_iterations\r\n        )\r\n     \r\n        decoder_outputs, final_context_state, final_sequence_length = tfa.seq2seq.dynamic_decode(\r\n            decoder, training=True, decoder_init_input=decoder_inputs, decoder_init_kwargs={\r\n                'initial_state': initial_state, 'sequence_length': target_sequence_length\r\n            })\r\n\r\n    elif mode == tf.estimator.ModeKeys.PREDICT and beam_width > 0:\r\n        start_tokens = tf.fill(\r\n            [tf.compat.v1.div(batch_size, beam_width)], hparams['sos_id'])\r\n\r\n        decoder = tfa.seq2seq.BeamSearchDecoder(\r\n            cell=decoder_cell,\r\n            embedding_fn=embedding_fn,\r\n            beam_width=beam_width,\r\n            output_layer=projection_layer,\r\n            maximum_iterations=maximum_iterations\r\n        )\r\n        decoder_outputs, final_context_state, final_sequence_length = tfa.seq2seq.dynamic_decode(\r\n            decoder, decoder_inputs=embedding_fn(decoder_inputs),\r\n            training=False, decoder_init_kwargs={\r\n                'start_tokens': start_tokens, 'end_token': hparams['eos_id'],\r\n                'initial_state': initial_state\r\n            })\r\n    else:\r\n        '''\r\n        start_tokens = tf.fill([batch_size], hparams.sos_id)\r\n\r\n        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\r\n            embedding_fn, start_tokens, hparams.eos_id)\r\n        \r\n        decoder = tf.contrib.seq2seq.BasicDecoder(\r\n            decoder_cell, helper, initial_state, output_layer=projection_layer)\r\n        '''\r\n\r\n        start_tokens = tf.fill(\r\n            [tf.compat.v1.div(batch_size, beam_width)], hparams['sos_id'])\r\n\r\n        decoder = tfa.seq2seq.BeamSearchDecoder(\r\n            cell=decoder_cell,\r\n            embedding_fn=embedding_fn,\r\n            beam_width=beam_width,\r\n            output_layer=projection_layer,\r\n            maximum_iterations=maximum_iterations\r\n        )\r\n\r\n       \r\n\r\n        decoder_outputs, final_context_state, final_sequence_length = tfa.seq2seq.dynamic_decode(\r\n            decoder, decoder_inputs=embedding_fn(decoder_inputs),\r\n            training=False, decoder_init_kwargs={\r\n                'start_tokens':start_tokens,\r\n                'end_token':hparams['eos_id'],\r\n                'initial_state': initial_state\r\n            })\r\n\r\n    return decoder_outputs, final_context_state, final_sequence_length\r\n\r\n\r\n```\r\nBesides,  I use tf.estimator.Estimator for training and evaluation.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49105\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49105\">No</a>\n"]}, {"number": 49104, "title": "Add GPU implem of sparse segment reduction ops [resubmission]", "body": "This is a resubmission of https://github.com/tensorflow/tensorflow/pull/47974 that includes an additional commit to (attempt to) workaround errors in the Windows CI build.\r\n\r\ncc @nluehr @sanjoy ", "comments": ["Hm.\r\nI think it might have made more progress this time, because the Windows build ran for >3 hours before failing; but for some reason I don't have a \"Details\" link to click on to see what went wrong.\r\nLet me know if it's still related to this PR.", "I don't see the \"details\" link either, kicked off another Kokoro run.", "Wow, looks like this is ICEing the compiler.\r\n\r\n@cheshire you ran into this too right?  Did you have to do something specific to work around this?", "Is the ICE related to this PR? It's in XLA, which this doesn't touch. (Also I don't think it showed up on the previous Kokoro run).\r\n\r\nStill no Details on the Windows Bazel GPU failure.\r\nI think the other failures are unrelated.", "> Is the ICE related to this PR? It's in XLA, which this doesn't touch. (Also I don't think it showed up on the previous Kokoro run).\r\n\r\nGood point, I'll kick off a second run to see if we get lucky.\r\n\r\nI was able to grab the Windows log though: https://gist.github.com/sanjoy/09105756945061ca4aa331948027f8fa, which is displaying an error I've never seen before \"ERROR: command succeeded, but not all targets were analyzed\"", "I assume it's these from the Windows log?\r\n```\r\nERROR: T:/src/github/tensorflow/py_test_dir/tensorflow/python/saved_model/BUILD:625:18: in py_library rule //py_test_dir/tensorflow/python/saved_model:load_options: target '//tensorflow/python/util:tf_export' is not visible from target '//py_test_dir/tensorflow/python/saved_model:load_options'. Check the visibility declaration of the former target if you think the dependency is legitimate\r\nERROR: T:/src/github/tensorflow/py_test_dir/tensorflow/python/saved_model/BUILD:50:18: in py_library rule //py_test_dir/tensorflow/python/saved_model:signature_constants: target '//tensorflow/python/util:tf_export' is not visible from target '//py_test_dir/tensorflow/python/saved_model:signature_constants'. Check the visibility declaration of the former target if you think the dependency is legitimate\r\nERROR: T:/src/github/tensorflow/py_test_dir/tensorflow/python/saved_model/BUILD:512:18: in py_library rule //py_test_dir/tensorflow/python/saved_model:revived_types: target '//tensorflow/python/util:tf_export' is not visible from target '//py_test_dir/tensorflow/python/saved_model:revived_types'. Check the visibility declaration of the former target if you think the dependency is legitimate\r\n```\r\n\r\nAlso looks like the ICE is still there but in a different file.", "Ben, can you please rebase?  Given the builds are green, I suspect whatever is causing the ICE in XLA has been resolved.", "I'm a little unclear on the state of this, does it still need to be rebased?", "Ben, the Linux GPU failures seem legitimate (segfault in //tensorflow/python/kernel_tests:segment_reduction_ops_test_gpu[k8-opt]), can you PTAL?", "I've rebased and added a fix for the new test failure.", "Is this still blocked on something internally?", "> Is this still blocked on something internally?\r\n\r\nNo, but I need to check it in manually because of merge conflicts and a failing test.  Should be in tree soon.", "Merged at f9c651943d442f98d4c7c36839bde70f27a9a135."]}, {"number": 49103, "title": "Tensorflow Certificate on Mac M1", "body": "**System information**\r\n- OS Platform and Distribution: Mac M1 Big Sur 11.3\r\n- TensorFlow version: 2.4.1\r\n- Python version:3.8\r\n- GPU model and memory: M1\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nAnyone has tried to take the TF certificate exam on a Mac M1 device?\r\n\r\nI've installed tensorflow-macos in an env and it works fine. However, the exam would create a new project in Pycharm using my global python and install original tensorflow 2.4.1. I tried to manually install original tensorflow 2.4.1 globally, but I got :\r\n\r\nProcess finished with exit code 132 (interrupted by signal 4: SIGILL)\r\n\r\nSo is there any way that I can use a existing env to run the exam or I get the original tensorflow working properly on Mac M1?\r\n\r\nMany thanks!\r\n", "comments": ["@Leon-lianglyu  Please follow the official [guide](https://www.tensorflow.org/extras/cert/Setting_Up_TF_Developer_Certificate_Exam.pdf) to setup the environment for mac OS. Also, You will get a very detailed information on TF Certification. Hope It will help you to set up  the environment and clear the Cerification as well.Thanks! ", "> @Leon-lianglyu Please follow the official [guide](https://www.tensorflow.org/extras/cert/Setting_Up_TF_Developer_Certificate_Exam.pdf) to setup the environment for mac OS. Also, You will get a very detailed information on TF Certification. Hope It will help you to set up the environment and clear the Cerification as well.Thanks!\r\n\r\nThanks for it. But I followed the offical guide and met the above problems.\r\n", "@Leon-lianglyu , You can find the similar issues here which could solve your issue here, [issue1](https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org), [issue2](https://stackoverflow.com/questions/65242614/why-does-loading-tensorflow-on-mac-lead-to-process-finished-with-exit-code-132)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49102, "title": "code error", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n**tensorflow/tensorflow/lite/micro/test_helpers.h::line25**\r\n```bash\r\n#include \"tensorflow/lite  //   kernels/internal/tensor_ctypes.h\"\r\n```\r\n**there are two slashs before kernels.obviously it is an error.**\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@liuyanghu\r\n\r\nCould you please elaborate your Query with clear details. We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\n", "I think it is just a small typo by @njeffrie  with https://github.com/tensorflow/tensorflow/commit/4a988e47921bc3655aefa7b68516af195067b33f", "Good call. I'll put up a change to fix this, thanks for the catch!", "created https://github.com/tensorflow/tensorflow/pull/49115 to address this."]}, {"number": 49101, "title": "`tf.keras.preprocessing.image.smart_resize` stable docs does not render properly", "body": "## URL(s) with the issue:\r\n\r\n> Please provide a link to the documentation entry, for example:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/smart_resize \r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIn the \"Stable\" tab of the [`tf.keras.preprocessing.image.smart_resize`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/smart_resize ) docs, the output docs seem like they're HTML-escaped and/or input markdown (opposed to actually rendering as HTML). \r\n\r\nFor reference, I'm reporting this bug from Chrome 90 on macOS. I get the same results on Firefox and Safari, also on macOS - likely, the generated docs page is incorrect.\r\n\r\n### Usage example\r\n\r\nN/A\r\n\r\n### Request visuals, if applicable\r\n\r\n> Are there currently visuals? If not, will it clarify the content?\r\n\r\nHere's a screenshot of what I see on Chrome 90 on macOS, under the \"stable\" tab:\r\n\r\n![Screen Shot 2021-05-10 at 5 18 41 PM](https://user-images.githubusercontent.com/14893287/117739761-d7a35500-b1b3-11eb-9575-a2d07c8cb151.png)\r\n\r\nIn contrast, here is the nightly tab, which works as intended:\r\n\r\n![Screen Shot 2021-05-10 at 5 19 21 PM](https://user-images.githubusercontent.com/14893287/117739839-04f00300-b1b4-11eb-911b-a873dc323207.png)\r\n\r\n\r\n### Submit a pull request?\r\n\r\nNot very familiar with the tensorflow docs API, but if it's a simple change, I wouldn't mind taking a crack at it!\r\n", "comments": ["@mattxwang , This has been fixed in nightly and will be fixed in the latest stable release, the issue was basically in the triple quote mistake in line 65 of [this](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/preprocessing/image.py#L65) file.", "> This has been fixed in nightly and will be fixed in the latest stable release, the issue was basically in the triple quote mistake in line 65 of this file.\r\n\r\nGreat, thank you so much for the quick reply! Is this issue good to close?", "@mattxwang , Yes, closing this issue. Thanks!"]}, {"number": 49100, "title": "[CherryPick:2.5] Dense shape cannot be empty", "body": "", "comments": []}, {"number": 49099, "title": "[CherryPick:2.5]Update jsoncpp to 1.9.4", "body": "", "comments": []}, {"number": 49098, "title": "Update stale.yml", "body": "Test PR", "comments": []}]