[{"number": 4762, "title": "Request for documentation: Loop implementation", "body": "I am trying to understand the implementation of `tf.while_loop` and everything that is built on top of it, because I am implementing a custom `tf.Graph` subclass, and finding that the way `tf.while_loop` is handled during gradient computation is important for what I am doing. \n\nHowever, I cannot find any documentation on the ops that comprise `tf.while_loop` \u2013 is there any internal documentation on this implementation?\n\nI am finding myself confused about some of the following concepts:\n- `WhileContext`, and, in general, the stack of contexts\n- Flows vs tensors\n- Frames\n\nSo far I've gotten quite a bit by just reading the source code, but it's pretty hard to build yourself a good mental model by going completely bottom up without having any high level picture of how the entire thing is organized.\n\nIf there is no intention to document these things (which would be completely understandable) please feel free to close this issue (although I would definitely appreciate some help or pointers as to where I can figure out the high-level overview of looping implementation).\n", "comments": ["@yuanbyu might be able to fill in some context here.\n", "I have been working on a doc which we hope to turn into a technical paper. I will give you a copy when it is ready.  In the meantime, feel free to send any specific questions to me.  \n\nWhileContext is at this point a python front-end concept to represent a while_loop. It can be nested to represent nested loops. Internally, a loop is a cyclic graph using five primitive ops: Enter, Exit, Switch, Merge, and NextIteration. A loop can be partitioned onto multiple machines/devices, which is done in core/graph/graph_partition.cc. Frame is a concept used in core/common_runtime/executor.cc, which manages the execution of a partition assigned to a device.  Most of the complexity in the python front-end is on backprop of while_loop.\n", "@yuanbyu Thanks, good to hear that a doc is being prepared.\n\nSo, here's what I understand so far. Perhaps you can correct any misunderstandings and also this will serve as a useful resource for the time being for anyone else who runs into this.\n1. `tf.while_loop` accepts a list of loop variables, a function mapping loop variables to a boolean, and a function mapping loop variables to a new set of loop variables.\n2. Internally, this is represented using the special nodes Enter, Exit, NextIteration, Switch, and Merge. Enter, Exit, NextIteration are all semantically equivalent to identity ops (they just forward their input to their output, potentially as a reference), but the fact that they have type Enter, Exit, NextIteration is used by the executor to handle them in a special way. The graph is constructed as follows:\n   - The loop variables are sent through \"Enter\" nodes.\n   - The Enter node results are then given to \"Merge\"  nodes. During the graph construction, the inputs to the \"Merge\"  nodes are two copies of each enter node; when the NextIteration nodes are constructed, the Merge nodes are fixed by replacing one of the Enter inputs with a NextIteration input. In this way, every Merge node (one per variable) gets an input from its respective variable's Enter and NextIteration nodes.\n   - The output of the Merge nodes is passed to the condition function, which takes them and outputs a boolean. This boolean is passed to a `LoopCond` node. This boolean, as well as the output of the Merge nodes, is passed to Switch nodes, again one per variable. The Switch nodes output a dead tensor to one of their outputs and a live tensor (the merge node output) to the other one, depending on the boolean.\n   - The output of the Switch node is sent to an Exit node (one per variable) or to an Identity op (one per variable), depending on whether the loop condition is false.\n   - The identity op output is given to the loop body, and the outputs of the loop body are fed to NextIteration ops; these ops are the ones patched back in as inputs to the Merge nodes.\n3. The executor has special support for these five primitive ops which make this structure into a loop:\n   - The executor has a concept of a Frame, which is essentially the current iteration of the innermost loop. A frame has state, where all the input and output tensors are stored in a flat vector; each op writes its outputs to a subset of the output vector and gets inputs from a subset of the input vectors; thus, the inputs and outputs of an op can be obtained by just going to the right offset in this vector of Entry values.\n4. A new frame is created when the executor sees an Enter node. A frame is removed when it sees an Exit node. The next iteration of the frame is progressed to when it sees a NextIteration node.\n5. When it sees a NextIteration node, it finds the child of that node (namely the Merge op) and calls `ActivateNode` on it, in order to continue the loop. Since nodes are not marked ready until all their inputs are non-dead, the nodes that get dead inputs from Switch (e.g. the loop is done) will not get run again.\n6. For every loop during forward propagation, a few things have to happen to create the backprop gradient graph:\n   - First of all, a loop is added to the forward propagation which counts the number of iterations. More accurately, the original loop is added to; this works because of the way the primitive ops are designed. This loop starts with a `f_count` Enter node and is created in `control_flow_ops.py` `AddForwardLoopCounter`.\n   - A `history_map` is maintained of tensors produced during forward prop, and whenever the backprop needs a tensor from the forward prop, a stack is introduced, and the forward prop has a `StackPush` added to it, while the backprop has a `StackPop` added to it that pops from the same stack. In that manner, the forward prop pushes anything the backprop will need onto a stack, and the backprop slowly consumes that stack.\n\nThe description above is not quite complete but I think I probably understand enough for what I want to do.\n\nQuestions:\n1. Why is there a `LoopCond` node? Why not pass the output of the condition directly to `Switch`?\n2. What was the motivation for such a seemingly complicated set of primitive ops? It seems like it's possible to build other control structures on top of them \u2013 is that the goal? Were these primitive ops chosen because they make it possible to implement the fairly complex gradient loop generation?\n3. What is an example usecase for `parallel_iterations`? (This is a simple question which might make sense to add to the `tf.while_loop` docs)\n", "Also, given that you are already working on documentation for this, feel free to close this issue if so desired \u2013 it is not very actionable.\n", "Thank you putting in the time to write that documentation above @gibiansky. Maybe something like that is more appropriate for a pull request? You've signed our CLA and have contributed to the documentation in the past. What would you recommend for him @yuanbyu?\n", "I have written a doc and would be happy to share it with you.\r\n\r\nFor your questions:\r\n\r\n1. LoopCond is just used as a unique marker so we could understand the graph structure in later stages of graph rewriting.  For example, rewriting the graph to support distributed execution of a loop.\r\n\r\n2. Yes, that was one of the main design considerations. If you are familiar with the dataflow machines invented in the 70s, you would not be surprised by the choice of the primitives. :-) The other main considerations are non-strictness, automatic differentiation, and parallel and distributed execution of both forward and backprop.\r\n\r\n3. A simple example is tf.map.  And it is one of the main reasons that the performance of dynamic_rnn can be as good as static unrolling. For example it allows the dynamic unrolling logic runs on CPU and the actual computation runs on GPU, completely in parallel.", "@yuanbyu was the technical doc finally posted somewhere online? It would help if you shared the link here, for those visiting this issue in the future, like me.", "@yuanbyu I'm also interested by this technical doc that you are referring too.", "@yuanbyu I'm also interested in this technical doc. How can I get the doc or somewhere to see that technical doc? ", "@skye I remember seeing some docs at some point, but can no longer find them.  Do you have any links?", "We have an internal design doc. There's nothing proprietary in it though, so lemme look into sharing it publicly.", "Just a heads up that we're working on this! Stay tuned. cc @wolffg", "Here is the doc: http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Thanks @skye!  Closing this out, since the doc is available."]}, {"number": 4761, "title": "contrib.seq2seq initial commit", "body": "As advised by @lukaszkaiser from #4686, initial commit of the `tf.contrib.seq2seq` folder.\n\nThe purpose of this pull request is to agree on a skeleton for the `tf.contrib.seq2seq` folder, I took inspiration from the [`tf.contrib.layers`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/) folder and the [`tf.contrib.losses`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/losses/) folder.\n\nAfter this skeleton has been accepted, I will start by filling out the `tf.contrib.seq2seq.loss` with #4382 , the `tf.contrib.seq2seq.rnn_decoder` with #4686 and a `tf.contrib.seq2seq.rnn_decoder_attention` including `kernel_test.loss_ops_test.py` and `kernel_test.layers_ops_test.py`.\n\nPlease note, as some of the calls (BUILD?) for the contrib section are scattered across various folders, I might be missing some calls or definitions somewhere.\n", "comments": ["Can one of the admins verify this patch?\n", "@alrojo, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @keveman and @tensorflower-gardener to be potential reviewers.\n", "This looks good to me, great thanks! Let's test it and get it in.\n", "@lukaszkaiser Would that require any action from me?\n", "I think not. Can one of the admins test and merge this please?\n", "Jenkins, test this please\n", "Aha! I forgot to add the `all_files` symlink in the `contrib/seq2seq/BUILD` file. I just pushed it, sorry for the inconvenience! Please try and rerun @jhseu \n\nOn a related note: Do you have a guide on how to run these \"Jenkins\" test locally so I can skip the trouble of having your admins debug for me? Thanks.\n", "You can reproduce the tests we run with tools/ci_build/ci_sanity.sh tools/ci_build/ci_parameterized_build.sh. Jenkins just runs those with different environment variables set. We definitely should document it better for contributors, though.\n\nJenkins, test this please\n", "@tensorflow-jenkins test this please.\n", "I have switched the style of coding to `tf.contrib.bayesflow` instead, as it seemed more matching (there is quite a few styles in the contrib folder).\n\nI couldn't get the tests @jhseu pointed out to work. But I ran the testing guide (step 1-4) from: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/README.md\n\nIt was very time consuming (~6 hours per run) but my last commit had no FAILED.\n\n@jhseu I would be interested in making a short tutorial for how to setup the testing environment (e.g. on a fresh ubuntu 14.04 GCP/AWS instance). Could you perhaps assist me in the steps required to make a complete replica of the Jenkins test environment? (I can make a separate issue for this if wanted)\n\nFresh Ubuntu 14.04\n- Install Docker\n- git clone https://github.com/tensorflow/tensorflow.git\n- cd tensorflow\n- ??\n", "@alrojo Does your `dynamic_rnn_decoder_attention` do beam-search in the graph?\nAlso, I can help out with adding a multi-hop attention option (similar to Memory Networks) to `seq2seq` for use cases that require transitive reasoning about inputs.\n", "@ethancaballero The `dynamic_rnn_decoder_attention` will not have beam search implented (yet!). Multi-layered decoder and beam search is first on my todo list.\n\nWhen this PR has been merged I will update the two other PRs (`sequence_loss` and `rnn_decoder`) and make a new with the `dynamic_rnn_decoder_attention` where we can discuss extensions. By multi-hop attention are you referring to: m_i in (https://arxiv.org/abs/1506.07285)?\n", "@tensorflow-jenkins test this please.\n", "@alrojo Yes, although I would instead use the Attention based GRU from equation 11 of https://arxiv.org/pdf/1603.01417.pdf to obtain contextual vector c_t that then updates episode memory m_t\u22121 (via a vanilla GRU) to produce m_t. There's an example tensorflow implementation of Attention based GRU here: \nhttps://github.com/therne/dmn-tensorflow/blob/master/utils/attn_gru.py\nAlso, this method might require a second encoder layer (referred to as input fusion layer in 1603.01417) on top of the normal word_level encoder to work correctly.\n\nAlternatively, one could use a weighted sum as a soft attention to compute contextual vector c_t during each hop (which is more similar to the current tf.nn.seq2seq api), but this weighted sum method loses positional & ordering information and as a result is significantly less accurate for use cases that require more than 2 hops.\n", "Okay, the `Sanity Checks` seemed to fail because the filegroups wasnt ordered alphabetically (I think). I have now updated the filegroups so the `\"//tensorflow/contrib/seq2seq:all_files\"` is alphabetically correctly placed in the `tensorflow/BUILD`'s filegroup.\n\nThis was the `FAILED` stack trace from the `Sanity Checks`:\n\n```\n=== Sanity check step 3 of 4: do_buildifier (buildifier check) ===\n\nRunning do_buildifier on 112 files\n\ntensorflow/BUILD # listsort unsafesort sort:filegroup.data\n\nbuildifier took 1 s\n\nFAIL: buildifier found errors and/or warnings in above BUILD files.\nbuildifier suggested the following changes:\n123a124\n>         \"//tensorflow/contrib/seq2seq:all_files\",\n126d126\n<         \"//tensorflow/contrib/seq2seq:all_files\",\nPlease fix manually or run buildifier <file> to auto-fix.\n```\n", "@yifeif could I have you try test it again? :) thanks\n", "@tensorflow-jenkins test this please\n", "Please merge this when possible guys!\n"]}, {"number": 4760, "title": "Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)", "body": "I've the following error. I'm using a conda installation of tensorflow. I'm struggling to try to use it with my GPU.\n\n```\nLoaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5103 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\nF tensorflow/core/kernels/conv_ops.cc:526] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\nAborted (core dumped)\n```\n\nwhich nvcc returns\n`/usr/local/cuda-7.5/bin/nvcc`\n\nnvcc version returns\n`Cuda compilation tools, release 7.5, V7.5.17`\n\nI tried downloading CuDNN v5.1 and did the following but it didn't work either\n\n```\nsudo cp lib* /usr/local/cuda-7.5/lib64/\nsudo cp include/cudnn.h /usr/local/cuda-7.5/include/\nsudo ldconfig\n\n```\n\nI tried on the other folder too\n\n```\nsudo cp lib* /usr/local/cuda/lib64/\nsudo cp include/cudnn.h /usr/local/cuda/include/\nsudo ldconfig\n```\n", "comments": ["@ritchieng Please provide info on your environment.  I.e. fill out the template that's provided when you click on the green \"New issue\" button.  Thanks!\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 4759, "title": "AttributeError: 'module' object has no attribute 'NodeDef'", "body": "I tried to run the distributed Tensorflow for inception-v3 example, but I have the following errors in worker nodes:\n\n```\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> 10.149.0.1:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> 10.149.0.3:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:206] Started server with target: grpc://localhost:2222\nTraceback (most recent call last):\n  File \"/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/inception_distributed_train.py\", line 120, in train\n    global_step = slim.variables.global_step()\n  File \"/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/slim/scopes.py\", line 155, in func_with_args\n    return func(*args, **current_args)\n  File \"/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/slim/variables.py\", line 242, in global_step\n    with tf.device(variable_device(device, 'global_step')):\n  File \"/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/slim/variables.py\", line 214, in variable_device\n    var_def = graph_pb2.NodeDef(name=var_name, op='Variable')\nAttributeError: 'module' object has no attribute 'NodeDef'\n```\n\nBased on the information, the line 214 is \nvar_def = graph_pb2.NodeDef(name=var_name, op='Variable')\n\nAnd at the beginning of variables.py, it has the line\nfrom tensorflow.core.framework import graph_pb2\n\nBut I could not find any graph_pb2 from tensorflow/tensorflow/core/framework. Any idea why I got this error? \n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nNo other related posts were found.\n\nEnvironment info\n\nOperating System: Redhat 7.2\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n-rw-r--r-- 1 root root 560184 May 8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root 16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root 19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root 394472 May 8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root 737516 May 8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a\n\nIf installed from source, provide \n1. The commit hash (git rev-parse HEAD): a63b0cbcabc79531e155a0663a08656debf2fe07\n2. The output of bazel version:\n.\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Sep 29 22:19:27 2016 (1475187567)\nBuild timestamp: 1475187567\nBuild timestamp as int: 1475187567\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["@hfutxrg I'm suspecting that there's a mismatch in your source code.\n\nYou mention that you've installed from source at commit hash a63b0cb.  But the stack trace above shows files under `/usr/lib/python2.7/site-packages/tensorflow`, which probably isn't where the sources are.\n\nTry to ensure you're actually picking up the code from whereever you put your sources.  See #4200 for a similar issue.\n", "I do not understand that. Shouldn't we have two locations? One location is our source, and another location is where we install tensorflow using pip.\n\nIn my environment, the tensorflow source is in \n/cm/shared/scratch/rengan/DL/tensorflow\n\nMy original post used /usr/lib/python2.7/site-packages/tensorflow as the installation location, but I just change the installation to\n/cm/shared/scratch/rengan/packages/usr/lib/python2.7/site-packages/tensorflow\n\nThe file system is NFS, so both the source and installation directories are shared by ps server, worker0 and worker1. \n\nThe thread you referred to didn't solve that issue. I also tried to include the source in PYTHONPATH\nexport PYTHONPATH=/cm/shared/scratch/rengan/DL/tensorflow:$PYTHONPATH\n\nBut when I ran the command on any worker node\nbazel-bin/inception/imagenet_distributed_train \\\n--batch_size=64 \\\n--max_steps=2000 \\\n--data_dir=/cm/shared/scratch/rengan/Data/imagenet-data \\\n--job_name='worker' \\\n--task_id=0 \\\n--ps_hosts='10.141.0.1:2222' \\\n--worker_hosts='10.141.0.2:2222,10.141.0.3:2222'\n\nit had error:\nTraceback (most recent call last):\n  File \"/cm/shared/scratch/rengan/DL/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py\", line 24, in <module>\n    import tensorflow as tf\n  File \"/cm/shared/scratch/rengan/DL/tensorflow/tensorflow/**init**.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/cm/shared/scratch/rengan/DL/tensorflow/tensorflow/python/**init**.py\", line 60, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/cm/shared/scratch/rengan/DL/tensorflow/tensorflow/python/**init**.py\", line 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\nImportError: cannot import name pywrap_tensorflow\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n", "@hfutxrg This is a little confusing.  There are two git repositories:\n\nhttps://github.com/tensorflow/tensorflow (let's call this the tensorflow repo)\nhttps://github.com/tensorflow/models (let's call this the models repo)\n\nFirst let's talk about the tensorflow repo.  In general, there are many ways to install the tensorflow repo:\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#download-and-setup\n\nYou're only supposed to pick one of the above ways to install tensorflow.  E.g. if you install via pip, that includes python source files that are consistent with the pip version that you installed.  If instead you install the sources (i.e. via `git clone ...`), that also includes python source files that are consistent.  If you end up with both a pip install and source install, you run the risk of having inconsistent source files.\n\nE.g. it looks like your pip install is under `/usr/lib/python2.7/site-packages/tensorflow`, while your source install is under `/cm/shared/scratch/rengan/DL/tensorflow`.  The problem you're hitting is that these two installs have different python source code.\n\nThe confusing part is that people typically install the models repo directly from sources (e.g. `git clone ...`).  I believe you've done that here.  There isn't a good versioning scheme in place between the tensorflow repo and the models repo, so things may get out-of-sync.  And that problem gets worse if you also have multiple version of the tensorflow repo (i.e. both from pip and from sources).\n\nOne way to ensure everything is consistent is to install both the tensorflow repo and the models repo from sources.  Another way is to pip-install a nightly version of tensorflow (see https://github.com/tensorflow/tensorflow#installation), and install the models repo from sources.\n\nLet me know if that helps.\n", "Hi @tatatodd, \n\nI installed both tensorflow and models from source. But even installing tensorflow from source, we still need pip to install it in the last step. \n\nBased on tensorflow document, \"When building from source, you will still build a pip package and install that.\" And the last step is \n\"$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.11.0rc0-py2-none-any.whl\"\n\nThat's why I have two locations. /cm/shared/scratch/rengan/DL/tensorflow is the location of tensorflow source code. And /usr/lib/python2.7/site-packages/tensorflow is the installed location after installation from source, or where the above command \"pip install\" installed to.\n\nI think the issue here is the out-of-sync between tensorflow and models repos. Perhaps models used some module not available on tensorflow repo anymore. But inception-v3 in models repo is the only distributed training example I could find. If there is any other distributed training model example in tensorflow repo, please let me know. Thanks. \n", "@hfutxrg I see, thanks for the explanation.  Yes, in that case I think the issue is that the tensorflow and models repos are out-of-sync.\n\nWrt distributed training, have you seen the following:\nhttps://github.com/tensorflow/models/tree/master/slim\nhttps://github.com/tensorflow/models/tree/master/slim#training-a-model-from-scratch\nhttps://github.com/tensorflow/models/blob/master/slim/deployment/model_deploy.py\n\nHope that helps!\n", "I have the same problem, also I found that `image_processing.py` must use `0.11`, because of `tf.image.resize_images`, but in `graph_pb2.py` there is no definitions about `NodeDef`\n", "Closing due to inactivity. I'll open again with new information. Thanks for helping with support @qingzew.\n"]}, {"number": 4758, "title": "AttributeError: type object 'NewBase' has no attribute 'is_abstract'", "body": "I uninstalled anaconda and reinstalled tensorflow using the pip installation. \n\nOriginally it was having trouble updating numpy, but after I fixed that now I'm getting this error:\n\n```\nimport tensorflow\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 98, in <module>\n    from tensorflow.python.platform import test\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/test.py\", line 63, in <module>\n    from tensorflow.python.framework import test_util\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/framework/test_util.py\", line 43, in <module>\n    from tensorflow.python.platform import googletest\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/googletest.py\", line 32, in <module>\n    from tensorflow.python.platform import benchmark  # pylint: disable=unused-import\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/benchmark.py\", line 122, in <module>\n    class Benchmark(six.with_metaclass(_BenchmarkRegistrar, object)):\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six.py\", line 566, in with_metaclass\n    return meta(\"NewBase\", bases, {})\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/benchmark.py\", line 117, in __new__\n    if not newclass.is_abstract():\nAttributeError: type object 'NewBase' has no attribute 'is_abstract'\n```\n\nCould this be because uninstalling anaconda left some residual effects. I noticed my $PATH still has anaconda prepended to it\n`$PATH -bash: /Users/usr/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/git/bin:/usr/texbin: No such file or directory`\n", "comments": ["@dxin1234 Please provide at least the command that you're running that leads to the error.  Also provide the information from the default template, which appears when you click on the green \"New issue\" button.\n\n---\n\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n### What other attempted solutions have you tried?\n\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n"]}, {"number": 4757, "title": "import meta graph bug reading model with scope name changed", "body": "Same issue for tf 0.10 and 0.11.rc0.\nThe problem is if you read one model, change the top scope name, write a new model, then you read the new model, but still you got the old top scope name.\nHow to reproduce \n- create model\n  python create-model.py\n  \n  ```\n  import tensorflow as tf  \n  \n  sess = tf.InteractiveSession()  \n  \n  with tf.variable_scope('old'):  \n  \n      w = tf.get_variable('w', shape=[1], initializer=tf.constant_initializer(1.0))  \n  \n  sess.run(tf.initialize_all_variables())  \n  \n  tf.train.Saver().save(sess, '/tmp/old.model')  \n  ```\n- read this model (using import meta graph), then rename top scope and save to one new model\n  python rename-scope.py\n  old_vars: [u'old/w:0']\n  new_vars: [u'new/w']  \n  \n  ```\n  import tensorflow as tf  \n  \n  sess = tf.InteractiveSession()  \n  \n  saver = tf.train.import_meta_graph('/tmp/old.model.meta')  \n  \n  saver.restore(sess, '/tmp/old.model')  \n  \n  src_vars = [v for v in tf.all_variables() if v.name.startswith('old')]  \n  \n  print('old_vars:', [v.name for v in src_vars])\n  \n  out_vars = {v.name[:v.name.rfind(':')].replace('old', 'new', 1): v for v in src_vars}  \n  \n  print('new_vars:', [key for key in out_vars])  \n  \n  tf.train.Saver(var_list=out_vars).save(sess, '/tmp/new.model') \n  ```\n- read new model(It is ok if you buid graph from scratch with new scope, but not ok if using import meta graph again)\n  # -------------------this is ok\n  \n  python read-renamed-buildgraph.py  \n  tf.all_variables: [u'new/w:0']\n  w val: [ 1.]\n  \n  ```\n  import tensorflow as tf  \n  \n  sess = tf.InteractiveSession()\n  \n  with tf.variable_scope('new'):\n      w = tf.get_variable('w', shape=[1], initializer=tf.constant_initializer(2.0))  \n  \n  tf.train.Saver().restore(sess, '/tmp/new.model')  \n  \n  print('tf.all_variables:', [v.name for v in tf.all_variables()])  \n  \n  print('w val:',  w.eval())\n  ```\n  # ---------------this is wrong\n  \n  python read-renamed-metagraph.py\n  tf.all_variables: [u'old/w:0']\n  \n  ```\n   import tensorflow as tf  \n  \n   sess = tf.InteractiveSession()  \n  \n   saver = tf.train.import_meta_graph('/tmp/new.model.meta')\n  \n   saver.restore(sess, '/tmp/new.model')\n  \n   print('tf.all_variables:', [v.name for v in tf.all_variables()])\n  ```\n", "comments": ["@chenghuige The problem is in your second step `rename-scope.py`, where you run the following two lines:\n\n```\nout_vars = {v.name[:v.name.rfind(':')].replace('old', 'new', 1): v for v in src_vars}\ntf.train.Saver(var_list=out_vars).save(sess, '/tmp/new.model')\n```\n\nI think you're assuming that this step is sufficient to rename the `old/w` variable to `new/w`.  In reality it's not.  Note that the python string replace method returns a copy of the string, and doesn't mutate the string itself:\nhttps://docs.python.org/2/library/string.html#string.replace\n\nWhat are you actually trying to do?\n", "@tatatodd you can see my example code of  \npython read-renamed-buildgraph.py it actually works if not reading using meta graph.\n\nfrom 'how to' of tensorflow.org\n\n[saving-and-restoring](https://www.tensorflow.org/versions/r0.11/how_tos/variables/index.html#saving-and-restoring)\n\n```\n# Create some variables.\nv1 = tf.Variable(..., name=\"v1\")\nv2 = tf.Variable(..., name=\"v2\")\n...\n# Add ops to save and restore only 'v2' using the name \"my_v2\"\nsaver = tf.train.Saver({\"my_v2\": v2})\n# Use the saver object normally after that.\n...\n```\n\nActually by doing this I can load two same models(models with same graph and just different variable values like bow model of step 1 and step 100) into the same session.\nI assume by doing this I save variable using new name. So to avoid loading two models with same weight.\n\nSo could I achive this without having to reload the model building graph from scratch ? Just load from meta graph?\n", "@chenghuige I'm sorry, my comment about python `string.replace` was irrelevant; I'd misinterpreted your code.\n\nBut the problem still remains in your second step `rename-scope.py`.  After that step, the checkpoint `/tmp/new.model` will have a single entry for `new/w`.  That is what you want.  But the MetaGraphDef `/tmp/new.model.meta` will still hold a graph that contains the `old/w` node.  That is because the `out_vars` that you passed to `tf.train.Saver` only renamed the checkpoint entry, but not the graph itself.\n\nThere is no bug in `import_meta_graph`, since it's just loading the graph that was provided.\n\nWe could try to make the argument that there's a bug in `tf.train.Saver`.  I.e. we passed it new names for some variables, so maybe it should rename the graph nodes too.  But that doesn't really work; e.g. the `old/w` variable also has related nodes `old/w/Initializer/Const`, `old/w/Assign`, `old/w/read` that are used to initialize the variable.  In general isn't possible for us to infer the renames that would be required.\n\n---\n\nSo basically the best way to accomplish your goal is your step 3 `read-renamed-buildgraph.py`.\n\nHope that helps!  If you have further questions, just comment on this issue, and I'll re-open.\n", "Very clear explanation, thanks! @tatatodd \n"]}, {"number": 4756, "title": "Fixed some typos.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@haosdent, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @asimshankar and @keveman to be potential reviewers.\n", "Mr Jenkins, test this please.\n", "Thanks for the fixes! Should be merged once tests pass.\n", "Many thanks!\n"]}, {"number": 4755, "title": "Fixed state test in StackBidirectionalRNN", "body": "One of the tests for StackBidirectionalRNN as not correct. Fixed.\n", "comments": ["Can one of the admins verify this patch?\n", "@joapaspe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener to be a potential reviewer.\n", "Jenkins, test this please.\n"]}, {"number": 4754, "title": "Nan values after applying gradients", "body": "Not sure it is a tf bug, I have also posted to stackoverflow so if consider not proper to be here close this. I find one similar question on stackoverlfow but no reply yet.\n[nan-in-summary-histogram](http://stackoverflow.com/questions/39854390/nan-in-summary-histogram)\n[why-am-i-getting-invalid-argument-nan-in-summary-histogram-for-histogramsummar](http://stackoverflow.com/questions/39828550/why-am-i-getting-invalid-argument-nan-in-summary-histogram-for-histogramsummar)\nMy program will face this some times(not every run will face this..), then if face this I can always reproduce this error loading from the last model I have saved before program crash due to nan. When rerun from this model, first train process seems fine using the model to generate loss(I have printed loss and shows no problem), but after applying gradients, the values of embedding variables will turn to Nan. So Nan in embedding will casue histogram collecting info crash.  If not use histogram, program will not crash(assertion fail) but since Nan exists the model is corrupted.\n\nSo what is the root cause of the nan problem? Confused as not know how to debug further and this program with same data and params will mostly run ok and only face this problem during some run..\n\nLoading existing model from: /home/gezi/temp/image-caption//model.flickr.rnn2.nan/model.ckpt-18000 Train from restored model: /home/gezi/temp/image-caption//model.flickr.rnn2.nan/model.ckpt-18000 I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 5235 get requests, put_count=4729 evicted_count=1000 eviction_rate=0.211461 and unsatisfied allocation rate=0.306781 I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110 2016-10-04 21:45:39 epoch:1.87 train_step:18001 duration:0.947 elapsed:0.947 train_avg_metrics:['loss:0.527'] ['loss:0.527'] 2016-10-04 21:45:39 epoch:1.87 eval_step: 18001 duration:0.001 elapsed:0.948 ratio:0.001 W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1 [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]] W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1 [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]] W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1 [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]] W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1 [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]] W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1 [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]] Traceback (most recent call last): File \"./train.py\", line 308, in tf.app.run()\n", "comments": ["I'm debugging a similar looking problem right now. I'll comment later on if we find anything. The graph summaries does not indicate why all of the sudden our policy output blows up.\n", "@hholst80 look forward  to see your progress.\nTo add more info. My program use hinge loss with margin 0.5, before step 18000 the result is fine, and loss is far below 0.5(about 0.138 at step 17000), then at 180000 the train loss will suddenly increase to 0.5(just looks like random guessing), though the scores and loss value with no nan yet. Then after applying gradients the relevant embdding matix values turn to nan. I have also printed the gradient values before applying to embedding, one gradient is all zero, not sure if this cause nan ? \ntf.Tensor 'rnn/gradients/AddN_5:0' shape=(1024, 1024) dtype=float32\n[[ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n ..., \n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\n\nI have also tried to retrain from step 17000.\n2016-10-05 06:50:36 epoch:1.77 train_step:17001 duration:1.044 elapsed:1.044 train_avg_metrics:['loss:0.139']  ['loss:0.139']\n2016-10-05 06:50:36 epoch:1.77 eval_step: 17001 train_avg_loss:['loss:0.139'] \n2016-10-05 06:50:36 epoch:1.77 eval_step: 17001 duration:0.002 elapsed:1.045 ratio:0.001\n2016-10-05 06:50:40 epoch:1.77 train_step:17002 duration:0.512 elapsed:4.025 train_avg_metrics:['loss:0.549']  ['loss:0.549']\n\nafter training step the loss increase to 0.5, and the gradients at 170000 also has all zero gradient.\n", "@chenghuige @hholst80 what version of tensorflow are you using?  Please fill out the following info (which is the default template when you click on the green \"New issue\" button)\n\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n", "@tatatodd  \n lsb_release -a\nLSB Version:    :core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch\nDistributor ID: CentOS\nDescription:    CentOS release 6.3 (Final)\nRelease:    6.3\nCodename:   Final\n\n ls /home/img/common/cuda-7.5/lib64/lib\\*  \n/home/img/common/cuda-7.5/lib64/libcublas_device.a\n/home/img/common/cuda-7.5/lib64/libcublas.so\n/home/img/common/cuda-7.5/lib64/libcublas.so.7.5\n/home/img/common/cuda-7.5/lib64/libcublas.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libcublas_static.a\n/home/img/common/cuda-7.5/lib64/libcudadevrt.a\n/home/img/common/cuda-7.5/lib64/libcudart.so\n/home/img/common/cuda-7.5/lib64/libcudart.so.7.5\n/home/img/common/cuda-7.5/lib64/libcudart.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libcudart_static.a\n/home/img/common/cuda-7.5/lib64/libcudnn.so\n/home/img/common/cuda-7.5/lib64/libcudnn.so.4\n/home/img/common/cuda-7.5/lib64/libcudnn.so.4.0.7\n/home/img/common/cuda-7.5/lib64/libcudnn_static.a\n/home/img/common/cuda-7.5/lib64/libcufft.so\n/home/img/common/cuda-7.5/lib64/libcufft.so.7.5\n/home/img/common/cuda-7.5/lib64/libcufft.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libcufft_static.a\n/home/img/common/cuda-7.5/lib64/libcufftw.so\n/home/img/common/cuda-7.5/lib64/libcufftw.so.7.5\n/home/img/common/cuda-7.5/lib64/libcufftw.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libcufftw_static.a\n/home/img/common/cuda-7.5/lib64/libcuinj64.so\n/home/img/common/cuda-7.5/lib64/libcuinj64.so.7.5\n/home/img/common/cuda-7.5/lib64/libcuinj64.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libculibos.a\n/home/img/common/cuda-7.5/lib64/libcurand.so\n/home/img/common/cuda-7.5/lib64/libcurand.so.7.5\n/home/img/common/cuda-7.5/lib64/libcurand.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libcurand_static.a\n/home/img/common/cuda-7.5/lib64/libcusolver.so\n/home/img/common/cuda-7.5/lib64/libcusolver.so.7.5\n/home/img/common/cuda-7.5/lib64/libcusolver.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libcusolver_static.a\n/home/img/common/cuda-7.5/lib64/libcusparse.so\n/home/img/common/cuda-7.5/lib64/libcusparse.so.7.5\n/home/img/common/cuda-7.5/lib64/libcusparse.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libcusparse_static.a\n/home/img/common/cuda-7.5/lib64/libnppc.so\n/home/img/common/cuda-7.5/lib64/libnppc.so.7.5\n/home/img/common/cuda-7.5/lib64/libnppc.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libnppc_static.a\n/home/img/common/cuda-7.5/lib64/libnppi.so\n/home/img/common/cuda-7.5/lib64/libnppi.so.7.5\n/home/img/common/cuda-7.5/lib64/libnppi.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libnppi_static.a\n/home/img/common/cuda-7.5/lib64/libnpps.so\n/home/img/common/cuda-7.5/lib64/libnpps.so.7.5\n/home/img/common/cuda-7.5/lib64/libnpps.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libnpps_static.a\n/home/img/common/cuda-7.5/lib64/libnvblas.so\n/home/img/common/cuda-7.5/lib64/libnvblas.so.7.5\n/home/img/common/cuda-7.5/lib64/libnvblas.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libnvrtc-builtins.so\n/home/img/common/cuda-7.5/lib64/libnvrtc-builtins.so.7.5\n/home/img/common/cuda-7.5/lib64/libnvrtc-builtins.so.7.5.18\n/home/img/common/cuda-7.5/lib64/libnvrtc.so\n/home/img/common/cuda-7.5/lib64/libnvrtc.so.7.5\n/home/img/common/cuda-7.5/lib64/libnvrtc.so.7.5.17\n/home/img/common/cuda-7.5/lib64/libnvToolsExt.so\n/home/img/common/cuda-7.5/lib64/libnvToolsExt.so.1\n/home/img/common/cuda-7.5/lib64/libnvToolsExt.so.1.0.0\n/home/img/common/cuda-7.5/lib64/libOpenCL.so\n\nimport tensorflow; print(tensorflow.**version**)\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n0.10.0\n\nI assume this problem might be related to enviroment , for in another machine (centos 7)  and training from scratch(step 0) I never face this nan issue.\n", "@chenghuige I notice that you have cuda 7.5 and cudnn 4.0.7.  Note that we recommend cuda 7.5 and cudnn 5:\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#optional-install-cuda-gpus-on-linux\n\nI'm not sure this is related to your problem, but it's definitely something to try, if possible.\n", "Closing due to inactivity. I'll open again if provided new information.\n"]}, {"number": 4753, "title": "tf.gfile.Glob blocks forever", "body": "### minimal reproducible example\n\n``` python\nimport tensorflow as tf\ntf.gfile.Glob('/path/to/imagenet/validation-*')\n```\n### environment info\n\nOperating System: Ubuntu 14.04 \nTensorFlow version (installed using the official binary): 0.11.0rc0\nPython version: 3.4.3\n", "comments": ["Updates:\n1. The problem exists in Python 2 and 3.\n2. Seems to be a bug in 0.11.0rc0, because it does work with 0.10.\n3. For small folders it works, but not for large folders like the ImageNet folder (it did work in 0.10!).\n", "@rohan100jain has done some work on gfile.Glob, can you take a look?  Thanks!\n", "What filesystem do you have these stored on? I tried to reproduce this with the files local but didn't seem to be slow. \n", "[https://en.wikipedia.org/wiki/IBM_General_Parallel_File_System](GPFS)\n\nPython's builtin globbing seems to work fine.\n", "I see... How is IsDirectory() implemented for that file system? How expensive is that call? Any pointers would be great to look into.\n\nMy hunch is that if that involves network I/O etc. to accomplish and we're doing that call serially in GetMatchingPaths() that would be the bottleneck. Multi-threading that piece of code should help... but I want to get to a reproducible scenario first so that we can compare the time taken before and after. \n", "Thanks a lot for looking into this. I don't know how IsDirectory() is implemented or where to look it up, but I'll try to find something.\n\n@rohan100jain Does your hypothesis explain why TensorFlow 0.10 seemed to work fine and 0.11.0RC0 does not?\n", "**0.10/0.11:** Yeah.. Tensorflow 0.10 used an older version of GetMatchingFiles that just looked at the base directory for matches and couldn't recursive look into sub dirs etc. We updated that in 0.11.\n\n**IsDirectory():** We have a plugin model for file systems... for a new file system like this I would imagine that someone would have implemented our file_system interface (tensorflow/core/platform/file_system.h). Would be good to see that implementation. \n", "@rohan100jain We haven't implemented anything for this filesystem. Besides the additional latency of a network filesystem, it behaves like other file systems. Is there anything I can run that provides output that's useful for debugging?\n", "I created a patch that might help your use case. Can you try it out and see if there is any difference? \n[parallelize_matching.txt](https://github.com/tensorflow/tensorflow/files/537671/parallelize_matching.txt)\n\nIf this helps then I can test / make that change. \n", "Unfortunately, it doesn't help. While the parallelization might make sense nevertheless, the main issue here seems to be that tf.gfile.Glob searches through all subfolders of \"/path/to/imagenet\", despite that a pattern like /path/to/imagenet/validation-\\* makes it impossible for any file in subfolders (that don't not start with validation) to match this pattern. Therefore, I think the solution is to make tf.gfile.Glob's recursion to stop earlier and not search in subfolders that already violate the pattern.\n", "To make it more concrete (sorry I didn't post this earlier):\nRunning `tf.gfile.Glob('/path/to/imagenet/validation-*')` takes about 10 minutes when there exists a subfolder `/path/to/imagenet/raw-data/` that contains roughly 1000 folders with roughly 1000 images each. Clearly, it shouldn't matter whether there is a subfolder `raw-data` or not, because no file in that folder can ever match `'/path/to/imagenet/validation-*'`. When I move `raw-data` somewhere else (outside of the `imagenet` folder), tf.gfile.Glob runs as fast as Python's native globbing (milliseconds).\n", "Okay yeah.. that makes a lot of sense... let me look into that a bit and perhaps come up with something better.\n", "@rohan100jain do we want to fix this in 0.11?\nIt looks like a new feature, so it might be better to add it in the next release.\n", "yeah.. this might take a little time to fix anyways and won't really make any API changes.\n", "I made commit https://github.com/tensorflow/tensorflow/commit/c15bb7b6f64fbc4bfd19aeccfd8b8df99012b74c\n\nLet me know if that helps... it bails out on paths that have no chances to match.\n", "I think this should be fixed now. Closing issue. Please reopen if its still a problem."]}, {"number": 4752, "title": "Use std::ptrdiff_t in place of POSIX-specific ssize_t.", "body": "The type `ssize_t` isn't defined on all platforms, but we use it in `core/lib`. `std::ptrdiff_t` seems to be the closest standard equivalent.\n", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @tensorflower-gardener to be potential reviewers.\n"]}, {"number": 4751, "title": "fix comment in notebook", "body": "", "comments": ["@davidbrai, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vincentvanhoucke and @urimend to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 4750, "title": "Error loading from meta graph, KeyError: u'RsqrtGrad'", "body": "On centos 6.3, I have tested both on tf 0.10 and 0.11rc0, all facing this problem.\nLoading meta graph from saved model meta graph file.\n\n File \"/home/gezi/mine/tensorflow-exp/util/melt/inference/predictor.py\", line 61, in restore\n    saver = tf.train.import_meta_graph(meta_graph)\n  File \"/home/img/common/tensorflow.gpu.bigdata/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1458, in import_meta_graph\n    return _import_meta_graph_def(read_meta_graph_file(meta_graph_or_file))\n  File \"/home/img/common/tensorflow.gpu.bigdata/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1348, in _import_meta_graph_def\n    producer_op_list=producer_op_list)\n  File \"/home/img/common/tensorflow.gpu.bigdata/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 252, in import_graph_def\n    op_def = op_dict[node.op]\nKeyError: u'RsqrtGrad'\n", "comments": ["@chenghuige can you create a minimal test case that reproduces this problem?\n", "@tatatodd  well it's hard to make a minimal test case.\nBut I have find the problem is due to mismatch of tensorflow versions.\nIt only occurs when training using tf 0.11 and loading using tf.0.10.\nSo this might not be a problem.  \n", "@chenghuige I see.\n\nI'll close this out for now, and you can comment on this issue or open a new issue if you still have problems.  Thanks!\n", "@tatatodd @chenghuige \r\nI've met with the same problem, for some reasons however I cant switch to tf 0.11 to load it.\r\nSo have you found any solutions with tensorflow 0.10?\r\nThanks!"]}, {"number": 4749, "title": "Minor instruction issue in functional_ops.py/scan", "body": "I had been using functional_ops.py/scan for implementing RNNs.\n\nI've just checked the instruction, and think this is little bit wrong. \n\nIn master branch, from line 419 in functional_ops.py, it says\n\n```\n  Args:\n    fn: The callable to be performed.  It accepts two arguments.  The first\n      will have the same (possibly nested) structure as `elems`.  The second\n      will have the same structure as `initializer` if one is provided,\n      otherwise it will have the same structure as `elems`.  Its output\n      must have the same structure as `initializer` if one is provided,\n      otherwise it must have the same structure as `elems`.\n```\n\n.\n\nBut actually the first argument of _fn_ should have the same structure as _initializer_, \n\nand the second should have the same structure as _elems_.\n\nAs we can see this in few examples just below, from line 448,\n\n``````\n  Examples:\n    ```python\n    elems = np.array([1, 2, 3, 4, 5, 6])\n    sum = scan(lambda a, x: a + x, elems)\n    # sum == [1, 3, 6, 10, 15, 21]\n    ```\n    ```python\n    elems = np.array([1, 2, 3, 4, 5, 6])\n    initializer = np.array(0)\n    sum_one = scan(\n        lambda a, x: x[0] - x[1] + a, (elems + 1, elems), initializer)\n    # sum_one == [1, 2, 3, 4, 5, 6]\n    ```\n    ```python\n    elems = np.array([1, 0, 0, 0, 0, 0])\n    initializer = (np.array(0), np.array(1))\n    fibonaccis = scan(lambda a, _: (a[1], a[0] + a[1]), elems, initializer)\n    # fibonaccis == ([1, 1, 2, 3, 5, 8], [1, 2, 3, 5, 8, 13])\n    ```\n``````\n\n, the instruction is slightly wrong.\n", "comments": ["@yhg0112 I believe you're right, thanks for filing this!  Would you like to submit a pull request to fix this?\n\nAdding @ebrevdo to keep him in the loop.\n", "@yhg0112 May you mind I create a pull request for this? Or you plan to fix this?\n", "Refer to\n\n```\n      packed_a = output_pack(a_flat)\n      a_out = fn(packed_a, packed_elems) <--\n      nest.assert_same_structure(\n```\n\nThe comment about `fn` is incorrect.\n", "Fyi look at tf.nn.dynamic_rnn also.\n", "@haosdent i absolutely don't mind. please create a pull request for this.\n", "@yhg0112 thank you, create a pr at https://github.com/tensorflow/tensorflow/pull/4870\n", "I think this is closed, as https://github.com/tensorflow/tensorflow/pull/4870 has been committed.\r\n"]}, {"number": 4748, "title": "Configure fail when install from source code", "body": "I Installed with following settings: centOS 7, CUDA 7.5, cudnn v4, python 2.7\n\nAfter I filling in the configurations using\n\n`./configure`\nand I got this error\n\n```\nERROR: /home/[...]/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/dexer/BUILD:3:1: no such target '//external:android/dx_jar_import': target 'android/dx_jar_import' not declared in package 'external' defined by /home/[...]/tensorflow/WORKSPACE and referenced by '@bazel_tools//src/tools/android/java/com/google/devtools/build/android/dexer:dexer'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\nConfiguration finished\n```\n\nWhen I tried with v0.9 there is no such problem. And I don't need those android features.\n\nAnybody encountered the same problem? How can I fix this issue?\n", "comments": ["+1 got the same problem when build it on centos\n", "I don't know whether something wrong with the java environment\n", "Guess what, using bazel with version 3.1.0 can solve the problem. Bazel team should really think of their downward compatibility\n"]}, {"number": 4747, "title": "Added Gitbook support for the documents.", "body": "This fixed #3810. This patch added a script to generate `SUMMARY.md`\nfor Gitbook and then we could use Gitbook to generate the documents in\npdf or epub format.\n", "comments": ["Can one of the admins verify this patch?\n", "The online preview address is https://haosdent.gitbooks.io/tensorflow-document/content/api_docs/\n", "@xmbrst, it looks reasonable to me, but you are the expert on tensorflow docs, so could you have a look?\n", "@haosdent Seems like this only works with relative path instead of full urls? For example, I see the links don't look right in [this page](https://haosdent.gitbooks.io/tensorflow-document/content/tutorials/tflearn/). \n", "hi, @terrytangyuan Thanks for your review, it's because the tensorflow document not follow the markdown syntax. For example,\n\n```\ntf.contrib.learn offers a variety of predefined models, called [`Estimator`s]\n(../../api_docs/python/contrib.learn.md#estimators), which you can use \"out of\n```\n\nshould be\n\n```\ntf.contrib.learn offers a variety of predefined models, called\n[`Estimator`s](../../api_docs/python/contrib.learn.md#estimators), which you can\n```\n\nWe should not insert a new line between `]` and `(`. How about let me fix these errors in documents?\n", "@haosdent Of course. Thanks!\n", "The links are fixed by @haosdent in #4800. @haosdent, would you like to regenerate the preview? \n", "@yifeif Thanks a lot for your ping! I found there are some links `[] ()` (There is a space between `]` and `(`) make the document not perfect after regenerate document. Do you think I should fix those  links first? Or we could continue to add Gitbook support and continue to fix these links later?  \n", "@haosdent thanks for your reply. How much work is involved to fix the links? I would say let's fix the links first. \n", "No problem, would post a patch tomorrow. Let me change this to `[WIP]` \n", "hi, @yifeif @terrytangyuan I create the pull request to fix remain links in https://github.com/tensorflow/tensorflow/pull/4850 Sorry for didn't check it carefully before.\n", "WoW Thanks a lot for your quick response. I have regenerated https://haosdent.gitbooks.io/tensorflow-document/content/tutorials/tflearn/ just now after the broken links fixed.\n", "@tensorflow-jenkins Test this please\n", "Didn't added\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n```\n\nbefore, fixed this just now. \n", "@tensorflow-jenkins Test this please\n", "Hi, @rohan100jain @yifeif Would you help to review this again? Thank you very much.\n", "Hey, thanks for looking into this!  We had a discussion about what the right thing is internally. We're publishing our docs on Github so people can contribute.  Whatever gets checked in here will be published soon after on tensorflow.org.  Alternate publishing routes are absolutely allowed, but we aren't going to accept them in our core docs repository.\n\nI'm going to close this. Feel free to reopen if you have further comments.\n"]}, {"number": 4746, "title": "ksize in max_pool", "body": "I am trying to build a convolution (followed by max_pool) for variable length input size. Since the length of the input in a batch should be the same, I set the batch size to 1. However, for some reason ksize in tf.nn.max_pool is a list attribute required at run-time. This would prevent any neural network that uses variable input size to apply max pool. Is there a workaround for this? Should ksize be an input tensor otherwise?\n", "comments": ["FYI this was also asked about in #1957\n\nWe welcome contributions!\n", "#1957 seems completely unrelated. Am I missing something?\n", "I'm sorry, I meant #1967.  Indeed 1957 is completely unrelated.\n", "I want to work on this issue\n", "found something useful for the development.\nhttp://stackoverflow.com/questions/33919948/how-to-set-adaptive-learning-rate-for-gradientdescentoptimizer\n", "It may be too late for @guoguo12's assistance, but if anyone else wants to jump on this I'm happy to answer questions.  The solution is to make a `MaxPoolV2` C++ op similar to `TopKV2`, and do the same for all the other convolutional ops.  We'd then deprecate the old op with `.Deprecated` and make the Python routines use the new version.  Here's `TopK` and `TopKV2` for comparison:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L2013\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L2048", "(You mean @guotong1988.)", "Whoops, sorry about that.", "Created a PR #9514 to try to address this issue. Would appreciate any review or comments.", "@ayushchd I guess it's a bit late but there is a work around if you need to do your max_pool on a unique dimension. In this case you do not need any kernel size nor strides so you can use reduce_max directly. ", "pool = tf.reduce_max(activation, axis=1, keep_dims=True)"]}, {"number": 4745, "title": "Added changes to scoping to the release notes", "body": "The changes to name scoping and variable scoping are important breaking changes and should be listed as major changes.\n", "comments": ["@jonasrauber, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @gunan and @tensorflower-gardener to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "This should also be in the r0.11 branch. Can you make another PR against r0.11?\n"]}, {"number": 4744, "title": "Poor performance with multi-GPU", "body": "I tried to train the example inception-v3 network on multiple GPUs. The following commands are used:\n/\\* use 4 GPUs */\nbazel-bin/inception/imagenet_train --num_gpus=4 --batch_size=64 --max_steps=2000 --train_dir=... --data_dir=...\n\n/*use 2 GPUs */\nexport CUDA_VISIBLE_DEVICES=0,1\nbazel-bin/inception/imagenet_train --num_gpus=2 --batch_size=32 --max_steps=2000 --train_dir=... --data_dir=...\n\n/*use 1 GPU */\nexport CUDA_VISIBLE_DEVICES=0\nbazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=16 --max_steps=2000 --train_dir=... --data_dir=...\n\nThe performance results are as follows:\nGPUs   Training time(s)       samples/sec\n1           657                          52.7\n2           844                          97.3\n4           1104                        150\n\nThe training time was got from the log file by subtracting the first time from the last time and therefore excluded the start-up time. For example\n2016-10-03 15:28:23.239148: step 0, loss = 13.08 (4.6 examples/sec; 13.916 sec/batch)\n....\n2016-10-03 15:46:47.830959: step 1990, loss = 12.21 (174.6 examples/sec; 0.367 sec/batch)\n\nNow the part that I do not understand is that the samples/sec scales normally with increasing number of GPUs, but the training time does not reduce. Instead the training also increases with more GPUs. \n\nHas anyone observed the similar case? \n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI tried to set --input_queue_memory_factor=0 as in the post https://github.com/tensorflow/models/issues/47, but it does not help.\n\nAlso the post https://github.com/tensorflow/tensorflow/issues/4272 has similar performance issue, but it was not solved.\n### Environment info\n\nOperating System: Redhat 7.2\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root   560184 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): a63b0cbcabc79531e155a0663a08656debf2fe07\n2. The output of `bazel version`: \n\n.\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Sep 29 22:19:27 2016 (1475187567)\nBuild timestamp: 1475187567\nBuild timestamp as int: 1475187567\n### Logs or other output that would be helpful\n\nBy the way, before the actual computation, the overhead of the initialization took >7minutes, is that normal? And I also got many warnings:\nWARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\nWARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\nWARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n\nWill all these warnings impact the performance?\n", "comments": ["@hfutxrg have you seen the following documentation:\nhttps://github.com/tensorflow/models/tree/master/inception\nhttps://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch\n\nIn particular the following section may be useful:\n\n---\n\nA crucial aspect of training a network of this size is training speed in terms of wall-clock time. The training speed is dictated by many factors -- most importantly the batch size and the learning rate schedule. Both of these parameters are heavily coupled to the hardware set up.\n\nGenerally speaking, a batch size is a difficult parameter to tune as it requires balancing memory demands of the model, memory available on the GPU and speed of computation. Generally speaking, employing larger batch sizes leads to more efficient computation and potentially more efficient training steps.\n\nWe have tested several hardware setups for training this model from scratch but we emphasize that depending your hardware set up, you may need to adapt the batch size and learning rate schedule.\n\nPlease see the comments in inception_train.py for a few selected learning rate plans based on some selected hardware setups.\n", "@tatatodd I did read those pages. But based on my understanding, the training time (wall-clock time) is negatively correlated with samples/sec. If samples/sec increases, then the training time should decrease. \n\nDid you mean there is no direct relationship between training time and samples/sec?\n", "@hfutxrg I see your point, and don't have an answer.  Perhaps @poxvoculi can shed some light on this?\n", "The phenomenon reported is interesting.  It's possible there's a bug in the form of a computational inaccuracy somewhere, but it also seems possible (and without further evidence more likely) that this is a natural consequence of SGD learning and the hyper-parameters you've chosen.  In particular, as you've increased the number of GPUs, you've also increased the batch size linearly.  \n\nThere is not a simple, direct relationship between samples/sec and training time.\n\nIt's not the case that 1 batch of 64 is likely to provide equivalent progress to the learning goal as 4 batches of 16.  The learning path from initialized parameters to fully-trained parameters is through a high-dimensional space, and likely very twisty.  Computing the gradient from a larger mini-batch gives a higher expectation of a good approximation of the true gradient at a given point, but doesn't generally justify a much larger learning step, i.e. a larger learning rate, because moving just a little bit can change the true gradient, regardless of whether one's approximation from the last point was accurate.\n\nPicking a minibatch size is a pragmatic compromise, best done empirically.  Suppose for the sake of argument that computing the gradient for a minibatch of size k takes k times longer than for a single sample, and that the expected goodness of the gradient improves by 1/(k+1) whenever you increase the minibatch size by 1.  In this case it seems pretty clear that the best SGD minibatch size is 1, because the expected advantage in gradient accuracy is worth less than being able to take steps faster.  In practice, especially with SIMD devices, the time to compute a minibatch is much less than k times larger than a simple sample, so the best minibatch size will usually be larger than 1.\n\nConsider your edge cases of batch sizes 64 and 16.  At 150 samples per sec, the large batch was 0.43 sec/step, and at 52 samples per sec the small batch was 0.31 secs/step, correct?   Using the small batch, your reported training time was 657 secs or 2120 steps.  Using the large batch the training time was 1104 sec or 2700 steps.   So, the larger batch in this case seems worse, both because the wall clock time is longer, and because the number of steps to completion is larger.\n\nBy the preceding argument, I'm less concerned with the inverse relation between samples/sec and training time than with steps/sec and training time.   What determines when training terminates?  Are you using the same learning rate in both cases?   It strikes me as rather suspicious that you're able to train a model in < 3000 steps, is this just a retraining?\n\nYou might try varying only the number of GPUs, without varying the batch size, to see what effect it has on steps to completion.  Make sure you process the same inputs, in the same order.\n", "Automatically closing due to lack of recent activity. Please reopen when additional information becomes available. Thanks!\n", "I am also experiencing a similar issue. I get almost same total training time while training my network (not cifar) on a single GPU (TitanX 12GB) and 4 GPUs(1080 8GB each), and selecting a proper batch_size via trial and error, single GPU outperforms the 4 GPUs. I anyone is willing to look into it I can provide the necessary details."]}, {"number": 4743, "title": "Add libcurl dependency to gcs_test Dockerfile", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke and @gunan to be potential reviewers.\n"]}, {"number": 4742, "title": "Atrous convolution does not preserve tensor shape", "body": "For an input with an undefined batch size, `atrous_conv2d` emits tensors where all except the final dimension are undefined:\n\n``` python\ninput = tf.placeholder(tf.float32, (None, 256, 256, 3))\n\nconv = tf.nn.conv2d(input, tf.zeros((3, 3, 3, 16)), strides=[1, 1, 1, 1], padding='SAME')\nprint(conv.get_shape()) # Correctly displays (?, 256, 256, 16)\n\ndilated = tf.nn.atrous_conv2d(input, tf.zeros((3, 3, 3, 16)), rate=2, padding='SAME')\nprint(dilated.get_shape()) # Displays (?, ?, ?, 16)\n```\n\n(For concrete batch sizes, everything works as expected.)\n\nTested on `0.10.0rc0`\n", "comments": ["Indeed I have reproduced the problem.  I believe @gpapan implemented atrous_conv2d, and might have thoughts on how easy this would be to fix.\n", ":+1: \n", ":+1: ", "Can I work on this? I think, I can solve this.", "That would be great @AnishShah, can you describe what the problem is?", "@vrv The problem is in this [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L974). It is using `ShapeOp` to estimate `paddings` for `ShapeToBatchOp`. That is why it is not able to predict the output shape. I tried few things, but I was unsuccessful. What do you suggest?", "@AnishShah, do you have any updates?", "I tried, but not able to solve it. Sorry.\n\nOn Feb 15, 2017 2:35 PM, \"Andrew Selle\" <notifications@github.com> wrote:\n\n> @AnishShah <https://github.com/AnishShah>, do you have any updates?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4742#issuecomment-279891930>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADB1P8wK8L7VKrPwcRuxAvXN3hPCgxaoks5rclZtgaJpZM4KNQDp>\n> .\n>\n", "I think this may also shrink the dimensions as well? I believe a source of confusion here may be due to varying definitions of atrous convolution depending on the paper being read. Basically, some papers defined atrous convolutions incorrectly when they really meant dilated convolutions. This is explained in [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122) with the authors' implementation in https://github.com/fyu/dilation.\r\n\r\nAlso see the related issue https://github.com/tensorflow/tensorflow/issues/3492.\r\n\r\nI think what people are hoping for is a new function, or perhaps simply an additional parameter to the atrous function, is the ability to specify a constant scale of the output data so this can behave the same as these papers where the output dimensions are the same as the input, since this is particularly useful for semantic segmentation.\r\n\r\nI believe this is implemented in `tensorflow/models/slim/.../resnet_utils.py` in the function [conv2d_same](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_utils.py#L77). It may even be simple enough to migrate that option directly upstream. @warmspringwinds is also very familiar with this and may be able to verify that everything I've said here is correct, or perhaps contribute some additional information.\r\n\r\n@vrv or @tatatodd regarding the TensorFlow API design, if this version of dilated convolution with constant input/output dimensions is supported directly in `tf.nn` should it be applied via:\r\n 1. `atrous_conv2d` with the `SAME` padding flag\r\n 2. `atrous_conv2d` with a separate parameter\r\n 3.  a totally separate function\r\n\r\n", "I'm going to delegate to @gpapan on this one, who knows more about semantic segmentation and atrous conv :).  I would suggest we'd need a totally separate function because of potential confusion between atrous and dilated.\r\n\r\nThat being said, perhaps someone just posts a good implementation of it here for now instead of having to add it to the API?  (Usually, a good sniff test for adding something to the API is whether it's used / fundamental in a state of the art model for an important problem.  Otherwise everything under the sun gets added to the core API and our team can't support it all).", "@ahundt In tensorflow, \"atrous convolution\" and \"dilated convolution\" are used as synonyms to mean \"dilated convolutions\" as in the [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122) paper you cited.\r\n\r\n@AnishShah tf.nn.convolution now provides a more generic interface for atrous convolution for any number of dimensions, and I believe it has slightly more complete shape inference, but there are still cases where it does not infer some of the output shape dimensions even when it could.  If you are going to add better shape inference code, I suggest adding it to tf.nn.convolution, as there is separate work underway (see #7545) to make atrous_conv2d simply forward to tf.nn.convolution.\r\n\r\nTo fix it you will need to use set_shape function on the output tensors to set the additional shape information.  I think it would be possible to do this inside of with_space_to_batch, specifically on the input_converted tensor and then again on the result_converted tensor.  You will unfortunately have to duplicate some of the work done in calculating the shapes for space_to_batch_nd and batch_to_space_nd.  The reason is that a tensor can either be constant or non-constant, but not partially constant.", "@jbms Thanks for your comment. Does the current code in  [with_space_to_batch](https://github.com/tensorflow/tensorflow/blob/57688fab6ad584034b1e6949e3101adc2c08ca10/tensorflow/python/ops/nn_ops.py#L143) or #7545 have a mode where the output tensor has the same dimensions as the input tensor?\r\n\r\nThis is the case for [conv2d_same](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_utils.py#L77) in `tensorflow/models`:\r\n```\r\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\r\n  \"\"\"[snip...]\r\n  Args:\r\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\r\n  [snip...]\r\n  Returns:\r\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\r\n      the convolution output.\r\n  \"\"\"\r\n```\r\n\r\nI think it would be very productive to add a note in [with_space_to_batch](https://github.com/tensorflow/tensorflow/blob/57688fab6ad584034b1e6949e3101adc2c08ca10/tensorflow/python/ops/nn_ops.py#L143) explaining what the output dimensions would be relative to given input dimensions in as they vary by configuration. \r\n\r\nRegarding your comment on atrous vs dilated convolutions, I quoted the following from a footnote in the [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122):\r\n\r\n> Some recent work mistakenly referred to the dilated convolution operator itself as the `algorithme a trous`. This is incorrect. The algorithme a trous applies a filter at multiple scales to produce a signal decomposition. The algorithm uses dilated convolutions, but is not equivalent to the dilated convolution operator itself.\r\n\r\nPerhaps this is a bit pedantic but if the paper is stating this correctly, wouldn't it mean TensorFlow is mistaken in its use of atrous and dilation as synonyms? This seems to imply that what is described as the atrous algorithm only dilated filter size, while the dilated version can be configured so the output is the same size as the input.\r\n\r\n", "Okay I answered my own question. Yes, both `tf.nn.atrous_conv2d` and `tf.nn.convolution` produce the same output dimensions with the `SAME` flag. I was mixing up the effect of filter size on output dimension, sorry about that. \r\n\r\nI made this test and ran it on tf 1.0 which does confirm the original issue with `None` values:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput_img_np = np.random.random((1, 256, 256, 1)).astype(np.float32)\r\nkernel =  np.random.random((6,6,1,1)).astype(np.float32)\r\n\r\nwith tf.Session() as sess:\r\n    concrete_input_op = tf.constant(input_img_np)\r\n    concrete_output_op = tf.nn.convolution(concrete_input_op, kernel, padding='SAME', dilation_rate=np.array([2, 2]))\r\n    concrete_output = sess.run(concrete_output_op)\r\n    \r\n    print('convolution + CONCRETE + SAME')\r\n    print('concrete_input_op: ', concrete_input_op.get_shape())\r\n    print('concrete_output_op: ', concrete_output_op.get_shape())\r\n    print('concrete_output:', concrete_output.shape)\r\n    assert(concrete_input_op.get_shape() == concrete_output_op.get_shape())\r\n\r\n\r\n    undef_input_op = tf.placeholder(tf.float32, shape=(None, 256, 256, 1))\r\n    undef_output_op = tf.nn.convolution(undef_input_op, kernel, padding='SAME', dilation_rate=np.array([2, 2]))\r\n    undef_output = sess.run(undef_output_op, feed_dict={undef_input_op: input_img_np})\r\n    \r\n    print('convolution + UNDEF + SAME')\r\n    print('undef_input_op: ', undef_input_op.get_shape())\r\n    print('undef_output_op: ', undef_output_op.get_shape())\r\n    print('undef_output:', undef_output.shape)\r\n    # This assert will correctly fail even though the shapes are ok because shapes are only partially known\r\n    # assert(undef_input_op.get_shape() == undef_output_op.get_shape())\r\n\r\n    valid_concrete_input_op = tf.constant(input_img_np)\r\n    valid_concrete_output_op = tf.nn.convolution(valid_concrete_input_op, kernel, padding='VALID', dilation_rate=np.array([2, 2]))\r\n    valid_concrete_output = sess.run(valid_concrete_output_op)\r\n    \r\n    print('convolution + CONCRETE + VALID')\r\n    print('valid_concrete_input_op: ', valid_concrete_input_op.get_shape())\r\n    print('valid_concrete_output_op: ', valid_concrete_output_op.get_shape())\r\n    print('valid_concrete_output:', valid_concrete_output.shape)\r\n\r\n\r\n    valid_undef_input_op = tf.placeholder(tf.float32, shape=(None, 256, 256, 1))\r\n    valid_undef_output_op = tf.nn.convolution(valid_undef_input_op, kernel, padding='VALID', dilation_rate=np.array([2, 2]))\r\n    valid_undef_output = sess.run(valid_undef_output_op, feed_dict={valid_undef_input_op: input_img_np})\r\n    \r\n    print('convolution + UNDEF + VALID')\r\n    print('valid_undef_input_op: ',  valid_undef_input_op.get_shape())\r\n    print('valid_undef_output_op: ', valid_undef_output_op.get_shape())\r\n    print('valid_undef_output:', valid_undef_output.shape)\r\n    # This assert will correctly fail even though the shapes are ok because shapes are only partially known\r\n    # assert(undef_input_op.get_shape() == undef_output_op.get_shape())\r\n    ############################################################################\r\n    # Now atrous\r\n    concrete_input_op = tf.constant(input_img_np)\r\n    concrete_output_op = tf.nn.atrous_conv2d(concrete_input_op, kernel, padding='SAME', rate=2)\r\n    concrete_output = sess.run(concrete_output_op)\r\n    \r\n    print('atrous_conv2d + CONCRETE + SAME')\r\n    print('concrete_input_op: ', concrete_input_op.get_shape())\r\n    print('concrete_output_op: ', concrete_output_op.get_shape())\r\n    print('concrete_output_op: ', concrete_output_op.get_shape())\r\n    print('concrete_output:', concrete_output.shape)\r\n    assert(concrete_input_op.get_shape() == concrete_output_op.get_shape())\r\n\r\n\r\n    undef_input_op = tf.placeholder(tf.float32, shape=(None, 256, 256, 1))\r\n    undef_output_op = tf.nn.atrous_conv2d(undef_input_op, kernel, padding='SAME', rate=2)\r\n    undef_output = sess.run(undef_output_op, feed_dict={undef_input_op: input_img_np})\r\n    \r\n    print('atrous_conv2d + UNDEF + SAME')\r\n    print('undef_input_op: ', undef_input_op.get_shape())\r\n    print('undef_output_op: ', undef_output_op.get_shape())\r\n    print('undef_output:', undef_output.shape)\r\n    # This assert will correctly fail even though the shapes are ok because shapes are only partially known\r\n    # assert(undef_input_op.get_shape() == undef_output_op.get_shape())\r\n\r\n    valid_concrete_input_op = tf.constant(input_img_np)\r\n    valid_concrete_output_op = tf.nn.atrous_conv2d(valid_concrete_input_op, kernel, padding='VALID', rate=2)\r\n    valid_concrete_output = sess.run(valid_concrete_output_op)\r\n    \r\n    print('atrous_conv2d + CONCRETE + VALID')\r\n    print('valid_concrete_input_op: ', valid_concrete_input_op.get_shape())\r\n    print('valid_concrete_output_op: ', valid_concrete_output_op.get_shape())\r\n    print('valid_concrete_output:', valid_concrete_output.shape)\r\n\r\n\r\n    valid_undef_input_op = tf.placeholder(tf.float32, shape=(None, 256, 256, 1))\r\n    valid_undef_output_op = tf.nn.atrous_conv2d(valid_undef_input_op, kernel, padding='VALID', rate=2)\r\n    valid_undef_output = sess.run(valid_undef_output_op, feed_dict={valid_undef_input_op: input_img_np})\r\n    \r\n    print('atrous_conv2d + UNDEF + VALID')\r\n    print('valid_undef_input_op: ',  valid_undef_input_op.get_shape())\r\n    print('valid_undef_output_op: ', valid_undef_output_op.get_shape())\r\n    print('valid_undef_output:', valid_undef_output.shape)\r\n    # This assert will correctly fail even though the shapes are ok because shapes are only partially known\r\n    # assert(undef_input_op.get_shape() == undef_output_op.get_shape())\r\n```\r\nWhich produces this output with the additional `None` values on the last set of printouts:\r\n```\r\nconvolution + CONCRETE + SAME\r\n('concrete_input_op: ', TensorShape([Dimension(1), Dimension(256), Dimension(256), Dimension(1)]))\r\n('concrete_output_op: ', TensorShape([Dimension(1), Dimension(256), Dimension(256), Dimension(1)]))\r\n('concrete_output:', (1, 256, 256, 1))\r\nconvolution + UNDEF + SAME\r\n('undef_input_op: ', TensorShape([Dimension(None), Dimension(256), Dimension(256), Dimension(1)]))\r\n('undef_output_op: ', TensorShape([Dimension(None), Dimension(256), Dimension(256), Dimension(1)]))\r\n('undef_output:', (1, 256, 256, 1))\r\nconvolution + CONCRETE + VALID\r\n('valid_concrete_input_op: ', TensorShape([Dimension(1), Dimension(256), Dimension(256), Dimension(1)]))\r\n('valid_concrete_output_op: ', TensorShape([Dimension(1), Dimension(246), Dimension(246), Dimension(1)]))\r\n('valid_concrete_output:', (1, 246, 246, 1))\r\nconvolution + UNDEF + VALID\r\n('valid_undef_input_op: ', TensorShape([Dimension(None), Dimension(256), Dimension(256), Dimension(1)]))\r\n('valid_undef_output_op: ', TensorShape([Dimension(None), Dimension(246), Dimension(246), Dimension(1)]))\r\n('valid_undef_output:', (1, 246, 246, 1))\r\natrous_conv2d + CONCRETE + SAME\r\n('concrete_input_op: ', TensorShape([Dimension(1), Dimension(256), Dimension(256), Dimension(1)]))\r\n('concrete_output_op: ', TensorShape([Dimension(1), Dimension(256), Dimension(256), Dimension(1)]))\r\n('concrete_output_op: ', TensorShape([Dimension(1), Dimension(256), Dimension(256), Dimension(1)]))\r\n('concrete_output:', (1, 256, 256, 1))\r\natrous_conv2d + UNDEF + SAME\r\n('undef_input_op: ', TensorShape([Dimension(None), Dimension(256), Dimension(256), Dimension(1)]))\r\n('undef_output_op: ', TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(1)]))\r\n('undef_output:', (1, 256, 256, 1))\r\natrous_conv2d + CONCRETE + VALID\r\n('valid_concrete_input_op: ', TensorShape([Dimension(1), Dimension(256), Dimension(256), Dimension(1)]))\r\n('valid_concrete_output_op: ', TensorShape([Dimension(1), Dimension(246), Dimension(246), Dimension(1)]))\r\n('valid_concrete_output:', (1, 246, 246, 1))\r\natrous_conv2d + UNDEF + VALID\r\n('valid_undef_input_op: ', TensorShape([Dimension(None), Dimension(256), Dimension(256), Dimension(1)]))\r\n('valid_undef_output_op: ', TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(1)]))\r\n('valid_undef_output:', (1, 246, 246, 1))\r\n```", "@ahundt , in TF atrous and dilated convolution mean the same thing. One of the parameters that they accept is ```rate``` which specifies the dilation rate. Definition of rate is consistent in the Deep Lab paper and the paper that you have cited. I think the confusion with the naming is similar to the case with ```deconvolution``` which a lot of people use to mean that they perform ```fractionally strided convolution```, while ```deconvolution``` at the same time refers to a completely different operation in Signal Processing field.\r\n\r\nThere are different ways to implement dilated convolution. TF has it implemented by sampling the input feature map which is described in the Deep Lab paper. The piece of code that you refer to actually uses this implementation under the hood.\r\n\r\nAt the same time, dilated convolution is itself an ordinary convolution -- meaning that if you apply it with the ```same``` padding, it should produce the output with the same spatial dimensions.\r\n\r\nIn case of Image Segmentation, dilated convolution is used to make it possible to use weights\r\nfrom Image Classification networks after reducing their in-network downsampling (by means of removing layers responsible for downsampling or setting their stride to ```1```). All of this allows to acquire the prediction map that is downsampled by a smaller factor (most Image Classification models have a downsampling factor of ```32``` --  for example, it can be reduced to ```8``` by following the approach described in these papers). After that, you can use bilinear upsampling or learn the upsampling kernel yourself during training to get the prediction map of the same size as the input image. You can find an example of adopting Resnet-101 for Image Segmentation by employing the aforementioned approach [here](https://github.com/warmspringwinds/tf-image-segmentation/blob/master/tf_image_segmentation/models/resnet_v1_101_8s.py#L108).", "@warmspringwinds Thanks! Got it now. \r\n\r\nAll, sorry I ended up hijacking the issue due to the mismatch between my mental model and the design. At least a test script came from it and I learned something, thanks for the clarifications. :-)", "@ahundt The precise definition of with_space_to_batch is given in the docstring.  However, the actual output dimensions depend entirely on the behavior of the underlying `op` that is passed as an argument.  I don't think it is possible to specify the output dimensions in a particularly concise way.  It isn't intended to be used directly normally, but rather is intended to be used to define new dilated operations.", "#8411 doesn't actually fix this issue --- it just adds some documentation, but does not actually improve the static tensor shape information, which is what this issue is about.", "Sorry about that.", "Sorry, that was actually my fault! I meant to write that it resolves a point of confusion discussed in this issue.", "No, it's my fault, I did edit your description to make the PR close this issue. That was a little optimistic.", "Aha didn't realize that, well at least things are set correctly now.", "This seems to be fixed in at least tensorflow version 1.1.0-rc2!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing since this seems obsolete, but please reopen if it needs attention."]}, {"number": 4741, "title": "Branch 135053757", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @dsmilkov and @charlesnicholson to be potential reviewers.\n", "@tensorflow-jenkins test this please.\n", "Known issue breaking jenkins test. Closing this one and will push again when rollback fix is submitted internally. \n"]}, {"number": 4740, "title": "fully_connected_preloaded_var.py unexpectedly slow", "body": "From http://stackoverflow.com/questions/39840323/benchmark-of-howto-reading-data -- `examples/tutorials/mnist/fully_connected_feed.py` runs roughly 10x faster per step (`feed_dict`) than `examples/tutorials/mnist/fully_connected_preloaded_var.py` (TF variable with `slice_input_producer`). Looking at Timeline (attached), it seems the training is bottlenecked on QueueDequeueMany which takes 13ms\nwhich is about 100x slower than corresponding segment on `feed_dict` example [timeline](%28https://github.com/tensorflow/tensorflow/files/507483/timeline.feed.json.zip%29). Also, the timing taken by `QueueDequeueMany` scales linearly with batch-size. IE, increasing batch-size to 200 makes it take 22ms.\n\n[timeline.var.json.zip](https://github.com/tensorflow/tensorflow/files/507476/timeline.var.json.zip)\n", "comments": ["Let's consolidate this to #4732, and follow-up on that issue.  Thanks!\n"]}, {"number": 4739, "title": "Feature request: Implement QR decomposition", "body": "Feature request:  A tensorflow op that performs numerically stable (works on non full rank matrices) QR decomposition along with gradients so that it can be used in layers and cost functions.\n", "comments": ["Contributions are welcome!\n", "@kstant0725 I want to try adding this implement.\n", "This seems to have been added in https://github.com/tensorflow/tensorflow/commit/715f951eb9ca20fdcef20bb544b74dbe576734da.", "I seem to get the following error when I try to use the QR in a network.  Was the gradient implemented?\r\n\r\nLookupError: No gradient defined for operation 'Qr_1' (op type: Qr)", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 4738, "title": "Feature Request: Include function argument defaults in the documentation", "body": "I have recently started using functions from tf.contrib. They speed things up a lot. Thank you. \n\nI did notice that some of the prototypes in the documents have their arguments replaced with `(*args, **kwargs)`. See tf.contrib.layers: https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers.html#layers-contrib. Perhaps this is automatically done if the number of arguments exceeds some number. Unfortunately the prototype is the only place where the argument defaults are shown. I have been looking at the source file, layers.py, to see the full prototype for the defaults which is fine, but probably not idea. \n\nMy request is to include the argument defaults somewhere in the documentation. I wouldn't mind long prototypes, i.e. getting rid of the `(*args, **kwargs)`. Or if people want the shorter prototypes, maybe the argument defaults could be automatically included somewhere in the description.\n", "comments": ["Thanks for filing the issue @markpwoodward!  Indeed the documentation is suboptimal.\n\n@martinwicke might have some ideas on who to involve to get this fixed.\n", "This is a problem with the `@arg_scope` decorator, which being a decorator, changes the function signature to `**kwargs` and `*args`. @sguada can we fix this by making our doc generator inspect through arg_scope and use the function args form the inside?\n", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 4737, "title": "No gradient defined for operation EluGrad", "body": "tf.nn.elu does not support second derivatives currently. \n\nThis would be awesome, as in variational autoencoder models these activation functions improve performance by a few nats in the objective compared to tanh, sigmoid (which do support second derivatives).\n\nIn the meantime I'm using `elu = lambda x: tf.select(x < 0., tf.exp(x) - 1., x)` which seems to work:\n\n```\nIn [29]: elu = lambda x: tf.select(x < 0., tf.exp(x) - 1., x)\n\nIn [30]: sess.run(tf.gradients(elu(z), z), {z: -1.})\nOut[30]: [0.36787945]\n\nIn [31]: sess.run(tf.gradients(elu(z), z), {z: 1.})\nOut[31]: [1.0]\n```\n", "comments": ["Contributions are welcome!\n", "I want to work on it !\n", "This issue should probably be closed.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 4736, "title": "Build Failing Compiling with CUDA 8.0, CUDNN 5.1.5, Bazel 0.3.1", "body": "Building Tensorflow with Bazel 0.3.1 + CUDA 8.0 + CUDNN 5.1.5 and latest GIT is failing\n\n`# bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\n\nHere is the end of the error.\n\n```\nexternal/local_config_cuda/cuda/include/math_functions.h(8897): error: cannot overload functions distinguished by return type alone\n\nexternal/local_config_cuda/cuda/include/math_functions.h(8901): error: cannot overload functions distinguished by return type alone\n\n./tensorflow/core/framework/allocator.h(154): warning: missing return statement at end of non-void function \"tensorflow::Allocator::RequestedS\nize\"\n\n2 errors detected in the compilation of \"/tmp/tmpxft_000027b0_00000000-7_spacetobatch_functor_gpu.cu.cpp1.ii\".\nERROR: /root/src/tensorflow/tensorflow/core/kernels/BUILD:1673:1: output 'tensorflow/core/kernels/_objs/batch_space_ops_gpu/tensorflow/core/ke\nrnels/spacetobatch_functor_gpu.cu.pic.o' was not created.\nERROR: /root/src/tensorflow/tensorflow/core/kernels/BUILD:1673:1: not all outputs were created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n### Environment info\n\n**Graphics card capability 3.0**\n\n`# lspci -v | grep -i nvidia`\n\n```\n01:00.0 VGA compatible controller: NVIDIA Corporation GK104 [GeForce GTX 760] (rev a1) (prog-if 00 [VGA controller])\n        Kernel driver in use: nvidia\n        Kernel modules: nouveau, nvidia_drm, nvidia\n01:00.1 Audio device: NVIDIA Corporation GK104 HDMI Audio Controller (rev a1)\n```\n\n`# git rev-parse HEAD`\n`3d35376a66cde4f3e614c746d3c8708d15caa1b5`\n\n`# bazel version`\n\n```\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Sun Oct 2 01:11:55 2016 (1475370715)\nBuild timestamp: 1475370715\nBuild timestamp as int: 1475370715\n```\n\n**CUDNN 5.1.5**\n\n```\nls -l /opt/cuda/lib64 | grep cudnn\nlrwxrwxrwx 1 root root        13 Oct  1 20:55 libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root        17 Oct  1 20:55 libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root  79337624 Oct  1 20:55 libcudnn.so.5.1.5\n-rw-r--r-- 1 root root  69756172 Oct  1 20:55 libcudnn_static.a`\n```\n\n**CUDA 8.0.44**\n\n```\n# ls -l /opt/cuda/lib64 | grep libcuda\n-rw-r--r-- 1 root root    558720 Sep 28 17:41 libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Sep 28 17:41 libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root        19 Sep 28 17:41 libcudart.so.8.0 -> libcudart.so.8.0.44\n-rwxr-xr-x 1 root root    415432 Sep 28 17:41 libcudart.so.8.0.44\n-rw-r--r-- 1 root root    775162 Sep 28 17:41 libcudart_static.a\n```\n\n**GCC 6.2.1**\n\n```\n# gcc --version\ngcc (GCC) 6.2.1 20160830\nCopyright (C) 2016 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n```\n### Other Issues Experienced\n\nFirst compile error was described here, i used @junyer's proposed solution\nhttps://github.com/google/re2/issues/102\n", "comments": ["I resolved the issue. I tried the default gcc for my system, 6.2.1, which failed. I then tried a symlink to gcc-5, which failed in another way (complained about the cross compiler), but when I provided the full path to /usr/bin/gcc-5 in the ./configure step, the compilation worked.\n", "Nice to hear you worked it out @derekwisong!\n"]}, {"number": 4735, "title": "Resource exhausted error in the middle of training", "body": "I train the inception v1 (slim) model on my own data set. 269 classes total.\nThe max training step is 60435 and batch size is 256 as below\n**--max_number_of_steps=60435\n--batch_size=256**\nThe model runs under GPU mode and I have 4 Tian X GPUs, with each has 12G GPU memory.\nThe Resource exhausted error happen after at least 46831 trainning steps, since i can see the last check point file is model.ckpt-46831.\n\nI do not know why the issue happen in the middle, but not at very beginning of the training process.\n\nThe error log report by Tensor Fow is as below:\n\n//other lines above.\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 11.16GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: \n**Limit:                 12049707828\nInUse:                 11984328960\nMaxInUse:              12038083584\nNumAllocs:               248036306\nMaxAllocSize:           2831155200**\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] *************************************************************************************************_xx\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] *_Ran out of memory trying to allocate 39.81MiB.**  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:968] Resource exhausted: OOM when allocating tensor with shape[256,832,7,7]\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors.ResourceExhaustedError'>, OOM when allocating tensor with shape[256,112,112,64]\n     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]\n     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference', defined at:\n  File \"train_image_classifier.py\", line 585, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"train_image_classifier.py\", line 482, in main\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n  File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/deployment/model_deploy.py\", line 195, in create_clones\n    outputs = model_fn(_args, *_kwargs)\n  File \"train_image_classifier.py\", line 466, in clone_fn\n    logits, end_points = network_fn(images)\n  File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/nets_factory.py\", line 103, in network_fn\n    return func(images, num_classes, is_training=is_training)\n  File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py\", line 288, in inception_v1\n    net, end_points = inception_v1_base(inputs, scope=scope)\n  File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py\", line 61, in inception_v1_base\n    net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\n    return func(_args, *_current_args)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 445, in convolution2d\n    outputs = normalizer_fn(outputs, *_normalizer_params)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\n    return func(_args, **current_args)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 250, in batch_norm\n    mean, variance = nn.moments(inputs, axis, shift=shift)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py\", line 835, in moments\n    y, axes, shift=shift, keep_dims=keep_dims, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py\", line 762, in sufficient_statistics\n    v_ss = math_ops.squared_difference(x, shift)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2347, in squared_difference\n    result = _op_def_lib.apply_op(\"SquaredDifference\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2386, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,112,112,64]\n     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]\n     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nTraceback (most recent call last):\n  File \"train_image_classifier.py\", line 585, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"train_image_classifier.py\", line 581, in main\n    sync_optimizer=optimizer if FLAGS.sync_replicas else None)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 781, in train\n    raise\n  File \"/usr/lib/python2.7/contextlib.py\", line 35, in **exit**\n    self.gen.throw(type, value, traceback)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 969, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 797, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 296, in stop_on_exception\n    yield\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 481, in run\n    self.run_loop()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 999, in run_loop\n    self._sv.global_step])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 717, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 915, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 985, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[256,112,112,64]\n     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]\n     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference', defined at:\n  File \"train_image_classifier.py\", line 585, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"train_image_classifier.py\", line 482, in main\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n  File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/deployment/model_deploy.py\", line 195, in create_clones\n    outputs = model_fn(_args, *_kwargs)\n  File \"train_image_classifier.py\", line 466, in clone_fn\n    logits, end_points = network_fn(images)\n  File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/nets_factory.py\", line 103, in network_fn\n    return func(images, num_classes, is_training=is_training)\n  File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py\", line 288, in inception_v1\n    net, end_points = inception_v1_base(inputs, scope=scope)\n  File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py\", line 61, in inception_v1_base\n    net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\n    return func(_args, *_current_args)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 445, in convolution2d\n    outputs = normalizer_fn(outputs, *_normalizer_params)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\n    return func(_args, **current_args)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 250, in batch_norm\n    mean, variance = nn.moments(inputs, axis, shift=shift)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py\", line 835, in moments\n    y, axes, shift=shift, keep_dims=keep_dims, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py\", line 762, in sufficient_statistics\n    v_ss = math_ops.squared_difference(x, shift)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2347, in squared_difference\n    result = _op_def_lib.apply_op(\"SquaredDifference\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2386, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,112,112,64]\n     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]\n     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n", "comments": ["It seems that this crash or OOM issue happen after program write .ckpt file and start the following training steps.\n\nthe error is after line : **INFO:tensorflow:global_step/sec: 0.648611**, which program  just output .ckpt and start the following training steps again. But why previous loop (output *.ckpt file and continue training ) has no issue?\n\nINFO:tensorflow:global_step/sec: 0.648596\nINFO:tensorflow:global step 15200: loss = 2.2506 (1.53 sec/step)\nINFO:tensorflow:global step 15400: loss = 2.5318 (1.54 sec/step)\nINFO:tensorflow:global step 15600: loss = 2.3716 (1.54 sec/step)\nINFO:tensorflow:global step 15800: loss = 2.2514 (1.54 sec/step)\nINFO:tensorflow:global step 16000: loss = 2.3969 (1.53 sec/step)\nINFO:tensorflow:global step 16200: loss = 2.4346 (1.54 sec/step)\nINFO:tensorflow:global step 16400: loss = 2.1810 (1.54 sec/step)\nINFO:tensorflow:global step 16600: loss = 2.1595 (1.54 sec/step)\nINFO:tensorflow:global step 16800: loss = 2.2038 (1.53 sec/step)\nINFO:tensorflow:global step 17000: loss = 2.2215 (1.54 sec/step)\nINFO:tensorflow:global step 17200: loss = 2.4344 (1.54 sec/step)\nINFO:tensorflow:global step 17400: loss = 2.1812 (1.54 sec/step)\nINFO:tensorflow:global_step/sec: 0.648348\nINFO:tensorflow:global step 17600: loss = 2.5870 (1.54 sec/step)\nINFO:tensorflow:global step 17800: loss = 2.4107 (1.54 sec/step)\nINFO:tensorflow:global step 18000: loss = 2.3646 (1.54 sec/step)\nINFO:tensorflow:global step 18200: loss = 2.2475 (1.54 sec/step)\nINFO:tensorflow:global step 18400: loss = 2.2889 (1.54 sec/step)\nINFO:tensorflow:global step 18600: loss = 2.3609 (1.54 sec/step)\nINFO:tensorflow:global step 18800: loss = 2.3364 (1.54 sec/step)\nINFO:tensorflow:global step 19000: loss = 2.3291 (1.53 sec/step)\nINFO:tensorflow:global step 19200: loss = 2.1952 (1.54 sec/step)\nINFO:tensorflow:global step 19400: loss = 2.3241 (1.54 sec/step)\nINFO:tensorflow:global step 19600: loss = 2.4700 (1.54 sec/step)\nINFO:tensorflow:global step 19800: loss = 2.2185 (1.53 sec/step)\n**INFO:tensorflow:global_step/sec: 0.648611**\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):   Total Chunks: 26, Chunks in use: 0 6.5KiB allocated for chunks. 104B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):   Total Chunks: 7, Chunks in use: 0 3.8KiB allocated for chunks. 28B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):  Total Chunks: 2, Chunks in use: 0 4.2KiB allocated for chunks. 8B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096):  Total Chunks: 2, Chunks in use: 0 12.2KiB allocated for chunks. 196B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536):     Total Chunks: 1, Chunks in use: 0 104.5KiB allocated for chunks. 4B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144):    Total Chunks: 2, Chunks in use: 0 598.2KiB allocated for chunks. 40.08MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576):   Total Chunks: 4, Chunks in use: 0 5.78MiB allocated for chunks. 4.06MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608):   Total Chunks: 1, Chunks in use: 0 9.91MiB allocated for chunks. 520.0KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216):  Total Chunks: 2, Chunks in use: 0 52.23MiB allocated for chunks. 7.66MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432):  Total Chunks: 1, Chunks in use: 0 53.59MiB allocated for chunks. 1.53MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):     Total Chunks: 1, Chunks in use: 0 147.00MiB allocated for chunks. 39.81MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 784.00MiB was 256.00MiB, Chunk State: \nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2328c00000 of size 1280\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2328c00500 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2328c00600 of size 256\n", "@civilmanxx please fill out the following information (which was part of the default template when you clicked on the green \"new issue\" button):\n\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n### What other attempted solutions have you tried?\n\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "Operating System: Ubuntu 16.04\n\nFor CUDA:\nls /usr/local/cuda/lib/libcud*\n**/usr/local/cuda/lib/libcudadevrt.a  /usr/local/cuda/lib/libcudart.so.7.5     /usr/local/cuda/lib/libcudart_static.a\n/usr/local/cuda/lib/libcudart.so    /usr/local/cuda/lib/libcudart.so.7.5.18**\n\nFor Tensorflow (from source):\n\ngit rev-parse HEAD\n3d35376a66cde4f3e614c746d3c8708d15caa1b5\n\nFor bazel version: \n\n**Build label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392**\n\n---\n\nAll other info is on the first 2 threads.\n\nHow to reproduce:\n\nTry ImageNet dataset, I use only 269 classes of total 1000. \nModel: Inception V1 (slim). \n**--max_number_of_steps=60435\n--batch_size=256\n--save_interval_secs=1200 \n--save_summaries_secs=1200** \nHardware: 4 Tian X GPU, with each has 12GB memery\n", "BTW, i reduce batch size to 128 and haven't met the issue again. I do not know why batch size=256 make program crash in the middle, but not at the very beginning. As see the log in thread 2, the program crash after dumping .ckpt file (line of **INFO:tensorflow:global_step/sec: 0.648611**)and continue the following training. Is there any Cuda Device Reset or Memory Clear is called after dump .ckpt file?\n", "@civilmanxx good to hear that running with a smaller batch size has fixed your immediate problem.\n\n@zheng-xq might know more about the specifics of why with a bigger batch size, we only crash in middle, and not at the beginning.  Or other things to look at.\n", "yeah, reduce batch size from 256 to 128 will double the training time.\n", "Depending on the width of the model, TensorFlow can be somewhat non-deterministic in terms of scheduling. Right now, it schedule ops that are ready to go eagerly. That could cause some models to use more memory sometimes. The effect is often not huge. \n\nBut this could make your training to fail of your model is dangerously close to the edge. A few things to try: \n1. Use a slightly smaller batch size. \n2. Set TF_CUDNN_WORKSPACE_LIMIT_IN_MB to a smaller number. The default is 4GB. \n3. If your model might have large number of parallel ops, group them together and add a control dependency to serialize them sometimes help. This is not recommended in general. \n\nIf it really takes a huge number of steps to fail, some groups tend to ignore it. Load from a previously good checkpoint and restart your training automatically. \n", "Thanks for your info.\n\n1, I can see at lease 4 code files have TF_CUDNN_WORKSPACE_LIMIT_IN_MB\nwhere can i change this value? Is there any config file, which i can set it once only?\n\n2, For your solution about resume training after crash. So far i do not know which command line can do this. Other popular DL framework, such as caffe support this directly.\n", "It is an environment variable. \n\nSet env-var to TF_CUDNN_WORKSPACE_LIMIT_IN_MB=100, so you only allow 100MB in scratch space for each conv op. \n", "so you mean i can set this value to 12000, that is 12GB, as I am using Tian X GPU. if reduce the chance of out of memory error, i can set a lower value, like 11000 or 10000. is that correct?\n", "@civilmanxx I had the same issue before. You can check whether your have enough space in the directory that the .ckpt files are stored. When I run inception-v3, by default those files are stored in /tmp and my machine only has 2 GB for /tmp, and then the \"resource exhausted\" error happened.\n", "it says run out of memory. I think it has no issue with hard disk. I do not put .ckpt file in /tmp folder, but another one, which should not have limitation on storage.\n", "> so you mean i can set this value to 12000, that is 12GB, as I am using Tian X GPU. if reduce the chance > of out of memory error, i can set a lower value, like 11000 or 10000. is that correct?\r\n\r\nNo. TF_CUDNN_WORKSPACE_LIMIT_IN_MB is how much to use for scratch space. This is temporary scratch space that is used by individual cudnn ops. If you set it to 12000, you will have no space in your GPU  to store tensors persistently or for TensorFlow to use as scratch space.\r\n\r\nYou are at the cusp of running out of memory. There is likely memory fragmentation going on which means that as your process goes on you are unable to allocate temporary buffers anymore. TF_CUDNN_WORKSPACE_LIMIT_IN_MB reduces the scratch space which reduces the chance you will see an out of memory. There are several things you \r\n\r\n1. Use  a smaller batch size. You said that worked with 128 but is twice as slow. What about 192?  Try using a binary search to choose the largest batch size.\r\n\r\n2. Restart the process periodically.  Checkpoint every step 10000 steps and stop the training process and  restart. This will reset the memory fragmentation.\r\n\r\n3. Improve the GPU block allocator to be more efficient. That memory allocator tries to avoid fragmentation. Insert instrumentation into it, try to measure the fragmentation and verify that is indeed your problem. Then, try to devise a better allocator algorithm that reduces fragmentation but doesn't reduce performance.\r\n\r\nObviously 1 and 2 are easy solutions and 3 is a long solution. Computers simply do not work well when pushed to the edge of memory utilization. Our memory allocators could be conservative and add a buffer and try to stop you at training step 0 before you get anywhere, but that would prevent some safe models from working and would not be guaranteed anyways. Memory allocation patterns are not deterministic in a parallel environment, unfortunately.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "I still get the same error and my batch size is only 10. But the input to my CNN is a 1x500 spectrogram. When I use a 1x200 spectrogram there's no error.", "In my case, using :\r\n\r\n`Keras.backend.clear_session() `\r\n\r\nright after I am done with a model and before calling or instantiating a new one solves the problem (assuming a single model runs well without issues).\r\n\r\nI found the answer here (full credit to that poster):\r\nhttp://forums.fast.ai/t/how-could-i-release-gpu-memory-of-keras/2023/11", "Hello, \r\nIts my first time posting an issue, its more of a query itd be nice if anyone can help me out asap :crossed_fingers:  \r\n\r\nI'm using google colab with a GPU backend, \r\n\r\nI'm having a similar issue,\r\n\r\n------error :{================================\r\n\r\nResourceExhaustedError (see above for traceback): \r\nOOM when allocating tensor with shape[12708,50320] \r\nand type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Initializer/random_uniform/RandomUniform}} = RandomUniform[T=DT_INT32, _class=[\"loc:@rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Assign\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Initializer/random_uniform/shape)]]\r\nHint:... ..... \r\n========================================}\r\n(let me know i you need the complete log)\r\n\r\n\r\n\r\nMODEL:------------------------------\r\n\r\nembedding_size = 128\r\nbatch_size = 8 (failing at 4 too )\r\ntar_vocab = 12580\r\n\r\ncell = mrc(  cells= [  lstm(num_units = embedding_size,  dtype=tf.float32 ), \r\n                               lstm(embedding_size), \r\n                               lstm (  tar_vocab) ])\r\nstate0 = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\r\n\r\n\r\ndef runn( inp, seq_len, batch_size=8 ):\r\n    seqs, _  = tf.nn.dynamic_rnn(cell= cell,\r\n                                     initial_state=state0,\r\n                                     inputs= inp,\r\n                                     sequence_length = seq_len ,\r\n                                     time_major=False )\r\n    return seqs \r\n\r\n\r\nlogits = runn(   )\r\ncost = reduce_mean(softmax_cross_entropy( logits, Y ) )\r\n \r\nopt = tf.train.AdamOptimizer(learning_rate = 0.0007)\r\nidk = opt.minimize( cost, aggregation_method = tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N )\r\n\r\n\r\n=============\r\n\r\nusing a generator for training batch data \r\n--------------------------------------------------\r\n\r\nWHAT I'VE tried:\r\na. reducing batch size from 16 to4 \r\nb.reducing embedding size from 512 to 64  \r\nc. I read the following thread on stack overflow seems really convincing  \r\n  https://stackoverflow.com/questions/36139889/rnn-model-running-out-of-memory-in-tensorflow\r\n  but none of the aggregation method's working   \r\n\r\n\r\nSUSPICIONS :\r\na. gpu memory is full before ram \r\nb. I'm not sure about tf internals but could it be coz I'm using a for loop inside a session  here's the training loop \r\n\r\ndef train( batch_size):\r\n    \r\n    test = gen( batch_size  )\r\n    batches = np.asarray(x_train).shape[0] // batch_size\r\n    batches=10 # breakPoint (NOT EVEN 10 BATCHES ARE GETTING PAST IT  of size 4 example  )\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        costs = []\r\n        for batch in batches :\r\n            sample = next(test)\r\n            sample_x = np.array(sample[0])\r\n            sample_y = np.array(sample[1])\r\n            \r\n            cost, _  = sess.run([ cost,idk],\r\n                                feed_dict= {\r\n                                    plc_x: sample_x, \r\n                                    plc_y: sample_y })\r\n            costs.append(cost)\r\n    return costs\r\n", "Notebook to reGenerate the: https://colab.research.google.com/drive/1Y93EdHeSIT5ik1PnVeJ1CfU7ClapiaOQ \r\n\r\nUPDATE  :\r\n\r\nlooks like the error is occuring at global_variable_initialisation \r\nheres the full log \r\n==================================================\r\nResourceExhaustedError                    Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1291     try:\r\n-> 1292       return fn(*args)\r\n   1293     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1276       return self._call_tf_sessionrun(\r\n-> 1277           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1278 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1366         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1367         run_metadata)\r\n   1368 \r\n\r\nResourceExhaustedError: OOM when allocating tensor with shape[12836,50320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Initializer/random_uniform/RandomUniform}} = RandomUniform[T=DT_INT32, _class=[\"loc:@rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Assign\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Initializer/random_uniform/shape)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nResourceExhaustedError                    Traceback (most recent call last)\r\n<ipython-input-38-e7395c10000b> in <module>()\r\n----> 1 train( batch_size )\r\n\r\n<ipython-input-37-97871c34bfb9> in train(batch_size)\r\n     11     with tf.Session(config=config) as sess:\r\n     12 \r\n---> 13         sess.run(tf.global_variables_initializer())\r\n     14 \r\n     15 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    885     try:\r\n    886       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 887                          run_metadata_ptr)\r\n    888       if run_metadata:\r\n    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1109       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1110                              feed_dict_tensor, options, run_metadata)\r\n   1111     else:\r\n   1112       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1284     if handle is None:\r\n   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1286                            run_metadata)\r\n   1287     else:\r\n   1288       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1306           self._config.experimental.client_handles_error_formatting):\r\n   1307         message = error_interpolation.interpolate(message, self._graph)\r\n-> 1308       raise type(e)(node_def, op, message)\r\n   1309 \r\n   1310   def _extend_graph(self):\r\n\r\nResourceExhaustedError: OOM when allocating tensor with shape[12836,50320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Initializer/random_uniform/RandomUniform}} = RandomUniform[T=DT_INT32, _class=[\"loc:@rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Assign\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Initializer/random_uniform/shape)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nCaused by op 'rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Initializer/random_uniform/RandomUniform', defined at:\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-34-dfec99fe8c0f>\", line 2, in <module>\r\n    res1  = run_bitch( plc_x, plc_seq_len, batch_size )\r\n  File \"<ipython-input-21-7a1aec9218d8>\", line 11, in run_bitch\r\n    time_major=False )\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py\", line 664, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py\", line 868, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3274, in while_loop\r\n    return_same_structure)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2994, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2929, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3243, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py\", line 834, in _time_step\r\n    skip_conditionals=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py\", line 275, in _rnn_step\r\n    new_output, new_state = call_cell()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py\", line 822, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 233, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\", line 364, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 769, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1484, in call\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 370, in __call__\r\n    *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\", line 364, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 759, in __call__\r\n    self.build(input_shapes)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\", line 149, in wrapper\r\n    output_shape = fn(instance, input_shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 923, in build\r\n    partitioner=maybe_partitioner)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 472, in add_variable\r\n    return self.add_weight(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\", line 278, in add_weight\r\n    getter=vs.get_variable)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 586, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py\", line 591, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1484, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1234, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 521, in get_variable\r\n    return custom_getter(**custom_getter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 236, in _rnn_get_variable\r\n    variable = getter(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 492, in _true_getter\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 920, in _get_single_variable\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 145, in __call__\r\n    return cls._variable_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 141, in _variable_call\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 120, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2441, in default_variable_creator\r\n    expected_shape=expected_shape, import_scope=import_scope)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 147, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1104, in __init__\r\n    constraint=constraint)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1212, in _init_from_args\r\n    initial_value(), name=\"initial_value\", dtype=dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 894, in <lambda>\r\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py\", line 483, in __call__\r\n    shape, -limit, limit, dtype, seed=self.seed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\", line 242, in random_uniform\r\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_random_ops.py\", line 733, in random_uniform\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[12836,50320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Initializer/random_uniform/RandomUniform}} = RandomUniform[T=DT_INT32, _class=[\"loc:@rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Assign\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Initializer/random_uniform/shape)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\n==============================================================  \r\n", "> In my case, using :\r\n> \r\n> `Keras.backend.clear_session() `\r\n> \r\n> right after I am done with a model and before calling or instantiating a new one solves the problem (assuming a single model runs well without issues).\r\n> \r\n> I found the answer here (full credit to that poster):\r\n> http://forums.fast.ai/t/how-could-i-release-gpu-memory-of-keras/2023/11\r\n\r\nI am facing with the same 'resource exhaustion' issue in middle of training. For some reason, I have to finetune a pre-trained 'Imagenet' inceptionV3 model for my task for 400 times in a loop. After some iterations, I face this problem. I tried using K.clear_session() but that throws another error ''Cannot interpret feed_dict key as Tensor: ' + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(7, 7, 3, 64), dtype=float32) is not an element of this graph.'\r\nAny suggestions?"]}, {"number": 4734, "title": "Unable to restore session with batch_norm", "body": "I am saving my model during training with saver = tf.train.Saver() and then conditionally saver.save(sess,filename). It works for my model and allows me to restore the session later if I do not use batch normalization. With batch_norm layers in the model (either the one in contrib.layers or contrib.slim) I get the following error when I try to restore the session:\n\n`FailedPreconditionError: Attempting to use uninitialized value Variable\n     [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable)]]\n     [[Node: conv0/moments/sufficient_statistics/Shape/_32 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_483_conv0/moments/sufficient_statistics/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Variable/read', defined at:\n  File \"/usr/lib/python2.7/dist-packages/spyderlib/widgets/externalshell/start_ipython_kernel.py\", line 197, in <module>\n    __ipythonkernel__.start()\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 458, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/minitornado/ioloop.py\", line 678, in start\n    self._handlers[fd](fd, events)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/minitornado/stack_context.py\", line 302, in wrapped\n    ret = fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/minitornado/stack_context.py\", line 302, in wrapped\n    ret = fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 279, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 247, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 396, in execute_request\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2660, in run_cell\n    interactivity=interactivity, compiler=compiler)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2770, in run_ast_nodes\n    if self.run_code(code):\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2820, in run_code\n    exec code_obj in self.user_global_ns, self.user_ns\n  File \"<ipython-input-1-3936af1513f1>\", line 1, in <module>\n    runfile('/home/jason/code/test_prototypes.py', wdir='/home/jason/code')\n  File \"/usr/lib/python2.7/dist-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 540, in runfile\n    execfile(filename, namespace)\n  File \"/home/jason/code/test_prototypes.py\", line 53, in <module>\n    l2_reg=l2_reg)\n  File \"model.py\", line 147, in __init__\n    W = _uniform_weight(filter_shape)\n  File \"model.py\", line 27, in _uniform_weight\n    return tf.Variable(initial)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 215, in __init__\n    dtype=dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 327, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1106, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2333, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1252, in __init__\n    self._traceback = _extract_stack()`\n", "comments": []}, {"number": 4733, "title": "Bidirectional RNN: forward and backward passing to the middle of the sentence", "body": "Hi all, I have a task that requires me to split a sentence into two and model the two parts respectively using bidirectional rnn, e.g.:\n\n**\"I think of A, but I am going to B tomorrow\"** split into **\"I think of A\"** and **\"but I am going to B tomorrow\"**; forward passing goes from \"I\" to \"A\" while backward passing goes from \"tomorrow\" to \"but\". This way two outputs are trained using directional rnn and then concatenated before feeding to the softmax layer.\n\nMy question is, is there an easier to achieve this using bidirectional_rnn in Tensorflow that I am not aware of? Or I have to split the sentence first, reverse the order of the 2nd part, and then model the two parts using two rnn networks respectively (which is what I am doing now). \n\nThanks!! \n", "comments": ["Thanks for the question @bluemonk482\n\nWe primarily use github issues to track bugs, installation issues, and feature requests.   This is a question better suited for StackOverflow.  Please ask it there and tag it with the `tensorflow` tag.  Thanks!\n"]}]