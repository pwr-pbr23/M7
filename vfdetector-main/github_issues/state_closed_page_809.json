[{"number": 29267, "title": "[TF 2.0 API Docs] tf.guarantee_const", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/guarantee_const\r\n\r\n## Description of issue (what needs changing):\r\n\r\nNo usage example is provided and the link does not exist. The raises are also not defined.\r\n\r\n### Correct links\r\n\r\nThe link does not exists and is also a simple text\r\n\r\n### Raises listed and defined\r\n\r\nRaises are not listed\r\n\r\n### Usage example\r\n\r\nNo usage example provided.\r\n\r\n### Submit a pull request?\r\n\r\nNo", "comments": ["This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Raised [a CL](https://critique-ng.corp.google.com/cl/370898060) to fix this issue. Thanks!", "@imransalam \r\nThis has been resolved in [CL](375565442), can you please confirm and move this to closed status.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29266, "title": "Clarified first sentence.", "body": "", "comments": ["Amazing work @VikramTiwari \r\nLGTM \ud83d\udc4d ", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 29265, "title": "[TF 2.0 API Docs] tf.group", "body": "https://github.com/tensorflow/tensorflow/issues/29262\r\nThe file changes are according to this issue raised. Added more in the description and made a simple usage example.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29265) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I've signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29265) for more info**.\n\n<!-- ok -->", "Can one of the admins verify this patch?", "@yashk2810 Can you please take a look on this PR? Thanks!", "@imransalam Can you please address Ubuntu Sanity errors? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 29264, "title": "[TF 2.0 API Docs] tf.clip_by_norm", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/clip_by_norm\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Raises listed and defined\r\n\r\nErrors are not defined.\r\n\r\n### Request visuals, if applicable\r\n\r\nNo visuals are included.\r\n", "comments": ["Added a PR #29264 for the fix."]}, {"number": 29263, "title": "Update group docstring in control_flow_ops", "body": "https://github.com/tensorflow/tensorflow/issues/29257\r\nThe file changes are according to this issue raised. Added more in the description and made a simple usage example.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29263) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 29262, "title": "[TF 2.0 API Docs] tf.group", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/group\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py\r\n\r\n## Description of issue (what needs changing):\r\n\r\nA clear description should be added and a proper usage example is to be added. \r\n\r\n### Clear description\r\n\r\nFor example:\r\nA group operations can run multiple operations at the same time, operations are not sequential and they will be executed when tf.group will be called.\r\n\r\n### Usage example\r\n\r\nThere is no usage example provided except for a link to where it's being used.\r\n\r\n### Submit a pull request?\r\nYes\r\nhttps://github.com/tensorflow/tensorflow/pull/29265", "comments": ["The documentation has been updated. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/group"]}, {"number": 29261, "title": "[TF 2.0 API Docs] tf.VariableAggregation", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/VariableAggregation\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe description is not opinionated about when to use this symbol, and unclear on what aggregation methods for combining gradients would be useful for.\r\n\r\n### Parameters defined\r\n\r\nParameters are poorly defined, and not formatted appropriately.\r\n\r\n### Returns defined\r\n\r\nReturns are not defined.\r\n\r\n### Raises listed and defined\r\n\r\nErrors are not defined.\r\n\r\n### Usage example\r\n\r\nNo usage example is provided.\r\n\r\n### Request visuals, if applicable\r\n\r\nNo visuals are included.", "comments": ["@noahhkim,\r\nSorry for the delayed response. Can you please let us know if you would be interested to contribute by submitting a PR? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29260, "title": "[TF 2.0 API Docs] tf.greater_equal", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/math/greater_equal\r\n\r\n## Description of issue (what needs changing):\r\n\r\nCorrect link is not provided in the sense that it is only a text and not an actual link to the file.\r\nNo usage example is given in the documentation.\r\nRaises are also not listed\r\n\r\n### Correct links\r\n\r\nCorrect link is not provided in the sense that it is only a text and not an actual link to the file.\r\n\r\n### Raises listed and defined\r\n\r\nRaises are also not listed\r\n\r\n### Usage example\r\n\r\nNo usage example is given in the documentation.", "comments": ["This is fixed under `tf.math` module. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/greater_equal"]}, {"number": 29259, "title": "Failed to load the native TensorFlow runtime", "body": "**System information**\r\n- OS Platform: Windows10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: Python 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)] on win32\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: Cuda V10.0.130/ cuDNN: cudnn-10.0-windows10-x64-v7.5.0.56\r\n- GPU model and memory: RTX 2070 8G\r\n- NVIDIA Driver: NVIDIA-SMI 416.34       Driver Version: 416.34       CUDA Version: 10.0\r\n\r\n**Packages**\r\nPackage              Version\r\n-------------------- -------\r\nabsl-py              0.7.1\r\nastor                0.8.0\r\ngast                 0.2.2\r\ngrpcio               1.21.1\r\nh5py                 2.9.0\r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.0\r\nMarkdown             3.1.1\r\nmock                 3.0.5\r\nnumpy                1.16.4\r\npip                  19.1.1\r\nprotobuf             3.8.0\r\nsetuptools           41.0.1\r\nsix                  1.12.0\r\ntensorboard          1.13.1\r\ntensorflow-estimator 1.13.0\r\ntensorflow-gpu       1.13.1\r\ntermcolor            1.1.0\r\nvirtualenv           16.6.0\r\nWerkzeug             0.15.4\r\nwheel                0.33.4\r\n\r\n**Description**\r\nI have tried every possible combination of CUDA/cuDNN and I still cannot make it work (last 3 days fighting, I need help). I even installed 3.7 (tried already with python 3.6.6 and 3.68)\r\n\r\n**Stack-trace**\r\n(venv) F:\\workspace\\ml>python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"F:\\workspace\\ml\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"F:\\workspace\\ml\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"F:\\workspace\\ml\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"F:\\workspace\\ml\\venv\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"F:\\workspace\\ml\\venv\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"F:\\workspace\\ml\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"F:\\workspace\\ml\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"F:\\workspace\\ml\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"F:\\workspace\\ml\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"F:\\workspace\\ml\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"F:\\workspace\\ml\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"F:\\workspace\\ml\\venv\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"F:\\workspace\\ml\\venv\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n**Paths**\r\nC:\\WINDOWS\r\nC:\\WINDOWS\\system32\r\nC:\\WINDOWS\\System32\\Wbem\r\nF:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\r\nF:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\extras\\CUPTI\\libx64\r\nF:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\r\nF:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\TensorRT-5.0.4.3\\lib\r\nF:\\Program Files\\NVIDIA GPU Computing Toolkit\\cuDNN\\cudnn-10.0-windows10-x64-v7.5.0.56\\bin\r\nF:\\Program Files\\NVIDIA GPU Computing Toolkit\\cuDNN\\cudnn-10.0-windows10-x64-v7.5.0.56\\include\r\nC:\\Users\\JIAM\\AppData\\Local\\Programs\\Python\\Python37-32\\\r\nC:\\Users\\JIAM\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\\r\nC:\\Program Files\\NVIDIA Corporation\\NVSMI\r\n...\r\n(In this order)\r\n", "comments": ["Installing tensorflow-gpu with pip proved to be one of the most frustrating experiences.\r\nI still do not know what it was missing, after a while looked and installed using conda.\r\n\r\nIf I ever find out what was the problem I will post, I almost certain it is the way the package is compiled; I used dependency walker and the only thing I noticed is an error related to windows API references which I am not even sure why were even there to begging with unless the package was compiled using a Windows 8 machine.\r\n", "Just to verify did you try to follow instructions from [TensorFlow website](https://www.tensorflow.org/install/pip) for installing TF binary. Also we need to install proper Microsoft Visual c++ version. Please have a look or let us know if you have already tried the steps. Thanks! ", "> Just to verify did you try to follow instructions from [TensorFlow website](https://www.tensorflow.org/install/pip) for installing TF binary. Also we need to install proper Microsoft Visual c++ version. Please have a look or let us know if you have already tried the steps. Thanks!\r\n\r\nI followed  the instructions. \r\nAs a note: Using dependency walker I found issues with API-MS-WIN-XXXXX.DLL and EXT-MS-WIN-XXX.\r\n ", "Please have a look on this [issue](https://github.com/tensorflow/tensorflow/issues/28848#issuecomment-495057676) and let us know if that helps to resolve the problem. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Instructions that I followed\r\n\r\nWithout CUDA: https://www.pugetsystems.com/labs/hpc/How-to-Install-TensorFlow-with-GPU-Support-on-Windows-10-Without-Installing-CUDA-UPDATED-1419/\r\n\r\nWith CUDA: https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781\r\n\r\nAlso useful for future reference: https://medium.com/@mainakdutta76/gpu-configuration-for-deep-learning-f35a8bf0f5e1\r\n\r\n"]}, {"number": 29258, "title": "Fix link to TF-TRT doc", "body": "", "comments": []}, {"number": 29257, "title": "[TF 2.0 API Docs] tf.greater", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/math/greater\r\n\r\n## Description of issue (what needs changing):\r\n\r\nCorrect link is not provided in the sense that it is only a text and not an actual link to the file.\r\nNo usage example is given in the documentation.\r\nRaises are also not listed\r\n\r\n### Correct links\r\n\r\nCorrect link is not provided in the sense that it is only a text and not an actual link to the file.\r\n\r\n### Raises listed and defined\r\n\r\nRaises are not listed in the documentation\r\n\r\n### Usage example\r\n\r\nThere is no usage example provided", "comments": ["This has been fixed. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/greater"]}, {"number": 29256, "title": "change 'py_func' to 'py_function' in https://www.tensorflow.org/beta/guide/data#applying_arbitrary_python_logic", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\n#https://www.tensorflow.org/alpha/guide/data#applying_arbitrary_python_logic\r\nhttps://www.tensorflow.org/beta/guide/data#applying_arbitrary_python_logic\r\n## Description of issue (what needs changing):\r\n\r\nminor typo, just need to update `py_func` in example so it is `py_function`\r\n\r\n### Clear description\r\n\r\nIn last 4 lines of code block underneath \"applying arbitrary python logic\" section\r\n\r\n```Python\r\ndataset = dataset.map(\r\n    lambda filename, label: tuple(tf.py_func(\r\n        _read_py_function, [filename, label], [tf.uint8, label.dtype])))\r\ndataset = dataset.map(_resize_function)\r\n```\r\n\r\n### Correct links\r\n\r\nYes\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\nYes\r\n\r\n### Usage example\r\n\r\nYes\r\n\r\n### Request visuals, if applicable\r\n\r\nNo, not needed\r\n\r\n### Submit a pull request?\r\n\r\nNo\r\n", "comments": ["Thanks for the report, David.\r\nThat file is over here, would you mind making the fix and then I can merge it: https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/data.md\r\n\r\n", "Closing this issue since the PR https://github.com/tensorflow/docs/pull/697 has been merged. Thanks!"]}, {"number": 29255, "title": "Update api.py to_code", "body": "\r\nReference https://github.com/tensorflow/tensorflow/issues/29254 for changes", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 29254, "title": "[TF 2.0 API Docs] tf.autograph.to_code", "body": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/to_code\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nInitial description could be clearer, instead of referring to another function as similar (`to_graph`), restate primary use case: \r\n* From: \u201cSimilar to to_graph, but returns Python source code as a string.\u201d\r\n* To: \u201c`to_code` is a low-level API that returns the AutoGraph generated Python source code as a string. This is similar to `to_graph`, which returns the TensorFlow graph, instead of Python.\u201d\r\n\r\n### Usage example\r\n\r\nNo usage example in docs, only references to guides/example, would suggest uplifting an example from a guide to the docs for completeness (ref https://www.tensorflow.org/alpha/guide/autograph)\r\n\r\n\r\n### Submit a pull request?\r\n\r\nYes", "comments": ["@ghchinoy I notice you raise a PR to resolve this issue. Unfortunately that was not against `master` so the PR was closed. Can you submit that PR against `master`. Thanks!", "@ghchinoy Can you submit this PR against `master`? You can copy and paste the closed PR and open it against `master` that will be helpful to the community. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29253, "title": "[TF 2.0] - freeze_graph not working with converted keras to tensorflow serving model using tf.keras.experimental.export_savedmodel", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nModel saved as tf.keras .hdf5 converted to tensorflow model using  tf.keras.experimental.export_savedmodel and when using freeze_graph to get a single .pb file running into error \r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nmodel_path = '/home/vsrira10/Desktop/model.hdf5'\r\nmodel = tf.keras.models.load_model(model_path)\r\ntf.keras.experimental.export_savedmodel(model,newdir)\r\n```\r\nAfter this a variables folder with files [checkpoint,variables.data-00000-of-00001,variables.index], saved_model.pb and assests folder created in newdir.\r\n\r\nI am trying to use saved_model.pb and variables.data-00000-of-00001 files to get single .pb frozen_graph\r\n**Other info / logs**\r\npython /home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py --input_graph=/home/vsrira10/Desktop/tf_models/saved_model.pb --input_checkpoint=/home/vsrira10/Desktop/tf_models/variables/variables.data-00000-of-00001 --output_graph=/home/vsrira10/Desktop/tf_models/frozen_graph.pb --output_node_names=classes,corners --input_binary=true\r\nTraceback (most recent call last):\r\n  File \"/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 492, in <module>\r\n    run_main()\r\n  File \"/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 489, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 488, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 382, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 341, in freeze_graph\r\n    input_graph_def = _parse_input_graph_proto(input_graph, input_binary)\r\n  File \"/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 252, in _parse_input_graph_proto\r\n    input_graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n", "comments": ["This is expected behavior. `freeze_graph` is not supported in 2.0. It will be removed from the pip in the near future.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29253\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29253\">No</a>\n", "> This is expected behavior. `freeze_graph` is not supported in 2.0. It will be removed from the pip in the near future.\r\n\r\nIf freeze_graph is not supported what should I use instead?\r\n", "In 2.0, the primary export format is [SavedModels](https://www.tensorflow.org/alpha/guide/saved_model) so APIs are built to directly support SavedModels. Inference on existing frozen graphs can be run using the `v1.compat` path.\r\n\r\nIf there is a tool that is currently not supporting SavedModels in 2.0, please report that as a separate issue.", "I'll try to comment here about this post:\r\n\r\n> This is expected behavior. `freeze_graph` is not supported in 2.0. It will be removed from the pip in the near future.\r\n\r\nWell, I need to use freeze_graph to have a saved model compatible with opencv cv::dnn::readNetFromTensorflow in C++. Have you have ever considered this situation? What are your plans to continue supporting this?\r\n\r\nThank you.\r\n\r\n@gargn \r\n", "Also, coreml doesn't work with TF 2.0 now, only with frozen graph. But I need to convert my model to it. ", "Seriously, this is kind of crazy.\r\n\r\nAn entire ecosystem was built around using protocol buffers - none of those tools work with TF 2.0 since TF 2.0 can't export PB's as far as I can tell, especially if TF 2.0 has bug fixes for models that won't export properly from TF 1.14 - \r\n\r\nSo right now I\r\n\r\n* can build a  model in TF 1.14 or 2.0\r\n* Cant export said model Keras h5 since it used TF.Layers rather than Keras layers\r\n* Cant export a PB since said model has  some weird bug thats yet to be fixed in TF 1.1.4\r\n* Cant export a PB in 2.0 runtime, using 1.x combat runtime in TF 2.0 triggers the TF 1.x bug.\r\n* Cant export SavedModel format to any other format\r\n* SavedModel PB defaults to some very weird SERVING PB which, I never asked for.\r\n* Tensorflow project doesn't provide a method of back porting a SavedModel to a PB in an external tool.\r\n\r\nI guess I can't use TF?\r\n", "I was able to generate a frozen tensorflow model from a TF 2.0 model. First step is to save the weights of the `tf.keras` model with `model.save_weights('xxx.h5')`. \r\n\r\nWith a conda environment or another computer with TF 1.xx installed (I had 1.14), you generate an untrained `tf.keras` model identical to the model you want to freeze and load the weights with `model.load_weights('xxx.h5')`. \r\n\r\nAt this point you can save the full model in a h5 file with `model.save(...)` and then freeze it.", "@efournie if it doesn\u2019t take a lot of time could you provide small code example? ", "I use a `create_model()` function to generate an untrained tf.keras model:\r\n\r\n```\r\ndef create_model():\r\n    # Define your tf.keras model here, used also to train your network in the TF 2.0 environment\r\n    # ...\r\n    return Model(m_input, m_output)\r\n```\r\nIn a conda environment with TF 2.0.0rc0 installed, I train the network generated with `model = create_model()`. Once trained, the model weights are saved with `model.save_weights('result_w.h5')`.\r\n\r\nI then activate another conda environment with TF 1.14 installed (in my case it is named tf1) and transfer the weights to a blank model:\r\n```\r\nD:\\dl>conda activate tf1\r\n(tf1) D:\\dl>python\r\nPython 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> from create_model import create_model\r\n>>> final_model = create_model()\r\n>>> final_model.load_weights('result_w.h5')\r\n>>> final_model.save('trained_model.h5')\r\n```\r\n\r\nTo freeze the model, I use the following `freeze_model.py` script:\r\n```\r\nimport tensorflow as tf\r\nimport argparse\r\nimport uuid\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow.keras as keras\r\nimport shutil\r\nimport os\r\nimport subprocess\r\nfrom pathlib import Path\r\nfrom constants import *\r\n\r\nanaconda_path = Path(\"C:/Program Files (x86)/Microsoft Visual Studio/Shared/Anaconda3_64/\")\r\ntf_tools_path = os.path.join(anaconda_path, \"Lib/site-packages/tensorflow/python/tools/\")\r\npython_cmd = os.path.join(anaconda_path, \"python.exe\")\r\nfreeze_script = os.path.join(tf_tools_path, \"freeze_graph.py\")\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n\r\nparser = argparse.ArgumentParser(description=\"Convert Keras .h5 file to Tensorflow .pb frozen model.\")\r\nparser.add_argument(\"--h5\", type=str, default=\"model.h5\", help=\"Keras .h5 file (input)\")\r\nparser.add_argument(\"--pb\", type=str, default=\"model.pb\", help=\"Tensorflow frozen model (output)\")\r\nargs = parser.parse_args()\r\n\r\nK.set_learning_phase(0)\r\n\r\nmodel = keras.models.load_model(args.h5)\r\nmodel_output = model.output.op.name\r\ntempdir = str(uuid.uuid4())\r\nos.mkdir(tempdir)\r\ntry:\r\n    temp_tf_file = str(os.path.join(tempdir, \"tf.ckpt\"))\r\n    saver = tf.train.Saver()\r\n    saver.save(K.get_session(), temp_tf_file)\r\n    tmp_meta_file = str(temp_tf_file) + \".meta\"\r\n\r\n    saver = tf.train.import_meta_graph(tmp_meta_file, clear_devices=True)\r\n    K.get_session().run(tf.global_variables_initializer())\r\n    K.get_session().run(tf.local_variables_initializer())\r\n    sess = K.get_session()\r\n    saver.restore(sess, tf.train.latest_checkpoint(os.path.expanduser(tempdir)))\r\n\r\n    output_node_names = model_output\r\n\r\n    # for fixing the bug of batch norm\r\n    gd = sess.graph.as_graph_def()\r\n    for node in gd.node:\r\n        if node.op == 'RefSwitch':\r\n            node.op = 'Switch'\r\n            for index in xrange(len(node.input)):\r\n                if 'moving_' in node.input[index]:\r\n                    node.input[index] = node.input[index] + '/read'\r\n        elif node.op == 'AssignSub':\r\n            node.op = 'Sub'\r\n            if 'use_locking' in node.attr: del node.attr['use_locking']\r\n        elif node.op == 'AssignAdd':\r\n            node.op = 'Add'\r\n            if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\r\n    converted_graph_def = tf.graph_util.convert_variables_to_constants(sess, gd, output_node_names.split(\",\"))\r\n    tf.train.write_graph(converted_graph_def, \".\", args.pb, as_text=False)\r\n    print(\"Done.\")\r\nfinally:\r\n    shutil.rmtree(tempdir)\r\n```\r\nI can now convert the trained model to a frozen pb file with `python freeze_model.py --h5 trained_model.h5 --pb trained_model.pb`\r\n\r\nI can't avoid saving the weights and transferring them to an untrained model because saving the full model in TF 2.0 and loading it in TF 1.14 doesn't work."]}, {"number": 29252, "title": "[TF 2.0] Dataset has no attribute 'make_one_shot_iterator'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): 2.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nReport attribute error as :\r\n **AttributeError: 'BatchDataset' object has no attribute 'make_one_shot_iterator'**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n# Install TensorFlow\r\n!pip install tensorflow==2.0.0-alpha0\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras.models as Models\r\nimport numpy as np\r\n\r\ninc_dataset = tf.data.Dataset.range(100)\r\ndec_dataset = tf.data.Dataset.range(0, -100, -1)\r\ndataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\r\nbatched_dataset = dataset.batch(4)\r\n\r\niterator = batched_dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n```\r\n\r\n", "comments": ["As per [Release 2.0.0-alpha0](https://www.libupdate.com/libs/e0a82003-3ced-44dd-82c7-f8196968440e), tf.data.Dataset.make_one_shot_iterator() has been deprecate  in V1, removed from V2, and added to tf.compat.v1.data.make_one_shot_iterator().\r\n", "@BingyuZhou Did you get chance to try as suggest by @srijithrajeev . I had tried and no error was shown.\r\ntf.compat.v1.data.make_one_shot_iterator(batched_dataset)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "is there a counterpart of one_shot_iterator in tf 2.0. how can I keep the context of a dataset?", "Can someone address the question from @breadbread1984? There doesnt appear to be any clear guidance in the docs regarding what the best practices are in the absence of `one_shot_iterator`.", "```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras.models as Models\r\nimport numpy as np\r\n\r\ninc_dataset = tf.data.Dataset.range(100)\r\ndec_dataset = tf.data.Dataset.range(0, -100, -1)\r\ndataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\r\nbatched_dataset = dataset.batch(4)\r\n\r\n\r\n#iterator = batched_dataset.make_one_shot_iterator()\r\n#next_element = iterator.get_next()\r\n\r\nfor next_element in batched_dataset:\r\n    tf.print(next_element)\r\n```", "iterator is necessary when you use multiple trainset alternatively for training. for example, training mtcnn requires both wider face and celeba. you have to use iterator rather than a loop. but I have solved the problem buy tf.data.Dataset.__iter__()", "Hi @breadbread1984, could you please elaborate a bit how you managed to work around this issue? I'm trying to run the mnist.py from models/official/mnist and it fails in the  eval_input_fn() for exact same reason - no  make_one_shot_iterator() method. \r\n\r\nHere is the code that fails:\r\n```\r\n\r\n  def eval_input_fn():\r\n    batched_dataset = dataset.test(flags_obj.data_dir).batch(flags_obj.batch_size)\r\n    return batched_dataset.make_one_shot_iterator().get_next()\r\n```", "@abratchik you can create iterator with\r\n\r\n```python\r\n  def eval_input_fn():\r\n    batched_dataset = dataset.test(flags_obj.data_dir).batch(flags_obj.batch_size)\r\n    return batched_dataset.__iter__()\r\n```\r\n\r\nyou can get a batch with \r\n\r\n```python\r\niter = eval_input_fn();\r\nsample = next(iter);\r\n```", "> @abratchik you can create iterator with\r\n> \r\n> ```python\r\n>   def eval_input_fn():\r\n>     batched_dataset = dataset.test(flags_obj.data_dir).batch(flags_obj.batch_size)\r\n>     return batched_dataset.__iter__()\r\n> ```\r\n> \r\n> you can get a batch with\r\n> \r\n> ```python\r\n> iter = eval_input_fn();\r\n> sample = next(iter);\r\n> ```\r\n\r\nthank you sir, that's the solution.", "You just use iter() function like below.\r\n\r\niterator = iter(batched_dataset)\r\nnext_element = iterator.get_next()", "What is the solution for an error like this for below function?\r\n\r\n```\r\ndef input_data():\r\n    filenames = get_filenames()\r\n    print(len(filenames))\r\n    train_dataset = tf.data.Dataset.from_tensor_slices(filenames)\r\n    train_dataset = train_dataset.shuffle(100).repeat()\r\n    train_dataset = train_dataset.map(parse_image, num_parallel_calls=4).batch(batch_size)\r\n    return train_dataset.make_one_shot_iterator()\r\n```\r\n\r\nError is\r\n```\r\nTraceback (most recent call last):\r\n  File \"soft_n_cut_loss.py\", line 307, in <module>\r\n    iterator = input_data()\r\n  File \"C:\\IU\\Spring 2020\\CSCI-B 657 Computer Vision\\Project\\unsupervised\\wnet2\\W-Net\\input_data.py\", line 26, in input_data\r\n    return train_dataset.make_one_shot_iterator()\r\nAttributeError: 'BatchDataset' object has no attribute 'make_one_shot_iterator'\r\n```\r\n\r\nIf I use the iterator s mentioned in the comments above then I get error as:\r\n```\r\nTraceback (most recent call last):\r\n  File \"soft_n_cut_loss.py\", line 307, in <module>\r\n    iterator = input_data()\r\n  File \"C:\\IU\\Spring 2020\\CSCI-B 657 Computer Vision\\Project\\unsupervised\\wnet2\\W-Net\\input_data.py\", line 27, in input_data\r\n    iterator = iter(train_dataset)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 334, in __iter__\r\n    raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\r\nRuntimeError: __iter__() is only supported inside of tf.function or when eager execution is enabled.\r\n```\r\n\r\nCan you please help me, how I can progress in this case? It would be great if someone can help me with the change I need to make in my function so that it works\r\n", "Hey,\r\n\r\nIt's working for me. I did below 2 things:\r\n\r\n1. Added this in import libraries, earlier it was import tensorflow as tf\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n```\r\n\r\n2. In main function, added the below snippet\r\n```\r\niterator = input_data()\r\nimages = iterator.get_next()\r\n```\r\n", "Hello,\r\nThis solves my error. \r\n\r\n\r\n```\r\n# import tensorflow as tf\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n```\r\n\r\n```\r\niterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\r\n```", "@kvijaysai  Hi, downgrading the v2 to fit the compatibility is not a good workaround to deal with such kind of things. You can try the following code\r\n``` python\r\ndef input_data():\r\n    filenames = get_filenames()\r\n    print(len(filenames))\r\n    train_dataset = tf.data.Dataset.from_tensor_slices(filenames)\r\n    train_dataset = train_dataset.shuffle(100).repeat()\r\n    train_dataset = train_dataset.map(parse_image, num_parallel_calls=4).batch(batch_size)\r\n    return train_dataset\r\ntrain_iter = iter(train_dataset)\r\nfor I in range(step_num):\r\n    input, output = train_iter.get_next()\r\n```\r\nYou could check [tf.data.Iterator](https://www.tensorflow.org/api_docs/python/tf/data/Iterator) for more info.", "I have got a similar problem. I deploy my trainingdataset and validation dataset on multi GPUs.\r\nfor the training dataset, I use the following code to build the dataset, while for the validation dataset, that requires me to iterate for each time of specified steps. Thus, I choose `tf.dataset.Iterator` to deal with such things, as the above comment shows.\r\nHowever, when I tried to deploy the `val_iter` to multi gpus, the error is thrown. Anyone could help me out with this.\r\nthe error is `'OwnedIterator' object has no attribute '_variant_tensor'` which points to the last line of the following code. Note that I use `iter` to make val_dataset iterable. but the TF2 forbids me to do that.\r\n``` python\r\nwith self.mirrored_strategy.scope():\r\n    self.model = model(config)\r\n    self.optimizer = tf.optimizers.Adam(learning_rate=self.config[\"lr\"])\r\n    self.train_dataset, self.val_dataset, self.test_dataset = dataloader()\r\n    self.train_dataset = self.mirrored_strategy.experimental_distribute_dataset(self.train_dataset)\r\n    self.val_dataset = self.mirrored_strategy.experimental_distribute_dataset(iter(self.val_dataset))\r\n```\r\nbtw, my TF version is the latest 2.3", "You can create an iterator using the following function (Tenforflow 2x):\r\n\r\n```python\r\ndef dataset_iterator(output_types, output_shapes):\r\n\r\n    dataset = tf.data.Dataset.from_generator(self._training_data_generator,\r\n                                                       output_types=output_types,\r\n                                                       output_shapes=output_shapes)\r\n    dataset = dataset.repeat().prefetch(prefetch_batches)      \r\n    dataset_iterator = dataset.__iter__()\r\n    \r\n    return dataset_iterator\r\n\r\n```", "> \r\njust return iterator is feasible, but im not sure how to make sure that self._training_data_generator is right,do i need to write a for-loop and yield something? here's part of my code\uff1a\r\n```python\r\n    def individual_batch_generator(self,inputs,targets):\r\n        assert len(inputs) == len(targets)\r\n        indices = np.arange(len(inputs))\r\n        np.random.shuffle(indices)\r\n        for index in range(0,len(inputs)):\r\n            if self.in_memory_data:\r\n                yield inputs[index], targets[index]\r\n            else:\r\n                input_features = self.individual_load_feature(inputs[index])\r\n                yield input_features, targets[index]\r\n```\r\n\r\nand when i run my code it still errors with input ran out of data, i just put the returned iterator into model.fit T.T\r\n"]}, {"number": 29251, "title": "Update ag_logging.py with clarity and reference", "body": "* fixes typo\r\n* adds clarity to text\r\n* adds reference to absl's logging doc page\r\n\r\nref https://github.com/tensorflow/tensorflow/issues/29250", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 29250, "title": "[TF 2.0 API Docs] tf.autograph.set_verbosity", "body": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/set_verbosity\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nDescription could be clearer:\r\n\r\n* Reference to Abseil's logging format could be referenced rather than only to Abseil, itself (user would have to hunt through docs to see the logging output format referenced)\r\n* There's a slight misspelling in the args for `alsologtostdout`\r\n    \u201c it is recommended to set this value to a larges number, like 10\u201d should be \u201c it is recommended to set this value to a large number, like 10\u201d\r\n\r\n### Submit a pull request?\r\n\r\nYes.\r\n", "comments": ["Closing this issue since the associated PR has been merged. Feel free to reopen if the problem still persists. Thanks!"]}, {"number": 29249, "title": "[TF 2.0 API Docs] Docstring for tf.train.experimental.enable_mixed_precision_graph_rewrite", "body": "In response to #29241\r\n\r\nImproved the docstring for `tf.train.experimental.enable_mixed_precision_graph_rewrite`:\r\n\r\n* Added example for using function\r\n* Added colab notebook to demonstrate speed-up without performance penalty\r\n* Added original graphic for loss scaling ([source](https://docs.google.com/presentation/d/1ZCREpnt2H7I7J6Xek-Z0bZ_qE3bIHFzbTUed8Vdw6Ag/edit?usp=sharing))\r\n* Added more information about graph rewrite operation\r\n* Added performance guide\r\n* Added exception information\r\n* Added more clarification to `loss_scale` argument\r\n\r\nA gist with the rendered docstring is [here](https://gist.github.com/tlkh/fa20c5bf3c8b48def4501cccff8b3559) for ease of review.\r\n\r\nThank you, any feedback or criticism is welcome.\r\n", "comments": ["Can you change the colours in the image as well? On my monitor the green and blue look fairly similar its hard to distinguish.", "I have made all the requested changes!", "@rthadur Can you make sure the links are updated when this is merged?", "Fixed the pylint 80 char line limit errors.", "Pushed the amended commit with the requested changes.", "@tlkh, when enabling \"enable_mixed_precision_graph_rewrite\", how about the data type of model network when defining the model?\r\nstill use FP32? if it is, how to output the FP16 trained model? Thanks!", "> @tlkh, when enabling \"enable_mixed_precision_graph_rewrite\", how about the data type of model network when defining the model?\r\n> still use FP32? if it is, how to output the FP16 trained model? Thanks!\r\n\r\nYou define your model as per normal in FP32. Before training (computing of gradients), your graph will be \"rewritten\" to have some nodes in FP16, others in FP32. This is **only used for training**, and if you save or export your model, it will be in FP32 as per normal.\r\n\r\nThis means that training is faster, but otherwise no behavior differs. You will not be able to export the actual \"mixed precision model\", but you don't have to since it doesn't run faster on CPU, and you can also run the graph rewrite before the next training session.", "Thank you very much @tlkh. However, FP16 model can run faster compared to the FP32 one on GPU. you mentioned \"you can also run the graph rewrite before the next training session.\" With this method, FP16 model can be saved? How to do this?\r\n\r\nIs there no way to save the FP16 model with auto-mixed precision training? thanks a lot.", "> Thank you very much tlkh. However, FP16 model can run faster compared to the FP32 one on GPU. you mentioned \"you can also run the graph rewrite before the next training session.\" With this method, FP16 model can be saved? How to do this?\r\n> Is there no way to save the FP16 model with auto-mixed precision training? thanks a lot.\r\n\r\nThere is no way to save the FP16 model, but **there is no need to**!\r\n\r\nExport the model as per normal. The next time you want to use it, just run the graph rewrite again. If loading it for inference (no optimizer), you can manually set it in the TensorFlow session:\r\n\r\n```python\r\n# TF1.x\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.rewrite_options.auto_mixed_precision = True\r\nsess = tf.Session(config=config)\r\n\r\n# TF2.x\r\ntf.config.optimizer.set_experimental_options({\r\n    \"auto_mixed_precision\": True\r\n})\r\n```", "@perfinion @martinwicke can you please review new changes.", "Thanks a lot, @tlkh. However, if FP16 trained network cannot be saved, it is difficult to use the network outside tensorflow after finishing the training.", "In addition, it seems that ops conversion from FP32 to FP16 can happen on new GPU, like Volta. TF cannot do this conversion for old one.", "> Thanks a lot, tlkh. However, if FP16 trained network cannot be saved, it is difficult to use the network outside tensorflow after finishing the training.\r\n> In addition, it seems that ops conversion from FP32 to FP16 can happen on new GPU, like Volta. TF cannot do this conversion for old one.\r\n\r\nIf you wish to use the network outside of TensorFlow for inference in FP16, typically just converting all the datatypes to FP16 will work. Only for training does there have to be special considerations. There are other toolkits to help with inference optimization outside of TensorFlow, such as [TensorRT](https://developer.nvidia.com/tensorrt).\r\n\r\nOnly Volta and Turing GPUs have the FP16 units and Tensor Cores that can benefit from mixed precision. Older GPUs will not see a benefit, hence the feature is not enabled for them.\r\n\r\n@recrusader let's move this discussion to a new GitHub issue if you wish to raise any concerns or suggestions. Keep this thread for discussion specifically about this PR.", "I asked this question in official model. However, no one can give me an answer. Thank you very much! I think that I have got the answer. ", "@perfinion @martinwicke requesting review once again.\r\n\r\nChanges made:\r\n\r\n* image and example notebook is now hosted by NVIDIA\r\n* docstring wording has been improved, and updated/merged with recent changes from master\r\n* docstring added to both `enable_mixed_precision_graph_rewrite` and `enable_mixed_precision_graph_rewrite_v1`, with the correct distinction made with which kinds of `Optimizer` are accepted\r\n\r\nThank you!", "@martinwicke thanks for pointing that out. Let me fix that anyway.\r\nEDIT: fixed the indent!", "Sorry about that! I fixed it. ", "> Hi tlkh,\r\n> \r\n> It looks like you miss understood martinwicke's comment.\r\n> \r\n> All the Args/Returns/Raises blocks need to be indented, or they will not be recognized by our linters, and will render incorrectly on tensorflow.org (They will be interpreted as markdown which will just flatten them into a single paragraph).\r\n> \r\n> Thanks.\r\n\r\nMarkDaoust Hmm I've addressed the changes but I'm not sure why GitHub is still complaining about \"1 change requested\". Do you need to approve it as well?\r\n\r\nEdit: never mind "]}, {"number": 29248, "title": "Clarified docs of Lambda function on usecase", "body": "Felt a bit unclear about the usecase between `Lambda` and subclassing. Tried to clear that up.", "comments": ["Can one of the admins verify this patch?", "@pavithrasv I reverted the last sentence, but I'm not sure why the checks aren't running. Any input?"]}, {"number": 29247, "title": "[TF 2.0 API Docs] tf.keras.layers.LSTM", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM\r\n\r\n*Suggestion: Where applicable, the documentation for this should be consistent with the base class [tf.keras.layers.RNN](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/RNN) and other derived classes.*\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\n 1. Use of backticks can be made more consistent and in line with the [Documentation Style - Write about code](https://www.tensorflow.org/community/contribute/docs_style#write_about_code). For example, the value \"True\" and \"False\" in the description is not surronded by backticks, as recommended by the Documentation Style guide.\r\n\r\n### Correct links\r\n\r\n 1. Link to the source code at \"python/keras/layers/recurrent_v2.py\" is incorrect. It points to https://github.com/tensorflow/tensorflow/tree/r2.0/tensorflow/python/keras/layers/recurrent_v2.py, which is a 404 page. Correct link (for master) should be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent_v2.py.\r\n\r\n### Parameters defined\r\n\r\n 1. (As with the Clear Heading section above) Use of backticks can be made more consistent and in line with the [Documentation Style - Write about code](https://www.tensorflow.org/community/contribute/docs_style#write_about_code). For example, the value \"True\" and \"False\" in the description is not surronded by backticks, as recommended by the Documentation Style guide.\r\n\r\n 1. `__init__(...)`:\r\n    - Parameter `time_major` is not defined.\r\n    - The default value is specified within the text of some of the parameter definitions *but not all*. For example, the definition for `unroll` is:\r\n       >Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences.  \r\n\r\n       whereas the definition for `return_state` is:  \r\n\r\n       > Boolean. Whether to return the last state in addition to the output.\r\n\r\n1. `get_dropout_mask_for_cell(...)`:\r\n    - First word of the definition of the parameters should be capitalized\r\n\r\n1. `get_initial_state(...)`:\r\n    - Parameter `inputs` is not defined.\r\n\r\n1. `reset_states(...)`:\r\n    - Parameter `states` is not defined.\r\n\r\n### Returns defined\r\n\r\nReturn value is not defined for the following:\r\n - `get_initial_state(...)`\r\n - `reset_dropout_mask()`\r\n - `reset_recurrent_dropout_mask()`\r\n - `reset_state()`\r\n\r\nFor the last three items above, perhaps it is sufficiently clear that nothing will be returned.\r\n\r\n### Raises listed and defined\r\n\r\nNo errors are defined.\r\n\r\n### Usage example\r\n\r\nNo usage examples are provided. However, the description does have links to relevant guides and tutorials as follows:\r\n\r\n - Used in the guide:\r\n    - [The Keras Functional API in TensorFlow](https://www.tensorflow.org/alpha/guide/keras/functional)  \r\n\r\n - Used in the tutorials:\r\n    - [Load text with tf.data](https://www.tensorflow.org/alpha/tutorials/load_data/text)\r\n    - [Text classification with an RNN](https://www.tensorflow.org/alpha/tutorials/text/text_classification_rnn)\r\n    - [Text generation with an RNN](https://www.tensorflow.org/alpha/tutorials/text/text_generation)\r\n\r\n### Request visuals, if applicable\r\n\r\nThere are current *no* visuals. LSTM itself might be too broad a topic to be dealt with comprehensively using visuals in this documentation page.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue?  \r\n**No.** *(I can fix the formatting and syntax issues, but populating the missing parameter definitions is currently beyond my level \ud83d\ude05)*\r\n\r\n\r\n### Related Issue\r\n\r\n#26197", "comments": ["This is now fixed with TF 2.4.1 core api docs. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"]}, {"number": 29246, "title": "OOM happens with tf.py_function", "body": "```\r\ndef visualize_box(image, boxes, pred):\r\n    image_pil = Image.fromarray(np.uint8((1 - image) * 255.0), \"RGB\")\r\n    draw = ImageDraw.Draw(image_pil)\r\n    w, h = image_pil.size\r\n    boxes *= tf.cast(tf.stack([h, w, h, w], axis=-1), tf.float32)\r\n    font = ImageFont.load_default()\r\n    for b, p in zip(boxes, pred):\r\n        # b (y_min, x_min, y_max, x_max)\r\n        if not b[2] > 0.0:\r\n            continue\r\n        draw.line(\r\n            [(b[1], b[0]), (b[1], b[2]), (b[3], b[2]), (b[3], b[0]), (b[1], b[0])],\r\n            fill=\"blue\",\r\n        )\r\n        p_text = str(np.int(p))\r\n        t_w, t_h = font.getsize(p_text)\r\n        margin = np.ceil(0.05 * t_h)\r\n        if b[0] > t_h:\r\n            t_bottom = b[0]\r\n        else:\r\n            t_bottom = b[2] + t_h\r\n        draw.rectangle(\r\n            [b[1], t_bottom - t_h - 2 * margin, b[1] + t_w, t_bottom], fill=\"blue\"\r\n        )\r\n        draw.text(                                                                                                                                    \r\n            (b[1] + margin, t_bottom - t_h - margin), p_text, fill=\"white\", font=font\r\n        )\r\n\r\n    context.context()._clear_caches() # from tensorflow.python.eager import context\r\n    del font, draw, w, h                                                                                                                              \r\n    gc.collect()\r\n    return np.expand_dims(np.array(image_pil, dtype=np.float32) / 255.0, 0)\r\n```\r\n```\r\n# images is [1, 416, 416, 3] tensor, allbox [None, 4], all_pred [None]\r\nlabeled = tf.py_function(\r\n                visualize_box, (images[0], all_box, all_pred), tf.float32\r\n            )\r\n```\r\nI call the above function is the estimator model function in eval mode. The oom happens after training for some steps. If I comment out the `py_function` then I don't face any OOM.\r\n\r\nI use Tensorflow 1.13.1 gpu py3 docker image.\r\n\r\n```\r\nLimit:                 10224323788\r\nInUse:                 10028396544\r\nMaxInUse:              10101860352\r\nNumAllocs:                 5509254\r\nMaxAllocSize:           3638034432\r\n```\r\n```\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[13,1024,52,52] and type float on /job:localhost/\r\nreplica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node main/small/predictor/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation inf\r\no.\r\n\r\n         [[node Mean (defined at main.py:176) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation inf\r\no.\r\n```\r\n", "comments": ["interestingly I cannot see the memory growth here,\r\n```\r\nimport gc\r\nfrom tensorflow.python.eager import context\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom PIL import Image, ImageFont, ImageDraw\r\nfrom tqdm import tqdm\r\nfrom tensorflow.contrib.memory_stats.python.ops.memory_stats_ops import BytesInUse\r\ndef visualize_box(image, boxes, pred):\r\n    image_num = image.numpy()\r\n    del image\r\n    image_pil = Image.fromarray(np.uint8((1 - image_num) * 255.0), \"RGB\")\r\n    draw = ImageDraw.Draw(image_pil) \r\n    w, h = image_pil.size\r\n    boxes *= tf.cast(tf.stack([h, w, h, w], axis=-1), tf.float32)\r\n    boxes_num = boxes.numpy()\r\n    pred_num = pred.numpy()\r\n    del boxes, pred\r\n    font = ImageFont.load_default()\r\n    for b, p in zip(boxes_num, pred_num):\r\n        # b (y_min, x_min, y_max, x_max)\r\n        if not b[2] > 0.0:\r\n            continue\r\n        draw.line(\r\n            [(b[1], b[0]), (b[1], b[2]), (b[3], b[2]), (b[3], b[0]), (b[1], b[0])],\r\n            fill=\"blue\",\r\n        )\r\n        p_text = str(p)\r\n        t_w, t_h = font.getsize(p_text)\r\n        margin = np.ceil(0.05 * t_h)\r\n        if b[0] > t_h:\r\n            t_bottom = b[0]\r\n        else:\r\n            t_bottom = b[2] + t_h\r\n        draw.rectangle(\r\n            [b[1], t_bottom - t_h - 2 * margin, b[1] + t_w, t_bottom], fill=\"blue\"\r\n        )   \r\n        draw.text(\r\n            (b[1] + margin, t_bottom - t_h - margin), p_text, fill=\"white\", font=font\r\n        )   \r\n        del p_text, t_w, t_h, margin, t_bottom\r\n    image = np.expand_dims(np.array(image_pil, dtype=np.float32) / 255.0, 0)\r\n    context.context()._clear_caches()\r\n    del font, draw, w, h, image_pil\r\n    gc.collect()                                                                                                                                      \r\n    return image\r\nimage = tf.random.uniform(minval=0, maxval=1, shape=(416,416,3))\r\nbox = tf.random.uniform(minval=0, maxval=1, shape=(30 ,4))\r\npred = tf.random.uniform(minval=0, maxval=10, shape=(30,))\r\na = tf.py_function(visualize_box, (image, box, pred), tf.float32)\r\nwith tf.device('/device:GPU:0'):  # Replace with device you are interested in\r\n  bytes_in_use = BytesInUse()\r\ntotal = 654654654\r\nwith tf.Session() as sess:\r\n  with tqdm(total=total) as pbar:\r\n    for i in tqdm(range(total)):\r\n      im, b = sess.run((a, bytes_in_use))\r\n      pbar.set_description(str(b))\r\n```\r\nI am running the same function. but not in estimator.\r\n[image](https://imgur.com/a/8XwaBgz)\r\nthe above is the bytes_in_use graph inside tf estimator under mode eval.", "@dchatterjee172  : Is this still an issue ?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29246\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29246\">No</a>\n"]}, {"number": 29245, "title": "[TF 2.0 API Docs] tf.image.transpose", "body": "Doc link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/transpose\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Usage example\r\n\r\nNo usage example is provided.\r\n", "comments": ["@Aqsa-K This was already fixed [here](https://www.tensorflow.org/api_docs/python/tf/image/transpose) in the recent TF version. Thanks!\r\n\r\nI am closing this issue as this was resolved. Thanks!"]}, {"number": 29244, "title": "[TF 2.0 API Docs] tf.io.extract_jpeg_shape", "body": "\r\nDoc link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/extract_jpeg_shape\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Correct links\r\n\r\nLink not provided. Path is written but href is not provided.\r\n\r\n### Raises listed and defined\r\n\r\nErrors are not defined.\r\n\r\n### Usage example\r\n\r\nNo usage example is provided.\r\n", "comments": ["@Aqsa-K,\r\nSorry for the delayed response. Your statement that is mentioned below, is not clear:\r\n\r\n> Link not provided. Path is written but href is not provided.\r\n\r\nCan you please elaborate it?  Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29243, "title": "[TF 2.0 API Docs] tf.image.rot90", "body": "Doc link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/rot90\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Usage example\r\n\r\nNo usage example is provided.\r\n\r\n### Request visuals, if applicable\r\n\r\nNo Visuals are included. A visual example of rotation can be added although not necessary.\r\n", "comments": ["@Aqsa-K Please point the correct link. Thanks!", "@gadagashwini the link is correct. I added the link for reference only. The heading was misleading and have removed it from the issue description. Apologies.", "@Aqsa-K Looks like Doc link provided above is pointing to error 404.Can you please check and Let me know if that is same with you. Thanks!", "I have copied the link again from the redirection in excel sheet. Please try now.  ", "Automatically closing this out since I understand it to be resolved by the PR #29610 (merged already), but please let me know if I'm mistaken.Thanks!"]}, {"number": 29242, "title": "[TF 2.0 API Docs] tf.math.maximum / tf.maximum", "body": "**System information**\r\n\r\nTensorFlow version: 2.0\r\nDoc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/maximum\r\n\r\n**Describe the documentation issue**\r\n\r\n**Links**\r\npython/ops/gen_math_ops.py\r\nThe implementation of the code is in c++.\r\nHowever, the documentation references a generated python file.\r\nwhich we can't open, or can't view a representative implementation.\r\n\r\nPerhaps, we can add a representative implementation for such function.\r\n\r\n**Usage Example**\r\nNo usage example is provided.\r\n\r\n", "comments": ["TF 2.4 docs are updated with the usage example. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/maximum"]}, {"number": 29241, "title": "[TF 1.14 API Docs] tf.train.experimental.enable_mixed_precision_graph_rewrite", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r1.14/api_docs/python/tf/train/experimental/enable_mixed_precision_graph_rewrite\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\n* Should better explain the graph rewrite algorithm (black/white/grey listed ops) and where to see the list of ops.\r\n* Overall language can be clearer and more precise.\r\n* Minor issues with the formatting.\r\n\r\n### Correct links\r\n\r\nLinks present are all correct.\r\n\r\n### Parameters defined\r\n\r\nBriefly explain what the value `\"dynamic\"` (the default value) for `loss_scale` does, and link to the symbol for that.\r\n\r\n### Returns defined\r\n\r\nReturn values are defined properly.\r\n\r\n### Raises listed and defined\r\n\r\nExceptions are not listed nor explained.\r\n\r\n### Usage example\r\n\r\nNo usage example, I can provide a simple code snippet.\r\n\r\nAdditionally, I want to provide a Colab notebook to demonstrate increase in speed without negative impact on accuracy (on CIFAR10 for example).\r\n\r\n### Request visuals, if applicable\r\n\r\n* Overview of mixed precision process/flow\r\n\r\n### Submit a pull request?\r\n\r\nYes, I have submitted a PR. PR is here #29249 \r\n", "comments": ["Closing this issue since the associated PR has been merged. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29241\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29241\">No</a>\n"]}, {"number": 29240, "title": "[XLA]support of async xlaop kernel  ", "body": "**System information**\r\n- TensorFlow version (you are using):\r\nr1.14\r\n- Are you willing to contribute it (Yes/No):\r\nYES\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nin the current tf version, there are two types of ops: OpKernel and AsyncOpKernel, but in the xla module, the only xlaop type is XlaOpKernel, which is a sync interface. For most of xlaops, this method works fine, but when it comes to some async op like horovod all reduce op, which derived from AsyncOpKernel in the tfop, this sync interface  will not work. I gonna to learn if it is possible to add a async xla op kernel type? if these idea is contradicted to the original design of XLA module, thanks for letting me know.    \r\n**Will this change the current api? How?**\r\nno\r\n**Who will benefit with this feature?**\r\nanyone who want to add a async op in xla\r\n**Any Other info.**\r\n", "comments": ["cc @sanjoy.  (I also want to cc Tres, but I can't find his github handle...)\r\n\r\nI don't quite understand what you're trying to accomplish by way of this feature request.\r\n\r\nAre you hoping to implement Horovod all-reduce *inside of XLA*?  (Note that XLA recently grew NCCL all-reduce support, which I understand is not the same thing.)", "Apart from what Justin said, `XlaOpKernel` is a \"symbolic execution\" kernel whose only job is to generate HLO instructions for TF nodes.  So it does not make sense for the `XlaOpKernel` class itself to allow async execution since it is only ever called by the TF->XLA compiler.  Now we may have to make a change to XLA itself to represent Horovod all-reduce and associated asynchronicity depending on the details, but that does not have anything to do with `XlaOpKernel`.", "Thanks for your update, i'm implementing this and looking forward to a good result. \r\nAnd  with your explanation, i think this issue should be closed. "]}, {"number": 29239, "title": "Edited Documentation to tf.dtypes.complex", "body": "defined Raises in Docs", "comments": []}, {"number": 29238, "title": "Edited Documentation to tf.dtypes.complex", "body": "defined Raises in Docs", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29238) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29238) for more info**.\n\n<!-- need_author_consent -->", "Please reopen a PR against master. "]}]