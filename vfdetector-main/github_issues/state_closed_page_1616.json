[{"number": 4431, "title": "Forward mode ad, directional derivatives", "body": "Say I have `outputs = f(inputs)` and `g` of the same shape as `inputs`. I'd like to compute the directional derivative of `outputs` with respect to `inputs` in the direction `g` - in other words, the derivative of `f(inputs + alpha * g)` with respect to `alpha` at the point `alpha=0`.\n\nThis is a straightforward application of forward-mode automatic differentiation, which should be pretty easy to implement (much easier than the already-implemented reverse-mode ad). Are there any plans to add this feature?\n", "comments": ["@andydavis1 or @rmlarsen might know whether there are such plans.\n", "I don't think we have any near term plans for this.\n", "As a breadcrumb for future people trying to solve this problem, \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/nn/python/ops/fwd_gradients.py\r\nperforms forward mode differentiation.", "Has anyone figured this out? Id like to compute the nth order directional derivatives in the direction of the gradient. I'd like to make an optimizer using this."]}, {"number": 4430, "title": "ERROR: failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command , CUDA 8.0, cudnn 5.1.5, Cuda compute 6.1", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttps://github.com/tensorflow/tensorflow/issues/4365\n### Environment info\n\nOperating System:\nUbuntu 16.04 [inside a docker container]\nhttps://hub.docker.com/r/nvidia/cuda/tags/\nlatest-ubuntu16.04\n\nGTX 1080\nCUDA 8.0\ncudnn 5.1.5\nCuda compute 6.1\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n/usr/local/cuda/lib64/libcudadevrt.a    /usr/local/cuda/lib64/libcudart.so.8.0.27  /usr/local/cuda/lib64/libcudnn.so.5\n/usr/local/cuda/lib64/libcudart.so      /usr/local/cuda/lib64/libcudart_static.a   /usr/local/cuda/lib64/libcudnn.so.5.1.5\n/usr/local/cuda/lib64/libcudart.so.8.0  /usr/local/cuda/lib64/libcudnn.so          /usr/local/cuda/lib64/libcudnn_static.a\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n   931ff427f3a55a9c6c734a4c325d6af1b53665c3\n2. The output of `bazel version`\n\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n\nNOTE: i've also tried with bazel 0.3.1 - same result\n\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures\n### What other attempted solutions have you tried?\n\ni've alsot tried LATEST clone and got configuration errors:\nERROR: /home/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\n\nsuch as in \nhttps://github.com/tensorflow/tensorflow/issues/4312\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nWARNING: /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/protobuf/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/highwayhash/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/re2/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nWARNING: /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/boringssl_git/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.\nINFO: Found 1 target...\nERROR: /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/protobuf/BUILD:331:1: Linking of rule '@protobuf//:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/execroot/tensorflow && \\\n  exec env - \\\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/external/protobuf/protoc bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o bazel-out/host/bin/external/protobuf/libprotoc_lib.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\n/usr/bin/env: 'python': No such file or directory\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 0.950s, Critical Path: 0.21s\n", "comments": ["ok, it was easier than expected! i just had to copy 'python' from miniconda directory to /usr/bin/env and i've ignored the configure errors [related to android / ios]\n"]}, {"number": 4429, "title": "A very simple script using Optimizer.compute_gradients() prodeces Nan for all gradients", "body": "Hi all, \n\n```\nimport tensorflow as tf\nimport numpy as np\n\nbatch_size = 5\ndim = 3\nhidden_units = 8\n\n\nsess = tf.Session()\n\nwith sess.as_default():\n    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n    w = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units]), name=\"w\")\n    b = tf.Variable(initial_value=tf.zeros(shape=[hidden_units]), name=\"b\")\n    logits = tf.nn.tanh(tf.matmul(x, w) + b)\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y,name=\"xentropy\")\n    # define model end\n\n\n    # begin training\n    optimizer = tf.train.GradientDescentOptimizer(1e-5)\n    grads_and_vars = optimizer.compute_gradients(cross_entropy, tf.trainable_variables())\n\n    # generate data\n    data = np.random.randn(batch_size, dim)\n    labels = np.random.randint(0, 10, size=batch_size)\n\n    sess.run(tf.initialize_all_variables())\n    gradients_and_vars = sess.run(grads_and_vars, feed_dict={x:data, y:labels})\n    for g, v in gradients_and_vars:\n        if g is not None:\n            print \"****************this is variable*************\"\n            print \"variable's shape:\", v.shape\n            print v\n            print \"****************this is gradient*************\"\n            print \"gradient's shape:\", g.shape\n            print g\n\nsess.close()\n```\n\nRun it, I got the following result.\n\n![selection_107](https://cloud.githubusercontent.com/assets/5330101/18612210/706d7be2-7d86-11e6-9796-ab98b73b74ae.png)\n\nAs you see all GRADIENTS are Nan. Is there anything wrong in my code or PC? I don't know why it cannot compute correct gradients.\n", "comments": ["I think the problem is that you are generating invalid labels - the network can only predict values [0, 8), but you generate labels in [0, 10).\n", "@vladfi1 Thanks. You're right. I made a silly mistake.\n"]}, {"number": 4428, "title": "Slim Tutorial and Script broken due to Commit 8d1b23d999a563c56236a3d250e18ef825e4e66e", "body": "Hey TF Team,\n\nCommit 8d1b23d999a563c56236a3d250e18ef825e4e66e broke the TF Slim Tutorial and TF Slim Script:\nhttps://github.com/tensorflow/models/blob/master/slim/scripts/finetune_inception_v3_on_flowers.sh\nReverting this file to 4d241449aeb3707143c64e1be106cc6828a0d7b0 solves the issue https://github.com/tensorflow/tensorflow/commit/4d241449aeb3707143c64e1be106cc6828a0d7b0#diff-197163b98ebda1e22c3ff820582bf0d5\n\nIn the latest TF version as well as the TF rc10 the TF Slim Image Classification is not running.\nTested on Offical Tensorflow Docker GPU Image.\n\nBest Wishes,\nPatrick\n", "comments": ["@annarev, since this is your change, could you take a look\n", "Closing due to lack of recent activity, please reopen if you have further. Please let us know if your issue is not resolved in the latest version. Thanks!", "Sorry, I somehow missed this issue earlier. Patrick, let me know if you still see the issue."]}, {"number": 4427, "title": "Clarify math or paper reference behind AttentionCellWrapper", "body": "The documentation for `AttentionCellWrapper` in contrib states that it's based on this [paper](https://arxiv.org/abs/1601.06733). However, the attention mechanism in the paper appears to be distinct from that in `AttentionCellWrapper`. For example the TF implementation involves convolutions and requires a fixed attention window, neither of which are a feature of the referenced paper. I suggest that either the correct paper is referenced, or the description is updated to reflect the actual math being implemented. The biggest issue right now is that it's unclear what the various options really mean (`attn_length`, `attn_size`, `attn_vec_size`, etc) as their descriptions are very concise and without a paper reference to ground what the documentation is referring to, it's impossible to know what the attention mechanism is doing short of going through the code line-by-line.\n", "comments": ["Thanks for the report. Notified the author.\n", "Hi @alquraishi, thanks for the report, indeed, this [paper](https://arxiv.org/pdf/1409.0473v7.pdf) is a better fit to describe the model we've implemented.\nWe'll fix the docs as soon as possible.\n", "@Monnoroch tbh I don't think that paper is a good description of the model either. I eventually went through the math and as far as I can tell it's not something that's from any paper I've come across before. At any rate, I ended up summarizing it (see below). If you think it would be useful to include the math directly feel free to use any part of this.\n\n![image](https://cloud.githubusercontent.com/assets/5205204/18845519/002325a6-83f0-11e6-859e-db589cda985d.png)\n", "@alquraishi, here's the original Magenta post, that describes the cell at hand: https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn with all the math an examples.\n", "@Monnoroch to the best of my understanding of the code, the math described in the Magenta post does _not_ match the math implemented in `AttentionCellWrapper`, though it does match the math in the paper you cited. Note that the original version of the Magenta code used its own version of attention, but was later changed to use `AttentionCellWrapper`. The math I wrote above should match the code of `AttentionCellWrapper`, unless I made a mistake somewhere.\n", "@alquraishi, that is very strange, as we have extensively tested that the original code and  `AttentionCellWrapper` around a usual LSTM cell behave exactly the same way.\nBut thanks, I will look into it again.\n", "The original code and `AttentionCellWrapper` may well be identical. I was just saying that the prose description in the Magenta post is not identical to the code. The differences are not large, but there are some subtle discrepancies.\n", "I haven't checked all the math but this implementation slices off the first encoded hidden state and appends on the most recent decoded hidden state which is not what's done in Bahdanau. Attention is typically used for translation tasks, this looks like a good technique for music generation but I think the class name and the paper reference are misleading.", "[This post](https://theneuralperspective.com/2016/11/20/recurrent-neural-network-rnn-part-4-attentional-interfaces/) gives a comprehensive and easy-to-follow overview of the current TF attention implementation in an encoder-decoder context. According to the author the implementation is based on the details at the end of the Bahdanau paper (https://arxiv.org/pdf/1409.0473v7). The author provides a nice walk-through of the details of attention and then goes into depth on the implementation. The discussion includes some detail on the use and motivation of conv2d in the code, which isn't mentioned in any papers that I've seen. Some of the variable names are slightly different but overall it really helped me understand what's going on.", "@jessedaniels The author in that post gives an incorrect explanation of the conv2d operation. The conv2d is just a clever trick to calculate W*h_t over all time steps. You can see the original code where he got that from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L589). You can also verify it yourself by running:\r\n```python\r\n[tf.matmul(tf.squeeze(ai, [1]), tf.squeeze(k, [0,1])) for ai in tf.split(1, attn_length, attention_states)]\r\n```\r\non some random matrices and seeing that you get the same values.\r\n\r\n@Monnoroch The issue with AttentionCellWrapper is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1129) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1101). This is slicing off the oldest encoded hidden state and appending the newest decoding hidden state and then passing those to the next time step. This means that for translation tasks, after you decode the first word you would never be able to run attention over the first encoded word again. Which is not what's done in Bahdanau. ", "@Monnoroch Another problem with AttentionCellWrapper is that the conv2d operation, which in the cited paper can be precomputed because the hidden states (input context) remain unchanged throughout the recurrent steps, is now repeated each time, where (1-1/attn_length) of the computation is not necessary.", "Closing due to lack of recent activity, please reopen if you have further. Thank you. ", "Hello, apologies if this might be an obvious question, but I have a basic question regarding applying the AttentionCellWrapper in practice -- in particular it seems that we never specify the memory (e.g. the set of encoder hidden states) during the creation of the wrapper. \r\n\r\nHow then does the module know where to attend? Does the memory get passed somehow through the initial state of the decoder? A basic example of how to use the wrapper in practice would definitely be much appreciated. Thanks!", "The paper use source sequence hidden state as attention values, but the code use dest hidden state pre?", "It is very strange to have a fixed-sized window in the implementation of AttentionCellWrapper, because there is no description about it in the Bahdanau paper. Of course, the behave of the window is not clear, which makes me confused. Can anyone explain it?", "it real confuse me a lot. but i found this \r\nhttps://github.com/tensorflow/tensorflow/commit/ec3f4d62979ef1e70e8e12e2568b13dad45fd39e\r\nit shows that the ref has changed from 06733 to 0437.\r\nbut in my opinion, read the previous one(06733) helps more than 0437.\r\nit tells why u should add the windows, and it address on a unit of a seq, and 0437 more like on a seq2seq.\r\nof course, the realization of the attentincellwrapper is a little different from the 06733 mentioned.\r\n@alquraishi  gives a excellent math,but some place i am not very clear:\r\nis \r\n![image](https://user-images.githubusercontent.com/2640812/34261076-fcb18b12-e6a2-11e7-80ee-246c0fa01561.png)\r\nshould be\r\n![image](https://user-images.githubusercontent.com/2640812/34261114-1b4f577a-e6a3-11e7-980e-af578267347a.png)\r\nand in the code, it only has \"state\", so the \"state\" means \"h_t and c_t\"?\r\n\r\n"]}, {"number": 4426, "title": "Fix stddev computation in TensorBoard examples. (#4054)", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@shohad25, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @petewarden and @danmane to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I believe this has been fixed at head.\n", "(and in r0.10)\n", "Can one of the admins verify this patch?\n", "(If it was in r0.10, wouldn't this PR diff to nothing?)\n", "Who knows what that diff is doing, but it's not a diff against r0.10: \n[retrain.py](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/examples/image_retraining/retrain.py) has no mention of `_sum(` in it.\n", "*shrug.  I guess we just have to re-push the website.\n", "I did that on the weekend. Should be fixed.\n"]}, {"number": 4425, "title": "tf.nn.softmax makes \"GPU sync failed Error\" for large size inputs.", "body": "### Environment info\n\nOperating System:\nDistributor ID: Ubuntu\nDescription:    Ubuntu 14.04.3 LTS\nRelease:        14.04\n### GPU info\n\nGPU :  GTX TITAN X \nDriver :  Driver Version: 367.44 \n\nInstalled version of CUDA and cuDNN: \n\nCUDA 7.5\nCUDNN 5.1.3\n\nIf installed from binary pip package, provide:\n\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/224/artifact/pip_test/whl/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nThis code makes errors.\n\n```\nimport tensorflow as tf\na=tf.constant(1.0,shape=[100*30, 30000]) \nb=tf.nn.softmax(a)\nsess=tf.InteractiveSession()\nb.eval()\nsess.close()\n```\n\nThis code doesn't make errors.\n\n```\nimport tensorflow as tf\na=tf.constant(1.0,shape=[20*30, 30000]) \nb=tf.nn.softmax(a)\nsess=tf.InteractiveSession()\nb.eval()\nsess.close()\n```\n\nI think this caused by input size of the softmax function.\n\nBut, I don't understand why.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\n```\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\n[1]    57277 abort (core dumped)  CUDA_VISIBLE_DEVICES=0 ipython\n```\n", "comments": ["I've also encountered this problem using v0.10.0 in the following issue: https://github.com/ibab/tensorflow-wavenet/issues/13\n\nHere is another example of crashing code:\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\nx = np.ones((256, 256), dtype=np.float32)\ns = tf.Session()\nX = tf.placeholder(tf.float32)\ny = tf.nn.softmax(X)\n\ns.run(y, feed_dict={X: x})\n```\n\nThe size of 256 seems to be significant, as it starts failing when the input dimensions go above that.\n\nThis might be a problem with the efficient reduction code in Eigen, as `cuda-memcheck` points to that part of the code, and the problem doesn't happen when using `tf.float64`.\n", "Can confirm this problem as well running Ubuntu 14.04 and GTX Titan X with CUDA 7.5 and cuDNN 4.0.7. Although, for me it fails with anything larger than 128 x 128, e.g. 129 x 129 makes it fail.\n", "I've encountered this too - in the mean time a work around is to temporarily cast the tensor to float64 like @ibab suggested\n", "For some reason I'm getting an error when I try to cast it to float64, specifically this:\n\n```\nTensor conversion requested dtype float64 for Tensor with dtype float32\n```\n\nAre there any known issues with upcasting to float64?\n", "This is reproduced on K80 and GTX Titan.\n\nAnother workaround is to implement softmax with primitives:\n\nhttps://gist.github.com/raingo/a5808fe356b8da031837\n", "Reproduced on new Titan X (Pascal) and GTX 1080. Casting suggested by @alexgkendall works:\n\n```\nimport numpy as np\nimport tensorflow as tf\n\nx = np.ones((256, 256), dtype=np.float32)\ns = tf.Session()\nX = tf.placeholder(tf.float32)\n#y = tf.nn.softmax(X)                                                 # fails\ny = tf.cast(tf.nn.softmax(tf.cast(X, tf.float64)), tf.float32)        # works\n\nout = s.run(y, feed_dict={X: x})\n```\n", "I believe this is fixed. I'm pushing a regression test that covers the input dimensions that were previously reported as being troublesome (ie 256x256 and 129x129). I'll close this issue in a few days once the test is available unless I hear that this is still causing problems for people.\n", "Regression test submitted in https://github.com/tensorflow/tensorflow/commit/9bff346137aa1c627f2dccb77fdda10863456b45\n", "I had the same problem when i was doing some experimental stuffs on tensorflow in jupyter notebook. The problem seemed to resolve itself when I closed other kernels that were using tensorflow.\r\nI hope this helps"]}, {"number": 4424, "title": "`tf.count_up_to` returns the old value", "body": "Environment info: Ubuntu 16.04, IPython interactive session\n\nInstalled version of CUDA and cuDNN: 7.5.18, 5.1.3\n\nInstalled from the nightly pip package for python 3.4.\n\nHow to reproduce:\n\n``` python\nimport tensorflow as tf\nwith tf.Session().as_default():\n  var = tf.Variable(0)\n  count = var.count_up_to(3)\n  tf.initialize_all_variables().run()\n  print(var.eval())\n  print(count.eval())\n  print(var.eval())\n```\n\nExpected output:\n\n```\n0\n1\n1\n```\n\nWhat I see instead:\n\n```\n0\n0\n1\n```\n\nFor some reason, the tensor from `tf.count_up_to` evaluates to the old value instead of the updated value.\n", "comments": ["I noticed that the documentation says (current behavior):\n\n> Returns: A Tensor. Has the same type as ref. A copy of the input before increment. If nothing else modifies the input, the values produced will all be distinct.\n\nBut it also says (similar to `tf.assign_add`)\n\n> This operation outputs \"ref\" after the update is done. This makes it easier to chain operations that need to use the updated value.\n", "Thanks for reporting. @vrv Could you take a look at this? Thanks.\n", "The documentation is wrong, I'll send a fix.\n"]}, {"number": 4423, "title": "set_shape cause gradient error in a condition case", "body": "### Environment info\n\nOperating System: Ubuntu 16.04\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed: [https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl)\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 0.10.0rc0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n``` python\nimport tensorflow as tf\na = tf.ones((2, 1, 2))\na_ = tf.cond(tf.equal(tf.shape(a)[1], 1), lambda: tf.tile(a, (1, 5, 1)), lambda: a)\na_.set_shape([None, 5, None])\ntf.gradients(a_, a)\n# => ValueError: Shapes (2, 5, 2) and (2, 1, 2) are not compatible\n```\n\nIf the set_shape line is removed, everything works fine\n\n``` python\ntf.gradients(a_, a)\n# => [<tf.Tensor 'gradients_5/AddN:0' shape=(2, 1, 2) dtype=float32>]\n```\n", "comments": ["Hi,\nAny reply for this problem?\n", "Hmm, I don't believe this is supported -- essentially what you're doing is to \"hint\" at TensorFlow to do a backward shape inference (which does not exist yet), and try to force the runtime to settle on the inferred branch.  \n\nI might be wrong - so assigning to @yuanbyu to take a look.\n", "@concretevitamin thanks for reply. I think i'm not wanting tensorflow to do some backward shape inference or things like that. What I'm confused about is that how setting static shape can have an influence on gradients computing. Because without that line, the code works pretty well. What I only hope is that line can help me know more about the static shape of `a_`\n", "You can see from the stack trace that tf.gradients uses static shape\ninformation\n\n> > > tf.gradients(a_, a)\n> > > tf.gradients(a_, a)\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File\n> > > \"/Users/yaroslav/anaconda2/envs/tensorflow_cpu/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\",\n> > > line 498, in gradients\n> > >     in_grad.set_shape(t_in.get_shape())\n\nOn Tue, Sep 27, 2016 at 6:36 PM, Shi Jiaxin notifications@github.com\nwrote:\n\n> @concretevitamin https://github.com/concretevitamin thanks for reply. I\n> think i'm not wanting tensorflow to do some backward shape inference or\n> things like that. What I'm confused about is that how setting static shape\n> can have an influence on gradients computing. Because without that line,\n> the code works pretty well. What I only hope is that line can help me know\n> more about the static shape of a_\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4423#issuecomment-250048969,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHA_daTUBxOQELHK8LbNQx42MLcCOks5qucSvgaJpZM4J_ihh\n> .\n", "Your set_shape() line implicitly adds something (i.e., tf.equal(tf.shape(a)[1], 1) is True) that the current TensorFlow is not able to infer. So tf.gradients tries to generate the gradient code for the false branch of the code because it can't statically determine that it is infeasible. Obviously this would cause the shape inference to fail. In general, don't use set_shape to make shapes more concrete when you need to compute gradients.", "@yuanbyu get it. thx"]}, {"number": 4422, "title": "Error while install tensorflow with pip install and python 3.5.2", "body": "I was trying to in tensorflow from pip following the standard instructions and I get this error every time:\n\n```\n$ pip install --ignore-installed --upgrade $TF_BINARY_URL\nCollecting tensorflow==0.10.0 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl\n  Using cached https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl\nCollecting six>=1.10.0 (from tensorflow==0.10.0)\n  Using cached six-1.10.0-py2.py3-none-any.whl\nCollecting wheel>=0.26 (from tensorflow==0.10.0)\nException:\nTraceback (most recent call last):\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/commands/install.py\", line 310, in run\n    wb.build(autobuilding=True)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/wheel.py\", line 750, in build\n    self.requirement_set.prepare_files(self.finder)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/req/req_set.py\", line 370, in prepare_files\n    ignore_dependencies=self.ignore_dependencies))\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/req/req_set.py\", line 522, in _prepare_file\n    finder, self.upgrade, require_hashes)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/req/req_install.py\", line 268, in populate_link\n    self.link = finder.find_requirement(self, upgrade)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/index.py\", line 442, in find_requirement\n    all_candidates = self.find_all_candidates(req.name)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/index.py\", line 400, in find_all_candidates\n    for page in self._get_pages(url_locations, project_name):\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/index.py\", line 545, in _get_pages\n    page = self._get_page(location)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/index.py\", line 648, in _get_page\n    return HTMLPage.get_page(link, session=self.session)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/index.py\", line 757, in get_page\n    \"Cache-Control\": \"max-age=600\",\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/requests/sessions.py\", line 487, in get\n    return self.request('GET', url, **kwargs)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/download.py\", line 378, in request\n    return super(PipSession, self).request(method, url, *args, **kwargs)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/requests/sessions.py\", line 475, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/requests/sessions.py\", line 585, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/cachecontrol/adapter.py\", line 36, in send\n    cached_response = self.controller.cached_request(request)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/cachecontrol/controller.py\", line 111, in cached_request\n    resp = self.serializer.loads(request, cache_data)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/cachecontrol/serialize.py\", line 114, in loads\n    return getattr(self, \"_loads_v{0}\".format(ver))(request, data)\n  File \"/home/brando90/envs/tf_gpu/lib/python3.5/site-packages/pip/_vendor/cachecontrol/serialize.py\", line 170, in _loads_v2\n    cached = json.loads(zlib.decompress(data).decode(\"utf8\"))\nzlib.error: Error -5 while decompressing data: incomplete or truncated stream\n\n```\n\nGoogling didn't help me so I'm here. Some more info:\n\nOperating System:\n\n```\nLinux openmind7 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nInstalled version of CUDA and cuDNN: \n    cuda 7.5 \n    cuDNN 5\n\nI know its good to give reproducible examples but I'm not doing anything special. I have a virgin conda environment with 3 packes:\n\n```\npip (8.1.2)\nsetuptools (27.2.0)\nwheel (0.29.0)\n```\n\nSome info about my python:\n\n```\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:53:06)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\n```\n", "comments": ["Use nightly build of tensorflow for python 3.5\nVisit https://github.com/tensorflow/tensorflow   scroll down\nClick on build history for python 3.5\nreplace storage..... link with this one\nhope it helps\n", "Closing automatically due to lack of recent activity. Please reopen when further information becomes available. Thank you.\n"]}, {"number": 4421, "title": "Update example link in the docs preprocessing section", "body": "The preprocessing logic has moved to `cifar10_input.py`. This tiny PR updates the docs link, fixes #4358.\n", "comments": ["@danijar, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman and @vrv to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Skipping tests for md only change.\n"]}, {"number": 4420, "title": "Out of Memory error running Inception v3 distributed on 4 machines. ", "body": "I'm trying to run Inception v3 (https://github.com/tensorflow/models/tree/master/inception) distributed on upto 32 machines. \n\nI'm seeing out of memory error when I run it on **4 machines**. \n\nHere is the error:\n\n```\nINFO:tensorflow:Started 0 queues for processing input data.\nE tensorflow/core/client/tensor_c_api.cc:485] OOM when allocating tensor with shape[2048,1001]\n     [[Node: gradients/logits/logits/weights/Regularizer/L2Regularizer/L2Loss_grad/mul = Mul[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/gpu:2\"](logits/logits/weights/read_S3003, gradients/logits/logits/weights/Regularizer/L2Regularizer/value_grad/tuple/control_dependency_1)]]\n     [[Node: gradients/AddN_48_S3319 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:3/cpu:0\", send_device=\"/job:worker/replica:0/task:0/gpu:2\", send_device_incarnation=-546941133885931708, tensor_name=\"edge_17701_gradients/AddN_48\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:3/cpu:0\"]()]]\nTraceback (most recent call last):\n  File \"imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/ubuntu/indu/models/inception/inception/inception_distributed_train.py\", line 286, in train\n    loss_value, step = sess.run([train_op, global_step])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[2048,1001]\n     [[Node: gradients/logits/logits/weights/Regularizer/L2Regularizer/L2Loss_grad/mul = Mul[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/gpu:2\"](logits/logits/weights/read_S3003, gradients/logits/logits/weights/Regularizer/L2Regularizer/value_grad/tuple/control_dependency_1)]]\n     [[Node: gradients/AddN_48_S3319 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:3/cpu:0\", send_device=\"/job:worker/replica:0/task:0/gpu:2\", send_device_incarnation=-546941133885931708, tensor_name=\"edge_17701_gradients/AddN_48\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:3/cpu:0\"]()]]\nCaused by op u'gradients/logits/logits/weights/Regularizer/L2Regularizer/L2Loss_grad/mul', defined at:\n  File \"imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/ubuntu/indu/models/inception/inception/inception_distributed_train.py\", line 215, in train\n    grads = opt.compute_gradients(total_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 229, in compute_gradients\n    return self._opt.compute_gradients(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py\", line 402, in _L2LossGrad\n    return op.inputs[0] * grad\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 754, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 903, in _mul_dispatch\n    return gen_math_ops.mul(x, y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1427, in mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'logits/logits/weights/Regularizer/L2Regularizer/L2Loss', defined at:\n  File \"imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n[elided 1 identical lines from previous traceback]\n  File \"imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/ubuntu/indu/models/inception/inception/inception_distributed_train.py\", line 154, in train\n    logits = inception.inference(images, num_classes, for_training=True)\n  File \"/home/ubuntu/indu/models/inception/inception/inception_model.py\", line 87, in inference\n    scope=scope)\n  File \"/home/ubuntu/indu/models/inception/inception/slim/inception_model.py\", line 326, in inception_v3\n    restore=restore_logits)\n  File \"/home/ubuntu/indu/models/inception/inception/slim/scopes.py\", line 155, in func_with_args\n    return func(*args, **current_args)\n  File \"/home/ubuntu/indu/models/inception/inception/slim/ops.py\", line 300, in fc\n    restore=restore)\n  File \"/home/ubuntu/indu/models/inception/inception/slim/scopes.py\", line 155, in func_with_args\n    return func(*args, **current_args)\n  File \"/home/ubuntu/indu/models/inception/inception/slim/variables.py\", line 290, in variable\n    trainable=trainable, collections=collections)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 830, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 673, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 217, in get_variable\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 202, in _true_getter\n    caching_device=caching_device, validate_shape=validate_shape)\n\n```\n\nI'm using EC2 G2.8XL instances. These instances have:\n1. Intel Xeon E5-2670 (Sandy Bridge) Processors\n2. 60 GB memory and\n3. Four GK104GL [GRID K520] GPU with 4 GB memory on each of them. \n4. 10 Gigabit NIC\n\nI'm running Ubuntu 14.04.4 LTS on these machines.\n\nI'm running one worker per GPU. So, in total there is **16 workers**. \nI'm running one PS per machine. So, **4 PS** in total. \n\nI'm using a **batch size of 8**. (4 machines run out of memory with a batch size of 8. 32 machines run out of memory even with a batch size of 2).\n\nInstalled version of CUDA and cuDNN:\nubuntu@ip-172-31-16-180:~$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n\nI installed TensorFlow from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\nubuntu@ip-172-31-16-180:~$ python -c \"import tensorflow; print(tensorflow.**version**)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.10.0rc0\n\nCould someone please help me figure how how to fix this and run Inception v3 in a cluster with 32 machines?\n\nMore info:\nHere are the commands I'm executing on the machines in the cluster:\n\n```\nOn machine1:\nCUDA_VISIBLE_DEVICES='' python imagenet_distributed_train.py --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=ps --task_id=0 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=0 > /tmp/worker0 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=1 > /tmp/worker1 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=2 > /tmp/worker2 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=3 > /tmp/worker3 2>&1 &\n\n\nOn machine2:\nCUDA_VISIBLE_DEVICES='' python imagenet_distributed_train.py --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=ps --task_id=1 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=4 > /tmp/worker4 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=5 > /tmp/worker5 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=6 > /tmp/worker6 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=7 > /tmp/worker7 2>&1 &\n\n\nOn machine3:\nCUDA_VISIBLE_DEVICES='' python imagenet_distributed_train.py --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=ps --task_id=2 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=8 > /tmp/worker8 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=9 > /tmp/worker9 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=10 > /tmp/worker10 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=11 > /tmp/worker11 2>&1 &\n\n\nOn machine4:\nCUDA_VISIBLE_DEVICES='' python imagenet_distributed_train.py --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=ps --task_id=3 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=12 > /tmp/worker12 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=13 > /tmp/worker13 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=14 > /tmp/worker14 2>&1 &\npython imagenet_distributed_train.py --batch_size=8 --data_dir=datadir --ps_hosts=worker1:2222,worker2:2222,worker3:2222,worker4:2222 --worker_hosts=worker1:2230,worker1:2231,worker1:2232,worker1:2233,worker2:2230,worker2:2231,worker2:2232,worker2:2233,worker3:2230,worker3:2231,worker3:2232,worker3:2233,worker4:2230,worker4:2231,worker4:2232,worker4:2233 --job_name=worker --task_id=15 > /tmp/worker15 2>&1 &\n```\n", "comments": ["Found solution here: http://stackoverflow.com/questions/39567835/tensorflow-out-of-memory-error-running-inception-v3-distributed-on-4-machines\n"]}, {"number": 4419, "title": "tf.get_variable without an explicit initializer fails for integer types", "body": "The following fails (shape and name are arbitrary):\n\n```\ntf.get_variable(name='foo', shape=(42,), dtype=tf.int32)\n```\n\nException: `TypeError: Expected int32, got -1.7320508075688772 of type 'float' instead.`\n\nIn contrast, using `tf.float32` works just fine.\n\nThe problem appears to be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L658\n\nIf an initializer is not provided (and a default one is not set), a uniform unit scaling init is used (notice that [`sqrt(3)==1.7320...`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops.py#L275)), which of course conflicts with the requested integer type.\n\nWhile this can be mitigated by doing something like:\n\n```\ntf.get_variable(name='foo', dtype=tf.int32, initializer=tf.zeros_initializer(shape=(42,), dtype=tf.int32))\n```\n\nit feels like a smarter default behavior based on the variable type is warranted (or at least a less cryptic error).\n\nTested on the current master.\n", "comments": ["@lukaszkaiser - Hey Lukasz, are you the right person to look at this?\n", "A smarter default behaviour would be nice, it's not too clear what it should be. I think we'd welcome a contribution from outside here.\n", "@lukaszkaiser I'm working to fix it \n", "Any progress on this? I can still get `TypeError: Expected int32, got -0.05 of type 'float' instead.` for tf.int32 Tensors without explicit initializer on Mac OS X, tf 1.0.0.", "A better error message would be nice, but I don't think a default initializer is a good idea: integers frighten and confuse me."]}, {"number": 4418, "title": "Update artifact version in tools/dist_test", "body": "", "comments": []}, {"number": 4417, "title": "Update artifact version in tools/dist_test", "body": "", "comments": []}, {"number": 4416, "title": "GPU Nightly links broken", "body": "The links to .whl files on https://github.com/tensorflow/tensorflow are broken for Linux GPU and Mac GPU\n\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\n\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.10.0-py2-none-any.whl\n", "comments": ["This is because since the version string was updated from 0.10.0rc0 to 0.10.0, there hasn't been a passing build in the Linux GPU nightly configuration yet. See history: http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/\n\nTo work around this temporarily, use URLs in which 0.10.0 is replaced with 0.10.0rc0. It should resolve itself when they pass.\nSorry about the inconvenience. \n", "The Linux GPU whl links should be working now.\n"]}, {"number": 4415, "title": "Branch 133403718", "body": "", "comments": ["PTAL, @andrewharp \n"]}, {"number": 4414, "title": "Branch 133399050", "body": "", "comments": []}, {"number": 4413, "title": "Unexpected behaviour for tf.InteractiveSession() in Jupyter Notebook", "body": "I'm not convinced this is a bug, but if I run `tf.InteractiveSession()` in a cell with no assignment (e.g. `sess = tf.InteractiveSession()`), the interactive session will work only as long as the call is the last operation in the notebook cell and the output is not suppressed with `;`. This tripped me over a few times, so I'm reporting it just in case this is not the intended behaviour.\n\nFWIW I think this is because IPython/Jupyter stores the session automatically in the cases I highlighted, making the command alike `sess = tf.InteractiveSession()` which works as expected.\n### Environment info\n\nOperating System: Linux\n\nInstalled version of CUDA and cuDNN: Using CPU version\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 0.10.0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI open a Jupyter notebook, and this works:\n\n``` python\nIn [1]: import tensorflow as tf\nIn [2]: tf.InteractiveSession()\nIn [3]: tf.constant(0).eval()\n```\n\nThis doesn't:\n\n``` python\nIn [1]: import tensorflow as tf\nIn [2]: tf.InteractiveSession();\nIn [3]: tf.constant(0).eval()\n```\n\nNeither does this one:\n\n``` python\nIn [1]: import tensorflow as tf\nIn [2]: tf.InteractiveSession()\n        print()\nIn [3]: tf.constant(0).eval()\n```\n\nThe both fail with:\n\n``` python\nValueError                                Traceback (most recent call last)\n<ipython-input-3-b43e5df4f2f7> in <module>()\n----> 1 tf.constant(0).eval()\n\n/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in eval(self, feed_dict, session)\n    557 \n    558     \"\"\"\n--> 559     return _eval_using_default_session(self, feed_dict, self.graph, session)\n    560 \n    561 \n\n/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in _eval_using_default_session(tensors, feed_dict, graph, session)\n   3640     session = get_default_session()\n   3641     if session is None:\n-> 3642       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n   3643                        \"session is registered. Use `with \"\n   3644                        \"sess.as_default()` or pass an explicit session to \"\n\nValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\n```\n", "comments": ["what if you do\n\n``````\nsess=tf.InteractiveSession()\nsess.run(tf.constant(0))\n\n```?\n\nI'm wondering if the `InteractiveSession` line even gets executed\n``````\n", "@yaroslavvb that works. do you have any idea why? \n\nIt certainly looks like its initializing a session but not setting the default session variable.\n\n```\nIn [1]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n\nIn [2]: tf.InteractiveSession();\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\npciBusID 0000:03:00.0\nTotal memory: 4.00GiB\nFree memory: 2.75GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:03:00.0)\n\nIn [3]: tf.constant(0).eval()\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-3-b43e5df4f2f7> in <module>()\n----> 1 tf.constant(0).eval()\n\n/home/hholst/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in eval(self, feed_dict, session)\n    557\n    558     \"\"\"\n--> 559     return _eval_using_default_session(self, feed_dict, self.graph, session)\n    560\n    561\n\n/home/hholst/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in _eval_using_default_session(tensors, feed_dict, graph, session)\n   3640     session = get_default_session()\n   3641     if session is None:\n-> 3642       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n   3643                        \"session is registered. Use `with \"\n   3644                        \"sess.as_default()` or pass an explicit session to \"\n\nValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\n\nIn [4]:\n```\n", "You are not saving `tf.InteractiveSession()` into any variable, so it gets garbage collected. Jupyter has helper variable `_` which stores output of last expression, so in first case this variable prevents your session from getting gc'ed. In the second case, the output of your cell is result of `print`, so your session doesn't get stored anywhere\n", "@yaroslavvb I also think it may be something related to the GC, but perhaps Jupyter has a more complex variable persistence system?\nIf you run a cell with\n\n``` python\ntf.InteractiveSession()\n```\n\nand then delete the contents of the cell, and run\n\n```\n1\n```\n\nThen `1` is stored in `_` (as verified by running `print(_)`), but the session still works (as verified by `tf.constant(0).eval()`). Could it be that Jupyter stores a history of outputs somewhere?\n\nIn any case, I understand that the best way to avoid all this is to assign the output of `tf.InteractiveSession()` to a variable, and I'm not sure if TensorFlow can do anything to avoid this confusion likely caused by Jupyter.\n\n@hholst80 Bit off-topic, but how did you set Jupyter to output such verbose cell output?\n", "If it was as simple as having the session variabel in the workspace, two evals in sequence would not work. The first one would overwrite _ and the session would be GC. I would say something is wrong with the internals of tf.InteractiveSession?\n\nPS. The output is from  ipython.\n", "there's Jupyter history  that saves output of previous evals\n\nOn Sep 17, 2016 4:26 PM, \"Henrik Holst\" notifications@github.com wrote:\n\n> If it was as simple as having the session variabel in the workspace, two\n> evals in sequence would not work. The first one would overwrite _ and the\n> session would be GC. I would say something is wrong with the internals of\n> tf.InteractiveSession?\n> \n> PS. The output is from ipython.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4413#issuecomment-247814314,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHH2NkkBPidMcBh4x9qV9Eg8brzLCks5qrHcggaJpZM4J_Ks4\n> .\n", "According to the documentation, you need to assign the result of `tf.InteractiveSession` to a variable:\nhttps://www.tensorflow.org/versions/r0.11/api_docs/python/client.html#InteractiveSession\n\nThat being said, the behavior seems to be different between 0.10 and nightly build, so this may be working differently now. Could you try it in the nightly build?\n", "Automatically closing due to lack of recent response. Please reopen when additional information becomes available. Thanks!\n"]}, {"number": 4412, "title": "Branch 133345765", "body": "", "comments": ["Will push again when the linter fixes are in. \n"]}, {"number": 4411, "title": "Add gradient to BiasAddGrad op.", "body": "A rather inelegant solution to fix #4174.\n", "comments": ["Can one of the admins verify this patch?\n", "@admcrae, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @tensorflower-gardener and @zheng-xq to be potential reviewers\n", "Changing assignment to @zheng-xq to reflect the reality of the review.\n", "Done. Note I haven't made any changes to the tests since your previous comment--I wasn't entirely sure what you wanted there (see my last comment above).\n", "LGTM.\n\n@tensorflow-jenkins test this please.\n", "@martinwicke, could you confirm the mac test failure is unrelated? Thanks. \n", "Yes, unrelated. Good to merge.\n", "@jhseu feel free to use your powers and override the failed test.\n"]}, {"number": 4410, "title": "[docs] Fix formatting-related issues and address #2083", "body": "Improving tensorflow/g3doc/get_started/os_setup.md by enforcing 80-char width limit, which is widely used in docs, removing trailing whitespaces and adressing point 2 from #2083.\n", "comments": ["Can one of the admins verify this patch?\n", "@omtcyfz, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @yifeif to be potential reviewers\n", "By the way, is the first point from #2083\n\n> In the \"pip installation\" and \"virtualenv installation\" section, should specify that the wheel package is specifically for Python 3.4. \n> As a side note, a wheel package for Python 3.5 would be greatly appreciated.\n\nStill the case? IIUC TensorFlow now has Python 3.5 wheel package. Please correct me if I am wrong.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins Test this please.\n", "@andrewharp Thank you very much!\n"]}, {"number": 4409, "title": "Support Python 2 and 3 in notebooks", "body": "Support Python 2 and 3 in notebooks\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins Test this please.\n", "Looks like the GPU failure here is preexisting and unrelated.\n", "Look like a network error, the boost tarball failed downloading.\n", "Overriding test failure, infrastructure failure.\n"]}, {"number": 4408, "title": "Variable initialization problem", "body": "Hi, I am getting a error \"Attempting to use uninitialized value\" on tf.initialize_all_variables when I try to incorporate the shape of a tensor via tf.shape in a loss function that depends on a dynamically shaped input tensor, and in between, on the variable that does not get initialized and on tf.scan. I am trying to get a minimal working example of the bug, but I couldn't reproduce it yet, so I thought I would ask whether this or a similar issue is already known before I continue. The only reproducible step is that the initialization works when I replace tf.to_float(tf.shape(x))[0] in my loss function with tf.constant(1.0), which makes me believe it has something to do with tf.shape or perhaps with the tf.scan.\n", "comments": ["Without a reproducible test case, it would probably be best to first ask around on StackOverflow and see if others can answer your question. If you cant get a good answer and StackOverflw, and if you can get a working example that reproduces the problem, please submit it here.\n"]}, {"number": 4407, "title": "Error with my own model trained in android demo ", "body": "SETTING CODE ANDROID \n\n  private static final int NUM_CLASSES = 26;\n  private static final int INPUT_SIZE = 299;\n  private static final int IMAGE_MEAN = 128;\n  private static final float IMAGE_STD = 128;\n  private static final String INPUT_NAME = \"Mul:0\";\n  private static final String OUTPUT_NAME = \"final_result:0\";\n\nERROR ADB LOGCAT \n\ntensorflow_jni.cc:304 Error during inference: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered kernels:\n\n<no registered kernels>\n\n [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]\n\n09-16 01:01:02.882 3006-3029/org.tensorflow.demo A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 3029 (InferenceThread)\n09-16 01:01:02.989 257-257/? A/DEBUG: pid: 3006, tid: 3029, name: InferenceThread  >>> org.tensorflow.demo <<<\n09-16 01:01:03.060 257-257/? A/DEBUG:     #05 pc 00610a98  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so\n09-16 01:01:03.061 257-257/? A/DEBUG:     #06 pc 00610c1c  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so\n09-16 01:01:03.061 257-257/? A/DEBUG:     #07 pc 00610c38  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so\n09-16 01:01:03.061 257-257/? A/DEBUG:     #08 pc 00083f40  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so\n09-16 01:01:03.061 257-257/? A/DEBUG:     #09 pc 0008415c  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so \n", "comments": ["I found the solution\n\nhttps://github.com/tensorflow/tensorflow/issues/1269   \n", "Closing, since it looks like you found a solution.\n", "Hey,\r\n\r\nMind please explain how do you compute INPUT_SIZE etc?\r\nThank you!"]}, {"number": 4406, "title": "Deal with 1d shape in variance_scaling_initializer.", "body": "Fixes #3825.\n\nAn initializer could be used anywhere. If the shape for variance_scaling_initializer is 1d, we can use the same value for both fan_in & fan_out.\n", "comments": ["@imironhead, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener and @theweiho to be potential reviewers\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins Test this please.\n"]}, {"number": 4405, "title": "Deal with 1d shape in variance_scaling_initializer.", "body": "[issue#3825](https://github.com/tensorflow/tensorflow/issues/3825)\n\nAn initializer could be used anywhere. If the shape for variance_scaling_initializer is 1d, we can use the same value for both fan_in & fan_out.\n", "comments": ["Can one of the admins verify this patch?\n", "@imironhead, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener and @theweiho to be potential reviewers\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "change author\n"]}, {"number": 4404, "title": "update and fix problem in API doc.", "body": "The parameters in bounding box should be [y_min, x_min, y_max, x_max]\n", "comments": ["@perhapszzy, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener to be a potential reviewer\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Last test failure was infrastructure. Overriding.\n"]}, {"number": 4403, "title": "Error while installation on virtual environment", "body": "I'm trying to install Tensor Flow on my virtual environment with Python 2.7.6. \n\nWhen I try the command `pip install --upgrade $TF_BINARY_URL` as mentioned in the [documentation](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#virtualenv-installation), I get the following error - \n\nDownloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\n  Downloading tensorflow-0.10.0-cp27-none-linux_x86_64.whl (36.6MB): 8.4MB downloaded\nCleaning up...\nException:\nTraceback (most recent call last):\n  File \"/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/commands/install.py\", line 278, in run\n    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\n  File \"/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/req.py\", line 1197, in prepare_files\n    do_download,\n  File \"/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/req.py\", line 1375, in unpack_url\n    self.session,\n  File \"/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 572, in unpack_http_url\n    download_hash = _download_url(resp, link, temp_location)\n  File \"/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 433, in _download_url\n    for chunk in resp_read(4096):\n  File \"/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 421, in resp_read\n    chunk_size, decode_content=False):\n  File \"/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/response.py\", line 236, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/naman/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/response.py\", line 183, in read\n    data = self._fp.read(amt)\n  File \"/usr/lib/python2.7/httplib.py\", line 573, in read\n    s = self.fp.read(amt)\n  File \"/usr/lib/python2.7/socket.py\", line 380, in read\n    data = self._sock.recv(left)\n  File \"/usr/lib/python2.7/ssl.py\", line 341, in recv\n    return self.read(buflen)\n  File \"/usr/lib/python2.7/ssl.py\", line 260, in read\n    return self._sslobj.read(len)\nSSLError: [Errno 1] _ssl.c:1429: error:1408F119:SSL routines:SSL3_GET_RECORD:decryption failed or bad record mac\n\nStoring debug log for failure in /tmp/tmpcvH6K0\n\nI'm using Ubuntu 14.04 wubi on top of Windows 7. Please sort this error. \n", "comments": ["That looks like possibly a transient network error. Did you try rerunning it again?\n", "I have tried it many times, but the same error again.\n", "You can try googling for that Python error. It is definitely not unique to TensorFlow, and given that the package works for me on Ubuntu 14, it has to either be your Python or SSL configuration or your network. You can try downloading the file manually with wget and then pip installing afterwards.\n\ni.e.\n\n`wget  $TF_BINARY_URL`\n`pip install <insert the filename that is downloaded>`\n", "A quick google search points to a known openssl issue. \r\nClosing."]}, {"number": 4402, "title": "explicit device specification of restore operation on distributed training", "body": "I'm trying the run the distributed training of Inception, and this is the error I'm getting, the moment workers connect to ps. \n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/restore_slice_1268': Could not satisfy explicit device specification '/job:ps/task:0/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/gpu:0, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:1/gpu:0\n     [[Node: save/restore_slice_1268 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/restore_slice_1268/tensor_name, save/restore_slice_1268/shape_and_slice)]]\n\nCaused by op u'save/restore_slice_1268', defined at:\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception_distributed_train.py\", line 233, in train\n    saver = tf.train.Saver()\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 861, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 519, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 272, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 187, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 203, in _restore_slice\n    preferred_shard, name=name)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n```\n\nI'm using the latest pip TensorFlow (0.10) on Cuda 7.5 and Cudnn v5.\n", "comments": ["This is a question better suited for StackOverflow. Please include more detailed information about your run there and tag it with the `tensorflow` tag.\n"]}]