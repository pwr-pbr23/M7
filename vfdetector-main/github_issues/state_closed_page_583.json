[{"number": 36191, "title": "[ROCm][XLA] Adding three passes to  amdgpu compiler", "body": "This CL (comes from discussions in #35991) introduced three passes to keep amdgpu_compiler in sync with nvptx_compiler:\r\n\r\n- `reduction_degernerate_dim_remover`\r\n- `reduction_dimension_grouper`\r\n- `reduction_layout_normalizer`\r\n\r\n@whchung @cheshire ", "comments": ["@cheshire Is it causing some internal failures? I don't have visibility in the `import/copybara` tab.", "That's strange, let me try again."]}, {"number": 36190, "title": "Use `config=v1` as this is `r1.15` branch.", "body": "Otherwise, by default we were building V2 code on a v1 version.", "comments": ["I want to link it to issue https://github.com/tensorflow/tensorflow/issues/36179 for history.\r\n\r\nbtw, I found that 1.15.1 is ~5% slower than 1.12. I benchmarked resnet50_kears and simple tensorflow-addition.\r\n\r\nJust in case I missed something, is it expected or this is regression to fix?\r\n\r\nNot sure if I have resources to debug it but maybe I could compare with v2 after upgrade.", "There is a large difference between 1.12 and 1.15.1. A better comparison would be between 1.15.0 and 1.15.1."]}, {"number": 36189, "title": "Unable to build \"Hello World\" for ESP32", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: source\r\n- Tensorflow version: ca0d5142f640d42037f22367ce3530b6b7a23b44\r\n- Target platform: ESP32\r\n\r\n**Describe the problem**\r\nBuilding the example Hello World for ESp32 fails.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n1. I successfully generated example by command:\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project`\r\n\r\n2. I run building project by command:\r\n`idf.py build`\r\n\r\n3. I receved compilation error:\r\n`[ 84%] Building C object esp-idf/protocomm/CMakeFiles/__idf_protocomm.dir/proto-c/constants.pb-c.c.obj\r\nIn file included from /home/dmytro/esp/projects/hello_world_tf/components/tfmicro/tensorflow/lite/micro/kernels/svdf.cc:25:\r\n/home/dmytro/esp/projects/hello_world_tf/components/tfmicro/tensorflow/lite/micro/kernels/activation_utils.h: In function 'float tflite::ops::micro::ActivationValFloat(TfLiteFusedActivation, float)':\r\n/home/dmytro/esp/projects/hello_world_tf/components/tfmicro/tensorflow/lite/micro/kernels/activation_utils.h:45:1: error: control reaches end of non-void function [-Werror=return-type]\r\n }\r\n ^\r\n`\r\n", "comments": ["I suppose issue related to changes made by [this commit.](https://github.com/tensorflow/tensorflow/commit/38ca4ecbf7418ace4d8c7edaf179cae7fc4c1ca5#diff-e574b564bf79a94413a08244c308291dL46)", "My English is very poor\uff0cshow mofidy code\r\n\r\n```\r\ndiff --git a/tensorflow/lite/micro/kernels/activation_utils.h b/tensorflow/lite/micro/kernels/activation_utils.h\r\n\r\nindex 62f3237bac..5fe05967ad 100644\r\n--- a/tensorflow/lite/micro/kernels/activation_utils.h\r\n+++ b/tensorflow/lite/micro/kernels/activation_utils.h\r\n@@ -41,6 +41,8 @@ inline float ActivationValFloat(TfLiteFusedActivation act, float a) {\r\n       return std::signbit(a);\r\n     case kTfLiteActSigmoid:\r\n       return 1.0f / (1.0f + std::exp(-a));\r\n+    default:\r\n+      return a;\r\n   }\r\n }\r\n\r\n```", "Any resolution on this yet? I am having the same problem. (but can build other examples e.g. micro_speech)", "@rockyrhodes , @Saduf2019 \r\n**The issue has updated.**\r\n\r\n@tensorflow/micro\r\n\r\n**System information**\r\n\r\n- Host OS Platform and Distribution: Linux Ubuntu 20.04\r\n- Tensorflow sources version: 71ff216576\r\n- Target platform: ESP32\r\n- ESP IDF version: v4.0.1\r\n\r\n**Describe the problem**\r\nBuilding the example Hello World for ESp32 fails.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n1. I have cloned TensorFlow sources.\r\n\r\n2. I successfully generated example by command:\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\n\r\n3. I run building project by command:\r\nidf.py build\r\n\r\n4. I receved compilation error:\r\ncc1plus: all warnings being treated as errors\r\nninja: build stopped: subcommand failed.\r\nninja failed with exit code 1\r\n\r\nFull txt log of building \"Hello World\" attached to this message.\r\n[Unable_to_build__Hello_World__for_ESP32.txt](https://github.com/tensorflow/tensorflow/files/5078582/Unable_to_build__Hello_World__for_ESP32.txt)\r\n", "Hi @mr-goldhands. I think the build issue is fixed with [this PR](./tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world), can you please check. If the problem is fixed you may close the issue, thank you.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36189\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36189\">No</a>\n"]}, {"number": 36188, "title": "Functions removed from tensorflow namespace between 1.15.0 and 1.15.1", "body": "I'm not sure if this was intended behavior, but if so it wasn't documented in the 1.15.1 release notes. I first hit this for `tf.set_random_seed()`, but it even effects the sample code for getting the version.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pip (binary wheel)\r\n- TensorFlow version (use command below): `v1.15.0-85-gfdb85890df 1.15.1`\r\n- Python version:` Python 3.7.3 (default, Mar 27 2019, 09:23:15)  [Clang 10.0.1 (clang-1001.0.46.3)]`\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nMethods/variables such as tensorflow.set_random_seed and tensorflow.VERSION were present in tensorflow 1.15.0 but are removed in 1.15.1\r\n\r\n**Describe the expected behavior**\r\nThe interface should be identical between 1.15.0 and 1.15.1.\r\n\r\n**Code to reproduce the issue**\r\n```shell\r\n# Works as expected\r\n$ pip install -q tensorflow==1.15.0\r\n$ python -c \"import tensorflow as tf; tf.set_random_seed(42)\"\r\n# Same command fails on 1.15.1\r\n$ pip install -q tensorflow==1.15.1\r\n$ python -c \"import tensorflow as tf; tf.set_random_seed(42)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'set_random_seed'\r\n```\r\nUsing the provided sample code to get the tensorflow version:\r\n```shell\r\n$ pip install -q tensorflow==1.15.0\r\n$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.15.0-rc3-22-g590d6eef7e 1.15.0\r\n\r\n# TF 1.0 interface fails\r\n$ pip install -q tensorflow==1.15.1\r\n$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'\r\n\r\n# TF 2.0 interface succeeds\r\n$ python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nv1.15.0-85-gfdb85890df 1.15.1\r\n```\r\n", "comments": ["I've worked around this in my code by using `tensorflow.compat.v1` instead, but I wouldn't expect such a major change to happen in a bugfix release.", "Sounds like this might be the same root cause as https://github.com/tensorflow/tensorflow/issues/36179 ?", "It is the same root cause. Sorry about that.", "No problem, thanks for confirming."]}, {"number": 36187, "title": "[ROCm][XLA:GPU] Fixing Atomic CAS codegen in ir_emitter", "body": "This is a follow-up to #35881 and #36110. This CL introduce changes in XLA ir emission logic to fix atomic CAS code-gen in `AMDGPU` side.\r\n\r\n- Line 321-324 make sure that GPU kernel variables are allocated (in private register files) at function entry, and made available before execution.\r\n- Line 347-359 make sure `cas_new_out_address` is cast to default address space.\r\n\r\n@cheshire @whchung ", "comments": ["Changes have been merged internally , seems like auto-merge did not happen so closing this issue."]}, {"number": 36186, "title": "Update the docstring of @tf.exports  methods in tf.nest.", "body": "This PR is to resolve the issue #36146.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36186) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36186) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I signed it!", "@punndcoder28 please sign CLA ", "@googlebot I signed it!", "@rthadur I've signed the CLA. I had made a mistake with the previous signing, I changed it and commented but the google bot is not replying.", "@punndcoder28 , If you are signing CLA ([here](https://cla.developers.google.com/)) then please sign CLA for all the email IDs which you are already logged in in your browser. I faced the same problem once but I got solution after posting the question to stackoverflow [here](https://stackoverflow.com/questions/57214178/how-should-i-troubleshoot-my-google-contributor-licence-agreement-issue-on-my-pu/57228116#57228116).", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36186) for more info**.\n\n<!-- ok -->", "@ashutosh1919 Thanks a lot!", "this PR is to fix issue #36146. (Mentioned so that whenever someone sees that issue, he/she won't start work on that because this PR is already there on that issue).", "Hey @mdanatg . \r\n\r\n> As you make changes to the code, be sure to run the unit tests locally to make sure they still pass: bazel test //tensorflow/python/util/.... It will save you a lot of time.\r\n\r\nI tried doing this but it is showing me this error \r\n```\r\nERROR: Skipping '//tensorflow/python/util/...': no targets found beneath 'tensorflow/python/util'\r\nERROR: no targets found beneath 'tensorflow/python/util'\r\n```\r\n\r\nShould I run `bazel build` first and then test?", "> ERROR: Skipping '//tensorflow/python/util/...': no targets found beneath 'tensorflow/python/util'\r\n> ERROR: no targets found beneath 'tensorflow/python/util'\r\n\r\nSorry, I gave you the wrong command line. It seems that all tests are bundled up in the parent directory. Here's the proper command:\r\n```\r\nbazel test //tensorflow/python:util_nest_test\r\n```\r\n\r\nBut you should run all tests to be sure, once util_nest_test passes. It will take longer the first time, but second calls should be faster once the results are cached:\r\n\r\n```\r\nbazel test //tensorflow/python/...\r\n```", "Thanks!", "I tried running the unit tests but I am getting a very cryptic error message saying \r\n```\r\nERROR: .../tensorflow/tensorflow/python/BUILD:694:1: C++ compilation of rule '//tensorflow/python:_pywrap_checkpoint_reader.so' failed (Exit 1)\r\n```\r\nThis is my first time running unit tests.", "That error is pretty obscure - not sure what's causing it. There should be a way to get more details about it. But first I'd double check the environment:\r\n * if you haven't already, go through the instructions to [set up and configure build](https://www.tensorflow.org/install/source#setup_for_linux_and_macos) (first two sections, until \"Build the pip package\")\r\n * run the tests at head, to make sure they pass without your changes (it's rare, but sometime the build does break)", "Which version of Bazel is needed. I have installed version 2.0.0 on my OSX. I tried `bazel build` and it works fine and I get the output \r\n```INFO: Build completed successfully, 1 total action```. `./configure` also works fine.", "If `./configure` doesn't complain then you should have the correct version. The tests should now pass at head.", "@mdanatg I am still getting the same cryptic error. I googled but none of the suggestions worked. I am still getting the same error.", "You need to provide more details (what exact commands did you run, whether you ran them at head of with your changes, the complete console output, etc.) otherwise I won't be able to help.", "I did the following commands:\r\n1. I wanted to check the head of the repo:\r\n```\r\ntensorflow git:(master) cat .git/HEAD\r\nref: refs/heads/master\r\n```\r\n2. Following which I did the `./configure` command and got the following output\r\n```\r\ntensorflow git:(master) ./configure                  \r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 1.2.1 installed.\r\nPlease specify the location of python. [Default is /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /Library/Python/2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Library/Python/2.7/site-packages]\r\n/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: \r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: \r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nDo you wish to build TensorFlow with iOS support? [y/N]: \r\nNo iOS support will be enabled for TensorFlow.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n```\r\n3. Then I built tensorflow using `bazel build` \r\n```\r\ntensorflow git:(master) bazel build\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=130\r\nINFO: Reading rc options for 'build' from /Users/puneethk/Desktop/Projects/Open-Source/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /Users/puneethk/Desktop/Projects/Open-Source/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python --action_env PYTHON_LIB_PATH=/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages --python_path=/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /Users/puneethk/Desktop/Projects/Open-Source/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/puneethk/Desktop/Projects/Open-Source/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:xla in file /Users/puneethk/Desktop/Projects/Open-Source/tensorflow/.tf_configure.bazelrc: --define with_xla_support=true\r\nINFO: Found applicable config definition build:macos in file /Users/puneethk/Desktop/Projects/Open-Source/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nWARNING: Usage: bazel build <options> <targets>.\r\nInvoke `bazel help build` for full description of usage and options.\r\nYour request is correct, but requested an empty set of targets. Nothing will be built.\r\nINFO: Analyzed 0 targets (0 packages loaded, 0 targets configured).\r\nINFO: Found 0 targets...\r\nINFO: Deleting stale sandbox base /private/var/tmp/_bazel_puneethk/4c7f735167707afd63484e296f46e040/sandbox\r\nINFO: Elapsed time: 1.643s, Critical Path: 0.03s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\n```\r\n4. Finally I tested it using `bazel test //tensorflow/python:util_nest_test` and got the following error at the end\r\n```\r\nERROR: /Users/puneethk/Desktop/Projects/Open-Source/tensorflow/tensorflow/core/util/BUILD:338:1: Executing genrule //tensorflow/core/util:version_info_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_puneethk/4c7f735167707afd63484e296f46e040/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/git/gen_git_source.runfiles/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 340, in <module>\r\n    generate(args.generate, args.git_tag_override)\r\n  File \"/private/var/tmp/_bazel_puneethk/4c7f735167707afd63484e296f46e040/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/git/gen_git_source.runfiles/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 272, in generate\r\n    git_version = get_git_version(data[\"path\"], git_tag_override)\r\n  File \"/private/var/tmp/_bazel_puneethk/4c7f735167707afd63484e296f46e040/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/git/gen_git_source.runfiles/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 172, in get_git_version\r\n    str(\"--work-tree=\" + six.ensure_str(git_base_path)), \"describe\",\r\nAttributeError: 'module' object has no attribute 'ensure_str'\r\nTarget //tensorflow/python:util_nest_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```", "That's odd. I suspect the libraries aren't configured properly. This line is particularly suspicious:\r\n\r\n```\r\nINFO: Reading rc options for 'build' from /Users/puneethk/Desktop/Projects/Open-Source/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python --action_env PYTHON_LIB_PATH=/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages --python_path=/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python --config=xla --action_env TF_CONFIGURE_IOS=0\r\n```\r\n\r\nIt looks like, although you specified Python 3 in ./configure, somehow bazel is still picking python2 binaries and that's definitely wrong.\r\n\r\nThe build instructions have some special steps for MacOS: https://www.tensorflow.org/install/source - have you followed those?", "Yes. I followed the steps from the build tutorial.", "Ok, I recommend filing an issue about this - the instructions should get you to a functional setup, and perhaps others who ran into this will know a solution.", "I have opened a new issue #36382 regarding the same.", "Hey @mdanatg . I ran the tests on a different machine and it passed. I ran the tests on the branch which had changes to the util.py.", "Great! Now you just need to update nest.py to use the new method.", "@mdanatg I had a doubt. Are the namespace methods in `tensorflow/python/util/util.h` call the helper methods in `tensorflow/python/util/util.cc`? I've made changes to both the files to support `MutableMapping` but getting build time errors. Also I couldn't find the definitions of methods  declared in the `util.h` file to add the definition for the method to support mutable mapping.", "> Are the namespace methods in `tensorflow/python/util/util.h` call the helper methods in `tensorflow/python/util/util.cc`?\r\n\r\nNot exactly. The `.h` file contains declarations for the functions inside the `.cc` file. That is, it has the function signature with no body, and the `.cc` file repeats the function signature, adding an actual body. Here you might find more information about why that's being done: https://www.learncpp.com/cpp-tutorial/header-files/\r\n\r\nI noticed the header file is missing from the PR - did you commit it?\r\n", "> I noticed the header file is missing from the PR - did you commit it?\r\n\r\nI haven't committed it yet as the bazel test was failing locally with an error. I am trying to fix that and then I'll commit.", "I am getting \r\n```\r\nImportError: tensorflow/python/_pywrap_utils.so: undefined symbol: _ZN10tensorflow4swig16IsMutableMappingEP7_object\r\n```\r\nerror when I try to build with my changes. I have `IsMutableMapping` declared in the `.h` file and defined with the same signature in the `.cc` file. Is there anything else that I have to do and that I am missing?", "There are probably a few other files that need changing. A search for `IsMapping` in all files shows two other instances, `python/util/util_wrapper.cc` and `tools/def_file_filter/symbols_pybind.txt`. You might also find the docs for [pybind11](https://pybind11.readthedocs.io/en/stable/) and [bazel](https://docs.bazel.build/versions/master/build-ref.html) useful in better understanding how things tie together.", "Looks like there is a lint error - see the Details link next to the Ubuntu Sanity check above, search for do_pylint in the logs.", "Yes. I noticed it as soon as you approved the PR. I see how to resolve that immediately.\n\nSent from my iPhone\n\n> On 12-Feb-2020, at 4:17 AM, Dan Moldovan <notifications@github.com> wrote:\n> \n> \ufeff\n> Looks like there is a lint error - see the Details link next to the Ubuntu Sanity check above, search for do_pylint in the logs.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "I don't usually enable pylint while coding as it gives me errors even if the code is right. I will enable it on my local machine and check what the issue is."]}, {"number": 36185, "title": "tf.data.Dataset.from_tensor_slices throws error when using an array of numbers instead of a string", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (64 bit), Windows-10-10.0.18362-SP0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI'm using tf.data.Dataset.from_tensor_slices to create a dataset from a dataframe. The dataframe has a column called \"Text\", which usually contains strings as entries. Executing it like that works fine. However, when I encode those strings to arrays of number, the function throws an error.\r\n\r\nThe number of the failed arrays is 2312, while the complete number of arrays is actually around 3600. So not every array is failing.\r\n\r\n**Describe the expected behavior**\r\nIt should work without error.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\ndef df_to_dataset(dataframe):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('Emotion')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  return ds\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\structure.py\", line 91, in normalize_element\r\n    spec = type_spec_from_value(t, use_fallback=False)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\structure.py\", line 457, in type_spec_from_value\r\n    (element, type(element).__name__))\r\nTypeError: Could not build a TypeSpec for 2666    [1424, 11572, 1372, 11572, 2289, 11584, 11572,...\r\n2091    [11637, 11572, 690, 11572, 11, 11572, 3376, 11...\r\n832     [11613, 11572, 72, 11572, 65, 11572, 472, 1157...\r\n2572    [355, 11572, 11613, 11572, 36, 11572, 11645, 1...\r\n2187              [182, 11572, 405, 11572, 1, 11572, 335]\r\n                              ...\r\n3427    [4416, 11572, 37, 11572, 4408, 11572, 2, 11572...\r\n2868    [20, 11572, 23, 11572, 11637, 11572, 254, 1157...\r\n1806    [6870, 11572, 9118, 11572, 5883, 11572, 5190, ...\r\n1266    [44, 11572, 525, 11572, 702, 11572, 63, 11572,...\r\n3047    [1795, 11572, 46, 11572, 503, 11572, 5, 11572,...\r\nName: Text, Length: 2312, dtype: object with type Series\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \".\\Mood recognition.py\", line 85, in <module>\r\n    train_ds = df_to_dataset(train, batch_size=batch_size)\r\n  File \".\\Mood recognition.py\", line 70, in df_to_dataset\r\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 435, in from_tensor_slices\r\n    return TensorSliceDataset(tensors)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 2354, in __init__\r\n    element = structure.normalize_element(element)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\structure.py\", line 96, in normalize_element\r\n    ops.convert_to_tensor(t, name=\"component_%d\" % i))\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1184, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1242, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 235, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Can't convert non-rectangular Python sequence to Tensor.\r\n```\r\n\r\n", "comments": ["It's probably an issue with the different length of the arrayy.", "Or if you have duplicated columns (the same names) in dataframe.", "Having this issue with 2.3, no duplicate columns or ragged arrays.", "I'm having the same issue with the same version of tensorflow  (2.3)", "Turns out I had a null value in one of the rows for the numeric columns in my data that was causing the issue", "> Turns out I had a null value in one of the rows for the numeric columns in my data that was causing the issue\r\n\r\nThis works for me...", "I am running into the same issue, but there isn't any null value"]}, {"number": 36184, "title": "./build_aarch64_lib.sh fails in densify.cc", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nUnable to build the  TensorFlow Lite static library for ARM64  using the master branch of the repo. It fails during compilation (logs added at the bottom)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nUsing the following instructions to build tensorflow lite for arm64\r\nhttps://www.tensorflow.org/lite/guide/build_arm64\r\n\r\n**Any other info / logs**\r\n\r\n`aarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/root/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/root/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/root/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/root/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/root/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/root/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include \\\r\n-o /root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal /root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/examples/minimal/minimal.o \\\r\n /root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lstdc++ -lpthread -lm -ldl\r\n/root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(densify.o): In function `tflite::ops::builtin::densify::Eval(TfLiteContext*, TfLiteNode*)':\r\ndensify.cc:(.text+0x378): undefined reference to `tflite::optimize::sparsity::FormatConverter<signed char>::FormatConverter(std::vector<int, std::allocator<int> > const&, TfLiteSparsity const&)'\r\ndensify.cc:(.text+0x384): undefined reference to `tflite::optimize::sparsity::FormatConverter<signed char>::SparseToDense(signed char const*)'\r\ndensify.cc:(.text+0x5f0): undefined reference to `tflite::optimize::sparsity::FormatConverter<float>::FormatConverter(std::vector<int, std::allocator<int> > const&, TfLiteSparsity const&)'\r\ndensify.cc:(.text+0x5fc): undefined reference to `tflite::optimize::sparsity::FormatConverter<float>::SparseToDense(float const*)'\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:286: recipe for target '/root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal' failed\r\nmake: *** [/root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\nmake: Leaving directory '/root/tensorflow'`", "comments": ["I'm getting something similar on the Librem5/PureOS\r\n```\r\nroot@pureos:/home/purism/voice_cmd/tensorflow# ./tensorflow/lite/tools/make/build_aarch64_lib.sh\r\n...\r\naarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include \\\r\n-o /home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal /home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/examples/minimal/minimal.o \\\r\n /home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lstdc++ -lpthread -lm -ldl\r\n/usr/bin/ld: /home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(densify.o): in function `tflite::ops::builtin::densify::Eval(TfLiteContext*, TfLiteNode*)':\r\ndensify.cc:(.text+0x348): undefined reference to `tflite::optimize::sparsity::FormatConverter<signed char>::FormatConverter(std::vector<int, std::allocator<int> > const&, TfLiteSparsity const&)'\r\n/usr/bin/ld: densify.cc:(.text+0x354): undefined reference to `tflite::optimize::sparsity::FormatConverter<signed char>::SparseToDense(signed char const*)'\r\n/usr/bin/ld: densify.cc:(.text+0x6c0): undefined reference to `tflite::optimize::sparsity::FormatConverter<float>::FormatConverter(std::vector<int, std::allocator<int> > const&, TfLiteSparsity const&)'\r\n/usr/bin/ld: densify.cc:(.text+0x6cc): undefined reference to `tflite::optimize::sparsity::FormatConverter<float>::SparseToDense(float const*)'\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [tensorflow/lite/tools/make/Makefile:287: /home/purism/voice_cmd/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\nmake: Leaving directory '/home/purism/voice_cmd/tensorflow'\r\n```", "Confirmed. Working on this.", "Thank you. In the mean time for anyone else struggling with this issue, I was able to backtrack to an older commit of the repo:\r\nfc7bce9b4ada6ef123b899ed88889923c9fafae6\r\n"]}, {"number": 36183, "title": "TFLite new quantizer brokes ALBERT model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): docker container with ubuntu 16.04, running on host with fedora 31\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.2.0-dev20200119\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nconfig = tr.AlbertConfig.from_pretrained(model_dir)\r\n    model = tr.TFAlbertForSequenceClassification.from_pretrained(\r\n        model_dir + 'pytorch_model.bin',\r\n        from_pt=True,\r\n        config=config\r\n    )\r\n    \r\ninput_ids: tf.TensorSpec = tf.TensorSpec((1, 128), dtype=tf.int32, name='input_ids')\r\nattn_mask: tf.TensorSpec = tf.TensorSpec((1, 128), dtype=tf.int32, name='attn_mask')\r\nmodel._set_inputs([input_ids, attn_mask], training=False)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\nconverter.experimental_new_quantizer = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.representative_dataset = representative_dataset\r\ntflite_model = converter.convert()\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-01-24 16:32:16.530758: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2020-01-24 16:32:16.554102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-24 16:32:16.557035: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-01-24 16:32:16.557052: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: bb9f01c415b3\r\n2020-01-24 16:32:16.557059: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: bb9f01c415b3\r\n2020-01-24 16:32:16.557145: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 435.21.0\r\n2020-01-24 16:32:16.557160: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 435.21.0\r\n2020-01-24 16:32:16.557164: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 435.21.0\r\n2020-01-24 16:32:16.557353: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-24 16:32:16.574660: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 3892860000 Hz\r\n2020-01-24 16:32:16.575312: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562d071745f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-24 16:32:16.575337: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-24 16:32:18.432 | INFO     | __main__:<module>:114 - loaded model & tokenizer\r\n2020-01-24 16:32:18.443 | INFO     | __main__:<module>:117 - loaded representative dataset\r\n2020-01-24 16:32:19.227068: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-01-24 16:32:19.227222: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-01-24 16:32:19.254757: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize\r\n2020-01-24 16:32:19.254798: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n2020-01-24 16:32:19.254802: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-24 16:32:21.189883: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-01-24 16:32:21.190007: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-01-24 16:32:22.539819: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize\r\n2020-01-24 16:32:22.539870: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 2861 nodes (-184), 3659 edges (-361), time = 906.902ms.\r\n2020-01-24 16:32:22.539875: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 2861 nodes (0), 3659 edges (0), time = 359.704ms.\r\nINFO:absl:Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n2020-01-24 16:35:00.745 | INFO     | __main__:<module>:123 - model conversion finished, writing to file..\r\n2020-01-24 16:35:00.752 | INFO     | __main__:<module>:128 - All Done!\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nSadly, tensorflow fails to save my model. I'm using fine-tuned version of ALBERT-base-v2(for sequence classification) from huggingface\\transformers.\r\n\r\n**Failure details**\r\nConversion went without any failures, but produced model is completely wrong, as it is 18x slower than model produced without calibration and experimental_new_quantizer(but with experimantal_new_converter) and this model produces trashed metrics:\r\n```\r\n# experimental_new_quantizer=True, experimental_new_converter=True and calibration is on\r\n{\"accuracy\": 0.025161092359619514, \"micro-f1\": 0.3419799704482023, \"macro-f1\": 0.17553049991957947}\r\ninference time: 18 sec \\ sample (on x86_64)\r\n# experimantal_new_quantizer=False, experimental_new_converter=True and no calibration\r\n{\"accuracy\": 0.4894139306535747, \"micro-f1\": 0.6913144244564617, \"macro-f1\": 0.5892280589520447}\r\ninference time: 1 sec \\ sample (on x86_64)\r\n```", "comments": ["`experimental_new_quantizer` is not tied to the new MLIR converter feature release, and it is not guaranteed to work at current stage. ", "@Rexhaif,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36183\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36183\">No</a>\n"]}, {"number": 36182, "title": "Broken link \"Share your TensorFlow Lite story\"", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThis button \r\n![image](https://user-images.githubusercontent.com/2697890/73086114-09ba7600-3ee1-11ea-9b27-1f75816b4e94.png)\r\n\r\nlinks to 403 page\r\n![image](https://user-images.githubusercontent.com/2697890/73086137-1a6aec00-3ee1-11ea-9fa1-0ace2995a872.png)", "comments": ["@Dok11 ,\r\nI just tried accessing the[ link](https://services.google.com/fb/forms/tensorflowcasestudy/) worked fine without any issues, can you try again and let us know?Thanks!", "Still 403 (google chrome and firefox)\r\n![image](https://user-images.githubusercontent.com/2697890/73167036-25539580-4108-11ea-8f10-a9f1cd1f057e.png)\r\n", "Lawrence looks like there is some permission error. Can you please take a look\r\n\r\nThanks", "It has been 17 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Trouble still here\r\n\r\n![image](https://user-images.githubusercontent.com/2697890/75065775-8a798b80-54fa-11ea-81c0-3eca3591dc33.png)", "@Dok11 May you clear your cache and / or try a different browser? I've had multiple members try the link and it works. Closing for now because there is no issue. If you want to share your story, email us directly at tflite@tensorflow.org! Thanks."]}, {"number": 36180, "title": "TFLite: add cmake support", "body": "Motivation:\r\nRaspberry Pi4 is gaining OpenGL ES3.1 support with Mesa 19.3. Additionally, many SoC's in the embedded space also supporting 3.1 or OpenCL 2.0. Also, there are desktop GPUs which support OpenCL 2.0. Make it possible to compile the GL & CL delegates easily and separatly (e.g. if just OpenGL ES3.1 is available).\r\n\r\nThis PR adds support for compiling TF Lite with CMake. CMake was choosen, because it's a settled build system, used very often in embedded area and also support by abseil & flatbuffers (which are requirements for GL & CL delegates). It mainly adds a lot of #ifdefs to the code... The PR isn't complete yet, but I quickly want to discuss, if CMake support is desired and if yes, we should deprecate Makefile support.\r\n\r\nTo use it:\r\n1. run download_dependencies.sh in tools/make\r\n2. run build.sh in tools/cmake (this will run cmake twice, first compiling flatc for the host, then TFLite for the target; additional parameters of build.sh are passed to the \"target\"-cmake run)\r\n\r\nMissing stuff: documentation/README.md, don't compile flatc when no delegate is built, etc.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36180) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36180) for more info**.\n\n<!-- ok -->", "@jsee23 can you please resolve conflicts ?", "@rthadur done", "Looks like want wouldn't to add CMake-specific code to our codebase. Our (limited) Makefile support is already a bit tricky to maintain, so I would unfortunately say no to \"adding CMake support\".", "Sorry, this ended up like no PR should look like: I mixed different things in one PR.\r\n1) adding CMake support: IMO, cmake is a better fit than Makefiles, as it's also cleanly supported on Windows hosts. We could deprecate the Makefiles and offer CMake support instead. @srjoglekar246  would you prefer this? Next steps could be e.g. support for a TFLite conan package (would simplify the start with TF Lite for some users...). IMO, we need a good 2nd build system in addition to bazel, as bazel perfectly fit for building Tensorflow, but it isn't widely adopted and people aren't used to it...\r\n2) de-couple the OpenCL & OpenGLES backends with #ifdefs: I saw a lot of embedded SoC's that either just support OpenCL 2.x **OR** OpenGL ES 3.1. With the #ifdef-ing, one could just compile in the GLES shaders (e.g. on a RPi4 with the latest mesa), just the OpenCL shaders (e.g. on a desktop system) or both.\r\n\r\nIf there is a common understanding for point 1, I will create a new PR and separate point 1 & 2. Please also comment if it make sense to follow up point 2.", "@impjdi to comment on selective building of GL/CL. Something like this makes sense to me, and I'm generally supportive, however, I think we might want to take a step back and see if we can restructure the code to more seamlessly allow this, without needing to sprinkle in #ifdefs in many different places.\r\n\r\nAs for CMake support, I'm sympathetic, but I think we'd need a proper engineering discussion before we move ahead with something like that. The only reason we have the Makefiles, currently, is that cross-compilation w/ Bazel can be difficult for certain systems. Ideally, we'd remove the parallel Makefile build system entirely, and rely on Bazel. Is there a reason Bazel doesn't work for your needs?", "Friendly ping on why bazel is insufficient for your needs. Maintaining two separate build systems is often a losing battle, and the TFLite Makefile exists only for legacy reasons.", "@jdduke Sorry, this took way too long :) Build systems are a very emotional thing for many people, I will try to stay on a technical path ;)\r\nI see two major requirements that people are trying out/integrate TFLite: (1) easy (cross) compilation for a embedded target & (2) integration with a package manager.\r\n1) IMO, documentation for cross-compilation in Bazel is limited or the whole process is too complicated? Didn't found any documentation/tutorial e.g. for cross-compilation for RPI. This is easier with just specifying CC, CXX and some flags with make, cmake, etc. Maybe it makes sense to take here the tool which is used the most? (don't have any numbers for CMake...)\r\n2) people start to use package managers even with C/C++ :) I think bazel isn't yet natively supported by Conan, vckpg, etc.\r\nMaybe you can cross-check with your colleagues from Abseil about their experiences? (see https://abseil.io/blog/20190402-cmake-support)", "OK, so leaving the CMake discussion aside, did you want to proceed with the ability to selectively build GL vs CL in the GPU delegate?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@jsee23 Can you please check jdduke's comments and resolve conflicts?. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 36179, "title": "tensorflow 1.15.1 produces V2 style pip with V2 enabled by default", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nMacbook Pro\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version:\r\n1.15\r\n- Python version:\r\n2.7\r\n- Installed using virtualenv? pip? conda?:\r\npip\r\n- Bazel version (if compiling from source):\r\nbazel-0.26.1\r\n- GCC/Compiler version (if compiling from source):\r\nclang\r\n- CUDA/cuDNN version:\r\nNo\r\n- GPU model and memory:\r\nNo\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am building tensorflow 1.15 wheel with MKL support from sources, branch r1.15. After I installed wheel \"pip install tensorflow-1.15.1-cp27-cp27m-macosx_10_14_x86_64.whl\", I see no backward compatibility.\r\n\r\n```\r\nkeras.backend.get_session()\r\nAttributeError: 'module' object has no attribute 'get_session'\r\n```\r\n\r\nI suspect that the following diff https://github.com/tensorflow/tensorflow/commit/e686d9c0bafce7749a7410aa2c0304c30e498f1d disabled V1 style pip.\r\n\r\nAs result package I produces is not behaving as official 1.15 package.\r\n\r\nis it expected?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build --local_ram_resources=2048 --config=mkl -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma //tensorflow/tools/pip_package:build_pip_package\r\n\r\nNote that I don't use --config=v1 because branch itself is 1.15.\r\n\r\nThen build a wheel \r\n\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tf_wheel \r\n\r\nit produces tensorflow-1.15.1-cp27-cp27m-macosx_10_14_x86_64.whl\r\nAfter I install it, v1 code doesn't work and I see that V2 is enabled:\r\n\r\npython\r\nPython 2.7.16 (default, Sep  2 2019, 11:59:44)\r\n[GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow.python import tf2\r\n>>> print(tf2.enabled())\r\nTrue\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\npip show tensorflow\r\nName: tensorflow\r\nVersion: 1.15.1\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /Users/vkuzmin/gocode/src/code.uber.internal/data/michelangelo-deeplearning-inference/env/lib/python2.7/site-packages\r\nRequires: keras-preprocessing, functools32, tensorflow-estimator, wheel, six, keras-applications, astor, tensorboard, wrapt, opt-einsum, termcolor, grpcio, mock, numpy, absl-py, gast, protobuf, backports.weakref, enum34, google-pasta\r\nRequired-by:\r\n\r\n", "comments": ["I guess that I need to checkout:\r\n```\r\ncommit 590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b (tag: v1.15.0)\r\nMerge: 07bf6634f6 b27ac431aa\r\nAuthor: Goldie Gadde <ggadde@google.com>\r\nDate:   Mon Oct 14 14:08:43 2019 -0700\r\n\r\n    Merge pull request #31861 from tensorflow-jenkins/relnotes-1.15.0rc0-16184\r\n\r\n    Update release notes for TensorFlow 1.15.0\r\n```\r\nand build. But it would be great to have 1.15.1 compatible with 1.15.0 too.", "And I think that the whole `contrib` module got removed conveniently!", "See [https://github.com/tensorflow/tensorflow/compare/v1.15.0...v1.15.1#diff-bb845bf0c867fef59b6527c0946799f9](https://github.com/tensorflow/tensorflow/compare/v1.15.0...v1.15.1#diff-bb845bf0c867fef59b6527c0946799f9) and navigate to\r\n* _files changed_\r\n* reload the page or go down to `tensorflow/tools/pip_package/BUILD`", "There are multiple changes here:\r\n\r\n1. First, you need to build with `--config=v1`, even though you are on the `r1.15` branch. Default bazel config is to use `--config=v2` by default\r\n2. We removed `contrib` and can no longer build it\r\n3. We removed support for python 2 as it is dead.", ">And I think that the whole contrib module got removed conveniently!\r\nThere are multiple changes here:\r\n\r\n> First, you need to build with --config=v1, even though you are on the r1.15 branch. Default bazel config is to use --config=v2 by default\r\n> We removed contrib and can no longer build it\r\n>We removed support for python 2 as it is dead.\r\n\r\nI'd expect that 1.15.1 is compatible with 1.15.0 and has only critical fixes./patches\r\n\r\nCan you confirm that this Issue (no v1 style, no compatibility with 1.15.0) is a problem/bug and should be fixed? \r\n\r\nI think it should be a high priority, because I see 1.15.1 was officially released today, and branch has tag 1.15.1. Because if somebody upgrade to 1.15.1 that will happen for sure, with all the changes above - technically 1.15.1 is broken.\r\n", "Please ignore release of today, we removed it and will regenerate.\r\n\r\nWe will make `--config=v1` be the default for the branch and bring this back to be compatible with 1.15.0.", "`r1.15` head should be fixed now. It will take a while until we can produce the pip though", "Can you also push the docker image for that?", "We never pushed a new docker image for patch releases. The image should be the same as for the main release.", "Hm, someone seems to have done it after all: https://hub.docker.com/layers/tensorflow/tensorflow/1.15.2-gpu/images/sha256-da7b6c8a63bdafa77864e7e874664acfe939fdc140cb99940610c34b8c461cd0\r\nThanks to [@angersson](https://hub.docker.com/u/angersson)!", "Yes, I was wrong and we needed an upgrade.\r\n\r\nI think we can close this now.", "Yes thank you :+1: "]}, {"number": 36178, "title": "How to build Tensorflow lite shared library ?", "body": "Hello\r\n\r\nI am looking for a modification of build to be created share \"so\" library for TensorFlow-lite -please can help what/where to modify to create a shared library?\r\n\r\nThank you.", "comments": ["For Ubuntu 18.04:\r\n\r\nsudo apt-get install python3 python3-pip\r\npip3 install numpy protobuf\r\n\r\ncd to tensorflow folder\r\n./configure (for me it only worked when using the path to py3, not py2)\r\n\r\nbazel build //tensorflow/lite:libtensorflowlite.so", "I used to build with build script `./tensorflow/lite/tools/make/build_aarch64_lib.sh` \r\nand as the target, I have an ARM device (aarch64) and build with host machine x86_64, the Ubuntu machine (cross-compile). [https://www.tensorflow.org/lite/guide/build_arm64](url). This should compile a static library in: `tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a`\r\nSo in case, I use Bazel as you indicated how can I specify the target ARM platform and I assume the cross-compilation?", "Never did this myself. You could try adding the following flags to the bazel build:\r\n--cpu=aarch64 --crosstool_top=//tools/aarch64_compiler:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain", "It's been built and I found it `/tensorflow/bazel-tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/`\r\nIn case I want to use a built shared library then I need to copy the following include headers and dynamic shared library to \"target machine\" `/usr/local/lib` and `/usr/local/include`.\r\nIt should or how to generate `bazel-genfiles` folder for headers and where to locate those headers?", "When you have the .so file, you can use CMake or Bazel or Visual Studio or whatever you like.\r\nI'm not a big fan of Bazel and Visual Studio. When using CMake it is as easy as adding this line\r\n`target_link_libraries(<insert project name> <insert location of .so file>)`\r\nto your CMakeLists.txt file.\r\nSee also: https://cmake.org/cmake/help/latest/command/target_link_libraries.html", "Why `bazel-genfiles` is not generated? What to reference tf-lite headers to? SO I'd like to have an application project out of the TensorFlow framework.", "To build for android aarch64 just pass --config=android_arm64 to the bazel build command\r\n\r\nbazel build -c opt --config=android_arm64 tensorflow/lite:libtensorflowlite.so\r\n", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36178\">No</a>\n", "is there a way to have TensorFlow-lite built as a dynamic library? I'm not looking for android solution but rather Linux (ubuntu-flavor) way.", "@peter197321 removing --config=androird_arm64 will build for x86, if you're using linux the binary generated should work on Linux"]}, {"number": 36177, "title": "While compiling `benchmark_model_plus_flex`: error: target 'saved_model_portable_proto' not declared in package ", "body": "- other issues: I found similar issues https://github.com/tensorflow/tensorflow/issues/30226 - with similar error message\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: na\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0, 2.1\r\n- Python version: 3.6.6\r\n- Installed using virtualenv\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): clang, ndk *21*\r\n- CUDA/cuDNN version: na\r\n- GPU model and memory: na\r\n\r\nI want to compile `benchmark_model_plus_flex` for tf v2.1 with ndk 21 but it fails.\r\n\r\nI also try to compile both `benchmark_model` and `benchmark_model_plus_flex` for ndk18 (as it is the last one offically supported) but it fails with `error: undefined reference to 'uselocale'` so I switched to ndk21 where at least `benchmark_model` is building.\r\n \r\nCommand:\r\n```bash\r\nbazel build -c opt   --config=android_arm   --cxxopt='--std=c++11'  \\\r\ntensorflow/lite/tools/benchmark:benchmark_model_plus_flex\r\n```\r\nLOG:\r\n```\r\nWARNING: /home/k.nowicki/code/tensorflow-master/tensorflow/core/kernels/BUILD:6549:12: in srcs attribute of cc_library rule //tensorflow/core/kernels:android_tensorflow_kernels: please do not import '//tenso\r\nrflow/c/kernels:bitcast_op.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/k.nowicki/code/tensorflow-master/tensorflow/core/BUILD:2024:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib: please do not import '//tensorflow/c/kernels:ops/\r\nbitcast.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nERROR: /home/k.nowicki/code/tensorflow-master/tensorflow/cc/saved_model/BUILD:40:1: no such target '//tensorflow/core:saved_model_portable_proto': target 'saved_model_portable_proto' not declared in package \r\n'tensorflow/core' defined by /home/k.nowicki/code/tensorflow-master/tensorflow/core/BUILD and referenced by '//tensorflow/cc/saved_model:reader'\r\nERROR: Analysis of target '//tensorflow/lite/tools/benchmark:benchmark_model_plus_flex' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 7.415s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (113 packages loaded, 10710 targets configured)\r\n```\r\n", "comments": ["There were some issues with ndk r18, we typically recommend r17c with a target API of 21.\r\n\r\nLooks like the dependency on saved_model for flex is a new one, I'll take a look.", "Actually, can you check and see if you can repro the build failure with the latest checkout from master (rather than 2.1)?", "I used code from master and now I have a different error. I have tested it for NDK r17c,r18,r21.\r\n\r\n```\r\nERROR: /home/k.nowicki/code/tensorflow/tensorflow/lite/experimental/delegates/hexagon/BUILD:66:1: C++ compilation of rule '//tensorflow/lite/experimental/delegates/hexagon:hexagon_delegate' failed (Exit 1)\r\ntensorflow/lite/experimental/delegates/hexagon/hexagon_delegate.cc:52:32: error: no member named 'make_unique' in namespace 'std'\r\n    auto hexagon_kernel = std::make_unique<HexagonDelegateKernel>();\r\n                          ~~~~~^\r\ntensorflow/lite/experimental/delegates/hexagon/hexagon_delegate.cc:52:44: error: 'HexagonDelegateKernel' does not refer to a value\r\n    auto hexagon_kernel = std::make_unique<HexagonDelegateKernel>();\r\n                                           ^\r\n./tensorflow/lite/experimental/delegates/hexagon/hexagon_delegate_kernel.h:40:7: note: declared here\r\nclass HexagonDelegateKernel {\r\n      ^\r\ntensorflow/lite/experimental/delegates/hexagon/hexagon_delegate.cc:52:67: error: expected expression\r\n    auto hexagon_kernel = std::make_unique<HexagonDelegateKernel>();\r\n                                                                  ^\r\n3 errors generated.\r\n```\r\n\r\nAdditionally, when I try to `./configure` for clang I have got an error:\r\n```\r\nERROR: Config value \"download_clang\" is not defined in any .rc file\r\n```\r\n\r\n```sh\r\nbazel --host_jvm_args=-Xmx5g  build -c opt   --config=android_arm   --cxxopt='--std=c++11'  tensorflow/lite/tools/benchmark:benchmark_model --jobs=5\r\n```\r\n", "Ah right, TF recently switched to using c++14 instead of c++11, can you update your --cxxopt flags accordingly? Some of the newer code (like Hexagon) does make use of C++14. We're still in the process of updating documentation.", "I pulled new commits and used c++14 flag, now `benchmark_model` compiles without errors but `benchmark_model_plus_flex` does not compile.\r\n\r\n```bash\r\nbazel --host_jvm_args=-Xmx3g  build -c opt   --config=android_arm   --cxxopt='--std=c++14'  tensorflow/lite/tools/benchmark:benchmark_model_plus_flex --jobs=3\r\n```\r\n\r\n```\r\nINFO: Analyzed target //tensorflow/lite/tools/benchmark:benchmark_model_plus_flex (11 packages loaded, 8328 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/k.nowicki/code/tensorflow/tensorflow/cc/saved_model/BUILD:43:1: C++ compilation of rule '//tensorflow/cc/saved_model:reader' failed (Exit 1)\r\nIn file included from tensorflow/cc/saved_model/reader.cc:16:\r\nIn file included from ./tensorflow/cc/saved_model/reader.h:24:\r\nIn file included from ./tensorflow/core/lib/core/status.h:19:\r\nIn file included from ./tensorflow/core/platform/status.h:24:\r\nIn file included from ./tensorflow/core/platform/logging.h:20:\r\nIn file included from ./tensorflow/core/platform/types.h:22:\r\n./tensorflow/core/platform/tstring.h:29:10: fatal error: 'absl/strings/string_view.h' file not found\r\n#include \"absl/strings/string_view.h\"\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\nTarget //tensorflow/lite/tools/benchmark:benchmark_model_plus_flex failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 145.965s, Critical Path: 9.48s\r\nINFO: 254 processes: 254 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "Thanks for flagging, you'll need to add `--config=monolithic` to your build invocation. I'll update the documentation for the `benchmark_model_plus_flex` target to make this clear.", "Updated the documentation, between `--cxxopt='--std=c++14'` and `--config=monolithic` you should be set.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36177\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36177\">No</a>\n"]}, {"number": 36176, "title": "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType).", "body": "Hello, I'm trying to convert my pytorch model to keras and I have ready onnx file for It. When I started to converting onnx to keras, I've got next error:\r\n```\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:Check input 0 (name 645).\r\nDEBUG:onnx2keras:Check input 1 (name 646).\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:mul:Convert inputs to Keras/TF layers if needed.\r\nWARNING:onnx2keras:mul:Failed to use keras.layers.Multiply. Fallback to TF lambda.\r\nWARNING:tensorflow:Layer 647 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\nWARNING:tensorflow:Layer 647 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Cast\r\nDEBUG:onnx2keras:node_name: 648\r\nDEBUG:onnx2keras:node_params: {'to': 7, 'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:Check input 0 (name 647).\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Cast\r\nDEBUG:onnx2keras:node_name: 649\r\nDEBUG:onnx2keras:node_params: {'to': 11, 'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:Check input 0 (name 648).\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Constant\r\nDEBUG:onnx2keras:node_name: 650\r\nDEBUG:onnx2keras:node_params: {'value': array(1.), 'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Div\r\nDEBUG:onnx2keras:node_name: 651\r\nDEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:Check input 0 (name 650).\r\nDEBUG:onnx2keras:Check input 1 (name 649).\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:div:Convert inputs to Keras/TF layers if needed.\r\nWARNING:tensorflow:Layer 651 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\nWARNING:tensorflow:Layer 651 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Constant\r\nDEBUG:onnx2keras:node_name: 652\r\nDEBUG:onnx2keras:node_params: {'value': array(224.), 'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Mul\r\nDEBUG:onnx2keras:node_name: 653\r\nDEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:Check input 0 (name 651).\r\nDEBUG:onnx2keras:Check input 1 (name 652).\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:mul:Convert inputs to Keras/TF layers if needed.\r\nWARNING:onnx2keras:mul:Failed to use keras.layers.Multiply. Fallback to TF lambda.\r\nWARNING:tensorflow:Layer 653 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\nWARNING:tensorflow:Layer 653 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Cast\r\nDEBUG:onnx2keras:node_name: 654\r\nDEBUG:onnx2keras:node_params: {'to': 7, 'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:Check input 0 (name 653).\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Mul\r\nDEBUG:onnx2keras:node_name: 655\r\nDEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:Check input 0 (name 654).\r\nDEBUG:onnx2keras:Check input 1 (name 654).\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:mul:Convert inputs to Keras/TF layers if needed.\r\nWARNING:onnx2keras:mul:Failed to use keras.layers.Multiply. Fallback to TF lambda.\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Unsqueeze\r\nDEBUG:onnx2keras:node_name: 657\r\nDEBUG:onnx2keras:node_params: {'axes': [0], 'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:Check input 0 (name 639).\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:unsqueeze:Work with numpy types.\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Unsqueeze\r\nDEBUG:onnx2keras:node_name: 659\r\nDEBUG:onnx2keras:node_params: {'axes': [0], 'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:Check input 0 (name 655).\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:######\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Converting ONNX operation\r\nDEBUG:onnx2keras:type: Concat\r\nDEBUG:onnx2keras:node_name: 660\r\nDEBUG:onnx2keras:node_params: {'axis': 0, 'change_ordering': False, 'name_policy': None}\r\nDEBUG:onnx2keras:...\r\nDEBUG:onnx2keras:Check if all inputs are available:\r\nDEBUG:onnx2keras:Check input 0 (name 657).\r\nDEBUG:onnx2keras:Check input 1 (name 1057).\r\nDEBUG:onnx2keras:The input not found in layers / model inputs.\r\nDEBUG:onnx2keras:Found in weights, add as a numpy constant.\r\nDEBUG:onnx2keras:Check input 2 (name 659).\r\nDEBUG:onnx2keras:... found all, continue\r\nDEBUG:onnx2keras:concat:Concat Keras layers.\r\nWARNING:onnx2keras:concat:!!! IMPORTANT INFORMATION !!!\r\nWARNING:onnx2keras:concat:Something goes wrong with concat layers. Will use TF fallback.\r\nWARNING:onnx2keras:concat:---\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\onnx2keras\\reshape_layers.py\", line 110, in convert_concat\r\n    name=keras_name)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\merge.py\", line 705, in concatenate\r\n    return Concatenate(axis=axis, **kwargs)(inputs)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 745, in __call__\r\n    inputs = nest.map_structure(_convert_non_tensor, inputs)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 535, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 535, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 743, in _convert_non_tensor\r\n    return ops.convert_to_tensor(x)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1184, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1242, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_conversion_registry.py\", line 52, in _default_conversion_function\r\n    return constant_op.constant(value, dtype, name=name)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 235, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/1/PycharmProjects/untitled/onnx_.py\", line 6, in <module>\r\n    k_model = onnx2keras.onnx_to_keras(model, [\"input_data\"])\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\onnx2keras\\converter.py\", line 177, in onnx_to_keras\r\n    keras_names\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\onnx2keras\\reshape_layers.py\", line 122, in convert_concat\r\n    layers[node_name] = lambda_layer(layer_input)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 745, in __call__\r\n    inputs = nest.map_structure(_convert_non_tensor, inputs)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 535, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 535, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 743, in _convert_non_tensor\r\n    return ops.convert_to_tensor(x)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1184, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1242, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_conversion_registry.py\", line 52, in _default_conversion_function\r\n    return constant_op.constant(value, dtype, name=name)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 235, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType).\r\n```\r\n\r\nThere is my code:\r\n```\r\nimport onnx\r\nimport onnx2keras\r\n\r\nmodel = onnx.load_model(\"model.onnx\")\r\n\r\nk_model = onnx2keras.onnx_to_keras(model, [\"input_data\"])\r\n```\r\n\r\nAnd here is my onnx file to reproduce my errors:\r\nhttps://drive.google.com/file/d/1NjYMbidm5WOB4SeeBw22dGzn-jZIrU2-/view?usp=sharing\r\n\r\nIs there a possibillity to convert onnx to keras using onnxruntime?\r\n\r\nThank you.", "comments": ["@TheArheus Please post this issue [here](https://github.com/nerox8664/onnx2keras/issues)  as we don't look at oonx2keras related issues.\r\n \r\nI am going to close this issue. Thanks!\r\n"]}, {"number": 36175, "title": "Low GPU usage while executing CNN with keras (tf backend)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version (use command below): tensorflow 2.0.0\r\n- Python version: Python 3.7.6\r\n- CUDA/cuDNN version: cuda10.0_0 / cudnn 7.6.5\r\n- GPU model and memory: RTX 2060 - 6GB\r\n\r\n**Describe the current behavior**\r\nWhile executing my conv neural net  I am getting GPU usage of 1%.\r\n\r\n**Describe the expected behavior**\r\nAs no other application is using my resource I should be able to use my GPU at least at 80-85 % \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n    except RuntimeError as e:\r\n        print(e)\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Convolution2D\r\nfrom tensorflow.keras.layers import MaxPooling2D\r\nfrom tensorflow.keras.layers import Flatten\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nclassifier = Sequential()\r\n\r\nclassifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\r\n\r\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\r\n\r\nclassifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\r\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\r\n\r\nclassifier.add(Flatten())\r\n\r\nclassifier.add(Dense(units = 128, activation = 'relu'))\r\nclassifier.add(Dense(units = 1, activation = 'sigmoid'))\r\n\r\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\ntrain_datagen = ImageDataGenerator(rescale = 1./255,\r\n                                   shear_range = 0.2,\r\n                                   zoom_range = 0.2,\r\n                                   horizontal_flip = True)\r\n\r\ntest_datagen = ImageDataGenerator(rescale = 1./255)\r\n\r\ntraining_set = train_datagen.flow_from_directory('dataset/training_set',\r\n                                                 target_size = (64, 64),\r\n                                                 batch_size = 4,\r\n                                                 class_mode = 'binary')\r\n\r\ntest_set = test_datagen.flow_from_directory('dataset/test_set',\r\n                                            target_size = (64, 64),\r\n                                            batch_size = 4,\r\n                                            class_mode = 'binary')\r\n\r\nclassifier.fit_generator(training_set,\r\n                         steps_per_epoch = 8000,\r\n                         epochs = 25,\r\n                         validation_data = test_set,\r\n                         validation_steps = 2000)\r\nGot error after execution of last section :\r\n\r\nclassifier.fit_generator(training_set,\r\n                             steps_per_epoch = 8000,\r\n                             epochs = 25,\r\n                             validation_data = test_set,\r\n                             validation_steps = 2000)\r\n\r\n```\r\n\r\n", "comments": ["This has always been a problem since Tensorflow 2.0 was released.", "@ChetanRajput,\r\nCould you please provide the supporting files you are using. Tried to reproduce the error but was unable to do so, please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/e15d127d27ab2cfea9bedd1b67e8c34d/36175.ipynb). Thanks!", "@ChetanRajput,\r\nAny updates regarding the supporting files required to reproduce this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36174, "title": "UnknownError: Failed to get convolution algorithm.", "body": "**System information**\r\n\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow version: tensorflow 2.0.0\r\n- Python version: Python 3.7.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- CUDA/cuDNN version: cuda10.0_0 / cudnn 7.6.5 \r\n- GPU model and memory: RTX 2060 - 6GB\r\n\r\n\r\n\r\n**Description of problem**\r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]\r\n\r\n**Exact sequence of commands / steps that are executed before running into the problem**\r\n\r\n**code :**\r\n\r\n```\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Convolution2D\r\nfrom tensorflow.keras.layers import MaxPooling2D\r\nfrom tensorflow.keras.layers import Flatten\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nclassifier = Sequential()\r\n\r\nclassifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\r\n\r\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\r\n\r\nclassifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\r\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\r\n\r\nclassifier.add(Flatten())\r\n\r\nclassifier.add(Dense(units = 128, activation = 'relu'))\r\nclassifier.add(Dense(units = 1, activation = 'sigmoid'))\r\n\r\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\ntrain_datagen = ImageDataGenerator(rescale = 1./255,\r\n                                   shear_range = 0.2,\r\n                                   zoom_range = 0.2,\r\n                                   horizontal_flip = True)\r\n\r\ntest_datagen = ImageDataGenerator(rescale = 1./255)\r\n\r\ntraining_set = train_datagen.flow_from_directory('dataset/training_set',\r\n                                                 target_size = (64, 64),\r\n                                                 batch_size = 4,\r\n                                                 class_mode = 'binary')\r\n\r\ntest_set = test_datagen.flow_from_directory('dataset/test_set',\r\n                                            target_size = (64, 64),\r\n                                            batch_size = 4,\r\n                                            class_mode = 'binary')\r\n\r\nclassifier.fit_generator(training_set,\r\n                         steps_per_epoch = 8000,\r\n                         epochs = 25,\r\n                         validation_data = test_set,\r\n                         validation_steps = 2000)\r\n```\r\n \r\n**Got error after execution of last section :** \r\n\r\n```\r\nclassifier.fit_generator(training_set,\r\n                             steps_per_epoch = 8000,\r\n                             epochs = 25,\r\n                             validation_data = test_set,\r\n                             validation_steps = 2000)\r\n```\r\n\r\n**Traceback**\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"D:\\Machine Learning\\Machine Learning A-Z Template Folder\\Part 8 - Deep Learning\\Section 40 - Convolutional Neural Networks (CNN)\\cnn.py\", line 70, in <module>\r\n    validation_steps = 2000)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1297, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\", line 265, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 973, in train_on_batch\r\n    class_weight=class_weight, reset_metrics=reset_metrics)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 264, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 311, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 252, in _process_single_batch\r\n    training=training))\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 127, in _model_loss\r\n    outs = model(inputs, **kwargs)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 256, in call\r\n    return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 708, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 860, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\convolutional.py\", line 197, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\", line 1134, in __call__\r\n    return self.conv_op(inp, filter)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\", line 639, in __call__\r\n    return self.call(inp, filter)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\", line 238, in __call__\r\n    name=self.name)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\", line 2010, in conv2d\r\n    name=name)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\", line 1031, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\", line 1130, in conv2d_eager_fallback\r\n    ctx=_ctx, name=name)\r\n\r\n  File \"C:\\Anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]\r\n\r\n```\r\n\r\n**Note :** \r\n`tf.test.is_gpu_available()` returns `True`", "comments": ["As per best of my knowledge versions are compatible so please have a thorough idea before mentioning the issue of incompatibility of versions.", "**Below code solved the issue :**\r\n\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n    except RuntimeError as e:\r\n        print(e)"]}, {"number": 36173, "title": "Fixed lower case dataset to Dataset", "body": "line 148\r\ndataset = tf.data.dataset.list_files\r\nto\r\ndataset = tf.data.Dataset.list_files", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36173) for more info**.\n\n<!-- need_sender_cla -->", "Unfortunately this is for a release branch and we don't accept non-critical PRs to such branches.\r\n\r\nPlease reopen against master"]}, {"number": 36172, "title": "AttributeError: module 'dataset_utils' has no attribute 'int64_list_feature'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nno, it is code pulled off a website\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 x64\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command 2.1.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 960m 4gb\r\n\r\n**Describe the current behavior**\r\nI am running this script that is below and I get this error, anyone have any help??\r\n\r\n**Describe the expected behavior**\r\nThe script to run\r\n\r\n**Code to reproduce the issue**\r\nI run python then that location of the script\r\n\r\n**Other info / logs**\r\n2020-01-23 22:14:41.289298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\minec\\Desktop\\Robotics_stuff\\TFconvert.py\", line 90, in <module>\r\n    xml_to_tf(PATH_TEST, PATH_RECORD_TEST)\r\n  File \"C:\\Users\\minec\\Desktop\\Robotics_stuff\\TFconvert.py\", line 82, in xml_to_tf\r\n    'image/object/class/label': dataset_util.int64_list_feature(classes),\r\nAttributeError: module 'dataset_utils' has no attribute 'int64_list_feature'\r\n\r\n___________________________________________________________________________________________________________\r\n# Importing all necessary libraries\r\nimport os\r\nimport sys\r\nimport xml.etree.ElementTree as ET\r\nimport tensorflow as tf\r\nsys.path.append(r\"C:\\Python37\\models\\models-master\\research\\slim\\datasets\"); \r\nimport dataset_utils as dataset_util\r\n\r\n# This are the path to the datasets and to the output files.\r\n# NEED TO BE UPDATED IN CASE THE DATASET CHANGES\r\nPATH_TEST = \"C:/Users/minec/Desktop/Robotics_stuff/FIRSTballDatabase/test/\"\r\nPATH_RECORD_TEST = \"test2.record\"\r\nPATH_TRAIN = \"C:/Users/minec/Desktop/Robotics_stuff/FIRSTballDatabase/train/\"\r\nPATH_RECORD_TRAIN = \"train2.record\"\r\n\r\nIMAGE_EXT = \".jpg\"\r\nIMAGE_FORMAT = b'jpg'\r\n\r\n# This function defines the different classes the dataset has and return a different number per each.\r\n# NEED TO BE UPDATED IN CASE THE DATASET CHANGES\r\ndef class_text_to_int(row_label):\r\n    if row_label == 'FIRSTball':\r\n        return 1\r\n    else:\r\n        return 0\r\n\r\n# Reads the xml and the images, and create the tf records files. \r\ndef xml_to_tf(path_input, path_output):\r\n    xml_list = []\r\n    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\r\n\r\n    writer = tf.io.TFRecordWriter(path_output)\r\n\r\n    files = os.listdir(path_input)\r\n    for file in files:\r\n        if file.endswith(\".xml\"):\r\n            xmlFile = path_input + file\r\n\r\n            tree = ET.parse(xmlFile)\r\n            root = tree.getroot()\r\n            \r\n            filename = root[1].text + IMAGE_EXT\r\n            width = int(root[4][0].text)\r\n            height = int(root[4][1].text)\r\n\r\n            xmins = []\r\n            xmaxs = []\r\n            ymins = []\r\n            ymaxs = []\r\n            classes_text = []\r\n            classes = []\r\n            \r\n\r\n            for member in root.findall('object'):\r\n                beer = member[0].text\r\n                xmin = int(member[4][0].text)\r\n                ymin = int(member[4][1].text)\r\n                xmax = int(member[4][2].text)\r\n                ymax = int(member[4][3].text)\r\n                \r\n                xmins.append(xmin/width)\r\n                xmaxs.append(xmax/width)\r\n                ymins.append(ymin/height)\r\n                ymaxs.append(ymax/height)\r\n                classes_text.append(beer.encode('utf8'))\r\n                classes.append(class_text_to_int(beer))\r\n\r\n            with tf.io.gfile.GFile(os.path.join(path_input, '{}'.format(filename)), 'rb') as fid:\r\n                encoded_jpg = fid.read()\r\n            tf_example = tf.train.Example(features=tf.train.Features(feature={\r\n                'image/height': dataset_util.int64_feature(height),\r\n                'image/width': dataset_util.int64_feature(width),\r\n                'image/filename': dataset_util.bytes_feature(filename.encode('utf8')),\r\n                'image/source_id': dataset_util.bytes_feature(filename.encode('utf8')),\r\n                'image/encoded': dataset_util.bytes_feature(encoded_jpg),\r\n                'image/format': dataset_util.bytes_feature(IMAGE_FORMAT),\r\n                'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\r\n                'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\r\n                'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\r\n                'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\r\n                'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\r\n                'image/object/class/label': dataset_util.int64_list_feature(classes),\r\n            }))\r\n            \r\n            writer.write(tf_example.SerializeToString())\r\n    writer.close()             \r\n    output_path = os.path.join(os.getcwd(), path_output)\r\n    print('Successfully created the TFRecords: {}'.format(output_path))\r\n\r\nxml_to_tf(PATH_TEST, PATH_RECORD_TEST)\r\nxml_to_tf(PATH_TRAIN, PATH_RECORD_TRAIN)\r\n___________________________________________________________________________________________________________\r\nIf anyone is able to help thanks!!", "comments": ["@Brayden1000 \r\nCan you please share colab link or simple standalone code along with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@Brayden1000 \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36171, "title": "[core] Sparse tensor valid index check", "body": "", "comments": ["Thanks for submitting a pull request to TensorFlow. Unfortunately, I don't think it improves the code in question, so I don't think it should be merged: the comment is redundant, and the logical operator change introduces an inconsistency where previously all terms were combined using the same operator."]}, {"number": 36170, "title": "Build fails with messages including \"undefined reference to symbol 'acos@@GLIBC_2.2.5' \", \"Linking of rule '@flatbuffers//:flatc' failed\"", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Attempting to build from source\r\n- TensorFlow version: 2.1.0 (attempting to build from master)\r\n- Python version: 3.8.0\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5\r\n- GPU model and memory: NVidia RTX 2080 Ti\r\n\r\n**Describe the problem**\r\n\r\nI am trying to build for python 3.8.0 since issues relating to python 3.8.0 should now be fixed (see #34433 ).  There is also no pip package that you can install with \"pip install tensorflow\" for python 3.8.0.\r\n\r\nBuild fails with the following messages:\r\n\r\n```\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (345 packages loaded, 27926 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base /home/daniel/.cache/bazel/_bazel_daniel/79db702fc9f94af7d11e11c5d64854d0/sandbox\r\nINFO: From Compiling tensorflow/core/platform/default/logging.cc [for host]:\r\ntensorflow/core/platform/default/logging.cc: In member function \u2018bool tensorflow::internal::LogFirstNState::ShouldLog(int)\u2019:\r\ntensorflow/core/platform/default/logging.cc:369:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (counter_value < n) {\r\n       ~~~~~~~~~~~~~~^~~\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc [for host]:\r\ncc1plus: warning: command line option \u2018-Wno-implicit-function-declaration\u2019 is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:\r\ncc1plus: warning: command line option \u2018-Wno-implicit-function-declaration\u2019 is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy.cc [for host]:\r\ncc1plus: warning: command line option \u2018-Wno-implicit-function-declaration\u2019 is valid for C/ObjC but not for C++\r\nINFO: From Compiling tensorflow/core/platform/protobuf.cc [for host]:\r\ntensorflow/core/platform/protobuf.cc: In member function \u2018virtual bool tensorflow::TStringOutputStream::Next(void**, int*)\u2019:\r\ntensorflow/core/platform/protobuf.cc:30:16: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (old_size < target_->capacity()) {\r\n       ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/platform/numbers.cc [for host]:\r\ntensorflow/core/platform/numbers.cc: In instantiation of \u2018T tensorflow::{anonymous}::locale_independent_strtonum(const char*, const char**) [with T = double]\u2019:\r\ntensorflow/core/platform/numbers.cc:197:60:   required from here\r\ntensorflow/core/platform/numbers.cc:65:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < special_num_str.length(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/platform/numbers.cc: In function \u2018std::__cxx11::string tensorflow::strings::HumanReadableNumBytes(tensorflow::int64)\u2019:\r\ntensorflow/core/platform/numbers.cc:459:8: warning: \u2018%lld\u2019 directive output may be truncated writing between 1 and 19 bytes into a region of size between 7 and 8 [-Wformat-truncation=]\r\n string HumanReadableNumBytes(int64 num_bytes) {\r\n        ^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/platform/numbers.cc:459:8: note: directive argument in the range [0, 9223372036854775807]\r\nIn file included from /usr/include/stdio.h:862:0,\r\n                 from /usr/include/c++/7/cstdio:42,\r\n                 from /usr/include/c++/7/ext/string_conversions.h:43,\r\n                 from /usr/include/c++/7/bits/basic_string.h:6361,\r\n                 from /usr/include/c++/7/string:52,\r\n                 from ./tensorflow/core/platform/numbers.h:19,\r\n                 from tensorflow/core/platform/numbers.cc:15:\r\n/usr/include/x86_64-linux-gnu/bits/stdio2.h:65:44: note: \u2018__builtin_snprintf\u2019 output between 3 and 22 bytes into a destination of size 8\r\n        __bos (__s), __fmt, __va_arg_pack ());\r\n                                            ^\r\nINFO: From Compiling external/llvm-project/llvm/lib/Support/APFloat.cpp [for host]:\r\nexternal/llvm-project/llvm/lib/Support/APFloat.cpp: In member function \u2018llvm::Expected<llvm::APFloatBase::opStatus> llvm::detail::IEEEFloat::convertFromDecimalString(llvm::StringRef, llvm::APFloatBase::roundingMode)\u2019:\r\nexternal/llvm-project/llvm/lib/Support/APFloat.cpp:2625:38: warning: \u2018D.llvm::decimalInfo::exponent\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     fs = roundSignificandWithExponent(decSignificand, partCount,\r\n          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                                       D.exponent, rounding_mode);\r\n                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/llvm-project/llvm/lib/Support/APFloat.cpp:2561:36: warning: \u2018D.llvm::decimalInfo::normalizedExponent\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n              (D.normalizedExponent + 1) * 28738 <=\r\n              ~~~~~~~~~~~~~~~~~~~~~~^~~~\r\nexternal/llvm-project/llvm/lib/Support/APFloat.cpp:2622:16: warning: \u2018D.llvm::decimalInfo::lastSigDigit\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     } while (p <= D.lastSigDigit);\r\n              ~~^~~~~~~~~~~~~~~~~\r\nexternal/llvm-project/llvm/lib/Support/APFloat.cpp:2581:58: warning: \u2018D.llvm::decimalInfo::firstSigDigit\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     partCount = static_cast<unsigned int>(D.lastSigDigit - D.firstSigDigit) + 1;\r\n                                           ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\r\nINFO: From Compiling external/llvm-project/llvm/lib/Support/UnicodeCaseFold.cpp [for host]:\r\nexternal/llvm-project/llvm/lib/Support/UnicodeCaseFold.cpp:8:1: warning: multi-line comment [-Wcomment]\r\n //   utils/unicode-case-fold.py \\\r\n ^\r\nINFO: From Compiling external/llvm-project/llvm/lib/Support/VirtualFileSystem.cpp [for host]:\r\nexternal/llvm-project/llvm/lib/Support/VirtualFileSystem.cpp: In member function \u2018std::unique_ptr<llvm::vfs::RedirectingFileSystem::Entry> llvm::vfs::RedirectingFileSystemParser::parseEntry(llvm::yaml::Node*, llvm::vfs::RedirectingFileSystem*, bool)\u2019:\r\nexternal/llvm-project/llvm/lib/Support/VirtualFileSystem.cpp:1471:5: warning: \u2018Kind\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     switch (Kind) {\r\n     ^~~~~~\r\nINFO: From Compiling external/llvm-project/llvm/lib/MC/ELFObjectWriter.cpp [for host]:\r\nexternal/llvm-project/llvm/lib/MC/ELFObjectWriter.cpp: In function \u2018uint64_t {anonymous}::ELFWriter::writeObject(llvm::MCAssembler&, const llvm::MCAsmLayout&)\u2019:\r\nexternal/llvm-project/llvm/lib/MC/ELFObjectWriter.cpp:1193:36: warning: \u2018AddrsigSection\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n       SectionOffsets[AddrsigSection] = std::make_pair(SecStart, SecEnd);\r\n                                    ^\r\nINFO: From Compiling external/llvm-project/llvm/lib/MC/MachObjectWriter.cpp [for host]:\r\nexternal/llvm-project/llvm/lib/MC/MachObjectWriter.cpp: In member function \u2018void llvm::MachObjectWriter::writeNlist(llvm::MachObjectWriter::MachSymbolData&, const llvm::MCAsmLayout&)\u2019:\r\nexternal/llvm-project/llvm/lib/MC/MachObjectWriter.cpp:381:13: warning: \u2018AliaseeInfo\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     Address = AliaseeInfo->StringIndex;\r\n     ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nERROR: /home/daniel/.cache/bazel/_bazel_daniel/79db702fc9f94af7d11e11c5d64854d0/external/flatbuffers/BUILD.bazel:51:1: Linking of rule '@flatbuffers//:flatc' failed (Exit 1)\r\n/usr/bin/ld: bazel-out/host/bin/external/flatbuffers/src/libflatbuffers.a(idl_parser.o): undefined reference to symbol 'acos@@GLIBC_2.2.5'\r\n//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/daniel/tensorflow/tensorflow/python/tools/BUILD:141:1 Linking of rule '@flatbuffers//:flatc' failed (Exit 1)\r\nINFO: Elapsed time: 103.160s, Critical Path: 25.44s\r\nINFO: 1225 processes: 1225 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n./configure\r\nYou have bazel 1.2.1 installed.\r\nPlease specify the location of python. [Default is /home/daniel/tf38/bin/python]: Default accepted\r\nPython library path to use.  Default is [/home/daniel/tf38/lib/python3.8/site-packages]: Default accepted\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: Y\r\nXLA JIT support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\nFound TensorRT 6 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include/x86_64-linux-gnu\r\nCUDA compute capabilities [Default is: 3.5,7.0]: 7.5\r\nDo you want to use clang as CUDA compiler? [y/N]: N\r\nnvcc will be used as CUDA compiler.\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: Default accepted\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: Default accepted\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\nConfiguration finished.\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nBuild fails with the above messages.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI have noticed in the past that builds can sometimes build with different bazel versions.  I tried bazel 0.26.1, 1.0.0 and 2.0.0.  However the build still fails with all these bazel versions.", "comments": ["After googling of this error, it sounds like there is a missing '-lm' flag.  However, I don't know how to incorporate this flag in to  the source.", "I was able to get the build to continue by doing the following:\r\n```\r\nnano third_party/flatbuffers/BUILD.bazel\r\n```\r\nAfter the line \"linkstatic = 1\", add this line:\r\n```\r\n    linkopts = [\"-lm\",\"-lpthread\"],\r\n```\r\n@Saduf2019 please let me know if this is the right thing to do.\r\nAll the best,\r\nDan", "@dbonner this is not an issue for 2.1. Try build from master a couple of days ago.\r\n\r\nPing @petewarden; seems the CL adf6e22e4af83afd55e0da3caa7e7959def1e6b6 breaks Linux builds. \r\n\r\nSpecifying `--linkopt=-lm` in `bazel build` does not fix this issue.", "Gently ping @smit-hinsu as well. \r\n\r\nSeems adf6e22e4af83afd55e0da3caa7e7959def1e6b6 breaks MacOS builds as well and got a band-aid fix with db7795727c1025759aa8c584ac433f6a010f7dcb.", "@byronyi do you mean that this problem\r\n\r\n- **is** an issue for the current head because adf6e22e4af83afd55e0da3caa7e7959def1e6b6 broke linux builds\r\n- and this was **not** an issue in the master from a couple of days ago before adf6e22e4af83afd55e0da3caa7e7959def1e6b6?", "@dbonner that is correct. Technically you need to revert both commits I mentioned above to build the current master.", "`--host_linkopt=-lm` should work.", "Observed the similar TensorFlow building failure (Ubuntu 16.04) starting from https://github.com/tensorflow/tensorflow/commit/adf6e22e4af83afd55e0da3caa7e7959def1e6b6\r\n\r\nUPD: `bazel build --host_linkopt=-lm ...` helps as a W/A.\r\n\r\nCC @petewarden", "the workaround mentioned above doesn't work at my side.\r\n\r\nfor those who are still struggling with this problem and using centos-7 with \"scl enable devtoolset-7\", I figured out you can workaournd this issue by adding \"BAZEL_LINKLIBS=-l%:libstdc++.a\" before \"bazel build\". you can have a try if other workaround doesn't work\r\n\r\n", "Is this issue fixed? building still leads to the following error\r\n\r\nLinking CXX executable ../../../../bin/intel64/Release/hello_classification\r\n/opt/intel/openvino_2020.1.023/opencv/lib/libopencv_imgproc.so.4.2.0: undefined reference to `expf@GLIBC_2.27'\r\n/opt/intel/openvino_2020.1.023/opencv/lib/libopencv_core.so.4.2.0: undefined reference to `logf@GLIBC_2.27'\r\ncollect2: error: ld returned 1 exit status\r\ninference-engine/ie_bridges/c/samples/object_detection_sample_ssd/CMakeFiles/object_detection_sample_ssd_c.dir/build.make:111: recipe for target '../bin/intel64/Release/object_detection_sample_ssd_c' failed\r\nmake[2]: *** [../bin/intel64/Release/object_detection_sample_ssd_c] Error 1\r\nCMakeFiles/Makefile2:6449: recipe for target 'inference-engine/ie_bridges/c/samples/object_detection_sample_ssd/CMakeFiles/object_detection_sample_ssd_c.dir/all' failed\r\nmake[1]: *** [inference-engine/ie_bridges/c/samples/object_detection_sample_ssd/CMakeFiles/object_detection_sample_ssd_c.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n/opt/intel/openvino_2020.1.023/opencv/lib/libopencv_imgproc.so.4.2.0: undefined reference to `expf@GLIBC_2.27'\r\n/opt/intel/openvino_2020.1.023/opencv/lib/libopencv_core.so.4.2.0: undefined reference to `logf@GLIBC_2.27'\r\ncollect2: error: ld returned 1 exit status\r\ninference-engine/samples/benchmark_app/CMakeFiles/benchmark_app.dir/build.make:156: recipe for target '../bin/intel64/Release/benchmark_app' failed\r\nmake[2]: *** [../bin/intel64/Release/benchmark_app] Error 1\r\nCMakeFiles/Makefile2:5622: recipe for target 'inference-engine/samples/benchmark_app/CMakeFiles/benchmark_app.dir/all' failed\r\nmake[1]: *** [inference-engine/samples/benchmark_app/CMakeFiles/benchmark_app.dir/all] Error 2\r\n/opt/intel/openvino_2020.1.023/opencv/lib/libopencv_imgproc.so.4.2: undefined reference to `expf@GLIBC_2.27'\r\n/opt/intel/openvino_2020.1.023/opencv/lib/libopencv_core.so.4.2: undefined reference to `logf@GLIBC_2.27'\r\ncollect2: error: ld returned 1 exit status\r\ninference-engine/samples/classification_sample_async/CMakeFiles/classification_sample_async.dir/build.make:108: recipe for target '../bin/intel64/Release/classification_sample_async' failed\r\nmake[2]: *** [../bin/intel64/Release/classification_sample_async] Error 1\r\nCMakeFiles/Makefile2:5680: recipe for target 'inference-engine/samples/classification_sample_async/CMakeFiles/classification_sample_async.dir/all' failed\r\nmake[1]: *** [inference-engine/samples/classification_sample_async/CMakeFiles/classification_sample_async.dir/all] Error 2\r\n/opt/intel/openvino_2020.1.023/opencv/lib/libopencv_imgproc.so.4.2.0: undefined reference to `expf@GLIBC_2.27'\r\n/opt/intel/openvino_2020.1.023/opencv/lib/libopencv_core.so.4.2.0: undefined reference to `logf@GLIBC_2.27'\r\ncollect2: error: ld returned 1 exit status\r\ninference-engine/samples/hello_classification/CMakeFiles/hello_classification.dir/build.make:110: recipe for target '../bin/intel64/Release/hello_classification' failed\r\nmake[2]: *** [../bin/intel64/Release/hello_classification] Error 1\r\nCMakeFiles/Makefile2:5764: recipe for target 'inference-engine/samples/hello_classification/CMakeFiles/hello_classification.dir/all' failed\r\nmake[1]: *** [inference-engine/samples/hello_classification/CMakeFiles/hello_classification.dir/all] Error 2\r\nMakefile:181: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n", "@dbonner,\r\n\r\nWe are checking to see if you still need help on this issue. Can you try building the latest stable version of TF i.e  `2.6.0` using this [guide](https://www.tensorflow.org/install/source) and let us know if the issue still persists in newer versions.Thanks!", "It's resolved. thanks\n\nOn Mon, 20 Sep 2021, 11:46 pm Sanat, ***@***.***> wrote:\n\n> @dbonner <https://github.com/dbonner>,\n>\n> We are checking to see if you still need help on this issue. Can you try\n> building the latest stable version of TF i.e 2.6.0 using this guide\n> <https://www.tensorflow.org/install/source> and let us know if the issue\n> still persists in newer versions.Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36170#issuecomment-922943348>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAB26QXOFFUDI4YOJK3VFM3UC43M3ANCNFSM4KLABISA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "@dbonner,\r\n\r\nThank you for your update, kindly move this issue to closed status as it is resolved.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36170\">No</a>\n"]}, {"number": 36169, "title": "Can't run TensorFlow Model Benchmark Tool", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nLinux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nCouldn't run on mobile\r\n- TensorFlow installed from (source or binary): \r\nsource\r\n- TensorFlow version:\r\nVersion: 1.14.0\r\n- Python version:\r\n2.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): \r\n1.2.1\r\n- GCC/Compiler version (if compiling from source):\r\n7.4.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI followed the tutorial in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark#tensorflow-model-benchmark-tool , but couldn't get past the (0) step.\r\nRan ./configure and set ndk to 14 and then tried to run\r\n`bazel build --cxxopt='--std=c++11' -c opt //tensorflow/examples/android:tensorflow_demo`\r\nAlso tried with \r\n`bazel build -c opt \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --cpu=armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  --config monolithic \\\r\n  tensorflow/tools/benchmark:benchmark_model`\r\n\r\nAlso tried with ndk 20 and no luck. log files for 1st and 2nd run are attached\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n2 files attached\r\n\r\n\r\n[log_1st_run.txt](https://github.com/tensorflow/tensorflow/files/4106252/log_1st_run.txt)\r\n[log_2nd_run.txt](https://github.com/tensorflow/tensorflow/files/4106253/log_2nd_run.txt)", "comments": ["@ffriande Could you please let us know what Tensorflow version are you facing the issue on.", "> @ffriande Could you please let us know what Tensorflow version are you facing the issue on.\r\n\r\nI've added it to the issue. It's Version: 1.14.0\r\n", "@ffriande are you able to see this on 1.15? Unfortunately I don't think we are taking any patch releases to 1.14 at the moment, but I'll see what I can do on just committing to the branch. ", "@ffriande Could you please confirm if the above comment helps resolve the issue.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36169\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36169\">No</a>\n"]}, {"number": 36168, "title": "@tf.custom_gradient does not behave as expected when used with @tf.function (for 2nd order derivatives)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-dev20200123 (observed in other versions)\r\n- **Python version**: 3.7.6\r\n\r\n\r\n\r\n- **CUDA/cuDNN version**: These are examples of versions though this has been replicated in other versions of cuda/cudnn\r\n```\r\nCUDA Version 10.1.243\r\n\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 6\r\n#define CUDNN_PATCHLEVEL 4\r\n--\r\n#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n\r\n\r\n- **GPU model and memory**: \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro P3200        On   | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   53C    P5     9W /  N/A |   5949MiB /  6078MiB |     10%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\n@tf.function\r\n@tf.custom_gradient\r\ndef _fn2(a, logdet_a):\r\n\r\n    def sec_order_grad(a_dash): # bug still appears if this fn outside custom grad call\r\n        return a_dash * 5., tf.ones_like(logdet_a) * 2.\r\n    \r\n    return a, sec_order_grad\r\n\r\n@tf.function\r\n@tf.custom_gradient\r\ndef fwd(a):\r\n    sign_a, logdet_a = tf.linalg.slogdet(a)\r\n\r\n#     logdet_a = tf.stop_gradient(logdet_a) # include to function\r\n    def grad(dy):\r\n        da = _fn2(a, logdet_a)\r\n        return dy * da\r\n\r\n    return 1., grad\r\n\r\nndim = 2\r\n\r\ndef call_gradient_1st():\r\n    a = tf.zeros((ndim, ndim))\r\n    with tf.GradientTape() as g:\r\n        g.watch(a)\r\n        y = fwd(a)\r\n    z = g.gradient(y, a)\r\n    return y, z\r\n\r\n\r\ndef call_gradient_2nd():\r\n    a = tf.zeros((ndim, ndim))\r\n    with tf.GradientTape() as g:\r\n        g.watch(a)\r\n        with tf.GradientTape() as gg:\r\n            gg.watch(a)\r\n            y = fwd(a)\r\n        z = gg.gradient(y, a)\r\n    gg = g.gradient(z, a)\r\n    return y, z, gg\r\n\r\ny, z = call_gradient_1st()\r\nprint(y, z)\r\ny, z, gg = call_gradient_2nd()\r\nprint(y, z, gg)\r\n```\r\n\r\n### Describe the problem\r\n\r\nAutograd attempts to call the gradient of a variable inside a custom gradient function. If the gradient is not computable (i.e. in this case when logdet has inf values) then it crashes. \r\n\r\nThis only appears in the second order (see trace: first order calls, second order doesn't).\r\n\r\nThis doesn't appear if we stop the gradient (see note in function 'fwd').\r\n\r\nThis doesn't appear if we *do not* use the @tf.function wrapper.\r\n\r\n### Source code / logs\r\n```\r\n2.2.0-dev20200123\r\ntf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(\r\n[[0. 0.]\r\n [0. 0.]], shape=(2, 2), dtype=float32)\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-6-af7d2f010d71> in <module>\r\n     48 y, z = call_gradient_1st()\r\n     49 print(y, z)\r\n---> 50 y, z, gg = call_gradient_2nd()\r\n     51 print(y, z, gg)\r\n\r\n<ipython-input-6-af7d2f010d71> in call_gradient_2nd()\r\n     42             gg.watch(a)\r\n     43             y = fwd(a)\r\n---> 44         z = gg.gradient(y, a)\r\n     45     gg = g.gradient(z, a)\r\n     46     return y, z, gg\r\n\r\n~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n   1032         output_gradients=output_gradients,\r\n   1033         sources_raw=flat_sources_raw,\r\n-> 1034         unconnected_gradients=unconnected_gradients)\r\n   1035 \r\n   1036     if not self._persistent:\r\n\r\n~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     75       output_gradients,\r\n     76       sources_raw,\r\n---> 77       compat.as_str(unconnected_gradients.value))\r\n\r\n~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _backward_function_wrapper(*args)\r\n   1303           break\r\n   1304       return backward._call_flat(  # pylint: disable=protected-access\r\n-> 1305           processed_args, remapped_captures)\r\n   1306 \r\n   1307     return _backward_function_wrapper, recorded_outputs\r\n\r\n~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1748       flat_outputs = forward_function.call(\r\n   1749           ctx, args_with_tangents,\r\n-> 1750           cancellation_manager=cancellation_manager)\r\n   1751     else:\r\n   1752       with ops.get_default_graph()._override_gradient_function(  # pylint: disable=protected-access\r\n\r\n~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    596               inputs=args,\r\n    597               attrs=attrs,\r\n--> 598               ctx=ctx)\r\n    599         else:\r\n    600           outputs = execute.execute_with_cancellation(\r\n\r\n~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Input is not invertible.\r\n\t [[node gradients/LogMatrixDeterminant_grad/MatrixInverse (defined at <ipython-input-6-af7d2f010d71>:43) ]]\r\n\t [[gradients/grad_ys_0/_4]]\r\n  (1) Invalid argument:  Input is not invertible.\r\n\t [[node gradients/LogMatrixDeterminant_grad/MatrixInverse (defined at <ipython-input-6-af7d2f010d71>:43) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__forward___backward_fwd_655_746]\r\n\r\nFunction call stack:\r\n__backward_fwd_655 -> __backward_fwd_655\r\n```\r\n\r\nThanks for any insight!\r\n", "comments": ["Issue is replicating on colab with Tf  2.2.0-dev20200123.\r\nPlease find the [gist here](https://colab.research.google.com/gist/gadagashwini/a80d6d940295d12a296ab434e9791371/untitled23.ipynb). Thanks!", "@saxenasaurabh could you triage?", "@allenlavoie do you have any ideas?", "I commented out the two `@tf.function` lines and I see the same \"Input not invertible\" error (from MatrixInverse via LogMatrixDeterminantGrad). Adding back the stop_gradient does avoid the error in either case. This is with 2.2.0-dev20200128.\r\n\r\nIt's totally possible that tf.function changes the gradient graphs for higher-order gradients in some cases. They're still correct (assuming the original registrations are correct), but there are cases where we could/should do extra pruning or avoid building some extra graphs in the first place. I don't see evidence of that issue from this repro, though.\r\n\r\nIs the argument that the stop_gradient should be implicit here? ", "My intuition would be that stop_gradient is implicit when functions are called within tf.custom_gradient", "> Issue is replicating on colab with Tf 2.2.0-dev20200123.\r\n> Please find the [gist here](https://colab.research.google.com/gist/gadagashwini/a80d6d940295d12a296ab434e9791371/untitled23.ipynb). Thanks!\r\n\r\nDid u find the fix to the problem", "@rajatkhanna1999 Please provide a standalone code to reproduce the issue. Thanks!", "I don't think we want tf.function to change gradient behavior. Please do insert stop_gradients as needed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36168\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36168\">No</a>\n"]}, {"number": 36167, "title": "Help! I have an issue when importing TF!", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nno, it is custom code but I did not write it https://medium.com/swlh/nvidia-jetson-nano-custom-object-detection-from-scratch-using-tensorflow-and-opencv-113fe4dba134 I got it from here and the TF conversion script\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- TensorFlow installed from (source or binary): I installed with pip install Tensorflow and Tensorflow-gpu\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 960M 4gb\r\n\r\n**Describe the current behavior**\r\nWhen I run the script I get the error Failed to load the native TensorFlow runtime. with a bunch of other errors that I will paste down bellow.\r\n\r\n**Describe the expected behavior**\r\nTo run the script lol\r\n\r\n**Code to reproduce the issue**\r\nI am just running python and then the directory to the .py file\r\n\r\nAny help is appreciated thanks!!!!\r\n\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\minec\\Desktop\\Robotics_stuff\\TFconvert.py\", line 4, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["This issue shows up in multiple places #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n \r\nNo such place mentions which DLL fails or differentiates from the others.", "> This issue shows up in multiple places #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n> \r\n> No such place mentions which DLL fails or differentiates from the others.\r\n\r\nYeah I've seen that it had been mentioned a lot of times any solutions that you have found or no?", "There are at least 3 possible scenarios:\r\n\r\n1. You need to install the MSVC 2019 redistributable\r\n2. Your CPU does not support AVX2 instructions\r\n3. Your CPU/Python is on 32 bits\r\n4. There is a library that is in a different location/not installed on your system that cannot be loaded.", "Could you share your CPU make and model?", "Hi @Brayden1000, \r\n\r\nDid you install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019?\r\n\r\nAccording to the docs at https://www.tensorflow.org/install/pip,\r\n\r\n> Install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019. Starting with the TensorFlow 2.1.0 version, the msvcp140_1.dll file is required from this package (which may not be provided from older redistributable packages). The redistributable comes with Visual Studio 2019 but can be installed separately:\r\n\r\n> Go to the [Microsoft Visual C++ downloads](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads),\r\nScroll down the page to the Visual Studio 2015, 2017 and 2019 section.\r\nDownload and install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019 for your platform.\r\nMake sure [long paths](https://superuser.com/questions/1119883/windows-10-enable-ntfs-long-paths-policy-option-missing) are enabled on Windows.\r\n\r\n> Install the [64-bit Python 3 release](https://www.python.org/downloads/windows/) for Windows (select pip as an optional feature).\r\n\r\nTensorflow 2.1.0 is compiled using MSVC 2019, which appears to require an additional DLL."]}, {"number": 36166, "title": "Tensorflow issue", "body": "I am getting this error when I run in PyCharm\r\n\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Siraj/PycharmProjects/cnn/cnn.py\", line 1, in <module>\r\n    from keras import layers\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Siraj\\PycharmProjects\\cnn\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n\r\nProcess finished with exit code 1\r\n\r\n\r\nThe following Code\r\n-----------------------------------\r\n`from keras import layers\r\nfrom keras import models\r\nfrom keras import regularizers\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 1)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(256, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dropout(0.65))\r\nmodel.add(layers.Dense(128, activation='relu' ,kernel_regularizer=regularizers.l2(0.002)))\r\nmodel.add(layers.Dense(6, activation='softmax'))\r\n\r\nmodel.summary()`", "comments": ["Please fill in the issue template and use ` ``` ` around code and error blocks.", "@sirajbb, Provide all the information asked in [Template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks", "@sirajbb, Any update", "@sirajbb, From the error trace it looks like Tensorflow installed incorrectly. Please follow the instructions to install Tensorflow from [here](https://www.tensorflow.org/install). Closing this issue as no activity. Feel free to open if issue still persists. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36166\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36166\">No</a>\n"]}, {"number": 36165, "title": "Only install ipykernel==5.1.1 and nbconvert==4.4.0 in Python3", "body": "The noted package versions are not available in Python 2.", "comments": []}, {"number": 36163, "title": "undeclared inclusion(s) in rule when build from source", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.1.0-rc2\r\n- Python version: intel 3.6.9\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 7.3\r\n- CUDA/cuDNN version: 10.0 or 10.1 (tested the boths)\r\n- GPU model and memory: Tesla K80\r\n\r\n\r\n\r\n**Describe the problem**\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n#compiling from source\r\ncd /xxxx/install/tf2\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit checkout tags/v2.1.0-rc2\r\ngit checkout -b v2.1\r\nconda activate tf2\r\nsource scl_source enable devtoolset-7 llvm-toolset-6.0\r\n./configure\r\n#input: /usr/bin/python3\r\n#input: /opt/anaconda3/lib/python3.6/site-packages\r\n#sequence: ok=enter, ok, ok, ok, y, y, 10.0; 7.6.4; 6.0.1; ok\r\n#input: /usr/local/cuda-10.0,/usr/local/cuda-10.0/bin,/usr/local/cuda-10.0/lib64,/usr/local/cuda-10.0/include,/opt/TensorRT-6.0.1.5,/opt/TensorRT-6.0.1.5/bin,/opt/TensorRT-6.0.1.5/lib,/opt/TensorRT-6.0.1.5/include,/opt/rh/devtoolset-7/root/usr/bin,/opt/rh/devtoolset-7/root/usr/lib64,/opt/rh/devtoolset-7/root/usr/include,/opt/rh/devtoolset-7/root/usr/libexec/gcc/x86_64-redhat-linux/7,/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include\r\n#input: 3.5,3.7,6.0,7.0,7.5\r\n#sequence: ok, ok, ok, ok\r\nbazel build --verbose_failures --config=v2 --config=opt --config=mkl --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n#ERROR IS HERE ...\r\n\r\n#cleaning for next try\r\nbazel clean --expunge --async\r\nrm -rf $HOME/.cache/bazel\r\ngit checkout tags/v2.1.0-rc2\r\ngit branch -d v2.1\r\ngit checkout -b v2.1\r\n\r\n**Any other info / logs**\r\nERROR: /home/xxx/.cache/bazel/_bazel_xxx/26195a8102390a78bf7f86e6341cc9bb/external/boringssl/BUILD:130:1: undeclared inclusion(s) in rule '@boringssl//:crypto':\r\nthis rule is missing dependency declarations for the following files included by 'external/boringssl/src/crypto/asn1/tasn_fre.c':\r\n  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stddef.h'\r\n  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stdint.h'\r\n  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/xxx/install/tf2/tensorflow/tensorflow/lite/toco/python/BUILD:77:1 undeclared inclusion(s) in rule '@boringssl//:crypto':\r\nthis rule is missing dependency declarations for the following files included by 'external/boringssl/src/crypto/asn1/tasn_fre.c':\r\n  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stddef.h'\r\n  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stdint.h'\r\n  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stdarg.h'\r\nINFO: Elapsed time: 61.246s, Critical Path: 3.70s\r\nINFO: 198 processes: 198 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@VitaMusic, Please follow the instructions mentioned in the [Tensorflow doc](https://www.tensorflow.org/install/source). Thanks!", "What is wrong in my install script ?\n\n\u043f\u0442, 24 \u042f\u043d\u0432 2020 \u0433., 10:56 gadagashwini <notifications@github.com>:\n\n> @VitaMusic <https://github.com/VitaMusic>, Please follow the instructions\n> mentioned in the Tensorflow doc\n> <https://www.tensorflow.org/install/source>. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36163?email_source=notifications&email_token=AC7AFFL463J2NAWTMAMBVXTQ7K3MVA5CNFSM4KK2RIJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJ2I4FY#issuecomment-578063895>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFI5BA472U7XHUZJE2LQ7K3MVANCNFSM4KK2RIJQ>\n> .\n>\n", "@VitaMusic, Can you try with removing the conda actvate tf2 step and remove the bazel cache files in /root/.cache/bazel/. let us know how it progresses. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36163\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36163\">No</a>\n"]}, {"number": 36162, "title": "Error importing tensorflow", "body": "**System information**\r\n- My code has been custom made\r\n- Windows 10\r\n- TensorFlow installed from pip\r\n- TensorFlow version 2.1.0\r\n- Python version 3.7\r\n- h5py version 2.10\r\n\r\n**Brief Description**\r\nI am part of a team that develops a standalone software for Windows that encountered an error when trying to import TensorFlow.\r\nThe software, called \"Hera\", is developed in Python and we are now trying to use a previously trained and saved model of a Neural Network to predict a specific feature. For this TensorFlow was used to create, train and save a neural network (outside of the Hera software), and now, when trying to implement TensorFlow on the software we are experiencing some issues.\r\n\r\n**Current behavior**\r\nWhen trying to use the library, importing it as \"import tensorflow as tf\", we get an error message related to h5py, which the program did not use before TensorFlow was added. \r\n\r\n**Things I have tried**\r\n- _Downgrading to TensorFlow to 2.0:_ Another error occurs, this being: ImportError(\"cannot import name 'Layer' from 'tensorflow.python.keras.engine.base_layer' (C:\\\\Development\\\\Hera3\\\\hera.dependencies\\\\OSGEO4W\\\\apps\\\\Python37\\\\lib\\\\site-packages\\\\tensorflow_core\\\\python\\\\keras\\\\engine\\\\base_layer.py)\")\r\n\r\n- _Downgrading h5py to 2.8:_ The same error continued to occur, without change.\r\n\r\n- _Uninstalling h5py:_ The program loads without a problem, however when trying to load the model, that is saved in an h5 file (the format that TensorFlow saves as), it is unable to do so since it does not possess the h5py module.\r\n\r\n**Error log line**\r\nAttributeError(\"type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'\")\r\n\r\n**Full log**\r\n```\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Configuring environment\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Checking GRASS path\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Checking QGis path\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Setting locale directory\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Setting resource directory\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Setting plugins directory\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Setting documentation directory\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Initializing GRASS\r\n2020-01-22 17:41:05 DEBUG: Config.configEnvironment - Setting PSRClasses directory: C:\\Development\\Hera3\\hera.dependencies\\psrclassesinterfacepython\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Initializing PSRClasses\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Initializing QGis\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment -   Creating QgsApplication\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment -   Calling initQgis() - Prefix path: C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\qgis\r\n2020-01-22 17:41:05 DEBUG: Config.configEnvironment - Setting environment variables: \r\n2020-01-22 17:41:05 DEBUG: Config.configEnvironment - GDAL_DATA: C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\share\\gdal\r\n2020-01-22 17:41:05 DEBUG: Config.configEnvironment - PATH: C:\\DEVELO~1\\Hera3\\HERA~1.DEP\\OSGEO4W\\apps\\qgis\\bin;C:\\DEVELO~1\\Hera3\\HERA~1.DEP\\OSGEO4W\\apps\\Python37;C:\\DEVELO~1\\Hera3\\HERA~1.DEP\\OSGEO4W\\apps\\Python37\\Scripts;C:\\DEVELO~1\\Hera3\\HERA~1.DEP\\OSGEO4W\\apps\\qt5\\bin;C:\\DEVELO~1\\Hera3\\HERA~1.DEP\\OSGEO4W\\apps\\Python37\\Scripts;C:\\DEVELO~1\\Hera3\\HERA~1.DEP\\OSGEO4W\\bin;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\system32\\WBem\r\n\r\n2020-01-22 17:41:05 DEBUG: Config.configEnvironment - PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW;.py\r\n2020-01-22 17:41:05 DEBUG: Config.configEnvironment - LD_LIBRARY_PATH: C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\grass\\grass78\\lib\r\n2020-01-22 17:41:05 DEBUG: Config.configEnvironment - GISBASE: C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\grass\\grass78\r\n2020-01-22 17:41:05 DEBUG: Config.configEnvironment - GRASS_SH: C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\msys\\bin\\sh.exe\r\n2020-01-22 17:41:05 DEBUG: Config.configEnvironment - PYTHONPATH: C:\\Users\\pedro\\.p2\\pool\\plugins\\org.python.pydev.core_7.3.0.201908161924\\pysrc\\pydev_sitecustomize;C:\\Development\\Hera3\\hera.application\\src;C:\\Development\\Hera3\\hera.application\\src\\plugins;C:\\Development\\Hera3\\hera.application\\test;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\qgis\\python;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\grass\\grass78\\bin;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\grass\\grass78\\etc;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\grass\\grass78\\etc\\python;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\Python37\\DLLs;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\Python37\\lib;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\Python37;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\Python37\\lib\\site-packages;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\grass\\grass78\\etc\\python;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\grass\\grass78\\etc;C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\grass\\grass78\\bin\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Loading language messages\r\n2020-01-22 17:41:05 INFO: Config.configEnvironment - Finished application initialization\r\n2020-01-22 17:41:06 DEBUG: __init__.wrapper - $HOME=C:\\Users\\pedro\r\n2020-01-22 17:41:06 DEBUG: __init__.wrapper - CONFIGDIR=C:\\Users\\pedro\\.matplotlib\r\n2020-01-22 17:41:06 DEBUG: __init__.wrapper - matplotlib data path: C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\Python37\\lib\\site-packages\\matplotlib\\mpl-data\r\n2020-01-22 17:41:06 DEBUG: __init__.rc_params_from_file - loaded rc file C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\Python37\\lib\\site-packages\\matplotlib\\mpl-data\\matplotlibrc\r\n2020-01-22 17:41:06 DEBUG: __init__.<module> - matplotlib version 3.0.0\r\n2020-01-22 17:41:06 DEBUG: __init__.<module> - interactive is False\r\n2020-01-22 17:41:06 DEBUG: __init__.<module> - platform is win32\r\n2020-01-22 17:41:06 DEBUG: __init__.<module> - loaded modules: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', 'zipimport', '_frozen_importlib_external', '_io', 'marshal', 'nt', 'winreg', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_abc', 'encodings.cp1252', 'site', 'os', 'stat', '_stat', 'ntpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', '_bootlocale', '_locale', 'types', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'functools', '_functools', 'google', 'mpl_toolkits', 'ruamel', 'sitecustomize', 'getpass', 'msvcrt', 'HeraDevEnv', 'hera', 'hera.Config', 'hera.PathManager', 'logging', 'time', 'traceback', 'linecache', 'tokenize', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'token', 'weakref', '_weakrefset', 'collections.abc', 'string', '_string', 'threading', 'atexit', 'PSRClassesIO', 'ctypes', '_ctypes', 'struct', '_struct', 'ctypes._endian', 'distutils', 'distutils.sysconfig', 'distutils.errors', 'PSRClasses', 'qgis', 'pathlib', 'fnmatch', 'posixpath', 'errno', 'urllib', 'urllib.parse', 'qgis.PyQt', 'qgis.PyQt.QtCore', 'PyQt5', 'sip', 'PyQt5.QtCore', 'qgis.gui', 'PyQt5.QtGui', 'PyQt5.QtWidgets', 'PyQt5.QtPrintSupport', 'PyQt5.Qsci', 'PyQt5.QtXml', 'PyQt5.QtSql', 'PyQt5.QtNetwork', 'qgis._core', 'qgis._gui', 'qgs25drendererwidget', 'qgsabstractdatasourcewidget', 'qgsabstractfilecontentsourcelineedit', 'qgsabstractprocessingparameterwidgetwrapper', 'qgsactionmenu', 'qgsadvanceddigitizingcanvasitem', 'qgsadvanceddigitizingdockwidget', 'qgsadvanceddigitizingfloater', 'qgsaggregatetoolbutton', 'qgsalignmentcombobox', 'qgsarrowsymbollayerwidget', 'qgsattributedialog', 'qgsattributeeditorcontext', 'qgsattributeform', 'qgsattributeformeditorwidget', 'qgsattributeforminterface', 'qgsattributeformrelationeditorwidget', 'qgsattributeformwidget', 'qgsattributetableaction', 'qgsattributetabledelegate', 'qgsattributetablefiltermodel', 'qgsattributetablemaplayeraction', 'qgsattributetablemodel', 'qgsattributetableview', 'qgsattributetypeloaddialog', 'qgsauthauthoritieseditor', 'qgsauthcerteditors', 'qgsauthcertinfo', 'qgsauthcertinfodialog', 'qgsauthcertmanager', 'qgsauthcerttrustpolicycombobox', 'qgsauthconfigeditor', 'qgsauthconfigselect', 'qgsauthconfiguriedit', 'qgsautheditorwidgets', 'qgsauthidentitieseditor', 'qgsauthimportcertdialog', 'qgsauthimportidentitydialog', 'qgsauthmethodedit', 'qgsauthmethodplugins', 'qgsauthserverseditor', 'qgsauthsettingswidget', 'qgsauthsslconfigdialog', 'qgsauthsslconfigwidget', 'qgsauthsslerrorsdialog', 'qgsauthsslimportdialog', 'qgsauthtrustedcasdialog', 'qgsblendmodecombobox', 'qgsblurwidget', 'qgsbrowserdockwidget', 'qgsbrowsertreeview', 'qgsbrushstylecombobox', 'qgsbusyindicatordialog', 'qgscategorizedsymbolrendererwidget', 'qgscentroidfillsymbollayerwidget', 'qgscharacterselectordialog', 'qgscheckablecombobox', 'qgscheckboxsearchwidgetwrapper', 'qgscodeeditor', 'qgscodeeditorcss', 'qgscodeeditorexpression', 'qgscodeeditorhtml', 'qgscodeeditorpython', 'qgscodeeditorsql', 'qgscollapsiblegroupbox', 'qgscollapsiblegroupboxbasic', 'qgscolorbox', 'qgscolorbrewercolorrampdialog', 'qgscolorbrewercolorrampwidget', 'qgscolorbutton', 'qgscolordialog', 'qgscoloreffectwidget', 'qgscolorpreviewwidget', 'qgscolorrampbutton', 'qgscolorrampshaderwidget', 'qgscolorrampwidget', 'qgscolorschemelist', 'qgscolorschememodel', 'qgscolorsliderwidget', 'qgscolorswatchdelegate', 'qgscolorswatchgrid', 'qgscolorswatchgridaction', 'qgscolortextwidget', 'qgscolorwheel', 'qgscolorwidget', 'qgscolorwidgetaction', 'qgscompoundcolorwidget', 'qgsconfigureshortcutsdialog', 'qgscoordinateboundspreviewmapwidget', 'qgscptcitycolorrampdialog', 'qgscredentialdialog', 'qgscurveeditorwidget', 'qgscustomdrophandler', 'qgscustomlayerorderwidget', 'qgsdashspacedialog', 'qgsdashspacewidget', 'qgsdatadefinedrotationdialog', 'qgsdatadefinedsizedialog', 'qgsdatadefinedsizelegendwidget', 'qgsdatadefinedvaluedialog', 'qgsdatadefinedwidthdialog', 'qgsdataitemguicontext', 'qgsdataitemguiprovider', 'qgsdataitemguiproviderregistry', 'qgsdatasourceselectdialog', 'qgsdatetimeedit', 'qgsdatetimesearchwidgetwrapper', 'qgsdefaultsearchwidgetwrapper', 'qgsdetaileditemdata', 'qgsdetaileditemdelegate', 'qgsdetaileditemwidget', 'qgsdial', 'qgsdialog', 'qgsdockwidget', 'qgsdoublespinbox', 'qgsdrawsourcewidget', 'qgsdualview', 'qgseditorconfigwidget', 'qgseditorwidgetautoconfplugin', 'qgseditorwidgetfactory', 'qgseditorwidgetregistry', 'qgseditorwidgetwrapper', 'qgseffectdrawmodecombobox', 'qgseffectstackcompactwidget', 'qgseffectstackpropertiesdialog', 'qgseffectstackpropertieswidget', 'qgsellipsesymbollayerwidget', 'qgsencodingfiledialog', 'qgsencodingselectiondialog', 'qgserrordialog', 'qgsexpressionbuilderdialog', 'qgsexpressionbuilderwidget', 'qgsexpressionhighlighter', 'qgsexpressionitem', 'qgsexpressionitemsearchproxy', 'qgsexpressionlineedit', 'qgsexpressionselectiondialog', 'qgsextentgroupbox', 'qgsexternalresourcewidget', 'qgsfeaturelistcombobox', 'qgsfeaturelistmodel', 'qgsfeaturelistview', 'qgsfeaturelistviewdelegate', 'qgsfeaturemodel', 'qgsfeatureselectiondlg', 'qgsfeatureselectionmodel', 'qgsfieldcombobox', 'qgsfieldconditionalformatwidget', 'qgsfieldexpressionwidget', 'qgsfieldvalidator', 'qgsfieldvalueslineedit', 'qgsfiledownloaderdialog', 'qgsfilewidget', 'qgsfilledmarkersymbollayerwidget', 'qgsfilterlineedit', 'qgsfindfilesbypatterndialog', 'qgsfindfilesbypatternwidget', 'qgsfloatingwidget', 'qgsfocuswatcher', 'qgsfontbutton', 'qgsfontmarkersymbollayerwidget', 'qgsformannotation', 'qgsgeometrygeneratorsymbollayerwidget', 'qgsgeometryrubberband', 'qgsglowwidget', 'qgsgradientcolorrampdialog', 'qgsgradientfillsymbollayerwidget', 'qgsgradientstopeditor', 'qgsgraduatedhistogramwidget', 'qgsgraduatedsymbolrendererwidget', 'qgsgroupboxcollapsebutton', 'qgsgroupwmsdatadialog', 'qgsgui', 'qgshashedlinesymbollayerwidget', 'qgsheatmaprendererwidget', 'qgshelp', 'qgshighlight', 'qgshillshaderendererwidget', 'qgshistogramwidget', 'qgshtmlwidgetwrapper', 'qgsifeatureselectionmanager', 'qgsidentifymenu', 'qgsimagesourcelineedit', 'qgsinvertedpolygonrendererwidget', 'qgskeyvaluewidget', 'qgsludialog', 'qgslayerpropertieswidget', 'qgslayertreeembeddedconfigwidget', 'qgslayertreeembeddedwidgetprovider', 'qgslayertreeembeddedwidgetregistry', 'qgslayertreemapcanvasbridge', 'qgslayertreeview', 'qgslayertreeviewdefaultactions', 'qgslayertreeviewindicator', 'qgslayertreeviewmenuprovider', 'qgslayoutcombobox', 'qgslayoutconfigobject', 'qgslayoutcustomdrophandler', 'qgslayoutdesignerinterface', 'qgslayoutitemabstractguimetadata', 'qgslayoutitembasewidget', 'qgslayoutitemcombobox', 'qgslayoutitemguigroup', 'qgslayoutitemguiregistry', 'qgslayoutitempropertiesdialog', 'qgslayoutitempropertieswidget', 'qgslayoutruler', 'qgslayoutunitscombobox', 'qgslayoutview', 'qgslayoutviewellipticalrubberband', 'qgslayoutviewmenuprovider', 'qgslayoutviewmouseevent', 'qgslayoutviewrectangularrubberband', 'qgslayoutviewrubberband', 'qgslayoutviewtool', 'qgslayoutviewtooladditem', 'qgslayoutviewtooladdnodeitem', 'qgslayoutviewtooleditnodes', 'qgslayoutviewtoolmoveitemcontent', 'qgslayoutviewtoolpan', 'qgslayoutviewtoolselect', 'qgslayoutviewtooltemporarykeypan', 'qgslayoutviewtooltemporarykeyzoom', 'qgslayoutviewtooltemporarymousepan', 'qgslayoutviewtoolzoom', 'qgslayoutviewtrianglerubberband', 'qgslegendfilterbutton', 'qgslimitedrandomcolorrampdialog', 'qgslimitedrandomcolorrampwidget', 'qgslinepatternfillsymbollayerwidget', 'qgslistwidget', 'qgslocatorwidget', 'qgslonglongvalidator', 'qgsmanageconnectionsdialog', 'qgsmapcanvas', 'qgsmapcanvasannotationitem', 'qgsmapcanvasitem', 'qgsmapcanvassnappingutils', 'qgsmapcanvastracer', 'qgsmaplayeraction', 'qgsmaplayeractionregistry', 'qgsmaplayercombobox', 'qgsmaplayerconfigwidget', 'qgsmaplayerconfigwidgetfactory', 'qgsmaplayerstylemanagerwidget', 'qgsmapmouseevent', 'qgsmapoverviewcanvas', 'qgsmaptip', 'qgsmaptool', 'qgsmaptooladvanceddigitizing', 'qgsmaptoolcapture', 'qgsmaptooledit', 'qgsmaptoolemitpoint', 'qgsmaptoolextent', 'qgsmaptoolidentify', 'qgsmaptoolidentifyfeature', 'qgsmaptoolpan', 'qgsmaptoolzoom', 'qgsmapunitscaledialog', 'qgsmapunitscalewidget', 'qgsmarkerlinesymbollayerwidget', 'qgsmenuheader', 'qgsmenuheaderwidgetaction', 'qgsmessagebar', 'qgsmessagebaritem', 'qgsmessagelogviewer', 'qgsmessageviewer', 'qgsmetadatawidget', 'qgsmultibandcolorrendererwidget', 'qgsmultiedittoolbutton', 'qgsnewauxiliaryfielddialog', 'qgsnewauxiliarylayerdialog', 'qgsnewgeopackagelayerdialog', 'qgsnewhttpconnection', 'qgsnewmemorylayerdialog', 'qgsnewnamedialog', 'qgsnewvectorlayerdialog', 'qgsnullsymbolrendererwidget', 'qgsowssourceselect', 'qgsopacitywidget', 'qgsoptionsdialogbase', 'qgsoptionsdialoghighlightbutton', 'qgsoptionsdialoghighlightcheckbox', 'qgsoptionsdialoghighlightgroupbox', 'qgsoptionsdialoghighlightlabel', 'qgsoptionsdialoghighlighttree', 'qgsoptionsdialoghighlightwidget', 'qgsoptionspagewidget', 'qgsoptionswidgetfactory', 'qgsorderbydialog', 'qgsorganizetablecolumnsdialog', 'qgspainteffectpropertieswidget', 'qgspainteffectwidget', 'qgspalettedrendererwidget', 'qgspanelwidget', 'qgspanelwidgetstack', 'qgspanelwidgetwrapper', 'qgspasswordlineedit', 'qgspencapstylecombobox', 'qgspenjoinstylecombobox', 'qgspenstylecombobox', 'qgspixmaplabel', 'qgspluginmanagerinterface', 'qgspointclusterrendererwidget', 'qgspointdisplacementrendererwidget', 'qgspointpatternfillsymbollayerwidget', 'qgspresetcolorrampdialog', 'qgspresetcolorrampwidget', 'qgsprevieweffect', 'qgsprocessingalgorithmconfigurationwidget', 'qgsprocessingalgorithmconfigurationwidgetfactory', 'qgsprocessingalgorithmdialogbase', 'qgsprocessingcontextgenerator', 'qgsprocessinggui', 'qgsprocessingguiregistry', 'qgsprocessingmaplayercombobox', 'qgsprocessingmodelerparameterwidget', 'qgsprocessingmultipleselectiondialog', 'qgsprocessingparameterwidgetcontext', 'qgsprocessingparameterwidgetfactoryinterface', 'qgsprocessingrecentalgorithmlog', 'qgsprocessingtoolboxmodel', 'qgsprocessingtoolboxmodelalgorithmnode', 'qgsprocessingtoolboxmodelgroupnode', 'qgsprocessingtoolboxmodelnode', 'qgsprocessingtoolboxmodelprovidernode', 'qgsprocessingtoolboxmodelrecentnode', 'qgsprocessingtoolboxproxymodel', 'qgsprocessingtoolboxtreeview', 'qgsprojectionselectiondialog', 'qgsprojectionselectiontreewidget', 'qgsprojectionselectionwidget', 'qgspropertyassistantwidget', 'qgspropertyoverridebutton', 'qgsproxystyle', 'qgsqmlwidgetwrapper', 'qgsquerybuilder', 'qgsrasterbandcombobox', 'qgsrasterfillsymbollayerwidget', 'qgsrasterformatsaveoptionswidget', 'qgsrasterhistogramwidget', 'qgsrasterlayersaveasdialog', 'qgsrastermarkersymbollayerwidget', 'qgsrasterminmaxwidget', 'qgsrasterpyramidsoptionswidget', 'qgsrasterrendererwidget', 'qgsrastertransparencywidget', 'qgsratiolockbutton', 'qgsrelationaggregatesearchwidgetwrapper', 'qgsrelationeditorwidget', 'qgsrelationreferencesearchwidgetwrapper', 'qgsrelationreferencewidget', 'qgsrelationreferencewidgetwrapper', 'qgsrelationwidgetwrapper', 'qgsrendererpropertiesdialog', 'qgsrendererrasterpropertieswidget', 'qgsrendererrulepropsdialog', 'qgsrendererrulepropswidget', 'qgsrendererwidget', 'qgsrubberband', 'qgsrulebasedrenderermodel', 'qgsrulebasedrendererwidget', 'qgssvgfillsymbollayerwidget', 'qgsscalecombobox', 'qgsscalerangewidget', 'qgsscalevisibilitydialog', 'qgsscalewidget', 'qgssearchquerybuilder', 'qgssearchwidgettoolbutton', 'qgssearchwidgetwrapper', 'qgsshadoweffectwidget', 'qgsshapeburstfillsymbollayerwidget', 'qgsshortcutsmanager', 'qgssimplefillsymbollayerwidget', 'qgssimplelinesymbollayerwidget', 'qgssimplemarkersymbollayerwidget', 'qgssinglebandgrayrendererwidget', 'qgssinglebandpseudocolorrendererwidget', 'qgssinglesymbolrendererwidget', 'qgsslider', 'qgssmartgroupcondition', 'qgssmartgroupeditordialog', 'qgssnapindicator', 'qgssnaptogridcanvasitem', 'qgssourceselectprovider', 'qgssourceselectproviderregistry', 'qgsspinbox', 'qgsstatusbar', 'qgsstyleexportimportdialog', 'qgsstylegroupselectiondialog', 'qgsstylemanagerdialog', 'qgsstylesavedialog', 'qgssublayersdialog', 'qgssubstitutionlistdialog', 'qgssubstitutionlistwidget', 'qgssvgmarkersymbollayerwidget', 'qgssvgselectordialog', 'qgssvgselectorgroupsmodel', 'qgssvgselectorlistmodel', 'qgssvgselectorwidget', 'qgssvgsourcelineedit', 'qgssymbolbutton', 'qgssymbollayerwidget', 'qgssymbollevelsdialog', 'qgssymbollevelswidget', 'qgssymbolselectordialog', 'qgssymbolselectorwidget', 'qgssymbolwidgetcontext', 'qgssymbolslistwidget', 'qgstabwidget', 'qgstablewidgetbase', 'qgstablewidgetitem', 'qgstaskmanagerwidget', 'qgstextformatdialog', 'qgstextformatpanelwidget', 'qgstextformatwidget', 'qgstextpreview', 'qgstransformwidget', 'qgstreewidgetitem', 'qgstreewidgetitemobject', 'qgsunitselectionwidget', 'qgsuserinputwidget', 'qgsvscrollarea', 'qgsvaliditycheckresultsmodel', 'qgsvaliditycheckresultswidget', 'qgsvaluemapsearchwidgetwrapper', 'qgsvaluerelationsearchwidgetwrapper', 'qgsvariableeditorwidget', 'qgsvectorfieldsymbollayerwidget', 'qgsvertexmarker', 'qgswidgetwrapper', 'qgswindowmanagerinterface', 'qgis.core', 'qgis.core.additions', 'qgis.core.additions.edit', 'qgis.core.additions.fromfunction', 'qgis.core.additions.qgstaskwrapper', 'qgis.core.additions.markerlinesymbollayer', 'qgis.core.additions.metaenum', 'qgis.core.additions.processing', 'qgis.core.additions.projectdirtyblocker', 'qgis.core.additions.qgsfeature', 'qgis.core.additions.qgsfunction', 'inspect', 'dis', 'opcode', '_opcode', 'qgis.core.additions.qgsgeometry', 'qgis.core.additions.qgssettings', 'qgis.core.additions.readwritecontextentercategory', 'qgis.core.additions.validitycheck', 'hera.I18N', 'gettext', 'locale', 'unicodedata', 'hera.PropertyManager', 'hera.EntityModel', 'json', 'json.decoder', 'json.scanner', '_json', 'json.encoder', 'hera.FileUtils', 'osgeo', 'imp', 'swig_runtime_data4', '_gdal', 'osgeo.ogr', 'osgeo._ogr', 'osgeo.osr', 'osgeo._osr', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'tempfile', 'random', 'math', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'hera.DataModel', 'datetime', '_datetime', 'copy', 'psrl', 'psrl.psrl', 'hera.PluginManager', 'hera.DynamicGui', 'matplotlib', 'distutils.version', 'pprint', 'subprocess', 'signal', '_winapi', 'urllib.request', 'base64', 'binascii', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'socket', '_socket', 'selectors', 'select', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'ssl', '_ssl', 'urllib.error', 'urllib.response', 'nturl2path', 'matplotlib.cbook', 'glob', 'gzip', 'numbers', 'numpy', '__future__', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._distributor_init', 'numpy.core', 'numpy.core.info', 'numpy.core.multiarray', 'numpy.core.overrides', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.umath', 'numpy.core.numerictypes', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core._internal', 'pickle', '_compat_pickle', '_pickle', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.shape_base', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._dtype_ctypes', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.info', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'ast', '_ast', 'numpy.linalg', 'numpy.linalg.info', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.utils', 'numpy.lib.histograms', 'numpy.lib.stride_tricks', 'numpy.lib.mixins', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.scimath', 'numpy.lib.polynomial', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'decimal', '_decimal', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft.info', 'numpy.fft.fftpack', 'numpy.fft.fftpack_lite', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random.mtrand', 'cython_runtime', 'mtrand', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'textwrap', 'numpy.ma.extras', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'difflib', 'unittest.suite', 'unittest.loader', 'unittest.main', 'argparse', 'unittest.runner', 'unittest.signals', 'numpy.testing._private', 'numpy.testing._private.utils', 'gc', 'numpy.testing._private.decorators', 'numpy.testing._private.nosetester', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'six', 'six.moves', 'matplotlib._version', 'dateutil', 'dateutil._version']\r\n2020-01-22 17:41:06 DEBUG: __init__.wrapper - CACHEDIR=C:\\Users\\pedro\\.matplotlib\r\n2020-01-22 17:41:06 DEBUG: font_manager.<module> - Using fontManager instance from C:\\Users\\pedro\\.matplotlib\\fontlist-v300.json\r\n2020-01-22 17:41:06 DEBUG: pyplot.switch_backend - Loaded backend Qt5Agg version unknown.\r\n2020-01-22 17:41:06 INFO: PluginManager.findAndLoadPlugins - Looking for plugins\r\n2020-01-22 17:41:06 INFO: PluginManager.findAndLoadPlugins - Loading plugin DevTools\r\n2020-01-22 17:41:06.767810: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-01-22 17:41:06.768117: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-01-22 17:41:07 INFO: PluginManager.findAndLoadPlugins - Loaded module: DevTools.PHSAlgorithms\r\n2020-01-22 17:41:07 INFO: PluginManager.findAndLoadPlugins - Loaded module: DevTools.ReservoirTool\r\nTraceback (most recent call last):\r\n  File \"C:\\Development\\Hera3\\hera.application\\src\\HeraDev.py\", line 1, in <module>\r\n    import HeraDevEnv\r\n  File \"C:\\Development\\Hera3\\hera.application\\src\\HeraDevEnv.py\", line 29, in <module>\r\n    PluginManager.findAndLoadPlugins()\r\n  File \"C:\\Development\\Hera3\\hera.application\\src\\hera\\PluginManager.py\", line 39, in findAndLoadPlugins\r\n    raise Exception('Could not import plugin modules: {:}. Exceptions: {:}'.format([n for _,n,_ in nextModules], exceptions))\r\nException: Could not import plugin modules: ['Config', 'RiverDataTool']. Exceptions: [AttributeError(\"type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'\"), AttributeError(\"type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'\")]\r\n```", "comments": ["@prpeixoto, can you provide the versions of \r\n```\r\nkeras-applications\r\nkeras-preprocessing\r\n```\r\nThanks", "@gadagashwini , these are the versions that are currently installed.\r\n\r\nkeras\r\n2.2.4-tf\r\nkeras-applications\r\n1.0.8\r\nkeras-preprocessing\r\n1.1.0", "Are Hera and tf under the same virtuel-env. There may be some conflicts version-wise.", "@prpeixoto, Tensorflow 2.1 support, `keras-applications 1.0.8`, `keras-preprocessing 1.1.0`.\r\n\r\nCan you follow below instructions to install Tensorflow 2.1 using PIP by creating virtual env.\r\n\r\n#Install tensorflow using pip virtual env \r\n$pip install virtualenv\r\n$virtualenv tf_2.1   # tf_2.1 is virtual env name\r\n$source tf_2.1/bin/activate\r\ntf_2.1 $ pip install tensorflow==2.1\r\ntf_2.1 $ python\r\n>>import tensorflow as tf\r\n>>tf.__version__\r\n2.1\r\nLet us know how it progresses. Thanks!", "Firstly, thank you for your responses. I tried the recommended actions and it didn't work. However, downgrading the H5py module to 2.9 seems to have done the trick. I had previously tried downgrading to 2.8 but, for some reason, skipped 2.9. I am very curious to know why this isn't an issue on the 2.9 but is in 2.10, something that if continues on future versions, will be a problem. \r\nThank you again @gadagashwini  and @colt18 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36162\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36162\">No</a>\n"]}, {"number": 36161, "title": "Logic Error in memory planner", "body": "Host OS Platform and Distribution \r\nNAME=\"Ubuntu\"\r\nVERSION=\"18.04 LTS\"\r\nID=ubuntu\r\nPRETTY_NAME=\"Ubuntu 18.04 LTS\"\r\nVERSION_ID=\"18.04\"\r\nPython versionL 2.7.15+\r\nTarget platform : K64F\r\n\r\nDescribe the problem\r\nI am trying to deploy a GRU model on the ARM Cortex M board(K64F). During memory allocation of the tensor, I get the following error message. I don't understand the error.\r\nIt would be quite helpful if somebody knows the solution or an explanation for this following error report.\r\n\r\n```\r\nLogic error in memory planner, tensor 17 has an invalid lifetime\r\nAllocateTensors() failed\r\n```\r\n\r\nThank You", "comments": ["Invalid lifetime indicates that the tensor is either never used as an input to an operation or as an output of an operation (or used as model input/output tensors).\r\nCan you retry with the top of tree version of TF - at the very least you will get better error reporting on invalid lifetime.\r\nAs a workaround you may be able to force a valid lifetime by adding the tensor as an input or output to the model. If the same tensor is used for multiple multiple operations you may run into #35121", "Thank you for your response. Is there a way to find out the name of tensor which is never used as an input? I tried to remove all unused tensors in the model but resulted in the same error. \r\nSince my model can be used only with Tensorflow 1.14 I could not test with Tensorflow 2.0.0.", "@namitajadhav : I assume the limitation on Tensorflow 1.14 is for training and converting the model to tflite - if that is the case you can still deploy the tflite/flatbuffer on current (top of tree) tensorflow master,\r\n\r\n@PeteBlackerThe3rd 's   tflite analyzer may help you to analyze your tflite file: https://github.com/PeteBlackerThe3rd/tflite_analyser ", "@namitajadhav Is this still an issue? Feel free to share your resolution if you have solved it.  Thanks!", "I tried to convert the trained model (trained using tensorflow 1.14). Tried converting using tensorflow 2.1.0 while enabling version 1 and the error still persists.\r\nI even tried to use tensorflow 2.1 to run example code as below.\r\n([https://github.com/tensorflow/tensorflow/issues/36219])\r\n But it resulted in the same error as described.\r\n Any support with this will be of great help.\r\n\r\nThank You\r\n", "@namitajadhav Could you provide your TFLite file? Its difficult to debug the tensor without seeing how its used in the model :-)", "I have attached two tflite files. The first one is created with tensorflow 1.14 which gives me a error of invalid lifetime. The second tflite is created with an example code of Keras and TF 2.1.0.\r\n\r\n[tflite.zip](https://github.com/tensorflow/tensorflow/files/4170875/tflite.zip)\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36161\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36161\">No</a>\n"]}, {"number": 36160, "title": "Op Request", "body": "**System information**\r\n- Windows 10\r\n- TensorFlow installed from source:\r\n- TensorFlow version 2.1 (cpu)\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, DIV, EXP, FILL, FULLY_CONNECTED, GATHER, LOGISTIC, MAX_POOL_2D, MUL, PACK, PRELU, REDUCE_MAX, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, SUB, SUM, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\evan.chu$\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\evan.chu$\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Evan.Chu$\\AppData\\Local\\Programs\\Python\\Python37\\Scripts\\toco_from_protos.exe\\__main__.py\", line 9, in <module>\r\n  File \"c:\\users\\evan.chu$\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\evan.chu$\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\evan.chu$\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\evan.chu$\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\evan.chu$\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\n\r\n```\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nhttps://github.com/arthurflor23/handwritten-text-recognition/blob/master/src/network/model.py (Flor implementation, not Bluche or Puigcerver)\r\n\r\n", "comments": ["Hi, I just followed the advice that @oanush gave in #36152, and it seems that it has fixed the issue. \r\n\r\n\"Please try including the code before tflite=converter.convert()\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nFor more information refer #35590\r\nIf the issue is still not resolved please provide us the complete code snippet used.Thanks!\"", "One additional issue, when I try to load the TFLite model I run into the issue:\r\n\r\nTraceback (most recent call last):\r\n  File \"details.py\", line 4, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"C:\\Users\\Evan.Chu$\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter.py\", line 247, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"C:\\Users\\Evan.Chu$\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter_wrapper\\tensorflow_wrap_interpreter_wrapper.py\", line 110, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: Encountered unresolved custom op: TensorListFromTensor.Node number 53 (TensorListFromTensor) failed to prepare.\r\n\r\nClearly the issue is still related to the previous issue, and I'm wondering if there is a permanent fix rather than the \"band-aid\" fix that I commented earlier that doesn't involve creating a custom op?\r\n\r\n", "Can you also add experimental_converter and custom ops flag?\r\n```python\r\nconverter.allow_custom_ops = True\r\nconverter.experimental_new_converter = True\r\n```", "Hi, forgot to update the issue earlier but I did try that and it was able to convert and load using the interpreter. Thanks for the help!\r\n", "> Hi, forgot to update the issue earlier but I did try that and it was able to convert and load using the interpreter. Thanks for the help!\r\n\r\n@ech\r\nHello, which model did you converted? I encountered one similar issue when converting efficientdet model. The issue is detailed in this \r\n\r\n> https://github.com/tensorflow/tensorflow/issues/37003#issue-569633708\r\n\r\n.\r\n\r\nCan you help me?"]}]