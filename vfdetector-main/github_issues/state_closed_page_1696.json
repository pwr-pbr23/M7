[{"number": 2020, "title": "Doc changes", "body": "", "comments": []}, {"number": 2019, "title": "docs only: removes sudo from pip/easy_install in instructions", "body": "`sudo`, where removed in the below instances, is superfluous, nonstandard, and is not a sensible default.  When using a virtualenv or similar, this can cause unsafety later on. Below examples work when pip/easy_install/npm are installed in userland. `pip` will complain if it needs root.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "PS: sorry for being a pedant.\n", "Also, I signed it!\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Wrong email address vis-a-vis CLA is fixed.\n", "We probably put it in because on managed corporate machines the sudo can be necessary. Let me test on a clean machine. I'm all for removing it if it works for people in general. \n", "Actually we left `sudo` off in our initial docs, and many of our users who weren't installing in virtualenvs kept running into permissions problems, so we added `sudo` and the ones who knew what they were doing left it off in virtualenvs ;).\n", "We already have no sudo in the virtualenv section of the docs. Sadly, as @vrv has said, in the non-virtualenv case, it is terribly common to require either a targeted chmod/chown or sudo. \n\nI'd rather not go back to fixing people's permissions. :(\n", "Oof. @martinwicke @vrv I am sympathetic to your position. To summarize:\n1. On a vanilla install, most users will need root.\n2. pip/easy_install/anything will give pretty clear permission denied errors\n3. Non-experienced users will hassle the maintainers personally if confused about #2\n4. #3 is a show-stopper at google scale\n\nI am sympathetic to your position and I can't even say you're making the wrong choice. That said, it gives me a sad. :(\n\nThanks in any event for your attention to this matter.\n", "Well \"personally\" is a little harsh -- nobody has yelled at me on the street about this yet. But it is a support burden we'd rather not go back to. I'm sad about it too.\n\nI'll close this sad PR. \n"]}, {"number": 2018, "title": "fix index.md & add png files at \"tensorflow/tensorflow/g3doc/images\" ", "body": "When I review TF tutorial using Github website at index.md on \"tensorflow/tensorflow/g3doc/tutorials/mnist/beginners/index.md\"\nThere are no image on index.md and others \n\nI think there are no png files. If I possible to upload png files on \n\nfor example \"tensorflow/tensorflow/g3doc/tutorials/mnist/beginners/index.md\"\n\nLine 18: <img style=\"width:100%\" src=\"../../../images/MNIST.png\">\n --> there is no MNIST.png at \"../../../images/MNIST.png\"\n\nLine 67: <img style=\"width:100%\" src=\"../../../images/MNIST-Matrix.png\">\n --> there is no MNIST-Matrix.png at \"../../../images/MNIST-Matrix.png\"\n\nLine 81: <img style=\"width:100%\" src=\"../../../images/mnist-train-xs.png\">\n --> there is no mnist-train-xs.png at \"../../../images/mnist-train-xs.png\"\n\nLine103: <img style=\"width:100%\" src=\"../../../images/mnist-train-ys.png\">\n --> there is no mnist-train-ys.png at \"../../../images/mnist-train-ys.png\"\n\nLine124: <img style=\"width:100%\" src=\"../../../images/softmax-weights.png\">\n --> there is no softmax-weights.png at \"../../../images/softmax-weights.png\"\n\nLine181: <img style=\"width:100%\" src=\"../../../images/softmax-regression-scalargraph.png\">\n --> there is no softmax-regression-scalargraph.png at \"../../../images/softmax-regression-scalargraph.png\"\n\nLine187: <img style=\"width:100%\" src=\"../../../images/softmax-regression-scalarequation.png\">\n --> there is no softmax-regression-scalarequation.png at \"../../../images/softmax-regression-scalarequation.png\"\n\nLine195: <img style=\"width:100%\" src=\"../../../images/softmax-regression-vectorequation.png\">\n --> there is no softmax-regression-vectorequation.png at \"../../../images/softmax-regression-vectorequation.png\"\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "We kept the images out of the repo, they are instead served from tensorflow.org. We keep the markdown in github so you can fix things, but it's not meant to be viewed on github.\n"]}, {"number": 2017, "title": "Tensor Forest regression", "body": "I was not able to find any documentation for Tensor Forest, however from the comment on line 90 in [tensor_forest.py](https://github.com/tensorflow/tensorflow/edit/master/tensorflow/contrib/tensor_forest/python/tensor_forest.py) it seems to follow that it is intended for regression as well as classification. However it does not accept floats as labels.\n\nThe following code:\n`params = tensor_forest.ForestHParams(num_classes=1, num_features=features, num_trees=10, max_nodes=1000).fill()`\n`graph_builder = tensor_forest.RandomForestGraphs(params)`\n`graph = graph_builder.training_graph(X, y)`\n\nReturns a value error:\n`TypeError:  Input 'input_labels' of 'CountExtremelyRandomStats' Op has type float32 that does not match expected type of int32.`\n", "comments": ["@gilberthendry \n", "The released version of Tensor Forest doesn't support regression yet, apologies for the lack of a good error message or documentation.  But sit tight, it's implemented and just hasn't made its way to github yet.  \n"]}, {"number": 2016, "title": "cuda 7.5 // AttributeError: type object 'NewBase' has no attribute 'is_abstract'", "body": "Hello everyone,\n\n   have the following problem after the tensorflow installation via pip:\n\n`>>> import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 94, in <module>\n    from tensorflow.python.platform import test\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/test.py\", line 62, in <module>\n    from tensorflow.python.framework import test_util\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py\", line 41, in <module>\n    from tensorflow.python.platform import googletest\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/googletest.py\", line 32, in <module>\n    from tensorflow.python.platform import benchmark  # pylint: disable=unused-import\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/benchmark.py\", line 112, in <module>\n    class Benchmark(six.with_metaclass(_BenchmarkRegistrar, object)):\n  File \"/usr/lib/python2.7/dist-packages/six.py\", line 617, in with_metaclass\n    return meta(\"NewBase\", bases, {})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/benchmark.py\", line 107, in __new__\n    if not newclass.is_abstract():\nAttributeError: type object 'NewBase' has no attribute 'is_abstract'`\n\nI'm using Nvidia GTX Titan X (comp. capability 5.2) and CUDA 7.5. Does anybody have an idea?\n\nMany thanks in advance for your help!\n\nCheers,\nA\n", "comments": ["This looks like the same issue as #1965, so I'm closing this issue as a duplicate. (On the other thread, somebody suggested upgrading the `six` library to fix this problem.)\n", "this does not help... that's why I have posted this issue. I have updated Six library, but still have the same problem...\n", "That's a pity - can you comment on the other issue with the version of Six you're using, to help narrow down the cause (since the underlying error is the same).\n", "I met it, too. If you have solved this problem, would you like to provide the solution on this website? Thank you very much!\n", "I solved this problem by reinstall the package six:\n`sudo pip uninstall six`\n`sudo pip install six --upgrade --target=\"/usr/lib/python2.7/dist-packages\"`\nYou can check the path of `python2.7/dist-packages`, and have a try.\n", "I solved this problem on mac osx doing this : \n`python`\n`import six`\nThen we localize the package six:\n`>>> six.__file__`\n`'/Library/Python/2.7/site-packages/six.pyc'`\nThen we upgrade the concerned package \n`sudo pip install six --upgrade --target=\"/Library/Python/2.7/site-packages/\"`\n", "I resolved this by uninstalling and reinstalling six -- without needing to flag a target.\n", "@petulla  @PierreGe  @yhlleo \nI have tried all the solution. But I am facing same error.\nI am using six - 1.10.0\n", "It worked after sudo pip install six --upgrade --target=\"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/\"\n", "Hi, It didn't work by doing uninstall and reinstall six..Because, my six is already the newest version.", "Hi @dearLilian , If it is not working for you then you can use Anaconda. One of my friends was not able to resolve this error so he installed Anaconda.", "Ok, thanks for you answer.\n\n\n> \u5728 2017\u5e7411\u67083\u65e5\uff0c\u4e0b\u53482:15\uff0cMAYANK JINDAL <notifications@github.com> \u5199\u9053\uff1a\n> \n> Hi @dearLilian <https://github.com/dearlilian> , If it is not working for you then you can use Anaconda. One of my friends was not able to resolve this error so he installed Anaconda.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/2016#issuecomment-341628737>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AKSx6J_QldAk5XW68kbzhn717CtF0qJPks5syq96gaJpZM4IKfUS>.\n> \n\n"]}, {"number": 2015, "title": "recurrent layer on top of convolution layer fails with GPU", "body": "### Environment info\n\nOperating System: Ubuntu 15.04\n\nInstalled version of CUDA and cuDNN: \nCUDA 7.5\ncuDNN 5.0.4\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root 189170 Oct 10  2015 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Oct 10  2015 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Oct 10  2015 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Oct 10  2015 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Oct 10  2015 /usr/local/cuda/lib/libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   pip list\n   tensorflow (0.7.1)\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.7.1\n### Problem description\n\nI simply try to put a recurrent layer on top of a convolution layer, which directly manipulate the inputs. It is ok with cpu, but fails with gpu. When i remove either the convolution layer or the recurrent layer, it works well with gpu. \n\nThe error message is \n\n> python: external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:223: static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, false>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, const Eigen::TensorReductionOpEigen::internal::SumReducer<float, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<const float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16> > > > >]: Assertion `cudaGetLastError() == cudaSuccess' failed.\n\nI run some black-box test (as i don't know a better way to debug it), and found that it raises error when it try to compute gradients with the statement \n`grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n                                      config.max_grad_norm)\n`\n\nThe following is a simplified version of the code which has the same problem. (There ought to be a softmax layer on top of the recurrent layer, but i replace it with sum to make it simpler)\n\n```\nimport numpy as np\nimport tensorflow as tf\n\nimport sys\nsys.path.append('/home/jiahua/tensorflow')\nimport math\nimport time\n\nfrom tensorflow.contrib.ctc import ctc_ops\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import rnn\nfrom conv_batch_normalizer import ConvolutionalBatchNormalizer\n\nimport load_data\n\nclass Model(object):\n\n  def __init__(self, is_training, config):\n    self._batch_size = batch_size = config.batch_size\n    self._freq_size = freq_size = config.freq_size\n    self._hidden_size = hidden_size = config.hidden_size\n    self._seq_max_length = seq_max_length = config.seq_max_length\n\n    num_channels = 10\n    filter_dimension = 2\n    conv_stride = 2\n\n    self._inputs = inputs = tf.placeholder(tf.float32, [batch_size, seq_max_length, freq_size]) \n    self._sequence_lengths = sequence_lengths = tf.placeholder(tf.int32, [batch_size])\n    inputs = tf.reshape(inputs, [batch_size, seq_max_length, freq_size, 1])\n\n    padding_size = filter_dimension - (seq_max_length - 1)  % conv_stride - 1 \n    print 'padding_size', padding_size\n    paddings = [[0, 0], [0, padding_size], [0, 0], [0, 0]]\n    inputs = tf.pad(inputs, paddings)\n\n    if is_training and config.keep_prob < 1:\n      inputs = tf.nn.dropout(inputs, config.keep_prob)\n    print 'inputs', tf.Tensor.get_shape(inputs)\n\n    parameters = []\n    # conv1\n    with tf.name_scope('conv1') as scope:\n      kernel = tf.Variable(tf.truncated_normal([filter_dimension, freq_size, 1, num_channels], dtype=tf.float32, stddev=1e-1), name='weights')\n      conv = tf.nn.conv2d(inputs, kernel, [1, conv_stride, conv_stride, 1], padding='VALID')\n      biases = tf.Variable(tf.constant(0.0, shape=[num_channels], dtype=tf.float32), trainable=True, name='biases')\n      bias = tf.nn.bias_add(conv, biases)\n      conv1 = tf.nn.relu(bias, name=scope)\n      parameters += [kernel, biases]\n\n    # batch normalization\n    print 'conv1', tf.Tensor.get_shape(conv1)\n\n    inter1 = tf.reshape(conv1, [batch_size, -1, 1, num_channels])\n    print 'inter1', tf.Tensor.get_shape(inter1)\n\n    new_length = (seq_max_length + padding_size - filter_dimension) / conv_stride + 1\n    print 'new length = ', new_length \n\n    # rnn\n    rnn_inputs = tf.reshape(inter1, [batch_size, new_length, num_channels])\n    print 'rnn_inputs', tf.Tensor.get_shape(rnn_inputs)\n\n    lstm_cell = rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0)\n    if is_training and config.keep_prob < 1:\n      lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n    cell = rnn_cell.MultiRNNCell([lstm_cell] * 2)\n\n    self._initial_state = cell.zero_state(batch_size, tf.float32)\n\n    rnn_outputs = []\n    state = self._initial_state\n    with tf.variable_scope(\"rnn\"):\n      for time_step in range(new_length):\n        if time_step > 0: tf.get_variable_scope().reuse_variables()\n        (cell_output, state) = cell(rnn_inputs[:, time_step, :], state)\n        rnn_outputs.append(cell_output)\n\n    print 'rnn_outputs ', tf.Tensor.get_shape(rnn_outputs[0]) , ' * ', len(rnn_outputs)\n    self._final_state = state\n\n    self._cost = cost = tf.reduce_sum(tf.add_n(rnn_outputs)) / batch_size\n\n    if not is_training:\n      return\n\n    self._lr = tf.Variable(0.0, trainable=False)\n    tvars = tf.trainable_variables()\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n                                      config.max_grad_norm)\n\n    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n    self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n\n  def assign_lr(self, session, lr_value):\n    session.run(tf.assign(self._lr, lr_value))\n\n  @property\n  def input_data(self):\n    return self._input_data\n\n  @property\n  def cost(self):\n    return self._cost\n\n  @property\n  def final_state(self):\n    return self._final_state\n\n  @property\n  def lr(self):\n    return self._lr\n\n  @property\n  def train_op(self):\n    return self._train_op\n\n  @property\n  def predict(self):\n    return self._predict\n\n\nclass Config(object):\n  batch_size = 2\n  freq_size = 2\n  hidden_size = 2\n  seq_max_length = 5\n  init_scale = 1.0\n  lr_decay = 0.5\n  max_epoch = 10\n  max_max_epoch = 100\n  keep_prob = 0.9\n  max_grad_norm = 5\n  learning_rate = 1\n\n\ndef run_epoch(session, m, data, eval_op):\n  start_time = time.time()\n  iters = 0\n  costs = 0.0\n  state = m._initial_state.eval()\n  for (inputs, labels_indices, labels_values, labels_shape, seq_lens) in data:\n    cost, state, _ = session.run([m._cost, m._final_state, eval_op],\n                                 {m._inputs: inputs,\n                                  m._initial_state: state})\n    costs += cost\n    iters += 1\n    if iters % 1 == 0:\n      print iters\n      print \"time: %f\" % (time.time() - start_time)\n      print \"cost: %f\" % cost\n  print 'finish'\n  print \"total time: %f\" % (time.time() - start_time)\n  return costs / iters\n  return 0.0\n\n\ndef main():\n  config = Config()\n  with tf.Graph().as_default(), tf.Session() as session:\n    initializer = tf.random_uniform_initializer(-config.init_scale,\n                                                config.init_scale)\n    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n      m = Model(is_training=True, config=config)\n    init_op = tf.initialize_all_variables()\n    session.run(init_op)\n\n    train_data = [([[[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]], [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]], [[0, 0], [0, 1], [1, 0], [1, 1]], [0, 1, 1, 2], [2, 2], [2, 2])]\n    for i in range(config.max_max_epoch):\n      lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n      print '#iter %d start lr: %f' % (i, config.learning_rate * lr_decay)\n      m.assign_lr(session, config.learning_rate * lr_decay)\n\n      print 'average loss: %f' % (run_epoch(session, m, train_data, m._train_op))\n\n\nif __name__ == \"__main__\":\n  main()\n```\n### What have you tried?\n\nBelow is the stack status printed using gdb python, in case you want to look at it. It seems that there is a problem when it tries to launch a cuda kernel to compute the l2loss.\n\n> # 0  0x00007ffff7826267 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:55\n> # 1  0x00007ffff7827eca in __GI_abort () at abort.c:89\n> # 2  0x00007ffff781f03d in **assert_fail_base (fmt=0x7ffff7981028 \"%s%s%s:%u: %s%sAssertion `%s' failed.\\n%n\", assertion=assertion@entry=0x7fffe3c03fe8 \"cudaGetLastError() == cudaSuccess\", file=file@entry=0x7fffe3c03f80 \"external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h\", line=line@entry=223, function=function@entry=0x7fffe3c04180 <Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const&, Eigen::GpuDevice const&)::__PRETTY_FUNCTION**> \"static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, false>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen:\"...) at assert.c:92\n> # 3  0x00007ffff781f0f2 in **GI___assert_fail (assertion=0x7fffe3c03fe8 \"cudaGetLastError() == cudaSuccess\", file=0x7fffe3c03f80 \"external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h\", line=223, function=0x7fffe3c04180 <Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const&, Eigen::GpuDevice const&)::__PRETTY_FUNCTION**> \"static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, false>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen:\"...) at assert.c:101\n> # 4  0x00007fffe2487e26 in Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const&, Eigen::GpuDevice const&) (expr=..., device=...) at external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:223\n> # 5  0x00007fffe24879ea in Eigen::TensorDevice<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::GpuDevice>::operator=Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> >(Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const&) (this=0x7fff357f91c0, other=...) at external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35\n> # 6  0x00007fffe248785a in tensorflow::functor::L2Loss<Eigen::GpuDevice, float>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16>, Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>) (this=0x7fff357f9250, d=..., input=..., output=...) at ./tensorflow/core/kernels/l2loss_op.h:32\n> # 7  0x00007fffe247ee64 in tensorflow::L2LossOp<Eigen::GpuDevice, float>::Compute (this=0x402f9e0, context=0x7fff357f9b20) at tensorflow/core/kernels/l2loss_op.cc:45\n> # 8  0x00007fffe293d927 in tensorflow::BaseGPUDevice::Compute (this=0x3dc5b60, op_kernel=0x402f9e0, context=0x7fff357f9b20) at tensorflow/core/common_runtime/gpu/gpu_device.cc:388\n> # 9  0x00007fffe2b61689 in tensorflow::(anonymous namespace)::ExecutorState::Process (this=0x3facbc0, tagged_node=..., scheduled_usec=0) at tensorflow/core/common_runtime/executor.cc:1092\n> # 10 0x00007fffe2b6cf89 in std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)>::operator()<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode&, long long&, void> (this=0x7fff200008c0, __object=0x3facbc0) at /usr/include/c++/4.9/functional:569\n> # 11 0x00007fffe2b6c3dc in std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::__call<void, 0ul, 1ul, 2ul>(<unknown type in /home/jiahua/tensorflow/bazel-bin/speech/model.runfiles/tensorflow/python/_pywrap_tensorflow.so, CU 0xdb55fb8, DIE 0xdbd4a7f>, std::_Index_tuple<0ul, 1ul, 2ul>) (this=0x7fff200008c0, __args=<unknown type in /home/jiahua/tensorflow/bazel-bin/speech/model.runfiles/tensorflow/python/_pywrap_tensorflow.so, CU 0xdb55fb8, DIE 0xdbd4a7f>) at /usr/include/c++/4.9/functional:1264\n> # 12 0x00007fffe2b6aa38 in std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::operator()<, void>(void) (this=0x7fff200008c0) at /usr/include/c++/4.9/functional:1323\n> # 13 0x00007fffe2b68c54 in std::_Function_handler<void(), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/4.9/functional:2039\n> # 14 0x00007fffe0b01732 in std::function<void ()>::operator()() const (this=0x7fff357f9dd0) at /usr/include/c++/4.9/functional:2439\n> # 15 0x00007fffe2da977a in tensorflow::thread::ThreadPool::Impl::WorkerLoop (this=0x3ddaa90) at tensorflow/core/lib/core/threadpool.cc:196\n> # 16 0x00007fffe2da91bb in tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int)::{lambda()#1}::operator()() const () at tensorflow/core/lib/core/threadpool.cc:123\n> # 17 0x00007fffe2da9b93 in std::_Function_handler<void(), tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, const tensorflow::ThreadOptions&, const string&, int)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/4.9/functional:2039\n> # 18 0x00007fffe0b01732 in std::function<void ()>::operator()() const (this=0x3ddba58) at /usr/include/c++/4.9/functional:2439\n> # 19 0x00007fffe2dcb472 in std::_Bind_simple<std::function<void ()> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x3ddba58) at /usr/include/c++/4.9/functional:1700\n> # 20 0x00007fffe2dcb3b7 in std::_Bind_simple<std::function<void ()> ()>::operator()() (this=0x3ddba58) at /usr/include/c++/4.9/functional:1688\n> # 21 0x00007fffe2dcb334 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() (this=0x3ddba40) at /usr/include/c++/4.9/thread:115\n> # 22 0x00007fffd4e4bf20 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n> # 23 0x00007ffff7bc26aa in start_thread (arg=0x7fff357fa700) at pthread_create.c:333\n> # 24 0x00007ffff78f7eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\n", "comments": ["Thankyou for the detailed bug report and repro instructions!  \n\nWe are looking at a similar bug report internally, and will update this post once we know more.\nThanks,\nPaul \n", "@prb12, @benoitsteiner: Any updates?  \n", "I can't reproduce the issue. Is this still a problem?\n", "@alphaf52: Is it still an issue?\n@prb12: Were you able to reproduce this at some point?\n", "hi, this is solved long ago, i can't remember what was wrong at that time, but it's gone\n"]}, {"number": 2014, "title": "Tensorboard not showing anything ", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nMac OS\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nNone installed.\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0rc0-py2-none-any.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.8.0rc0\n### Steps to reproduce\n1. tensorboard --logdir=board \n### What have you tried?\n1. checked things according to the instructions in ready.md\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/Users/jcyk/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/TAG' on path /Users/jcyk/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/TAG\nWARNING:tensorflow:Unable to read TensorBoard tag\nStarting TensorBoard  on port 6006\n(You can navigate to http://0.0.0.0:6006)\n\n[board.zip](https://github.com/tensorflow/tensorflow/files/225193/board.zip)\n", "comments": ["+1. I have similar issue. \nI checked the logdir and the events file Test and Train are in the directory.\n\n`(tensorflow)[ arnaudlejeune /tmp/mnist_logs ] find /tmp/mnist_logs/ | grep events | xargs ls -lh\n-rw-r--r--  1 arnaudlejeune  wheel   2,6M 20 avr 11:40 /tmp/mnist_logs/test/events.out.tfevents.1461170332.MacBook-Pro-de-ARNAUD.local\n-rw-r--r--  1 arnaudlejeune  wheel    19M 20 avr 11:40 /tmp/mnist_logs/train/events.out.tfevents.1461170331.MacBook-Pro-de-ARNAUD.local`\n", "I was seeing the same issue.  I updated to a [nightly build](http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastBuild/) and the problem is gone.\n", "@jcyk, @arlejeun: Does updating to a more recent version fix this for you?\n", "Have a similar issue with a recent build of tensorflow - the .runfiles paths are not correct, e.g. tensorboard is looking for `/home/ubuntu/tensorflow-py2.7/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/org_tensorflow/external/paper_item/paper-item.html`,\nwhich is actually located in `/home/ubuntu/tensorflow-py2.7/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/paper_item/paper-item.html`\n\nSymlinking directories into the right places fixes the issue.\n", "@kadrach: I'm going to close since the original problem seems to have been fixed, but please open a separate issue if you have a different extant problem. \n"]}, {"number": 2013, "title": "Question:How does Tensorflow deal with numpy.nan values?", "body": "1. raise error\n2. treat them as empty: I would say this is the proper way but the developers have to some math works to let the codes work\n3. treat them as zero\n", "comments": ["Different parts of TensorFlow treat them differently. Float computations (usually?)\npropagate them. Int conversion treats them as 0. Int computations fail with Python parts of TensorFlow often raise an error on \"NaN\", ie, trying to add a NaN summary to histogram will fail with Python\nexception.\n\na=tf.constant(1.0)\nb = tf.constant(0.0)\nc = a/b \\* 0          # nan\nd = c+1             # nan+1 = nan\ne = tf.to_int32(d)  # int(nan) = 0\nf = c - c           # nan - nan = nan\nsess = create_session()\nprint sess.run(c)\nprint sess.run(d)\nprint sess.run(e)\nprint sess.run(f)\n\nOn Mon, Apr 18, 2016 at 8:20 PM, zhang8473 notifications@github.com wrote:\n\n> 1. raise error\n> 2. treat them as empty: I would say this is the proper way but the\n>    developers have to some math works to let the codes work\n> 3. treat them as zero\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2013\n"]}, {"number": 2012, "title": "Updated docstrings to link to descriptions of padding algorithms.", "body": "It took a bit of searching to find out what these terms \"VALID\" and \"SAME\" meant, so I put a link in the docstring.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Ok updated PR to use fancy forwarders and squashed into 1 commit. \n\nAnything else to do?\n", "Jenkins, test this please.\n", "I wonder what these failures mean. Maybe I should sync up with latest master and reapply diffs?\n", "conflicts need to be resolved, I think the failures are infra failures, we can ignore them.\n", "friendly ping to update conflicts \n", "ah thanks, yes I'll take care of this :)\n", "Jenkins, test this please.\n"]}, {"number": 2011, "title": "mod operation missing on GPU", "body": "To verify:\n\n```\n\ndef run_summarize(run_metadata):\n  print \"***\"\n  for device in run_metadata.step_stats.dev_stats:\n    print device.device\n    for node_stats in device.node_stats:\n      print '   ', node_stats.node_name\n\ndef sessrun(*arglist, **dictlist):\n  run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n  run_metadata = tf.RunMetadata()\n  dictlist['options'] = run_options\n  dictlist['run_metadata'] = run_metadata\n  result = tf.get_default_session().run(*arglist, **dictlist)\n  run_summarize(run_metadata)\n  return result\n\ntf.reset_default_graph()\nsess = create_session()\na = tf.constant(10)\nb = tf.constant(5)\nc = a%b\nsessrun(c)\n\n***\n/job:localhost/replica:0/task:0/cpu:0\n    _SOURCE\n    mod\n    _SINK\n/job:localhost/replica:0/task:0/gpu:0\n    _SOURCE\n    Const_1\n    Const\n    _SINK\n```\n", "comments": ["Just stumbled across MatMul being not defined for GPU for inputs are double. Maybe we need a tracking spreadsheet for all the missing op/type/device combinations instead of individual bugs?\n", "I believe this has been fixed.\n"]}, {"number": 2010, "title": "Merge internal changes.", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@martinwicke are these failures known?\n", "I'm taking this one over.\n"]}, {"number": 2009, "title": "Include TensorBoard TAG in pip_package", "body": "", "comments": ["Tested building the pip package with this, and the TAG file is now present.\n"]}, {"number": 2008, "title": "multivariate_normal", "body": "Is there a function can output random values from a multivariate normal distribution?? or how to  implement this with tensorflow?\n", "comments": ["Hi, this is probably a better question for either the discussion mailing list or StackOverflow -- can you please repost at one of those [venues](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/index.md#community-)?\n"]}, {"number": 2007, "title": "Add mavx/mavx2 switches to ci_parameterized_build", "body": "", "comments": []}, {"number": 2006, "title": "Duplicated description", "body": "No need to point out twice.\n", "comments": ["Can one of the admins verify this patch?\n", "Shape and type are different things.\n", "The wording isn't great. However, this is an autogenerated file. You'd have to modify the source of this doc which is the docstring of the function in question.\n"]}, {"number": 2005, "title": "No ImageNet folder", "body": "No ImageNet folder found at `/usr/local/lib/python2.7/dist-packages/tensorflow/models/image`.  Only `mnist` and `cifar10`.\n\nAm I missing something?\n\nInstalled with `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp27-none-linux_x86_64.whl` on Ubuntu 15.1\n", "comments": ["You can get the sources from github (check out the code at the r0.8 branch if you're using the r0.8 wheel).  The models code aren't really core libraries, just examples.  I'd like to remove mnist/cifar10 from the pip wheel too.  Unless someone can convince me otherwise, it's weird to me to have models be part of the pip installed package.\n", "+1 to @vrv. I think part of the reason it was \"convenient\" to have this is to paper over import issues for the various `input_data` modules. We could consider moving these to `tf.contrib` (or shoehorn them into skflow's dataset support).\n", ":+1: \n"]}, {"number": 2004, "title": "Open source performance benchmarks", "body": "In #2001 we discussed about the performance benchmark, I suggest maybe we can have some benchmark code (in automatically tests or something else) for common NN architecture like Alex, Inception v3, ResNet  on some famous dataset like cifar10, cifar100, ImageNet.\nBy doing this, we can continuously trace our performance when we update our algorithms or code.\nAlso, we could show our benchmark test on our website or github page.\n", "comments": ["Besides, if we plan to have the performance benchmark code, maybe I could help some part\n", "@ebrevdo has worked on benchmarking tools in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/benchmark.py\n\nWe use this internally to catch regressions because we log them to internal databases.  We'd like to have this also in the OSS world too, but we haven't figured out all the tooling yet.\n", "The main blocker I see now is a way to write the [TestResults](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/test_log.proto) protos we generate into a BigQuery table; which means being able to automatically generate and update the table's structure when the TestResults proto is updated.\n\nThis tool could be a start, but I'm unaware of who is maintaining it and it doesn't support proto3 format (which TestReport is):\n\nhttps://github.com/GoogleCloudPlatform/protoc-gen-bq-schema\n\nAnyone interested in getting this working?  I don't have the bandwidth...  marking as contributions welcome.\n", "@caisq  feel free to remove self; just wanted you on the participants list so you see relevant updates here.\n", "https://www.tensorflow.org/performance/benchmarks  and we have started to updated code other teams are using for benchmarks to try and ensure best practices."]}, {"number": 2003, "title": "[skflow] Added additional callbacks for monitor", "body": "cc: @ilblackdragon \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please test\n"]}, {"number": 2002, "title": "LSTM*BlockOp: Monolithic kernels for the LSTM, approx 40-50% faster.", "body": "These are monolithic OpKernel for LSTMs. Compared to python BasicLSTMCell impl on 1 layer PTB this is a 40-50% performance difference on Titan X. Also, this is much more efficient if you want to create extremely large graphs with thousands of timesteps (i.e., replace many simple ops with 1 op in the graph).\n\nNew kernels:\nLSTMCellBlock, LSTMCellBlockGrad, LSTMBlock, LSTMBlockGrad\n\nNotes:\nOn the GPU, performance delta varies between Eigen::Contraction and CUBLAS as a function of batch_size (and to a lesser extent cell_size). Recommend use power of two as usual for cell_size and batch_size.\n\nI did not add the peephole (diagonal) connections, this is trivial and only a few extra lines of C++ code if really desired.\n\nBenchmark results on Titan X... I changed the PTB script to use 1 layer LSTM and batch size of 32, otherwise everything same as the original ptb_word_lm.py\nBasicLSTMCell 1layer PTB:\n0.007 perplexity: 61.504 speed: 18630 wps\n0.107 perplexity: 44.461 speed: 22189 wps\n0.207 perplexity: 42.393 speed: 22336 wps\n0.306 perplexity: 41.897 speed: 22280 wps\n0.406 perplexity: 41.164 speed: 22235 wps\n0.506 perplexity: 40.266 speed: 22133 wps\n0.606 perplexity: 39.321 speed: 22157 wps\n0.706 perplexity: 37.898 speed: 22178 wps\n0.806 perplexity: 36.951 speed: 22151 wps\n0.906 perplexity: 36.175 speed: 22141 wps\nEpoch: 13 Train Perplexity: 35.804\nEpoch: 13 Valid Perplexity: 122.880\nTest Perplexity: 117.428\n\nLSTMCellBlock 1layer PTB:\n0.007 perplexity: 61.092 speed: 26158 wps\n0.107 perplexity: 44.842 speed: 33217 wps\n0.207 perplexity: 42.963 speed: 33537 wps\n0.306 perplexity: 42.496 speed: 33671 wps\n0.406 perplexity: 41.753 speed: 33727 wps\n0.506 perplexity: 40.867 speed: 33763 wps\n0.606 perplexity: 39.931 speed: 33787 wps\n0.706 perplexity: 38.443 speed: 33805 wps\n0.806 perplexity: 37.481 speed: 33817 wps\n0.906 perplexity: 36.692 speed: 33820 wps\nEpoch: 13 Train Perplexity: 36.290\nEpoch: 13 Valid Perplexity: 123.468\nTest Perplexity: 117.972\n\nAcknowledgements: lots of discussions w/ ebrevdo!\n", "comments": ["Can one of the admins verify this patch?\n", "Very cool!  I'll let Eugene look at this in detail but:\n\n1) Tests need to be written.  See how the other LSTM composition ops are tested and you'll get a sense of what we require\n\n2) Documentation: none of your code or methods have any documentation.\n\n3) I don't see any .cu.cc files, so I'm not sure how anything is running on the GPU -- are you sure you haven't missed including a bunch of files?\n", "4) It would be nice if this interface works well with the cudnn LSTM interface in CuDNN r5.  What would be needed to get that working? \n", "Also, let us know how we can help you to get bazel to work -- it's going to be a painful process if you're unable to test  locally.\n", "@vrv  are bazel tests\\* supposed to run outside of Google? I can't get them to run at all on my local machine, not sure if its my env.\n", "Yes, they can and should :)\n", "You should just be able to say `bazel test //tensorflow/...` anywhere inside the code tree and have the tests run. What's the errors you're seeing?\n", "The most common hiccup for I've seen for a failing `bazel test` is not having the google/protobuf files copied (ie not cloning the repository with --recurse-submodules)\n", "ive figured out the test, tests coming real soon... as soon as my comcast internet comes back online...\n\n> On Apr 19, 2016, at 1:15 AM, Sam Abrahams notifications@github.com wrote:\n> \n> The most common hiccup for I've seen for a failing bazel test is not having the google/protobuf files copied (ie not cloning the repository with --recurse-submodules)\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n", "(some) tests added!\n", "im thinking of changing the interface, i.e., its weight matrix compatible w the existing BasicLSTMCell, so maybe add a new flag to BasicLSTMCell(..., use_monolithic=False) something like that? that way later on we can just swap the flag to default True and everyone get the goodies of a monolithic op?\n", "That would be nice, although that particular interface may not be eternally unchanging, @ebrevdo, what's the last status on the layers/RNN work?\n\nI'd also like to understand what @vrv asked -- if we wanted to use cuDNN5's LSTM functionality, how hard would that be from where you are now?\n", "I'm tempted to replace the current BasicLSTMCell with this implementation; so long as you can get unit tests that show that the results are numerically identical w/in machine epsilon for fw and bprop.\n\nOne thing to keep in mind: we are considering adding back support for split states, where state_size() is allowed to return a tuple; and in this case, each **call** is allowed to accept and return a tuple of states.\n", "If we replace the implementation with a monolithic op, it'll be under the covers, not exposed directly.\n", "@ebrevdo on the note of numerical precision comparison ... done! see the tests, it compares the results w the existing for the fprop and bprop (i.e., using build graph of BasicLSTMCell and build graph of the monolithic op, run graphs + compare results... i.e., i basically copied to rnn dynamic vs static test that u wrote).\n\nhowever the test doesn't compare the states, my monolithic state is a bit different than the existing state, i.e., my state contains all the gate activations as well as the c and the h, while iirc the BasicLSTMCell state only has the c and the h. i did compare the state in the fprop (see test in the rnn_cell_test, but not the grad for the backprop (at least not explicitly, implicitly we do via the other grads).\n", "@vrv , @martinwicke warning: i only looked at cuDNN once so i could be wrong, but iirc, cuDNN requires a workspace scratch for the fprop and bprop, meaning u need to allocate memory in the fprop to pass back thru in the backprop:\ncudnnGetRNNWorkspaceSize\nwhich i presume we can just treat it a state.\n\nhowever, im not familiar enuf w the TF infrastructure on how to pass the state/scratch back during the backward pass, since we don't know the function behind cudnnGetRNNWorkspaceSize and thus we can't guarantee the python side will know how much memory to allocate for the state_size?\n", "@wchan would it make sense to add the batch normalization option to your blocks see: #1736 ? Would love to try it out together!\n", "That would not be appropriate to implement in the Blocks C++ ops; as this\ncontains enough complexity in one fused op that it's already a stretch to\nget it merged into core TF.\n\nOn Wed, Apr 20, 2016 at 1:14 AM, Dr. Kashif Rasul notifications@github.com\nwrote:\n\n> @wchan https://github.com/wchan would it make sense to add the batch\n> normalization option to your blocks see: #1736\n> https://github.com/tensorflow/tensorflow/issues/1736 ? Would love to\n> try it out together!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-212317863\n", "We'd like to make sure this is compatible with cudnn's LSTM implementation and API, but that will take some time to test and validate, and this is an awesome op to have.  So in the meantime, can you move this code to tensorflow/contrib/rnn/ ?\n\nSee some of the other examples of custom C++ ops in contrib/ for how to pattern your files and tests and imports.   https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/linear_optimizer is one example.\n\nThanks!\n", "I agree cuDNN is important, but that I think it should be independent?\ni.e., this is important for CPU platforms as well.\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Wed, Apr 20, 2016 at 6:05 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> We'd like to make sure this is compatible with cudnn's LSTM implementation\n> and API, but that will take some time to test and validate, and this is an\n> awesome op to have. So in the meantime, can you move this code to\n> tensorflow/contrib/rnn/ ?\n> \n> See some of the other examples of custom C++ ops in contrib/ for how to\n> pattern your files and tests and imports. Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-212676765\n", "From an interface point of view, it's not totally independent.  Like I said, we'd be happy to get this in first via contrib/.\n", "I think to add the cuDNN LSTM we need to modify / add the capability to the stream_executor. Is something that google is going to do or can we provide patches? \n[The LSTM kernel written by Nvidia seems to have quite a few optimizations](https://devblogs.nvidia.com/parallelforall/optimizing-recurrent-neural-networks-cudnn-5/).\n\nI also have a feeling that this LSTM block is very specialized to be part of tensorflow's API. \nIs there a way that a user could load this operation as a \"plug-in\" ? \n", "You can provide patches!  Yes, you can write a custom op that does this and add it to tensorflow/contrib like I mentioned above, and we'd be happy to add it.\n", "Sorry, I'll be away for a while.  Assigning @vrv to overlook the rest of the review (or triage reassignment).\n", "@vrv for refactoring to contrib, did you just want to refactor the python interface? or also the ops / kernels as well?\n", "I'd like all the code to be in contrib for now and we'll later do the work of moving into the core.\n", "@vrv, i tried following the other examples to refactor into contrib but im hitting a wall...\n\nIt compiles properly, but during runtime I cant find my op :(\n\nRuntimeError: Op type not registered 'LSTMCellBlock'\n\nany ideas, this is supposed to be registered in the contrib/rnn/kernels/lstm_ops.cc file, but somehow its not being loaded in the test? Im guessing this is more of a blaze BUILD rule rather than src code error? thanks!\n\ncommand im running:\n\nbazel test -c opt --config=cuda //tensorflow/contrib/rnn:all\n", "You may be missing a step from the [instructions here](https://www.tensorflow.org/versions/r0.8/how_tos/adding_an_op/index.html).\n", "hmm... i think im missing the:\n_lstm_ops = load_op_library(resource_loader.get_path_to_datafile(\n    '_lstm_ops.so'))\nassert _lstm_ops, 'Could not load _lstm_ops.so'\n\nhowever, when i do that i get:\nE tensorflow/stream_executor/cuda/cuda_blas.cc:2209] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:1559] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nF tensorflow/stream_executor/cuda/cuda_platform.cc:180] Check failed: ::perftools::gputools::port::Status::OK() == (MultiPlatformManager::RegisterPlatform(std::move(platform))) (OK vs. Internal: platform is already registered with name: \"CUDA\") \n", "@keveman load_library strikes again, help?\n", "I've refactored the code into contrib, the tests have been updated as well. PTAL, let me know what else needs to be done for this PR to be approved. Thanks all!\n", "@wchan I noticed that `nvcc (7.5)` was taking a really long time to compile `lstm_ops_gpu.cu.cc`. I don't have enough cycles today to do a detailed breakdown, but do you mind reporting the compilation time for that file?\n", "@keveman i've added a new tf_custom_kernel_library PTAL, is that the correct solution? if i don't have the tf_custom_kernel_library, i can't put the .cc file w/o putting it into the gpu_srcs\n", "@keveman , FYI, had to add\n\ncopts = if_cuda([\"-DGOOGLE_CUDA=1\"]),\n\nto the cc_binary in tensorflow.bzl or else the GPU kernels wouldn't register. is that correct, or should i move the register code into the .cu.cc file?\n", "Yep, that's good.\n\nOn Thu, Apr 28, 2016, 10:57 PM William Chan notifications@github.com\nwrote:\n\n> @keveman https://github.com/keveman , FYI, had to add\n> \n> copts = if_cuda([\"-DGOOGLE_CUDA=1\"]),\n> \n> to the cc_binary in tensorflow.bzl or else the GPU kernels wouldn't\n> register. is that correct, or should i move the register code into the\n> .cu.cc file?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-215632593\n", "Is there a way to detect in the python autodiff graph which input vars need to be differentiated? i.e., in the monolithic op, even if we only care about dw/db, we still always compute dx even though it may not be needed (i.e., 1 layer RNN, only need dw dont need to bprop the dx or when x is a constant).\n\nI've created an attr that can control this behaviour, but ideally, this can be detected by the python graph to pass to the op to avoid computing the dx when its not necessary.\n", "OK... New tests+benchmarks added! Rewrote the op to allow more parallelism. Compared earlier version, it now uses more memory but is faster. Simplified the kernel code quite a bit too.\n\nWhen \"parallel_dw == True\" (option in the op), its pretty much faster across the board on an variety of minibatches / cell_size configurations. On certain small (but common) minibatch sizes (i.e., 32/64), we can get double the speed. WOOT!\n\nSee below for benchmarks (benchmarks shamelessly mirrored from existing python/kernel_tests/rnn_test.py).\n\nCalculation: Static Unroll with Basic LSTM vs. Block LSTM\nbatch    max_t   units   gpu   parallel_dw   dt(basic)   dt(block)   dt(basic)/dt(block)\n512    50    512   False   True    1.608925    1.595009    0.991351\n512    50    512   False   False   1.622124    1.652539    1.018750\n512    50    512   True    True    0.105598    0.100758    0.954168\n512    50    512   True    False   0.105723    0.110407    1.044307\n512    50    256   False   True    0.570323    0.557588    0.977670\n512    50    256   False   False   0.548141    0.565626    1.031899\n512    50    256   True    True    0.051169    0.041841    0.817712\n512    50    256   True    False   0.049043    0.049948    1.018455\n512    50    128   False   True    0.255528    0.223692    0.875412\n512    50    128   False   False   0.265746    0.230062    0.865724\n512    50    128   True    True    0.035591    0.025530    0.717325\n512    50    128   True    False   0.039220    0.030153    0.768819\n256    50    512   False   True    0.914745    0.912568    0.997620\n256    50    512   False   False   0.898728    0.957362    1.065242\n256    50    512   True    True    0.065084    0.061613    0.946667\n256    50    512   True    False   0.065379    0.066195    1.012489\n256    50    256   False   True    0.345520    0.321437    0.930302\n256    50    256   False   False   0.339179    0.331827    0.978325\n256    50    256   True    True    0.045273    0.030890    0.682309\n256    50    256   True    False   0.043981    0.035227    0.800952\n256    50    128   False   True    0.189186    0.148082    0.782732\n256    50    128   False   False   0.193971    0.150826    0.777567\n256    50    128   True    True    0.035081    0.022858    0.651583\n256    50    128   True    False   0.039035    0.024608    0.630408\n128    50    512   False   True    0.571939    0.545305    0.953431\n128    50    512   False   False   0.554707    0.597802    1.077690\n128    50    512   True    True    0.052550    0.046426    0.883464\n128    50    512   True    False   0.053724    0.049175    0.915331\n128    50    256   False   True    0.235361    0.207240    0.880520\n128    50    256   False   False   0.231161    0.224550    0.971402\n128    50    256   True    True    0.037826    0.027088    0.716106\n128    50    256   True    False   0.034500    0.028032    0.812499\n128    50    128   False   True    0.153420    0.112691    0.734525\n128    50    128   False   False   0.139047    0.134652    0.968391\n128    50    128   True    True    0.042392    0.023966    0.565348\n128    50    128   True    False   0.041555    0.021308    0.512771\n64   50    512   False   True    0.404943    0.384559    0.949663\n64   50    512   False   False   0.403185    0.450789    1.118070\n64   50    512   True    True    0.048875    0.037370    0.764596\n64   50    512   True    False   0.048246    0.038403    0.795976\n64   50    256   False   True    0.179140    0.155737    0.869363\n64   50    256   False   False   0.172691    0.176327    1.021057\n64   50    256   True    True    0.036864    0.024936    0.676441\n64   50    256   True    False   0.036052    0.024993    0.693251\n64   50    128   False   True    0.147388    0.122999    0.834523\n64   50    128   False   False   0.123700    0.122942    0.993868\n64   50    128   True    True    0.039172    0.020994    0.535938\n64   50    128   True    False   0.037731    0.018184    0.481955\n32   50    512   False   True    0.321763    0.303199    0.942308\n32   50    512   False   False   0.313588    0.384228    1.225262\n32   50    512   True    True    0.039348    0.032741    0.832085\n32   50    512   True    False   0.041097    0.033916    0.825274\n32   50    256   False   True    0.156073    0.135813    0.870190\n32   50    256   False   False   0.155621    0.171671    1.103136\n32   50    256   True    True    0.040236    0.025395    0.631153\n32   50    256   True    False   0.049391    0.024777    0.501641\n32   50    128   False   True    0.144160    0.111829    0.775730\n32   50    128   False   False   0.136285    0.118303    0.868056\n32   50    128   True    True    0.045720    0.024474    0.535300\n32   50    128   True    False   0.046344    0.023173    0.500020\n16   50    512   False   True    0.264227    0.244503    0.925353\n16   50    512   False   False   0.267688    0.349579    1.305921\n16   50    512   True    True    0.045465    0.031925    0.702196\n16   50    512   True    False   0.041305    0.032435    0.785265\n16   50    256   False   True    0.155288    0.120286    0.774599\n16   50    256   False   False   0.142004    0.181369    1.277210\n16   50    256   True    True    0.035678    0.025734    0.721294\n16   50    256   True    False   0.041158    0.024548    0.596429\n16   50    128   False   True    0.146916    0.117762    0.801561\n16   50    128   False   False   0.108035    0.122011    1.129364\n16   50    128   True    True    0.040217    0.024973    0.620957\n16   50    128   True    False   0.036523    0.022020    0.602924\n", "Perhaps it's worthwhile to not have the parallel_dw option?  Is it much\nhigher memory consumption?\n\nOn Fri, May 6, 2016 at 2:43 PM, William Chan notifications@github.com\nwrote:\n\n> OK... New tests+benchmarks added! Rewrote the op to allow more\n> parallelism. Compared earlier version, it now uses more memory but is\n> faster. Simplified the kernel code quite a bit too.\n> \n> When \"parallel_dw == True\" (option in the op), its pretty much faster\n> across the board on an variety of minibatches / cell_size configurations.\n> On certain small (but common) minibatch sizes (i.e., 32/64), we can get\n> double the speed. WOOT!\n> \n> See below for benchmarks (benchmarks shamelessly mirrored from existing\n> python/kernel_tests/rnn_test.py).\n> \n> Calculation: Static Unroll with Basic LSTM vs. Block LSTM\n> batch max_t units gpu parallel_dw dt(basic) dt(block) dt(basic)/dt(block)\n> 512 50 512 False True 1.608925 1.595009 0.991351\n> 512 50 512 False False 1.622124 1.652539 1.018750\n> 512 50 512 True True 0.105598 0.100758 0.954168\n> 512 50 512 True False 0.105723 0.110407 1.044307\n> 512 50 256 False True 0.570323 0.557588 0.977670\n> 512 50 256 False False 0.548141 0.565626 1.031899\n> 512 50 256 True True 0.051169 0.041841 0.817712\n> 512 50 256 True False 0.049043 0.049948 1.018455\n> 512 50 128 False True 0.255528 0.223692 0.875412\n> 512 50 128 False False 0.265746 0.230062 0.865724\n> 512 50 128 True True 0.035591 0.025530 0.717325\n> 512 50 128 True False 0.039220 0.030153 0.768819\n> 256 50 512 False True 0.914745 0.912568 0.997620\n> 256 50 512 False False 0.898728 0.957362 1.065242\n> 256 50 512 True True 0.065084 0.061613 0.946667\n> 256 50 512 True False 0.065379 0.066195 1.012489\n> 256 50 256 False True 0.345520 0.321437 0.930302\n> 256 50 256 False False 0.339179 0.331827 0.978325\n> 256 50 256 True True 0.045273 0.030890 0.682309\n> 256 50 256 True False 0.043981 0.035227 0.800952\n> 256 50 128 False True 0.189186 0.148082 0.782732\n> 256 50 128 False False 0.193971 0.150826 0.777567\n> 256 50 128 True True 0.035081 0.022858 0.651583\n> 256 50 128 True False 0.039035 0.024608 0.630408\n> 128 50 512 False True 0.571939 0.545305 0.953431\n> 128 50 512 False False 0.554707 0.597802 1.077690\n> 128 50 512 True True 0.052550 0.046426 0.883464\n> 128 50 512 True False 0.053724 0.049175 0.915331\n> 128 50 256 False True 0.235361 0.207240 0.880520\n> 128 50 256 False False 0.231161 0.224550 0.971402\n> 128 50 256 True True 0.037826 0.027088 0.716106\n> 128 50 256 True False 0.034500 0.028032 0.812499\n> 128 50 128 False True 0.153420 0.112691 0.734525\n> 128 50 128 False False 0.139047 0.134652 0.968391\n> 128 50 128 True True 0.042392 0.023966 0.565348\n> 128 50 128 True False 0.041555 0.021308 0.512771\n> 64 50 512 False True 0.404943 0.384559 0.949663\n> 64 50 512 False False 0.403185 0.450789 1.118070\n> 64 50 512 True True 0.048875 0.037370 0.764596\n> 64 50 512 True False 0.048246 0.038403 0.795976\n> 64 50 256 False True 0.179140 0.155737 0.869363\n> 64 50 256 False False 0.172691 0.176327 1.021057\n> 64 50 256 True True 0.036864 0.024936 0.676441\n> 64 50 256 True False 0.036052 0.024993 0.693251\n> 64 50 128 False True 0.147388 0.122999 0.834523\n> 64 50 128 False False 0.123700 0.122942 0.993868\n> 64 50 128 True True 0.039172 0.020994 0.535938\n> 64 50 128 True False 0.037731 0.018184 0.481955\n> 32 50 512 False True 0.321763 0.303199 0.942308\n> 32 50 512 False False 0.313588 0.384228 1.225262\n> 32 50 512 True True 0.039348 0.032741 0.832085\n> 32 50 512 True False 0.041097 0.033916 0.825274\n> 32 50 256 False True 0.156073 0.135813 0.870190\n> 32 50 256 False False 0.155621 0.171671 1.103136\n> 32 50 256 True True 0.040236 0.025395 0.631153\n> 32 50 256 True False 0.049391 0.024777 0.501641\n> 32 50 128 False True 0.144160 0.111829 0.775730\n> 32 50 128 False False 0.136285 0.118303 0.868056\n> 32 50 128 True True 0.045720 0.024474 0.535300\n> 32 50 128 True False 0.046344 0.023173 0.500020\n> 16 50 512 False True 0.264227 0.244503 0.925353\n> 16 50 512 False False 0.267688 0.349579 1.305921\n> 16 50 512 True True 0.045465 0.031925 0.702196\n> 16 50 512 True False 0.041305 0.032435 0.785265\n> 16 50 256 False True 0.155288 0.120286 0.774599\n> 16 50 256 False False 0.142004 0.181369 1.277210\n> 16 50 256 True True 0.035678 0.025734 0.721294\n> 16 50 256 True False 0.041158 0.024548 0.596429\n> 16 50 128 False True 0.146916 0.117762 0.801561\n> 16 50 128 False False 0.108035 0.122011 1.129364\n> 16 50 128 True True 0.040217 0.024973 0.620957\n> 16 50 128 True False 0.036523 0.022020 0.602924\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-217565864\n", "(i.e., always have it \"True\") via the split ops\n\nOn Fri, May 6, 2016 at 2:47 PM, Eugene Brevdo ebrevdo@gmail.com wrote:\n\n> Perhaps it's worthwhile to not have the parallel_dw option?  Is it much\n> higher memory consumption?\n> \n> On Fri, May 6, 2016 at 2:43 PM, William Chan notifications@github.com\n> wrote:\n> \n> > OK... New tests+benchmarks added! Rewrote the op to allow more\n> > parallelism. Compared earlier version, it now uses more memory but is\n> > faster. Simplified the kernel code quite a bit too.\n> > \n> > When \"parallel_dw == True\" (option in the op), its pretty much faster\n> > across the board on an variety of minibatches / cell_size configurations.\n> > On certain small (but common) minibatch sizes (i.e., 32/64), we can get\n> > double the speed. WOOT!\n> > \n> > See below for benchmarks (benchmarks shamelessly mirrored from existing\n> > python/kernel_tests/rnn_test.py).\n> > \n> > Calculation: Static Unroll with Basic LSTM vs. Block LSTM\n> > batch max_t units gpu parallel_dw dt(basic) dt(block) dt(basic)/dt(block)\n> > 512 50 512 False True 1.608925 1.595009 0.991351\n> > 512 50 512 False False 1.622124 1.652539 1.018750\n> > 512 50 512 True True 0.105598 0.100758 0.954168\n> > 512 50 512 True False 0.105723 0.110407 1.044307\n> > 512 50 256 False True 0.570323 0.557588 0.977670\n> > 512 50 256 False False 0.548141 0.565626 1.031899\n> > 512 50 256 True True 0.051169 0.041841 0.817712\n> > 512 50 256 True False 0.049043 0.049948 1.018455\n> > 512 50 128 False True 0.255528 0.223692 0.875412\n> > 512 50 128 False False 0.265746 0.230062 0.865724\n> > 512 50 128 True True 0.035591 0.025530 0.717325\n> > 512 50 128 True False 0.039220 0.030153 0.768819\n> > 256 50 512 False True 0.914745 0.912568 0.997620\n> > 256 50 512 False False 0.898728 0.957362 1.065242\n> > 256 50 512 True True 0.065084 0.061613 0.946667\n> > 256 50 512 True False 0.065379 0.066195 1.012489\n> > 256 50 256 False True 0.345520 0.321437 0.930302\n> > 256 50 256 False False 0.339179 0.331827 0.978325\n> > 256 50 256 True True 0.045273 0.030890 0.682309\n> > 256 50 256 True False 0.043981 0.035227 0.800952\n> > 256 50 128 False True 0.189186 0.148082 0.782732\n> > 256 50 128 False False 0.193971 0.150826 0.777567\n> > 256 50 128 True True 0.035081 0.022858 0.651583\n> > 256 50 128 True False 0.039035 0.024608 0.630408\n> > 128 50 512 False True 0.571939 0.545305 0.953431\n> > 128 50 512 False False 0.554707 0.597802 1.077690\n> > 128 50 512 True True 0.052550 0.046426 0.883464\n> > 128 50 512 True False 0.053724 0.049175 0.915331\n> > 128 50 256 False True 0.235361 0.207240 0.880520\n> > 128 50 256 False False 0.231161 0.224550 0.971402\n> > 128 50 256 True True 0.037826 0.027088 0.716106\n> > 128 50 256 True False 0.034500 0.028032 0.812499\n> > 128 50 128 False True 0.153420 0.112691 0.734525\n> > 128 50 128 False False 0.139047 0.134652 0.968391\n> > 128 50 128 True True 0.042392 0.023966 0.565348\n> > 128 50 128 True False 0.041555 0.021308 0.512771\n> > 64 50 512 False True 0.404943 0.384559 0.949663\n> > 64 50 512 False False 0.403185 0.450789 1.118070\n> > 64 50 512 True True 0.048875 0.037370 0.764596\n> > 64 50 512 True False 0.048246 0.038403 0.795976\n> > 64 50 256 False True 0.179140 0.155737 0.869363\n> > 64 50 256 False False 0.172691 0.176327 1.021057\n> > 64 50 256 True True 0.036864 0.024936 0.676441\n> > 64 50 256 True False 0.036052 0.024993 0.693251\n> > 64 50 128 False True 0.147388 0.122999 0.834523\n> > 64 50 128 False False 0.123700 0.122942 0.993868\n> > 64 50 128 True True 0.039172 0.020994 0.535938\n> > 64 50 128 True False 0.037731 0.018184 0.481955\n> > 32 50 512 False True 0.321763 0.303199 0.942308\n> > 32 50 512 False False 0.313588 0.384228 1.225262\n> > 32 50 512 True True 0.039348 0.032741 0.832085\n> > 32 50 512 True False 0.041097 0.033916 0.825274\n> > 32 50 256 False True 0.156073 0.135813 0.870190\n> > 32 50 256 False False 0.155621 0.171671 1.103136\n> > 32 50 256 True True 0.040236 0.025395 0.631153\n> > 32 50 256 True False 0.049391 0.024777 0.501641\n> > 32 50 128 False True 0.144160 0.111829 0.775730\n> > 32 50 128 False False 0.136285 0.118303 0.868056\n> > 32 50 128 True True 0.045720 0.024474 0.535300\n> > 32 50 128 True False 0.046344 0.023173 0.500020\n> > 16 50 512 False True 0.264227 0.244503 0.925353\n> > 16 50 512 False False 0.267688 0.349579 1.305921\n> > 16 50 512 True True 0.045465 0.031925 0.702196\n> > 16 50 512 True False 0.041305 0.032435 0.785265\n> > 16 50 256 False True 0.155288 0.120286 0.774599\n> > 16 50 256 False False 0.142004 0.181369 1.277210\n> > 16 50 256 True True 0.035678 0.025734 0.721294\n> > 16 50 256 True False 0.041158 0.024548 0.596429\n> > 16 50 128 False True 0.146916 0.117762 0.801561\n> > 16 50 128 False False 0.108035 0.122011 1.129364\n> > 16 50 128 True True 0.040217 0.024973 0.620957\n> > 16 50 128 True False 0.036523 0.022020 0.602924\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-217565864\n", "right. right now the default is parallel_dw == True, but we should probably remove the option.\n", "Is Eigen Contraction supposed to support doubles?\nGetting a bunch of these errors w/ doubles.\n\nptxas error   : Entry function '_ZN5Eigen22EigenContractionKernelIdlNS_8internal28TensorContractionInputMapperIdlLi1ENS_15TensorEvaluatorIKNS_9TensorMapINS_6TensorIKdLi2ELi1ElEELi16EEENS_9GpuDeviceEEENS_5arrayIlLm1EEESD_Li4ELb0ELb0ELi0EEENS2_IdlLi0ENS3_IKNS4_INS5_IdLi2ELi1ElEELi16EEESA_EESD_SD_Li4ELb1ELb0ELi0EEENS1_16blas_data_mapperIdlLi0ELi0EEEEEvT1_T2_T3_T0_SP_SP_' uses too much shared data (0x12000 bytes, 0xc000 max)\nptxas error   : Entry function '_ZN5Eigen22EigenContractionKernelIdlNS_8internal28TensorContractionInputMapperIdlLi1ENS_15TensorEvaluatorIKNS_9TensorMapINS_6TensorIKdLi2ELi1ElEELi16EEENS_9GpuDeviceEEENS_5arrayIlLm1EEESD_Li4ELb1ELb0ELi0EEENS2_IdlLi0ENS3_IKNS4_INS5_IdLi2ELi1ElEELi16EEESA_EESD_SD_Li4ELb1ELb0ELi0EEENS1_16blas_data_mapperIdlLi0ELi0EEEEEvT1_T2_T3_T0_SP_SP_' uses too much shared data (0x12000 bytes, 0xc000 max)\nptxas error   : Entry function '_ZN5Eigen22EigenContractionKernelIdlNS_8internal28TensorContractionInputMapperIdlLi1ENS_15TensorEvaluatorIKNS_9TensorMapINS_6TensorIKdLi2ELi1ElEELi16EEENS_9GpuDeviceEEENS_5arrayIlLm1EEESD_Li4ELb0ELb0ELi0EEENS2_IdlLi0ENS3_IKNS4_INS5_IdLi2ELi1ElEELi16EEESA_EESD_SD_Li4ELb0ELb0ELi0EEENS1_16blas_data_mapperIdlLi0ELi0EEEEEvT1_T2_T3_T0_SP_SP_' uses too much shared data (0x12000 bytes, 0xc000 max)\nptxas error   : Entry function '_ZN5Eigen22EigenContractionKernelIdlNS_8internal28TensorContractionInputMapperIdlLi1ENS_15TensorEvaluatorIKNS_9TensorMapINS_6TensorIKdLi2ELi1ElEELi16EEENS_9GpuDeviceEEENS_5arrayIlLm1EEESD_Li4ELb1ELb0ELi0EEENS2_IdlLi0ENS3_IKNS4_INS5_IdLi2ELi1ElEELi16EEESA_EESD_SD_Li4ELb0ELb0ELi0EEENS1_16blas_data_mapperIdlLi0ELi0EEEEEvT1_T2_T3_T0_SP_SP_' uses too much shared data (0x12000 bytes, 0xc000 max)\nptxas error   : Entry function '_ZN5Eigen22EigenContractionKernelIdlNS_8internal28TensorContractionInputMapperIdlLi1ENS_15TensorEvaluatorIKNS_9TensorMapINS_6TensorIKdLi2ELi1ElEELi16EEENS_9GpuDeviceEEENS_5arrayIlLm1EEESD_Li4ELb0ELb0ELi0EEENS2_IdlLi0ENS3_IKNS4_INS5_IdLi2ELi1ElEELi16EEESA_EESD_SD_Li4ELb0ELb1ELi0EEENS1_16blas_data_mapperIdlLi0ELi0EEEEEvT1_T2_T3_T0_SP_SP_' uses too much shared data (0x12000 bytes, 0xc000 max)\nptxas error   : Entry function '_ZN5Eigen22EigenContractionKernelIdlNS_8internal28TensorContractionInputMapperIdlLi1ENS_15TensorEvaluatorIKNS_9TensorMapINS_6TensorIKdLi2ELi1ElEELi16EEENS_9GpuDeviceEEENS_5arrayIlLm1EEESD_Li4ELb1ELb0ELi0EEENS2_IdlLi0ENS3_IKNS4_INS5_IdLi2ELi1ElEELi16EEESA_EESD_SD_Li4ELb0ELb1ELi0EEENS1_16blas_data_mapperIdlLi0ELi0EEEEEvT1_T2_T3_T0_SP_SP_' uses too much shared data (0x12000 bytes, 0xc000 max)\nptxas error   : Entry function '_ZN5Eigen22EigenContractionKernelIdlNS_8internal28TensorContractionInputMapperIdlLi1ENS_15TensorEvaluatorIKNS_9TensorMapINS_6TensorIKdLi2ELi1ElEELi16EEENS_9GpuDeviceEEENS_5arrayIlLm1EEESD_Li4ELb0ELb0ELi0EEENS2_IdlLi0ENS3_IKNS4_INS5_IdLi2ELi1ElEELi16EEESA_EESD_SD_Li4ELb1ELb1ELi0EEENS1_16blas_data_mapperIdlLi0ELi0EEEEEvT1_T2_T3_T0_SP_SP_' uses too much shared data (0x12000 bytes, 0xc000 max)\nptxas error   : Entry function '_ZN5Eigen22EigenContractionKernelIdlNS_8internal28TensorContractionInputMapperIdlLi1ENS_15TensorEvaluatorIKNS_9TensorMapINS_6TensorIKdLi2ELi1ElEELi16EEENS_9GpuDeviceEEENS_5arrayIlLm1EEESD_Li4ELb1ELb0ELi0EEENS2_IdlLi0ENS3_IKNS4_INS5_IdLi2ELi1ElEELi16EEESA_EESD_SD_Li4ELb1ELb1ELi0EEENS1_16blas_data_mapperIdlLi0ELi0EEEEEvT1_T2_T3_T0_SP_SP_' uses too much shared data (0x12000 bytes, 0xc000 max)\n", "Should be fine.  Thought you were using cublas for contractions?  @benoitsteiner @rmlarsen \n", "fyi, i found a workaround.\n", "@wchan  -- really appreciate your work here. I think its going to benefit many.\n\nJust to voice @kashif concerns, would it make sense to integrate batch norm into the lstm as a flag? \n\nIf the goal is to increase the network's speed to convergence, this paper shows that batch normalization can make a remarkable difference especially for multiple stacked layers. \n\nhttps://arxiv.org/abs/1510.01378\n\ntorch implementation here:\nhttps://github.com/iassael/torch-bnlstm\n", "@LeavesBreathe even if we do it, i'd like it in a separate pull request, not this one.\n", "What's the current status of this?  ready to review and test and merge?  \n", "I've removed the sequence block op (and related tests), this should simplify the CL and it wasn't immediately clear to me which input format to use if we did have a sequence op (i.e., there are certain advantages to using a list of 2-d tensors rather than a 3-dim tensor w/ time as dim). PTAL.\n", "Okay, once Eugene gives the LGTM we'll test and merge.\n", "A few more comments.\n", "Jenkins, test this please.\n", "Looks like tests are failing..\n", "(46 / 260) Python test-on-install PASSED (2160 ms):\ntensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py\n(47 / 260) Python test-on-install FAILED (1331 ms):\ntensorflow/contrib/rnn/python/kernel_tests/rnn_test.py\n  Log @: /workspace/pip_test/tests/logs/tensorflow/contrib/rnn/python/kernel_tests/rnn_test.py.log\n============== BEGINS failure log content ==============\nTraceback (most recent call last):\n  File \"/workspace/pip_test/tests/rnn_test.py\", line 14, in <module>\n    from tensorflow.python.kernel_tests import rnn_test\nImportError: cannot import name rnn_test\n============== ENDS failure log content ==============\n\ndebugging... for some reason it imports properly on my local machine...\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Wed, May 11, 2016 at 10:27 PM, ebrevdo notifications@github.com wrote:\n\n> Looks like tests are failing..\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-218663245\n", "Jenkins, test this please.\n", "I am implementing the cudnn RNN operations and I based my work on top of this patch, so I have been lurking on this thread. I saw that the latest patches remove the block LSTM and just have a single cell. One of the speeding factor of the Cudnn implementation is the pipelining of operations when running multiple cells in sequence. I was going with a RNNBlock operation similar to the LSTMBlock operation, but now if that is gone, is still this a good strategy to have multiple execution steps in one operation? \n", "@Mistobaan, the prev code is here:\nhttps://github.com/wchan/tensorflow/tree/lstm_block\n\nYes, as you mentioned, the cuDNN op requires a stacked sequence (rather than 1 time step). I think there's a few tricky things that need to be done before we can call the cuDNN API, namely it requires some memory to be shared between the fprop and bprop, and the documentation doesn't tell us how much memory is allocated. There are several hacks to accomplish this, but I'm not sure if they are \"correct\" to do.\n\nAlso, some things to consider is as @ebrevdo mentioned, if we do have a sequence RNN op, do we want a list of 2d-tensors as input or list of 3d-tensors as input. I forget the layout of the cuDNN LSTM API, so we might want to match that too, which might not match the layout of this op.\n", "I feel like jenkins isn't actually testing my code... do I need to be whitelisted or something to get jenkins to test when i do \"Jenkins, test this please.\"?\n", "@wchan \n- yes that is actually true, I thought that you could create the Back Operator from the Forward Operator and thus passing some reference from it.  My work in progress code is in my branch [feature/cudnn-rnn-lstm](https://github.com/Mistobaan/tensorflow/tree/feature/cudnn-rnn-lstm)  \n- yes you need to be whitelisted to actually run the testing. \n", "Jenkins, test this please.\n", "FYI we are also working on cudnn rnn API. It's a bit more tricky than it\nlooks.\n\nOn Thu, May 12, 2016, 12:41 AM Fabrizio Milo notifications@github.com\nwrote:\n\n> @wchan https://github.com/wchan\n> - yes that is actually true, I thought that you could create the Back\n>   Operator from the Forward Operator and thus passing some reference from it.\n>   My work in progress code is in my branch feature/cudnn-rnn-lstm\n>   https://github.com/Mistobaan/tensorflow/tree/feature/cudnn-rnn-lstm\n> - yes you need to be whitelisted to actually run the testing.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-218683138\n", "@vrv oh I erroneously thought from a previous thread that you guys were not working on it. Is the rnn cudnn conversation / WIP code public ? I would like to get involved.\n", "@vrv the status right now is the tests run fine on my desktop, but fails on jenkins... not sure exactly why...\n\nthe tests fail on the python stage where i try and do a import:\nfrom tensorflow.python.kernel_tests import rnn_test\n\nbut its complaining the import is not allowed:\nImportError: cannot import name rnn_test\n\nnot sure why... my local blaze test works fine...\n", "@martinwicke we could use some help with the issue @wchan is experiencing above.\n", "@Mistobaan i don't think so - @zheng-xq for context.\n", "@caisq any ideas why there would be any difference here?\n", "Figured I'd clone and try testing on my rig to give another data point- hopefully it is of some use. Here are the errors I get when trying to run `bazel test` on the current branch:\n\nCPU test\n\n```\n$ bazel test tensorflow/contrib/rnn:rnn_cell_test\n    Error etc.\n$ cat bazel-out/local_linux-fastbuild/testlogs/tensorflow/contrib/rnn/rnn_cell_test/test.log\n\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n-----------------------------------------------------------------------------\n.python: external/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:350: Eigen::TensorEvaluator<const Eigen::TensorCwiseBinaryOp<BinaryOp, LeftArgType, RightArgType>, Device>::TensorEvaluator(const XprType&, const Device&) [with BinaryOp = Eigen::internal::scalar_sum_op<const float>; LeftArgType = const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16>; RightArgType = const Eigen::TensorBroadcastingOp<const Eigen::array<int, 2ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16> >; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorCwiseBinaryOp<BinaryOp, LeftArgType, RightArgType>, Device>::XprType = Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_sum_op<const float>, const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16>, const Eigen::TensorBroadcastingOp<const Eigen::array<int, 2ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16> > >]: Assertion `dimensions_match(m_leftImpl.dimensions(), m_rightImpl.dimensions())' failed.\nexternal/bazel_tools/tools/test/test-setup.sh: line 52: 17295 Aborted                 \"$@\"\n```\n\n---\n\nGPU test\n\n```\n$ bazel test --config=\"cuda\" --verbose_failures tensorflow/contrib/rnn:rnn_cell_test\n\n...\n1 error detected in the compilation of \"/tmp/tmpxft_000021e0_00000000-10_lstm_ops_gpu.cu.compute_52.cpp1.ii\".\nERROR: .../wchan/tensorflow/contrib/rnn/BUILD:16:1: output 'tensorflow/contrib/rnn/_objs/python/ops/_lstm_ops_gpu/tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.pic.o' was not created.\nERROR: .../wchan/tensorflow/contrib/rnn/BUILD:16:1: not all outputs were created.\nTarget //tensorflow/contrib/rnn:rnn_cell_test failed to build\n```\n", "hmm, let me take a look, cause jenkins actually got the rnn_cell_test to\npass (just the rnn_test failed because the python requires rnn_test from\ncore/python and the import isn't working).\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Fri, May 13, 2016 at 5:54 PM, Sam Abrahams notifications@github.com\nwrote:\n\n> Figured I'd clone and try testing on my rig to give another data point-\n> hopefully it is of some use. Here are the errors I get when trying to run bazel\n> test on the current branch:\n> \n> CPU test\n> \n> $ bazel test tensorflow/contrib/rnn:rnn_cell_test\n>     Error etc.\n> $ cat bazel-out/local_linux-fastbuild/testlogs/tensorflow/contrib/rnn/rnn_cell_test/test.log\n> \n> ## exec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n> \n> .python: external/eigen_archive/eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:350: Eigen::TensorEvaluator<const Eigen::TensorCwiseBinaryOp<BinaryOp, LeftArgType, RightArgType>, Device>::TensorEvaluator(const XprType&, const Device&) [with BinaryOp = Eigen::internal::scalar_sum_op<const float>; LeftArgType = const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16>; RightArgType = const Eigen::TensorBroadcastingOp<const Eigen::array<int, 2ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16> >; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorCwiseBinaryOp<BinaryOp, LeftArgType, RightArgType>, Device>::XprType = Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_sum_op<const float>, const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16>, const Eigen::TensorBroadcastingOp<const Eigen:!\n>  :array&lt\n>  ;int, 2ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16> > >]: Assertion `dimensions_match(m_leftImpl.dimensions(), m_rightImpl.dimensions())' failed.\n> external/bazel_tools/tools/test/test-setup.sh: line 52: 17295 Aborted                 \"$@\"\n> \n> ---\n> \n> GPU test\n> \n> $ bazel test --config=\"cuda\" --verbose_failures tensorflow/contrib/rnn:rnn_cell_test\n> \n> ...\n> 1 error detected in the compilation of \"/tmp/tmpxft_000021e0_00000000-10_lstm_ops_gpu.cu.compute_52.cpp1.ii\".\n> ERROR: .../wchan/tensorflow/contrib/rnn/BUILD:16:1: output 'tensorflow/contrib/rnn/_objs/python/ops/_lstm_ops_gpu/tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.pic.o' was not created.\n> ERROR: .../wchan/tensorflow/contrib/rnn/BUILD:16:1: not all outputs were created.\n> Target //tensorflow/contrib/rnn:rnn_cell_test failed to build\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-219190656\n", "What's the status of this?  What is blocking this  cc @wchan @ebrevdo @caisq \n", "Batch Normalization which was mentioned above. I think it is better to implement Recurrent Batch Normalization: http://arxiv.org/pdf/1603.09025v1.pdf\n", "marking this as 'awaiting response' since I don't know what's holding up progress on this\n", "Closing due to inactivity / apparent lack of interest\n", "William is working on this internally.\nOn Jun 7, 2016 5:59 PM, \"Vijay Vasudevan\" notifications@github.com wrote:\n\n> Closed #2002 https://github.com/tensorflow/tensorflow/pull/2002.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#event-685164337, or mute\n> the thread\n> https://github.com/notifications/unsubscribe/ABtimzX8zGVJPDNsPQZIkqXBN6njngvjks5qJhPggaJpZM4IJMkM\n> .\n", "@ebrevdo Can you say when this performance improvement will be merged into master?\n", "@ebrevdo  I think this was merged in https://github.com/tensorflow/tensorflow/commit/e5a1c6a933eeae54ca69bc9eadf54c51f1614519\n\nprecise performance figures of the latest merge would be nice\n", "It looks like there are also options for gridlstm and more:\n\n+### Fused RNNCells\n +@@LSTMFusedCell\n +\n +### LSTM-like cells\n +@@CoupledInputForgetGateLSTMCell\n +@@TimeFreqLSTMCell\n +@@GridLSTMCell\n\nif I understand it correctly, would we call:\n\n`cell = tf.rnn_cell.GridLSTMCell(size)`\n", "The last 2 were just added to documentation (we've had them for a couple of\nmonths).  Look at the source code of GridLSTMCell  and its **init** to see\nhow it works.\n\nWorking on getting performance benchmarks into the code.\n\nOn Mon, Aug 1, 2016 at 6:52 AM, LeavesBreathe notifications@github.com\nwrote:\n\n> It looks like there are also options for gridlstm and more:\n> \n> +### Fused RNNCells\n> +@@LSTMFusedCell\n> +\n> +### LSTM-like cells\n> +@@CoupledInputForgetGateLSTMCell\n> +@@TimeFreqLSTMCell\n> +@@GridLSTMCell\n> \n> if I understand it correctly, would we call:\n> \n> cell = tf.rnn_cell.GridLSTM(size)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-236586085,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimyQOintUUI43GPUc90kCNbzXEzv9ks5qbfoKgaJpZM4IJMkM\n> .\n", "@ebrevdo when I add an optimizer to the a graph with LSTMFusedCell, it gives an error\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'int'\n\nwhen I remove the optimizer and try to perform the forward pass only, it works, but the performance (on GTX 960) is horrible, it is 10 times slower than original LSTM (maybe it was tuned for Titan X ?)\n", "There's a bug atm that it only runs on CPU.... fix coming soon!\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Mon, Aug 1, 2016 at 2:56 PM, ASDen notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo when I add an optimizer to the\n> graph LSTMFusedCell, it gives an error\n> TypeError: unsupported operand type(s) for *: 'NoneType' and 'int'\n> \n> when I remove the optimizer and try only to perform the forward pass, it\n> works, but the performance (on GTX 960) is horrible, it is 10 times slower\n> than original LSTM (maybe it was tuned for Titan X ?)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-236719958,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABFFdKersV2WFy8sjXQqenPQjy2wfXm7ks5qbmuMgaJpZM4IJMkM\n> .\n", "@ebrevdo thanks for letting me know about the `gridlstm` -- wish I had seen this earlier. \n\n@wchan  looking for the gpu version -- big thanks again for such a speed up. \n", "@LeavesBreathe fyi there are two versions of GridLSTM, one in the main rnn_cell file and the other in contrib.\n", "@wchan \"There's a bug atm that it only runs on CPU.... fix coming soon!\"\n\nIt doesn't work with Adam or Momentum Optimizer on CPU too. The same error as @ASDen said above. Is it supposed to work with optimizers?\n", "Ok, I see that FusedLSTM supposes to work with optimizers but there is the following error with stack trace when tf.train.AdamOptimizer(learning_rate).minimize(cost) is executed:\n\nFile \"/Users/vostryakov/projects/senses/env/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/Users/vostryakov/projects/senses/env/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/Users/vostryakov/projects/senses/env/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/Users/vostryakov/projects/senses/env/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/lstm_ops.py\", line 266, in _LSTMFusedCellGrad\n    use_peephole=op.get_attr(\"use_peephole\"))\n  File \"<string>\", line 132, in lstm_fused_cell_grad\n  File \"/Users/vostryakov/projects/senses/env/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/vostryakov/projects/senses/env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2319, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/Users/vostryakov/projects/senses/env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1711, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/Users/vostryakov/projects/senses/env/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/lstm_ops.py\", line 297, in _LSTMFusedCellGradShape\n    tensor_shape.TensorShape([batch_size, cell_size \\* 4]),\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'int'\n", "@ASDen can you provide a gist with the entire traceback?\n\nOn Mon, Aug 1, 2016 at 9:59 PM, Anatoly Vostryakov <notifications@github.com\n\n> wrote:\n> \n> @wchan https://github.com/wchan \"There's a bug atm that it only runs on\n> CPU.... fix coming soon!\"\n> \n> It doesn't work with Adam or Momentum Optimizer on CPU too. The same error\n> as @ASDen https://github.com/ASDen said above. Is it supposed to work\n> with optimizers?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-236798194,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim9PrQsxfNaWDe0IRFLVBZf5lZojfks5qbs6bgaJpZM4IJMkM\n> .\n", "@ebrevdo https://gist.github.com/ASDen/9e77782041f68d15a4cb68fff4fd765f\n", "William, looks like you don't check that cell_size is defined?\n\nOn Aug 2, 2016 9:35 AM, \"ASDen\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo\n> https://gist.github.com/ASDen/9e77782041f68d15a4cb68fff4fd765f\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-236962591,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim2-L3jl0SijZTnlqJ_MTqXPCQ0HIks5qb3HngaJpZM4IJMkM\n> .\n", "@wchan I investigated the bug and the problem is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/lstm_ops.py#L294\n\nFor some reason op.inputs[0] and op.inputs[1] doesn't have a shape during gradient calculations. Is it possible to fix it?\n\nUpdate: After I set cell_size manually in the place above, training started to work correctly and significantly faster. Thank you for Fused LSTM and will wait a fix!\n", "William, do you want to submit a fix for this one?\n"]}, {"number": 2001, "title": "feature request: improve the training speed of GPU", "body": "tensorflow seems to have a good performance on CPU compared to other frameworks, however relatively poor on GPU according to third-party benchmark\n![image](https://cloud.githubusercontent.com/assets/11470826/14587474/6e4106ae-04e6-11e6-9102-0a4e63b2b82f.png)\n![image](https://cloud.githubusercontent.com/assets/11470826/14587481/862ef96a-04e6-11e6-8e43-1bc4ebff7e22.png)\nCould I ask the reason why this happen? I belive Tensorflow is the hottest Deep Learning Framework on github. If we could make the performance of Tensorflow better, it will help a lot of people.\n\nAnyone can help or contribute to this issue? \n", "comments": ["All of those benchmark numbers are quite stale -- things should be much faster, though there's still room for improvement.  If you have a specific benchmark with up to date numbers, we can definitely take a look!\n"]}, {"number": 2000, "title": "Make \"Get Started\" example Python 3 compatible", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1999, "title": "Add confusion matrix to tensorflow", "body": "This MR add a confusion matrix implementation for tensorflow. The function is named confusion_matrix and is a metric operation.\n\nThe function receives three parameters, both predictions and targets labels and an optional num_classes:\n\n``` python\ntf.contrib.metrics.confusion_matrix(predictions, targets, num_classes=None)\n```\n\nIf num_classes is not provided, it will be computed using both predictions and targets parameters.\n\nBoth predictions and targets must be an 1-D array with dtype int32 or int64. This function will then infer the valid labels from both arrays and create a confusion matrix for them. The confusion matrix will always be a square matrix with shape L X L, considering L as the number of valid labels for a given classification task. The dtype of the confusion matrix will be the same as the predictions and target ones.\n\nFor example, given a prediction array: [1, 2, 3, 1], and a target array: [1, 2, 1, 1], the generated confusion matrix would be:\n\n``` python\n[[0 0 0],\n [2 0 0],\n [0 1 0],\n [1 0 0]]\n```\n\nThe rows of the matrix represent the predictions array and the columns, the targets one.\nFinally, this function was implemented only for CPU use. This MR resolves the issue #1606, but there is still work to be done on tensorboard considering this function.\n", "comments": ["Can one of the admins verify this patch?\n", "+1 to this. Nice, simple API, similar to the scipy version.\n", "@lucasmoura you also have to run:\n\n```\nbazel run tensorflow/core/ops/compat:update_ops\n```\n\nto update the `ops_history` file I think\n", "@kashif: no longer, we update the ops docs ourselves now.\n", "@vrv ah cool good to know! thanks\n", "@concretevitamin: Are you working on a out-of-place version of scatter?  I'd prefer that confusion matrix was implemented as a Python wrapper on top of something more fundamental like that.\n", "@girving: not sure if these are exactly what you want, but we have a [ScatterNdAdd functor](https://github.com/tensorflow/tensorflow/blob/cc9bfbf8ef4a3dea6514ad939d238f7442188247/tensorflow/core/kernels/sparse_tensor_dense_add_op.h) and a [SparseTensorDenseAddOp](https://github.com/tensorflow/tensorflow/blob/cc9bfbf8ef4a3dea6514ad939d238f7442188247/tensorflow/core/ops/sparse_ops.cc#L393) now. Will they work for this purpose?\n", "The functor looks good; `SparseTensorDenseAdd` isn't quite the right since you have to pass zeros but otherwise works.  For now, let's implement the confusion matrix on top of `SparseTensorDenseAdd` in Python, maybe with a TODO to use something better if it comes around.\n", "Okay, I will change my implementation to be built on top of SparseTensorDenseAdd\n", "@girving Should I close this merge request ? Since confusion matrix will be just a python wrapper for an existing tensorflow op, the code I have written will no longer be useful, right ?\n", "Would you be interested in modifying your PR instead?  I think it's just a few lines of Python.  I would call it `confusion_matrix`, but otherwise the interface is good.\n", "@girving Wait, I think I misunderstood what I should do. I originally thought that I should reimplement the confusion matrix using only a python wrapper for SparseTensorDenseAdd, but should I left the c++ implementation and add this new python code as well ?\n", "@lucasmoura: Your original interpretation was correct: it's better to remove the C++ implementation and just have the few lines of Python. \n", "@girving Okay, I will start working on it today\n", "@girving Done. But I don't know if the way I have created the values and indexes array is the best approach. If there is a better one, please let me know.\n", "Here's an alternative implementation (details such as name skipped):\n\n```\ndef confusion_matrix(predictions, target, num_classes=None):\n  if num_classes is None:\n    num_classes = tf.max(tf.reduce_max(predictions), tf.reduce_max(target)) + 1\n  shape = tf.pack([num_classes, num_classes])\n  sparse = tf.SparseTensor(values=1, indices=tf.transpose(tf.pack([predictions, target])), shape=shape)\n  return tf.sparse_add(tf.zeros(shape, dtype=tf.int32), sparse)\n```\n\nIt probably has a few bugs, but hopefully it's close and conveys the general idea.\n", "@girving I have tested the code locally and there is a problem in the fact that both sparse_add parameters are not SparseTensors.\n", "@lucasmoura could you paste the errors somewhere?  `tf.sparse_add()` should support both S+S and S+D.\n", "@concretevitamin No problem\n\n``` python\nimport tensorflow as tf\nimport numpy as np\n\npredictions = np.array([1, 2, 3])\ntargets = np.array([3, 2, 1])\nn = 4\nsess = tf.InteractiveSession()\nprint tf.sparse_add(tf.zeros(tf.pack([n, n])), tf.transpose(tf.pack([predictions, targets]))).eval()\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-6-e7c79bd8a1b6> in <module>()\n----> 1 print tf.sparse_add(tf.zeros(tf.pack([n, n])), tf.transpose(tf.pack([predictions, targets]))).eval()\n\n/home/lucas/Envs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.pyc in sparse_add(a, b, thresh)\n    193   \"\"\"\n    194   if not any(isinstance(inp, ops.SparseTensor) for inp in [a, b]):\n--> 195     raise TypeError(\"At least one input should be SparseTensor; do you mean to\"\n    196                     \" use tf.add()?\")\n    197 \n\nTypeError: At least one input should be SparseTensor; do you mean to use tf.add()?\n```\n", "Yeah, in your snippet both operands are passed as dense Tensors.  Geoffrey's snippet actually does Dense + Sparse. \n", "Apologies: I had a broken version first and then edited it to be more\ncorrect.\n\nOn Thu, Apr 21, 2016, 5:50 PM Zongheng Yang notifications@github.com\nwrote:\n\n> Yeah, in your snippet both operands are passed as dense Tensors.\n> Geoffrey's snippet actually does Dense + Sparse.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1999#issuecomment-213184911\n", "@girving I have implemented confusion matrix according to your implementation. Thanks for displaying this approach, I should have used tensorflow built-in functions to implement confusion matrix.\n\nFurthermore, I could not use values=[1] in SparseTensor. It is required that values has the same amount of elements as the first dimension of the indices matrix. \n\nFinally, if there is anything that I can improve in the function or its tests, please let me know.\n", "Thanks!  Can you merge the two commits into one?  Tests for new functionality should be added in the same commit as the functionality.\n", "@girving Done, I have fixed the issues and merged the commits into one.\n", "Excellent!  The only code comment is the names, but you may want to rerun and update the tests.  Then I'll test and merge.\n", "@girving Done, I have fixed both names.\n", "Jenkins, test this please.\n", "Ug, build timed out.  @martinwicke: Should I just retry?\n", "Yes, try again.\n\nOn Fri, Apr 22, 2016 at 1:57 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> Ug, build timed out. @martinwicke https://github.com/martinwicke:\n> Should I just retry?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1999#issuecomment-213586047\n", "Jenkins, test this please yet again.\n", "Weird, I don't see any relation of both build failures with the code I am adding. I will look into the BUILD file I have modified to run the confusion matrix tests to see if it is the cause of the trouble.\n", "@girving Should I keep rebasing this branch or wait for another jenkins build to see the errors ?\n", "Sorry, slow start today.  Unfortunately it looks like the extra rebase destroyed the link to the tests, so I'll rerun them again. :/\n", "Jenkins, test this please.\n", "@girving Sorry for that, I realized that after I have already applied the rebase on my branch. But the errors given by Jenkins are still the same ones as the last run. However, I don't see how my code is affecting the matmul tests.\n", "Thanks!  I don't think that failure is related.\n", "So is this documented in the API? Are the confusion matrix accumulated and updated through multiple testing batches at testing time? Thanks.\n", "@jinghuangzhu: https://www.tensorflow.org/versions/master/api_docs/python/contrib.metrics.html#confusion_matrix\n"]}, {"number": 1998, "title": "wrong step_time calculation in RNN translation", "body": "step_time is the total time for handling a batch of training data. In [translate.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py#L175):\n\n```\n      # Get a batch and make a step.\n      start_time = time.time()\n      encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                   target_weights, bucket_id, False)\n      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n\n```\n\nI just interested to know why the consuming time is divided to FLAGS.steps_per_checkpoints? @lukaszkaiser \n", "comments": ["It was my fault. \nstep_time accumulates the total running times of steps during a checkpoint.\n"]}, {"number": 1997, "title": "Training accuracy falling down after some epoches since upgrading to 0.8.0rc0", "body": "I'm trying to use a simple network to classify some 64*64 images. in v0.7 it worked fine and consistent. The training phase was showing a consistent  average growth in accuracy. But since I'm trying to use 0.8, the accuracy fall down occasionally from ~0.9 to ~0!\nI'm using cpu  version for python 2.7\n\nis it a bug in TF or I'm doing something wrong?\n###### \n### Environment info\n\nOperating System: Ubuntu 14.04\nI've installed pip with the following command : sudo apt-get install python-pip python-dev\n1. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.8.0rc0\n   #############################################\n   ### my learning code is as below:\n\n```\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(tf.float32, shape=[None, 64,64])\ny_ = tf.placeholder(tf.float32, shape=[None, 31])\ninput = tf.reshape(x,shape=[-1,64*64])\nw1 = tf.Variable(tf.random_uniform([64*64,128],minval=-0.1,maxval=0.1),dtype=tf.float32)\nb1 = tf.Variable(tf.random_uniform([128],minval=-0.1,maxval=0.1),dtype=tf.float32);\ny1_ = tf.nn.softmax(tf.matmul(input, w1) + b1)\nkeep_prob = tf.placeholder(tf.float32)\ny1 = tf.nn.dropout(y1_, keep_prob)\nw2 = tf.Variable(tf.random_uniform([128,31],minval=-0.1,maxval=0.1),dtype=tf.float32)\nb2 = tf.Variable(tf.random_uniform([31],minval=-0.1,maxval=0.1),dtype=tf.float32);\ny2 = tf.nn.softmax(tf.matmul(y1, w2) + b2)\ncross_entropy = -tf.reduce_sum(y_*tf.log(y2))\nlearn_rate = tf.placeholder(tf.float32)\ntrain_step = tf.train.AdamOptimizer(learn_rate).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y2,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsess.run(tf.initialize_all_variables())\nd = Dataset()\ndat = d.load()\nfor i in range(9000):\n    if i%200==0:\n        a = accuracy.eval(feed_dict={x: dat.data, y_: dat.label,keep_prob: 1.0,learn_rate:1e-2})\n        print(\"step %d, training accuracy %g\" % (i, a))\n    train_step.run(feed_dict={x: dat.data, y_: dat.label,keep_prob: 0.8 ,learn_rate:1e-2})\n\nfor i in range(5000):\n    if i%100==0:\n        a = accuracy.eval(feed_dict={x: dat.data, y_: dat.label,keep_prob: 1.0,learn_rate:1e-3})\n        print(\"step %d, training accuracy %g\" % (i, a))\n    train_step.run(feed_dict={x: dat.data, y_: dat.label,keep_prob: 0.8,learn_rate:1e-3 })\n```\n# \n\nsome line of the output is as below:\nstep 7400, training accuracy 0.870539\nstep 7600, training accuracy 0.875406\nstep 7800, training accuracy 0.877677\nstep 8000, training accuracy 0.882868\nstep 8200, training accuracy 0.884491\nstep 8400, training accuracy 0.887735\nstep 8600, training accuracy 0.888709\nstep 8800, training accuracy 0.893251\nstep 0, training accuracy 0.894549\nstep 100, training accuracy 0.895198\nstep 200, training accuracy 0.894873\nstep 300, training accuracy 0.895198\nstep 400, training accuracy 0.894873\nstep 500, training accuracy 0.895198\nstep 600, training accuracy 0.895198\nstep 700, training accuracy 0.895522\nstep 800, training accuracy 0.895522\nstep 900, training accuracy 0.895847\nstep 1000, training accuracy 0.896171\nstep 1100, training accuracy 0.896171\nstep 1200, training accuracy 0.896496\nstep 1300, training accuracy 0.89682\nstep 1400, training accuracy 0\nstep 1500, training accuracy 0\nstep 1600, training accuracy 0\nstep 1700, training accuracy 0\n", "comments": ["Can you try reducing your learning rate? If your original optimization was close to being numerically unstable, then a small numerical change in the upgrade could cause training to diverge consistently.\n", "The case is not about divergence of the learning accuracy. the first loop is using 0.01 as learning rate and the second one is using 0.001. In both cases it's not divergent (as you can see the accuracy is increasing). My problem is the accuracy of learning is suddenly falling to zero! And I'm experiencing this since I'm using 0.8rc0 in 0.7 I had no such problem.\n", "I guess the issue is whether this is due to a bug, or part of normal training procedure. Accuracy suddenly falling to zero is a common occurrence in neural network training. Typically it happens when your neural network weights overflow or underflow. Can you see what the norm of your parameters looks like over time? Also, another suggestion is to try retuning your hyper-parameters. (ie, trying different learning rate, training algorithm, etc) There might be some numeric difference between 0.7 and 0.8, but it's not clear which one is the \"correct\" one. \n", "I've printed the value of W matrices in case the accuracy falls down And surprisingly all values was nan!\nSo I think it's a bug and is not about my code! \n", "BTW cross entropy is nan and it makes other elements nan\n", "It's common to get `NaN` during training. For instance you have an expression `y_*tf.log(y2)`. If `y2` is 0 and `y_` is 0, then `tf_log(y2)` is -infinity, and 0*(-infinity) is NaN.\n", "Yeah I got that. Thanks for your help @yaroslavvb . Since I've find out that cross entropy is causing the NaN problem I'm trying to use other cross entropies. but sadly non is as fast as this.\n", "One workaround is to do `tf.log(y2+1e-8)` instead of `tf.log(y2)`\n", "@yaroslavvb This is awesome, thank you!"]}, {"number": 1996, "title": "Distributed tensorflow on Mesos", "body": "In the [distributed howto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md): \"We are working on tools for launching tasks programmatically, e.g. using a cluster manager like Kubernetes. If there are particular cluster managers for which you'd like to see support, please raise a GitHub issue.\"\nIt could be interesting to have CPU and GPU dockefiles ready for distributed than can run in a scalable way on Mesos (with Marathon and Kubernetes)\n", "comments": ["/cc @mesosphere\n", "We(@douban)'re developing a lightweight mesos framework (named `tfmesos`) to run the tensorflow (0.8+) in Docker on Mesos. It already works now but maybe not production ready yet. If anybody is interested, we would like to open-source the experimental code. If there're any better distributed implementation in the future, we would like to mark tfmesos with deprecated flag.\n\nThere's still a dependency conflict on `protobuf` between tensorflow and mesos. We made a dirty patch to solve this and mesos crews are working on it right now: https://issues.apache.org/jira/browse/MESOS-5186\n", "@mckelvin I'm interested in the implementation.  Is it possible to share with the rest us that the possibilities opened by `tfmesos`?  For example, how does it simplify the process of running a distributed tensorflow?\n", "@mckelvin Yes. This issue was tagged \"contribution welcome\". So If you have something to share probably could be transformed in a pull request soon.\n", "@mrry What do you think? Do you have some feedback on how to handle this so that could be easier to integrate with TF repository with a PR?\n", "I'm agnostic as to whether this would be better as a standalone repository, or integrated into somewhere like `tf.contrib`. One of the concerns is that there might be version skew between TensorFlow `HEAD` and an external repository: although we're trying our best to keep the API stable, the distributed runtime libraries are pretty new, and we might want want to change them between releases, so having it local might be better. On the other hand, I'm not sure how easy it would be to add Mesos to our test matrix, and I don't want to put our testing team on the hook for that.\n\nHopefully the integration can be relatively simple, and exist as a set of Python scripts somewhere (though I don't have enough experience with Mesos to say). There might be some changes required in the core, so I'll be watching this thread, and prepared to respond to feature requests.\n", "@mrry Some initial work is at https://github.com/douban/tfmesos\n", "@bhack You got it! I've pushed the initial code to https://github.com/douban/tfmesos as well as https://hub.docker.com/r/tfmesos/tfmesos/ (@windreamer is the main developer of tfmesos).\n\nIf you have [mesos+docker](http://mesos.apache.org/documentation/latest/docker-containerizer/) installed, you can run the demo now. Before you start, you should pull tfmesos docker images on mesos server and slaves, via: `docker pull tfmesos/tfmesos` .\n\n**Notice:** There're still some unsolved issues in tfmesos and it's not production ready. For example, the tfmesos container is running in `root` mode, which is dangerous. We're still working on it. Feel free to PR / issue if you have any idea/suggestion.\n\n``` python\n# coding: utf-8\n# ~/demo.py\n\nimport sys\nimport tensorflow as tf\nfrom tfmesos import cluster\n\n\ndef main(argv):\n    jobs_def = [\n        {\n            \"name\": \"ps\",\n            \"num\": 2\n        },\n        {\n            \"name\": \"worker\",\n            \"num\": 2\n        },\n    ]\n    mesos_master = sys.argv[1]\n    with cluster(jobs_def, master=mesos_master, quiet=False) as targets:\n        with tf.device('/job:ps/task:0'):\n            a = tf.constant(10)\n\n        with tf.device('/job:ps/task:1'):\n            b = tf.constant(32)\n\n        with tf.device(\"/job:worker/task:1\"):\n            op = a + b\n\n        with tf.Session(targets['/job:worker/task:0']) as sess:\n            print sess.run(op)\n\n\nif __name__ == '__main__':\n    main(sys.argv)\n```\n\n```\nmckelvin@mesos1 ~ $ cat ./run.sh\n#!/bin/sh\ndocker run \\\n    -e MESOS_MASTER=mesos1 \\\n    -e DOCKER_IMAGE=tfmesos/tfmesos:latest \\\n    --net=host \\\n    -v /home/mckelvin/demo.py:/tmp/demo.py \\\n    --rm \\\n    -it \\\n    tfmesos/tfmesos:latest \\\n    python /tmp/demo.py mesos1\n```\n\n```\nmckelvin@mesos1 ~ $ ./run.sh\n2016-04-18 09:59:22,804 [INFO] [tfmesos.scheduler] Tensorflow cluster registered. ( http://mesos1:5050/#/frameworks/8beedc27-4bea-4f33-85b9-b440697419bd-0293 )\n2016-04-18 09:59:26,142 [INFO] [tfmesos.scheduler] Device /job:ps/task:0 activated @ grpc://mesos1:52382\n2016-04-18 09:59:26,150 [INFO] [tfmesos.scheduler] Device /job:ps/task:1 activated @ grpc://mesos1:44664\n2016-04-18 09:59:26,158 [INFO] [tfmesos.scheduler] Device /job:worker/task:0 activated @ grpc://mesos1:32984\n2016-04-18 09:59:26,166 [INFO] [tfmesos.scheduler] Device /job:worker/task:1 activated @ grpc://mesos1:32032\n42\n2016-04-18 09:59:26,190 [DEBUG] [tfmesos.scheduler] exit\n```\n", "/cc @mtamburrano\n", "@mckelvin I think that the docker image could be based on https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md. \nExpecially if we want to have GPU support.\n", "the main problem is `Mesos` by default does not handle GPUs as resources. so `tfmesos` now focuses on building a CPU-based distributed cluster. I've no idea how to share GPU resources among tasks. @bhack do u use GPU on ur mesos cluster\uff1f\n", "http://www.nvidia.com/object/apache-mesos.html and https://mesosphere.com/blog/2015/11/10/mesos-nvidia-gpus/\n", "@windreamer See also https://github.com/NVIDIA/nvidia-docker/issues/60\n", "Related news: [Dcos is opensource now](https://mesosphere.com/blog/2016/04/19/open-source-dcos/)\n", "We are trying to test tensorflow on mesos on GPU. If somebody it is interested can see https://github.com/douban/tfmesos/pull/3. We also need to think to a smarter auto device placement  in cluster scenarios like with mesos. See also https://github.com/tensorflow/tensorflow/issues/2126\n", "Hi, I'm new Mesos contributor and I have a little experience developing Mesos frameworks. While I'm studying the possibility to run Tensorflow as a native framework I found this thread. ^.^\n\nI saw that you're talking about GPU resource in Mesos. Take a look: \n\nhttps://issues.apache.org/jira/browse/MESOS-4424\n", "@jvanz we are trying `nvidia-docker` to help us allocate GPU resources, base on @3XX0 's suggestion\n\nexperimental work is here:\nhttps://github.com/douban/tfmesos/pull/3\n\nbut before all of there, I think this JIRA should be fixed first:\nhttps://issues.apache.org/jira/browse/MESOS-5186\n", "@windreamer for issue 5186 I suggest you to open directly a PR at https://github.com/apache/mesos/pulls\n", "@windreamer @girving How do you think this can be initally contributed to TF? I think that if you can create a PR we can attract a more extended users base.\n", "sure\uff0cit is an honour to contribute these small code base to TF. however, i think TF prefers k8s over mesos or yarn.\n", "K8s is in house but I don't that @mrry is against Mesos contribution.\n", "by the way, `tfmesos` depend on `pymesos` as a driver, (or you can use mesos native python driver, however it is a lot heavier). I do not know whether TF cares about external dependencies.\n", "@bhack @windreamer: We'd be delighted to see TensorFlow working on Mesos (and YARN, and other cluster managers). I don't know anything about how `tfmesos` is structured, but if it can exist as external code, that would be best. It shouldn't be necessary to add a dependency from the core `tensorflow` module to `pymesos` or any other cluster manager, unless I'm missing something. We could investigate putting things in `tensorflow.contrib` if the dependency were optional.\n", "@mrry Yes probably in tensorflow.contrib to have a little bit more exposition to the project.\n", "If somebody is interested there is a releated presentation done at Mesoscon 2016 with a demo of [Spark+Tensorflow on GPU](https://docs.google.com/presentation/d/1Y1IUlWV6g1HzD1wYIYXy6AmbfnczWfjvvmqqpeDFBic/edit?usp=docslist_api)\n", "tfmesos was updated by @windreamer to support Tensorflow 0.9 with https://github.com/windreamer/tfmesos/commit/fdde4824dbdaec9967b5de368d8469016e8ad342\n", "@jhseu See also last comments of https://github.com/douban/tfmesos/pull/3\n", "Has anyone gotten this to work properly on Mesos? I am the one who built the GPU support into Mesos and very interested in learning what limitations (if any) there are with running a distributed tensorflow with GPUs on Mesos.\n", "@klueska want to chat about it over VC next week? _email redacted_\n", "Sure. Sounds good. We can coordinate over email.\n\nOn Friday, August 26, 2016, Jonathan Hseu notifications@github.com wrote:\n\n> @klueska https://github.com/klueska want to chat about it over VC next\n> week? jhseu@google.com <javascript:_e(%7B%7D,'cvml','jhseu@google.com');>\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1996#issuecomment-242887616,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAF4o0DaWR-1OyKu1J9Y5Mnn802G7steks5qj5PjgaJpZM4IJHgJ\n> .\n\n## \n\n~Kevin\n", "Can you send then a public update?\n", "Ok, we can run a distributed `TensorFlow` training using `TFMesos` with or without GPU now (thanks to `nvidia-docker` project). And we also provide a script `tfrun` to submit a `Between-graph Replicated Training` script to Mesos cluster (the distributed `inception` model is using this mode).\n\nI think although `TFMesos` is still experimental, but we believe this a good start to better integrate `TensorFlow` to the existing Big-Data Ecosystem and support bigger and deeper models in the future.\n", "As we have already discussed we need to take a decision on the new http://mesos.apache.org/documentation/latest/container-image/\n", "@bhack yes, Apache Mesos 1.0 introduces a lot of new features, and I need more time to decide which is the best way.\n", "and unfortunately https://issues.apache.org/jira/browse/MESOS-5186 is still unresolved...\n", "For what it's worth, the only reliable / supported path for GPUs in Mesos\ngoing forward will be via the unified containerizer. i.e. what is discussed\nin the link sent by bhack before: http://mesos.apache.org/documentation/latest/container-image/\n\nDevelopment for the docker containerizer is currently under development,\nbut using it will not be the recommended mode of operation.\n\nRegarding MESOS-5168, what features of proto3 do you require? I know we\naren't planning on bumping Mesos to protobuf 3.0 anytime soon (though there\nare long term plans to do so). Barring this change, what else could be done\nto unblock this?\n\nOn Wed, Aug 31, 2016 at 1:27 PM windreamer notifications@github.com wrote:\n\n> and unfortunately https://issues.apache.org/jira/browse/MESOS-5186 is\n> still unresolved...\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1996#issuecomment-243735434,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAF4o5LcDt2-hae6R5V3tq3LMepFye6Vks5qlWUOgaJpZM4IJHgJ\n> .\n", "@klueska proto3 support is vital, or we will end up with a version confliction between `mesos.interface` and `TensowFlow`. Frankly speaking, I can not understand this \"one-line-modification\" is blocking for months... For the `existing user` of mesos, they can keep on using proto2 as usual\n\nI would like to try the mesos containerization way to launch the image, but it is still a bit mess for me to figure out how to enable this and GPU support. I need more time, and any suggestion and contribution is definitely welcome!\n", "Also is there a DC/OS plan?\n", "@windreamer I think the reason that the JIRA was probably never resolved is that it's not clear that the fix being proposed is the right one. It may happen to work for your particular use case, but many of mesos's probufs aren't written in a way that is compatible with proto3 clients.  For example, many of mesos's protobufs still contain `required` fields. We don't want people to blindly do a `pip install protobuf` (which installs 3.0 by default) and then start writing clients that will break in subtle ways when interacting with proto2 data coming over the wire. If you know of a general workaround for this, I'm sure it would gladly be accepted. \n\nRegarding problems figuring out how to enable GPU support -- I can help with that. We basically mimic the functionality of nvidia-docker so that anything that runs in nvidia-docker should now be able to run in mesos as well.  Consider the following example:\n\n```\n$ mesos-master \\\n      --ip=127.0.0.1 \\\n      --work_dir=/var/lib/mesos\n\n$ mesos-agent \\\n      --master=127.0.0.1:5050 \\\n      --work_dir=/var/lib/mesos \\\n      --image_providers=docker \\\n      --executor_environment_variables=\"{}\" \\\n      --isolation=\"docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia\"\n\n$ mesos-execute \\\n      --master=127.0.0.1:5050 \\\n      --name=gpu-test \\\n      --docker_image=nvidia/cuda \\\n      --command=\"nvidia-smi\" \\\n      --framework_capabilities=\"GPU_RESOURCES\" \\\n      --resources=\"gpus:1\"\n```\n\nThe flags of note here are: \n\n```\n  mesos-agent: \n      --isolation=\"docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia\" \n\n  mesos-execute: \n      --resources=\"gpus:1\" \n      --framework_capabilities=\"GPU_RESOURCES\" \n```\n\nWhen launching an agent, both the `cgroups/devices` and the `gpu/nvidia` isolation flags are required for Nvidia GPU support in Mesos. Likewise, the `docker/runtime` and `filesystem/linux` flags are needed to enable running docker images with the unified containerizer.\n\nThe `cgroups/devices` flag tells the agent to restrict access to a specific set of devices when launching a task (i.e. a subset of the devices listed in `/dev`). The `gpu/nvidia` isolation flag allows the agent to grant / revoke access to GPUs on a per-task basis. It also handles automatic injection of the Nvidia libraries / volumes into the container if the label `com.nvidia.volumes.needed = nvidia_driver` is present in the docker image. The `docker/runtime` flag allows the agent to parse docker image files and containerize them. The `filesystem/linux` flag says to use linux specific functionality when creating / entering the new mount namespace for the container filesystem. \n\nIn addition to these agent isolation flags, Mesos requires frameworks that want to consume GPU resources to have the GPU_RESOURCES framework capability set. Without this, the master will not send an offer to a framework if it contains GPUs. The choice to make frameworks explicitly opt-in to this GPU_RESOURCES capability was to keep legacy frameworks from accidentally consuming a bunch of non-GPU resources on any GPU-capable machines in a cluster (and thus blocking your GPU jobs from running). It's not that big a deal if all of your nodes have GPUs, but in a mixed-node environment, it can be a big problem.\n\nFinally, the  `--resources=\"gpus:1\"` flag tells the framework to only accept offers that contain at least 1 GPU. This is just an example of consuming a single GPU, you can (and probably should) build your framework to do something more interesting.\n\nHopefully you can extrapolate things from there. Let me know if you have any questions.\n", "I played a little bit with running Tensorflow on Mesos, without the GPU support.\n\nRegarding to the protobuf 2 dependency issue, I removed the protobuf 2 dependency from Mesos by not setting the protobuf 2 path in PYTHONPATH when starting the executor. It will pick the protobuf 3 dependency if installed.\n\nTo make it production-ready, we may have to handle a few failure cases: master failure, agent failure,  executor failure, network partition, message lost, service discovery, etc. So I am looking into other Mesos-based framework such as Marathon and see whether it handles all the failure cases for us.\n", "@YuefengZhou Marathon? In the DC/OS flavour?\n", "@bhack I have an experimental Mesos/Marathon cluster setup in my machine. I haven't looked into DC/OS yet. But I guess if it can work on Marathon smoothly with fault tolerance, it is not difficult to switch to DC/OS setup.\n", "GPU support will be included in Marathon 1.3 (being released in the next\ncouple of week). It will only be supported for the unified containerizer\nthough. There are still some hurdles to getting it supported all the way\nthrough DC/OS, but we plan to have those issues resolved by the DC/OS 1.9\nrelease (mid October).\n\nOn Wed, Aug 31, 2016 at 11:03 PM Yuefeng Zhou notifications@github.com\nwrote:\n\n> @bhack https://github.com/bhack I have a experimental Mesos/Marathon\n> cluster setup in my machine. I haven't looked into DC/OS yet. But I guess\n> if it can work on Marathon smoothly with fault tolerance, it is not\n> difficult to switch to DC/OS setup.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1996#issuecomment-243900376,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAF4ozyLc0KkKR6OddfDCqGqGWtIlO0Xks5qlew4gaJpZM4IJHgJ\n> .\n", "@klueska for `Unified Containerization Support` I open an issue https://github.com/douban/tfmesos/issues/12 , we can further discuss the detail there.\n\nAnd for proto3, if my understanding is right, protobuf is both a compiler and runtime library. For compiler, proto3 compiler is not backward-compatible with proto2. But as long as library is generated, proto3 runtime is backward-compatible with proto2, as least I believe so.\n\nSo which compiler used to build mesos library is managed by mesos itself. Only driver developers (such as [pymesos](https://github.com/douban/pymesos) of our own) may need to pay attention to this problem. For end user, I believe both proto2 or proto3 are ok with them.\n\nthis is why I don't think https://issues.apache.org/jira/browse/MESOS-5186 is a big issue to be resolved until Mesos 2.0\n", "/cc @vicki-c if interested in this topic.\n", "Why this rapid position change by google? Was self assigned to google just 17 days ago.\n", "Unified Containerization support for Apache Mesos 1.0 is merged now https://github.com/douban/tfmesos/issues/12#issuecomment-248848889\n", "I accidentally added these tags here and not in the issue I intended to. Sorry.\n", "We have examples running on Marathon in the new repo: [github.com/tensorflow/ecosystem](https://github.com/tensorflow/ecosystem).\n\nClosing this issue. If you have any changes you want to make, please create pull requests or issues in the new repo.\n", ">There's still a dependency conflict on protobuf between tensorflow and mesos. \r\n\r\nThanks a lot to @mckelvin , after we clean the compatible issues. http://issues.apache.org/jira/browse/MESOS-5186 is committed to master just now.  ", "@windreamer https://reviews.apache.org/r/56238/", "I know this thread is closed, but I wanted to point out the new release of distributed TensorFlow on DC/OS that we announced today. https://mesosphere.com/blog/tensorflow-gpu-support-deep-learning/"]}, {"number": 1995, "title": "Update RELEASE.md", "body": "Took out James Wexler (Googler) and added Yuan Tang (not Googler).\n", "comments": []}, {"number": 1994, "title": "(Do not merge - developing) Add tests for user ops", "body": "as a part of test-on-install in pip.sh\n", "comments": ["CL submitted internally for this. Will be synced to external shortly. This PR will be closed without merging.\n"]}, {"number": 1993, "title": "Cannot reshape variable with `validate_shape=False`", "body": "Given the following code:\n\n``` python\nimport tensorflow as tf\n\nx = tf.Variable(tf.zeros((3, 4)), validate_shape=False)\ntransposed = tf.reshape(x, [tf.shape(x)[1], -1])\n\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n\nprint(sess.run(transposed))\n```\n\nI got the following error:\n\n```\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\n```\n", "comments": ["Any comment on closing ?"]}, {"number": 1992, "title": "scipy needs to be imported before tensorflow", "body": "### Environment info\n\nOperating System: \nUbuntu 14.04.4 LTS\n\nInstalled version of CUDA and cuDNN: \ncuda-7.5\ncuDNN 5\n### Steps to reproduce\n1. import tensorflow\n2. import scipy.misc\n3. scipy.misc.imread fails with \"IOError: broken data stream when reading image file\"\n\nImporting scipy before tensorflow solves the issue.\n", "comments": ["Which TensorFlow version/Scipy version? Is there more debugging information? (ie, which line of scipy fails). I tried on last month's Anaconda install of scipy/tensorflow and it seems to work for me\n", "@keveman: I'm almost certain this has the same root cause as #1924.\n", "@yaroslavvb \n\nSince PIL is probably the culprit:\n\n```\n>>> PIL.VERSION\n'1.1.7'\n>>> PIL.PILLOW_VERSION\n'2.3.0'\n```\n\nFor tensorflow, I cloned and installed from source today\n`commit 36d89cf8f7bdfb17daf93f8439d95018b8cda431`\n\n```\nimage_data = misc.imread(image_path, flatten=1)\n  File \"/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py\", line 127, in imread\n    return fromimage(im,flatten=flatten)\n  File \"/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py\", line 189, in fromimage\n    im = im.convert('F')\n  File \"/usr/lib/python2.7/dist-packages/PIL/Image.py\", line 713, in convert\n    self.load()\n  File \"/usr/lib/python2.7/dist-packages/PIL/ImageFile.py\", line 234, in load\n    raise_ioerror(e)\n  File \"/usr/lib/python2.7/dist-packages/PIL/ImageFile.py\", line 56, in raise_ioerror\n    raise IOError(message + \" when reading image file\")\nIOError: broken data stream when reading image file\n```\n\n@mrry Yeah \n", "PIL is indeed the culprit; I'm getting the same error, and I'm sidestepping scipy entirely. Even if the image is read into memory via StringIO. grr.\n", "@keveman: Assigning this to you, but feel free to combine with the related bugs.\n", "Fixed with 5405394\n"]}, {"number": 1991, "title": "Fix SAME padding calc when stride>ksize", "body": "Adds a failing test case for conv2d ksize=1, stride=2, padding=same, input=4x4\n\nAdds tests convering that case for pooling ops too.\n\nIt seems cudnn doesn't like `ksize<stride` when `ksize>1`, thus the commented out test.\n\n@vrv \n", "comments": ["Can one of the admins verify this patch?\n", "> In one case above (R=C=3, S=2, K=1), we have: R' = C' = 2, Pr = Pc = 1 \\* 2 + 1 - 3 = 0, so it feels like the padding calculation is wrong for that case, since it's now calculated as '1' even though it's unneeded.\n> \n> In the case that failed for you: R=C=4, S=2, K=1 we have R'=C'=2, Pr = Pc = 1 \\* 2 + 1 - 4 = -1, so we have negative padding. My guess is that you changed the code to avoid tripping the check failure -- if you use std::max(0, calculated_padding), I'm thinking the math will generally work out. What do you think?\n\nLet's consider another case (padding=SAME) where the kernel is bigger than 1x1 and the strides are bigger than the kernel, such that the kernel lays across the boundary of the image.\n\nSo take R=C=4, K=2, S=3. R'=C'=2 and \n\n```\npad_needed_width = (R' - 1)S + K - R = 1*3 + 2 - 4 = 1\npad_left =  pad_needed_width / 2 = 0\npad_right = pad_needed_width - pad_left  = 1\n```\n\nThat seems okay, it calculates some 2x2 output matrix. In the case of the tests we'd have `expected == [44, 28, 41, 16]`.\n\nBut in Theano with `border_mode='full'` it pads with one on all sides and calculates `expected == [4, 25, 70, 144]`.\n\nUsing this patch, TF would calculate:\n\n```\npad_needed_width = (R' - 1)S + max(S,K) - R = 1*3 + 3 - 4 = 2\npad_left = pad_needed_width / 2 = 1\npad_right = pad_needed_width - pad_left  = 1\n```\n\nSee the second test here: https://gist.github.com/ry/273e0a8b83e33f891afbe09a6130c72c\n\nI think the way Theano does it seems more centered.\n", "@zheng-xq: thoughts?  I know we don't do padding exactly the same as other frameworks, so I'm not entirely sure what to do here.\n", "To me, the right thing to do seems to be:\n\n```\n// keep pad_needed_height the unchanged.  \n     int pad_needed_height =\n          (*new_height - 1) * row_stride + filter_height - in_height;\n\n// if the padding is negative, truncate it to zero. \n      if (pad_needed_height < 0) {\n        pad_needed_height = 0;\n      }\n\n// The same goes to \"pad_needed_width\"\n```\n", "I've read the example more carefully, and I still like a clipping-based approach. \n\nIf R=C=5, K=2, S=3. R'=C'=2\nThe clipping method:\npad_needed_width = (R' - 1)S + K - R = 1*3 + 2 - 5 = 0\n\nThe proposed method:\npad_needed_width = (R' - 1)S + max(S,K) - R = 1*3 + 3 - 5 = 1\n\nI would say the clipping method is better. \n\nThe weird thing TF does with R=C=4 can also be attributed to the fact TF prefers adding an extra padding on the right. If it used the other way around, a few corner cases would be slightly easier to handle and matches Theano in that case. But at this point, too many models have already been trained this way. And this benefit seems small to make a global switch. \n", "I'm okay with that. I think the difference is minimal. I made a graphic showing the two cases with  R=C=4, K=2, S=3. The colored areas show the kernel. The first is the algorithm i proposed (and what theano uses, I think) and the other is the \"clipping\"\n\n![padding](https://cloud.githubusercontent.com/assets/80/14623521/eb35a498-05a0-11e6-81ec-9818f49875d0.png)\n", "I've updated the patch with the clipping method and another test.\n", "Awesome, thanks for helping us work through this!.  @tensorflow-jenkins: test this please\n"]}]