[{"number": 50314, "title": "Add Decomposition of Resource Scatter Add in TF Compiler", "body": "", "comments": ["The code looks about right: is there a corresponding test update?", "@stellaraccident not yet, I'll add it on this file: https://github.com/tensorflow/tensorflow/blob/21c46e80edd69d352ae8ff01811e9631644f790f/tensorflow/compiler/mlir/tensorflow/tests/decompose_resource_ops.mlir :) ", "> The code looks about right: is there a corresponding test update?\r\n\r\nAdded :) ", "I am trying to fix my bug S.T AMD ROCm CI can pass, but I can't seem to access/view the logs on jenkins \ud83e\udd14 ", "The link to the build failure is wrong. Poking around I was able to get your error log:\r\n\r\nhttp://ml-ci.amd.com:21096/job/tensorflow/job/github-prs-upstream-master/job/AMD-ROCm-Community-CI-Build/view/change-requests/job/PR-50314/lastBuild/console", "> The link to the build failure is wrong. Poking around I was able to get your error log:\r\n> \r\n> http://ml-ci.amd.com:21096/job/tensorflow/job/github-prs-upstream-master/job/AMD-ROCm-Community-CI-Build/view/change-requests/job/PR-50314/lastBuild/console\r\n\r\nThanks @powderluv, after giving it an additional Builder Option/Signature for the TensorScatterAddOp seems to have done the trick locally at least :) "]}, {"number": 50313, "title": "[TF-TRT] Python TRT version utils function refactoring", "body": "@bixia1\r\nAdd IsLinkedTensorRTVersionGreaterEqual to refactor the TRT version utils.\r\nPR aims to clean up some functionalities in the python codebase of TF-TRT.\r\n", "comments": ["@DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted ? Thanks!", "@bixia1 modifications requested applied", "@bixia1 any update ? I need this PR to proceed removing TRT 5 and 6.", "@bixia1 unnecessary function removed as you requested. AFAIK we are good to go ;) ", "@gbaned looks like for some reasons the PR is approved but not checked by Kokoro & not ready to pull.\r\nAnything I need to get this through ?\r\n\r\nThanks\r\n", "Some tests fail:\r\nTraceback (most recent call last):\r\n  File \"/tensorflow/python/compiler/tensorrt/test/binary_tensor_weight_broadcast_test.py\", line 70, in setUp\r\n    if trt_utils.IsLinkedTensorRTVersionGreaterEqual(7):\r\n  File \"/tensorflow/python/compiler/tensorrt/utils.py\", line 65, in IsLinkedTensorRTVersionGreaterEqual\r\n    return _IsTensorRTVersionGreaterEqual(ver, major, minor, patch)\r\n  File \"/tensorflow/python/compiler/tensorrt/utils.py\", line 57, in _IsTensorRTVersionGreaterEqual\r\n    _trt_ver = LooseVersion(\".\".join(_trt_ver))\r\nTypeError: sequence item 0: expected str instance, int found\r\nbixia\r\n", "@bixia1 pushed the fix", "@bixia1 I reviewed your comments and push a fix for most of it.\r\nI haven't changed/updated the parts related to TRT 5 & 6. I'm grouping all these changes for the PR: https://github.com/tensorflow/tensorflow/pull/50782/ And I would prefer not mixing things up", "Closing in favor of #50782. Agreed with Bixia to merged the changes into #50782"]}, {"number": 50312, "title": "[Go] replace deprecated protocol buffers module", "body": "This PR replaces deprecated Go module [github.com/golang/protobuf](https://pkg.go.dev/github.com/golang/protobuf) with its successor: [google.golang.org/protobuf](https://pkg.go.dev/google.golang.org/protobuf).", "comments": ["I've disabled a genop test.  The output of google.golang.org/protobuf  text marshaler is intentionally unstable ([adds random whitespace](https://github.com/protocolbuffers/protobuf-go/blob/v1.26.0/internal/encoding/text/encode.go#L226)).  This test passed by luck/chance when I ran but may only randomly do so because it makes a []byte comparison of output."]}, {"number": 50311, "title": "[MLIR][TOSA] Adding tf.BatchMatMulV2 legalization.", "body": "Legaize tf.BatchMatMulV2 with tosa.MatMul.\r\nIf input rank == 3, trivially lower to tosa.MatMul.\r\nIf input rank > 3, reshape input tensors to rank 3 tensor, feed to tosa.MatMul, and then reshape from rank 3 to output tensor shape.", "comments": ["Tagging @rsuderman and @stellaraccident for review request."]}, {"number": 50310, "title": "Remove `noincompatible_prohibit_aapt1` from bazelrc", "body": "`--noincompatible_prohibit_aapt1` is a noop and is being removed from bazel in a future version.\r\n\r\nSee\r\n\r\nhttps://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/2070#20d4703d-abdf-44cc-a6b6-2f2c9a38bc48\r\n\r\nhttps://github.com/bazelbuild/bazel/commit/aefd107e163087d7dbc822b3342455d91669a58e", "comments": []}, {"number": 50309, "title": "Cudnn Frontend API respects the TF32 switch", "body": "For the TF using legacy Cudnn APIs, we use a global switch `tensor_float_32_execution_enabled` to determine whether the TF32 math should be used or not. However, it has no control of the new Cudnn frontend APIs (`TF_CUDNN_USE_FRONTEND=1`).\r\n\r\nThis PR fixes this issue by allowing the frontend API to respect the TF32 switch. Besides, we replaced the existing and cumbersome execution plan filters with a generic filter that can filter out (1) winograd engines, (2) non-deterministic engines, (3) tensor core engines when specified.\r\n\r\nThis PR currently relies on the pending PR: https://github.com/tensorflow/tensorflow/pull/50181. \r\n\r\ncc. @nluehr ", "comments": ["Just provide more background here: The v8 API will not use tensor math types any more. So, which engine uses tensor core will be detected from engine's numerical note (CUDNN_NUMERICAL_NOTE_TENSOR_CORE). In some cases, we don't want tensor cores to be enabled, e.g., some accuracy sensitive unit tests using FP32 on Ampere, which by default will use TF32 and tensor cores. "]}, {"number": 50307, "title": "TFLite Model Maker: Support custom input shape", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Not sure if I am up to the task\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI want to use the TensorFlow Lite Model Maker to train a custom object detection model. I need a custom input shape, lets say 501x300. I tried changing the hparam `\"image_size\"` to `\"420x420\"`, but I get the following error with the given code:\r\n\r\n<details>\r\n  <summary>Error</summary>\r\n\r\n  ```\r\n  ---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-4ed9b6a821a2> in <module>\r\n----> 1 model = object_detector.create( train_data,\r\n      2                                 validation_data=validation_data,\r\n      3                                 model_spec=spec,\r\n      4                                 epochs=1,\r\n      5                                 batch_size=4,\r\n\r\nc:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\object_detector.py in create(cls, train_data, model_spec, validation_data, epochs, batch_size, train_whole_model, do_train)\r\n    285     if do_train:\r\n    286       tf.compat.v1.logging.info('Retraining the models...')\r\n--> 287       object_detector.train(train_data, validation_data, epochs, batch_size)\r\n    288     else:\r\n    289       object_detector.create_model()\r\n\r\nc:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\object_detector.py in train(self, train_data, validation_data, epochs, batch_size)\r\n    154       validation_ds, validation_steps, val_json_file = self._get_dataset_and_steps(\r\n    155           validation_data, batch_size, is_training=False)\r\n--> 156       return self.model_spec.train(self.model, train_ds, steps_per_epoch,\r\n    157                                    validation_ds, validation_steps, epochs,\r\n    158                                    batch_size, val_json_file)\r\n\r\nc:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\model_spec\\object_detector_spec.py in train(self, model, train_dataset, steps_per_epoch, val_dataset, validation_steps, epochs, batch_size, val_json_file)\r\n    262             val_json_file=val_json_file,\r\n    263             batch_size=batch_size))\r\n--> 264     train.setup_model(model, config)\r\n    265     train.init_experimental(config)\r\n    266     model.fit(\r\n\r\nc:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow_examples\\lite\\model_maker\\third_party\\efficientdet\\keras\\train.py in setup_model(model, config)\r\n    111 def setup_model(model, config):\r\n    112   \"\"\"Build and compile model.\"\"\"\r\n--> 113   model.build((None, *config.image_size, 3))\r\n    114   model.compile(\r\n    115       steps_per_execution=config.steps_per_execution,\r\n\r\nc:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in build(self, input_shape)\r\n    417                            'method accepts an `inputs` argument.')\r\n    418         try:\r\n--> 419           self.call(x, **kwargs)\r\n    420         except (errors.InvalidArgumentError, TypeError):\r\n    421           raise ValueError('You cannot build your model by calling `build` '\r\n\r\nc:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow_examples\\lite\\model_maker\\third_party\\efficientdet\\keras\\train_lib.py in call(self, inputs, training)\r\n    883 \r\n    884   def call(self, inputs, training):\r\n--> 885     cls_outputs, box_outputs = self.base_model(inputs, training=training)\r\n    886     for i in range(self.config.max_level - self.config.min_level + 1):\r\n    887       cls_outputs[i] = self.classes(cls_outputs[i])\r\n\r\nc:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\r\n   1010         with autocast_variable.enable_auto_cast_variables(\r\n   1011             self._compute_dtype_object):\r\n-> 1012           outputs = call_fn(inputs, *args, **kwargs)\r\n   1013 \r\n   1014         if self._activity_regularizer:\r\n\r\nc:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\r\n    668       except Exception as e:  # pylint:disable=broad-except\r\n    669         if hasattr(e, 'ag_error_metadata'):\r\n--> 670           raise e.ag_error_metadata.to_exception(e)\r\n    671         else:\r\n    672           raise\r\n\r\nValueError: in user code:\r\n\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:243 call  *\r\n        result = smart_cond.smart_cond(training,\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:668 _call_attribute  **\r\n        return instance.__call__(*args, **kwargs)\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:828 __call__\r\n        result = self._call(*args, **kwds)\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:871 _call\r\n        self._initialize(args, kwds, add_initializers_to=initializers)\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:725 _initialize\r\n        self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2969 _get_concrete_function_internal_garbage_collected\r\n        graph_function, _ = self._maybe_define_function(args, kwargs)\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3361 _maybe_define_function\r\n        graph_function = self._create_graph_function(args, kwargs)\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3196 _create_graph_function\r\n        func_graph_module.func_graph_from_py_func(\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:990 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:634 wrapped_fn\r\n        out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    c:\\users\\felix\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py:267 restored_function_body\r\n        raise ValueError(\r\n\r\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n      Positional arguments (2 total):\r\n        * Tensor(\"inputs:0\", shape=(None, 420, 420, 3), dtype=float32)\r\n        * False\r\n      Keyword arguments: {}\r\n    \r\n    Expected these arguments to match one of the following 4 option(s):\r\n    \r\n    Option 1:\r\n      Positional arguments (2 total):\r\n        * TensorSpec(shape=(None, 320, 320, 3), dtype=tf.float32, name='inputs')\r\n        * True\r\n      Keyword arguments: {}\r\n    \r\n    Option 2:\r\n      Positional arguments (2 total):\r\n        * TensorSpec(shape=(None, 320, 320, 3), dtype=tf.float32, name='input_1')\r\n        * False\r\n      Keyword arguments: {}\r\n    \r\n    Option 3:\r\n      Positional arguments (2 total):\r\n        * TensorSpec(shape=(None, 320, 320, 3), dtype=tf.float32, name='input_1')\r\n        * True\r\n      Keyword arguments: {}\r\n    \r\n    Option 4:\r\n      Positional arguments (2 total):\r\n        * TensorSpec(shape=(None, 320, 320, 3), dtype=tf.float32, name='inputs')\r\n        * False\r\n      Keyword arguments: {}\r\n  ```\r\n\r\n</details>\r\n\r\n```\r\nspec = EfficientDetModelSpec(\r\n    model_name=\"efficientdet-lite0\",\r\n    uri=\"https://tfhub.dev/tensorflow/efficientdet/lite0/feature-vector/1\",\r\n    hparams={\r\n        \"image_size\": \"420x420\"\r\n    }\r\n)\r\ntrain_data, validation_data, test_data = object_detector.DataLoader.from_csv(\"...\")\r\nmodel = object_detector.create( train_data,\r\n                                validation_data=validation_data,\r\n                                model_spec=spec,\r\n                                epochs=1,\r\n                                batch_size=4,\r\n                                train_whole_model=True)\r\n```\r\n\r\nAccording to the clear error message it seems that this is not supported yet or am I missing something?\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who wants custom input shape for their object detection model. Which should be pretty common.\r\n", "comments": ["Please refer this recently published tutorial https://www.youtube.com/watch?v=sarZ_FZfDxs and see if it helps. Thanks!", "I watched the video, but custom input shape ist not mentioned in it. I asked whether or not this is supported in the TensorFlow Forum and it was confirmed that is not supported yet:\r\n\r\nhttps://discuss.tensorflow.org/t/is-it-possible-to-use-a-custom-input-shape-with-efficient-det/2099/7?u=felithium", "Thanks for sharing your solution here, this will help the future readers. Closing this issue now since it was answered on forum. ", "Why was this issue closed? The requested feature is not supported yet.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Can this be reopened? This feature is not supported yet afaik."]}, {"number": 50306, "title": "layer order in functional API graph models", "body": "Do the layers in a graph model that is defined with the functional API have a specific order in the model. \r\nFor example, when I have an Add layer that adds the outputs of two other layers, how is it decided which of the two layers comes first in the model?\r\nI am wondering since in most cases the order seems to be the same as the order the layers are given, but not always.\r\nThis can be confusing, for example, when I tried to get the weights of the model, the order of them was not as expected in rare cases.\r\nThe following code illustrates the issue. I expected to find the weights of the Conv2D before the DepthwiseConv2D when I perform  model.get_weights().\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\nin0Con6326 = tf.keras.layers.Input(shape=([2, 2, 2]))\r\nin0Dep66009 = tf.keras.layers.Input(shape=([2, 2, 2]))\r\nin0Con7723 = tf.keras.layers.Input(shape=([1, 1, 1]))\r\nCon6326 = keras.layers.Conv2D(3, (2, 2),strides=(1, 1), padding='valid' )(in0Con6326)\r\nDep66009 = keras.layers.DepthwiseConv2D((2, 1),strides=(2, 2), padding='valid',)(in0Dep66009)\r\nCon7723 = keras.layers.Concatenate(axis=3, )([Dep66009,in0Con7723])\r\nAdd80107 = keras.layers.Add()([Con6326,Con7723])\r\nmodel = tf.keras.models.Model(inputs=[in0Con6326,in0Dep66009,in0Con7723], outputs=Add80107)\r\nprint(model.get_weights())\r\nprint(model.summary())\r\n```\r\n", "comments": ["@rschumi0 ,\r\n\r\nWe see that the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue and also please let us know the tensorflow version you are using?\r\n\r\nThanks!", "@tilakrayal \r\nI was not sure which issue template would be the best, so I used the one for miscellaneous issues, which doesn't have a specific structure. \r\nI was running the example with the following System details:\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n    TensorFlow installed from (source or binary): binary\r\n    TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n    Python version: 3.8.5", "@sachinprasadhs ,\r\nI was able to reproduce the code in tf v2.4,v2.5 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/7ffc05b837104aacabc57fb67dd1f901/50306.ipynb).", "Since deep learning model is usually a  **directed acyclic graphs(DAG)** of layers and functional API is the way to do that way.\r\nYou can represent your layers in a graphical representation to understand the flow like below. \r\n![image](https://user-images.githubusercontent.com/73069040/122880346-8a400900-d357-11eb-8649-5448ffc63b8e.png)\r\n\r\nIf you want to get a layer weight then you can get the layer by it's name or you can iterate over the model layers and get the weights for the layer. \r\nPlease find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/7b9946de5965ad626aef9f8b17aaeb32/50306.ipynb) for more details. Thanks!", "@sachinprasadhs Thank You for your answer. I am aware of this visualization method and also that it is possible to get the weights from an individual layer.\r\nHowever, it would still be interesting to know if there is a specific order in which the DAG is processed?\r\nIn particular, in the example why is the DepthwiseConv2D before the Conv2D when the weights of the whole model are obtained?\r\nAre deeper branches processed first or what could be the reason for this order?", "The reason is Tensorflow tries to optimize using shortest path finding in the layers arranged in a Topological order of DAG network. \r\nYou can see the detailed explanation of DAG using mathematical concepts [here](https://en.wikipedia.org/wiki/Directed_acyclic_graph). ", "@sachinprasadhs I see, thanks.\r\nCould you please refer me to the source code where the DAG is processed or ordered?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "You may want to to refer https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/utils/vis_utils.py#L67", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50306\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50306\">No</a>\n"]}, {"number": 50304, "title": "how to auto format python code~", "body": "I change some code in tensorflow internal .\r\nI use clang-format to be perfectly compatible with c++ code, but I can\u2019t find a tool that is perfectly compatible with python code anyway.  @_@~ \r\nplease help me, I would like to get an auto-format tools. \r\nI'm try to auto format by rcfile in .//tensorflow/tools/ci_build/, and I try yapf, autopep8, pylint.\r\n\r\nI'm using tf-1.15, I find a same problem on tf 2.3", "comments": ["```sh tensorflow/tools/ci_build/ci_sanity.sh```    \r\nlogs of error or warning, like:\r\ntensorflow/python/ops/standard_ops.py:45: [W0622(redefined-builtin), ] Redefining built-in 'slice'\r\ntensorflow/python/ops/standard_ops.py:75: [W0622(redefined-builtin), ] Redefining built-in 'abs'\r\ntensorflow/python/ops/standard_ops.py:75: [W0622(redefined-builtin), ] Redefining built-in 'pow'\r\ntensorflow/python/ops/standard_ops.py:75: [W0622(redefined-builtin), ] Redefining built-in 'complex'\r\ntensorflow/python/ops/standard_ops.py:75: [W0622(redefined-builtin), ] Redefining built-in 'round'\r\ntensorflow/python/ops/standard_ops.py:75: [W0622(redefined-builtin), ] Redefining built-in 'range'\r\ntensorflow/python/ops/standard_ops.py:109: [W0622(redefined-builtin), ] Redefining built-in 'zip'\r\n\r\ntensorflow/python/ops/linalg/linear_operator.py:334: [E1102(not-callable), LinearOperator.shape_tensor] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:369: [E1102(not-callable), LinearOperator.batch_shape_tensor] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:396: [E1102(not-callable), LinearOperator.tensor_rank] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:412: [E1102(not-callable), LinearOperator.tensor_rank_tensor] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:455: [E1102(not-callable), LinearOperator.domain_dimension_tensor] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:499: [E1102(not-callable), LinearOperator.range_dimension_tensor] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:558: [E1102(not-callable), LinearOperator.assert_non_singular] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:591: [E1102(not-callable), LinearOperator.assert_positive_definite] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:617: [E1102(not-callable), LinearOperator.assert_self_adjoint] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:671: [E1102(not-callable), LinearOperator.matmul] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:674: [E1102(not-callable), LinearOperator.matmul] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:721: [E1102(not-callable), LinearOperator.matvec] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:753: [E1102(not-callable), LinearOperator.determinant] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:782: [E1102(not-callable), LinearOperator.log_abs_determinant] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:864: [E1102(not-callable), LinearOperator.solve] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:867: [E1102(not-callable), LinearOperator.solve] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:924: [E1102(not-callable), LinearOperator.solvevec] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:947: [E1102(not-callable), LinearOperator.adjoint] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:975: [E1102(not-callable), LinearOperator.inverse] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:1000: [E1102(not-callable), LinearOperator.cholesky] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:1021: [E1102(not-callable), LinearOperator.to_dense] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:1053: [E1102(not-callable), LinearOperator.diag_part] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:1070: [E1102(not-callable), LinearOperator.trace] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:1087: [E1102(not-callable), LinearOperator.add_to_tensor] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:1111: [E1102(not-callable), LinearOperator.eigvals] self._name_scope is not callable\r\ntensorflow/python/ops/linalg/linear_operator.py:1136: [E1102(not-callable), LinearOperator.cond] self._name_scope is not callable \r\n\r\n", "I am trying to add linting pre-commit in our developer container at https://github.com/tensorflow/tensorflow/pull/48371\r\n\r\nIf you use master you can already run the plain `pylint` command.\r\n\r\nI think that you can also use `yapf` for autoformatting if you set the style as python Google style.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> I am trying to add linting pre-commit in our developer container at #48371\r\n> \r\n> If you use master you can already run the plain `pylint` command.\r\n> \r\n> I think that you can also use `yapf` for autoformatting if you set the style as python Google style.\r\n\r\nthx. I will try .", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50304\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50304\">No</a>\n"]}, {"number": 50303, "title": "Build fails with GCC-11", "body": "Installation of Py-TensorFlow fails for below targets with GCC-11.\r\n\r\n1. `third_party/abseil-cpp/absl/synchronization/internal/graphcycles.cc\r\n`\r\n2. `external/com_google_absl/absl/synchronization/internal/graphcycles.cc`\r\n\r\n3. `external/ruy/ruy/block_map.cc`\r\n\r\n**Fix-**\r\nIn respective header file, add `#include <limits>` and `#include <stdexcept>`\r\n", "comments": ["@samcom12 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n", "**@tilakrayal**   Thanks for an early update.\r\nPlease see the below details for your reference.\r\n\r\nOS Platform and Distribution : `linux-centos7-skylake_avx512`\r\nTensorFlow installed from (source or binary):` installed from Source`\r\nTensorFlow version: `2.4.1`\r\nPython version: Tried with `Python-3.8.5` and `Python-3.9.0`\r\nInstalled using virtualenv? pip? conda?: Using `Spack`, Link- https://github.com/spack/spack.git\r\nBazel version (if compiling from source):`3.7.2`\r\nGCC/Compiler version (if compiling from source): `GCC-11.1.0`\r\nCUDA/cuDNN version: Compiling without GPU support\r\nGPU model and memory: Not applicable\r\n\r\nVariant 1: `spack install  --dont-restage py-tensorflow %gcc@11.1.0 ~cuda~nccl+mpi+numa ^openblas  ^openjdk ^python@3.9.0 ^mpich`\r\nVariant 2: `spack install  --dont-restage py-tensorflow %gcc@11.1.0 ~cuda~nccl+mpi+numa ^openblas  ^openjdk ^python@3.8.5 ^mpich` \r\n\r\nAs stated earlier, we need to explicitly include `<limits>` in builds of Bazel, grpc, ruy packages.\r\n\r\n\r\n", "We need to use-\r\n1. latest abesil-cpp from https://github.com/abseil/abseil-cpp.git\r\n2. latest Bazel if supported or patching older Bazel for GCC-11.\r\n3. Integrate latest ruy- https://github.com/google/ruy.git", "Can you send a PR please? You can tag me (`@mihaimaruseac`) so I'll get notified to quickly review and approve", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50303\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50303\">No</a>\n", "This problem still exist."]}, {"number": 50302, "title": "tensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n  no,the code from a example in [here](https://tensorflow.google.cn/tutorials/quickstart/advanced)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   windows 10\r\n- TensorFlow installed from (source or binary):\r\n   Installed using PIP\r\n- TensorFlow version (use command below):2.4.0\r\n- Python version:3.8\r\n- CUDA/cuDNN version:\r\n   cuda:11.3\r\n   cudnn:8.2.0\r\n- GPU model and memory:NVIDIA GeForce RTX3060 Laptop GPU, 6g\r\n\r\n**Describe the current behavior**\r\nwhen i runing the code from example like this:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense\r\nimport os\r\n\r\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\r\n\r\n# \u52a0\u8f7d\u6570\u636e\r\nmnist = tf.keras.datasets.mnist\r\n# dataset = mnist.load_data()\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n# Add a channels dimension(60000, 28, 28)->(60000, 28, 28, 1)\r\nx_train = x_train[..., tf.newaxis]\r\nx_test = x_test[..., tf.newaxis]\r\n\r\n# \u7528tf.data\u5c06\u6570\u636e\u6253\u4e71\u5e76\u5206\u6210batch,Batch_size = 32\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\r\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\r\n\r\n\r\nclass MyModel(Model):\r\n    def get_config(self):\r\n        pass\r\n\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.conv1 = Conv2D(32, 3, activation='relu')\r\n        self.flatten = Flatten()\r\n        self.d1 = Dense(128, activation='relu')\r\n        self.d2 = Dense(10, activation='softmax')\r\n\r\n    def call(self, x, training=None, mask=None):\r\n        x = self.conv1(x)\r\n        x = self.flatten(x)\r\n        x = self.d1(x)\r\n        return self.d2(x)\r\n\r\n\r\nmodel = MyModel()\r\n\r\n# \u8bbe\u7f6e\u635f\u5931\u51fd\u6570\u4ee5\u53ca\u4f18\u5316\u5668\r\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\noptimizer = tf.keras.optimizers.Adam()\r\n# \u9009\u62e9\u8861\u91cf\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u635f\u5931\u4ee5\u53ca\u51c6\u786e\u7387\r\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\r\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n\r\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\r\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\r\n\r\n\r\n# \u4f7f\u7528tf.GradientTape\u8bad\u7ec3\u6a21\u578b\r\n@tf.function\r\ndef train_step(images, labels):\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(images)\r\n        # \u8ba1\u7b97\u635f\u5931\r\n        loss = loss_object(labels, predictions)\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n    train_loss(loss)\r\n    train_accuracy(labels, predictions)\r\n\r\n\r\n# \u6d4b\u8bd5\u6a21\u578b\r\n@tf.function\r\ndef test_step(images, labels):\r\n    predictions = model(images)\r\n    t_loss = loss_object(labels, predictions)\r\n    test_loss(t_loss)\r\n    test_accuracy(labels, predictions)\r\n\r\n\r\nEPOCHS = 5\r\nfor epoch in range(EPOCHS):\r\n    # \u5728\u4e0b\u4e00\u4e2aepoch\u5f00\u59cb\u65f6\uff0c\u91cd\u7f6e\u8bc4\u4f30\u6307\u6807\r\n    train_loss.reset_states()\r\n    train_accuracy.reset_states()\r\n    test_loss.reset_states()\r\n    test_accuracy.reset_states()\r\n\r\n    for images, labels in train_ds:\r\n        train_step(images, labels)\r\n    for test_images, test_labels in test_ds:\r\n        test_step(test_images, test_labels)\r\n\r\n    template = 'Epoch:{} loss:{} accuracy:{}, test loss:{}, test accuracy:{}'\r\n    print(template.format(epoch + 1,\r\n                          train_loss.result(),\r\n                          train_accuracy.result() * 100,\r\n                          test_loss.result(),\r\n                          test_accuracy.result() * 100))\r\n```\r\ni have a error like this:\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!\r\n\t [[node my_model/conv2d/Conv2D (defined at E:/python/learnTensorFlow/UserGuide/\u5feb\u901f\u5165\u95e8.py:35) ]] [Op:__inference_train_step_531]\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node my_model/conv2d/Conv2D:\r\n Cast (defined at E:/python/learnTensorFlow/UserGuide/\u5feb\u901f\u5165\u95e8.py:58)\r\nFunction call stack:\r\ntrain_step\r\n```\r\nwhen i runing the code, the full output info is:\r\n```\r\n2021-06-16 17:38:03.802929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-06-16 17:38:13.895377: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-06-16 17:38:13.921252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6\r\ncoreClock: 1.425GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n2021-06-16 17:38:13.921449: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-06-16 17:38:13.933513: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-06-16 17:38:13.933669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-06-16 17:38:13.937037: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-06-16 17:38:13.938456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-06-16 17:38:13.940974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-06-16 17:38:13.943627: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-06-16 17:38:13.944370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-06-16 17:38:13.944532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-06-16 17:38:13.944860: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-06-16 17:38:13.951923: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x201621b5d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-06-16 17:38:13.952116: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-06-16 17:38:13.952322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6\r\ncoreClock: 1.425GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n2021-06-16 17:38:13.952504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-06-16 17:38:13.952593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-06-16 17:38:13.952686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-06-16 17:38:13.952770: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-06-16 17:38:13.952857: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-06-16 17:38:13.952940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-06-16 17:38:13.953019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-06-16 17:38:13.953121: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-06-16 17:38:13.953223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-06-16 17:38:14.323054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-06-16 17:38:14.323167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-06-16 17:38:14.323238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-06-16 17:38:14.323417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4733 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-06-16 17:38:14.326010: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x201295d7b30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-06-16 17:38:14.326134: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\r\n2021-06-16 17:38:14.799589: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-06-16 17:38:14.825461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-06-16 17:38:15.341945: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-06-16 17:38:15.343490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-06-16 17:38:16.059391: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops.cc:1106 : Not found: No algorithm worked!\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"D:\\PyCharm Community Edition 2021.1.1\\plugins\\python-ce\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"D:\\PyCharm Community Edition 2021.1.1\\plugins\\python-ce\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"E:/python/learnTensorFlow/UserGuide/\u5feb\u901f\u5165\u95e8.py\", line 86, in <module>\r\n    train_step(images, labels)\r\n  File \"D:\\anaconda3\\envs\\LearnKeras_GPU\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"D:\\anaconda3\\envs\\LearnKeras_GPU\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"D:\\anaconda3\\envs\\LearnKeras_GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2943, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"D:\\anaconda3\\envs\\LearnKeras_GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"D:\\anaconda3\\envs\\LearnKeras_GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"D:\\anaconda3\\envs\\LearnKeras_GPU\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!\r\n\t [[node my_model/conv2d/Conv2D (defined at E:/python/learnTensorFlow/UserGuide/\u5feb\u901f\u5165\u95e8.py:35) ]] [Op:__inference_train_step_531]\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node my_model/conv2d/Conv2D:\r\n Cast (defined at E:/python/learnTensorFlow/UserGuide/\u5feb\u901f\u5165\u95e8.py:58)\r\nFunction call stack:\r\ntrain_step\r\n```", "comments": ["@WoJiaYouTingShu ,\r\n\r\nEvery TensorFlow release is compatible with a certain version, for more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu).In this case, can you please try installing TensorFlow v2.4 with CUDA 11.0 and cuDNN 8 and check if you are facing the same error. \r\n\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-2.5.0 | 3.6-3.9 | MSVC 2019 | Bazel 3.7.2 | 8.1 | 11.2\r\ntensorflow_gpu-2.4.0 | 3.6-3.8 | MSVC 2019 | Bazel 3.1.0 | 8.0 | 11.0\r\ntensorflow_gpu-2.3.0 | 3.5-3.8 | MSVC 2019 | Bazel 3.1.0 | 7.6 | 10.1\r\n\r\n\r\n\r\nI was able to execute the code without any issues.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/ec6c25c79d3976cb0ceb6052ce93c9c7/untitled50302.ipynb).Thanks\r\n", "I reinstalled the cudn version, but the problem still exists, I'm installing cudnn8.0 and cuda11.0 this time.\r\nThe cuda11.0 have two different versions, i installed the first version:\r\n+ cuda toolkit 11.0\r\n+ cuda toolkit 11.0 update1\r\n\r\nThe cudnn8.0 for cuda11.0 is available in the following versions, I chose cudnn8.0.5 :\r\n+ cudnn8.0.1 for cuda11.0\r\n+ cudnn8.0.2 for cuda11.0\r\n+ cudnn8.0.3 for cuda11.0\r\n+ cudnn8.0.4 for cuda11.0\r\n+ cudnn8.0.5 for cuda11.0\r\n\r\nCan you tell me which version I should choose? Thank you.\r\n@tilakrayal ", "@WoJiaYouTingShu ,\r\n\r\nCan you please try to install cudnn8.0.1 for cuda11.0. As mentioned above code is executing with out any issues please find the [gist](https://colab.research.google.com/gist/tilakrayal/ec6c25c79d3976cb0ceb6052ce93c9c7/untitled50302.ipynb).\r\n\r\nPlease feel free to move this issue to closed status and please submit a new issue from this [link](https://github.com/tensorflow/models/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50302\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50302\">No</a>\n"]}, {"number": 50301, "title": "Unable to convert .pb file into .tflite", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installation (pip package or built from source):\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n#### Option A: Reference colab notebooks\r\n\r\n1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.\r\n2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).\r\n\r\n```\r\n(You can paste links or attach files by dragging & dropping them below)\r\n- Provide links to your updated versions of the above two colab notebooks.\r\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\r\n```\r\n\r\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\r\n\r\n```\r\n(You can paste links or attach files by dragging & dropping them below)\r\n- Include code to invoke the TFLite Converter Python API and the errors.\r\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\r\n```\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- Model produces wrong results and/or has lesser accuracy.\r\n- Model produces correct results, but it is slower than expected.\r\n\r\n### 4. (optional) RNN conversion support\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n### 5. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I am doing an Image classification problem on Google Colab, The link for the code on Colab is https://colab.research.google.com/drive/1dpJ05nLl-stqu8lC9P4mwoIEbBmyxFBU\r\n\r\nI am able to successfully retrain an efficeintdet-do model with custom mages. \r\nWithin Colab i am able to do reasonable corect image classification as well, but when i convert the model (.pb file) into tflite, \r\nUpon invoking Interpreter i upon printing print(input_details) the result is \r\n\r\n[{'name': 'serving_default_input_tensor:0', 'index': 0, 'shape': array([1, 1, 1, 3], dtype=int32), 'shape_signature': array([ 1, -1, -1,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\nThe shape : array [1, 1, 1, 3] should have been shape[1, 250, 250, 3]. I am unable to understand how this conversion automatically happened, because during tflite creation i have specifically called concrete_func.inputs[0].set_shape([1, 250, 250, 3])\r\n\r\n", "@VirVisha I am unable to access your Colab. Can you provide a snippet of how you are converting your model using the TFLite converter?", "I have made the access to the notebook public the link is https://drive.google.com/file/d/1dpJ05nLl-stqu8lC9P4mwoIEbBmyxFBU/view?usp=sharing", "Your code seems to be working on object detection with an SSD model. The steps to convert that are different, and we need to use a TFLite-specific script as opposed to exporter_main_v2. See [this Colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb), specifically \"Export & run with TensorFlow Lite\" section & below. You might need to adapt it a bit for your use-case.", "@srjoglekar246  As suggested in the colab notebook link you shared , I changed the code section of !python object_detection/exporter_main_v2.py \\\r\n    --trained_checkpoint_dir {last_model_path} \\\r\n    --output_directory {output_directory} \\\r\n    --pipeline_config_path {pipeline_file}\r\n\r\nto \r\n\r\n!python object_detection/export_tflite_graph_tf2.py \\\r\n  --pipeline_config_path {pipeline_file} \\\r\n  --trained_checkpoint_dir {last_model_path} \\\r\n  --output_directory {output_directory}\r\n\r\nSubsequent to this there are error messages generated \r\n\r\n2021-06-17 05:06:22.085451: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-06-17 05:06:24.864052: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\n2021-06-17 05:06:24.903511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-06-17 05:06:24.904230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2021-06-17 05:06:24.904282: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-06-17 05:06:24.908384: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-06-17 05:06:24.908475: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n2021-06-17 05:06:24.910463: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\r\n2021-06-17 05:06:24.911133: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\r\n2021-06-17 05:06:24.922260: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\r\n2021-06-17 05:06:24.923550: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\r\n2021-06-17 05:06:24.924141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-06-17 05:06:24.924331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-06-17 05:06:24.925346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-06-17 05:06:24.926879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-06-17 05:06:24.928056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-06-17 05:06:24.929028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2021-06-17 05:06:24.929158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-06-17 05:06:24.930647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-06-17 05:06:24.931713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-06-17 05:06:24.931807: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-06-17 05:06:25.620583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-06-17 05:06:25.620658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n2021-06-17 05:06:25.620679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \r\n2021-06-17 05:06:25.620885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-06-17 05:06:25.621582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-06-17 05:06:25.622246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-06-17 05:06:25.622862: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2021-06-17 05:06:25.622913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\r\nI0617 05:06:25.629735 140131339650944 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b0\r\nI0617 05:06:25.629959 140131339650944 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\r\nI0617 05:06:25.630064 140131339650944 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 3\r\nI0617 05:06:25.634973 140131339650944 efficientnet_model.py:147] round_filter input=32 output=32\r\nI0617 05:06:25.658451 140131339650944 efficientnet_model.py:147] round_filter input=32 output=32\r\nI0617 05:06:25.658674 140131339650944 efficientnet_model.py:147] round_filter input=16 output=16\r\nI0617 05:06:25.722052 140131339650944 efficientnet_model.py:147] round_filter input=16 output=16\r\nI0617 05:06:25.722274 140131339650944 efficientnet_model.py:147] round_filter input=24 output=24\r\nI0617 05:06:25.884342 140131339650944 efficientnet_model.py:147] round_filter input=24 output=24\r\nI0617 05:06:25.884578 140131339650944 efficientnet_model.py:147] round_filter input=40 output=40\r\nI0617 05:06:26.054194 140131339650944 efficientnet_model.py:147] round_filter input=40 output=40\r\nI0617 05:06:26.054428 140131339650944 efficientnet_model.py:147] round_filter input=80 output=80\r\nI0617 05:06:26.316348 140131339650944 efficientnet_model.py:147] round_filter input=80 output=80\r\nI0617 05:06:26.316609 140131339650944 efficientnet_model.py:147] round_filter input=112 output=112\r\nI0617 05:06:26.575680 140131339650944 efficientnet_model.py:147] round_filter input=112 output=112\r\nI0617 05:06:26.575894 140131339650944 efficientnet_model.py:147] round_filter input=192 output=192\r\nI0617 05:06:26.927866 140131339650944 efficientnet_model.py:147] round_filter input=192 output=192\r\nI0617 05:06:26.928075 140131339650944 efficientnet_model.py:147] round_filter input=320 output=320\r\nI0617 05:06:27.010624 140131339650944 efficientnet_model.py:147] round_filter input=1280 output=1280\r\nI0617 05:06:27.047516 140131339650944 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\r\nTraceback (most recent call last):\r\n  File \"object_detection/export_tflite_graph_tf2.py\", line 161, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"object_detection/export_tflite_graph_tf2.py\", line 157, in main\r\n    FLAGS.centernet_include_keypoints, FLAGS.keypoint_label_map_path)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/export_tflite_graph_lib_tf2.py\", line 348, in export_tflite_model\r\n    max_detections, use_regular_nms)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/export_tflite_graph_lib_tf2.py\", line 71, in __init__\r\n    self._process_config(pipeline_config)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/export_tflite_graph_lib_tf2.py\", line 104, in _process_config\r\n    image_resizer_config.WhichOneof('image_resizer_oneof')))\r\nValueError: Only fixed_shape_resizeris supported with tflite. Found keep_aspect_ratio_resizer", "Subsequent to this i checked the same application with ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz and followed steps as mentioned in the \"Export & run with TensorFlow Lite\" .\r\n\r\nInference is happening reasonably correct and the model.tflite file is also getting created with the print(input_details) showing me\r\n\r\n[{'name': 'input', 'index': 12, 'shape': array([  1, 320, 320,   3], dtype=int32), 'shape_signature': array([  1, 320, 320,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\nNow when i export the tflite file with relevant labels.txt to Android application i am getting an error \r\n\r\nCaused by: java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (input) with 1228800 bytes from a Java Buffer with 150528 bytes.\r\n\r\n\r\n", "Upon closer investigation of on revisiting colab notebook, the link is https://colab.research.google.com/drive/1Hs6-UVhtHZCpKz-6JcoS8xFTG24jBA6c?usp=sharing \r\nI found that saved_model.pb is of around 10MB but the model.tflite which was created is of only 2KB..\r\n\r\nI followed\r\n\r\nStep 1\r\n !python object_detection/export_tflite_graph_tf2.py \\\r\n  --pipeline_config_path {pipeline_file} \\\r\n  --trained_checkpoint_dir {last_model_path} \\\r\n  --output_directory {output_directory}\r\n\r\nand subsequently Step 2\r\n\r\n!tflite_convert --saved_model_dir='/content/gdrive/MyDrive/TFLite/fine_tuned_model/saved_model' --output_file='/content/gdrive/MyDrive/TFLite/model.tflite'\r\n\r\nI even dropped the model.tflite file on Netron  found this as  description\r\n\r\n![image](https://user-images.githubusercontent.com/85932233/122429088-206adc80-cfb0-11eb-8543-271598838ee0.png)\r\n\r\nUnable to decipher if there was anything else needed to be done. The above colab notebook was trained for 2000 steps and only Bill Clinton images were trained. That way the inference is working fine in colab \r\n", "Aah the exporting script doesn't support EfficientDets. From the detection zoo, we support [SSD MobileNet](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md) & [CenterNet](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/centernet_on_device.ipynb). If you want EfficientDet necessarily, you can try [Model Maker](https://www.tensorflow.org/lite/tutorials/model_maker_object_detection).", "For the above colab notebook i am using \r\nMODELS_CONFIG = {\r\n    'ssd': {\r\n        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\r\n        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\r\n        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\r\n        'batch_size': 8\r\n    }\r\n}", "Aah. Your TFLite converter needs to be running TF 2.4 or later. We added these features after TF 2.3.", "I am using Tensorflow version 2.5 for these colab notebooks", "@srjoglekar246 If i share tflite file and labels.txt can you check that the model is correct. Though when i placed the tflite in netron app, it seems to me it is correct.\r\n\r\nprint(input_details) \r\n[{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1, 300, 300,   3], dtype=int32), 'shape_signature': array([  1, 300, 300,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\nAfter this i used interpreter.set_tensor(input_index, input_tensor)\r\n\r\nFollowed by call interpreter.invoke()\r\n\r\nAfter words I checked the output_details i get the following output\r\n\r\nprint(output_details)\r\n[{'name': 'StatefulPartitionedCall:3', 'index': 247, 'shape': array([ 1, 10,  4], dtype=int32), 'shape_signature': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}},\r\n {'name': 'StatefulPartitionedCall:2', 'index': 248, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, \r\n{'name': 'StatefulPartitionedCall:1', 'index': 249, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, \r\n{'name': 'StatefulPartitionedCall:0', 'index': 250, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\nSupposedly the output should be an image as seen during the inference in colab, how can I interpret the same from the above details \r\n", "The model isn't correct. For some reason, your model just contains zeros as the outputs.\r\n\r\nHow are you converting the exported SavedModel to TFLite? Can you share a snippet?", "I am using following code snippets \r\n\r\nStep 1\r\n!python /content/gdrive/MyDrive/TFLite_Check/models/research/object_detection/export_tflite_graph_tf2.py \\\r\n    --pipeline_config_path {pipeline_file} \\\r\n    --trained_checkpoint_dir {last_model_path} \\\r\n    --output_directory {outd}\r\n\r\nStep 2\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/gdrive/MyDrive/TFLite_Check/freezetflite/saved_model/')\r\n\r\nStep 3\r\n\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\n\r\nStep 4 \r\ntflite_model = converter.convert()\r\n\r\nStep 5 \r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\nStep 6\r\nwith tf.io.gfile.GFile('/content/gdrive/MyDrive/TFLite_Check/test/model.tflite', 'rb') as f:\r\n    model_content = f.read()\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=model_content)\r\ninput_details = interpreter.get_input_details()\r\n\r\nThe Link to the complete notebook is https://colab.research.google.com/drive/1kisxBtImUS7sT2aQlsEKfcXaEnsKumGw?usp=sharing\r\n\r\nWhen i print input details \r\nprint(input_details) \r\nThe ouptut is [{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1, 300, 300,   3], dtype=int32), 'shape_signature': array([  1, 300, 300,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\nAfter passing the image as an input when check the output \r\noutput_details = interpreter.get_output_details()\r\nprint(output_details) i get  following output \r\n\r\n[{'name': 'StatefulPartitionedCall:3', 'index': 239, 'shape': array([ 1, 10,  4], dtype=int32), 'shape_signature': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:2', 'index': 240, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:1', 'index': 241, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:0', 'index': 242, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]", "Your code seems fine to me. See [this Colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb) for how to run the model in Python (specifically, the `Test .tflite model` section)", "Did that, here is the notebook of your code https://colab.research.google.com/drive/1B4AEQJasbIRNmk-tOr0rt_VFvM4-w-qy?usp=sharing\r\n\r\nAt the end in the last cell there was an issue i came across  \r\n\r\nlabel_id_offset = 1\r\nfor i in range(len(test_images_np)):\r\n  input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\r\n  boxes, classes, scores = detect(interpreter, input_tensor)\r\n\r\n  plot_detections(\r\n      test_images_np[i][0],\r\n      boxes[0],\r\n      classes[0].astype(np.uint32) + label_id_offset,\r\n      scores[0],\r\n      category_index, figsize=(15, 20), image_name=\"gif_frame_\" + ('%02d' % i) + \".jpg\")\r\n\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-58-eb5ebe4f5c3c> in <module>()\r\n     12   plot_detections(\r\n     13       test_images_np[i][0],\r\n---> 14       boxes[0],\r\n     15       classes[0].astype(np.uint32) + label_id_offset,\r\n     16       scores[0],\r\n\r\nIndexError: invalid index to scalar variable.\r\n\r\n", "@srjoglekar246 I retrieved the scores parameter in the notebook https://drive.google.com/file/d/1HEpM5LtJ6Hx4D9oEpAXlv1C7mwpEvqbw/view?usp=sharing in the plot detections functions in the last cell of the notebook.\r\n\r\nThe print(scores[0][0]) output gives me 0.9150113 which when i corroborated against the output figure is same, So i presume that this is giving me the accuracy of the input image match\r\n\r\nNow i have to port this to Android.\r\nBefore this i just want to summarize how i generated the tflite file.\r\n\r\nStep 1\r\n!python /content/gdrive/MyDrive/TFLite4/models/research/object_detection/export_tflite_graph_tf2.py \\\r\n    --pipeline_config_path {pipeline_file} \\\r\n    --trained_checkpoint_dir {last_model_path} \\\r\n    --output_directory {outd}\r\n\r\nThis saves and create a saved_model directory containing assets, saved_model.pb in a folder freeezetflite in the above notebook\r\n\r\nStep 2 \r\nIn order to generate the tflite file I implemented the following commands\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/gdrive/MyDrive/TFLite4/freezetflite/saved_model/')\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\n\r\ntflite_model = converter.convert()\r\n\r\nimport pathlib\r\ntflite_model_path = pathlib.Path('model.tflite')\r\n\r\nwith tf.io.gfile.GFile(tflite_model_path, 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\nThis generates model.tflite file in the test directory\r\n\r\nStep 3\r\nI created interpreter and set the input tensor followed by invoking it, the ouptut i capture in boxes, classes and scores\r\n\r\nwith tf.io.gfile.GFile('/content/gdrive/MyDrive/TFLite4/test/model.tflite', 'rb') as f:\r\n    model_content = f.read()\r\ninterpreter = tf.lite.Interpreter(model_content=model_content)\r\n\r\ninterpreter.allocate_tensors()\r\n\r\nboxes = interpreter.get_tensor(output_details[0]['index'])\r\nclasses = interpreter.get_tensor(output_details[1]['index'])\r\nscores = interpreter.get_tensor(output_details[2]['index'])\r\n\r\nThis when passed into the plot_detections function gives me the output of the input image \r\n![mobile_capture](https://user-images.githubusercontent.com/85932233/123200880-3a844d80-d4cf-11eb-9405-7883f6cae84f.jpg)\r\n\r\nI trained the model with images of Bill Clinton and got the above input\r\n\r\nI hope that this all is correct way of doing.\r\n\r\nNow I want to port this to Android, as pointed out by you https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb do i have to adapt the above tflite file using the mechanism pointed out in the link https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection\r\n\r\n", "Awesome work! And yes, the link you have seems right. Specifically, see [this section](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android#switch-between-inference-solutions-task-library-vs-tflite-interpreter) for how to customize the app code based on your use case. For example, if you want to go the lib_interpreter route mentioned there, then you will have to modify the code [here](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L167) based on your model. ", "I will close this bug since the title issue is resolved. Feel free to followup with comments about the Android app if you have any. But you seem to have gotten the inference logic figured out in Python.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50301\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50301\">No</a>\n", "@srjoglekar246 Thanks a ton for your guidance and surely now I am porting this to Android "]}, {"number": 50300, "title": "why the environment variable CUDA_VISIBLE_DEVICES no use", "body": "hi, when i run tensorflow  project, i set those two  environment variables below to make the tensorflow use the GPUs as i expected, but the settings did not effect as i expected.\r\n\r\nexport CUDA_DEVICE_ORDER=\"PCI_BUS_ID\"\r\nexport CUDA_VISIBLE_DEVICES=\"5,6\"\r\n\r\nas  i set the variable CUDA_VISIBLE_DEVICES as above, when i run tensorflow project , it only use the GPU:0 and GPU:1, it seems that it ignore what i set , just start with GPU:0.\r\n\r\ncan anyone give some advices?\r\nthanks a lot.\r\n\r\nPS my tensorflow version is 2.4.1", "comments": ["Tensorflow always labels devices starting from zero, but it _does_ use `CUDA_VISIBLE_DEVICES`. So, what TensorFlow labels as GPUs 0 and 1 should in fact be your system's GPUs 5 and 6. You can check that by monitoring GPU activity using tools such as `nvidia-smi` or `nvtop` from the command line while a TensorFlow job making use of the GPUs is running.", "@nkjulia ,\r\n\r\nPlease take a look at this link which provide more information on CUDA_VISIBLE_DEVICES. [Link1](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars),[Link2](https://www.tensorflow.org/guide/gpu).Thanks!", "> @nkjulia ,\r\n> \r\n> Please take a look at this link which provide more information on CUDA_VISIBLE_DEVICES. [Link1](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars),[Link2](https://www.tensorflow.org/guide/gpu).Thanks!\r\n\r\nok~ thanks", "> Tensorflow always labels devices starting from zero, but it _does_ use `CUDA_VISIBLE_DEVICES`. So, what TensorFlow labels as GPUs 0 and 1 should in fact be your system's GPUs 5 and 6. You can check that by monitoring GPU activity using tools such as `nvidia-smi` or `nvtop` from the command line while a TensorFlow job making use of the GPUs is running.\r\n\r\n\r\n\r\n> Tensorflow always labels devices starting from zero, but it _does_ use `CUDA_VISIBLE_DEVICES`. So, what TensorFlow labels as GPUs 0 and 1 should in fact be your system's GPUs 5 and 6. You can check that by monitoring GPU activity using tools such as `nvidia-smi` or `nvtop` from the command line while a TensorFlow job making use of the GPUs is running.\r\n\r\ni checked the usage of GPUS when running a task with specified setting, no questions now, thank u."]}, {"number": 50299, "title": "The latest docker image cannot work on my computer", "body": "<em>I pulled the latest tensorflow-gpu image but I cannot run this image </em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow version: docker image: 'tensorflow/tensorflow:latest-gpu-jupyter'\r\n- CUDA/cuDNN version:CUDA version 10.2\r\n- GPU model and memory: 2080ti * 4, each with memory 11019MB\r\n- Docker version 19.03.8, build afacb8b7f0\r\n\r\n\r\n\r\n**Describe the problem**\r\nSince tensorflow-io installed by pip cannot work together with tensorflow installed by conda, I decided to use the docker image of tensorflow. I installed nvidia-docker according to this page: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker. And the result is this:\r\n ![\u6355\u83b7](https://tvax4.sinaimg.cn/large/006dgyGJgy1grk3opnyi6j30sd0h3goc.jpg)\r\nThen, I pulled the tensorflow image by this:\r\n`docker pull tensorflow/tensorflow:latest-gpu-jupyter`\r\nand run this image by this:\r\n`docker run --gpus=all -it tensorflow/tensorflow:latest-gpu bash`\r\nthen, I meet this error:\r\n_docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:432: running prestart hook 0 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: requirement error: unsatisfied condition: cuda>=11.2, please update your driver to a newer version, or use an earlier cuda container\\\\\\\\n\\\\\\\"\\\"\": unknown.\r\nERRO[0000] error waiting for container: context canceled_\r\nWhy would it give this error: unsatisfied condition: cuda>=11.2\r\n\r\n\r\n**Any other info / logs**\r\nThat's all images in this computer\r\n![\u6355\u83b7](https://tvax4.sinaimg.cn/large/006dgyGJgy1grk3yb08ygj30tb06tq4o.jpg)", "comments": ["@hnqmz The error clearly states that `\"requirement error: unsatisfied condition: cuda>=11.2, please update your driver to a newer version, or use an earlier cuda container\\\\n\\\"\"\": unknown.\"`. \r\n\r\nIt means that the latest tensorflow you are trying to install requires `cuda drivers >= 11.2` but you have `10.2` drivers. So,  select other container that requires only `cuda-10.2`.  Check https://www.tensorflow.org/install/docker for information. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50299\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50299\">No</a>\n"]}, {"number": 50297, "title": "[ROCm] Adding rosolver integration in XLA cusolver_context + cusolver_rewriter for ROCm platform", "body": "see individual commit message for details\r\n\r\n/cc @chsigg @cheshire @hawkinsp ", "comments": ["My concern with the current PR is it may lead to confusion, as reading code full of CU*** prefixes I would assume it's CUDA-specific and does not touch ROCm. How hard would it be to abstract away the types you need to change? Also probably to change the name of the class? (Just \"Solver\" is too generic, what would be an appropriate domain-name prefix?)", "@cheshire pushed out a couple of commits to rename the cu* names to gpu* names in the cusolver_* files.  please re-review", "> minor nits, mostly LG!\r\n\r\n@cheshire pushed out another commit with the changes you requested...please re-review"]}, {"number": 50296, "title": "[_Derived_]RecvAsync is cancelled while running tensorflow RNN tutorial code on GPU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:  CUDA: 11.2.0\r\n- GPU model and memory: NVIDIA Titan RTX 24 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am running this tutorial https://www.tensorflow.org/text/tutorials/text_classification_rnn and during  training I am getting below error,\r\n\r\nEpoch 1/10\r\n 67/391 [====>.........................] - ETA: 18s - loss: 0.6928 - accuracy: 0.4977\r\n---------------------------------------------------------------------------\r\nCancelledError                            Traceback (most recent call last)\r\n<ipython-input-25-d9321db1417e> in <module>\r\n----> 1 model.fit(train_dataset, epochs=10,\r\n      2                     validation_data=test_dataset,\r\n      3                     validation_steps=30)\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1181                 _r=1):\r\n   1182               callbacks.on_train_batch_begin(step)\r\n-> 1183               tmp_logs = self.train_function(iterator)\r\n   1184               if data_handler.should_sync:\r\n   1185                 context.async_wait()\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    887 \r\n    888       with OptionalXlaContext(self._jit_compile):\r\n--> 889         result = self._call(*args, **kwds)\r\n    890 \r\n    891       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    915       # In this case we have created variables on the first call, so we run the\r\n    916       # defunned version which is guaranteed to never create variables.\r\n--> 917       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    918     elif self._stateful_fn is not None:\r\n    919       # Release the lock early so that multiple threads can perform the call\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   3021       (graph_function,\r\n   3022        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n-> 3023     return graph_function._call_flat(\r\n   3024         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n   3025 \r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1958         and executing_eagerly):\r\n   1959       # No tape is watching; skip to running the function.\r\n-> 1960       return self._build_call_outputs(self._inference_function.call(\r\n   1961           ctx, args, cancellation_manager=cancellation_manager))\r\n   1962     forward_backward = self._select_forward_and_backward_functions(\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    589       with _InterpolateFunctionError(self):\r\n    590         if cancellation_manager is None:\r\n--> 591           outputs = execute.execute(\r\n    592               str(self.signature.name),\r\n    593               num_outputs=self._num_outputs,\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57   try:\r\n     58     ctx.ensure_initialized()\r\n---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n\r\nCancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node Adam/Adam/update/AssignSubVariableOp/_57}}]]\r\n\t [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_54]] [Op:__inference_train_function_22715]\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should work ok without issues. I have not changed any code, it is a tutorial from tensorflow official documentation. If you train on CPU there is no issue so something wrong with training on GPU\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nIt is very easy to reproduce. Just run https://www.tensorflow.org/text/tutorials/text_classification_rnn in a jupyter notebook on a machine that has GPU\r\n\r\nThis seems to be similar to https://github.com/tensorflow/tensorflow/issues/33721, opening this because that previous issue was closed without a resolution.\r\n", "comments": ["@dhavalsays ,\r\n\r\nI haven't faced any issue while executing the code in tf [v2.5](https://colab.research.google.com/gist/tilakrayal/d529b1c30cf0006c200aca1edfa5c81d/text_classification_rnn.ipynb) and [v2.4](https://colab.research.google.com/gist/tilakrayal/ccad84d997d457a573a834e205ec4357/2-4text_classification_rnn.ipynb).Please find the gist.\r\n\r\nplease try to execute code in virtual environment and limiting [GPU memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) using any of the methods listed in this guide and check if it helps.Thanks!\r\n\r\n", "I tried limiting GPU memory growth [check here, line # 3](https://github.com/codebasics/deep-learning-keras-tf-tutorial/blob/master/43_text_classification_rnn/rnn_text_classification.ipynb) but still getting error at line # 26\r\n@tilakrayal I can see it works ok in Google colab but there is definitely a problem when someone runs this on local environment that has following,\r\ntf 2.5\r\nCUDA 11.2 (Windows 10), NVIDIA Titan RTX\r\nPython 3.8.5\r\nAlso many others are facing same problem in https://github.com/tensorflow/tensorflow/issues/33721 so it is not just me!", "Have you tried reducing your batch size to a smaller value in addition to limiting gpu memory growth?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50296\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50296\">No</a>\n"]}, {"number": 50295, "title": "[INTEL MKL] Fix to Fused Batch Normalization Primitive Caching", "body": "This PR is a bug fix to oneDNN enabled fused_batch_normalization primitive caching. Earlier code did not use data_format as a part of hash-key for primitive retrieval from cache. As a result when a graph has both NHWC and NCHW format earlier code was using same primitive which is not correct.", "comments": []}, {"number": 50294, "title": "[Intel MKL] Fix for eager_c_api_test with oneDNN Enabled", "body": "This fix resolves the failure with //tensorflow/c/eager:c_api_test running with oneDNN enabled with stock TF under a config=CUDA build executed on a GPU by preventing mkl_eager_op_rewrite for non cpu devices.\r\n", "comments": []}, {"number": 50293, "title": "Update release notes for 2.1-2.4 patch releases", "body": "", "comments": []}, {"number": 50292, "title": "When CUDA libs cannot be loaded, TF should exit", "body": "When a job cannot acquire GPU due to some CUDA libs missing, TF should fail early instead of falling back to CPU.  \r\nIf TF ignores the error and continue, it could potentially cause unexpected behavior in MWMS - E.g one worker has GPU while one doesn't. Currently when MWMS is running with mixed setup like this, it just hung.\r\n\r\nBased on the discussion where the original change of falling back to CPU when CUDA lib cannot be loaded(https://github.com/tensorflow/tensorflow/commit/fcfb99bc3aa3110feae5233dbe4634bffa70dc59) was originated from: https://groups.google.com/a/tensorflow.org/g/developers/c/MUq0ee3g-xk/m/4H84DeiAAQAJ , it mentioned two options facing CUDA lib not being installed properly\r\n 1. fallback to CPU\r\n 2. exit with error message.\r\n\r\noption 2 makes more sense to me as I stated above.", "comments": ["@rohan100jain would you mind taking a look?", "@sanjoy @mihaimaruseac can you take a look? This might break a lot of users but might be a clearer UX. One naive question is that do we distinguish between folks having an NVIDIA driver but the wrong CUDA setup vs folks not having any NVIDIA gpus at all?", "There's also the issue that users installing `tensorflow` on systems with no GPU will get a broken package if this lands. Any thing they try to do on TF would result in an exit.\r\n\r\nShould we instead control this via an enviroment variable / config flag?", "@mihaimaruseac in this case, TF won't even invoke this gpu_device.cc. It will only be invoked when gpu is available and required by user. ", "Oh, thank you, then I guess we can take this in and run some testing after it gets into tf-nightly to prevent unwanted regression", "@mihaimaruseac sounds good, thanks!", "> @sanjoy @mihaimaruseac can you take a look? This might break a lot of users but might be a clearer UX. One naive question is that do we distinguish between folks having an NVIDIA driver but the wrong CUDA setup vs folks not having any NVIDIA gpus at all?\n\nTo answer your question, yes, we distinguish these two cases - the gpu_device.cc is invoked only when gpu is available to the tf process.", "@mihaimaruseac it seems build is failing due to some unrelated test cases failing. Any idea on this?", "@gbaned how about this PR? anything I can help to get it merged?", "> This looks fine overall, but let's land this (immediately) after TF 2.6 is cut so that users have time to complain if it breaks them.\r\n\r\n@sanjoy Just an FYI that this breaks pluggable devices that were previously depending on the `tf-nightly` package when CUDA libraires are not installed. This is not blocking since people can use `tf-nightly-cpu` instead, but the [pluggable devices tutorial](https://github.com/tensorflow/community/pull/352/files) says to install the former which can be confusing.", "> @sanjoy Just an FYI that this breaks pluggable devices that were previously depending on the `tf-nightly` package when CUDA libraires are not installed. This is not blocking since people can use `tf-nightly-cpu` instead, but the [pluggable devices tutorial](https://github.com/tensorflow/community/pull/352/files) says to install the former which can be confusing.\r\n\r\nThanks, looks like being cautious here w.r.t. TF 2.6 paid off. :)\r\n\r\nCC @penpornk to advise on PluggableDevice.", "@PatriceVignola That part of the tutorial was written before PluggableDevice landed in TF 2.5. Now the users can just install TF 2.6 (or 2.5) instead of the nightly packages. We'll update the tutorial. \r\n\r\n> Thanks, looks like being cautious here w.r.t. TF 2.6 paid off. :)\r\n\r\nThank you for landing this after 2.6, @sanjoy! :)", "I actually think we should roll this back.\r\n\r\nNow users that install tensorflow (tf-nightly for now, but from TF 2.7 where this PR exists) won't be able to use it if they don't have CUDA libs installed:\r\n\r\n```\r\n(venv) mihaimaruseac@ankh:/tmp/tff$ python\r\nPython 3.9.2 (default, Feb 28 2021, 17:03:44) \r\n[GCC 10.2.1 20210110] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-07-28 12:57:55.695347: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-07-28 12:57:55.695388: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>> tf.constant([1,2,3])\r\n2021-07-28 12:58:18.836570: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-07-28 12:58:18.836700: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\r\n2021-07-28 12:58:18.836808: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\r\n2021-07-28 12:58:18.836905: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2021-07-28 12:58:18.837002: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2021-07-28 12:58:18.837098: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\r\n2021-07-28 12:58:18.837193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\r\n2021-07-28 12:58:18.837297: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\r\n2021-07-28 12:58:18.837321: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1829] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/tmp/tff/venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 271, in constant\r\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n  File \"/tmp/tff/venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 283, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"/tmp/tff/venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 308, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/tmp/tff/venv/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 105, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File \"/tmp/tff/venv/lib/python3.9/site-packages/tensorflow/python/eager/context.py\", line 538, in ensure_initialized\r\n    config_str = self.config.SerializeToString()\r\n  File \"/tmp/tff/venv/lib/python3.9/site-packages/tensorflow/python/eager/context.py\", line 987, in config\r\n    self._initialize_physical_devices()\r\n  File \"/tmp/tff/venv/lib/python3.9/site-packages/tensorflow/python/eager/context.py\", line 1320, in _initialize_physical_devices\r\n    devs = pywrap_tfe.TF_ListPhysicalDevices()\r\ntensorflow.python.framework.errors_impl.InternalError: Cannot dlopen all CUDA libraries.\r\n```\r\n\r\nNote, there is no explicit call of a CUDA operation. We should just fallback to the CPU implementation here.", "@mihaimaruseac thanks, I will work on this today.", "Note that there are also reasons to want GPU and not CUDA. This broke all of our vulkan tests when we tried to upgrade our TF pip package."]}, {"number": 50291, "title": "[tf.data] add AutotuneOptions for performance tuning of dataset operations", "body": "This PR adds the `tf.data.AutotuneOptions` class, which is a split of options from `tf.data.experimental.OptimizationOptions`.\r\n\r\n- [x] add new proto message for `AutotuneOptions` and extend the `Options` message.\r\n- [x] add `tf.data.AutotuneOptions` class.\r\n- [x] add `autotune` attribute to `Options`.\r\n- [x] regenerate golden APIs.\r\n- [x] handle backward-comaptibility with `options.experimental_optimization` attributes.\r\n- [x] add/modify `options_test` w.r.t the new `autotune` attribute.\r\n- [x] switch to `autotune_options` in:\r\n    - [x] `tensorflow/core/kernels/data/finalize_dataset_op.cc`\r\n    - [x] `tensorflow/core/data/dataset_utils.cc`\r\n    - [x] `tensorflow/core/data/root_dataset.cc `\r\n- [x] update RELEASE.md.\r\n\r\nTEST LOG\r\n```\r\nIINFO: Elapsed time: 42.805s, Critical Path: 41.39s\r\nINFO: 354 processes: 130 internal, 224 local.\r\nINFO: Build completed successfully, 354 total actions\r\n//tensorflow/python/data/kernel_tests:options_test                       PASSED in 3.6s\r\n\r\nINFO: Build completed successfully, 354 total actions\r\n\r\n---------------\r\n\r\nINFO: Found 13 test targets...\r\nINFO: Elapsed time: 23.775s, Critical Path: 22.98s\r\nINFO: 44 processes: 31 internal, 13 local.\r\nINFO: Build completed successfully, 44 total actions\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:map_parallelization_test (cached) PASSED in 3.8s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:autotune_buffer_sizes_test PASSED in 5.0s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:filter_fusion_test PASSED in 13.6s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:grappler_test PASSED in 5.6s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:grappler_test_gpu PASSED in 5.7s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:hoist_random_uniform_test PASSED in 6.1s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:map_and_batch_fusion_test PASSED in 5.6s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:map_and_filter_fusion_test PASSED in 9.0s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:map_fusion_test PASSED in 9.8s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:noop_elimination_test PASSED in 6.6s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:reorder_data_discarding_ops_test PASSED in 5.5s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:shuffle_and_repeat_fusion_test PASSED in 5.5s\r\n//tensorflow/python/data/experimental/kernel_tests/optimization:optimization_test PASSED in 23.0s\r\n  Stats over 2 runs: max = 23.0s, min = 7.0s, avg = 15.0s, dev = 8.0s\r\n\r\n```\r\n\r\ncc: @jsimsa Let me know if I have missed something. Thanks!", "comments": ["Thank you! Would it be possible for this CL to avoid the redundancy between `experimental_optimization.autotune*` options and `autotune.*` options? Could we remove the former and add a mechanism that would reroute getting and setting of the deprecated attributes to the new attributes? (similar to my changes to ThreadingOptions)", "@jsimsa made the changes. Please let me know how it looks.", "@jsimsa can you please take a look.", "@kvignesh1420 I have limited bandwidth to help you work on this PR, so let's put this on hold for now ... it is okay if this does not make it into the TF 2.6 release", "@jsimsa apologies for the continuous requests. I wanted to finish this before the release so was a bit hasty. Thanks again!", "@kvignesh1420  Can you please resolve conflicts? Thanks!", "My suggestion would be to close this PR. I plan to work on this myself later this quarter.", "@jsimsa okay!"]}, {"number": 50290, "title": "[TFLite] Update MultiplyByQuantizedMultiplier to use single-rounding instead of double-rounding", "body": "Hello,\r\n\r\nThis PR changes the rounding of the int32 `MultiplyByQuantizedMultiplier` methods to a single-rounding instead of a double-rounding in a way similar to what was done in https://github.com/google/ruy/pull/227 and adapts the tests to the change.\r\n\r\nThe new int32 `MultiplyByQuantizedMultiplier` adds a restriction on the quantized multiplier, it must now be a positive integer in the same way as in the int64 version of `MultiplyByQuantizedMultiplier`. This restriction also matches the one in the quantization scaling defined in the TOSA specification. To achieve that the SUB and DIV operators were slightly adapted to avoid any negative quantized multiplier.\r\n\r\n\r\nThibaut\r\n", "comments": ["Thank you for the review. \r\n\r\n@renjie-liu I split the PR in two. The https://github.com/tensorflow/tensorflow/pull/50615 PR changes the SUB and DIV kernels to avoid the usage of negative quantized multipliers in `MultiplyByQuantizedMuliplier` and I left this one to concentrate on the `MultiplyByQuantizedMuliplier` rounding changes and the tests adaptation. The first one should be merged before this one for the tests to pass.", "Looking into the failing tests (https://source.cloud.google.com/results/invocations/a9cc4ded-4646-4e73-9355-654127ead78c/targets) it seems there are still some rounding problems with the depth-wise convolution.\r\n\r\nThe uint8 DepthwiseConv kernel has a configurable rounding through the [DepthwiseConvOutputRounding](https://github.com/tensorflow/tensorflow/blob/aff4216fafd55a107ec617d534b5b49ef31f475a/tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h#L47) enum. I was wondering if there was a reason for it as for what I know no other kernel has such configurable rounding? Should I just modify all the calls to use the same rounding or could I eventually remove this configurability altogether and use `MultiplyByQuantizedMultiplier` everywhere?\r\n\r\nThere is also some differences between the reference uint8 kernel that uses [kAwayFromZero](https://github.com/tensorflow/tensorflow/blob/54a8a3b373918bed29d7cfbdb8fd0a6c6065967c/tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h#L280) while the optimized uint8 kernel uses [kUpward](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h#L2112).", "Sorry about the long hold, I think we can deprecate the `AwayFromZero` path but only take the `Upward` path.\r\n\r\nThe `Upward` path should be very similar to what you did, and we already allow the rounding difference for the test.", "@renjie-liu I updated the PR and adapted the `DepthwiseConvRound` rounding to the change in `MultiplyByQuantizedMultiplier`. I also adapted some kernels that were using `DepthwiseConvOutputRounding::kAwayFromZero` to now use `DepthwiseConvOutputRounding::kUpward`. \r\n\r\nNote also that https://github.com/tensorflow/tensorflow/pull/50615#issuecomment-879788763 will need to be taken care of before testing and merging this PR.\r\n\r\n@jalexstark Thank you for the review.\r\n\r\nI don't know any kernel that initializes the accumulator with a quantized 0.5 value, do you eventually have an example? As you said it'll not work if there is a non-shift multiplication (i.e. `quantized_multiplier != 1`) and as the old `MultiplyByQuantizedMultiplier` was already doing a double-rounding it seems odd that some kernels would add an extra rounding by initializing the accumulator to a quantized 0.5.", "@jalexstark I tried with 10 000 and the tests go well but there are some failures with 100 000 though they also appear without the `MultiplyByQuantizedMultiplier` change.", "> @jalexstark I tried with 10 000 and the tests go well but there are some failures with 100 000 though they also appear without the `MultiplyByQuantizedMultiplier` change.\r\n\r\nI suppose that we have to live with that. The difficulty has been that the burden of fuzz testing the numerics was a bit high, so the test size was reduced. Thanks.", "Thanks Tessil. Could we add some unit test for the logic so this PR itself is test? (I understand we have most tests passed before adding the #if macro).\r\n\r\nProbably adding a test similar to the tests in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util_test.cc#L430 and we can use \"defines = \" (example: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/BUILD#L137-L139) to enable the flag.", "Thanks @jianlijianli I added a `quantization_util_test_single_rounding` test binary with `TFLITE_SINGLE_ROUNDING=1` in addition of `quantization_util_test` with `TFLITE_SINGLE_ROUNDING=0`.\r\n\r\nIn addition of this change I also changed the max `input_beta_real_multiplier` in `PreprocessSoftmaxScaling` to accommodate the single-rounding restrictions on the `shift` so that the tests can pass. This change isn't protected by the `TFLITE_SINGLE_ROUNDING` constant as it's a case that shouldn't really occur in practice (we would need an `input_beta_real_multiplier >= (1 << 30)`) but I can add an `#ifdef` if you think it's necessary.", "Thanks Tessil, for making all the improvements. I added a few comments. Please take a look.\r\n\r\n> shouldn't really occur in practice (we would need an input_beta_real_multiplier >= (1 << 30)) but I can add an #ifdef if you think it's necessary.\r\n\r\nUnderstand it's unlikely to happen, but let's add a #if just to be on the safe side. We can remove those after we toggle the flag.\r\n\r\nThanks!", "Thanks, I added the necessary `#if` and kept the old saturating behaviour as the default.", "Thanks Tessil. The PR looks great.", "Thanks @jianlijianli for the review, I added some comments.", "@jianlijianli Can you please review this PR ? Thanks!", "@Tessil Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned Thanks, the latest commit should fix it."]}, {"number": 50289, "title": "[ROCm] Making C++14 as the default when building TF with --config=rocm", "body": "Currently when building TF with `--config=rocm`, we default to using C++11 (`-std=c++11` is explicitly added as a `cxx_flag` inthe crosstool wrapper).\r\n\r\nLinux CPU builds (i.e. non-ROCm builds) already use C++14 as the default, and hipcc/hipclang will soon update the default to be C++14 as well.\r\n\r\nThis PR/commit updates the crosstool wrapper to add `-std=c++14` as the `cxx_flag` (instead of the current `-std=c++11`)\r\n\r\n------------------------\r\n\r\n/cc @cheshire @chsigg ", "comments": ["@cheshire gentle ping"]}, {"number": 50288, "title": "Keras generic_utils Mangles ConvLSTM2D Default Layer Name", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.9.4\r\n\r\n\r\n**Describe the current behavior**\r\n```python\r\n>>> import tensorflow as tf\r\n>>> tf.keras.layers.ConvLSTM2D(1, 1).name\r\n'conv_lst_m2d_0'\r\n```\r\n\r\n**Describe the expected behavior**\r\n```python\r\n>>> import tensorflow as tf\r\n>>> tf.keras.layers.ConvLSTM2D(1, 1).name\r\n'conv_lstm_2d_0'\r\n```\r\n\r\n**Other info**\r\nProblem caused here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L2435\r\nIf a name is not provided to a `Layer` then it used the `generic_utils` function `to_snake_case`, which shows unexpected behavior here:\r\n```python\r\n>>> from tensorflow.python.keras.utils import generic_utils\r\n>>> generic_utils.to_snake_case('ConvLSTM2D')\r\n'conv_lst_m2d'\r\n```\r\n\r\nAn ad hoc fix here seems inelegant in `generic_utils`. Moreover, changing the regex in the linked function will likely have repercussions for many other names where this may not be an issue.\r\n\r\nThoughts on setting a default name within ConvLSTM2D __init__ instead? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional_recurrent.py#L154\r\nSomething passed to super like `name=kwargs.get('name', backend.unique_object_name('conv_lstm_2d', zero_based=True))`? I don't have an elegant solution here.", "comments": ["Ok here is a quick fix which appears to me at least to not affect anything other than intended. Bonus is a special naming case for ReLU, which doesn't need to be included. All I did beyond that is remove the numerical regex pattern in the first regex line.\r\n\r\n```python\r\nimport re\r\nfrom tensorflow.python.keras.utils.generic_utils import to_snake_case\r\nfrom tensorflow.python.keras.engine.base_layer import Layer\r\n\r\nlayers = Layer.__subclasses__()\r\n\r\n\r\ndef to_snake_case_proposed(name):\r\n    name = name.replace('ReLU', 'Relu')\r\n    intermediate = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\r\n    insecure = re.sub('([a-z])([A-Z])', r'\\1_\\2', intermediate).lower()\r\n    # If the class is private the name starts with \"_\" which is not secure\r\n    # for creating scopes. We prefix the name with \"private\" in this case.\r\n    if insecure[0] != '_':\r\n        return insecure\r\n    return 'private' + insecure\r\n\r\n\r\nfor layer in sorted(layers, key=lambda x: x.__name__):\r\n    current = to_snake_case(layer.__name__)\r\n    proposed = to_snake_case_proposed(layer.__name__)\r\n    if current != proposed:\r\n        print(current, proposed)\r\n```\r\n\r\nOutput (left is current behavior/default, right is with the proposed change):\r\n\r\n```text\r\nconv_lst_m2d_cell conv_lstm2d_cell\r\nleaky_re_lu leaky_relu\r\np_re_lu p_relu\r\nre_lu relu\r\nthresholded_re_lu thresholded_relu\r\n```\r\nLooks like better defaults to me.", "@craymichael Could you please submit a PR to fix the issue if you are aware of the changes required. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I opened PR #50374 to resolve this issue, awaiting feedback from a tensorflower", "As mentioned here https://github.com/tensorflow/tensorflow/pull/50374#issuecomment-884517069 , the issue has been fixed in a separate change , closing this issue. Thank you ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50288\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50288\">No</a>\n"]}, {"number": 50287, "title": "Update learning_rate_schedule.py", "body": "Make sure math.pi has the same dtype as completed_fraction\r\n\r\nThe dtype of initial_learning_rate could be different from math.pi and this can lead to problems like:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nschedule = tf.keras.optimizers.schedules.CosineDecay(\r\n  initial_learning_rate=np.float64(0.001),\r\n  decay_steps=100\r\n)\r\nschedule(1)\r\n```\r\n```\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py in __call__(self, step)\r\n    630       completed_fraction = global_step_recomp / decay_steps\r\n    631       cosine_decayed = 0.5 * (1.0 + math_ops.cos(\r\n--> 632           constant_op.constant(math.pi) * completed_fraction))\r\n    633 \r\n    634       decayed = (1 - self.alpha) * cosine_decayed + self.alpha\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\r\n   1232         #   r_binary_op_wrapper use different force_same_dtype values.\r\n   1233         x, y = maybe_promote_tensors(x, y, force_same_dtype=False)\r\n-> 1234         return func(x, y, name=name)\r\n   1235       except (TypeError, ValueError) as e:\r\n   1236         # Even if dispatching the op failed, the RHS may be a tensor aware\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in _mul_dispatch(x, y, name)\r\n   1573     return sparse_tensor.SparseTensor(y.indices, new_vals, y.dense_shape)\r\n   1574   else:\r\n-> 1575     return multiply(x, y, name=name)\r\n   1576 \r\n   1577 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    204     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    205     try:\r\n--> 206       return target(*args, **kwargs)\r\n    207     except (TypeError, ValueError):\r\n    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in multiply(x, y, name)\r\n    528   \"\"\"\r\n    529 \r\n--> 530   return gen_math_ops.mul(x, y, name)\r\n    531 \r\n    532 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py in mul(x, y, name)\r\n   6238       return _result\r\n   6239     except _core._NotOkStatusException as e:\r\n-> 6240       _ops.raise_from_not_ok_status(e, name)\r\n   6241     except _core._FallbackException:\r\n   6242       pass\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6895   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6896   # pylint: disable=protected-access\r\n-> 6897   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6898   # pylint: enable=protected-access\r\n   6899 \r\n\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:Mul]\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50287) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "The corresponding change has been made to keras-team/keras. Closing this PR now.", "Thank you for merging the changes."]}, {"number": 50286, "title": "Keras Reshape layers are not folded when converted to TFLite", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11, Ubuntu 20.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): `2.5.0`, `tf-nightly`\r\n\r\n### 2. Code\r\n\r\nKeras `Reshape` layers are not correctly constant folded when converted to TFLite.\r\n\r\nConsider the following two models which either use `tf.keras.layers.Reshape((50, 2))(x)`  or `tf.reshape(x, (-1, 50, 2))` to reshape tensors. The Keras layer here is equivalent to  calling `tf.reshape(x, (tf.shape(x)[0], 50, 2))` which unfortunately doesn't get fused due to a dynamic batch dimension.\r\nThis can be worked around by manually setting the Keras model batch size to 1, but I think this should be handled by the converter instead so that the default Keras reshape layer also gets converted to an efficient reshape op.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef unfused_model():\r\n    img = tf.keras.layers.Input(shape=(3, 3, 10))\r\n    x = tf.keras.layers.Conv2D(100, (3, 3))(img)\r\n    x = tf.keras.layers.Reshape((50, 2))(x)\r\n    return tf.keras.Model(img, x)\r\n\r\ndef fused_model():\r\n    img = tf.keras.layers.Input(shape=(3, 3, 10))\r\n    x = tf.keras.layers.Conv2D(100, (3, 3))(img)\r\n    x = tf.reshape(x, (-1, 50, 2))\r\n    return tf.keras.Model(img, x)\r\n\r\ndef convert_model(model, filename):\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    with open(filename, \"wb\") as f:\r\n        f.write(converter.convert())\r\n\r\nconvert_model(unfused_model(), \"/tmp/unfused_model.tflite\")\r\nconvert_model(fused_model(), \"/tmp/fused_model.tflite\")\r\n```\r\n\r\n`unfused_model.tflite` | `fused_model.tflite`\r\n---|---\r\n<img width=\"219\" alt=\"Screenshot 2021-06-15 at 18 36 12\" src=\"https://user-images.githubusercontent.com/13285808/122098756-207ca800-ce09-11eb-9880-2b91b147cb5d.png\"> | <img width=\"145\" alt=\"Screenshot 2021-06-15 at 18 36 05\" src=\"https://user-images.githubusercontent.com/13285808/122098753-1fe41180-ce09-11eb-9530-fc319eac8097.png\">\r\n\r\n### 3. Possible Solution\r\n\r\nI think it would be possible to implement a `tfl.reshape` constant folder even for the dynamic batch size case that checks if only one dimension of the reshape is dynamic and replaces this with `-1`. However, this would opt out of possible shape error checking in cases the reshape would be invalid. On the other hand from looking at the netron graph it seems like the batch dimension is fixed to 1 anyway, so I am not sure why the reshape didn't get folded correctly in the first place, but let me know if I am missing something.", "comments": ["Thanks for filing up the feature request. Also, we are welcoming the community contribution in TensorFlow Lite MLIR converter regarding this feature request.", "> Also, we are welcoming the community contribution in TensorFlow Lite MLIR converter regarding this feature request.\r\n\r\nI am not sure if I would consider this as feature request, to me it seems more like a bug in the fusion rules which people tend to run into when using e.g. the Keras application MobileNets (see https://github.com/tensorflow/tensorflow/issues/45256#issuecomment-737985193).\r\n\r\nKeras Reshape layers used to be converted correctly in TensorFlow version 2.2 with the MLIR graphdef converter since the batch size was always set to one before passing it into the MLIR converter:\r\nhttps://github.com/tensorflow/tensorflow/blob/691563cb3e9c0d46058e2a7b8d270bc29a0dfe19/tensorflow/lite/python/lite.py#L824-L827\r\n\r\n@abattery in the current converter it still looks like the batch size is fixed to one at some point when looking at the tflite flatbuffer (although I am not sure where this happens), or is it intended that the batch size stays dynamic? If so could you comment on the proposed solution I outlined above, since I am not 100% sure what the desired behaviour in case of dynamic dimension would be here.", "I have to say that this problem stil exist in the latest version of tensorflow (colab 2.7.0)\r\n\r\nIt prevents me from generating a working model on the Arduino 33 ble.\r\n\r\nThis is a serious problem because we cannot even run this [official demo](https://www.tensorflow.org/lite/performance/post_training_integer_quant)", "The solution should be resolved by setting Keras `batch_size` to 1. As this issue has a workaround and doesn't block major workflows, it will be marked as closed. "]}, {"number": 50285, "title": "Device interconnect StreamExecutor with strength 1 edge matrix", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu \r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.6.5\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am running the BoostedTree, and for the only binary problem, I faced the following error.\r\nI am using the CPU version of the TensorFlow, and it is working fine for multi-class classification\r\n```python\r\nSkipping registering GPU devices...\r\n2021-06-15 17:58:51.972609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-06-15 17:58:51.972635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \r\n2021-06-15 17:58:51.972653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N \r\n2021-06-15 17:58:51.973854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N \r\n```\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n```python\r\n installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n\r\n```\r\n", "comments": ["@samanemami ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\nThanks!\r\n\r\n\r\n\r\n", "Also please try to execute code in virtual environment and limiting GPU [memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) using any of the methods listed in this guide and check if it helps.Thanks!", "@tilakrayal  Thank you.\r\nI fixed the issue by limiting the GPU."]}, {"number": 50284, "title": "[ROCm] Fix for a bug in the ROCMFftPlan::UpdateScratchAllocator routine.", "body": "This was the cause of some unit-test failures in JAX...see the following github issue for more details\r\n\r\nhttps://github.com/ROCmSoftwarePlatform/frameworks-internal/issues/240\r\n\r\nmany thanks to @ekuznetsov139 for coming up with this fix.\r\n\r\n--------------------------------\r\n\r\n/cc @cheshire @chsigg @hawkinsp ", "comments": []}, {"number": 50283, "title": "Fix incorrect values in recorded_allocation on BE machines", "body": "This PR addresses incorrect `count` and `requested_bytes` values in `recorded_allocation` when testing model allocations.\r\n\r\nTest case `//tensorflow/lite/micro:recording_micro_allocator_test` fails on Big-Endian systems because `RecordingSimpleMemoryAllocator::AllocateFromTail()` would be called one more time and the `alloc_count_` and `requested_tail_bytes_` fields would be added repeatedly.\r\n\r\n`FlatBufferVectorToTfLiteTypeArray()` function in `micro_allocated.cc` has been modified to fix this by invoking the base class version of `AllocateFromTail()` function (`allocator->SimpleMemoryAllocator::AllocateFromTail()`) on BE machines, rather than the derived class version (`allocator->AllocateFromTail()`) which might update the value of `alloc_count_` and `requested_tail_bytes_\u00a0fields` besides allocating the space. \r\n\r\nThis PR will solve the issue #45455.\t", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Hi @advaitjain , could you please take a look at this PR? Thank you very much!", "@kun-lu20 Can you please resolve conflicts? Thanks!", "Hi @gbaned , I think the reason of conflicts is that the TF Lite Micro codebase has been moved out of TensorFlow repo. Should I move the code changes to the new TFLM repo? Thanks!", "The change from this PR looks reasonable to me. Please create a PR in the [tflite-micro](https://www.github.com/tensorflow/tflite-micro) repo.", "@advaitjain Thanks! will do."]}, {"number": 50282, "title": "mix precison bfloat16 lstm/conv1d support", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 **2.5.0**\r\n\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: 32G, tesla V100\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import mixed_precision\r\n \r\npolicy = mixed_precision.Policy('bfloat16')  #'mixed_float16' 'mixed_bfloat16'\u7b49\u591a\u79cd\u9009\u62e9\uff0c\u540e\u7eed\u7684\u90fd\u4f1a\u662f\u8be5\u8bbe\u7f6e\u7684\u7c7b\u578b\r\nmixed_precision.set_global_policy(policy)\r\nprint('Compute dtype: %s' % policy.compute_dtype)\r\nprint('Variable dtype: %s' % policy.variable_dtype)\r\n \r\n\r\ninputs = keras.Input(shape=(784,), name='digits')\r\nif tf.config.list_physical_devices('GPU'):\r\n  print('The model will run with 4096 units on a GPU')\r\n  num_units = 4096\r\nelse:\r\n  print('The model will run with 64 units on a CPU')\r\n  num_units = 64\r\n \r\n#dense test #\u6d4b\u8bd5cpu/GPU\u4e0a\u53ef\u4ee5\u8dd1\r\ndense1 = layers.Dense(num_units, activation='relu', name='dense_1')\r\nx = dense1(inputs)\r\ndense2 = layers.Dense(num_units, activation='relu', name='dense_2')\r\nx = dense2(x)\r\n \r\n#lstm gpu test\r\ninputs = tf.random.normal([32, 10, 8])\r\nlstm = tf.keras.layers.LSTM(4)\r\noutput = lstm(inputs)\r\nprint(output.shape)\r\n\"\"\"\r\n>>> output = lstm(inputs)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 668, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1023, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2625, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 586, in build\r\n    self.cell.build(step_input_shape)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\", line 270, in wrapper\r\n    output_shape = fn(instance, input_shape)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2361, in build\r\n    self.recurrent_kernel = self.add_weight(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 639, in add_weight\r\n    variable = self._add_variable_with_custom_getter(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 810, in _add_variable_wi\r\nth_custom_getter\r\n    new_variable = getter(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 127, in make_vari\r\nable\r\n    return tf_variables.VariableV1(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 260, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 206, in _variable_v1_call\r\n    return previous_getter(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 199, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\", line 2612, in default_variable_cr\r\neator\r\n    return resource_variable_ops.ResourceVariable(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1584, in __init__\r\n    self._init_from_args(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1722, in _init_from_a\r\nrgs\r\n    initial_value = initial_value()\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/initializers/initializers_v2.py\", line 606, in __ca\r\nll__\r\n    q, r = gen_linalg_ops.qr(a, full_matrices=False)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/gen_linalg_ops.py\", line 2072, in qr\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of bfloat16 is not in the list of allowed values: double, float, half, comple\r\nx64, complex128\r\n        ; NodeDef: {{node Qr}}; Op<name=Qr; signature=input:T -> q:T, r:T; attr=full_matrices:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT\r\n_HALF, DT_COMPLEX64, DT_COMPLEX128]> [Op:Qr]\r\n```\r\n\"\"\"\r\n \r\n#conv1d gpu test\r\ninput_shape = (4, 10, 128)\r\nx = tf.random.normal(input_shape)\r\ny = tf.keras.layers.Conv1D(32, 3, activation='relu',input_shape=input_shape[1:])(x)\r\n'''\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1030, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 249, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 1012, in convolution_v2\r\n    return convolution_internal(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 1142, in convolution_internal\r\n    return op(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 602, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 602, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 1907, in conv1d\r\n    return array_ops.squeeze(result, [spatial_start_dim])\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 535, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 4471, in squeeze\r\n    return gen_array_ops.squeeze(input, axis, name)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 10176, in squeeze\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Tried to squeeze dim index -3 for tensor with 1 dimensions. [Op:Squeeze\r\n'''\r\n#conv2d gpu test\r\ninput_shape = (4, 28, 28, 3)\r\nx = tf.random.normal(input_shape)\r\ny = tf.keras.layers.Conv2D(2, 3, activation='relu', input_shape=input_shape[1:])(x)\r\n'''\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1030, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 267, in call\r\n    outputs = nn.bias_add(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 3377, in bias_add\r\n    return gen_nn_ops.bias_add(\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 678, in bias_add\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute BiasAdd as input #1(zero-based) was expected to be a float tensor but is a bfloat16 tensor [Op:BiasAdd]\r\n'''\r\n**Describe the expected behavior**\r\nlstm/conv1d run successfully as dense layers\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@BuaaAlban Was able to reproduce the issue in TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/acab2c7110dbad7ca7f1acef921ff160/untitled.ipynb).Thanks!", "Based on error message, it looks like only double, float, half, complex64, complex128 dtypes are implemented.\r\n\r\n```\r\nInvalidArgumentError: Value for attr 'T' of bfloat16 is not in the list of allowed values: double, float, half, complex64, complex128\r\n\t; NodeDef: {{node Qr}}; Op<name=Qr; signature=input:T -> q:T, r:T; attr=full_matrices:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]> [Op:Qr]\r\n```\r\n\r\nCan you please open it as a feature in keras-team/keras repo as Keras development moved to that repo.\r\n\r\nKeras development moved to another repository to focus on only keras. Could you please repost this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50282\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50282\">No</a>\n"]}]