[{"number": 49481, "title": "Fix divide by zero error in `fractional_pool_common.cc`.", "body": "PiperOrigin-RevId: 371126221\nChange-Id: Iea4b2f363aaeb116ab460e3bc592c687484af344", "comments": []}, {"number": 49480, "title": "Fix divide by zero error in `fractional_pool_common.cc`.", "body": "PiperOrigin-RevId: 371126221\nChange-Id: Iea4b2f363aaeb116ab460e3bc592c687484af344", "comments": []}, {"number": 49479, "title": "Fix divide by zero error in `fractional_pool_common.cc`.", "body": "PiperOrigin-RevId: 371126221\nChange-Id: Iea4b2f363aaeb116ab460e3bc592c687484af344", "comments": []}, {"number": 49478, "title": "Fix divide by zero error in `fractional_pool_common.cc`.", "body": "PiperOrigin-RevId: 371126221\nChange-Id: Iea4b2f363aaeb116ab460e3bc592c687484af344", "comments": []}, {"number": 49477, "title": "Validate work in `QuantizedAdd`, ensure at least one element.", "body": "PiperOrigin-RevId: 370127996\nChange-Id: I57c6f3e01afdeada84737820a131590137463855", "comments": []}, {"number": 49476, "title": "Validate work in `QuantizedAdd`, ensure at least one element.", "body": "PiperOrigin-RevId: 370127996\nChange-Id: I57c6f3e01afdeada84737820a131590137463855", "comments": []}, {"number": 49475, "title": "Validate work in `QuantizedAdd`, ensure at least one element.", "body": "PiperOrigin-RevId: 370127996\nChange-Id: I57c6f3e01afdeada84737820a131590137463855", "comments": []}, {"number": 49474, "title": "Validate work in `QuantizedAdd`, ensure at least one element.", "body": "PiperOrigin-RevId: 370127996\nChange-Id: I57c6f3e01afdeada84737820a131590137463855", "comments": []}, {"number": 49473, "title": "Incorrect requirements.txt in Magic Wand Example", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**\r\n- TensorFlow installed from (source or binary): **Source** \r\n- Tensorflow version (commit SHA if source): f26800a1e5b1199cfc1a5aca916edc836b541687\r\n- TensorFlow Python package version: **2.4.0 (attempted)**\r\n- Python version: **Python 3.8 64-bit**\r\n- Pip version: **Pip 21.1.1**\r\n\r\n**Describe the problem**\r\n\r\nFor the [TensorFlow Lite Micro Magic Wand example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/magic_wand), attempting to install requirements following instructions in `README.md` fails.\r\n\r\n```\r\nCollecting numpy==1.16.2\r\n  Downloading numpy-1.16.2.zip (5.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.1 MB 477 kB/s \r\n...\r\nERROR: Cannot install numpy==1.16.2 and tensorflow==2.4.0 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    The user requested numpy==1.16.2\r\n    tensorflow 2.4.0 depends on numpy~=1.19.2\r\n```\r\n\r\nEven when numpy is manually updated to 1.19.2 and the model is trained, (I think) tensorflow 2.4.0 doesn't recognize the operation:\r\n\r\n```\r\n...\r\nDidn't find op for builtin opcode 'RESHAPE' version '1'\r\nFailed to get registration from op code RESHAPE\r\nFailed starting model allocation.\r\n...\r\n```\r\n\r\n*Caveat*\r\n\r\nTo get the above result, I converted the Lite model with `xxd -i model.tflite > magic_wand_model_data.cc`, copied the model data and length from the TensorFlow tree into [the Zephyr tree's TensorFlow Magic Wand sample](https://github.com/zephyrproject-rtos/zephyr/tree/main/samples/modules/tensorflow/magic_wand) `magic_wand_model_data.cc`, and ran it on a Renode-emulated `litex_vexriscv` board.\r\n\r\nMaybe there's a factor in that process that could've introduced an issue (and in that case, my apologies for filing this issue), but I also re-did that process after changing the `requirements.txt` back to the previous commit's `numpy==1.16.2` and `tensorflow==2.0.0-beta1`:\r\n\r\n```\r\ncd tensorflow/lite/micro/examples/magic_wand/train\r\npython3.7 -m venv ./venv # following README.md\r\nsource venv/bin/activate\r\npip install --upgrade pip # if necessary\r\npip install -r requirements.txt\r\n...\r\n```\r\n\r\nI didn't have the same issues during operation (although the model didn't recognize the sample inputs correctly this time I trained it - it's supposed to go RING CIRCLE RING CIRCLE):\r\n\r\n```\r\n*** Booting Zephyr OS build v2.6.0-rc1-310-g2a9b32d43f31  ***\r\nGot accelerometer, label: accel-0\r\nWING:\r\n*         *         *\r\n *       * *       *\r\n  *     *   *     *\r\n   *   *     *   *\r\n    * *       * *\r\n     *         *\r\nWING:\r\n...\r\nWING:\r\n...\r\n```\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n```\r\ncd tensorflow/lite/micro/examples/magic_wand/train\r\npython3.8 -m venv ./venv\r\nsource venv/bin/activate\r\npip install --upgrade pip # if necessary\r\npip install -r requirements.txt\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n`pip install -r requirements.txt` should work and the scripts should work correctly with the TensorFlow version in the `requirements.txt`.\r\n\r\n**Possible solutions**\r\n\r\n- Downgrade requirements.txt `tensorflow==2.0.0-beta1`\r\n\r\nOR\r\n\r\n- Upgrade `numpy==1.19.2` to match `tensorflow==2.4.0`\r\n- Rewrite scripts to accommodate TensorFlow 2.4.0 (?)\r\n\r\n(This is my first time filing an issue on TensorFlow, so my apologies if this isn't a bug and please let me know if you'd like me to remove / add anything.)\r\n", "comments": ["Hi Lauren,\r\n\r\nThanks for raising the issue.\r\n\r\nI think the deps issues is related to [this](https://github.com/tensorflow/tflite-micro/pull/93/files) where the TF version is bumped to 2.5  I guess we can relax the version on numpy. For example \r\n\r\n```\r\nnumpy\r\n```\r\n\r\ninstead of \r\n```\r\nnumpy==1.16.2\r\n```\r\n\r\nDo you mind help verify and raise a PR if possible? Thanks!\r\n\r\n\r\n\r\nRegarding the model accuracy, if you're experiencing issues with the current model. Feel free to checkout the previous version, which might work better for you.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/59c06b9016700dbf1ab0cefc062d247345cdd0f0#diff-66dac78b23c096b0d1aad4338feef6d2233ae2579ca9f171f0c2ec78ffc80158\r\n\r\n\r\n\r\nMoving forward, we have just released the next generation gesture recognition work, which is awesome.\r\n\r\nYou can find more information from this video: https://www.youtube.com/watch?v=jqVCR2MUJEs\r\n\r\nFor training using TFJS:\r\nhttps://github.com/googlecreativelab/air-snare\r\n\r\nFor deployment on the device (Arduino)\r\nhttps://github.com/googlecreativelab/tf4micro-motion-kit\r\n\r\n\r\nIt would be interesting to see if we can port this work on the board you're working on :-) The new work requirers BLE to collect training data and BLE to update the model on the fly.\r\n\r\n\r\nThanks,\r\nTiezhen", "Now, in the `requirements.txt` file the version of `numpy` has been updated to `1.21.0` for the `tensorflow` version `2.5.3`, you can check the details [here](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/magic_wand/train/requirements.txt). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49473\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49473\">No</a>\n"]}, {"number": 49472, "title": "Make the community-builds link more prominent", "body": "@andrew-leaver What do you think of this?", "comments": []}, {"number": 49471, "title": "Misc TFLM cleanup", "body": " * upstreaming change from https://github.com/tensorflow/tflite-micro/pull/91\r\n\r\n * Fix hifimini build by excluding known failing tests. Tested with:\r\n   ```\r\n   make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG build -j8\r\n   ```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 49470, "title": "Do not use mirror.tensorflow.org for TFLM flatbuffer download.", "body": "Some users are reporting intermittent flakiness with the flatbuffer download from mirror.tensorflow.org\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 49468, "title": "all_ops_resolver and custom ops", "body": "In case I had to implement a generic op unsupported by TF Lite and specify layers of my model, for example like this:\r\n\r\n`tflite::MicroMutableOpResolver<2> micro_op_resolver; `\r\n`micro_op_resolver.AddConv2D(); `\r\n`micro_op_resolver.AddX();`\r\n\r\nwhere AddX() is the unsupported op which I have to create, what for its registration in all_ops_resolver.cc?\r\n\r\n", "comments": ["https://www.tensorflow.org/lite/guide/ops_custom#example_custom_sin_operator shows implementation of custom ops which you may use as reference for your case. In case the guide is not covering your areas of interest perhaps you can mention what is it that you are looking for and what changes you wish to see in the guide so that it may help for use cases like yours.\r\nThank you.", "In my case I'm not sure: \r\n- which folder to put my implementation of a custom operator in;\r\n- if I have to modify all_ops_resolver.cc or not.", "Added Nat as this is for tflite micro.\r\nNat can you please advice here.\r\n\r\nThanks", "Apologies for the delay - \r\nall_ops_resolver.cc simply catches all possible ops in TFLM. If you are creating a custom op and your example uses a MicroMutableOpResolver, you do not need to add your op to all_ops_resolver.\r\n\r\nWe typically do add all TFLM supported ops to the all_ops_resolver so that users can use all_ops_resolver rather than creating a custom MicroMutableOpResolver.\r\n\r\nIf you have follow-up questions please file an issue against the [TFLM repo](https://github.com/tensorflow/tflite-micro/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc)."]}, {"number": 49467, "title": "Removal of PersistentTensor breaks custom ops (Horovod)", "body": "The recent commit https://github.com/tensorflow/tensorflow/commit/f8153ae87a88586ac1c364116208cfb144c9b64a#diff-b35354f62819de66aaa049a9498cccc261c108a7c488d39e04882110bdee65b5 removed the `PersistentTensor` from the TensorFlow C++ API. For Horovod, we rely on this functionality to allocate the \"fusion buffer\" that packs multiple smaller tensors into a single buffer during allreduce [here](https://github.com/horovod/horovod/blob/master/horovod/common/fusion_buffer_manager.cc#L39) which ultimately calls into [here](https://github.com/horovod/horovod/blob/master/horovod/tensorflow/mpi_ops.cc#L198).\r\n\r\nThe docs recommend using `Tensor` with `allocate_temp` instead, but this does not seem like a viable workaround. My understanding is these tensors will not survive past the life of a single Op. Other frameworks including PyTorch and MXNet provide similar mechanisms for allocating long-lived tensor buffers, it seems reasonable TensorFlow should continue to do the same. Or is there a workaround I am missing?\r\n\r\nThanks.\r\n\r\ncc @reedwm @romerojosh @DEKHTIARJonathan\r\n", "comments": ["@reedwm @sanjoy @pkanwar23 could you help addressing the issue or advising how we can work around the issue ?\r\n\r\n@nluehr for viz", "Hello Travis,\r\nTensors allocated with allocate_temp will survive past the life of a single Op. This call really needs to be renamed just \"allocate\"  or allocate_tensor for consistency.\r\n\r\nPersistent tensor implementation that was just removed, and you are using, was a wrapper around Tensor class and allocate_temp.\r\n\r\nIt is completely safe to do the flowing update to your code:\r\n\r\nFROM:\r\n```\r\nPersistentTensor ptensor_;\r\nTensor* second_tensor = nullptr;\r\n    OP_REQUIRES_OK(context, context->allocate_persistent(\r\n                                value, shape, &ptensor_, &second_tensor));\r\nconst Tensor& tensor = *ptensor_.AccessTensor(context);\r\n```\r\n\r\nTO\r\n```\r\nTensor tensor;\r\nOP_REQUIRES_OK(\r\n        context, context->allocate_temp(value, shape, &tensor));\r\n```", "Thanks @gromovkg, that seems to have worked! I think what through me off about `allocate_temp` was [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.h#L251) section of the docs that reads: \"Tensor must not be used after kernel construction is complete.\" Can you clarify at which point the tensor can no longer be safely used?", "@tgaddair You are welcome!\r\n\r\nTensor doesn't directly store the tensor contents. Tensor has a pointer to a TensorBuffer, which holds the tensot's contents and is refcounted. When a Tensor is copied, it doesn't copy the tensor contents but just copies the TensorBuffer pointer and increases the refcount. When the TensorBuffer's refcount reaches zero, it is destructed. In practice, usually only tensors reference TensorBuffers so when the last Tensor referencing a TensorBuffer is destructed, the TensorBuffer is also destructed.", "I see, thanks for clarifying! I'll go ahead and close this issue then since it sounds like this is what we're looking for.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49467\">No</a>\n"]}, {"number": 49466, "title": "Bump tensorflow from 2.4.0 to 2.5.0 in /tensorflow/lite/micro/examples/magic_wand/train", "body": "Bumps [tensorflow](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.0.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow's releases</a>.</em></p>\n<blockquote>\n<h2>TensorFlow 2.5.0</h2>\n<h1>Release 2.5.0</h1>\n<h2>Major Features and Improvements</h2>\n<ul>\n<li>Support for Python3.9 has been added.</li>\n<li><code>tf.data</code>:\n<ul>\n<li><code>tf.data</code> service now supports strict round-robin reads, which is useful for synchronous training workloads where example sizes vary. With strict round robin reads, users can guarantee that consumers get similar-sized examples in the same step.</li>\n<li>tf.data service now supports optional compression. Previously data would always be compressed, but now you can disable compression by passing <code>compression=None</code> to <code>tf.data.experimental.service.distribute(...)</code>.</li>\n<li><code>tf.data.Dataset.batch()</code> now supports <code>num_parallel_calls</code> and <code>deterministic</code> arguments. <code>num_parallel_calls</code> is used to indicate that multiple input batches should be computed in parallel. With <code>num_parallel_calls</code> set, <code>deterministic</code> is used to indicate that outputs can be obtained in the non-deterministic order.</li>\n<li>Options returned by <code>tf.data.Dataset.options()</code> are no longer mutable.</li>\n<li>tf.data input pipelines can now be executed in debug mode, which disables any asynchrony, parallelism, or non-determinism and forces Python execution (as opposed to trace-compiled graph execution) of user-defined functions passed into transformations such as <code>map</code>. The debug mode can be enabled through <code>tf.data.experimental.enable_debug_mode()</code>.</li>\n</ul>\n</li>\n<li><code>tf.lite</code>\n<ul>\n<li>Enabled the new MLIR-based quantization backend by default\n<ul>\n<li>The new backend is used for 8 bits full integer post-training quantization</li>\n<li>The new backend removes the redundant rescales and fixes some bugs (shared weight/bias, extremely small scales, etc)</li>\n<li>Set <code>experimental_new_quantizer</code> in tf.lite.TFLiteConverter to False to disable this change</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><code>tf.keras</code>\n<ul>\n<li><code>tf.keras.metrics.AUC</code> now support logit predictions.</li>\n<li>Enabled a new supported input type in <code>Model.fit</code>, <code>tf.keras.utils.experimental.DatasetCreator</code>, which takes a callable, <code>dataset_fn</code>. <code>DatasetCreator</code> is intended to work across all <code>tf.distribute</code> strategies, and is the only input type supported for Parameter Server strategy.</li>\n</ul>\n</li>\n<li><code>tf.distribute</code>\n<ul>\n<li><code>tf.distribute.experimental.ParameterServerStrategy</code> now supports training with Keras <code>Model.fit</code> when used with <code>DatasetCreator</code>.</li>\n<li>Creating <code>tf.random.Generator</code> under <code>tf.distribute.Strategy</code> scopes is now allowed (except for <code>tf.distribute.experimental.CentralStorageStrategy</code> and <code>tf.distribute.experimental.ParameterServerStrategy</code>). Different replicas will get different random-number streams.</li>\n</ul>\n</li>\n<li>TPU embedding support\n<ul>\n<li>Added <code>profile_data_directory</code> to <code>EmbeddingConfigSpec</code> in <code>_tpu_estimator_embedding.py</code>. This allows embedding lookup statistics gathered at runtime to be used in embedding layer partitioning decisions.</li>\n</ul>\n</li>\n<li>PluggableDevice\n<ul>\n<li>Third-party devices can now connect to TensorFlow as plug-ins through <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md\">StreamExecutor C API</a>.\nand <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md\">PluggableDevice</a> interface.\n<ul>\n<li>Add custom ops and kernels through <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20190814-kernel-and-op-registration.md\">kernel and op registration C API</a>.</li>\n<li>Register custom graph optimization passes with <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20201027-modular-tensorflow-graph-c-api.md\">graph optimization C API</a>.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"https://github.com/oneapi-src/oneDNN\">oneAPI Deep Neural Network Library (oneDNN)</a> CPU performance optimizations from <a href=\"https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html\">Intel-optimized TensorFlow</a> are now available in the official x86-64 Linux and Windows builds.\n<ul>\n<li>They are off by default. Enable them by setting the environment variable <code>TF_ENABLE_ONEDNN_OPTS=1</code>.</li>\n<li>We do not recommend using them in GPU systems, as they have not been sufficiently tested with GPUs yet.</li>\n</ul>\n</li>\n<li>TensorFlow pip packages are now built with CUDA11.2 and cuDNN 8.1.0</li>\n</ul>\n<h2>Breaking Changes</h2>\n<ul>\n<li>The <code>TF_CPP_MIN_VLOG_LEVEL</code> environment variable has been renamed to to <code>TF_CPP_MAX_VLOG_LEVEL</code> which correctly describes its effect.</li>\n</ul>\n<h2>Bug Fixes and Other Changes</h2>\n<ul>\n<li><code>tf.keras</code>:\n<ul>\n<li>Preprocessing layers API consistency changes:\n<ul>\n<li><code>StringLookup</code> added <code>output_mode</code>, <code>sparse</code>, and <code>pad_to_max_tokens</code> arguments with same semantics as <code>TextVectorization</code>.</li>\n<li><code>IntegerLookup</code> added <code>output_mode</code>, <code>sparse</code>, and <code>pad_to_max_tokens</code> arguments with same semantics as <code>TextVectorization</code>. Renamed <code>max_values</code>, <code>oov_value</code> and <code>mask_value</code> to <code>max_tokens</code>, <code>oov_token</code> and <code>mask_token</code> to align with <code>StringLookup</code> and <code>TextVectorization</code>.</li>\n<li><code>TextVectorization</code> default for <code>pad_to_max_tokens</code> switched to False.</li>\n<li><code>CategoryEncoding</code> no longer supports <code>adapt</code>, <code>IntegerLookup</code> now supports equivalent functionality. <code>max_tokens</code> argument renamed to <code>num_tokens</code>.</li>\n<li><code>Discretization</code> added <code>num_bins</code> argument for learning bins boundaries through calling <code>adapt</code> on a dataset. Renamed <code>bins</code> argument to <code>bin_boundaries</code> for specifying bins without <code>adapt</code>.</li>\n</ul>\n</li>\n<li>Improvements to model saving/loading:\n<ul>\n<li><code>model.load_weights</code> now accepts paths to saved models.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow's changelog</a>.</em></p>\n<blockquote>\n<h1>Release 2.5.0</h1>\n<!-- raw HTML omitted -->\n<h2>Breaking Changes</h2>\n<ul>\n<li>\n<!-- raw HTML omitted -->\n</li>\n<li>The <code>TF_CPP_MIN_VLOG_LEVEL</code> environment variable has been renamed to to\n<code>TF_CPP_MAX_VLOG_LEVEL</code> which correctly describes its effect.</li>\n</ul>\n<h2>Known Caveats</h2>\n<ul>\n<li><!-- raw HTML omitted --></li>\n<li><!-- raw HTML omitted --></li>\n<li><!-- raw HTML omitted --></li>\n</ul>\n<h2>Major Features and Improvements</h2>\n<ul>\n<li>\n<p><!-- raw HTML omitted --></p>\n</li>\n<li>\n<p><!-- raw HTML omitted --></p>\n</li>\n<li>\n<p>TPU embedding support</p>\n<ul>\n<li>Added <code>profile_data_directory</code> to <code>EmbeddingConfigSpec</code> in\n<code>_tpu_estimator_embedding.py</code>. This allows embedding lookup statistics\ngathered at runtime to be used in embedding layer partitioning decisions.</li>\n</ul>\n</li>\n<li>\n<p><code>tf.keras.metrics.AUC</code> now support logit predictions.</p>\n</li>\n<li>\n<p>Creating <code>tf.random.Generator</code> under <code>tf.distribute.Strategy</code> scopes is now allowed (except for <code>tf.distribute.experimental.CentralStorageStrategy</code> and <code>tf.distribute.experimental.ParameterServerStrategy</code>). Different replicas will get different random-number streams.</p>\n</li>\n<li>\n<p><code>tf.data</code>:</p>\n<ul>\n<li>tf.data service now supports strict round-robin reads, which is useful\nfor synchronous training workloads where example sizes vary. With strict\nround robin reads, users can guarantee that consumers get similar-sized\nexamples in the same step.</li>\n<li>tf.data service now supports optional compression. Previously data would\nalways be compressed, but now you can disable compression by passing\n<code>compression=None</code> to <code>tf.data.experimental.service.distribute(...)</code>.</li>\n<li><code>tf.data.Dataset.batch()</code> now supports <code>num_parallel_calls</code> and\n<code>deterministic</code> arguments. <code>num_parallel_calls</code> is used to indicate that\nmultiple input batches should be computed in parallel. With\n<code>num_parallel_calls</code> set, <code>deterministic</code> is used to indicate that\noutputs can be obtained in the non-deterministic order.</li>\n<li>Options returned by <code>tf.data.Dataset.options()</code> are no longer mutable.</li>\n<li>tf.data input pipelines can now be executed in debug mode, which\ndisables any asynchrony, parallelism, or non-determinism and forces\nPython execution (as opposed to trace-compiled graph execution) of\nuser-defined functions passed into transformations such as <code>map</code>. The\ndebug mode can be enabled through <code>tf.data.experimental.enable_debug_mode()</code>.</li>\n</ul>\n</li>\n<li>\n<p><code>tf.lite</code></p>\n<ul>\n<li>Enabled the new MLIR-based quantization backend by default\n<ul>\n<li>The new backend is used for 8 bits full integer post-training quantization</li>\n<li>The new backend removes the redundant rescales and fixes some bugs (shared weight/bias, extremely small scales, etc)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d\"><code>a4dfb8d</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49124\">#49124</a> from tensorflow/mm-cherrypick-tf-data-segfault-fix-...</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/2107b1dc414edb3fc78e632bca4f4936171093b2\"><code>2107b1d</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49116\">#49116</a> from tensorflow-jenkins/version-numbers-2.5.0-17609</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/16b813906fcb46306aef29a04ddd0cbdb4e77918\"><code>16b8139</code></a> Update snapshot_dataset_op.cc</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/86a0d86cb5da6a28b78ea7f886ec2831d23f6d6b\"><code>86a0d86</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49126\">#49126</a> from geetachavan1/cherrypicks_X9ZNY</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/9436ae693ef66a9efb7e7e7888134173d9a0821d\"><code>9436ae6</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49128\">#49128</a> from geetachavan1/cherrypicks_D73J5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/6b2bf99cd9336026689579b683a709c5efcb4ae9\"><code>6b2bf99</code></a> Validate that a and b are proper sparse tensors</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/c03ad1a46d5b3f23df67dad03185a0ee16020c96\"><code>c03ad1a</code></a> Ensure validation sticks in banded_triangular_solve_op</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/12a6ead7ac968c402feb85ce0a8069ccbc6bf735\"><code>12a6ead</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49120\">#49120</a> from geetachavan1/cherrypicks_KJ5M9</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/b67f5b8a0a098c34c71c679aa46480035c46886e\"><code>b67f5b8</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49118\">#49118</a> from geetachavan1/cherrypicks_BIDTR</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/a13c0ade86295bd3a8356b4b8cc980cf0c5e70e0\"><code>a13c0ad</code></a> [tf.data][cherrypick] Fix snapshot segfault when using repeat and prefecth</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow&package-manager=pip&previous-version=2.4.0&new-version=2.5.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/tensorflow/tensorflow/network/alerts).\n\n</details>", "comments": ["Looks like tensorflow is up-to-date now, so this is no longer needed."]}, {"number": 49465, "title": "Can you please clarify the logic behind \"one-hot encoding the labels to use MSE\"", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/keras/train_and_evaluate/#custom_losses\r\n\r\nThank you for the very detailed explanation about Train and Evaluate but I have a query. Raised a [Stack Overflow question](https://stackoverflow.com/questions/67638773/can-you-please-clarify-the-logic-behind-one-hot-encoding-the-labels-to-use-mse) as well.\r\n\r\n## Description of issue (what needs changing):\r\nNothing needs a change but I want to understand the statement,\r\n\r\n> We need to one-hot encode the labels to use MSE\r\n\r\nand want to understand exactly, how **`One-Hot-Encoding`** and **`MSE`** work together (because **`One-Hot-Encoding`** just creates columns with 0's and a 1), in the code mentioned below in the [Documentation of Keras Train and Evaluate ](https://www.tensorflow.org/guide/keras/train_and_evaluate/#custom_losses)\r\n\r\n```python\r\ndef custom_mean_squared_error(y_true, y_pred):\r\n    return tf.math.reduce_mean(tf.square(y_true - y_pred))\r\n\r\n\r\nmodel = get_uncompiled_model()\r\nmodel.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)\r\n\r\n# We need to one-hot encode the labels to use MSE\r\ny_train_one_hot = tf.one_hot(y_train, depth=10)\r\nmodel.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)\r\n```", "comments": ["@worldpeaceaspirer ,\r\n\r\nThis is not a TensorFlow bug or feature request, it is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a larger community that reads questions there. Thanks!\r\n\r\nPlease refer to this links for more information about one-hot encoding.[link1](https://towardsdatascience.com/building-a-one-hot-encoding-layer-with-tensorflow-f907d686bf39),[link2](https://www.tensorflow.org/tutorials/text/word_embeddings#one-hot_encodings)\r\n\r\nThanks!", "@worldpeaceaspirer \r\nThe Adam Optimizer can, in theory,minimize MSE loss for a one-hot-encoded vector,there is no mathematical barrier,but that does not mean it is the best choice in this case.\r\nThe link demonstrates custom loss functions and not teach you ML.\r\nyou do not need to apply OHE if your application is classification you can use sparse categorical entropy loss on labelencoded data.\r\n\r\nAs mentioned above, please create a stackoverflow issue for any further queries, and move this to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49465\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49465\">No</a>\n"]}, {"number": 49464, "title": "Training a Model", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NC\r\n- GPU model and memory: KVM\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to train a new model. I am the official Tensorflow tutorial. nevertheless I block towards the end on the training of the model (script : model_main_tf2.py )\r\n`python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config\r\n2021-05-21 15:27:42.920609: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-05-21 15:27:42.921036: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-05-21 15:27:47.225166: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-05-21 15:27:47.225469: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-05-21 15:27:47.225561: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (reco-objets): /proc/driver/nvidia/version does not exist\r\n2021-05-21 15:27:47.226220: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nW0521 15:27:47.227876 139889528895296 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nW0521 15:27:47.228754 139889528895296 mirrored_strategy.py:379] Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\nI0521 15:27:47.234278 139889528895296 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\nINFO:tensorflow:Maybe overwriting train_steps: None\r\nI0521 15:27:47.242940 139889528895296 config_util.py:552] Maybe overwriting train_steps: None\r\nINFO:tensorflow:Maybe overwriting use_bfloat16: False\r\nI0521 15:27:47.243463 139889528895296 config_util.py:552] Maybe overwriting use_bfloat16: False\r\nWARNING:tensorflow:From /home/sic/.local/lib/python3.6/site-packages/object_detection/model_lib_v2.py:551: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nW0521 15:27:47.352285 139889528895296 deprecation.py:336] From /home/sic/.local/lib/python3.6/site-packages/object_detection/model_lib_v2.py:551: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nTraceback (most recent call last):\r\n  File \"model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 551, in train_loop\r\n    train_dataset_fn)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 337, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1168, in experimental_distribute_datasets_from_function\r\n    return self.distribute_datasets_from_function(dataset_fn, options)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1160, in distribute_datasets_from_function\r\n    dataset_fn, options)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 597, in _distribute_datasets_from_function\r\n    options)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 168, in get_distributed_datasets_from_function\r\n    input_contexts, dataset_fn, options)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 1566, in __init__\r\n    input_contexts, self._input_workers, dataset_fn))\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 2301, in _create_datasets_from_function_with_input_context\r\n    dataset = dataset_fn(ctx)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 546, in train_dataset_fn\r\n    input_context=input_context)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/object_detection/inputs.py\", line 898, in train_input\r\n    reduce_to_frame_fn=reduce_to_frame_fn)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py\", line 210, in build\r\n    decoder = decoder_builder.build(input_reader_config)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/object_detection/builders/decoder_builder.py\", line 64, in build\r\n    load_keypoint_depth_features=input_reader_config\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/object_detection/data_decoders/tf_example_decoder.py\", line 416, in __init__\r\n    default_value=''),\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/object_detection/data_decoders/tf_example_decoder.py\", line 89, in __init__\r\n    label_map_proto_file, use_display_name=False)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/object_detection/utils/label_map_util.py\", line 201, in get_label_map_dict\r\n    label_map = load_labelmap(label_map_path_or_proto)\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/object_detection/utils/label_map_util.py\", line 168, in load_labelmap\r\n    label_map_string = fid.read()\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 117, in read\r\n    self._preread_check()\r\n  File \"/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 80, in _preread_check\r\n    compat.path_to_str(self.__name), 1024 * 512)\r\ntensorflow.python.framework.errors_impl.NotFoundError: PATH_TO_BE_CONFIGURED; No such file or directory\r\n`\r\ndo you have an idea?\r\n\r\n\r\nI confirm that I have the folder where the file is present:\r\nmodel_dir=models/my_ssd_resnet50_v1_fpn \r\npipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config\r\n \r\n\r\nthanks\r\n", "comments": ["@Florent4223 \r\n\r\nLooking at the error log, seems like this is similar to issues [#3762](https://github.com/tensorflow/models/issues/3762), [#3954](https://github.com/tensorflow/models/issues/3954).Please let us know if you still face the problem.Thanks\r\n", "Hello\nThank you for the feedback. I confirm that the anomaly reported in the\ntopics indicated are different. The 1st touches another script (which\nI did successfully), the 2nd touches the same script, but I use the\ncorrect command (in any case the one indicated on the Tensorflow site)\n\nThe problem therefore remains\n\nThanks\n\n\nLe ven. 21 mai 2021 \u00e0 16:26, UsharaniPagadala ***@***.***> a\n\u00e9crit :\n\n> @Florent4223 <https://github.com/Florent4223>\n>\n> Looking at the error log, seems like this is similar to issues #3762\n> <https://github.com/tensorflow/models/issues/3762>, #3954\n> <https://github.com/tensorflow/models/issues/3954>.Please let us know if\n> you still face the problem.Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/49464#issuecomment-845987780>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AMA2JWIQGW2CU5FGSIN4EWLTOZUSJANCNFSM45JHFFNQ>\n> .\n>\n", "@Florent4223 \r\n\r\nCould you please share the standalone code/colab gist.Thanks", "sorry  I have find the error, my erro was in pipeline.config\n\nbut after starting the training, I never indicated the 100%\n\nat the end I stop the process\n\n[code]\nWARNING:tensorflow:Unresolved object in checkpoint:\n(root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.beta\nW0528 11:36:52.936134 140000602519360 util.py:162] Unresolved object\nin checkpoint: (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.beta\nWARNING:tensorflow:Unresolved object in checkpoint:\n(root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.moving_mean\nW0528 11:36:52.936249 140000602519360 util.py:162] Unresolved object\nin checkpoint: (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.moving_mean\nWARNING:tensorflow:Unresolved object in checkpoint:\n(root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.moving_variance\nW0528 11:36:52.936391 140000602519360 util.py:162] Unresolved object\nin checkpoint: (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.moving_variance\nWARNING:tensorflow:A checkpoint was restored (e.g.\ntf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not\nall checkpointed values were used. See above for specific issues. Use\nexpect_partial() on the load status object, e.g.\ntf.train.Checkpoint.restore(...).expect_partial(), to silence these\nwarnings, or use assert_consumed() to make the check explicit. See\nhttps://www.tensorflow.org/guide/checkpoint#loading_mechanics for\ndetails.\nW0528 11:36:52.936520 140000602519360 util.py:170] A checkpoint was\nrestored (e.g. tf.train.Checkpoint.restore or\ntf.keras.Model.load_weights) but not all checkpointed values were\nused. See above for specific issues. Use expect_partial() on the load\nstatus object, e.g. tf.train.Checkpoint.restore(...).expect_partial(),\nto silence these warnings, or use assert_consumed() to make the check\nexplicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics\nfor details.\nWARNING:tensorflow:From\n/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:602:\ncalling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is\ndeprecated and will be removed in a future version.\nInstructions for updating:\nUse fn_output_signature instead\nW0528 11:37:12.378952 139997839599360 deprecation.py:534] From\n/home/sic/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:602:\ncalling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is\ndeprecated and will be removed in a future version.\nInstructions for updating:\nUse fn_output_signature instead\nProcessus arr\u00eat\u00e9\n\n[/code]\n\n\n\nLe ven. 28 mai 2021 \u00e0 10:19, Florent Decitre ***@***.***> a\n\u00e9crit :\n\n> Hello\n> Thank you for the feedback. I confirm that the anomaly reported in the topics indicated are different. The 1st touches another script (which I did successfully), the 2nd touches the same script, but I use the correct command (in any case the one indicated on the Tensorflow site)\n>\n> The problem therefore remains\n>\n> Thanks\n>\n>\n> Le ven. 21 mai 2021 \u00e0 16:26, UsharaniPagadala ***@***.***>\n> a \u00e9crit :\n>\n>> @Florent4223 <https://github.com/Florent4223>\n>>\n>> Looking at the error log, seems like this is similar to issues #3762\n>> <https://github.com/tensorflow/models/issues/3762>, #3954\n>> <https://github.com/tensorflow/models/issues/3954>.Please let us know if\n>> you still face the problem.Thanks\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/49464#issuecomment-845987780>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AMA2JWIQGW2CU5FGSIN4EWLTOZUSJANCNFSM45JHFFNQ>\n>> .\n>>\n>\n", "@Florent4223,\r\nCan you please help us to reproduce your issue so that we can try to resolve it? Thanks! ", "Hello\r\nThank you for the feedback.\r\nI follow the procedure of the site https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\r\n\r\nI specify that beforehand I installed all the required packages + tensorflow + TF API\r\n\r\n1) I created the same folder architecture as the doc\r\n2 ) I found some images of my object, and I annotated them with labelimg and divided between test and training\r\n3) I created the label map\r\n4) Convert *.xml to *.record wicth python generate_tfrecord.py\r\n5) I recovered ssd_resnet50_v1_fpn_640x640_coco17_tpu-8  and configure (it was my previous error I had configured it incorrectly)\r\n6) I now want to train the model on my new object with the model_main_tf2.py script\r\n\r\nThis \"training\" lasts a few seconds and stops by itself\r\n\r\nI hope I answered the question and I remain at your disposal for any questions thank you", "initiale command:\r\n\r\n```\r\ncardio@cardiohp:~/Tensorflow/workspace/training_demo$ python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config\r\n```\r\nlogs:\r\n```\r\n2021-06-09 11:38:53.793053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-06-09 11:38:53.793085: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-06-09 11:38:56.018428: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: Ne peut ouvrir le fichier d'objet partag\u00e9: Aucun fichier ou dossier de ce type\r\n2021-06-09 11:38:56.018469: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-06-09 11:38:56.018508: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cardiohp): /proc/driver/nvidia/version does not exist\r\n2021-06-09 11:38:56.018786: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nW0609 11:38:56.019701 140657181075264 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nW0609 11:38:56.019944 140657181075264 mirrored_strategy.py:379] Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\nI0609 11:38:56.022413 140657181075264 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\nINFO:tensorflow:Maybe overwriting train_steps: None\r\nI0609 11:38:56.026477 140657181075264 config_util.py:552] Maybe overwriting train_steps: None\r\nINFO:tensorflow:Maybe overwriting use_bfloat16: False\r\nI0609 11:38:56.026618 140657181075264 config_util.py:552] Maybe overwriting use_bfloat16: False\r\nWARNING:tensorflow:From /home/cardio/.local/lib/python3.8/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nW0609 11:38:56.052769 140657181075264 deprecation.py:330] From /home/cardio/.local/lib/python3.8/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nINFO:tensorflow:Reading unweighted datasets: ['annotations/train.record']\r\nI0609 11:38:56.059633 140657181075264 dataset_builder.py:163] Reading unweighted datasets: ['annotations/train.record']\r\nINFO:tensorflow:Reading record datasets for input file: ['annotations/train.record']\r\nI0609 11:38:56.059836 140657181075264 dataset_builder.py:80] Reading record datasets for input file: ['annotations/train.record']\r\nINFO:tensorflow:Number of filenames to read: 1\r\nI0609 11:38:56.059911 140657181075264 dataset_builder.py:81] Number of filenames to read: 1\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nW0609 11:38:56.059973 140657181075264 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From /home/cardio/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nW0609 11:38:56.067752 140657181075264 deprecation.py:330] From /home/cardio/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nWARNING:tensorflow:From /home/cardio/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nW0609 11:38:56.105435 140657181075264 deprecation.py:330] From /home/cardio/.local/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nWARNING:tensorflow:From /home/cardio/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nW0609 11:39:03.725294 140657181075264 deprecation.py:330] From /home/cardio/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nWARNING:tensorflow:From /home/cardio/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nW0609 11:39:07.325738 140657181075264 deprecation.py:330] From /home/cardio/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nWARNING:tensorflow:From /home/cardio/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nW0609 11:39:09.254939 140657181075264 deprecation.py:330] From /home/cardio/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\n2021-06-09 11:39:11.756661: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-06-09 11:39:11.784659: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2595000000 Hz\r\n2021-06-09 11:39:12.178699: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 39321600 exceeds 10% of free system memory.\r\n2021-06-09 11:39:12.206940: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 39321600 exceeds 10% of free system memory.\r\n2021-06-09 11:39:12.213958: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 39321600 exceeds 10% of free system memory.\r\n2021-06-09 11:39:12.227869: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 39321600 exceeds 10% of free system memory.\r\n2021-06-09 11:39:12.238191: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 39321600 exceeds 10% of free system memory.\r\n/home/cardio/.local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:435: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\r\n  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\r\nWARNING:tensorflow:From /home/cardio/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:602: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse fn_output_signature instead\r\nW0609 11:39:57.860589 140653728098048 deprecation.py:528] From /home/cardio/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:602: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse fn_output_signature instead\r\nProcessus arr\u00eat\u00e9\r\n```\r\n\r\nthanks for your answer", "Can you try with the latest `tf-nightly`?", "@Florent4223 ,\r\nAs commented please try tf-nightly or tf v2.6 and let us know if the issue still persists.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49464\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49464\">No</a>\n"]}, {"number": 49463, "title": "Random number generation with tf.function in tf 2.5.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nExample provided on your website here: https://www.tensorflow.org/guide/random_numbers#passing_generators_as_arguments_to_tffunction leads to num_traces=2. This results in excessive retracing and thus slowdown in custom code. Noticed it after upgrading to 2.5.0 from 2.3.1.\r\n\r\n**Describe the expected behavior**\r\nAs documentation describes num_traces=1 is expected, which it does produce in tf 2.3.1. \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf \r\n\r\nnum_traces = 0\r\n@tf.function\r\ndef foo(g):\r\n  global num_traces\r\n  num_traces += 1\r\n  return g.normal([])\r\nfoo(tf.random.Generator.from_seed(1))\r\nfoo(tf.random.Generator.from_seed(2))\r\nprint(num_traces)\r\n```\r\n", "comments": ["I was able to reproduce the issue in TF v2.5.0,2.3.1 and nightly.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/0a281490403fa60c744e9faccd5f4b81/49463.ipynb) here.", "Yeah, different `tf.random.Generator` objects used to not cause retracing, but the mechanism to achieve that (making `tf.random.Generator` a subclass of `CompositeTensor`) is broken in many cases. `tf.random.Generator` is basically a `tf.Variable`, and since we aren't able to make `tf.Variable` a `CompositeTensor`, neither can we for `tf.random.Generator`. Notice that if you do `foo(tf.Variable(1)); foo(tf.Variable(2))`, you'll also get 2 traces, so the current `tf.random.Generator` behavior is consistent with `tf.Variable`. I'll update the RNG guide. ", "Guide updated.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49463\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49463\">No</a>\n"]}, {"number": 49462, "title": "XLA with tf.function failed on tf2.5", "body": "I can run the tutorial code (https://www.tensorflow.org/xla/tutorials/jit_compile) successfully. But using XAL with `tf.function` on my code failed with the following errors:\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run __inference_train_multiple_steps_269495: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::ResourceDeleter [Op:__inference_train_multiple_steps_269495]\r\n```\r\nIs there anything I missed when using it?\r\n", "comments": ["@cswhjiang \r\nCould you please share the code where you face error along with XLA and tf.function.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49461, "title": "Need clarification regarding SGD Optimizer", "body": "I have a question regarding `SGD Optimizer`. Same issue has been raised in [Stack Overflow](https://stackoverflow.com/questions/67636925/need-clarification-regarding-sgd-optimizer) as well.\r\n\r\nThere are 3 types of `Gradient Descent Algorithm`:\r\n\r\n1. Batch Gradient Descent\r\n2. Mini-Batch Gradient Descent and\r\n3. Stochastic Gradient Descent\r\n\r\n**`Stochastic Gradient Descent`** is an `Algorithm` in which one `Instance` from `Training Set` is taken at `Random` and the **`Weights`** are updated with respect to that `Instance`.\r\n\r\n**`SGD Optimizer`** is slightly deviating from the above definition where it can accept the **`batch_size`** of more than 1. Can someone clarify this deviation?\r\n\r\nBelow code seems to be in line with the definition of **`Stochastic Gradient Descent`**:\r\n\r\n    model.compile(optimizer = 'sgd', loss = 'mse')\r\n    model.fit(x, y,epochs = 500, batch_size = 1,verbose=1)\r\n\r\nHowever, below code seems to be confusing/deviating (since `batch_size` > 1):\r\n\r\n    model.compile(optimizer = 'sgd', loss = 'mse')\r\n    model.fit(x, y,epochs = 500, batch_size = 32, verbose=1)\r\n\r\nThank you in advance for the clarification.", "comments": ["@worldpeaceaspirer \r\nCould you please go through this [article](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/), it helps\r\n\r\nGitHub is mainly for addressing bugs in build/installation and performance.Keep track the issue posted in Stack overflow as you mentioned.There is a big community to support and learn from your questions and  Please move this to closed status. Thanks\r\n", "@UsharaniPagadala,\r\nIt's been answered in Stack Overflow. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49461\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49461\">No</a>\n"]}, {"number": 49460, "title": "mixed precision training failed on tf2.5", "body": " The following momentum updating steps failed when mixed precision is enabled.\r\n```\r\n        cur_step = tf.cast(cur_step, tf.float32)  \r\n        total_step = tf.cast(total_step, tf.float32)  # tensor\r\n        PI = tf.constant(math.pi, tf.float32)\r\n        tau = 1.0 - (1.0 - self.tau_base) * (tf.math.cos(PI * cur_step / total_step) + 1.0) / 2.0\r\n\r\n        for param_online, param_target in zip(self.online_net.trainable_variables, self.target_net.trainable_variables):\r\n            param_target.assign(param_target * tau + param_online * (1.0 - tau))\r\n```\r\nAnd here is the error message\r\n```\r\nTypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.\r\n```\r\nIs there anything wrong with the code?", "comments": ["@cswhjiang ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset or colab gist to reproduce the issue reported here.\r\n\r\nThanks!\r\n", "@cswhjiang ,\r\nPlease take a look at these issues with similar error log.#33077,#33484,[link](https://stackoverflow.com/questions/59541629/typeerror-input-y-of-mul-op-has-type-float32-that-does-not-match-type-int32).It helps.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49460\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49460\">No</a>\n"]}, {"number": 49459, "title": "text.tokenizer not removing filters char from text ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubutnu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):'2.5.0'\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nThe filter parameter of `tf.keras.preprocessing.text.Tokenizer` does not seem to filter out characters included in the \"filters\" paramter. those characters have an index in keys of `.get_config()`. Is this expected? \r\n\r\nThe description of the \"filters parameteres reads \" a string where each element is a character that will be filtered from the texts.\". \r\nHence I would not expect these characters to have been given an index, (perhaps they all could be given one OOV index)\r\n \r\n**Describe the expected behavior**\r\nThe characters included in the filters paramter would not be having different indices in the tokenizer. Either all have one oov index or none. \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf \r\n\r\nchar_tokenizer = tf.keras.preprocessing.text.Tokenizer(\r\n    num_words=None,\r\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\r\n    lower=True, char_level=True,\r\n    document_count=0, \r\n)\r\n\r\nchar_tokenizer.fit_on_texts(['a%$#^ *'])\r\nprint(char_tokenizer.get_config()['index_word'])\r\n```\r\n\r\n`>> {\"a\": 1, \"%\": 2, \"$\": 3, \"#\": 4, \"^\": 5, \" \": 6, \"*\": 7}`\r\n\r\nDespite these special characters being included in the filters parameter, they are not filtered out and have been given an index. \r\n\r\nExpected behaviour\r\n`>> {\"a\": 1}`\r\n\r\nIs this expected? \r\n", "comments": ["You have to set arg `char_level=False` to achieve the later (expected) result.\r\n```python\r\nimport tensorflow as tf \r\n\r\nchar_tokenizer = tf.keras.preprocessing.text.Tokenizer(\r\n    num_words=None,\r\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\r\n    lower=True, char_level=False,\r\n    document_count=0, \r\n)\r\n\r\nchar_tokenizer.fit_on_texts(['a%$#^ *'])\r\nprint(char_tokenizer.get_config()['index_word'])\r\n```\r\nOutput:\r\n```python\r\n{\"1\": \"a\"}\r\n```", "@ymodak `char_level` is set to `True` because the use case this to get character embeddings, as opposed to word embeddings", "Would you like to move this issue to keras. \r\n\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Moved this issue to keras-team/keras repo, you can track the status of the issue from the above link, closing the issue here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49459\">No</a>\n"]}, {"number": 49458, "title": "Process finished with exit code 132 (interrupted by signal 4: SIGILL) on apple m1", "body": "print(\"hello\")\r\nimport tensorflow\r\nprint(\"hello\")\r\nIt only prints hello before importing TensorFlow running inside the PyCharm IDE and then python stop working. How can I solve this problem?\r\n\r\nVersions:\r\n\r\nPython version 3.6\r\nTensorFlow version 2.5.0\r\nIDE PyCharm\r\nOS Mac M1 chip\r\n![code image](https://user-images.githubusercontent.com/72116944/119127546-1659ca80-ba52-11eb-8881-98384fe206e2.jpeg)\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49458\">No</a>\n"]}, {"number": 49457, "title": "404 from https://www.tensorflow.org/get_started/embedding_viz", "body": "This is the link from http://projector.tensorflow.org/ to \"Open documentation\" (the circle with a question mark inside)", "comments": ["@ToonTalk ,\r\n\r\nWe see that the issue template has not been filled, could you please fill the [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nThanks!", "we have a tracker for this https://github.com/tensorflow/tensorboard/issues/4896\r\nThanks!"]}, {"number": 49455, "title": "Colab doesnt use GPU despite connected to GPU runtime", "body": "My code worked well with GPU in Colab a few days ago. But today it was very slow. After checking the availability of GPU fI found out GPU isn't getting used despite connected to one. ", "comments": ["@drkhan107 \r\nColab offers optional accelerated compute environments, including GPU and TPU. Executing code in a GPU or TPU runtime does not automatically mean that the GPU or TPU is being utilized. To avoid hitting your GPU usage limits, we recommend switching to a standard runtime if you are not utilizing the GPU. Choose Runtime > Change Runtime Type and set Hardware Accelerator to None.For more info please go through this [Tensorflow with GPU](https://colab.research.google.com/notebooks/gpu.ipynb).Thanks\r\n\r\n\r\n", "This is probably due to https://github.com/googlecolab/colabtools/issues/2013\r\n\r\nAs a workaround you should be able to use the GPU on colab with TF 2.4 for the time being.", "@drkhan107 \r\n\r\nThank you for your update, If you could resolve it. kindly move this issue to closed status, and keep track that  issue mentioned or else subscribe. \r\nThanks\r\n", "Thank you. I will try TF 2.4", "@drkhan107 \r\n\r\nCould you please move this to closed status as it is resolved.Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49455\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49455\">No</a>\n"]}, {"number": 49454, "title": "[MLIR][DISC] Upgrade to use the new `reifyReturnTypeShapes` interface.", "body": "The new interface is more safe to be used during dialect conversion\r\n(e.g. converting from tensor world to buffer world).", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49454) for more info**.\n\n<!-- need_author_cla -->", "@joker-eph @gbaned `import/copybara \u2014 An error happened while migrating the change`. Is there any detail about this? What can I do for this?"]}, {"number": 49453, "title": "Completely different training results on TF 2.4.1 and 2.0.0", "body": "Hi! How are you? Hope y'all good.\r\n\r\nI have a very specific scenario and it would be awesome to have your input. I'm working on a project in which I've developed a training script using TF 2.4.1. Just to give you context, I used MobileNetV2 as the base model to feature extraction and just a Dense layer with one neuron, and, with time, it got necessary to have different base models so as to perform a benchmark, so I started using different base models, such as InceptionV3 and VGG16, defining it as a command option while running my script.\r\n\r\nHowever, so as to the others in my research team to run my script, I had to use TF 2.0.0 due to a software limitation on the laboratory machine (CUDA 10 instead of 11), and that's when it started getting weird. I checked on the documentation if anything in the methods that I was using changed, so as to adapt them. The only changed that I saw was that in TF 2.4.1, the `fit` method supported generators, so I changed my script (using TF 2.0.0) to use the `fit_generator` method, as you can see below:\r\n- TF 2.4.1\r\n  ```python\r\n    training = model.fit(\r\n        generator=train_data_generator,\r\n        epochs=EPOCHS_NUMBER,\r\n        validation_data=valid_data_generator,\r\n        callbacks=callbacks,\r\n    )\r\n  ``` \r\n\r\n- TF 2.0.0\r\n  ```python\r\n    training = model.fit_generator(\r\n        generator=train_data_generator,\r\n        epochs=EPOCHS_NUMBER,\r\n        validation_data=valid_data_generator,\r\n        callbacks=callbacks,\r\n    )\r\n  ```\r\n\r\nBesides that, I changed nothing in my script, which you can find at the end of this description. So, okay, what's the problem, right? It occurs that the validation loss does not decrease while running the script using TensorFlow 2.0.0. I tried to check the documentation, if there were any breaking changes or something like this, after checking if the data and the seeds were the same. However, I wasn't able to find anything that helped me understand why the validation loss changed so much, the performance was degraded and due to the fact that I'm using early stopping, the training was finishing earlier than expected, resulting in a worse test performance than the one that I got running with TF 2.4.1. For instance, running the same script, initializing the seed with the same value, using the two different versions, I got the following results:\r\n\r\n- TF 2.0.0\r\n\r\nname |  val_accuracy | val_loss | val_precision | val_recall | test_accuracy | test_loss | test_precision | test_recall\r\n-- | -- | -- | -- | -- | -- | -- | -- | -- |\r\nfold_5 | 80.2899% | 0.442051 | 85.1852% | 73.3333% | 81.2022% | 0.456165 | 93.0514% | 67.3961%\r\nfold_4 |  77.3913% | 0.507347 | 85.6604% | 65.7971% | 83.4973% | 0.412218 | 95.0000% | 70.6783%\r\nfold_3 |  75.9420% | 0.517585 | 78.7781% | 71.0145% | 76.3934% | 0.515756 | 81.6273% | 68.0525%\r\nfold_2 |  74.8191% | 0.595039 | 95.2381% | 52.1739% | 74.7541% | 0.521118 | 95.9350% | 51.6411%\r\nfold_1 | 73.0825% | 0.652833 | 92.5532% | 50.2890% | 72.5683% | 0.590006 | 96.3964% | 46.8271%\r\n\r\nFor instance, in the fold 5, this is how the loss and the accuracy are evolving with time:\r\n![image](https://user-images.githubusercontent.com/19495917/119069893-57ec6600-b9bd-11eb-9b43-3bd9d9ae5bc1.png)\r\n\r\n- TF 2.4.1\r\n\r\nname | val_accuracy | val_loss | val_precision | val_recall | test_accuracy | test_loss | test_precision | test_recall\r\n-- | -- | -- | -- | -- | -- | -- | -- | -- |\r\nfold_5 | 96.6667% | 0.096043 | 96.0000% | 97.3913% | 95.5191% | 0.156118 | 92.2764% | 99.3435%\r\nfold_4 | 94.4928% | 0.134758 | 92.5208% | 96.8116% | 96.3934% | 0.145110 | 93.6214% | 99.5624%\r\nfold_3  | 96.0870% | 0.094560 | 96.7647% | 95.3623% | 96.3934% | 0.136126 | 94.1667% | 98.9059%\r\nfold_2 | 97.3951% | 0.101784 | 97.6676% | 97.1014% | 96.6120% | 0.131193 | 94.5607% | 98.9059%\r\nfold_1 | 96.3821% | 0.095308 | 97.3451% | 95.3757% | 96.5027% | 0.118284 | 95.1168% | 98.0306%\r\n\r\nIn the same fold (hoping that the seed is set correctly), but using the newest TF version, we have the following graphics:\r\n![image](https://user-images.githubusercontent.com/19495917/119070039-a7cb2d00-b9bd-11eb-874e-ebea8ed94050.png)\r\n\r\nIt's possible to notice in the first plot that it seems that nothing is happening in the validation data. It seems that the model kind of learns something, because the training loss is decreasing, but \"nothing\" is happening with the validation loss. I honestly tried to search for a lot of things in order to explain this behavior, since it's exactly the same code but wasn't able to find anything useful. I do understand that, depending on the implementation, fluctuations might happen, but it seems to me that the scenario that I presented before is really specific.\r\n\r\nUnfortunately, this is a no-go for my project because I do need to run the experiments in the other machine, which has CUDA 10, but I develop the whole application in the computer that I have access to, which is my personal one. \r\n\r\nGiven this scenario, can you, please, help me trying to understand what is going on? Am I missing any other change that I have to do in the script? Is this type of behavior expected? I would appreciate any help! It might be worth saying that for me to test it, I installed both CUDAs (10.1 and 11) on my computer and created two different _conda_ environments so I can switch between them easily.\r\n\r\nI'm sorry if I'm not in the right channel to ask this, and if it's not, please redirect me to the correct one. Thank you so much!\r\n\r\nBelow you can find my training script:\r\n```python\r\nfrom pathlib import Path\r\n\r\nimport mlflow.tensorflow\r\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\r\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.metrics import Precision, Recall\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\nfrom classes.Evaluator import Evaluator\r\nfrom utils.args import get_args_train\r\nfrom utils.inputs import get_inputs_paths_and_targets\r\nfrom utils.model import build_model, get_preprocess_function\r\nfrom utils.seed import set_seeds\r\n\r\n# Setting the random_state to get reproducible results\r\nseed = set_seeds()\r\n\r\n# Constants\r\nROOT_FOLDER = Path(__file__).resolve().parent.parent\r\nFOLDS_NUMBER = 5\r\nEPOCHS_NUMBER = 100\r\nEPOCHS_WAITING_FOR_IMPROVEMENT = 5\r\n\r\n# Gets which input we're going to use\r\nargs = get_args_train()\r\nIMAGE_FOLDER_PATH = ROOT_FOLDER / f\"crop_result_prop_{args.proportion}\"\r\n\r\n\r\nclass TrainingAndValidationMetrics(Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        mlflow.log_metrics(logs, step=epoch)\r\n\r\n\r\n# Instantiates GroupKFold class to split into train and test\r\ngroup_k_fold = GroupKFold(n_splits=FOLDS_NUMBER)\r\n\r\n# Configuration dictionary that is going to be used to compile the model\r\nconfig = {\r\n    \"optimizer\": \"adam\",\r\n    \"loss\": \"binary_crossentropy\",\r\n    \"metrics\": [\"accuracy\", Precision(), Recall()],\r\n}\r\n\r\n# Gets the dataframe that are going to be used in the flow_from_dataframe\r\n# method from the ImageDataGenerator class\r\ninput_df = get_inputs_paths_and_targets(args.proportion)\r\n\r\nidg = ImageDataGenerator(\r\n    fill_mode=\"nearest\",\r\n    preprocessing_function=get_preprocess_function(args.model.lower()),\r\n)\r\n\r\n# Using GroupKFold only to guarantee that a group (in this case, the slide)\r\n# will contain data only in the train or the test group\r\nfor train_idx, test_idx in group_k_fold.split(\r\n    input_df.input, input_df.target, input_df.slide\r\n):\r\n    train_data = input_df.iloc[train_idx]\r\n    test_data = input_df.iloc[test_idx]\r\n\r\n    # Break here is being used to get only the first fold\r\n    break\r\n\r\nprint(\"### Testing data distribution ###\")\r\nprint(f\"{test_data.groupby('slide').count()}\")\r\n\r\ngenerator_kwargs = {\r\n    \"directory\": IMAGE_FOLDER_PATH,\r\n    \"x_col\": \"input\",\r\n    \"y_col\": \"target\",\r\n    \"seed\": seed,\r\n    \"classes\": [\"0\", \"1\"],\r\n}\r\n\r\n# Generator that will be used to evaluate the model\r\ntest_data_generator = idg.flow_from_dataframe(\r\n    test_data, class_mode=\"binary\", shuffle=False, **generator_kwargs\r\n)\r\n\r\n# Instantiates the Early Stopping that is going to be used\r\n# in the fit method\r\nearly_stopping = EarlyStopping(\r\n    monitor=\"val_loss\", patience=EPOCHS_WAITING_FOR_IMPROVEMENT\r\n)\r\n\r\n# Defines the list of callbacks that is an argument of the fit\r\n# method\r\ncallbacks = [early_stopping, TrainingAndValidationMetrics()]\r\n\r\n# K-fold Cross Validation model evaluation\r\nkfold = StratifiedKFold(n_splits=FOLDS_NUMBER, shuffle=True, random_state=seed)\r\n\r\ncurrent_fold = 1\r\n# Starts run on mlflow to register metrics (experiment)\r\nmlflow.start_run(\r\n    run_name=f\"{args.model.lower()}\",\r\n    tags={\"data_proportion\": args.proportion, \"environment\": \"main\"},\r\n)\r\n\r\n\r\n# Folds iteration\r\nfor train_idx, val_idx in kfold.split(train_data.input, train_data.target):\r\n\r\n    # Builds the model\r\n    model = build_model(args.model.lower(), [Dense(1, activation=\"sigmoid\")], config)\r\n\r\n    # Starts run on mlflow to register metrics (runs)\r\n    with mlflow.start_run(run_name=f\"fold_{current_fold}\", nested=True):\r\n\r\n        fitting_data = train_data.iloc[train_idx]\r\n        val_data = train_data.iloc[val_idx]\r\n\r\n        mlflow.log_text(\r\n            f\"Training data: \\n{fitting_data.groupby('target').count()} \\n\"\r\n            f\"Validation data: \\n{val_data.groupby('target').count()} \\n\"\r\n            f\"Data proportion: {args.proportion} \\n\"\r\n            f\" ----- Using Stratified KFold with seed 1 ----- \",\r\n            artifact_file=\"data_description.txt\",\r\n        )\r\n\r\n        train_data_generator = idg.flow_from_dataframe(\r\n            fitting_data, class_mode=\"binary\", **generator_kwargs\r\n        )\r\n        valid_data_generator = idg.flow_from_dataframe(\r\n            val_data, class_mode=\"binary\", **generator_kwargs\r\n        )\r\n\r\n        training = model.fit(\r\n            train_data_generator,\r\n            epochs=EPOCHS_NUMBER,\r\n            validation_data=valid_data_generator,\r\n            callbacks=callbacks,\r\n        )\r\n\r\n        # Logging model\r\n        mlflow.keras.log_model(keras_model=model, artifact_path=\"model\")\r\n\r\n        evaluator = Evaluator(model, training, test_data_generator, test_data.target)\r\n\r\n        # Classification report\r\n        test_metrics = evaluator.evaluate_model()\r\n        mlflow.log_metrics(test_metrics)\r\n\r\n        # Saves the classification report generated from the predict method\r\n        mlflow.log_text(\r\n            evaluator.generate_classification_report(),\r\n            artifact_file=\"classification_report.txt\",\r\n        )\r\n\r\n        # Saves the accuracy and the loss per epoch\r\n        mlflow.log_figure(\r\n            evaluator.generate_training_history_image(),\r\n            artifact_file=\"accuracy_loss_epochs.png\",\r\n        )\r\n\r\n        # Saves ROC figure\r\n        mlflow.log_figure(evaluator.generate_roc_figure(), artifact_file=\"roc.png\")\r\n\r\n    current_fold += 1\r\n\r\nmlflow.end_run()\r\n```\r\n", "comments": ["@FabianaFerreira,\r\n\r\nThis issue is not a TensorFlow bug or feature request, it is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a larger community that reads questions there. \r\n\r\nThanks!", "Thank you so much, @tilakrayal and sorry for posting on the wrong community channel! I'm closing the issue."]}, {"number": 49452, "title": "Add missing validation in `QuantizedBatchNormWithGlobalNormalization`", "body": "PiperOrigin-RevId: 370123451\nChange-Id: Id234d6dab1ec21230bb8e503dba30f899af87f33", "comments": []}, {"number": 49451, "title": "Add missing validation in `QuantizedBatchNormWithGlobalNormalization`", "body": "PiperOrigin-RevId: 370123451\nChange-Id: Id234d6dab1ec21230bb8e503dba30f899af87f33", "comments": []}, {"number": 49450, "title": "Add missing validation in `QuantizedBatchNormWithGlobalNormalization`", "body": "PiperOrigin-RevId: 370123451\nChange-Id: Id234d6dab1ec21230bb8e503dba30f899af87f33", "comments": []}]