[{"number": 14418, "title": "enum34 is only required for Python < 3.4", "body": "I ran into problems when building TensorFlow 1.4.0 on top of Python 3.6.3 because `enum34` is pulled in, but not compatible with Python 3.6, i.e. `python -c 'import tensorflow'` lead to:\r\n\r\n```\r\nAttributeError: module 'enum' has no attribute 'IntFlag'\r\n```\r\n\r\nSee also https://bitbucket.org/stoneleaf/enum34/issues/19.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "How would this work if we build TF on python 3.5, but it ends up getting installed on a python 3.3 system?\r\n", "@gunan Good question, I didn't take that into account...\r\n\r\nIn that case I guess you can keep requiring `enum34` for both Python 3.4 and 3.5 too.\r\n\r\nBut for Python 3.6, it's wrong to install it. The installation works fine, but as soon as you try to use TensorFlow, it will crash hard on top of Python 3.6 with the error I mentioned.", "@boegel Interesting finding. @gunan @av8ramit  I think this has to do with the fact that we don't do real testing on python 3.6 (on Linux or Mac) currently.", "@caisq we are aware. This is already being worked on.\r\nThis is slowed down due to the confusion about multiple python versions supported. A protocol about what python versions are going to be supported is being prepared, and we will clarify our python 3 support story with that going forward.\r\n\r\n@boegel one of my concerns is the evaluation time of this file (which I do not fully know).\r\nIs it being evaluated when we are building the pip package? So if we build the pip package on a py 3.6 system,  then if people end up installing this on a py 3.4 system, do they not get py 3.4?\r\nI guess if setup.py gets evaluated during installation, this should be OK.\r\n@yifeif in case she can answer this concern.", "Looks like we can also do something like https://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies\r\nI would expect this to be evaluated during installation.", "Great, let's go with that instead then.\r\n\r\n@boegel, would you like to update your PR? we will also need you to take care of the Google CLA.\r\nor we can make this change on our end.", "Closing in favor of #14730 since that one has the CLA taken care of."]}, {"number": 14417, "title": "cuda 8.0 is not available now on NVIDIA's website", "body": "latest cuda version is 9.0.x, but what tf depends on is 8.0.x! and it seems that nvidia does not provide downloading site for 8.0 now~~~\r\nhow can i solve this problem?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "sorry, i find it now~~~", "Please tell me how to find cuda8.0 and cudnn5.1?"]}, {"number": 14416, "title": "can eager mode make use of multi-thread automatically?", "body": "just like session mode:\r\n\r\nconfig = tf.ConfigProto(device_count={\"CPU\": 4},\r\n                inter_op_parallelism_threads = 1,   \r\n                intra_op_parallelism_threads = 4,  \r\n                log_device_placement=True)  \r\nwith tf.Session(config = config) as sess:  \r\n...", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14415, "title": "tensorflow-master install from sources doesn't install with pip3", "body": "### System information\r\n- **OS Platform and Distribution**: Ubuntu 17.04\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**:  tensorflow-master\r\n- **Python version**:  3.5.3\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.9.4\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: NVIDIA 1080\r\n\r\n### Describe the problem\r\nThe bazel commands only appear to build Python 2.7 packages, not Python 3.5 packages.\r\n\r\n### To reproduce\r\nFollowing [Installing TensorFlow from Sources: Ubuntu][https://www.tensorflow.org/install/install_sources], I get  [this bazel configuration](https://github.com/tensorflow/tensorflow/files/1458780/tf_configure.bazelrc.txt) from running configure.\r\n\r\nI build and install with\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nsudo bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg/\r\n```\r\nbut the wheel generated is for Python 2.7:\r\n```\r\nls /tmp/tensorflow_pkg\r\ntensorflow-1.4.0rc1-cp27-cp27mu-linux_x86_64.whl\r\n```\r\nand so I can't install it with pip3:\r\n```\r\n$ sudo -H  pip3 install  /tmp/tensorflow_pkg/tensorflow-1.4.0rc1-cp27-cp27mu-linux_x86_64.whl\r\n[sudo] password for kevin: \r\ntensorflow-1.4.0rc1-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform.\r\n\r\n$ sudo -H  pip install  /tmp/tensorflow_pkg/tensorflow-1.4.0rc1-cp27-cp27mu-linux_x86_64.whl\r\nRequirement already satisfied: tensorflow==1.4.0rc1 from file:///tmp/tensorflow_pkg/tensorflow-1.4.0rc1-cp27-cp27mu-linux_x86_64.whl in /usr/local/lib/python2.7/dist-packages\r\n```\r\nImporting tensorflow with Python3 fails.\r\n", "comments": ["The bazel configuration from `./configure` seems to refer to Python 2:\r\n\r\n```\r\nbuild --force_python=py2\r\nbuild --host_force_python=py2\r\nbuild --python_path=\"/usr/bin/python\"\r\ntest --force_python=py2\r\ntest --host_force_python=py2\r\n```\r\n\r\nYou may have wanted to set `./configure` to `/usr/bin/python3` instead of `/usr/bin/python`. On Ubuntu, I think `/usr/bin/python` refers to Python 2.7.\r\n\r\nSince this looks like a configuration issue and not a bug, I'm closing this issue to keep the tracker focused. If you can nail this down to an error in our build pipeline, feel free to reopen or file another issue. Otherwise, please ask for help on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow), which is much better suited to community support. Thanks!", "@angersson if you're making Python version decisions based on paths that's not a good thing.  It appears to be a bug in configure.py\r\n```\r\ntensorflow-r1.4$ ./configure\r\nYou have bazel 0.7.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n/usr/local/lib/python3.5/dist-packages\r\n```\r\n  \r\n```\r\n$ python --version\r\nPython 3.5.3\r\n\r\n$ which python\r\n/usr/bin/python\r\n```\r\n", "@angersson I can't reopen the issue because you closed it, not I and I'm not a collaborator.  Given the info in the comment above, please reopen.  Thanks.", "`./configure` runs the python path provided and [asks for the version from the binary](https://github.com/tensorflow/tensorflow/blob/34609f1501ad87dde6d2b2ec0dbdf794b773e008/configure.py#L177).\r\n\r\nTo test that this is a bug, can you reply with the output of the following in a fresh shell without extra configuration (like `sh`):\r\n\r\n- `./configure` with `/usr/bin/python3` instead of the default\r\n- `/usr/bin/python3 -c 'import sys; print(sys.version[0])'`\r\n- `/usr/bin/python -c 'import sys; print(sys.version[0])'`\r\n- `python -c 'import sys; print(sys.version[0])'`", "```\r\n$ ./configure\r\nYou have bazel 0.7.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n...\r\nConfiguration finished\r\n$ /usr/bin/python3 -c 'import sys; print(sys.version[0])'\r\n3\r\n$ /usr/bin/python -c 'import sys; print(sys.version[0])'\r\n2\r\n$ python -c 'import sys; print(sys.version[0])'\r\n3\r\n```", "It looks like ./configure's default setting of `/usr/bin/python` indeed points to a python 2 binary, and explicitly specifying `/usr/bin/python3` is what's needed here. That should resolve your problem.\r\n\r\nIf you have any more environment configuration issues, please ask for help on Stack Overflow, which is much better suited to community support. Thanks!", "@angersson the fix breaks the build:\r\n```\r\n bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n...\r\nERROR: /home/kevin/Deadpool/tensorflow-master/tensorflow/python/BUILD:2990:1: undeclared inclusion(s) in rule '//tensorflow/python:_pywrap_tensorflow_internal.so':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/python/pywrap_tensorflow_internal.cc':\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/ndarrayobject.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/ndarraytypes.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/npy_common.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/numpyconfig.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/_numpyconfig.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/npy_endian.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/npy_cpu.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/utils.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/__multiarray_api.h'\r\n  'bazel-out/local_linux-py3-opt/genfiles/external/local_config_python/python_include/numpy/npy_interrupt.h'\r\nbazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc: In function 'PyObject* _wrap_PyRecordReader_New(PyObject*, PyObject*)':\r\nbazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc:6210:138: warning: 'arg2' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     result = (tensorflow::io::PyRecordReader *)tensorflow::io::PyRecordReader::New((string const &)*arg1,arg2,(string const &)*arg3,arg4);\r\n                                                                                                                                          ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option \"-Wno-self-assign\"\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1161.436s, Critical Path: 162.47s\r\nFAILED: Build did NOT complete successfully\r\n```", "Looks like problem had to do with tensorflow-master as it stood when I downloaded.  tensorflow-r1.4 works fine."]}, {"number": 14414, "title": "Error importing Tensorflow", "body": "I have tensorflow 1.4.0rc1 installed, I get errors when I tried to import tensorflow in my project.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:/Documents/TA/Skripsi/Python/TI08/Revisi Team/timeseries.py\", line 5, in <module>\r\n    from tensorflow.contrib import learn\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"D:\\Program Files (x86)\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n```\r\n\r\nCan someone help me how to solve this error? Thanks!", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14413, "title": "Can't get support template", "body": "I seem to have lost the support template when I open issues in tensorflow on Chrome.\r\n\r\nAny way to get it back?", "comments": ["It's back now.  Weird."]}, {"number": 14412, "title": "Fix typo. <Copybara Experiment DO NOT MERGE>", "body": "Fix typo in tensorflow/python/layers/base_test.py", "comments": []}, {"number": 14411, "title": "Revert \"golang: ~2x speedup for encodeTensor()\"", "body": "Reverts tensorflow/tensorflow#14368\r\n\r\nThis makes go:test fail.", "comments": ["This is a good change in principle, please open a new PR to reinstate it."]}, {"number": 14410, "title": "Is there a clean way to upgrade my TF (built from sources in production env) from current 1.31 to 1.4?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "OK thanks!"]}, {"number": 14409, "title": "added patch needed for jetson tx2/1", "body": "arm64 does not support NUMA, so TryToReadNumaNode in cuda_gpu_executor.cc has to be modified accordingly to avoid a failure at runtime. \r\n\r\nCurrently everybody using a jetson board needs to apply this patch manually, which is a pain if you forget, because it fails at runtime, and not at build. This means that you compile for a couple of hours only to realize you should've done this first. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please", "How should we move forward with this? @jhseu "]}, {"number": 14408, "title": "Fix Jenkins build error (`//tensorflow/go:test`) on HEAD", "body": "This fix fixes Jenkins build error of `//tensorflow/go:test` on HEAD introduced by #14368:\r\n\r\n/cc @anight @asimshankar @martinwicke \r\n\r\n```\r\nINFO: From Testing //tensorflow/go:test:\r\n==================== Test output for //tensorflow/go:test:\r\n...\r\n...\r\n--- FAIL: TestNewTensor (0.00s)\r\n\ttensor_test.go:112: NewTensor([5]): <nil>\r\n\ttensor_test.go:115: NewTensor([5]) = &{0x7f71f4009e80 [1]}, want nil\r\n\ttensor_test.go:112: NewTensor([5]): <nil>\r\n\ttensor_test.go:115: NewTensor([5]) = &{0x7f71f400af10 [1]}, want nil\r\nFAIL\r\nFAIL\tgithub.com/tensorflow/tensorflow/tensorflow/go\t0.392s\r\nok  \tgithub.com/tensorflow/tensorflow/tensorflow/go/op\t0.138s\r\n================================================================================\r\n```\r\n\r\nThe error was introduced by #14368 because Uint32 and Uint64 has not been supported by TensorFlow yet.\r\n\r\nSee Ln 74 of `tensorflow/go/tensor_test.go`\r\nCompare Ln 334 and 316 of `tensorflow/go/tensor.go` before this PR.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "This has happened because I was expecting https://github.com/tensorflow/tensorflow/pull/14331 to be merged before this one.\r\n\r\nThe proposed fix is fine, we can revert it when (if) https://github.com/tensorflow/tensorflow/pull/14331 is merged.", "Sorry, I saw this too late, I already reverted. Could you roll the original forward with your fix?", "I'll create a new pr, thanks.", "Thanks @anight and looking forward to the PR. I will close this one."]}, {"number": 14407, "title": "Failed to connect to dl.google.com port 443: Operation timed out", "body": "when I run the simple project of ios, I just run \"pod install\", but I get the error message of the title, however, if I copy the url \"https://dl.google.com/dl/cpdc/2cf20b661cbb3374/TensorFlow-experimental-1.1.1.tar.gz\" to my safari, it download speed is fast, who can help me? By the way, my ruby/cocoapods all are newest, thx.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14406, "title": "cmake build type is hard coded in grpc.cmake", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows Server 2012\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\n92838685241ca22ea2797e937e380bdbe8325784\r\n\r\n- **Python version**: \r\n3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nVS 2015\r\n\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\n\"D:\\src\\tensorflow\\b\\packages\\cmake\\bin\\cmake\" -G \"Visual Studio 14 2015 Win64\" D:\\src\\tensorflow\\s\\tensorflow\\contrib\\cmake -DCMAKE_BUILD_TYPE=Debug -DPYTHON_EXECUTABLE=\"D:\\src\\tensorflow\\b\\packages\\python\\python.exe\" -DPYTHON_LIBRARIES=\"D:\\src\\tensorflow\\b\\packages\\python\\libs\\python35.lib\" -DSWIG_EXECUTABLE=D:\\src\\tensorflow\\b\\packages\\swigwin.3.0.9\\tools\\swigwin-3.0.9\\swig.exe -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_BUILD_PYTHON_TESTS=OFF \r\n\r\nThen build tensorflow.vcxproj\r\n\r\n### Describe the problem\r\nbuild failed\r\n\r\n### Source code / logs\r\n", "comments": []}, {"number": 14405, "title": "`keras` model with `tf.keras.layers.Conv2D` layers compiled with `tf.losses.softmax_cross_entropy` loss does not converge", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: simple MNIST model in keras\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8\r\n- **TensorFlow installed from (source or binary)**: binary from pip\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.5.3\r\n\r\n### Describe the problem\r\nWhen using `tf.losses.softmax_cross_entropy` in a `tf.keras` model with `tf.keras.layers.Conv2D` layer, `model.fit()` does not converge, although no exception raised loss does not change.\r\nWhen change loss function to `tf.keras.losses.categorical_crossentropy`, or layer to `tf.layers.Conv2D`, it converges quickly.\r\n\r\n### Source code / logs\r\n`model.fit()` log:\r\nTrain on 50000 samples, validate on 10000 samples\r\nEpoch 1/5\r\n50000/50000 [==============================] - 84s - loss: 2.3622 - acc: 0.0986 - val_loss: 2.3662 - val_acc: 0.0950\r\nEpoch 2/5\r\n50000/50000 [==============================] - 83s - loss: 2.3633 - acc: 0.0978 - val_loss: 2.3662 - val_acc: 0.0950\r\nEpoch 3/5\r\n50000/50000 [==============================] - 83s - loss: 2.3633 - acc: 0.0978 - val_loss: 2.3662 - val_acc: 0.0950\r\nEpoch 4/5\r\n50000/50000 [==============================] - 83s - loss: 2.3633 - acc: 0.0978 - val_loss: 2.3662 - val_acc: 0.0950\r\nEpoch 5/5\r\n50000/50000 [==============================] - 84s - loss: 2.3633 - acc: 0.0978 - val_loss: 2.3662 - val_acc: 0.0950\r\n9984/10000 [============================>.] - ETA: 0s \r\nTest loss: 2.362950. Test accuracy: 0.098200", "comments": ["Can you provide a minimum viable sample of code that replicates this issue?", "Please see the [notebook](https://gist.github.com/tranhungnghiep/f2465272561aaaf68a01ad567d6c4292) for replicate, note the two different outputs when changing `loss` in the model.\r\n\r\nNow when I check again, it is quite subtle. When changing the configuration a little the behavior will change, not sure why.", "It looks like this is a configuration problem -- if your recent changes haven't resolved your issue, please ask a question on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@angersson imho, I think this is a bug. Notice that for the same model, `tf.keras.losses.categorical_crossentropy` lets it converge, but `tf.losses.softmax_cross_entropy` does not. So this shows a case when the compatibility in `tf` fails. Thanks for looking into it.", "The two functions don't even take the same arguments. One takes logits and one takes prediction. They are never compatible.", "OK, I see. I was confused by its name. Putting the logits in and it works fine. Thanks."]}, {"number": 14404, "title": "Fix a build error in windows debug build", "body": "Required by std::_Debug_range<_FwdIt> in std::lower_bound function. Fix #14396", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please. "]}, {"number": 14403, "title": "modified convolution document", "body": "fix  #14027", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the PR. The changes look good. However, you should apply them to every relevant docstring in this file. Currently it's only applied to the docstring of the private base class, which isn't publicly visible.", "Jenkins, test this please", "@jhseu  Please retest it, thanks.", "@tensorflow-jenkins test this please"]}, {"number": 14402, "title": "Encountered a training error when using slim with multi-GPU connect by PIX type", "body": "Hi all, I encountered a training error when I training model in multi GPU.    \r\nThe key condition is using slim to write model and training in multi GPU(GTX TITAN X) which connect by PIX type.\r\n\r\nHow to reproduce:\r\n * 1. checkout office code in [here](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10)\r\n * 2. copy code in below(slim model code) and replace it to cifar10.py file. (Because only slim code can reproduce this issue)\r\n * 3. start to training: ```CUDA_VISIBLE_DEVICES=1,0 python cifar10_multi_gpu_train.py --num_gpus=2```\r\n * 4. an incorrect loss will appear. (Also GPU usage is incorrect)\r\n\r\nNote 1: Check GPU info in below. this issue only happend when I using GPU 0,1 or GPU 2,3 or GPU 4,5 or GPU 6,7(connect by PIX). In this case, everything is ok If I using other combinations   \r\nNote 2: I can't reproduce this issue in GTX 1080 ti   \r\n\r\nThank you so much!\r\n\r\nThe training error log in below:\r\n\r\n```\r\nlinux@172.25.52.02:~/models/tutorials/image/cifar10: CUDA_VISIBLE_DEVICES=1,0 python cifar10_multi_gpu_train.py --num_gpus=2\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\n2017-11-09 18:13:02.264529: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are\r\navailable on your machine and could speed up CPU computations.\r\n2017-11-09 18:13:06.359127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:05:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 11.81GiB\r\n2017-11-09 18:13:06.784690: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x5f00120 exists before initializing the StreamExecutor\r\n. We haven't verified StreamExecutor works with that.\r\n2017-11-09 18:13:06.786072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties:\r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:04:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 11.81GiB\r\n2017-11-09 18:13:06.786507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1\r\n2017-11-09 18:13:06.786527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y\r\n2017-11-09 18:13:06.786592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y\r\n2017-11-09 18:13:06.786623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\r\n2017-11-09 18:13:06.786635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:04:00.0)\r\n2017-11-09 18:13:06.830077: I tensorflow/core/common_runtime/simple_placer.cc:697] Ignoring device specification /device:GPU:1 for node 'tower_1/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0\r\n2017-11-09 18:13:06.830125: I tensorflow/core/common_runtime/simple_placer.cc:697] Ignoring device specification /device:GPU:0 for node 'tower_0/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0\r\n2017-11-09 18:13:09.804877: step 0, loss = 2.30 (119.9 examples/sec; 1.067 sec/batch)\r\n2017-11-09 18:13:17.096214: step 10, loss = 2.30 (421.3 examples/sec; 0.304 sec/batch)\r\n2017-11-09 18:13:23.708459: step 20, loss = 2.30 (390.0 examples/sec; 0.328 sec/batch)\r\n2017-11-09 18:13:31.362454: step 30, loss = 2.30 (362.5 examples/sec; 0.353 sec/batch)\r\n2017-11-09 18:13:38.225150: step 40, loss = 2.30 (363.3 examples/sec; 0.352 sec/batch)\r\n2017-11-09 18:13:44.695175: step 50, loss = 2.30 (389.8 examples/sec; 0.328 sec/batch)\r\n2017-11-09 18:13:51.994245: step 60, loss = 2.30 (362.5 examples/sec; 0.353 sec/batch)\r\n2017-11-09 18:13:58.623306: step 70, loss = 2.30 (421.2 examples/sec; 0.304 sec/batch)\r\n2017-11-09 18:14:05.480061: step 80, loss = 2.30 (390.6 examples/sec; 0.328 sec/batch)\r\n2017-11-09 18:14:11.952376: step 90, loss = 2.30 (389.2 examples/sec; 0.329 sec/batch)\r\n2017-11-09 18:14:18.375831: step 100, loss = 293778.34 (362.7 examples/sec; 0.353 sec/batch)\r\nTraceback (most recent call last):\r\n  File \"cifar10_multi_gpu_train.py\", line 283, in <module>\r\n    tf.app.run()\r\n  File \"/home/zhangjiguo/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"cifar10_multi_gpu_train.py\", line 278, in main\r\n    train()\r\n  File \"cifar10_multi_gpu_train.py\", line 251, in train\r\n    assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\r\nAssertionError: Model diverged with loss = NaN\r\n```\r\n\r\nMy GPU list with command `nvidia-smi -L`    \r\n\r\n```\r\nGPU 0: GeForce GTX TITAN X (UUID: GPU-2c9601fc-369e-fce7-d139-8c7144700def)\r\nGPU 1: GeForce GTX TITAN X (UUID: GPU-eb4d8327-7058-39fe-e510-3437fe258745)\r\nGPU 2: GeForce GTX TITAN X (UUID: GPU-62fbacf7-0120-5c6b-ceea-ad026fddc537)\r\nGPU 3: GeForce GTX TITAN X (UUID: GPU-15be755d-9982-f4f4-4455-1d648d836b6e)\r\nGPU 4: GeForce GTX TITAN X (UUID: GPU-e48fae89-debd-9769-3851-61ba44acd6e1)\r\nGPU 5: GeForce GTX TITAN X (UUID: GPU-f796ad02-e104-04b5-d6c1-ff72d872654f)\r\nGPU 6: GeForce GTX TITAN X (UUID: GPU-7e27fd1e-ba3b-2dd3-6d03-960b8c76a0be)\r\nGPU 7: GeForce GTX TITAN X (UUID: GPU-8de522e7-f375-ab05-28ff-5ddfd9e325b0)\r\n```\r\n\r\nand GPU connection info with command `nvidia-smi topo -m`\r\n\r\n```\r\n       \tGPU0   \tGPU1   \tGPU2   \tGPU3   \tGPU4   \tGPU5   \tGPU6   \tGPU7   \tmlx4_0 \tCPU Affinity\r\nGPU0   \t X     \tPIX    \tPHB    \tPHB    \tSOC    \tSOC    \tSOC    \tSOC    \tSOC    \t0-13,28-41\r\nGPU1   \tPIX    \t X     \tPHB    \tPHB    \tSOC    \tSOC    \tSOC    \tSOC    \tSOC    \t0-13,28-41\r\nGPU2   \tPHB    \tPHB    \t X     \tPIX    \tSOC    \tSOC    \tSOC    \tSOC    \tSOC    \t0-13,28-41\r\nGPU3   \tPHB    \tPHB    \tPIX    \t X     \tSOC    \tSOC    \tSOC    \tSOC    \tSOC    \t0-13,28-41\r\nGPU4   \tSOC    \tSOC    \tSOC    \tSOC    \t X     \tPIX    \tPHB    \tPHB    \tPHB    \t14-27,42-55\r\nGPU5   \tSOC    \tSOC    \tSOC    \tSOC    \tPIX    \t X     \tPHB    \tPHB    \tPHB    \t14-27,42-55\r\nGPU6   \tSOC    \tSOC    \tSOC    \tSOC    \tPHB    \tPHB    \t X     \tPIX    \tPHB    \t14-27,42-55\r\nGPU7   \tSOC    \tSOC    \tSOC    \tSOC    \tPHB    \tPHB    \tPIX    \t X     \tPHB    \t14-27,42-55\r\nmlx4_0 \tSOC    \tSOC    \tSOC    \tSOC    \tPHB    \tPHB    \tPHB    \tPHB    \t X\r\n\r\nLegend:\r\n\r\n  X   = Self\r\n  SOC  = Connection traversing PCIe as well as the SMP link between CPU sockets(e.g. QPI)\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing a single PCIe switch\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\nslim model code: (Please replace these code to cifar10.py)\r\n\r\n```\r\ndef inference(images):\r\n\t# reimplement by slim\r\n    slim = tf.contrib.slim\r\n    conv1 = slim.conv2d(images, 64, [5, 5], scope='conv1',\r\n                       weights_regularizer=slim.l2_regularizer(0.0),\r\n                       weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\r\n                       activation_fn=tf.nn.relu)\r\n    pool1 = slim.max_pool2d(conv1, [3, 3], 2, padding='SAME', scope='poll1')\r\n    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\r\n    conv2 = slim.conv2d(norm1, 64, [5, 5], scope='conv2',\r\n                       weights_regularizer=slim.l2_regularizer(0.0),\r\n                       weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),\r\n                       biases_initializer=tf.constant_initializer(0.1),\r\n                       activation_fn=tf.nn.relu)\r\n    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\r\n    pool2 = slim.max_pool2d(norm2, [3, 3], 2, padding='SAME', scope='poll2')\r\n    flatten2 = slim.flatten(pool2)\r\n    local3 = slim.fully_connected(flatten2, 384, scope='local3',\r\n                       weights_regularizer=slim.l2_regularizer(0.004),\r\n                       weights_initializer=tf.truncated_normal_initializer(stddev=0.04),\r\n                       biases_initializer=tf.constant_initializer(0.1),\r\n                       activation_fn=tf.nn.relu)\r\n    local4 = slim.fully_connected(local3, 192, scope='local4',\r\n                       weights_regularizer=slim.l2_regularizer(0.004),\r\n                       weights_initializer=tf.truncated_normal_initializer(stddev=0.04),\r\n                       biases_initializer=tf.constant_initializer(0.1),\r\n                       activation_fn=tf.nn.relu)\r\n    softmax_linear = slim.fully_connected(local4, NUM_CLASSES, scope='softmax_linear',\r\n                       weights_regularizer=slim.l2_regularizer(0.0),\r\n                       weights_initializer=tf.truncated_normal_initializer(stddev=1/192.0),\r\n                       biases_initializer=tf.constant_initializer(0.0),\r\n                       activation_fn=None)\r\n    return softmax_linear\r\n```\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:CentOS Linux release 7.2.1511\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:Tensorflow 1.2.0/ 1.0.1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 1.4.5\r\n- **GCC/Compiler version (if compiling from source)**:4.8.5\r\n- **CUDA/cuDNN version**:CUDA 8.0 and cuDNN 5.0\r\n- **GPU model and memory**:TITAN X with 12GB\r\n- **Exact command to reproduce**:\r\n", "comments": ["This might be related to the customized model you defined, as the loss going to nan caused this training error. We can't debug or reproduce this since it seems it's hardware specific. Please debug further yourself and let us know if there is a bug on tensorflow side.\r\n \r\n ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Hi there, can you open this. I'm facing the exact same issue with the thread owner, with GTX 1080ti"]}, {"number": 14401, "title": "CMake: configure default string values of options properly", "body": "Because cmake configures defaults values as ON or OFF only,\r\nstring values as default doesn't work.\r\n\r\nThus, when it is set \"OFF\", we need to re-set the values.\r\n\r\nFixes #14400\r\n\r\nSigned-off-by: MyungJoo Ham <myungjoo.ham@samsung.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please."]}, {"number": 14400, "title": "[CMake] Default values of some options are not properly set", "body": "```tensorflow_PATH_CUDNN_STATIC_LIB``` and ```tensorflow_PATH_NCCL_STATIC_LIB``` are supposed to be configured as the value of ```tensorflow_PATH_STATIC_LIB``` by default.\r\n\r\nHowever, because cmake options's default values are either OFF or ON, this doesn't work as supposed.\r\n\r\nI'll post a Pull Request addressing this issue in a few minutes.", "comments": []}, {"number": 14399, "title": "Fixed typos in comments", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 14398, "title": "name_scope is indistinguishable from string type ", "body": "variable_scope yields a class VariableScope when entered, while name_scope yields a string. \r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.name_scope(\"n1\"):\r\n    with tf.name_scope(\"n2\") as n2:\r\n        print(\"name_scope: {}\".format(n2))\r\n        print(\"get_name_scope: {}\".format(tf.contrib.framework.get_name_scope()))\r\n\r\nwith tf.variable_scope(\"v1\"):\r\n    with tf.variable_scope(\"v2\") as v2:\r\n        print(\"variable_scope: {}\".format(v2))\r\n        print(\"get_name_scope: {}\".format(tf.get_variable_scope()))\r\n\r\n# output\r\n# name_scope: n1/n2/\r\n# get_name_scope: n1/n2\r\n# variable_scope: <tensorflow.python.ops.variable_scope.VariableScope object at 0x11ba34d68>\r\n# get_name_scope: <tensorflow.python.ops.variable_scope.VariableScope object at 0x11ba34d68>\r\n```\r\n\r\nI think string might be unsafe. Moreover, sometimes we have to hack a \"/\" suffix if we want to reenter the current name scope. \r\n\r\nEspecially, it's difficult to check the type when `name_scope` is an argument for a function.\r\n```python\r\ndef func(my_name_scope, my_variable_scope):\r\n    if isinstance(my_variable_scope, (VariableScope, variable_scope)):\r\n        # do something\r\n    else:\r\n        raise TypeError()\r\n\r\n    if isinstance(my_name_scope, (six.string_types, name_scope)):\r\n        # how to do if it's an invalid string?\r\n    else:\r\n        raise TypeError()\r\n```\r\n\r\nWill tensorflow plan to introduce a class like VariableScope, say NameScope, in the future? Thanks.", "comments": ["@martinwicke ", "That's a fair point, because name_scope is about op names only, it's tied much stronger to strings. \r\n\r\nI don't think we'll change that, it is much more likely that variable_scope will disappear from view much more than it is that we'll increase the scope of name_scope."]}, {"number": 14397, "title": "remove duplicated code", "body": "I'm remove duplicated code, and to avoid missing when adding new error code.\r\n", "comments": ["Can one of the admins verify this patch?", "Seems like it was intentionally done this way, but adding @skye to chime in.", "@josh11b actually wrote this, but I agree we should keep it as-is. It's actually less error-prone in the long run, as it doesn't depend on TF_Code and the proto being in sync (and will fail if we add a new code to the proto without adding it to TF_Code).", "@skye @jhseu It has delayed for a long time. the modification will be less error-prone in future, and remove duplicated code implementation.", "Closing as per Skye's comments."]}, {"number": 14396, "title": "Unable to compile tensorflow in Win/Debug mode", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows Server 2012\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\n92838685241ca22ea2797e937e380bdbe8325784\r\n\r\n- **Python version**: \r\n3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nVS 2015\r\n\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\n\"D:\\src\\tensorflow\\b\\packages\\cmake\\bin\\cmake\" -G \"Visual Studio 14 2015 Win64\" D:\\src\\tensorflow\\s\\tensorflow\\contrib\\cmake -DCMAKE_BUILD_TYPE=Debug -DPYTHON_EXECUTABLE=\"D:\\src\\tensorflow\\b\\packages\\python\\python.exe\" -DPYTHON_LIBRARIES=\"D:\\src\\tensorflow\\b\\packages\\python\\libs\\python35.lib\" -DSWIG_EXECUTABLE=D:\\src\\tensorflow\\b\\packages\\swigwin.3.0.9\\tools\\swigwin-3.0.9\\swig.exe -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_BUILD_PYTHON_TESTS=OFF \r\n\r\nThen build tensorflow.vcxproj\r\n\r\n### Describe the problem\r\nbuild failed\r\n\r\n### Source code / logs\r\n```\r\n ##[error]C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\xutility(958,0): Error C2678: binary '<': no operator found which takes a left-hand operand of type 'tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file D:\\src\\tensorflow\\s\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n     11>C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\xutility(958): error C2678: binary '<': no operator found which takes a left-hand operand of type 'tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file D:\\src\\tensorflow\\s\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc) [D:\\src\\tensorflow\\b\\debug\\tf_core_kernels.vcxproj]\r\n          C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\thread(224): note: could be 'bool std::operator <(std::thread::id,std::thread::id) noexcept' (compiling source file D:\\src\\tensorflow\\s\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n          C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\system_error(436): note: or       'bool std::operator <(const std::error_condition &,const std::error_condition &) noexcept' (compiling source file D:\\src\\tensorflow\\s\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n          C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\system_error(427): note: or       'bool std::operator <(const std::error_code &,const std::error_code &) noexcept' (compiling source file D:\\src\\tensorflow\\s\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n          C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\xutility(958): note: while trying to match the argument list '(tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator, tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator)' (compiling source file D:\\src\\tensorflow\\s\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n          C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\xutility(967): note: see reference to function template instantiation 'void std::_Debug_range2<_InIt>(_RanIt,_RanIt,std::_Dbfile_t,std::_Dbline_t,std::random_access_iterator_tag)' being compiled\r\n                  with\r\n                  [\r\n                      _InIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n                      _RanIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator\r\n                  ] (compiling source file D:\\src\\tensorflow\\s\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n          C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\algorithm(2196): note: see reference to function template instantiation 'void std::_Debug_range<_FwdIt>(_InIt,_InIt,std::_Dbfile_t,std::_Dbline_t)' being compiled\r\n                  with\r\n                  [\r\n                      _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n                      _InIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator\r\n                  ] (compiling source file D:\\src\\tensorflow\\s\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n          C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\algorithm(2206): note: see reference to function template instantiation '_FwdIt std::lower_bound<_FwdIt,_Ty,std::less<void>>(_FwdIt,_FwdIt,const _Ty &,_Pr)' being compiled\r\n                  with\r\n                  [\r\n                      _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n                      _Ty=tensorflow::int64,\r\n                      _Pr=std::less<void>\r\n                  ] (compiling source file D:\\src\\tensorflow\\s\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n          D:\\src\\tensorflow\\s\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc(115): note: see reference to function template instantiation '_FwdIt std::lower_bound<tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,tensorflow::int64>(_FwdIt,_FwdIt,const _Ty &)' being compiled\r\n                  with\r\n                  [\r\n                      _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n                      _Ty=tensorflow::int64\r\n                  ]\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14395, "title": "Installing TensorFlow for C", "body": "# Installing TensorFlow for C\r\n\r\nTensorFlow\u5728 [`c_api.h`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h)\u4e2d\u5b9a\u4e49\u4e86\u4e00\u5957 C API,\u7528\u6765\u63d0\u4f9b\u9002\u5408\u4e8e[\u5efa\u7acb\u548c\u5176\u4ed6\u8bed\u8a00\u7684\u7ed1\u5b9a](https://www.tensorflow.org/extend/language_bindings).\r\n\u8fd9\u5957API\u503e\u5411\u4e8e\u7b80\u5355\u6027\u548c\u4e00\u81f4\u6027\uff0c\u800c\u4e0d\u662f\u65b9\u4fbf\u3002\r\n\r\n## \u652f\u6301\u7684\u5e73\u53f0\r\n\r\n\u4f60\u53ef\u80fd\u4f1a\u5728\u4e0b\u9762\u7684\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u5b89\u88c5\u7528\u4e8eC\u7684TensorFlow:\r\n  * Linux\r\n  * Mac OS X\r\n\r\n\r\n## \u5b89\u88c5\r\n\u91c7\u53d6\u4e0b\u9762\u51e0\u6b65\u6765\u5b89\u88c5\u7528\u4e8eC\u7684TensorFlow\u5e93\uff0c\u7136\u540e\u6253\u5f00\u7528\u4e8eC\u7684TensorFlow\uff1a\r\n  1.\u9009\u62e9\u4f60\u5c06\u4f1a\u4ec5\u4ec5\u8fd0\u884c\u7528\u4e8eC\u7684TensoFlow\u5728CPU(S)\u4e0a\uff0c\u8fd8\u662f\u6709GPU(S)\u7684\u5e2e\u52a9\u3002\u4e3a\u4e86\u5e2e\u4f60\u505a\u51fa\u9009\u62e9\uff0c\u5728\u4ee5\u4e0b\u6307\u5357\u4e2d\u9605\u8bfb\u8fd9\u4e00\u8282\uff0c\u6807\u9898\u4e3a\u201c\u51b3\u5b9a\u5b89\u88c5\u54ea\u4e2aTensorFlow\u201d\uff1a\r\n       * @{$install_linux#determine_which_tensorflow_to_install$Installing TensorFlow on Linux}\r\n       * @{$install_mac#determine_which_tensorflow_to_install$Installing TensorFlow on Mac OS}\r\n\r\n  2.\u901a\u8fc7\u8c03\u7528\u4e0b\u9762\u7684shell\u547d\u4ee4\uff0c\u4e0b\u8f7d\u5e76\u4e14\u89e3\u538bTensorFlow\u7684C\u5e93\u5230`/usr/local/lib`\uff1a\r\n         TF_TYPE=\"cpu\" # Change to \"gpu\" for GPU support\r\n         OS=\"linux\" # Change to \"darwin\" for Mac OS\r\n         TARGET_DIRECTORY=\"/usr/local\"\r\n         curl -L \\\r\n           \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-${TF_TYPE}-${OS}-x86_64-1.4.0-rc0.tar.gz\" |\r\n           sudo tar -C $TARGET_DIRECTORY -xz\r\n\r\n`tar` \u547d\u4ee4\u4f1a\u89e3\u538bTensorFlow C\u5e93\u5230`TARGET_DIRECTORY`\u7684\u5b50\u76ee\u5f55 `lib`\u4e2d . \u6bd4\u5982\u6307\u5b9a `/usr/local`\u4f5c\u4e3a`TARGET_DIRECTORY`,\u90a3\u4e48 `tar`\u5c31\u4f1a\u89e3\u538bTensorFlow C\u5e93\u5230`/usr/local/lib`.\r\n\r\n\u5982\u679c\u4f60\u66f4\u5e0c\u671b\u89e3\u538b\u5e93\u5230\u4e0d\u540c\u7684\u76ee\u5f55\uff0c\u90a3\u4e48\u76f8\u5e94\u7684\u8c03\u6574`TARGET_DIRECTORY`\r\n\r\n  3.\u5728\u4e0a\u4e00\u6b65\u4e2d\uff0c\u5982\u679c\u4f60\u6307\u5b9a\u4e86\u4e00\u4e2a\u7cfb\u7edf\u76ee\u5f55(\u6bd4\u5982,`/usr/local`)\u4f5c\u4e3a`TARGET_DIRECTORY`, \u7136\u540e\u8fd0\u884c`ldconfig`\u914d\u7f6e\u94fe\u63a5\u5668.\r\n  \u6bd4\u5982:\r\n  <pre><b>sudo ldconfig</b></pre>\r\n  \u5982\u679c\u4f60\u6307\u5b9a\u4e86\u4e00\u4e2a `TARGET_DIRECTORY`\u800c\u4e0d\u662f\u7cfb\u7edf\u76ee\u5f55, (\u6bd4\u5982, `~/mydir`)\uff0c\u90a3\u4e48\u4f60\u5fc5\u987b\u8bbe\u5b9a\u4f60\u7684\u89e3\u538b\u76ee\u5f55(\u6bd4\u5982, `~/mydir/lib`) \u5230\u4e24\u4e2a\u73af\u5883\u53d8\u91cf\u4e2d\u3002\r\n  \u6bd4\u5982:\r\n  <pre> <b>export LIBRARY_PATH=$LIBRARY_PATH:~/mydir/lib</b> # For both Linux and Mac OS X\r\n  <b>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/mydir/lib</b> # For Linux only\r\n  <b>export DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:~/mydir/lib</b> # For Mac OS X only</pre>\r\n\r\n## \u9a8c\u8bc1\u4f60\u7684\u5b89\u88c5\r\n\u5728\u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\uff0c\u65b0\u5efa\u6587\u4ef6\uff0c\u8f93\u5165\u4ee5\u4e0b\u4ee3\u7801\uff0c\u6587\u4ef6\u547d\u540d\u4e3a`hello_tf.c`:\r\n```c\r\n#include <stdio.h>\r\n#include <tensorflow/c/c_api.h>\r\n\r\nint main() {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n  return 0;\r\n}\r\n```\r\n\r\n### \u7f16\u8bd1\u548c\u8fd0\u884c\r\n\r\n\u8c03\u7528\u4ee5\u4e0b\u547d\u4ee4\u6765\u7f16\u8bd1 `hello_tf.c`\r\n<pre><b>gcc hello_tf.c</b></pre>\r\n\u8fd0\u884c\u751f\u6210\u7684\u53ef\u6267\u884c\u6587\u4ef6\u5e94\u8be5\u8f93\u51fa\u4ee5\u4e0b\u6d88\u606f:\r\n<pre><b>a.out</b>\r\nHello from TensorFlow C library version <i>number</i></pre>\r\n\r\n### \u5b9a\u4f4d\u95ee\u9898\r\n\u5982\u679c\u7a0b\u5e8f\u7f16\u8bd1\u5931\u8d25\uff0c\u6700\u6709\u53ef\u80fd\u7684\u9519\u8bef\u662f`gcc`\u627e\u4e0d\u5230TensorFlow C\u5e93.\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u65b9\u6cd5\u662f\u4e3a`gcc`\u6307\u5b9a`-I` \u548c `-L`\u9009\u9879.\u6bd4\u5982,`TARGET_LIBRARY`\u662f`/usr/local`, \u4f60\u5e94\u8be5\u8fd9\u6837\u8c03\u7528`gcc`: \r\n\r\n<pre><b>gcc -I/usr/local/include -L/usr/local/lib hello_tf.c -ltensorflow</b></pre>\r\n\r\n\u5982\u679c\u6267\u884c`a.out`\u5931\u8d25,\u4f60\u5c31\u8981\u95ee\u95ee\u81ea\u5df1\u8fd9\u51e0\u4e2a\u95ee\u9898\u4e86:\r\n  * \u8fd9\u4e2a\u7a0b\u5e8f\u7f16\u8bd1\u6709\u6ca1\u6709\u9519\u8bef\uff1f\r\n  * \u662f\u5426\u6309\u7b2c\u4e09\u6b65 [\u5b89\u88c5](#\u5b89\u88c5), \u6307\u5b9a\u4e86\u6b63\u786e\u7684\u73af\u5883\u53d8\u91cf\u7684\u76ee\u5f55?\r\n  * \u662f\u5426\u6709\u6b63\u786e\u7684`export`\u8fd9\u4e9b\u73af\u5883\u53d8\u91cf?\r\n\r\n\u5982\u679c\u4f60\u4ecd\u7136\u4f1a\u6709\u7f16\u8bd1\u6216\u8005\u8fd0\u884c\u7684\u9519\u8bef\u4fe1\u606f, \u8bf7\u5230[StackOverflow](www.stackoverflow.com/questions/tagged/tensorflow)\u5bfb\u627e\u6216\u8005\u8bf7\u6c42\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Sorry, changing the installation instructions in this way, whatever its merits, would make it hard for our english-speaking users to read them."]}, {"number": 14394, "title": "TF Detect app doesnt work with my own weights", "body": "Hello guys, I followed the instruction of object detection android app and successfully run the app using the pretrained tiny-yolo-voc. \r\n\r\nNow I have trained another model by myself with my own dataset(1class) using Darknet. Then I convert the weights file to pb type format using the Darkflow command\r\n`./flow --model cfg/yolo-1class.cfg --load bin/yolo-marker_1class.weights --savepb --verbalise`\r\n\r\nUsing the summerize_graph tool, I checked the yolo-marker_1class.pb has the same 'input' and 'output' node.\r\n\r\nAfter i manually copy this pb file into the asset folder and click Run button, I had this error which is not happened when I build when the pretrained tiny-yolo-voc.pb\r\nCan anyone help me please ? \r\n![1](https://user-images.githubusercontent.com/11288381/32594925-38e2801c-c571-11e7-8992-4cd36675fb34.png)\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14393, "title": "Error:Execution failed for task ':buildNativeBazel'.", "body": " This problem occured with the android example in android studio.\r\n ->A problem occurred starting process 'command '/usr/local/bin/bazel''", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14392, "title": "The weights argument in Keras's Embedding does not work", "body": "I am using Tensorflow 1.4.0.\r\n\r\nAccording to this [blog post](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html), we can use the weights argument in the call to Embedding to specify some matrix that represents a pre-trained word embeddings (see the section titled Preparing the Embedding Layer).\r\n\r\nHowever, this code does not work:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.python.estimator.model_fn import EstimatorSpec\r\nfrom tensorflow.contrib.keras.api.keras.layers import Embedding, Dense\r\nfrom tensorflow.contrib.keras.api.keras.initializers import Constant\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    x = tf.constant([[1]])\r\n    labels = tf.constant([[10.]])\r\n    \r\n    # Let m be our pre-trained word embeddings\r\n    m = np.array([[1, 2], [3, 4]], np.float32)\r\n    with tf.name_scope('Embedding_Layer'):\r\n        # Create an embedding layer and load m into it\r\n        n = Embedding(2, 2, weights=[m], input_length=1, name='embedding_matrix_1', trainable=False)\r\n\r\n    lookup = n(x)\r\n    lookup = tf.Print(lookup, [lookup])\r\n\r\n    preds = Dense(1)(lookup)\r\n    loss = tf.reduce_mean(labels - preds)\r\n    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss, tf.train.get_global_step())\r\n\r\n    eval_metric_ops = {'accuracy': tf.metrics.accuracy(labels, preds)}\r\n    return EstimatorSpec(mode=mode, loss=loss, train_op=train_op, eval_metric_ops=eval_metric_ops)\r\n\r\n\r\nmodel = tf.estimator.Estimator(model_fn)\r\nmodel.train(input_fn=lambda: None, steps=1)\r\n```\r\n\r\n`lookup` should print [[3 4]] but instead random numbers are printed.\r\n\r\nThe solution is to define `n` as follows:\r\n\r\n```\r\nn = Embedding(2, 2, embeddings_initializer=Constant(m), input_length=1, name='embedding_matrix_1',\r\n                      trainable=False)\r\n```", "comments": ["I think the api has been changed.  Please visit\r\nhttps://keras.io/layers/embeddings/ \r\nfor more details on Embedding constructor.\r\n\r\n ", "Hi,\r\n\r\nSo what is the new syntax to load pretrained word vectors now that the weights parameter is deprecated?\r\n\r\nDo you know @bignamehyp ?\r\n\r\nDo we use `embeddings_initializer=Constant` instead @hsm207 ?\r\n\r\nThanks for the help!", "Recently I also noted that the weights parameter is deprecated,so what is new method to use pretrained weight in embedding? ", "@BoPengGit @Huijun-Cui I think setting `embedding_initializer` to the pretrained word vectors and setting that embedding layer's `trainable` property to `false` is the way to go.", "@hsm207 but how? can you provide a working example? embedding_initializer can accept custom initializers, but I do not see a way to give weights ...", "@farmeh are you using tensorflow 2.0?", "@hsm207 no, keras 2.2.4 and tensorflow 1.14.0.", "To fix the issue you must do something like this @hsm207 @farmeh \r\n\r\n```\r\n  from keras.initializers import Constant\r\n  embedding_layer = Embedding(input_len, input_size, \r\n                              embeddings_initializer=Constant(your_embedded_matrix), trainable=False)\r\n```"]}, {"number": 14391, "title": "MonitoredTrainingSession does not initialize after restore", "body": "### Describe the problem\r\n`local_init_op` can be passed to the SessionManager through the scaffold argument in MonitoredTrainingSession. From the doc's from session manager: The `local_init_op` is an `Operation` that is run always after a new session was created. This does not work as expected in the below example.\r\n\r\n### Exact command to reproduce\r\n```\r\nglobal_step = tf.contrib.framework.get_or_create_global_step()\r\na1 = tf.Variable([1,2,3], name='a1')\r\na2 = tf.Variable([1,2,3], name='a2')\r\ntrain = tf.assign(global_step, global_step +1)\r\nsaver = tf.train.Saver(var_list=[a1],\r\n                        reshape = False,\r\n                        sharded = False,\r\n                        max_to_keep = 100,\r\n                        keep_checkpoint_every_n_hours = 10000.0,\r\n                        name = \"CheckpointSaver\",\r\n                        restore_sequentially = False,\r\n                        saver_def = None,\r\n                        builder = None,\r\n                        defer_build = False,\r\n                        allow_empty = False,\r\n                        write_version = tf.train.SaverDef.V2,\r\n                        pad_step_number = False,\r\n                        save_relative_paths = False)\r\nhooks = [tf.train.CheckpointSaverHook(checkpoint_dir = \"ckpt/\",\r\n                         save_secs = None,\r\n                         save_steps = 1,\r\n                         saver = saver,\r\n                         checkpoint_basename = 'model_to_test.ckpt',\r\n                         scaffold = None,\r\n                         listeners = None)]\r\nscaffold = tf.train.Scaffold(saver=saver, local_init_op = tf.variables_initializer([a2]))\r\nwith tf.train.MonitoredTrainingSession(master = '',\r\n                         is_chief = True,\r\n                         checkpoint_dir = \"ckpt/\",\r\n                         scaffold=scaffold,\r\n                         hooks = hooks,\r\n                         chief_only_hooks = [],\r\n                         save_checkpoint_secs=None,\r\n                         save_summaries_steps=None,\r\n                         save_summaries_secs=None,\r\n                         config=None,\r\n                         stop_grace_period_secs=120,\r\n                         log_step_count_steps=100) as mon_sess:\r\n    print(mon_sess.run(a1))\r\n    print(mon_sess.run(a2))\r\n    mon_sess.run(train)\r\n```\r\nThe first time you run this, two variables `a1` and `a2` will be initialized by the implied (default) initializer from the `MonitoredTrainingSession` and a checkpoint file will be written to disk for only a1; expected behavior, no errors. The second time you run this, it should load a1 from the previous checkpoint and initialize `a2` through the `local_init_op` given through the scaffold. But it doesn't, instead: \r\n\r\n> RuntimeError: Init operations did not make model ready for local_init.  Init op: group_deps, init fn: None, error: Variables not initialized: global_step, a2\r\n\r\nA hack that circumvents the problem by not using `local_init_op` is suggested here (as well as a reiteration of the expected behavior):\r\nhttps://stackoverflow.com/questions/43336553/how-to-use-tf-train-monitoredtrainingsession-to-restore-only-certain-variables\r\n\r\n\r\n\r\n\r\n### System information\r\n- **TensorFlow version (use command below)**:\r\n('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n- **Python version**: \r\n2.7.10\r\n\r\n", "comments": ["Never mind, found it, see here:\r\nhttps://stackoverflow.com/questions/43336553/how-to-use-tf-train-monitoredtrainingsession-to-restore-only-certain-variables", "I read the source code then figure it out how tf.train.MonitoredTrainingSession works.\r\nThe key point is you should use a new empty checkpoint directory, otherwise the 'init_op' or other ops won't work.\r\nThe complete answer is here:\r\nhttps://stackoverflow.com/questions/48017748/in-tensorflow-when-graph-is-modified-how-to-use-monitoredtrainingsession-to-r/48071975#48071975"]}, {"number": 14390, "title": "`variable_scope` use `auxiliary_name_scope` to control whether to create new name scope", "body": "Now `variable_scope` always create a new name scope (side-effect?) when invoked. The behavior will result in name scope collision mentioned in #13429 . Hence we proposed to add a parameter `auxiliary_name_scope` to control whether to create new name scope or not.\r\n\r\nIf accepted, we can reenter `variable_scope` and its original name scope without side-effect:\r\n```python\r\nwith tf.variable_scope(s, auxiliary_name_scope=False) as ss:\r\n     with tf.name_scope(ss.original_name_scope) as n:\r\n          # do something\r\n```\r\n### How to test\r\n\r\n+ [x] add unit tests\r\n+ [ ] pass all tests.", "comments": ["Can one of the admins verify this patch?", "Since the PR requires API review, cc @lukaszkaiser @martinwicke ", "The code is good and I think this addresses a bunch of needs in probably the simplest possible way, thanks! Still, it's an API change, need to wait for Martin (and maybe others) to decide.", "~I revise `auxiliary_name_scope` to support name scope argument, and summary as below:~\r\n+ ~`auxiliary_name_scope=True`, create new name scope as before by default.~\r\n+ ~`auxiliary_name_scope=None` or `False`, don't create any name scope.~\r\n+ ~`auxiliary_name_scope=some_name_scope`, reuse it if `some_name_scope`  is `name_scope`.~\r\n\r\nNow we can write one line code to reuse both variable_scope and its name_scope:\r\n\r\n```python\r\nwith tf.variable_scope(s, auxiliary_name_scope=s.original_name_scope) as ss:\r\n    # do something\r\n```\r\nor\r\n```python\r\nwith tf.variable_scope(s, auxiliary_name_scope=None) as ss:\r\n     with tf.name_scope(ss.original_name_scope) as n:\r\n          # do something\r\n```", "@lukaszkaiser Might you take a look again? Thanks.", "Thanks. I agree with you that the commits later introduce too much complexity. I'd like to revert it to the first version which only supports boolean. And if name scope is requested in the future, we could add the function easily later.", "Yes, that'd be great.", "I have reverted codes to boolean version:\r\n+ `auxiliary_name_scope=True`, create new name scope as before by default.\r\n+ `auxiliary_name_scope=False`, don't create any name scope.", "Jenkins, test this please.", "Github failure?\r\n\r\n```bash\r\nBuild was aborted\r\nAborted by unknown\r\nUnable to get pull request builder trigger!!\r\n```", "unrelated failure?\r\n`Invocation: Tool Failed`", "Sanity Checks failed again. git-fetch seems refused by Github, right?\r\n\r\n```bash\r\n[EnvInject] - Loading node environment variables.\r\nBuilding remotely on cpu33-slave (slave cpu-slave) in workspace /var/lib/jenkins/workspace/tensorflow-pull-requests-sanity\r\n > git rev-parse --is-inside-work-tree # timeout=10\r\nFetching changes from the remote Git repository\r\n > git config remote.origin.url https://github.com/tensorflow/tensorflow.git # timeout=10\r\nFetching upstream changes from https://github.com/tensorflow/tensorflow.git\r\n > git --version # timeout=10\r\n > git fetch --tags --progress https://github.com/tensorflow/tensorflow.git +refs/pull/*:refs/remotes/origin/pr/*\r\nBuild was aborted\r\nAborted by unknown\r\nUnable to get pull request builder trigger!!\r\nSetting status of a1faeb5123de7a63e8a432c818092d7b7da56702 to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/10748/ and message: 'FAILURE\r\n```", "Hi, @jhseu. Could you retest it? Thanks", "Jenkins, test this please.", "(API review: good to go)", "Thanks very much for your review and help, @martinwicke @lukaszkaiser @yifeif ."]}, {"number": 14389, "title": "Forward declare condition_variable, fix for #14388", "body": "Necessary to enable friendship with mutex on msvc", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed the cla", "CLAs look good, thanks!\n\n<!-- ok -->"]}]