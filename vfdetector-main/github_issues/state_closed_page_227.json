[{"number": 47764, "title": "validate_indices is deprecated and will be removed in a future version. Instructions for updating: The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.?", "body": "I was trying to train a classification model, this is my model\r\n\r\n```\r\ntext_inputs = tf.keras.layers.Input(shape=(), dtype=tf.string)\r\npreprocessor = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\r\ntokenize = hub.KerasLayer(preprocessor.tokenize)\r\ntokenized_inputs = [tokenize(text_inputs)]\r\nseq_length = 512\r\nbert_pack_inputs = hub.KerasLayer(\r\n    preprocessor.bert_pack_inputs,\r\n    arguments=dict(seq_length=seq_length))\r\nencoder_inputs = bert_pack_inputs(tokenized_inputs)\r\nencoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1\", trainable=True)\r\nencoder_outputs = encoder(encoder_inputs)['pooled_output']\r\noutput_layer = tf.keras.layers.Dense(1, activation='sigmoid')(encoder_outputs)\r\n```\r\n\r\n\r\n```\r\nmodel = tf.keras.Model(inputs=[text_inputs], outputs=[output_layer])\r\nmodel.summary()\r\n\r\n\r\n# class weights\r\ntarget_labels = y_train.tolist()\r\nclass_weights = compute_class_weight(\r\n    \"balanced\", classes=np.unique(target_labels), y=target_labels\r\n)\r\nclass_weights = dict(zip(np.unique(target_labels), class_weights))\r\n\r\nmodel.compile(tf.keras.optimizers.Adam(3e-05, epsilon=1e-08, clipnorm=1.0), loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"acc\"])\r\n\r\nmodel.fit(x_train, y_train, batch_size=64, validation_data=(x_valid, y_valid), epochs=10, callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)], class_weight=class_weights)\r\n```\r\n\r\n\r\n```\r\nWARNING:tensorflow:From /home/intellectfaces/Desktop/Work/apps/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\r\nWARNING:tensorflow:From /home/intellectfaces/Desktop/Work/apps/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\r\nEpoch 1/10\r\n 740/7930 [=>............................] - ETA: 32:56 - loss: 0.6752 - acc: 0.5807\r\n\r\n```\r\n\r\nWhat does these warnings mean? It only appears when I use class_weights option in model.fit, if i remove that argument no warnings. Does this warning cause any issue with what my model is learning, am I not suppose to use class_weights anymore?\r\n\r\nPlease provide some assistance.\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5 nightly gpu\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: 24GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@John-8704,\r\nOn running the given code snippet, I am facing an error stating `KeyError: 'CaseFoldUTF8'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f5086cbfc1fa33f014fb61053c69bbb5/47764.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here?\r\n\r\nAlso, if you are not facing any errors you can suppress the warning logs by adding the below code snippet before importing TensorFlow\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \r\nimport tensorflow as tf\r\n.....\r\n```\r\n Thanks!", "> @John-8704,\r\n> On running the given code snippet, I am facing an error stating `KeyError: 'CaseFoldUTF8'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f5086cbfc1fa33f014fb61053c69bbb5/47764.ipynb).\r\n> \r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here?\r\n> \r\n> Also, if you are not facing any errors you can suppress the warning logs by adding the below code snippet before importing TensorFlow\r\n> \r\n> ```\r\n> import os\r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \r\n> import tensorflow as tf\r\n> .....\r\n> ```\r\n> \r\n> Thanks!\r\n\r\n@amahendrakar To solve the KeyError: 'CaseFoldUTF8', please install and import tensorflow_text library because it is required to use preprocess layer. Make sure to import tensorflow_text before tensorflow.\r\n\r\nAnd actually I am not supressing the warnings, I want to understand why it's throwing those warnings when I try to run it with class_weights.", "@John-8704,\r\nThank you for the update. After importing `tensorflow-text`, I am facing an error stating `NameError: name 'y_train' is not defined`. \r\n\r\nCould you please provide the dataset you are using, so that we can reproduce the issue on our end. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47764\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47764\">No</a>\n"]}, {"number": 47763, "title": "Clarify behavior of array_to_img(scale=True)", "body": "I was confused why my images looked wrong (e.g., `apply_brightness_shift` does nothing?!). I had interpreted `scale=True` to mean \"multiply by 255 to go from `[0,1]` format to `[0,255]` format\". This is _not_ what it does. I've copied the more precise docs from https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/utils.py#L271-L272\r\n\r\n(I've also made the `x` doc more precise - it does not just accept a Numpy array; e.g. it accepts tensors)", "comments": []}, {"number": 47762, "title": "Fix build failure undefined reference to hwloc_linuxio_component on aarch64", "body": "This is the same fix as previously in the file but for aarch64 instead of ppc64le, for the same reason.", "comments": []}, {"number": 47761, "title": "Critical loss of accuracy when converting from MobileNet V2 model to tflite", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 1.13.1 (and 2.0.0)\r\n\r\n\r\n\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n- Model produces wrong results and/or has lesser accuracy.\r\n\r\nI have retrained a Mobilenet_V3_1.0 model using Deeplabcut. When I convert the model to tflite, the accuracy drops to <0.01 whereas the accuracy was previously >0.9. If I do the same, but retrain a resnet_50 model, the tflite conversion works correctly, giving pretty much the same accuracy as the original.\r\n\r\nI also found that the accuracy of the tflite model is inversely proportional to the accuracy (or training iteration) of the original retrained model. So the more training iterations and the better the accuracy of the model, the worse the accuracy of the tflite model.\r\n\r\nHere's the code I used to convert the graph (which works fine on a model retrained from a resnet base)\r\n\r\nvideo =<path to video>\r\nmodel_file<path to .pb model file>\r\n\r\ncap = cv2.VideoCapture(video)\r\nret, frame = cap.read()\r\nwith tf.io.gfile.GFile(model_file, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n\ttf.import_graph_def(graph_def, name=\"DLC\")\r\n\r\ngraph.finalize()\r\nop_names = [str(op.name) for op in graph.get_operations()]\r\n\r\noutput_nodes = [op_names[-1], op_names[-2]]\r\noutput_nodes = [on.replace(\"DLC/\", \"\") for on in output_nodes]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n                model_file,\r\n                [\"Placeholder\"],\r\n                output_nodes,\r\n                input_shapes={\"Placeholder\": [1, frame.shape[0], frame.shape[1], 3]},\r\n            )\r\ntflite_model = converter.convert()\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\nAnd then for calling the tflite model: \r\ntflite_interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ntflite_interpreter.allocate_tensors()\r\ninputs = tflite_interpreter.get_input_details()\r\noutputs = tflite_interpreter.get_output_details()\r\n\r\nret, frame = cap.read()\r\nim = np.expand_dims(frame, axis=0).astype(np.float32)\r\ntflite_interpreter.set_tensor(\r\n\t\t        inputs[0][\"index\"],\r\n\t\t        im\r\n\t\t    )\r\ntflite_interpreter.invoke()\r\n\r\noutputs_np = [\r\n\t\t           tflite_interpreter.get_tensor(outputs[0][\"index\"]),\r\n\t\t           tflite_interpreter.get_tensor(outputs[1][\"index\"]),\r\n\t\t        ]\r\n\r\nIn short - I'm not sure why the conversion would work fine for a retrained resnet model and not a mobilenet model? Any ideas?\r\nThanks!", "comments": ["Could you try your TFLite conversion script from the recent TF version? FYI, you can bring your TF v1 model file from the newer TFLiteConverter.", "The most recent version of TFLiteConverter doesn't have a \"from_frozen_graph\" option...", "Actually, you can find the frozen graph option under the `tf.compat.v1.lite.TFLiteConverter`.", "Hi, unfortunately I still have the same trouble. The code (conversion etc) works well for my model when trained using a resnet backbone, but not when I use mobilenet. The error also increase the more training iterations I complete.", "It looks like you're doing classification with video stream if I read the code correctly, any reason not doing this with the static image?\r\nOne rough idea I have is that accuracy>0.9(or >0.99 based on your other issue: https://github.com/DeepLabCut/DeepLabCut-live/issues/43) sounds awesome, but there's not enough resource on how did you train and evaluate.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47761\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47761\">No</a>\n"]}, {"number": 47760, "title": "Slower and bigger model after int8 quantization", "body": "@tensorflow/micro\r\n\r\n**Describe the problem**\r\nI'm running the person_detection example in tensorflow/lite/micro.\r\n\r\nWhen comparing the old uint8 and the new int8 models, the int8 model requires more space as it's using both SOFTMAX and RESHAPE ops. It also seems to be slower than the old uint8 model.\r\n\r\nWhat is the advantage of using the new int8 model?", "comments": ["@johanforslund \r\nThis depends on the model and its implementation, the new models reduce latency and memory usage by hardware.\r\n\r\nKindly open a stackoverflow issue for this as it is not a bug or feature request, Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. \r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@johanforslund Did you run the helloworld and micro speech from micro example before? Can I ask what the code size it is? like the size of binary file, .hex file.... ", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47759, "title": "Cuda version should be 11 instead of 10 in Install CUDA with apt(docs)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu16.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nJust a minor change in documentation in [Install CUDA with apt](https://www.tensorflow.org/install/gpu#install_cuda_with_apt) where CUDA 10.0 needs to be updated to CUDA 11.0.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@Arghya999,\r\nThank you for reporting the bug. \r\n\r\nThe issue is already being tracked in an internal CL. The documentation will be updated once the CL is approved.", "@Arghya999,\r\nLooks like the issue is fixed, the version numbers have been updated in the guide. \r\n\r\nCould you please confirm if we can close this issue. Thanks!", "Confirm the fix.Thanks."]}, {"number": 47758, "title": "estimator guide page  doesn't work!", "body": "![image](https://user-images.githubusercontent.com/45354219/110937207-62fb9800-8375-11eb-8495-01b475df1007.png)\r\nhttps://www.tensorflow.org/guide/estimators\r\n\r\n\ud83d\ude21", "comments": ["Can you try accessing with https://www.tensorflow.org/guide/estimator ?\r\nAlso if you can point me to the webpage where you found the dead link, it will be great and help me to fix it. Thanks!"]}, {"number": 47757, "title": "Getting issues while evaluting the object detection model on tensorflow 2.O", "body": "Greetings,\r\n\r\nI am trying TF-OD API for object detection. I am getting issues while evaluating the model by given methods in the documentation. It always showing out of memory. Even I tried on the colab. How should I evaluate the model.\r\n\r\nTensorflow Version- 2.4.1\r\nCUDA:- True with 2 GB graphics\r\nPython Version- 3.8 Ananconda\r\nPlatform- Windows 10\r\n\r\n## URL(s) with the issue:\r\nhttps://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#evaluating-the-model-optional\r\n\r\n\r\nThanks for helping out.", "comments": ["@codePerfectPlus \r\nPlease share the colab gist for us to analyse.\r\n\r\nYou may also refer to : [link](https://stackoverflow.com/questions/36927607/how-can-i-solve-ran-out-of-gpu-memory-in-tensorflow), #32707, [link1](https://github.com/tensorflow/tensorflow/issues/40646#issuecomment-647224203)", "@Saduf2019  Thanks for helping out. Issue is fixed now.", "@codePerfectPlus \r\nThank you for your update, glad the issue is resolved."]}, {"number": 47756, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, MUL, PACK, RESHAPE, SHAPE, STRIDED_SLICE, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: Conv3D, Conv3DBackpropInputV2, MaxPool3D.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, MUL, PACK, RESHAPE, SHAPE, STRIDED_SLICE, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: Conv3D, Conv3DBackpropInputV2, MaxPool3D.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["@aliceruget,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "I recommend enabling the Select TF option to use TF ops for your model. https://www.tensorflow.org/lite/guide/ops_select", "Thank you very much for the quick response @amahendrakar and @abattery!\r\n\r\nMy TensorFlow version is 2.2.0. \r\nI can't unfortunately share the main code or the dataset @amahendrakar because of its confidentiality ... I can try to find another code that would give the same error ? \r\n\r\nI indeed enabled the TF option @abattery which erased the error on Conv3D and MaxPool3D in this way :\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nThis however kept the error on Conv3DBackpropInputV2. \r\n\r\nI can avoid this error on Conv3DBackpropInputV2 by adding converter.allow_custom_ops=True. \r\nHowever, when I run afterwards\r\ninterpreter = tf.lite.Interpreter(model_path=model_path_lite)\r\ninterpreter.allocate_tensors(), \r\nthen get the  error    \r\nFile \"keras_code_lite.py\", line 11, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/home/ar432/venv/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 242, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/ar432/venv/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 110, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: Encountered unresolved custom op: Conv3DBackpropInputV2.Node number 2 (Conv3DBackpropInputV2) failed to prepare.\r\n\r\n\r\n\r\n\r\n", "Conv3DBackpropInputV2 op is now supported at the tf-nightly version. Please consider upgrading TF version.", "Thanks, using tf-nightly solved my issue! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47756\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47756\">No</a>\n"]}, {"number": 47755, "title": "Cached augmentation in segmentation tutorial - this does not increase dataset size", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/images/segmentation\r\n\r\n## Description of issue (what needs changing):\r\n\r\nAugmentation, to increase the size of the dataset, has to convert one source datapoint into many augmented datapoints. But in this tutorial, augmentation is applied once to each datapoint - effectively keeping the dataset size the same. The root cause is that augmentation is applied before a `Dataset.cache()`.\r\n\r\nThese are the relevant lines of the tutorial:\r\n\r\n```python\r\n@tf.function\r\ndef load_image_train(datapoint):\r\n  input_image = tf.image.resize(datapoint['image'], (128, 128))\r\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\r\n\r\n  # This bit is random augmentation - mixed into the load function\r\n  if tf.random.uniform(()) > 0.5:\r\n    input_image = tf.image.flip_left_right(input_image)\r\n    input_mask = tf.image.flip_left_right(input_mask)\r\n\r\n  input_image, input_mask = normalize(input_image, input_mask)\r\n\r\n  return input_image, input_mask\r\n\r\n# Here we load the dataset, including augmentation\r\ntrain = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\r\n\r\n# Then we cache that single round of augmentation, and repeat that single round forever\r\ntrain_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\r\n```\r\n\r\nI believe this should look more like (untested):\r\n\r\n```python\r\n\r\n@tf.function\r\ndef load_image_train(datapoint):\r\n  input_image = tf.image.resize(datapoint['image'], (128, 128))\r\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\r\n\r\n  input_image, input_mask = normalize(input_image, input_mask)\r\n\r\n  return input_image, input_mask\r\n\r\n# We can cache here, because the cached dataset is deterministic\r\ntrain = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE).cache()\r\n\r\n@tf.function\r\ndef random_augment(input_image, input_mask):\r\n  if tf.random.uniform(()) > 0.5:\r\n    input_image = tf.image.flip_left_right(input_image)\r\n    input_mask = tf.image.flip_left_right(input_mask)\r\n  return input_image, input_mask\r\n\r\n# Now apply augmentation after caching, getting different results each time\r\ntrain_dataset = train.map(random_augment).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\r\n```", "comments": ["(Happy to contribute change to tutorial, if we agree that this is a problem)", "@jameshfisher I think the tutorial should probably mention the advantages and disadvantages. Image augmentation quickly becomes the bottleneck, and depending on your goals both ways can be useful.\r\n", "@RobertBiehl what are the advantages of the current approach (cached augmentation) vs no augmentation? As far as I can see, there aren't any, but I might be missing something ...?", "FYI, [this post was popular a few days ago](https://news.ycombinator.com/item?id=26767441), about what seems to be an identical issue with PyTorch tutorials.", "Thanks @jameshfisher, your suggestion is much better. Caching right before augmentation will give most of the benefits of caching, while still allowing augmentation to be re-applied each epoch, so that the model gets to train on a wider variety of inputs. If you're still interested in submitting a change, I'd be happy to review.", "> @RobertBiehl what are the advantages of the current approach (cached augmentation) vs no augmentation? As far as I can see, there aren't any, but I might be missing something ...?\r\n\r\nI guess it is an edge case. E.g. if for some reason the fact that you augment changes the input data distribution in a way you want, and for performance reasons you accept the fact that the augmentation is frozen for the rest of the training. (e.g. if the reason for augmentation is not just _producing_ more training data but changing the input in some way).\r\n\r\nFor the tutorial you suggestion definitely makes sense.", "Thank you for the awesome feedback @jameshfisher @RobertBiehl @aaudiber \ud83d\udc4d We'll check this out cc @MarkDaoust "]}, {"number": 47754, "title": "Support tflite kernel unittest in cmake.", "body": "We don't have the kernel test case with cmake build.\r\nThis pr try to port the bazel setting into cmake for the kernel tests.\r\n\r\nHere are the output message for the tests:\r\n```\r\n/tensorflow/tensorflow/lite/build$ ctest\r\n\r\nTest project /tensorflow/lite/build\r\n        Start   1: internal-averagepool_quantized_test\r\n  1/137 Test   #1: internal-averagepool_quantized_test ......................   Passed    0.18 sec\r\n        Start   2: internal-batch_to_space_nd_test\r\n  2/137 Test   #2: internal-batch_to_space_nd_test ..........................   Passed    0.00 sec\r\n        Start   3: internal-conv_per_channel_quantized_16x8_test\r\n  3/137 Test   #3: internal-conv_per_channel_quantized_16x8_test ............   Passed    1.24 sec\r\n  ....\r\n        Start 137: test_util_test\r\n137/137 Test #137: test_util_test ...........................................   Passed    0.00 sec\r\n\r\n100% tests passed, 0 tests failed out of 137\r\n```\r\n", "comments": ["@terryheo @mihaimaruseac \r\nI update the UpdateOpVersion() interface in this pr. You guys might be worried about it.\r\nI show the the detail reason and the implementation in the comment.\r\nPlease check:\r\nhttps://github.com/tensorflow/tensorflow/pull/47754/commits/31f79ad804cccc071b030a3950106558edd8b30e\r\nhttps://github.com/tensorflow/tensorflow/pull/47754/commits/582bf0d3ac33fc10156f737c0d42f3adee54409a", "@terryheo \r\nCould you please check this commit for the mutable schema interface?\r\nhttps://github.com/tensorflow/tensorflow/pull/47754/commits/bb088b080a72e790c18e8164ce9fb828b2cec5e4\r\nIt builds the host-side flatc using cmake ExternalProject_Add(). Then, we could use the flatc to generate the mutable schema interface.", "@terryheo \r\nAll review comments are addressed.\r\nDo I need to squash the fixing commits?", "> Do I need to squash the fixing commits?\r\n\r\nNo, it's fine. This PR will be merged as a single commit with history.", "Really appreciate the contribution!"]}, {"number": 47753, "title": "I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll", "body": "I run my code on pycharm\r\ninclude :  import tensorflow as tf\r\n\r\nIn the run window show me the red line: \r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n\r\ntf version: tensorflow-2.4.0rc4-cp37-cp37m-win_amd64.whl\r\n\r\nHow can I fix it\r\n", "comments": ["@6368696a6961,\r\nThe messages are just information logs stating that TensorFlow has successfully detected the GPU on your machine. You can safely ignore them.\r\n\r\nYou can also disable them by changing the log level using the below code snippet\r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\n\r\nPlease find the gist of the same [here](https://colab.research.google.com/gist/amahendrakar/2d37b3975927274613d098cb1f6428cc/47753.ipynb). \r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @6368696a6961,\r\n> The messages are just information logs stating that TensorFlow has successfully detected the GPU on your machine. You can safely ignore them.\r\n> \r\n> You can also disable them by changing the log level using the below code snippet\r\n> \r\n> ```\r\n> import os\r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\n> import tensorflow as tf\r\n> ```\r\n> \r\n> Please find the gist of the same [here](https://colab.research.google.com/gist/amahendrakar/2d37b3975927274613d098cb1f6428cc/47753.ipynb).\r\n> \r\n> Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47753\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47753\">No</a>\n"]}, {"number": 47752, "title": "Tensorflow Random segmentation faults", "body": "I am trying to run the demo code from official tensorflow [website][1]\r\nI am attaching the full code (copied and arranged) here for ease\r\n```\r\nimport tensorflow as tf\r\n\r\n# print(\"1\")\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\nimport time\r\nimport os\r\n\r\n# print(\"2\")\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\n\r\n\r\n# @tf.function\r\ndef train_step(x, y):\r\n    with tf.GradientTape() as tape:\r\n        logits = model(x, training=True)\r\n        loss_value = loss_fn(y, logits)\r\n    grads = tape.gradient(loss_value, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n    train_acc_metric.update_state(y, logits)\r\n    return loss_value\r\n\r\n\r\n# @tf.function\r\ndef test_step(x, y):\r\n    val_logits = model(x, training=False)\r\n    val_acc_metric.update_state(y, val_logits)\r\n\r\n\r\ninputs = keras.Input(shape=(784,), name=\"digits\")\r\nx1 = layers.Dense(64, activation=\"relu\")(inputs)\r\nx2 = layers.Dense(64, activation=\"relu\")(x1)\r\noutputs = layers.Dense(10, name=\"predictions\")(x2)\r\nmodel = keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n# Instantiate an optimizer.\r\noptimizer = keras.optimizers.SGD(learning_rate=1e-3)\r\n# Instantiate a loss function.\r\nloss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\ntrain_acc_metric = keras.metrics.SparseCategoricalAccuracy()\r\nval_acc_metric = keras.metrics.SparseCategoricalAccuracy()\r\n# Prepare the training dataset.\r\nbatch_size = 64\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\nx_train = np.reshape(x_train, (-1, 784))\r\nx_test = np.reshape(x_test, (-1, 784))\r\n\r\n# Reserve 10,000 samples for validation.\r\nx_val = x_train[-10000:]\r\ny_val = y_train[-10000:]\r\nx_train = x_train[:-10000]\r\ny_train = y_train[:-10000]\r\n\r\n# Prepare the training dataset.\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\r\n\r\n# Prepare the validation dataset.\r\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\r\nval_dataset = val_dataset.batch(batch_size)\r\n\r\nepochs = 2\r\nfor epoch in range(epochs):\r\n    print(\"\\nStart of epoch %d\" % (epoch,))\r\n    start_time = time.time()\r\n\r\n    # Iterate over the batches of the dataset.\r\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\r\n        loss_value = train_step(x_batch_train, y_batch_train)\r\n\r\n        # Log every 200 batches.\r\n        if step % 200 == 0:\r\n            print(\r\n                \"Training loss (for one batch) at step %d: %.4f\"\r\n                % (step, float(loss_value))\r\n            )\r\n            print(\"Seen so far: %d samples\" % ((step + 1) * 64))\r\n\r\n    # Display metrics at the end of each epoch.\r\n    train_acc = train_acc_metric.result()\r\n    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\r\n\r\n    # Reset training metrics at the end of each epoch\r\n    train_acc_metric.reset_states()\r\n\r\n    # Run a validation loop at the end of each epoch.\r\n    for x_batch_val, y_batch_val in val_dataset:\r\n        test_step(x_batch_val, y_batch_val)\r\n\r\n    val_acc = val_acc_metric.result()\r\n    val_acc_metric.reset_states()\r\n    print(\"Validation acc: %.4f\" % (float(val_acc),))\r\n    print(\"Time taken: %.2fs\" % (time.time() - start_time))\r\n    print(\"end\")\r\n```\r\nWithout any reason, this code enters Segmentation Fault in Tensorflow 2.3.1 right at the beginning\r\n```\r\n>python dummy.py \r\n2021-03-11 17:45:52.231509: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nSegmentation fault (core dumped)\r\n```\r\nInterestingly if I put some random print statements at the very start(those ```print(\"1\")``` etc statements, the code will execute till the end and suffer segmentation fault at the end(redundant output not shown)\r\n```\r\nStart of epoch 1\r\nTraining loss (for one batch) at step 0: 1.0215\r\nSeen so far: 64 samples\r\nTraining loss (for one batch) at step 200: 0.9116\r\nSeen so far: 12864 samples\r\nTraining loss (for one batch) at step 400: 0.4894\r\nSeen so far: 25664 samples\r\nTraining loss (for one batch) at step 600: 0.5636\r\nSeen so far: 38464 samples\r\nTraining acc over epoch: 0.8416\r\nValidation acc: 0.8296\r\nTime taken: 3.16s\r\nend\r\nSegmentation fault (core dumped)\r\n```\r\nAnother observation is, if I uncomment the `@tf.function` on top of my `trainStep` and `testStep` functions, the code enters into segfault again but after it prints\r\n```Start of epoch 0```\r\n\r\nSystem Specs - \r\nOS - Ubuntu 14.04\r\nCPU - Intel Core I7\r\nRAM - 64GB\r\nCUDA version - 10.1\r\nCUDNN - 7.6\r\nTF version - 2.3.1(installed via ```pip```)\r\nVRAM - 12GB\r\nGPU - NVIDIA GTX1080TI\r\n\r\nUsing conda environment with python3.6/python3.7(tried both separately)\r\nEven tried python virtualenv and still got the issue\r\n\r\nI thought it might be some issue with my TF2.3 version. Hence I tried everything from TF2.0.0 till TF2.3.2 (CUDA 11 is not supported on Ubuntu 14 and hence can't work with TF2.4.1). I either enter ```Segmentation Fault``` at some place or the other. If not that I get this error.\r\n\r\n```*** Error in `python': double free or corruption (!prev): 0x000000000167c080 ***```\r\nI even got libtcmalloc-minimal4 which was suggested for double free error on another thread.\r\n\r\n### The code only worked for TF2.0.x without any issue. But I can't use this, as 2.0.x does not support RaggedTensor backprop as mentioned in [this](https://github.com/tensorflow/tensorflow/issues/35802) thread.\r\n`\r\nCan someone explain what is going wrong with my Tensorflow package?\r\n\r\n\r\n\r\n\r\n  [1]: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch", "comments": ["@ajinkyaambatwar,\r\nTensorFlow pip packages require Ubuntu 16.04 or later (64-bit). For more information, please take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\nCould you please update Ubuntu and let us know if you are still facing the same issue. Thanks!", "Sure @amahendrakar , I will try and update you regarding the issue!", "Hi @amahendrakar , sorry for delayed response. After upgrading to Ubuntu 18, the issue didn't appear!\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47752\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47752\">No</a>\n"]}, {"number": 47751, "title": "Any way to train a TF1 graph in TF2 without using tf.compat.v1.Session?", "body": "Hello, hope y\u2019all are staying safe and healthy! Currently my group is migrating from TF 1.8 to TF 2.4.1. Some of our work is dependent on `tensorflow_gan` back when it used to be under `models/research`. We would take the initial graph generated and then train it with our own framework. The files are:\r\n\r\n```\r\ncheckpoint\r\nmodel.ckpt.data-00000-of-00001\r\nmodel.ckpt.meta\r\nmodel.ckpt.index\r\n```\r\n\r\nAnd below was our TF 1.8 code that would train the GAN. I apologize that due to policy, I cannot copy and paste, but this is just to have an idea:\r\n\r\n```\r\ntf.train.import_meta_graph(graph_path + \u2018.meta\u2019)\r\ngraph = tf.get_default_graph()\r\ngen_input = graph.get_tensor_by_name(\u201cinputs/gen_input:0\u201d)\r\ngen_loss = graph.get_tensor_by_name(\u201closses/truediv:0\u201d)\r\ngen_train_op =  graph.get_tensor_by_name(\u201ctrain_ops/generator_train/train_op/control_dependency:0\u201d)\r\ngen_output = graph.get_tensor_by_name(\u201cGenerator/output/add:0\u201d)\r\ndis_input = graph.get_tensor_by_name(\u201cinputs/x_placeholder:0\u201d)\r\n\r\nwith tf.Session() as sess:\r\n\twith tf.variable_scope(tf.get_variable_scope(), reuse=False):\r\n\t\tassert tf.get_variable_scope().reuse == False\r\n\ttf.train.Saver().restore(sess, restore_path)\r\n\r\n\tfor iteration in range(max_epochs):\r\n\t\treal_data, fake_data = dataset.get_next_batch()\r\n\t\t_, loss = sess.run([gen_train_op, gen_loss], feed_dict={gen_input:fake_data, dis_input:real_data})\r\n```\r\n\r\nWe would like to be as close to TF2 and use Keras as much as possible. Is there any way to wrap the tensors from `graph.get_tensor_by_name` or the whole graph into a `tf.Keras.Model`, or must we use `tf.compat.v1.Session`? We\u2019d ideally like to convert this into a `saved_model.pb` that can be imported and used like a `tf.Keras.Model` for additional training, not just inference.\r\n\r\nIf not, is it possible to combine a loaded graph with a `tf.Keras.Model` for training? For instance, having a model be a backbone to another, and then train on the whole model.\r\n\r\nThanks!\r\n\r\n\r\n", "comments": ["`get_tensor_by_name` is not something that should be used in TF2. (You should not be relying on variable or tensor names directly).\r\n\r\nvariable_scope is also not currently supported in TF2-proper as-is (outside of sessions, which are realistically TF1 just included in the TF2 binaries).\r\n\r\nYou *may* be able to put your variable scopes inside of a `tf.compat.v1.make_template` and have it work (and then you can add the make_template's variables to a Keras layer if you'd like).\r\n\r\n\r\nWe are currently exploring alternatives that will allow variable_scope code to be used more effectively from w/in Keras (purely as a form of a migration tooling), so you may be able to repurpose this test prototype (that still has debugging logs/notes attached):\r\n\r\n```\r\nimport tensorflow.python.ops.variable_scope as variable_scope\r\nimport contextlib\r\nimport re\r\n\r\n# TODO: May need to avoid writing variables to graph collection even when\r\n# Using the eager variable store. (right now it looks like they do get written)\r\n# Otherwise the variables may become global/\r\n# never get cleaned up.\r\n\r\n# Also note: This will have higher python overheads in eager than purely-object-attr-based accesses.\r\n# Should be fine in a python function though?\r\n\r\nclass _RegularizerCapturingEagerVariableStore(variable_scope._VariableStore):\r\n  def __init__(self, *args, **kwargs):\r\n    super().__init__(*args, **kwargs)\r\n    self.regularizers = {}\r\n    self._store_eager_variables = tf.compat.v1.executing_eagerly_outside_functions # Yay code archeology!\r\n    self._observed_created_variables = []\r\n\r\n  def get_variable(self, *args, **kwargs):\r\n    # print(\"Created: \", args, kwargs)\r\n    # Todo: this extraction is a bit sketchy & depends on exact api of internal _VariableStore\r\n    if len(args) > 4:\r\n      regularizer = args[4]\r\n    else:\r\n      regularizer = kwargs.get('regularizer')\r\n    var = super().get_variable(*args, **kwargs)\r\n    if regularizer is not None:\r\n      # print(\"Caught reg!: \", var.name, regularizer)\r\n      self.regularizers[var.name] = regularizer\r\n    if not (id(var) in [id(x[0]) for x in self._observed_created_variables]):\r\n      self._observed_created_variables.append((var, 'new'))\r\n    else:\r\n      self._observed_created_variables.append((var, 'reused'))\r\n\r\n    return var\r\n\r\n\r\nclass VariableCapturer(tf.Module):\r\n  def __init__(self):\r\n    self._var_store = _RegularizerCapturingEagerVariableStore()\r\n    self._variables = {}\r\n    self._observed_created_variables = []\r\n\r\n  def _variable_creator(self, next_creator, **kwargs):\r\n    var = next_creator(**kwargs)\r\n    self._variables[var.name] = var\r\n\r\n    # Weirdness: may need this due to weird tracking not triggering on dict inserts for keras layers thing? TBD since this wasn't tested on nightly.\r\n    # As of nightly modules are supposed to be tracked by layers.\r\n    self._variables = self._variables\r\n\r\n    if not (id(var) in [id(x[0]) for x in self._observed_created_variables]):\r\n      self._observed_created_variables.append((var, 'new'))\r\n    else:\r\n      self._observed_created_variables.append((var, 'reused'))\r\n    return var\r\n\r\n  @contextlib.contextmanager\r\n  def scope(self):\r\n    with tf.variable_creator_scope(self._variable_creator), variable_scope.with_variable_store(self._var_store):\r\n      yield\r\n\r\n  def get_regularization_losses(self, scope=None):\r\n    # scope regex matching taken directly from collections scope regex matching\r\n    losses = {}\r\n    if scope is None:\r\n      for var_name, regularizer in self._var_store.regularizers.items():\r\n        losses[var_name] = regularizer(self._variables[var_name])\r\n    else:\r\n      regex = re.compile(scope)\r\n      for var_name, regularizer in self._var_store.regularizers.items():\r\n        if regex.match(var_name):\r\n          losses[var_name] = regularizer(self._variables[var_name])\r\n    return losses\r\n\r\n```\r\n\r\n(We cannot currently provide advice or support around this prototype though since it is still very much a work-in-progress. It may also require the tf nightlies / tf 2.5 to be out if you want to track tf.Modules from inside keras layers/models).\r\n\r\n If as you say you'd like to be as close to TF2 and use Keras as much as possible and this is urgent, we'd suggest rewriting the core pieces of GAN functionality that you need with Keras. If you have a few months to wait then we may have a more official shim for variable_scope ready. Or you could always see if compat.v1.make_template works.\r\n"]}, {"number": 47750, "title": "Dataset Iterator fails with exception: InvalidArgumentError: Requires delta != 0: 0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: GTX 980M\r\n\r\n**Describe the current behavior**\r\nDataset iterator fails intermittently for an unknown reason, with error InvalidArgumentError: Requires delta != 0: 0. Sometimes the error will happen after thousands of iterations, other times the error will happen immediately.\r\n\r\n**Describe the expected behavior**\r\nThe Dataset Iterator should succeed, or at least provide a useful error message describing what the issue is. I have no idea what delta is or why it is 0.\r\n\r\n**Standalone code to reproduce the issue**\r\nI can't provide my full pipeline for IP reasons, though this is generally it. I've basically got a bunch of files that I transform, then sample evenly from.\r\n\r\n```\r\ndef get_balanced_class_repeating_dataset(folder_list, parser):\r\n    num_files_to_shuffle = 20\r\n    # TODO: fix num_samples to use partition multiple once partitions get big enough\r\n    num_samples_to_shuffle = 200  # some multiple of the size of a partition to improve shuffling\r\n    dataset_array = []\r\n    for folder in folder_list:\r\n        file_count = len(folder_list[folder])\r\n        filename_list = tf.data.Dataset.from_tensor_slices(folder_list[folder]).shuffle(file_count).repeat(-1)\r\n        ds = filename_list.interleave(lambda filename: tf.data.TFRecordDataset(filename), cycle_length=num_files_to_shuffle)  # shuffle the files then interleave 10\r\n        ds = ds.shuffle(num_samples_to_shuffle)\r\n        ds = ds.repeat(-1)  # repeat the shuffled dataset, not the other way around\r\n        dataset_array.append(ds)\r\n    dataset = tf.data.experimental.sample_from_datasets(dataset_array, weights=None, seed=None)\r\n    dataset = dataset.map(lambda record: parser(record))\r\n    return dataset\r\n```\r\n\r\n    def get_training_dataset(self):\r\n        dataset = get_balanced_class_repeating_dataset(self.train_files, self._parse_tfrecord_train)\r\n        dataset = dataset.batch(self.batch_size)\r\n        return dataset\r\n\r\n**Other info / logs** \r\nStack Trace\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\lukeb\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 2113, in execution_mode\r\n    yield\r\n  File \"C:\\Users\\lukeb\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 730, in _next_internal\r\n    ret = gen_dataset_ops.iterator_get_next(\r\n  File \"C:\\Users\\lukeb\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 2578, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\Users\\lukeb\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Requires delta != 0: 0\r\n\t [[{{node range}}]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/health-AI/python/cardiac/v03/train_model.py\", line 31, in <module>\r\n    model_trainer.train()\r\n  File \"C:\\health-AI\\python\\cardiac\\v03\\trainers\\v1\\classifier_trainer_v1.py\", line 32, in train\r\n    for train_step, (data, labels) in enumerate(train_ds):\r\n  File \"C:\\Users\\lukeb\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 747, in __next__\r\n    return self._next_internal()\r\n  File \"C:\\Users\\lukeb\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 739, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"C:\\Users\\lukeb\\Anaconda3\\envs\\tf2\\lib\\contextlib.py\", line 131, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Users\\lukeb\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 2116, in execution_mode\r\n    executor_new.wait()\r\n  File \"C:\\Users\\lukeb\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\executor.py\", line 69, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Requires delta != 0: 0\r\n\t [[{{node range}}]]\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["@LukeBolly,\r\nIn the given code snippet, you have defined the functions but are not calling them anywhere. Without a reproducible code, it would be difficult for us to debug the issue. \r\n\r\n> I can't provide my full pipeline for IP reasons\r\n\r\nIn this case, could you please provide a dummy code to reproduce the issue reported here? Thanks!", "One of my functions uses tf.range which has a delta parameter so I think that was the cause. It would be nice if the iterator logged which op it was actually failing at though I guess that's not possible when dataset processing is compiled as a graph.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47750\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47750\">No</a>\n"]}, {"number": 47749, "title": "[determinism] Add GPU determinism for fp types in GPU SparseTensorDenseMatMul", "body": "Add GPU determinism for fp types in GPU SparseTensorDenseMatMul. The supported types are fp32, fp16 and complex64. For non-supported types including fp64 and complex128, a unimplemented exception will be  thrown if environment variable `TF_DETERMINISTIC_OPS` is set to be `1` or `true`. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47749) for more info**.\n\n<!-- need_sender_cla -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47749) for more info**.\n\n<!-- need_sender_cla -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47749) for more info**.\n\n<!-- need_sender_cla -->", "Typo in PR title: determinisim -> determinism", "@gbaned: please let me review this PR first. Then we'll move it out of WIP/draft and then it can be further reviewed, if considered necessary.\r\n\r\nTagging @sanjoy for visibility.", "@googlebot  I signed it!", "@wenscarl: This is feedback about your commit messages. The following three commits do much more than what the associated messages suggest:\r\n\r\n* [`[determinism] RequireDeterminism check moved to GPU functor`](https://github.com/tensorflow/tensorflow/pull/47749/commits/fabe465d4dbbccabf67a1e0554b758228fba6c6c)\r\n* [`[determinism] Improve indentations and exception string literals`](https://github.com/tensorflow/tensorflow/pull/47749/commits/f1913a393f0efc6c763c7bf61dd87fda01a571ff)\r\n* [`[determinism] Rename GpuRecastAtomicAdd kernel`](https://github.com/tensorflow/tensorflow/pull/47749/commits/8f53dfc3a48b736416a7162f494f6b5a1cbf838c)\r\n\r\nThe first line of a commit message should summarize what the commit does, not list one of the many things it does. In cases like this, where each commit is addressing a review step, I recommend,\r\n\r\n`[determinism] Address review, step n, on PR 47749`,\r\n\r\nreplacing `n` with the review step that is being addressed by the commit. We're currently on review step 4 for this PR; your next commit will be for step `4`.\r\n\r\nThese commit messages should be meaningful so that the list of commits in the PR make sense and also so that the commits make sense in the commit log of the master branch after the merge (assuming the commits don't get squashed).", "@wenscarl, there is no review step 0. I reviewed it four times: 1, 2, 3, 4. The first time I reviewed this PR was step 1, or pass 1, or round 1.\r\n\r\n`768a1a6` addresses review (step 1) - there is no step zero in the review.\r\n`9ec0be6` fixes a typo - your original commit message made sense\r\n`1f6f4b6` addresses review (step 2)\r\n`a1398b2` addresses review (step 3)\r\n`d11a6f8` addressees review (step 4) but it also, arguably, changes the code in a way that could be summarized in one line.", "Also, note the CI build failures. Please investigate and fix those.", "Since we just changed something fundamental, please will you also make sure to run again locally with the determinism-related-functionality of the device-under-test temporarily broken (e.g. produce a different exception message) to ensure that the tests are still actually running when determinism is enabled.", "Please remove WIP from title and change from draft to regular.", "@gbaned please will you assign to @sanjoy to also review.\r\n\r\nAlso, can all these commits be squashed together during the merge.", "@wenscarl Can you please resolve conflicts? Thanks!", "> @wenscarl Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned  The conflict is resolved. Please move  forward. Thanks!", "I only skimmed this PR so far, but it appears it achieves determinism by summing float16 and float32 values with a float64 accumulator, using AtomicAdd. But I think this is still nondeterministic in certain cases. For example, suppose you sum the following float32 numbers but with a float64 accumulator: `1 + -1 + 2**-60`. If you compute `(1 + -1) + 2**-60`, you get `2**-60`, but if you compute `1 + (-1 + 2**-60)`, you get `0`.\r\n\r\nI'm trying to think of a way to resolve this, but I haven't thought of anything yet. @wenscarl @duncanriach @sanjoy any ideas?", "Would Kahan summation help?\n\nOn Fri, Apr 23, 2021, 3:32 PM Reed ***@***.***> wrote:\n\n> I only skimmed this PR so far, but it appears it achieves determinism by\n> summing float16 and float32 values with a float64 accumulator, using\n> AtomicAdd. But I think this is still nondeterministic in certain cases. For\n> example, suppose you sum the following float32 numbers but with a float64\n> accumulator: 1 + -1 + 2**-60. If you compute (1 + -1) + 2**-60, you get\n> 2**-60, but if you compute 1 + (-1 + 2**-60), you get 0.\n>\n> I'm trying to think of a way to resolve this, but I haven't thought of\n> anything yet. @wenscarl <https://github.com/wenscarl> @duncanriach\n> <https://github.com/duncanriach> @sanjoy <https://github.com/sanjoy> any\n> ideas?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/47749#issuecomment-825975440>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG2OGK54QUOZTE5BVRDTKHYRDANCNFSM4ZBQZ42A>\n> .\n>\n", "Related old discussion here:\n\nhttps://forums.developer.nvidia.com/t/atomicadd-kahan-summation/37155/4\n\nOn Fri, Apr 23, 2021, 3:39 PM ebrevdo ***@***.***> wrote:\n\n> Would Kahan summation help?\n>\n> On Fri, Apr 23, 2021, 3:32 PM Reed ***@***.***> wrote:\n>\n> > I only skimmed this PR so far, but it appears it achieves determinism by\n> > summing float16 and float32 values with a float64 accumulator, using\n> > AtomicAdd. But I think this is still nondeterministic in certain cases.\n> For\n> > example, suppose you sum the following float32 numbers but with a float64\n> > accumulator: 1 + -1 + 2**-60. If you compute (1 + -1) + 2**-60, you get\n> > 2**-60, but if you compute 1 + (-1 + 2**-60), you get 0.\n> >\n> > I'm trying to think of a way to resolve this, but I haven't thought of\n> > anything yet. @wenscarl <https://github.com/wenscarl> @duncanriach\n> > <https://github.com/duncanriach> @sanjoy <https://github.com/sanjoy> any\n> > ideas?\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/pull/47749#issuecomment-825975440\n> >,\n> > or unsubscribe\n> > <\n> https://github.com/notifications/unsubscribe-auth/AANWFG2OGK54QUOZTE5BVRDTKHYRDANCNFSM4ZBQZ42A\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/47749#issuecomment-825977615>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG7IGE7MDSDTYOJZG3LTKHZJZANCNFSM4ZBQZ42A>\n> .\n>\n", "That's a good suggestion, but unfortunately even Kahan summation can cause nondeterminism :(\r\n\r\nIn particular, consider summing `2**60 + 1 + 2**-60 - 1 - 2**60`. The true result is `2**-60`, which can be obtained with float32 arithmetic by rearranging the terms. But if summed without rearranging the terms, even with float64 accumulation and Kahan summation, results in `0`. The issue is Kahan summation only stores two floating-point accumulators to represent the sum, but there are three numbers with very different magnitudes in the summation.", "@reedwm. This is really good, and valid, challenge to this approach. Thank you. I didn't think of this when I [challenged it](https://github.com/tensorflow/tensorflow/issues/39751#issuecomment-664764793), and then [studied it](https://github.com/tensorflow/tensorflow/issues/39751#issuecomment-665365501), on PR [39751](https://github.com/tensorflow/tensorflow/issues/39751).\r\n\r\nI hope we can find a solution.", "@ebrevdo, thanks for the suggestion. I had to look up [Kahan summation](https://en.wikipedia.org/wiki/Kahan_summation_algorithm). To extend what @reedwm said, the challenge is not necessarily to minimize error (which is what Kahan summation attempts to do) but to get the same error no matter what the order of operations is. It seems to me that it would be tricky and/or very costly to parallelize Kahan summation in any case. In its serial form, Kahan summation would be deterministic, but so would normal floating-point summation.\r\n\r\nBTW, @wenscarl and I worked on other approaches to making this op deterministic, approaches that ensure reproducible ordering of operations, but we found those to be much slower (after a certain amount of time spent tuning them), and harder to make deadlock-safe, than this one. Hopefully we can make this one work. However, it does seem to have a fundamental flaw.", "I tried thinking of a solution, but unfortunately have not been able to think of any way to make this op deterministic on the GPU. What approaches have you and @wenscarl come up with?\r\n\r\nThe only solution I know of would be to run the op on the CPU when determinism is enabled, but this might have significant performance impacts and unfortunately would also mean we could not use most of this PR (we could keep the tests, modifying them to only run on the CPU)", "The other solutions are variations on the following algorithm, which @wenscarl came up with:\r\n\r\nWith each thread-block, each thread writes its matmul result into shared memory. Then bitonic sort (using all threads in the block) deterministically sorts the results into groups with matching output addresses. Then the right-most thread (the accumulator thread) in each group serially (and therefore deterministically) reduces the results for the group and uses CUDA atomicAdd to write the result out to memory. All the accumulator threads in the same thread-block will be, by definition, writing to different addresses and therefore there is no concern about nondeterministic ordering of those atomic additions, at least from within the same thread-block. On the other hand, multiple thread-blocks may be writing to the same address using CUDA atomicAdd. To ensure a deterministic result in this case, the accumulator threads wait for their thread-block\u2019s turn to write to memory using a GPU memory semaphore based on the thread-block ID.\r\n\r\nVariations include inter-thread-block synchronization mechanism details, whether to use atomics, and different approaches to intra-thread-block reduction.", "Note that the approach that is currently in this PR, the approach that seems to be fatally flawed, was developed when the op only supported `tf.float32` on GPU. The addition of support for `tf.float64` and `tf.complex128` on GPU meant that this approach could not be extended to all GPU data types. An advantage of switching to another approach is that it would almost certainly extend to all data types supported on GPU.\r\n\r\nSwitching to a different approach will likely take significant extra development time, even if it's one that we've already explored.\r\n\r\nIt seems to me that defaulting to running an op on CPU is an alternative approach to throwing exceptions; I'm wondering if it's a better approach (there are pros and cons either way). I think that, for now, we should stick with the plan of throwing exceptions (rather than running on CPU) and consider revisiting the plan.", "Thoughts on the following plan, @reedwm?\r\n\r\nLeave this PR open for a few days, maybe a week, to see if a solution bubbles up for the apparently unsolvable problem with this approach.\r\n\r\nIf no way of recovering this approach arises, then we close this PR to memorialize the dead-end without painting over it.\r\n\r\nThen either I or @wenscarl will open either:\r\n\r\n1. a PR to add throwing of GPU determinism-unimplemented exceptions for this op or\r\n2. a PR to add a GPU-deterministic implementation for this op via another approach.\r\n\r\nOr possibly (1) followed by (2).", "That sounds good to me. Agreed we should not go with running on the CPU for now; we can reconsider in the future.", "In terms of an alternative approach, it might be possible to recast this op as something that looks a bit like a weighted sparse segment reduction. If so, the deterministic GPU kernel in https://github.com/tensorflow/tensorflow/pull/47974 might be useful. (Actually I already have a follow-up PR to that one that adds weights in order to support the Grad implementation).", "Okay, @reedwm. Please will you close this PR. The next step (see the [plan](https://github.com/tensorflow/tensorflow/pull/47749#issuecomment-827275294) above) is (for me) to implement exception-throwing for GPU kernels.", "Are the tests from this PR still usable? If the CPU version is deterministic, the tests can be added now, to be run on the CPU only. Otherwise, they can be added once either the CPU or GPU version has a deterministic version.", "The CPU version is deterministic, although this PR did not run the tests on CPU (because it was focused on providing GPU-determinism). The PR that adds the exception-throwing for this op will also reuse the determinism tests from this PR and run them on the CPU as well as the exception tests from this PR."]}, {"number": 47747, "title": "Fix Non max suppression float16 kernel", "body": "cc @mingxingtan ", "comments": ["I tested it for efficientdet and it worked, but looks like this PR changes existing API for v3 and v4, it might be difficult to check in this PR (due to the backward compatibility).", "> I tested it for efficientdet and it worked, but looks like this PR changes existing API for v3 and v4, it might be difficult to check in this PR (due to the backward compatibility).\r\n\r\nYes, ```T_threshold``` breaks the test case that I added. I want to know is there a better way to keep iou_threshold and boxes the same type. \r\nChanges ```.Attr(\"T_threshold: {half, float} = DT_FLOAT\")``` to ```.Attr(\"T_threshold: {half, float} = T\")``` could fix backward compatibility?", "> I tested it for efficientdet and it worked, but looks like this PR changes existing API for v3 and v4, it might be difficult to check in this PR (due to the backward compatibility).\r\n\r\nI revert v2,v3 and v4 apis, since they break backward compatibility. Maybe they should be discussed in another PR.", "@fsx950223  Can you please check @mingxingtan's comments and keep us posted ? Thanks!", "Thanks @fsx950223 \r\nThe code looks good to me. Running some internal tests now. Will try to get it in asap."]}, {"number": 47745, "title": "[INTEL MKL] Enabling TF + OneDNN in stock TF build", "body": "Refactor code to do the following:\r\n\r\n1) code that is included in stock TF build will be guarded by macro INTEL_MKL\r\n\r\n2) code that is included only when --config=mkl is used is now guarded by macro ENABLE_MKL\r\n\r\n3) when building stock TF for x86 Linux/Windows the macro INTEL_MKL will be defined, and won't be defined for other architectures.\r\n\r\n4) Added runtime env variable ENABLE_ONEDNN_OPTS that will enable oneDNN optimizations in stock TF.\r\n\r\n5) for config=mkl, ENABLE_MKL will be defined.\r\n\r\nThis PR assumes \r\n\r\na) oneDNN is upgraded to 2.1 (PR #: https://github.com/tensorflow/tensorflow/pull/47743) and \r\n\r\nb) stock TF and TF+oneDNN are using same oneDNN build (https://github.com/tensorflow/tensorflow/pull/47679)\r\n", "comments": ["@gzmkl  Can you please resolve conflicts? Thanks!\r\n", "@gbaned  The merge conflicts were related to oneDNN upgrade PR. They are gone after the upgrade PR is merged", "@gzmkl By the way, if any change requires long testing time and you prefer to do it later. Please feel free to just add a TODO instead.", "@penpornk  Thank you for the code review. Most code change suggestions are very good and I have done code change. \r\nLocal test will take a while but will try to push them to the PR branch today. ", "@gzmkl Thank you very much for the quick responses! Please take your time. :) \r\nThe oneDNN v2.1 upgrade is still not in the clear. It made one test timed out (due to long compilation time) so I submitted a [fix](https://github.com/tensorflow/tensorflow/commit/bf4516eec17a6685ef07ed823529dce140b842c2) this morning. I'll have to wait for tonight's nightly test results to make sure v2.1 will stick. The soonest this PR can be merged is tomorrow (if nothing else goes wrong).", "@penpornk I have committed changes per your suggestions. Please let me know if there is anything missing.\r\nThanks a lot. ", "@gzmkl  Can you please check @penpornk's comments and keep us posted ? Thanks!", "> @gzmkl Can you please check @penpornk's comments and keep us posted ? Thanks!\r\n\r\nYes, I just did that. Thanks!", "@gzmkl The changes from this PR have been merged in https://github.com/tensorflow/tensorflow/commit/4a24610212cb69f1bc36cfaff19af064ec35a1d6. Somehow Github doesn't mark it as merged. I'm closing this PR now. Thank you again for the PR! :)", "Hey, I'm noticing some build failures in a downstream project ([IREE](https://github.com/google/iree)) after this was merged. Full logs are [here](https://source.cloud.google.com/results/invocations/8747de95-2981-4fa3-bc1d-591a5f868a88/targets/iree%2Fgcp_ubuntu%2Fcmake-bazel%2Flinux%2Fx86-swiftshader%2Fpresubmit/log). Maybe we're using different flags/compilers to build.\r\n\r\nRelevant logs snippet:\r\n\r\n```\r\nERROR: /home/kbuilder/.cache/bazel/_bazel_kbuilder/1900d0fac5123d725c8d2e08b3f8c209/external/org_tensorflow/tensorflow/core/kernels/mkl/BUILD:299:22: C++ compilation of rule '@org_tensorflow//tensorflow/core/kernels/mkl:mkl_softmax_op' failed (Exit 1): clang failed: error executing command /usr/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 262 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox clang failed: error executing command /usr/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 262 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nIn file included from external/org_tensorflow/tensorflow/core/kernels/mkl/mkl_softmax_op.cc:22:\r\nIn file included from external/org_tensorflow/tensorflow/core/framework/numeric_op.h:19:\r\nIn file included from external/org_tensorflow/tensorflow/core/framework/op_kernel.h:24:\r\nIn file included from external/org_tensorflow/tensorflow/core/framework/allocator.h:28:\r\nIn file included from external/org_tensorflow/tensorflow/core/platform/logging.h:27:\r\nexternal/org_tensorflow/tensorflow/core/platform/default/logging.h:280:9: error: call to function 'operator<<' that is neither visible in the template definition nor found by argument-dependent lookup\r\n  (*os) << v;\r\n        ^\r\nexternal/org_tensorflow/tensorflow/core/platform/default/logging.h:339:3: note: in instantiation of function template specialization 'tensorflow::internal::MakeCheckOpValueString<dnnl::memory::format_tag>' requested here\r\n  MakeCheckOpValueString(comb.ForVar1(), v1);\r\n  ^\r\nexternal/org_tensorflow/tensorflow/core/platform/default/logging.h:384:1: note: in instantiation of function template specialization 'tensorflow::internal::MakeCheckOpString<dnnl::memory::format_tag, dnnl::memory::format_tag>' requested here\r\nTF_DEFINE_CHECK_OP_IMPL(Check_NE, !=)  // Use CHECK(x == NULL) instead.\r\n^\r\nexternal/org_tensorflow/tensorflow/core/platform/default/logging.h:357:38: note: expanded from macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n      return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext); \\\r\n                                     ^\r\nexternal/org_tensorflow/tensorflow/core/util/mkl_util.h:472:7: note: in instantiation of function template specialization 'tensorflow::internal::Check_NEImpl<dnnl::memory::format_tag, dnnl::memory::format_tag>' requested here\r\n      DCHECK_NE(format_tag, memory::format_tag::undef);\r\n      ^\r\nexternal/org_tensorflow/tensorflow/core/platform/default/logging.h:418:31: note: expanded from macro 'DCHECK_NE'\r\n#define DCHECK_NE(val1, val2) CHECK_NE(val1, val2)\r\n                              ^\r\nexternal/org_tensorflow/tensorflow/core/platform/default/logging.h:405:30: note: expanded from macro 'CHECK_NE'\r\n#define CHECK_NE(val1, val2) CHECK_OP(Check_NE, !=, val1, val2)\r\n                             ^\r\nexternal/org_tensorflow/tensorflow/core/platform/default/logging.h:401:40: note: expanded from macro 'CHECK_OP'\r\n#define CHECK_OP(name, op, val1, val2) CHECK_OP_LOG(name, op, val1, val2)\r\n                                       ^\r\nexternal/org_tensorflow/tensorflow/core/platform/default/logging.h:395:31: note: expanded from macro 'CHECK_OP_LOG'\r\n      ::tensorflow::internal::name##Impl(                      \\\r\n                              ^\r\n<scratch space>:124:1: note: expanded from here\r\nCheck_NEImpl\r\n^\r\nexternal/org_tensorflow/tensorflow/core/util/mkl_util.h:184:22: note: 'operator<<' should be declared prior to the call site or in namespace 'dnnl'\r\ninline std::ostream& operator<<(std::ostream& os,\r\n                     ^\r\n1 error generated.\r\n```\r\n\r\nThis is the problematic line:\r\nhttps://github.com/tensorflow/tensorflow/blob/a45aa6f1731cc7e89b01b0d0da199841f28213c8/tensorflow/core/util/mkl_util.h#L472\r\n\r\nThis can be fixed by either removing that line or moving this `operator<<` from `namespace tensorflow` to `namespace dnnl`: https://github.com/tensorflow/tensorflow/blob/a45aa6f1731cc7e89b01b0d0da199841f28213c8/tensorflow/core/util/mkl_util.h#L184-L194\r\n\r\nHow do you want to proceed?", "@ScottTodd Sorry about the break and thank you for proposing the fix!\r\nI'll leave this to @gzmkl and @agramesh1 to decide. I think making changes in TensorFlow is probably easier than in oneDNN (namespace `dnnl` is in an external library.)", "@penpornk thanks, we are looking at it now.", "> @ScottTodd Sorry about the break and thank you for proposing the fix!\r\n> I'll leave this to @gzmkl and @agramesh1 to decide. I think making changes in TensorFlow is probably easier than in oneDNN (namespace `dnnl` is in an external library.)\r\n\r\nChanges don't need to be made in the external library, simply putting\r\n\r\n```\r\nnamespace dnnl {\r\ninline std::ostream& operator<<(std::ostream& os,\r\n                                const memory::format_tag& tag) {\r\n  if (tag == memory::format_tag::undef) {\r\n    os << \"undef\";\r\n  } else if (tag == memory::format_tag::any) {\r\n    os << \"any\";\r\n  } else {\r\n    os << \"invalid\";\r\n  }\r\n  return os;\r\n}\r\n}  // namespace\r\n```\r\n\r\nabove `namespace tensorflow`: https://github.com/tensorflow/tensorflow/blob/a45aa6f1731cc7e89b01b0d0da199841f28213c8/tensorflow/core/util/mkl_util.h#L52-L56\r\n\r\nworks when I build locally. (I think `memory::format_tag` is inlined without being fully qualified as `dnnl::memory::format_tag`, the macros aren't compiling as expected)\r\n\r\nThat's just one solution though. Not sure what works best for your projects.", "After discussing with team and reviewing the related code in mkl_util.h,  removing the related line of code is the right choice.\r\nWe will submit an PR. ", "@ScottTodd @penpornk  A PR to fix the issue has been submitted here https://github.com/tensorflow/tensorflow/pull/48030.\r\nThanks!"]}, {"number": 47743, "title": "[Intel MKL] Upgrading to oneDNN 2.1", "body": "This PR upgrades oneDNN version to 2.1. It also includes a patch to fix 2 build errors with certain gcc versions. So [this ](https://github.com/tensorflow/tensorflow/commit/ce7ad9e58aabea67595afa0d601f7277d93ba7e9#diff-501d1399b467b9f36e528126219bea5c3c9d6b2156248f6fe44e5c33eae5f0f5) patch is also included in this PR. ", "comments": ["@penpornk yes, this error was also there last time and we could not reproduce it. Can you share the compiler version used for that build?", "@mahmoud-abuzaina It's gcc 7. It uses the `--crosstool_top=//third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010:toolchain` bazel option. I'll see if the dockerfile can reproduce this.", "@mahmoud-abuzaina Are the oneDNN unit test failures related to this PR?", "No, we expect that unit test to fail. There other pending PR to fix it. That PR got deprioritized.\r\nFor the build error, we tried with several compiler versions including gcc7 so far but for some reason we could not reproduce the issue. ", "This PR is being reverted because of a nightly build failure. I'm finding a way to reproduce this in docker. Will post commands to reproduce later.\r\n```\r\nERROR: .../mkl_dnn_v1/BUILD.bazel:124:11: C++ compilation of rule '@mkl_dnn_v1//:dnnl_single_threaded' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010-nvcc-cuda10.1/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF ... (remaining 87 argument(s) skipped)\r\nmkl_dnn_v1/src/cpu/x64/gemm/f32/jit_avx512_core_f32_copy_at_kern_autogen.cpp: In member function 'virtual void dnnl::impl::cpu::x64::jit_avx512_core_f32_copy_at_kern::generate()':\r\nmkl_dnn_v1/src/cpu/x64/gemm/f32/jit_avx512_core_f32_copy_at_kern_autogen.cpp:29:6: internal compiler error: in equal_mem_array_ref_p, at tree-ssa-scopedtables.c:429\r\n void jit_avx512_core_f32_copy_at_kern::generate() {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n0xcf338f equal_mem_array_ref_p\r\n\t/dt7-src/gcc/tree-ssa-scopedtables.c:429\r\n0xcf338f hashable_expr_equal_p\r\n\t/dt7-src/gcc/tree-ssa-scopedtables.c:466\r\n0xcf338f expr_elt_hasher::equal(expr_hash_elt* const&, expr_hash_elt* const&)\r\n\t/dt7-src/gcc/tree-ssa-scopedtables.c:916\r\n0xcf44b0 hash_table<expr_elt_hasher, xcallocator>::find_slot_with_hash(expr_hash_elt* const&, unsigned int, insert_option)\r\n\t/dt7-src/gcc/hash-table.h:889\r\n0xcf3d61 hash_table<expr_elt_hasher, xcallocator>::find_slot(expr_hash_elt* const&, insert_option)\r\n\t/dt7-src/gcc/hash-table.h:414\r\n0xcf3d61 avail_exprs_stack::lookup_avail_expr(gimple*, bool, bool)\r\n\t/dt7-src/gcc/tree-ssa-scopedtables.c:156\r\n0xc6a046 optimize_stmt\r\n\t/dt7-src/gcc/tree-ssa-dom.c:1691\r\n0xc6a046 dom_opt_dom_walker::before_dom_children(basic_block_def*)\r\n\t/dt7-src/gcc/tree-ssa-dom.c:1184\r\n0x1159872 dom_walker::walk(basic_block_def*)\r\n\t/dt7-src/gcc/domwalk.c:265\r\n0xc6a96d execute\r\n\t/dt7-src/gcc/tree-ssa-dom.c:459\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nPlease include the complete backtrace with any bug report.\r\nSee <https://gcc.gnu.org/bugs/> for instructions.\r\n```", "We are trying to reproduce the failure. Is the Linux version ubuntu 16.04?", "@mahmoud-abuzaina Thank you! Yes, it's Ubuntu 16.04."]}, {"number": 47742, "title": "Error while using tensorflow==2.4.1  #tensorflow-probability==0.12.1, for the code that works with TF1.15 and TFP 0.8.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, referring to https://github.com/zhulingchen/tfp-tutorial\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- TensorFlow version (use command below):v1.15.4-39-g3db52be 1.15.5 (code works fine with TF15)\r\n- Python version:3.6\r\n- CUDA/cuDNN version:11\r\n- GPU model and memory:12GB\r\n\r\n**Describe the current behavior**\r\nLearning TFP. the Code works fine with TF15 and TFP0.8, but error while using with TF2.4 and TFP0.9\r\n\r\nNote: same code works in TF2.4 by adding below lines\r\n`import tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()`\r\nBut would not like to use this, since I would like to use the improvements made in TF2.4, including tf.distribute.MirroredStrategy\r\n\r\n**Describe the expected behavior**\r\nThe TFP code should work with TF2.4, please guide me to make changes \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/rrklearn2020/learn_tfp.git\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n-```\r\n-------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-24-3cf063f5b2a9> in <module>\r\n    103                               validation_data=(x_test, y_test),\r\n    104                               shuffle=True,\r\n--> 105                               callbacks=callbacks)\r\n    106                 else:\r\n    107                     print('Using real-time data augmentation.')\r\n\r\n............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1097                 batch_size=batch_size,\r\n   1098                 _r=1):\r\n-> 1099               callbacks.on_train_batch_begin(step)\r\n   1100               tmp_logs = self.train_function(iterator)\r\n   1101               if data_handler.should_sync:\r\n\r\n............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_begin(self, batch, logs)\r\n    442     \"\"\"\r\n    443     if self._should_call_train_batch_hooks:\r\n--> 444       self._call_batch_hook(ModeKeys.TRAIN, 'begin', batch, logs=logs)\r\n    445 \r\n    446   def on_train_batch_end(self, batch, logs=None):\r\n\r\n............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)\r\n    292 \r\n    293     if hook == 'begin':\r\n--> 294       self._call_batch_begin_hook(mode, batch, logs)\r\n    295     elif hook == 'end':\r\n    296       self._call_batch_end_hook(mode, batch, logs)\r\n\r\n............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_begin_hook(self, mode, batch, logs)\r\n    301     \"\"\"Helper function for `on_*_batch_begin` methods.\"\"\"\r\n    302     hook_name = 'on_{mode}_batch_begin'.format(mode=mode)\r\n--> 303     self._call_batch_hook_helper(hook_name, batch, logs)\r\n    304 \r\n    305     if self._check_timing:\r\n\r\n............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs)\r\n    358         if numpy_logs is None:  # Only convert once.\r\n    359           numpy_logs = tf_utils.to_numpy_or_python_type(logs)\r\n--> 360         hook(batch, numpy_logs)\r\n    361 \r\n    362     if self._check_timing:\r\n\r\n............/tf2x_tfp_venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_begin(self, batch, logs)\r\n    690     \"\"\"\r\n    691     # For backwards compatibility.\r\n--> 692     self.on_batch_begin(batch, logs=logs)\r\n    693 \r\n    694   @doc_controls.for_subclass_implementers\r\n\r\n<ipython-input-8-f80d1df575fd> in on_batch_begin(self, batch, logs)\r\n      8     def on_batch_begin(self, batch, logs=None):\r\n      9         if self.update_per_batch:\r\n---> 10             n_batch_per_epoch = int(np.ceil(self.params['samples'] / self.params['batch_size']))\r\n     11             idx_total_batch = (self.epoch - self.n_silent_epoch) * n_batch_per_epoch + batch + 1\r\n     12             kl_weight = (idx_total_batch / n_batch_per_epoch) / self.n_annealing_epoch\r\n\r\nKeyError: 'samples'\r\n\r\n\u200b\r\n\r\n\r\n```", "comments": ["@rrklearn2020,\r\nPlease take a look at [this guide](https://www.tensorflow.org/guide/migrate) to migrate your code from TensorFlow 1.x to 2.x and check if it helps. Thanks!", "@amahendrakar, I did check this link (before raising the support request), some changes are made, but still, there are errors. I have attached the ipynb file. Please support. ", "@rrklearn2020,\r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the error easily. Thanks!", "I tried to debug little more and found that the error is caused by the class as shown below.\r\n\r\n![image](https://user-images.githubusercontent.com/70491128/111737668-66e85800-8856-11eb-88c8-62974fe689cb.png)\r\n\r\n\r\nIt would be a great help if you can share some hints to update this area with TF2.x\r\n(Note: This class is from https://github.com/zhulingchen/tfp-tutorial. and works well with TF1.15 and TFP <0.9)", "Was able to reproduce the issue with TF v2.3, [TF v2.4](https://colab.research.google.com/gist/amahendrakar/d03d72ef716567aec789a049c38ea555/47742.ipynb) and TF-nightly.\r\n\r\nWhereas with [TF v1.15.4](https://colab.research.google.com/gist/amahendrakar/3e0c27e5b990767cba1d137f0fc44d9b/47742-1-15.ipynb), I was able to run the code without any issues. Please check the linked gist for reference. Thanks!", "It would be a great help if you can share some hints to update this area with TF2.x", "@rmothukuru  @amahendrakar @tensorflowbutler \r\n\r\nPlease support. Waiting for your feedback..", "Hi @rrklearn2020 ! \r\nWe are checking to see whether you still need help in this issue . Have you tried  Latest version TF 2.6 yet?. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47742\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47742\">No</a>\n"]}, {"number": 47741, "title": "What is the supported sequence length of bert-tiny and bert-small?", "body": "Here are few bert models I came across recently but couldn't figure out the maximum supported sequence length, Is it 512 tokens per sentence?\r\n\r\nDoes these models have the same 512 token limit as the original bert-base model\r\n\r\n\r\n```\r\nL=2 | 2/128 (BERT-Tiny) | 2/256 | 2/512 | 2/768\r\nL=4 | 4/128 | 4/256 (BERT-Mini) | 4/512 (BERT-Small) | 4/768\r\n\r\n```\r\n", "comments": ["@John-8704 \r\nThe input dimension is dependent on the dimension of positional embedding [table created](https://github.com/google-research/bert/blob/master/README.md), the model should be able to handle 256 sequence length.\r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n", "@Saduf2019 Just have a doubt, You are saying it should handle 256 sequence length but according to papers isn't BERT suppose to handle texts upto 512 sequence length? Does these versions of Bert can't handle 512 sequence length?\r\n\r\n**Sure, will post such questions at Stackoverflow going forward. Thank you.**", "@John-8704\r\nAs per your request on Bert tiny, it should be able to handle 256[ you can download a model, inspect its input layers], yes Bert base can handle 512. Please move this to closed status and create a issue on stackOverflow as it is a large community to answer."]}, {"number": 47740, "title": "Update core.py", "body": "Mirrored cl/357056439\r\nFixes GitHub #45637", "comments": []}, {"number": 47739, "title": "The code does not work", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Closing this as spam."]}, {"number": 47738, "title": "Change a1825c95 breaks TFLite for Raspberry Pi", "body": "When compiling the TensorFlow Lite Python wheel for Raspberry Pi (as described on https://www.tensorflow.org/lite/guide/build_cmake_pip), the result throws an exception when I try to use it:\r\n\r\n~~~~\r\nTraceback (most recent call last):\r\n  File \".../venv/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 45, in <module>\r\n    from tensorflow.lite.python import metrics_portable as metrics\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  ...\r\n    import tflite_runtime.interpreter as tflite\r\n  File \".../venv/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 47, in <module>\r\n    from tensorflow.lite.python import metrics_nonportable as metrics\r\nModuleNotFoundError: No module named 'tensorflow'\r\n~~~~\r\n\r\nThe offending lines were added in change a1825c95, in the file `tensorflow/lite/python/interpreter.py`:\r\n\r\n~~~~\r\ndiff --git a/tensorflow/lite/python/interpreter.py b/tensorflow/lite/python/interpreter.py\r\nindex f7ef3b34ba6..5c5898b6d4d 100644\r\n--- a/tensorflow/lite/python/interpreter.py\r\n+++ b/tensorflow/lite/python/interpreter.py\r\n@@ -40,6 +40,13 @@ else:\r\n     return lambda x: x\r\n \r\n \r\n+try:\r\n+  from tensorflow.lite.python import metrics_portable as metrics\r\n+except ImportError:\r\n+  from tensorflow.lite.python import metrics_nonportable as metrics\r\n+# pylint: enable=g-import-not-at-top\r\n+\r\n+\r\n class Delegate(object):\r\n   \"\"\"Python wrapper class to manage TfLiteDelegate objects.\r\n \r\n@@ -321,6 +328,9 @@ class Interpreter(object):\r\n             delegate._get_native_delegate_pointer())  # pylint: disable=protected-access\r\n     self._signature_defs = self.get_signature_list()\r\n \r\n+    self._metrics = metrics.TFLiteMetrics()\r\n+    self._metrics.increase_counter_interpreter_creation()\r\n+\r\n   def __del__(self):\r\n     # Must make sure the interpreter is destroyed before things that\r\n     # are used by it like the delegates. NOTE this only works on CPython\r\n~~~~\r\n\r\nI removed these lines from the copy of `interpreter.py` after installing the wheel and the rest of the code works fine.\r\n\r\nIt appears that these metrics are needed for unit testing, but something needs to be changed so they are not used when the TFLite package is run a system where TensorFlow itself is not present (e.g. a small platform like Raspberry Pi).", "comments": ["@terryheo could you take a look at this report?", "Verified. WIP", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47738\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47738\">No</a>\n", "This change doesn't fix the problem.  The metrics_portable.py file also has a dependency on tensorflow.  It includes the following line:\r\n\r\n    from tensorflow.lite.python import metrics_interface\r\n\r\nWhich breaks on Raspberry Pi.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47738\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47738\">No</a>\n", "Thanks.  It now runs on my Pi without modification."]}, {"number": 47737, "title": "Building TFLite Python wheel consumes excessive memory", "body": "I am building the TFLite runtime using the instructions described at https://www.tensorflow.org/lite/guide/build_cmake_pip\r\n\r\nWhen I run this (in a Linux VM with 8GB RAM and 8GB swap), the build process consumes all memory and crashes.  There are hundreds of instances of the C compiler running at once, which is causing the problem.\r\n\r\nIt appears that there are two places where a raw `-j` (without any number of jobs) is passed to Make, causing it to spawn parallel builds without limit.\r\n\r\nI made the following changes locally to fix it:\r\n\r\n~~~~\r\n$ git diff\r\ndiff --git a/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh b/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh\r\nindex 832106ec7a8..75adb911078 100755\r\n--- a/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh\r\n+++ b/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh\r\n@@ -113,7 +113,7 @@ case \"${TENSORFLOW_TARGET}\" in\r\n     ;;\r\n esac\r\n \r\n-cmake --build . --verbose -j -t _pywrap_tensorflow_interpreter_wrapper\r\n+cmake --build . --verbose -t _pywrap_tensorflow_interpreter_wrapper\r\n cd \"${BUILD_DIR}\"\r\n \r\n case \"${TENSORFLOW_TARGET}\" in\r\ndiff --git a/tensorflow/lite/tools/pip_package/setup.py b/tensorflow/lite/tools/pip_package/setup.py\r\nindex a85053b1602..e0ab72cd82a 100644\r\n--- a/tensorflow/lite/tools/pip_package/setup.py\r\n+++ b/tensorflow/lite/tools/pip_package/setup.py\r\n@@ -82,7 +82,7 @@ def make_args(target='', quiet=True):\r\n   args = ([\r\n       'make', 'SHELL=/bin/bash', 'BUILD_WITH_NNAPI=false', '-C', TENSORFLOW_DIR\r\n   ] + MAKE_CROSS_OPTIONS +\r\n-          ['-f', RELATIVE_MAKEFILE_PATH, '-j',\r\n+          ['-f', RELATIVE_MAKEFILE_PATH, # '-j',\r\n            str(get_build_cpus())])\r\n   if quiet:\r\n     args.append('--quiet')\r\ndiff --git a/tensorflow/tools/ci_build/devtoolset/build_devtoolset.sh b/tensorflow/tools/ci_build/devtoolset/build_devtoolset.sh\r\nindex f56f834743c..b79076f52a9 100755\r\n--- a/tensorflow/tools/ci_build/devtoolset/build_devtoolset.sh\r\n+++ b/tensorflow/tools/ci_build/devtoolset/build_devtoolset.sh\r\n@@ -114,7 +114,7 @@ cd \"${TARGET}-build\"\r\n       --with-linker-hash-style=\"gnu\" \\\r\n       --with-tune=\"generic\" \\\r\n       && \\\r\n-    make -j 42 && \\\r\n+    make -j 16 && \\\r\n     make install\r\n \r\n # Create the devtoolset libstdc++ linkerscript that links dynamically against\r\ndiff --git a/tensorflow/tools/ci_build/linux/cmake/run.sh b/tensorflow/tools/ci_build/linux/cmake/run.sh\r\nindex d9bf4f01b5d..52c6bd5daa6 100755\r\n--- a/tensorflow/tools/ci_build/linux/cmake/run.sh\r\n+++ b/tensorflow/tools/ci_build/linux/cmake/run.sh\r\n@@ -29,7 +29,7 @@ pushd build\r\n cmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake\r\n # When building do not use all CPUs due to jobs running out of memory.\r\n # TODO(gunan): Figure out why we run out of memory in large GCE instances.\r\n-make --jobs 20 tf_python_build_pip_package\r\n+make tf_python_build_pip_package\r\n \r\n virtualenv cmake_test --system-site-packages\r\n source cmake_test/bin/activate\r\n@@ -44,7 +44,7 @@ mv $WHEEL_FILE_PATH $FIXED_WHEEL_PATH\r\n pip install --upgrade $FIXED_WHEEL_PATH\r\n \r\n # Run all tests.\r\n-ctest -C Release --output-on-failure -j\r\n+ctest -C Release --output-on-failure\r\n \r\n # Finalize and go back to the initial directory.\r\n deactivate\r\n~~~~\r\n\r\nPlease consider incorporating these changes.  Especially the changes to `build_pip_package_with_cmake.sh` and `pip_package/setup.py`, which I believe are the files directly responsible for the problem.  With these changes, the Python wheel compiles fine and the system's memory usage remains within normal sizes.", "comments": ["@terryheo could you review this suggestion?", "Could you confirm if only \"build_pip_package_with_cmake.sh\" changes fix your issue?\r\nBTW, what's the number of available cores in your VM?", "Yes, that change alone is sufficient.  It prevented multiple instances of the compiler and memory usage for the entire VM never exceeded 1GB.\r\n\r\nThe VM is configured for only one core, so the scripts (which is most of them) that check the number of cores are passing `-j 1` as they should.", "Thanks for the confirmation. I'll work on it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47737\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47737\">No</a>\n", "Now you can configure it as following.\r\n```\r\n$ BUILD_NUM_JOBS=1 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh\r\n```", "This doesn't solve the problem.  The default number of jobs is still unlimited, which means anybody following the instructions on the web page (https://www.tensorflow.org/lite/guide/build_cmake_pip) will hit the same problem.\r\n\r\nAllowing the user to specify the number of jobs on the command line is a workaround, not a solution.  The script needs to do what most of the other build scripts do and set the default number of jobs to equal the number of CPUs.\r\n\r\nAdditionally, it doesn't appear to be possible to set this variable on the command line when cross-compiling via a Docker container.  At least I couldn't figure out how to do it.", "May I suggest this change as a more proper fix.\r\n\r\nNear the top of the file:\r\n\r\n    NUM_CPUS=$(grep -c ^processor /proc/cpuinfo)\r\n\r\nIn the cmake line:\r\n\r\n    cmake --build . --verbose -j ${BUILD_NUM_JOBS:-$NUM_CPUS} -t _pywrap_tensorflow_interpreter_wrapper\r\n\r\n"]}, {"number": 47736, "title": "I cannot predict with f.data.experimental.bucket_by_sequence_length", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA v11.0, cuDNN 8\r\n- GPU model and memory: Nvidia Quadro P2000 64 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI cannot predict with f.data.experimental.bucket_by_sequence_length.\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI don't know if there is a bug in my code or there is a bug in the implementation. How come training with dataset using tf.data.experimental.bucket_by_sequence_length is ok, but predicting is not ok? My error has to do with incorrect dimension but isn't that what tf.data.experimental.bucket_by_sequence_length  and assigning shaped of (None, X) are used for?\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\r\n\r\ndef gen_train():\u2028    \r\n         for x, y in zip(X_train, y_train):\u2028        \r\n                yield (x, y)\u2028\u2028\r\ndef gen_val():\u2028    \r\n         for x, y in zip(X_test, y_test):\u2028        \r\n         yield (x, y)\u2028\u2028\u2028\r\n\r\nX_train_to_dataset = tf.data.Dataset.from_generator(lambda: X_train,\u2028 output_types=(tf.float64),\u2028 output_shapes=(tf.TensorShape([None, len(X_train[0][0])])))\r\n\r\ntrain_data = tf.data.Dataset.from_generator(gen_train,\u2028 output_types=(tf.float64,\r\ntf.float64),output_shapes(tf.TensorShape([None, len(X_train[0][0])]),\u2028  tf.TensorShape([None, len(y_train[0][0])])))\r\n\r\n\u2028\u2028train_data = tf.data.Dataset.zip((train_data.map(lambda c, d: c), train_data.map(lambda a, b: tf.expand_dims(b[:, 1], axis=1))))\u2028\u2028\r\n\r\nval_data = tf.data.Dataset.from_generator(gen_val,\u2028 output_types=(tf.float64, tf.float64),\u2028  output_shapes=(tf.TensorShape([None, len(X_train[0][0])]),\u2028 tf.TensorShape([None, len(y_train[0][0])]))\u2028)\u2028\u2028\r\n\r\nval_data = tf.data.Dataset.zip((val_data.map(lambda c, d: c), val_data.map(lambda a, b: tf.expand_dims(b[:, 1], axis=1))))\u2028\r\n\r\ndef element_length_fn(x, y):\u2028    \r\n         return tf.shape(x)[0]\u2028\u2028\u2028\r\n\r\nmax_train = len(max(X_train, key=len))\u2028\r\nmin_train = len(min(X_train, key=len))\u2028\r\nmax_val = len(max(X_test, key=len))\u2028\r\nmin_val = len(min(X_test, key=len))\u2028\u2028\r\nbatch_size = 64\u2028\r\nbucket_boundaries_train = list(np.arange(min_train, max_train, 10)) \r\nbucket_batch_sizes_train = [batch_size] * (len(bucket_boundaries_train) + 1)\r\nbucket_boundaries_val = list(np.arange(min_val, max_val, 10)) \r\nbucket_batch_sizes_val = [batch_size] * (len(bucket_boundaries_val) + 1)\r\n\r\n\u2028\u2028train_data = train_data.apply(tf.data.experimental.bucket_by_sequence_length(\r\n              element_length_func=element_length_fn,\u2028\r\n              bucket_batch_sizes=bucket_batch_sizes_train,\r\n              bucket_boundaries=bucket_boundaries_train\u2028)\u2028)\u2028\u2028\r\n\r\nval_data = val_data.apply(\u2028tf.data.experimental.bucket_by_sequence_length(\r\n              element_length_func=element_length_fn,\u2028\r\n              bucket_batch_sizes=bucket_batch_sizes_val,\r\n              bucket_boundaries=bucket_boundaries_val,)\r\n\r\ndef create_model():\u2028    \r\n     i = Input(shape=(None, len(X_train[0][0])))\u2028\r\n     x = Bidirectional(LSTM(128, return_sequences=True))(i)\u2028    \r\n     x = TimeDistributed(Dense(1))(x)\u2028    \r\n     return tf.keras.Model(inputs=i, outputs=x)\r\n\r\nmodel = create_model()\u2028    \r\nmodel.compile(\u2028loss='mse',\u2028optimizer='adam',metrics=[metrics.RootMeanSquaredError()])\u2028\u2028\r\nmodel.fit(train_data,\r\n         epochs=5,\r\n         validation_data=val_data\u2028)\r\n\r\nNormLayer = preprocessing.Normalization()\u2028\r\nNormLayer.adapt(X_train_to_dataset)\u2028\r\nmodel = create_model_bdlstm128(NormLayer)\u2028\r\n\r\nmodel.compile(\u2028loss='mse',optimizer='adam',\u2028metrics=[metrics.RootMeanSquaredError()])\u2028\r\n\r\nmodel.fit(\u2028train_data, epochs=5000,\u2028 validation_data=val_data)\r\n\r\ny_pred = model.predict(val_data.map(lambda a, b: a))\u2028 \r\n\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe below is the error for ```y_pred = model.predict(val_data.map(lambda a, b: a))\u2028 ```:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"C:\\Users\\ipancorbo\\Anaconda3\\envs\\tf_recsys\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1646, in predict\r\n    all_outputs = nest.map_structure_up_to(batch_outputs, concat, outputs)\r\n  File \"C:\\Users\\ipancorbo\\Anaconda3\\envs\\tf_recsys\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 1159, in map_structure_up_to\r\n    return map_structure_with_tuple_paths_up_to(\r\n  File \"C:\\Users\\ipancorbo\\Anaconda3\\envs\\tf_recsys\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 1257, in map_structure_with_tuple_paths_up_to\r\n    results = [\r\n  File \"C:\\Users\\ipancorbo\\Anaconda3\\envs\\tf_recsys\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 1258, in <listcomp>\r\n    func(*args, **kwargs) for args in zip(flat_path_gen, *flat_value_gen)\r\n  File \"C:\\Users\\ipancorbo\\Anaconda3\\envs\\tf_recsys\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 1161, in <lambda>\r\n    lambda _, *values: func(*values),  # Discards the path arg.\r\n  File \"C:\\Users\\ipancorbo\\Anaconda3\\envs\\tf_recsys\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 2716, in concat\r\n    return array_ops.concat(tensors, axis=axis)\r\n  File \"C:\\Users\\ipancorbo\\Anaconda3\\envs\\tf_recsys\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\ipancorbo\\Anaconda3\\envs\\tf_recsys\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1677, in concat\r\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n  File \"C:\\Users\\ipancorbo\\Anaconda3\\envs\\tf_recsys\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1192, in concat_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\Users\\ipancorbo\\Anaconda3\\envs\\tf_recsys\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [64,29,1] vs. shape[3] = [64,30,1] [Op:ConcatV2] name: concat\r\n\r\n```", "comments": ["Ok, I realized that this is probably because model.predict returns a numpy array and thus concatenates every prediction (which are obviously, in my case, not equal size, and hence the concatenation error). Does tf.keras have no solution for this??", "@inespancorbo \r\nThe code provided is not reproducible, please share a colab gist for us to replicate the error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47736\">No</a>\n"]}, {"number": 47735, "title": "Update normalization_v2.py", "body": "Mirrored cl/362162412\r\nFixes GitHub #47608", "comments": []}, {"number": 47734, "title": "Add \\r and \\n to DEFAULT_STRIP_REGEX", "body": "I worked with text vectorization and I thought that \\r and \\n needed to be cropped off. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47734) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Can you explain more of what's going wrong for you here?\r\n\r\nWhitespace should be removed during the token splitting step, which comes after the standardization step where this regex is applied. You can also always pass custom callables for `standardize` or `split` if you want custom behavior. But as a default, I do not think we the standardization step to strip whitespace.", "> Can you explain more of what's going wrong for you here?\r\n> \r\n> Whitespace should be removed during the token splitting step, which comes after the standardization step where this regex is applied. You can also always pass custom callables for `standardize` or `split` if you want custom behavior. But as a default, I do not think we the standardization step to strip whitespace.\r\n\r\nA lot of the data that I had had \\n and \\r, and I think that it is probably a common occurrence in text data, which is why I thought it should be part of the default punctuation removal part. I kind of think it is redundant to have a callable that basically does the exact same thing as the default program with just these two characters, right?", "A minimal code example might help us get to the bottom of this!\r\n\r\nTo be clear `\\n` and `\\r` should already be stripped, along with spaces, tabs, and other whitespace, just during the \"splitting\" step, not the \"standardization\" step. You won't see them in a regex in this file, as we call into a standard tf utility for whitespace splitting. Removing whitespace in the standardization step would be unwanted, as it could actually end up fusing separate tokens across whitespace.\r\n\r\nHere's an example of how things work today.\r\n\r\n```\r\ntext_vectorization = TextVectorization()\r\ntext_vectorization.adapt([\"foo\\r\\nbar\\nbaz   foo\"])\r\ntext_vectorization.get_vocabulary()\r\n> ['', '[UNK]', 'foo', 'baz', 'bar']\r\ntext_vectorization([\"foo\\r\\nbar\\nbaz   foo\"])\r\n> <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[2, 4, 3, 2]])>\r\n```\r\n\r\nI believe with your change we could end up getting a vocabulary of \"foobarbaz foo\", which is not what we want in this case.", "Oh! ok, I see. \r\n\r\n> A minimal code example might help us get to the bottom of this!\r\n> \r\n> To be clear `\\n` and `\\r` should already be stripped, along with spaces, tabs, and other whitespace, just during the \"splitting\" step, not the \"standardization\" step. You won't see them in a regex in this file, as we call into a standard tf utility for whitespace splitting. Removing whitespace in the standardization step would be unwanted, as it could actually end up fusing separate tokens across whitespace.\r\n> \r\n> Here's an example of how things work today.\r\n> \r\n> ```\r\n> text_vectorization = TextVectorization()\r\n> text_vectorization.adapt([\"foo\\r\\nbar\\nbaz   foo\"])\r\n> text_vectorization.get_vocabulary()\r\n> > ['', '[UNK]', 'foo', 'baz', 'bar']\r\n> text_vectorization([\"foo\\r\\nbar\\nbaz   foo\"])\r\n> > <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[2, 4, 3, 2]])>\r\n> ```\r\n> \r\n> I believe with your change we could end up getting a vocabulary of \"foobarbaz foo\", which is not what we want in this case.\r\n\r\nOk, I see. That makes sense to split it then. I think I incorrectly assumed that it wouldn't happen just by looking at the regex. That all seems good, then!"]}, {"number": 47733, "title": "Note tf.cast truncates floating-point when casting to integral types", "body": "I had assumed that it would round to the nearest integer; I was wrong.\r\n\r\nI assume that this behavior is deliberate and should be defined, rather than left undefined or implementation-defined.", "comments": ["This is the default behavior:\r\n\r\n```\r\n>>> int(3.7)\r\n3\r\n>>> int(-3.7)\r\n-3\r\n```"]}, {"number": 47732, "title": "micro: CUMSUM PR2", "body": "Create the reference implementation in its own header so that micro\r\ncan use it without the unrelated depedencies of reference_ops.h.\r\n\r\nPR step 2 for issue #47290", "comments": []}]