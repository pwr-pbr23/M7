[{"number": 19605, "title": "how to distribute train using tf estimator, is there an example?", "body": "Is distributed training using tf estimator still under development?\r\nAre there any examples show how to train distributedly using tf estimator(using the develop version)?\r\nFollowing the https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig, I could not work it out, could you provide some examples?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@chengmengli06 , can you clarify what sort of distributed training you had in mind?", "What about https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md? Is it enough? Need it to be extendend or better exposed in docs?", "I want to distribute train with multi machine multi gpus(olderly use something like TF_CONFIG), and this could not be done with MirrorStrategy. How could I do that?@bhack @karmel ", "For multi-node, I suggest you take a look at [tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) and its treatment of [distributed workers](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L439) . Tensor2tensor also has [documentation on how they handle distributed training](https://github.com/tensorflow/tensor2tensor/blob/master/docs/distributed_training.md). Is that what you had in mind?", "I want a solution like MirroredStrategy. Is there any other strategy in developing? Distributed training itself is not a problem, I have learned how to do it now. But I want to know how to integrate it with tf.estimator?@karmel", "Ah, that makes sense. DistributionStrategies will add multi-node support (cc @anj-s ), but in the meantime, the Tensor2Tensor example linked above uses the standard distribution mechanisms paired with Estimators ([here](https://github.com/tensorflow/tensor2tensor/blob/9304249f8efa20a60565b1f3a4fde43732e89556/tensor2tensor/utils/trainer_lib.py#L201)). \r\n\r\n@ispirmustafa , are there other examples of distributed training with Estimators that you can point @chengmengli06 to?", "chengmengli06@ Multi node MirroredStrategy is work in progress. You can see an initial implementation [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/multi_worker_strategy.py). \r\nYou can use the Estimator's [train_and_evaluate API](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate) to do distributed training across machines in the meantime.", "@chengmengli06  https://github.com/tensorflow/models/pull/4277 is a cifar10 example that using Estimator's `train_and_evaluate` API  as @anj-s mentioned for multi-nodes distributed training.  Hope it help in some way.  \r\nBTW, looking forward to the multi-node MirroredStrategy :-)", "@anj-s Will be the multi-nodes strategiy also multi-gpu on the single node?", "bhack@ I am sorry I did not understand the question. Do you mean are the two strategies the same? ", "@anj-s I meant if using multi-nodes will let to automatically mirror on the single node if multi gpu are exposed. I.e. 4 nodes with 1 gpu x node + 2 nodes with 4 gpus x nodes.", "bhack@ Yes, multi-node strategy will mirror the model on multiple GPUs if they are available. ", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue, as the primary question has been addressed. Please re-open or open a new question if other issues arise.", "Same question. The link is broken.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md", "See https://github.com/tensorflow/docs/blob/r1.15/site/en/guide/distribute_strategy.ipynb"]}, {"number": 19604, "title": "Fix Android Build", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@fdoumet I'm not sure of the context of this PR. Which android build have you fixed? Can you tell me how I can verify this?", "As you can see, the nightly Android builds are failing. This fixes it. To verify it, you can follow the instructions in `/contrib/makefile` and `contrib/android`", "@aselle @gunan can you look at this? I don't think I have the expertise to review appropriately.", "Ugh, we have another cmakefile under contrib/android?\r\nAdding @petewarden, as he is the owner of the android build.\r\n\r\nIn our CI, android builds are passing. ci.tensorflow.org is deprecated, and it has last ran the android build a month ago. So if you are referring to that one the results there are obsolete.\r\nWe are working to expose the new CIs results.", "@petewarden @aselle please review.", "Thanks", "Nagging Reviewer @aselle, @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "Sorry for the slow review. We're no longer actively maintaining the cmake Android builds for mainline TensorFlow, we're deprecating that in favor of TF Lite, so closing for now."]}, {"number": 19603, "title": "terminate called after throwing an instance of 'std::bad_alloc', over 20Gb memory free", "body": "Training a small model in Keras, getting this error after about 20 epochs (of 10000 samples).  While training is running, the system is chugging along happily with about 43Gb of memory free.  At about epoch 20, suddenly everything stops and free memory drops to about 20Gb before throwing the error.\r\n\r\nSeems to be the same issue as #9487\r\n```\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\n```\r\n\r\n\r\ntf_env data:\r\n\r\n> == cat /etc/issue ===============================================\r\n> Linux 7ff559301433 4.10.0-37-generic #41-Ubuntu SMP Fri Oct 6 20:20:37 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n> VERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\n> VERSION_ID=\"16.04\"\r\n> VERSION_CODENAME=xenial\r\n> \r\n> == are we in docker =============================================\r\n> Yes\r\n> \r\n> == compiler =====================================================\r\n> c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\n> Copyright (C) 2015 Free Software Foundation, Inc.\r\n> This is free software; see the source for copying conditions.  There is NO\r\n> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n> \r\n> \r\n> == uname -a =====================================================\r\n> Linux 7ff559301433 4.10.0-37-generic #41-Ubuntu SMP Fri Oct 6 20:20:37 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n> \r\n> == check pips ===================================================\r\n> numpy (1.14.3)\r\n> protobuf (3.4.0)\r\n> tensorflow-gpu (1.3.0)\r\n> tensorflow-tensorboard (0.1.8)\r\n> \r\n> == check for virtualenv =========================================\r\n> False\r\n> \r\n> == tensorflow import ============================================\r\n> tf.VERSION = 1.3.0\r\n> tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\n> tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\n> Sanity check: array([1], dtype=int32)\r\n> \r\n> == env ==========================================================\r\n> LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n> DYLD_LIBRARY_PATH is unset\r\n> \r\n> == nvidia-smi ===================================================\r\n> Mon May 28 18:57:01 2018       \r\n> +-----------------------------------------------------------------------------+\r\n> | NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\r\n> |-------------------------------+----------------------+----------------------+\r\n> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n> |===============================+======================+======================|\r\n> |   0  GeForce GTX 750 Ti  Off  | 00000000:0B:00.0 Off |                  N/A |\r\n> | 33%   30C    P0     1W /  38W |     11MiB /  2000MiB |      0%      Default |\r\n> +-------------------------------+----------------------+----------------------+\r\n> |   1  GeForce GTX 660     Off  | 00000000:14:00.0 N/A |                  N/A |\r\n> | 30%   34C    P0    N/A /  N/A |     11MiB /  1999MiB |     N/A      Default |\r\n> +-------------------------------+----------------------+----------------------+\r\n>                                                                                \r\n> +-----------------------------------------------------------------------------+\r\n> | Processes:                                                       GPU Memory |\r\n> |  GPU       PID   Type   Process name                             Usage      |\r\n> |=============================================================================|\r\n> |    1                    Not Supported                                       |\r\n> +-----------------------------------------------------------------------------+\r\n> \r\n> == cuda libs  ===================================================\r\n> /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n> /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n> \r\n\r\nThe data loading code isn't important, just opening files and getting 64x64 pixel chunks of each.  I'm attaching the actual NN code though.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.utils import multi_gpu_model\r\nfrom keras.layers import Convolution2D, MaxPooling2D, Conv2DTranspose\r\n\r\nfrom keras import backend as K\r\nK.clear_session()\r\n\r\nimport numpy as np\r\n\r\nimport dataPool2 as dp\r\n\r\ndef psnr(img1, img2):\r\n    #print(img1.shape, img2.shape)\r\n    #assert img1.shape == img2.shape\r\n\r\n    mse = tf.losses.mean_squared_error(img1, img2)\r\n\r\n    score = 3 * tf.log((255 ** 2) / mse)\r\n    \r\n    return 1 / score\r\n\r\ndef srcnnModel():\r\n\tmodel = Sequential()\r\n\tmodel.add(Convolution2D(64, (9, 9), activation='relu', padding='same', input_shape=(64,64,1)))\r\n\tmodel.add(Convolution2D(32, (1, 1), activation='relu', padding='same'))\r\n\tmodel.add(Convolution2D(1, (5, 5), padding='same'))\r\n\t\r\n\tmodel.compile(loss='mse',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\n\treturn model\r\n\r\nwith tf.device('/cpu:0'):\r\n    model = srcnnModel()\r\n\r\nprint(model.summary())\r\n\t\r\nparallel_model = multi_gpu_model(model, gpus=2)\r\nparallel_model.compile(loss=psnr,\r\n                       optimizer='adam')\r\n\t\t\t\t   \r\n# Generate Data\r\nxYchan, xCrCb, yYchan, origY = dp.createDataset(\"test_images/\", 10000)\r\n\r\nparallel_model.fit(np.array(xYchan), np.array(yYchan), epochs=200, batch_size=128, callbacks=[\r\n                keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=3, verbose=0, mode='auto')],) \r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "After further testing, changing batch size, adding/removing \"clear_session()\", and running in single GPU mode all have no effect on the error.", "More testing...\r\n\r\nRemoving \"with tf.device('/cpu:0'):\" allows me to see more details on the error.  I'm running out of GPU memory.  It seems crazy that a model with 8129 parameters and 5-10,000 samples won't fit in 4Gb?", "@aselle do you know who can look at this kind of issue?", "What size tensors are you feeding and what is the size of the intermediate tensors?", "@reedwm, do you have any further insights?", "Not sure what the issue could be. Perhaps @zheng-xq has some ideas?\r\n\r\nWithout a self-contained example, we cannot effectively debug this.", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I'm facing the same issue, I have an 8GB of RAM and an GPU with 4GB.", "Encounterd the same problem in TF2.0. For me it was padding='same'. Will throw error even before running graph with data", "I am also facing this same issue on my GCP VM (containing 2 V100s). I am using [this example](https://www.pyimagesearch.com/2017/10/30/how-to-multi-gpu-training-with-keras-python-and-deep-learning/) by PyImageSearch for testing. I have carefully used `tf.keras` and also made sure that I am using `tf.compat.v2.keras.utils.multi_gpu_model`. Here's some error trace:\r\n\r\n```\r\n2019-11-19 03:07:59.945565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device i\r\nnterconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-19 03:07:59.945624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1\r\n \r\n2019-11-19 03:07:59.945680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y\r\n \r\n2019-11-19 03:07:59.945736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N\r\n \r\n2019-11-19 03:07:59.946193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] succ\r\nessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA\r\n node, so returning NUMA node zero\r\n2019-11-19 03:07:59.946714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] succ\r\nessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA\r\n node, so returning NUMA node zero\r\n2019-11-19 03:07:59.947233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] succ\r\nessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA\r\n node, so returning NUMA node zero\r\n2019-11-19 03:07:59.947712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created \r\nTensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14828 MB memory) -> phys\r\nical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability\r\n: 7.0)\r\n2019-11-19 03:07:59.947868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] succ\r\nessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA\r\n node, so returning NUMA node zero\r\n2019-11-19 03:07:59.948324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created \r\nTensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14828 MB memory) -> phys\r\nical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability\r\n: 7.0)\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\nAborted\r\n```\r\n "]}, {"number": 19602, "title": "[Feature Request] Named Dimensions", "body": "Having named dimensions would be a nice improvement to dynamic shapes.\r\nI'm thinking about a similar system like [xarray's](http://xarray.pydata.org/en/stable/data-structures.html#dataset).\r\n\r\nExample of current code:\r\n```\r\nimages = tf.placeholder(tf.float32, shape=(None, None, None, 3))\r\nimages_shape = tf.shape(images)\r\nones = tf.ones(images_shape, dtype=images.dtype)\r\nimages_plus1 = images + ones \r\nprint(images_plus1)\r\n```\r\n> <tf.Tensor 'add:0' shape=(?, ?, ?, 3) dtype=float32>\r\n\r\nDue to the \"None\" - Dimensions it's hard to distinguish them, especially when using reshaping ops.\r\nIn my opinion some type of \"tf.Dimension\" would be better:\r\n```\r\nimage_dim_0 = tf.Dimension(size=None, name=\"image_dim_0\")\r\nimage_dim_1 = tf.Dimension(size=None, name=\"image_dim_1\")\r\nimage_dim_2 = tf.Dimension(size=None, name=\"image_dim_2\")\r\n\r\nimages = tf.placeholder(tf.float32, shape=(image_dim_0, image_dim_1, image_dim_2, 3))\r\nimages_shape = tf.shape(images)\r\nones = tf.ones(images_shape, dtype=images.dtype)\r\nimages_plus1 = images + ones \r\nprint(images_plus1)\r\n```\r\n> <tf.Tensor 'add:0' shape=(**image_dim_0:** ?, **image_dim_1:** ?, **image_dim_2:** ?, 3) dtype=float32>\r\n\r\nAlso, this maybe helps to do some shape inference?\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: none\r\n- **GCC/Compiler version (if compiling from source)**: none\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: none\r\n", "comments": ["/CC @martinwicke", "We actually have an attempt at this: [contrib/labeled_tensor](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/labeled_tensor).\r\n\r\nI will close this issue. Contributions to labeled_tensor are welcome, although at this point, I believe it is abandoned (?).", "Thanks for the hint, this fits my request very well.\r\nIt's a pity that these tensor labels are not natively supported by every tensor.\r\n", "I think supporting named dimensions would still be a very useful feature. The link above is dead now because contrib is gone. JAX also supports this as *axes names*. Are there still ideas for adding this idea to TensorFlow?", "I've opened a new issue with a more detailed proposal: https://github.com/tensorflow/tensorflow/issues/38228. @martinwicke, could you please take a look at it?", "We are evaluating this project in the context of implementing a stronger\ntype system.\n\n@agarwal-ashish FYI\n", "Good to know, thanks."]}, {"number": 19601, "title": "fix typo", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@noppefoxwolf Please sign the CLA.", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 19600, "title": "Windows: fail gracefully when undname.exe is not found", "body": "/cc @gunan \r\nImprove error messages", "comments": ["Thanks for fixing the error message, Gunhan! "]}, {"number": 19599, "title": "BUILD: dont force stripping", "body": "Build systems must not strip binaries, it makes it impossible for\r\ndistros to ship debugging symbols for packages.\r\n\r\nbazel build has a --strip option to allow the user to generate stripped\r\nbinaries in a configurable way, that should be used instead.\r\n\r\nhttps://fedoraproject.org/wiki/Packaging:Debuginfo\r\nhttps://wiki.gentoo.org/wiki/Project:Quality_Assurance/Backtraces#Stripping\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["I think we ran into binary size issues (severe ones) if we do not strip earlier than all the way in the end.", "I guess we could make this change, but also add `build --strip=always` to the bazelrc?", "These strip args are in the rule for the final \r\n.so file tho so isnt this only at the end?\r\n\r\nI have not seen any size or ram issues from users on gentoo with this patch so far. \r\n\r\nIf size issues are a problem tho, bazel has a --strip=always which would strip every binary along the way which would probably be even better than the -s for these two .so's.\r\n\r\nI have no issue with some other way as long as there is a way it can be toggled off since otherwise our package manager throws huge QA warnings", "@martinwicke \r\nYou may not even need to force it in the bazelrc since the default is to strip things by default in bazel\r\nfrom: https://docs.bazel.build/versions/master/command-line-reference.html\r\n\r\n--strip=<always, sometimes or never> default: \"sometimes\"\r\nSpecifies whether to strip binaries and shared libraries (using \"-Wl,-- strip-debug\"). The default value of 'sometimes' means strip iff -- compilation_mode=fastbuild.\r\n\r\n--compilation_mode=<fastbuild, dbg or opt> [-c] default: \"fastbuild\"\r\nSpecify the mode the binary will be built in. Values: 'fastbuild', ' dbg', 'opt'. \r\n\r\nnothing in the docs say to set --compilation_mode so its probably always fastbuild by default already.", "We set -c opt in bazelrc, so I think we would need the additional setting. That's not a big deal though, if that makes your life easier?", "Oops okay i missed that, incidentally -c opt / --config=opt / --copt= is super confusing.\r\n\r\nIn that case I'll update this to add strip=always, overriding that option is much easier than carrying a patch around.", "Great. If you can do within the next couple of hours, we'll get this patch in before 1.9.", "@martinwicke done :)\r\nrebased the first unchanged and added a second commit", "@av8ramit assuming this passes tests (which I believe it should), can we wait for this before cut?", "@martinwicke okay sounds good"]}, {"number": 19598, "title": "python_configure.bzl: Find bash binary path through BAZEL_SH env var.", "body": "This helps avoid invoking the wrong bash binary when \"Bash on Ubuntu on Windows\"\r\nis installed.\r\n\r\nFixed https://github.com/tensorflow/tensorflow/issues/11735\r\n\r\n//cc @gunan ", "comments": []}, {"number": 19597, "title": "generate-pc.sh: add option to set libdir", "body": "64bit libraries are usually installed into /usr/lib64/, this adds an option to specify libdir when generating the pkgconfig file.\r\n", "comments": []}, {"number": 19596, "title": "Undefined symbol _rdft and _cblas_sgemm in libtensorflow-lite.a", "body": "Hi,\r\n\r\nI am working on creating this SDK which is working fine if installed TensorflowLite via **_pod install_**\r\nBut, when creating this same SDK by steps mentioned in the following link (https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/lite/g3doc/ios.md) it gives the issue mentioned below:\r\n\r\n> Undefined symbols for architecture arm64:\r\n>   \"_rdft\", referenced from:\r\n>       tflite::internal::Spectrogram::ProcessCoreFFT() in libtensorflow-lite.a(spectrogram.o)\r\n>   \"_cblas_sgemm\", referenced from:\r\n>       tflite::cblas_ops::Conv(float const*, tflite::Dims<4> const&, float const*, tflite::Dims<4> const&, float const*, tflite::Dims<4> const&, int, int, int, int, float, float, float*, tflite::Dims<4> const&, float*, tflite::Dims<4> const&) in libtensorflow-lite.a(conv.o)\r\n> ld: symbol(s) not found for architecture arm64\r\n\r\nthats when using TensorFlow version - r1.8\r\n\r\nAlso libtensorflow-lite.a is not created when using TensorFlow version - r1.7\r\n\r\n> 1 error generated.\r\n> make: *** [/Users/quantiphi/TFLite/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/kernels/internal/spectrogram.o] Error 1\r\n\r\n\r\nHave I written custom code - No\r\nOS Platform and Distribution - MacOS\r\nTensorFlow - N/A\r\nTensorFlow version - r1.8\r\nBazel version - 0.11.0\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A\r\n", "comments": ["`_rdft` comes from fftw library.\r\n\r\nMy solution is downloading fftw and link with it, please see my fork:\r\n \r\nhttps://github.com/syoyo/tensorflow/commit/d4ba9c8e9387b5ad62511aaa95adb2427d3d9952#diff-57135143920cd34bc2ac188b5934308eR82\r\n\r\nFor `_cblas_sgemm`, you may solve it by linking with BLAS library provided by iOS SDK, but I'm not sure.\r\n", "The build issue was fixed in commit f1f1d5172fe5bfeaeb2cf657ffc43ba744187bee. \r\nThe fix is in master branch, but not in r1.8 unfortunately. \r\nIt will definitely be in the next release branch. \r\n\r\nFor now, you can either use master branch, or cherry-pick the commit on top of r1.8 locally. \r\n", "thanks @miaout17 , the commit f1f1d51 have solved **\"_rdft\", referenc.....** issue\r\nbut I'm still facing the issue mentioned below,\r\n\r\n> \"_cblas_sgemm\", referenced from:\r\n> tflite::cblas_ops::Conv(float const*, tflite::Dims<4> const&, float const*, tflite::Dims<4> const&, float const*, tflite::Dims<4> const&, int, int, int, int, float, float, float*, tflite::Dims<4> const&, float*, tflite::Dims<4> const&) in libtensorflow-lite.a(conv.o)\r\n> ld: symbol(s) not found for architecture arm64\r\n\r\nIts there in master branch as well as in r1.8 branch ", "Thanks, Added Accelerate.framework to solve this issue\r\n\r\n> \"_cblas_sgemm\", referenced from:\r\n> tflite::cblas_ops::Conv(float const*, tflite::Dims<4> const&, float const*, tflite::Dims<4> const&, float const*, tflite::Dims<4> const&, int, int, int, int, float, float, float*, tflite::Dims<4> const&, float*, tflite::Dims<4> const&) in libtensorflow-lite.a(conv.o)\r\n> ld: symbol(s) not found for architecture arm64"]}, {"number": 19595, "title": "hardening-check return \"Fortify Source functions: no, only unprotected functions found!\" for some \"so\"s, for example - tensorflow/contrib/image/python/ops/_distort_image_ops.so ", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: 12G, K80\r\n- **Exact command to reproduce**:  hardening-check ./.cache/bazel/_bazel_pengwa/85985b4501e3e12ee6a73770d2b8b659/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/rnn/python/ops/_lstm_ops.so\r\n\r\n\r\n\r\n### Describe the problem\r\nrun hardening-check on Ubuntu16.04 against all the \"so\"es built from TensorFlow source code, in the result, some of \" Fortify Source functions\" return \"yes (some protected functions found)\", some return \"no, only unprotected functions found!\". \r\n\r\nI assume those \"so\"es who return \"no, only unprotected functions found!\" are not fortified, could anybody help confirm? If yes, it that a bug?\r\n\r\nhere are the result example s for a few \"so\"es: \r\n\r\n> ./.cache/bazel/_bazel_pengwa/85985b4501e3e12ee6a73770d2b8b659/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/image/python/ops/_distort_image_ops.so:\r\n Position Independent Executable: no, regular shared library (ignored)\r\n Stack protected: yes\r\n **Fortify Source functions: no, only unprotected functions found!**\r\n Read-only relocations: yes\r\n Immediate binding: yes\r\n./.cache/bazel/_bazel_pengwa/85985b4501e3e12ee6a73770d2b8b659/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/image/python/ops/_single_image_random_dot_stereograms.so\r\n./.cache/bazel/_bazel_pengwa/85985b4501e3e12ee6a73770d2b8b659/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/image/python/ops/_single_image_random_dot_stereograms.so:\r\n Position Independent Executable: no, regular shared library (ignored)\r\n Stack protected: yes\r\n **Fortify Source functions: yes (some protected functions found)**\r\n Read-only relocations: yes\r\n Immediate binding: yes\r\n./.cache/bazel/_bazel_pengwa/85985b4501e3e12ee6a73770d2b8b659/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/libsvm/python/ops/_libsvm_ops.so\r\n./.cache/bazel/_bazel_pengwa/85985b4501e3e12ee6a73770d2b8b659/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/libsvm/python/ops/_libsvm_ops.so:\r\n Position Independent Executable: no, regular shared library (ignored)\r\n Stack protected: yes\r\n **Fortify Source functions: no, only unprotected functions found!**\r\n Read-only relocations: yes\r\n Immediate binding: yes\r\n./.cache/bazel/_bazel_pengwa/85985b4501e3e12ee6a73770d2b8b659/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/ffmpeg/ffmpeg.so\r\n./.cache/bazel/_bazel_pengwa/85985b4501e3e12ee6a73770d2b8b659/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/ffmpeg/ffmpeg.so:\r\n Position Independent Executable: no, regular shared library (ignored)\r\n Stack protected: yes\r\n **Fortify Source functions: yes (some protected functions found)**\r\n Read-only relocations: yes\r\n Immediate binding: yes\r\n", "comments": ["Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19594, "title": "Using 1.7's TF_C_API_GRAPH_CONSTRUCTION=0 in 1.8.", "body": "After your 1.8 change I am unable to programatically obtain architecture of loaded neural network (neural network saved by SavedModel functionality, custom tf.Estimator [or any tf.Estimator for that matter]). \r\n\r\nIn 1.7 I could get all of the operations by ```graph.get_operations()```. Those were ordered (exactly the same as network layout, e.g. input operations first, followed by dense/convolutional/other layers up to network/graph output). Furthermore I could infer architecture by their names like conv2d etc. Right now (version 1.8) operations are **unordered** and their placement does not make much sense.\r\n\r\nMy question is: Is it possible to do now? Should I use some script from your repository to get those names in order (other than environment variable provided right now)?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks for reporting this. I believe that get_operations() technically doesn't specify an order, but it seems useful to get the operations in the order created and it turns out to not be hard to fix. I'll work on getting a fix checked in."]}, {"number": 19593, "title": "Does AWS (4.4.0-1060-aws) with g2.2X large support TensorFlow - GPU ?", "body": "Installed CUDA 9.0 and CuDNN on Ubuntu 16.04 (4.4.0-1060-aws; g2.2X large).\r\n\r\nI got an error as - The GPU Driver (GPU Name: Persistence-M; NVIDIA-SMI 396.26) is not TensorFlow compatible. Below is my Linux screenshot).\r\n2018-05-24 22:08:28.707495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: GRID K520, pci bus id: 0000:00:03.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\r\n\r\nDoes AWS (4.4.0-1060-aws) with g2.2X large support TensorFlow - GPU ?\r\n\r\nThanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19592, "title": "[TF Mobile] Adding codes to print Logcat log of statString for analizing the perf\u2026", "body": "\u2026ormance per layers\r\n\r\nAdding codes to print Logcat log of statString for analizing the performance per layers", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "> I signed it\r\ncan you please make sure to use same username and email id while signing CLA", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 19591, "title": "Fix typo", "body": "Fix typo", "comments": []}, {"number": 19590, "title": "Fix typo", "body": "Fix typo", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 19589, "title": "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory", "body": "Hi\r\n\r\nI installed CUDA Toolkit 9.2 and CuDNN on Ubuntu 16.04 (4.4.0-1060-aws; g2.2X large) with strictly followed the installation instructions from these two urls -\"https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions\" and \"https://docs.nvidia.com/deeplearning/sdk/cudnn-install/\". My CUDA and CuDNN installation were successful. I verified the CuDNN installation as mentioned at the installation guide.\r\nBut, after installing \"tensorflow-gpu\", it throws the above error. I installed Anaconda and then \"tensorflow-gpu\" with pip command as: $pip install -U tensorflow-gpu\r\nThe \"tensorflow-gpu\" installation became fully successful.\r\n\r\nThese are some more details of my configurations:\r\n NVIDIA-SMI 396.26\r\n\r\n$cat /usr/local/cuda/version.txt\r\nCUDA Version 9.2.88\r\nCUDA Patch Version 9.2.88.1\r\n\r\n$which nvcc\r\n/usr/local/cuda-9.2/bin/nvcc\r\n\r\n$nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Wed_Apr_11_23:16:29_CDT_2018\r\nCuda compilation tools, release 9.2, V9.2.88\r\n\r\nPlease suggest me. Thanks.\r\n", "comments": ["@arpan8514 I think you re-install cuda 9.0.\r\nI got this error when i install cuda 9.1. \r\nThanks\r\n", "@ViNgoVan Earlier, I got an error using CUDA 9.0 as - The GPU Driver (GPU  Name: Persistence-M; NVIDIA-SMI 396.26) is not TensorFlow compatible. (Below is my Linux screenshot).\r\n\r\n2018-05-24 22:08:28.707495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: GRID K520, pci bus id: 0000:00:03.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\r\n\r\nAny idea on this ? Could you please tell me what CuDNN version you used ?\r\n\r\nWhat is your Architecture Configuration ? Is it AWS (4.4.0-1060-aws) with g2.2X large ?\r\n\r\nThanks.\r\n\r\n", "@arpan8514 this is the link for cuda9.0 [https://developer.nvidia.com/cuda-90-download-archive] and cudnn version is 9.0", "@athuldevin I used that only :) But, as mentioned, I got error with my GPU Device - Cuda compute capability 3.0 - which is not TensorFlow compatible.", "@ViNgoVan Any idea on how to solve on AWS (4.4.0-1060-aws) with g2.2X large ?", "@arpan8514 - This post helped me resolve it - http://www.python36.com/how-to-install-tensorflow-gpu-with-cuda-9-2-for-python-on-ubuntu/", "Nagging Assignee @jart: It has been 167 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this still an issue for you? Did you get a chance to try @sampathweb suggestion? \r\nYou are right about the CUDA compute capability limitation. TensorFlow GPU version requires CUDA\u00ae Compute Capability 3.5 or higher. However people in the past were able to install it with compute capability 3.0 by building from sources unfortunately that's not officially supported by the TF team. "]}, {"number": 19588, "title": "Using Dataset api with Estimator in MirroredStrategy,  Non-DMA-safe string tensor error", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: centos\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**:1.8.0 \r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:4.8.5\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**:GeForce GTX 1080Ti * 4\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nUsing mutilple gpu by MirroredStrategy, Get ' Non-DMA-safe string tensor may not be copied from/to a GPU.' error\r\n\r\n### Source code / logs\r\n\r\n![image](https://user-images.githubusercontent.com/12636388/40601170-a716ad26-6286-11e8-9fb1-7b5d1b6c6545.png)\r\n![image](https://user-images.githubusercontent.com/12636388/40601198-c0ad1f36-6286-11e8-956c-3023fcc86292.png)\r\n![image](https://user-images.githubusercontent.com/12636388/40601204-c6ecdaf8-6286-11e8-96cd-588ef276f5b4.png)\r\n![image](https://user-images.githubusercontent.com/12636388/40601209-cbc60c16-6286-11e8-8dd0-6b54df3ab8fe.png)\r\n\r\n", "comments": ["I meet the same problem @skye  in using object detection apis", "Can you provide code to repro the problem?", "Rohan/Priya: I'm guessing this is what happens when a `tf.string` tensor goes through `prefetch_to_devices()`, but unclear whether it should be handled in the client program (e.g. by splitting out strings from the prefetched dataset) or in the `FunctionBufferingResource` (e.g. by allowing some outputs to be \"host memory\" only).", "def get_inputs(mode, csv_file, batch_size, label_list, preprocess):\r\n    iterator_initializer_hook = IteratorInitializerHook()\r\n    def inputs():\r\n        is_training = mode==estimator.ModeKeys.TRAIN\r\n        ds = tf.data.TextLineDataset(csv_file).skip(1)   \r\n                \r\n        def classification_parse_line(line):\r\n            columns = ['img','label']\r\n            img_name, label = tf.decode_csv(\r\n                line, \r\n                record_defaults = [[''],['']]) \r\n            # assume every pic is rgb\r\n            image_decoded = tf.image.decode_png(\r\n                tf.read_file(img_name),\r\n                channels=3)\r\n            image = preprocess(image_decoded)\r\n            \"\"\"image = image_preprocessing_fn(\r\n                image, \r\n                image_size,\r\n                image_size)\"\"\"\r\n            return image,label\r\n        \r\n        cpu_num = multiprocessing.cpu_count()\r\n        ds = ds.apply(\r\n            tf.contrib.data.map_and_batch(\r\n                classification_parse_line,\r\n                batch_size=batch_size,\r\n                num_parallel_batches=cpu_num))   \r\n        ds = ds.prefetch(None)\r\n        iterator = ds.make_initializable_iterator()\r\n   \r\n        iterator_initializer_hook.iterator_initializer_func = lambda sess: sess.run(iterator.initializer)\r\n        return ds\r\n    \r\n    return iterator_initializer_hook, inputs\r\n\r\n distribution = tf.contrib.distribute.MirroredStrategy()\r\n config = tf.estimator.RunConfig(\r\n        model_dir=args.model_dir,\r\n        tf_random_seed=912,\r\n        save_summary_steps=args.save_summary_steps,\r\n        save_checkpoints_steps=args.save_interval_steps,\r\n        keep_checkpoint_max=5*get_num_replicas(),\r\n        train_distribute=distribution,\r\n        session_config=session_config\r\n    )\r\n\r\n classifier = tf.estimator.Estimator(\r\n        my_model,\r\n        config=config,\r\n        params=params)\r\n    for epoch in range(args.num_epochs):\r\n        logger.info('Starting epoch %d / %d' % (\r\n            epoch + 1, args.num_epochs))\r\n        classifier.train(\r\n            train_ds,\r\n            hooks=[train_ds_hook])\r\n        classifier.evaluate(\r\n             val_ds,\r\n            hooks=[val_ds_hook])", "Nothing special, its just common code with MirroredStrategy. \r\nIf replace MirroredStrategy with OneDeviceStrategy, everything went well", "Nagging Assignees @rohan100jain, @guptapriya: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "As Derek mentioned, currently we don't have a mechanism for specifying if some outputs should be in host memory and we assume (to a large extent) that they'd be in device memory. Strings can't be in device memory, hence the bug. I shall work on having a dynamic method of identifying which outputs should be allocated on the host / device. Stay tuned for a fix in a bit.", "@rohan100jain can this issue be closed now that your fix has been merged? "]}, {"number": 19587, "title": "how to use tflite with arm compute library\uff1f", "body": "as the title says", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I also want to know it.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This is currently not yet supported, but we are hoping to look into it sometime.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19586, "title": "[TFlite]Not supported (ELU, ResizeNearestNeighbor ExpandDims)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**:1.7.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:no\r\n- **GCC/Compiler version (if compiling from source)**:no\r\n- **CUDA/cuDNN version**:no\r\n- **GPU model and memory**:no\r\n- **Exact command to reproduce**:yes\r\n\r\n### Describe the problem\r\nI tried to convert some TF model to \".tflite\" format. But I was encountered with \"No supporting operation\". \r\n\r\n\r\n### Source code / logs\r\n\r\ntensorflow/contrib/lite/toco/tflite/export.cc:304] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you hava a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Elu, ExpandDims, ResizeNearestNeighbor.\r\n\r\n", "comments": ["Could you use resize bilinear instead? That is currently supported?", "Usually we want to not add every option unless it is really needed, because are trying to keep our binary size very small. Every op costs more space.", "ResizBilinear or ResizeNearestNeighbor is quite essential to build recent network architecture...\r\nBecause Feature Pyramid Network architecture is commonly used recently...\r\nCould you have any plan to support Resize ops in TFLite?? Please...\r\nAccording to [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md), ResizeBilinear is still not supported yet.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Tracked here: https://github.com/tensorflow/tensorflow/issues/21526"]}, {"number": 19585, "title": "coco error ", "body": "\r\nNotFoundError: ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb; No such file or directory", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "thanks.\nNow, I am in trouble cuda . please help me :(\nImportError: libcublas.so.8.0: cannot open shared object file: No such file\nor directory\nFailed to load the native TensorFlow runtime.\n\n\nCuda 9.0\ncuDNN 7.0.5\n\n\u200b\n\n\n\n2018-06-26 15:51 GMT+03:00 Alfred Sorten Wolf <notifications@github.com>:\n\n> It has been 29 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19585#issuecomment-400294458>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGUyx145s857Vlxxum2Baqjb5D1HxTrNks5uAi5IgaJpZM4UPcjv>\n> .\n>\n\n\n\n-- \n\nSayg\u0131lar\u0131mla\nAy\u015fen \u00d6z\u00fcn T\u00dcRK\u00c7ET\u0130N\n", "computer OS -> Linux mint 17\ndisplay cart ->  VGA compatible controller: Advanced Micro Devices, Inc.\n[AMD/ATI] Whistler [Radeon HD 6630M/6650M/6750M/7670M/7690M] (rev ff)\n\n\n\n2018-06-27 0:13 GMT+03:00 Ay\u015fen \u00d6z\u00fcn T\u00fcrk\u00e7etin <aysenozun@gmail.com>:\n\n> thanks.\n> Now, I am in trouble cuda . please help me :(\n> ImportError: libcublas.so.8.0: cannot open shared object file: No such\n> file or directory\n> Failed to load the native TensorFlow runtime.\n>\n>\n> Cuda 9.0\n> cuDNN 7.0.5\n>\n> \u200b\n>\n>\n>\n> 2018-06-26 15:51 GMT+03:00 Alfred Sorten Wolf <notifications@github.com>:\n>\n>> It has been 29 days with no activity and the awaiting response label was\n>> assigned. Is this still an issue?\n>>\n>> \u2014\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/19585#issuecomment-400294458>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AGUyx145s857Vlxxum2Baqjb5D1HxTrNks5uAi5IgaJpZM4UPcjv>\n>> .\n>>\n>\n>\n>\n> --\n>\n> Sayg\u0131lar\u0131mla\n> Ay\u015fen \u00d6z\u00fcn T\u00dcRK\u00c7ET\u0130N\n>\n>\n\n\n-- \n\nSayg\u0131lar\u0131mla\nAy\u015fen \u00d6z\u00fcn T\u00dcRK\u00c7ET\u0130N\n", "Hi,\r\nhttps://www.quora.com/Does-CUDA-work-on-AMD-GPUs\r\n\r\nYou should use cpu mode i believe.", "thanks. But, I going to take a lot of time deep learning model. :(  do you\nnow another way ? Would it be troublesome set up the two together ? well\nboth GPU and CPU , do you now?\n\n2018-06-27 0:47 GMT+03:00 Cevat Bostanc\u0131o\u011flu <notifications@github.com>:\n\n> Hi,\n> https://www.quora.com/Does-CUDA-work-on-AMD-GPUs\n>\n> You should use cpu mode i believe.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19585#issuecomment-400472934>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGUyx_G-6UDcTC9REwLd1kEwiBzePt6-ks5uAqv1gaJpZM4UPcjv>\n> .\n>\n\n\n\n-- \n\nSayg\u0131lar\u0131mla\nAy\u015fen \u00d6z\u00fcn T\u00dcRK\u00c7ET\u0130N\n", "i can't clearly understand but ;\r\n--if you want to say it takes a long time with CPU training, yes it is and if you want to have compute power you should use gpu(mostly nvidia gpu if you dont want to deal with amd gpu software issues)\r\n-- i installed gpu & cpu support before but i am just newbie and that was like 1,5 years ago. they were working fine together.\r\n\r\nSo my basic recommendation is if its working, go with cpu mode.\r\n\r\nRegards.\r\n", "okey I can understand thanks :)\n\n2018-06-27 1:24 GMT+03:00 Cevat Bostanc\u0131o\u011flu <notifications@github.com>:\n\n> i can't clearly understand but ;\n> --if you want to say it takes a long time with CPU training, yes it is and\n> if you want to have compute power you should use gpu(mostly nvidia gpu if\n> you dont want to deal with amd gpu software issues)\n> -- i installed gpu & cpu support before but i am just newbie and that was\n> like 1,5 years ago. they were working fine together.\n>\n> So my basic recommendation is if its working, go with cpu mode.\n>\n> Regards.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19585#issuecomment-400481897>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGUyxysgbHjTTC5B6S8jgod7byjKgJsUks5uArSdgaJpZM4UPcjv>\n> .\n>\n\n\n\n-- \n\nSayg\u0131lar\u0131mla\nAy\u015fen \u00d6z\u00fcn T\u00dcRK\u00c7ET\u0130N\n", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please close this issue if it's addressed.\r\n\r\n"]}, {"number": 19583, "title": "[Documentation] No documentation for building TensorFlow with Windows through Bazel", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: dcb10b1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.13.1\r\n- **GCC/Compiler version (if compiling from source)**: Visual Studio C++ 2017\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: `bazel build -s --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\n\r\nBased on https://github.com/tensorflow/tensorflow/issues/18296#issuecomment-386685428, it looks like TensorFlow already supports Windows build with Bazel. However, there is no documentation about Windows + Bazel support, and my attempts have failed (see steps below).\r\n\r\nIt would be good to add documentation for building TensorFlow with Windows + Bazel. That will help greatly.\r\n\r\n\r\n### Source code / logs\r\n\r\nAs I don't have a Windows machine, I tried to build TensorFlow with the Virtual Machine of Window 10, provided by Microsoft https://developer.microsoft.com/en-us/windows/downloads/virtual-machines\r\n\r\nThe VM is **Windows 10 Enterprise (Evaluation - Build 201805)**. It  (Expire on 7/30/2018)**, and **Visual Studio 2017** has been pre-installed.\r\n\r\nThe VM image is deployed on macOS with VMware Fusion, and allocated `4 CPU + 8GB Memory`.\r\n\r\nHere are the steps I tried until I encountered the failure:\r\n1. Install msys2 (`msys2-x86_64-20161025.exe`) to `C:\\msys64`.\r\n2. Downlaod Bazel (`bazel-0.13.1-windows-x86_64.exe`) and place it to `C:\\Users\\user\\bazel.exe`. Add `C:\\Users\\user` to `PATH`.\r\n3. Set `BAZEL_VC` with `SET BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC`\r\n4. Verified `bazel build //examples/cpp:hello-world` works\r\n5. ~~Install python 3.6.5 (`python-3.6.5-amd64.exe`) to `C:\\Users\\Python36`~~\r\n   **Install Anaconda (Anaconda3-5.1.0-Windows-x86_64.exe) to C:\\Users\\Anaconda** (update)\r\n6. `git clone tensorflow`\r\n7.  `python configure.py` and always use the default value (return).\r\n8. `bazel build -s --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n*Update: Python 3.6 install could be replaced by Anaconda for numpy*\r\n\r\nBelow is the output:\r\n```\r\nC:\\Users\\user\\tensorflow>python configure.py\r\nYou have bazel 0.13.1 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\Python36\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\Python36\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\Python36\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]:\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]:\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]:\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n\r\nC:\\Users\\user\\tensorflow>\r\nC:\\Users\\user\\tensorflow>\r\nC:\\Users\\user\\tensorflow>bazel build -s --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\n..................\r\nWARNING: C:/users/user/_bazel_user/4opxzgxi/external/protobuf_archive/WORKSPACE:1: Workspace name in C:/users/user/_bazel_user/4opxzgxi/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nWARNING: C:/users/user/_bazel_user/4opxzgxi/external/absl_py/WORKSPACE:1: Workspace name in C:/users/user/_bazel_user/4opxzgxi/external/absl_py/WORKSPACE (@io_abseil_py) does not match the name given in the repository's definition (@absl_py); this will cause a build error in future versions\r\nERROR: C:/users/user/tensorflow/util/python/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n        File \"C:/users/user/tensorflow/third_party/py/python_configure.bzl\", line 291\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"C:/users/user/tensorflow/third_party/py/python_configure.bzl\", line 251, in _create_local_python_repository\r\n                _check_python_bin(repository_ctx, python_bin)\r\n        File \"C:/users/user/tensorflow/third_party/py/python_configure.bzl\", line 204, in _check_python_bin\r\n                _fail((\"--define %s='%s' is not execut...)))\r\n        File \"C:/users/user/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n                fail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Python36/python.exe' is not executable. Is it the python binary?\r\n and referenced by '//util/python:python_headers'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 17.944s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (126 packages loaded)\r\n\r\nC:\\Users\\user\\tensorflow>\r\n```", "comments": ["@yongtang Do you have Bash on Windows installed? You might hit https://github.com/tensorflow/tensorflow/issues/11735\r\nI happened to just sent a [PR ](https://github.com/tensorflow/tensorflow/pull/19598)to fix the issue. Can you try if it helps?\r\n\r\nSorry we don't have a specific document for building TF with Bazel on Windows for now. We'll definitely add one once TF fully migrates from CMake to Bazel on Windows. But generally, it shouldn't be much different from other platforms.", "Thanks @meteorcloudy for the help! The PR helps and the bash issue is resolved. However, after another run I encountered the following issue:\r\n```\r\nC:\\Users\\user\\tensorflow>bazel build -s --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\n....................\r\nWARNING: C:/users/user/_bazel_user/4opxzgxi/external/protobuf_archive/WORKSPACE:1: Workspace name in C:/users/user/_bazel_user/4opxzgxi/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nWARNING: C:/users/user/_bazel_user/4opxzgxi/external/absl_py/WORKSPACE:1: Workspace name in C:/users/user/_bazel_user/4opxzgxi/external/absl_py/WORKSPACE (@io_abseil_py) does not match the name given in the repository's definition (@absl_py); this will cause a build error in future versions\r\nERROR: C:/users/user/tensorflow/tensorflow/tools/pip_package/BUILD:117:1: no such package '@png_archive//': Traceback (most recent call last):\r\n        File \"C:/users/user/tensorflow/third_party/repo.bzl\", line 88\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/users/user/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/users/user/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/user/_bazel_user/4opxzgxi/external/png_archive -i C:/users/user/tensorflow/third_party/png_fix_rpi.patch':\r\nStdout:\r\nStderr: Timed out and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@png_archive//': Traceback (most recent call last):\r\n        File \"C:/users/user/tensorflow/third_party/repo.bzl\", line 88\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/users/user/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/users/user/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/user/_bazel_user/4opxzgxi/external/png_archive -i C:/users/user/tensorflow/third_party/png_fix_rpi.patch':\r\nStdout:\r\nStderr: Timed out\r\nINFO: Elapsed time: 78.668s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (139 packages loaded)\r\n\r\nC:\\Users\\user\\tensorflow>\r\n```\r\n\r\nIt seems bash is not able to invoke `patch`. I installed `patch` inside the `msys2` with `pacman`.\r\n\r\nShould I install `patch` to `PATH` or there are other configuration needed?", "@yongtang Looks like bash did found `patch`, so no need to add `patch` to `PATH`. The \"Timed out\" error seems strange to me. Can you try to run `patch -p1 -d C:/users/user/_bazel_user/4opxzgxi/external/png_archive -i C:/users/user/tensorflow/third_party/png_fix_rpi.patch` under MSYS, see what happens?", "Thanks @meteorcloudy. Under MSYS it seems to work fine:\r\n```\r\nuser@WinDev1804Eval MSYS ~\r\n$ patch -p1 -d C:/users/user/_bazel_user/4opxzgxi/external/png_archive -i C:/users/user/tensorflow/third_party/png_fix_rpi.patch\r\npatching file scripts/pnglibconf.h.prebuilt\r\n\r\nuser@WinDev1804Eval MSYS ~\r\n$\r\n```\r\n\r\nBut when invoking Bazel, the error still exists.", "Hmm, that's strange.\r\nHow about running\r\n```\r\nC:\\msys64\\usr\\bin\\bash.exe -l -c \"patch -p1 -d C:/users/user/_bazel_user/4opxzgxi/external/png_archive -i C:/users/user/tensorflow/third_party/png_fix_rpi.patch\"\r\n```\r\nunder cmd.exe?", "Ah I think the issue seems to be the quote:\r\n\r\nWithout quote:\r\n```\r\n\r\nC:\\Users\\user\\tensorflow>C:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/user/_bazel_user/4opxzgxi/external/png_archive -i C:/users/user/tensorflow/third_party/png_fix_rpi.patch\r\n```\r\n\r\n*Ctrl-C to break as it hangs*\r\n\r\nWith quote:\r\n```\r\nC:\\Users\\user\\tensorflow>C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch -p1 -d C:/users/user/_bazel_user/4opxzgxi/external/png_archive -i C:/users/user/tensorflow/third_party/png_fix_rpi.patch\"\r\npatching file scripts/pnglibconf.h.prebuilt\r\n\r\nC:\\Users\\user\\tensorflow>\r\n```", "@meteorcloudy After playing with different configurations and quotes, I am still having trouble to have bazel build moving forward. I am wondering if there are some MSYS related environment variable sv(like TERM or other env?) that needs to be set for bash.exe to work correctly?", "@yongtang Indeed this is strange, I have know idea why this is happening, I also guess it's some issue with MSYS. Can you try a newer MSYS version? eg. http://repo.msys2.org/distrib/x86_64/msys2-x86_64-20180531.exe", "i disabled in .....\\tensorflow\\third_party\\repo.bzl  lines  87-88 where it try apply `patch` command nonexisting on windows\r\n\r\n```\r\n#  if ctx.attr.patch_file != None:\r\n#    _apply_patch(ctx, ctx.attr.patch_file)\r\n```\r\n\r\nnow my repo.bzl looks like\r\n```\r\n# Copyright 2017 The TensorFlow Authors. All rights reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#    http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n\"\"\"Utilities for defining TensorFlow Bazel dependencies.\"\"\"\r\n\r\n_SINGLE_URL_WHITELIST = depset([\r\n    \"arm_compiler\",\r\n    \"ortools_archive\",\r\n])\r\n\r\ndef _is_windows(ctx):\r\n  return ctx.os.name.lower().find(\"windows\") != -1\r\n\r\ndef _wrap_bash_cmd(ctx, cmd):\r\n  if _is_windows(ctx):\r\n    bazel_sh = _get_env_var(ctx, \"BAZEL_SH\")\r\n    if not bazel_sh:\r\n      fail(\"BAZEL_SH environment variable is not set\")\r\n    cmd = [bazel_sh, \"-l\", \"-c\", \" \".join(cmd)]\r\n  return cmd\r\n\r\ndef _get_env_var(ctx, name):\r\n  if name in ctx.os.environ:\r\n    return ctx.os.environ[name]\r\n  else:\r\n    return None\r\n\r\n# Executes specified command with arguments and calls 'fail' if it exited with\r\n# non-zero code\r\ndef _execute_and_check_ret_code(repo_ctx, cmd_and_args):\r\n  result = repo_ctx.execute(cmd_and_args, timeout=10)\r\n  if result.return_code != 0:\r\n    fail((\"Non-zero return code({1}) when executing '{0}':\\n\" + \"Stdout: {2}\\n\"\r\n          + \"Stderr: {3}\").format(\" \".join(cmd_and_args), result.return_code,\r\n                                  result.stdout, result.stderr))\r\n\r\ndef _repos_are_siblings():\r\n  return Label(\"@foo//bar\").workspace_root.startswith(\"../\")\r\n\r\n# Apply a patch_file to the repository root directory\r\n# Runs 'patch -p1'\r\ndef _apply_patch(ctx, patch_file):\r\n  # Don't check patch on Windows, because patch is only available under bash.\r\n  if not _is_windows(ctx) and not ctx.which(\"patch\"):\r\n    fail(\"patch command is not found, please install it\")\r\n  cmd = _wrap_bash_cmd(\r\n    ctx, [\"patch\", \"-p1\", \"-d\", ctx.path(\".\"), \"-i\", ctx.path(patch_file)])\r\n  _execute_and_check_ret_code(ctx, cmd)\r\n\r\ndef _apply_delete(ctx, paths):\r\n  for path in paths:\r\n    if path.startswith(\"/\"):\r\n      fail(\"refusing to rm -rf path starting with '/': \" + path)\r\n    if \"..\" in path:\r\n      fail(\"refusing to rm -rf path containing '..': \" + path)\r\n  cmd = _wrap_bash_cmd(ctx, [\"rm\", \"-rf\"] + [ctx.path(path) for path in paths])\r\n  _execute_and_check_ret_code(ctx, cmd)\r\n\r\ndef _tf_http_archive(ctx):\r\n  if (\"mirror.bazel.build\" not in ctx.attr.urls[0] and\r\n      (len(ctx.attr.urls) < 2 and\r\n       ctx.attr.name not in _SINGLE_URL_WHITELIST)):\r\n    fail(\"tf_http_archive(urls) must have redundant URLs. The \" +\r\n         \"mirror.bazel.build URL must be present and it must come first. \" +\r\n         \"Even if you don't have permission to mirror the file, please \" +\r\n         \"put the correctly formatted mirror URL there anyway, because \" +\r\n         \"someone will come along shortly thereafter and mirror the file.\")\r\n  ctx.download_and_extract(\r\n      ctx.attr.urls,\r\n      \"\",\r\n      ctx.attr.sha256,\r\n      ctx.attr.type,\r\n      ctx.attr.strip_prefix)\r\n  if ctx.attr.delete:\r\n    _apply_delete(ctx, ctx.attr.delete)\r\n#  if ctx.attr.patch_file != None:\r\n#    _apply_patch(ctx, ctx.attr.patch_file)\r\n  if ctx.attr.build_file != None:\r\n    # Use BUILD.bazel to avoid conflict with third party projects with\r\n    # BUILD or build (directory) underneath.\r\n    ctx.template(\"BUILD.bazel\", ctx.attr.build_file, {\r\n        \"%prefix%\": \"..\" if _repos_are_siblings() else \"external\",\r\n    }, False)\r\n\r\ntf_http_archive = repository_rule(\r\n    implementation=_tf_http_archive,\r\n    attrs={\r\n        \"sha256\": attr.string(mandatory=True),\r\n        \"urls\": attr.string_list(mandatory=True, allow_empty=False),\r\n        \"strip_prefix\": attr.string(),\r\n        \"type\": attr.string(),\r\n        \"delete\": attr.string_list(),\r\n        \"patch_file\": attr.label(),\r\n        \"build_file\": attr.label(),\r\n    })\r\n\"\"\"Downloads and creates Bazel repos for dependencies.\r\n\r\nThis is a swappable replacement for both http_archive() and\r\nnew_http_archive() that offers some additional features. It also helps\r\nensure best practices are followed.\r\n\"\"\"\r\n```", "Is @MartinAbilev 's answer the accepted one at this point? How did that work for you, Martin?\r\n\r\nThanks,\r\nZach", "no i after installed `path` for windows   just not rnember how exactly ... :/   \r\n\r\nhttp://gnuwin32.sourceforge.net/packages/patch.htm\r\n\r\nor maybe\r\n\r\n```\r\n 0\r\ndown vote\r\n\r\nYou need to use the pacman package manager in your MSYS2 shell to install patch:\r\n\r\n    Open MSYS2 (from the Start Menu, or by running c:\\tools\\msys64\\msys2.exe)\r\n\r\n    Run this command:\r\n\r\n    pacman -Syu patch\r\n\r\n```", "Nagging Assignee @meteorcloudy: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A preview of the document on building TensorFlow with Bazel on Windows is currently available here:\r\nhttps://gist.github.com/meteorcloudy/8e5f1aab7c7fa87b16ae28e6f8fd3fd2\r\n\r\nIt will be published to the official site with the release of TensorFlow r1.11", "In case anyone is interested in a similar guide for RHEL: https://github.com/tensorflow/tensorflow/issues/22053", "> \u554a\uff0c\u6211\u8ba4\u4e3a\u95ee\u9898\u4f3c\u4e4e\u662f\u5f15\u7528\uff1a\r\n> \r\n> \u6ca1\u6709\u5f15\u7528\uff1a\r\n> \r\n> ```\r\n> \r\n> C:\\Users\\user\\tensorflow>C:\\msys64\\usr\\bin\\bash.exe -l -c patch -p1 -d C:/users/user/_bazel_user/4opxzgxi/external/png_archive -i C:/users/user/tensorflow/third_party/png_fix_rpi.patch\r\n> ```\r\n> _Ctrl-C\u5728\u6302\u8d77\u65f6\u4e2d\u65ad_\r\n> \r\n> \u62a5\u4ef7\uff1a\r\n> \r\n> ```\r\n> C:\\Users\\user\\tensorflow>C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch -p1 -d C:/users/user/_bazel_user/4opxzgxi/external/png_archive -i C:/users/user/tensorflow/third_party/png_fix_rpi.patch\"\r\n> patching file scripts/pnglibconf.h.prebuilt\r\n> \r\n> C:\\Users\\user\\tensorflow>\r\n> ```\r\nhow to slove this problem, thx?", "Can you try to reinstall `patch`? \r\n`C:\\msys64\\usr\\bin\\pacman -S patch`", "@taotaolin and @meteorcloudy I had the same problem on Windows 10 running the `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` command. It would fail on png archive patch. \r\n\r\nThe root cause of the failure is that Bazel should quote the patch command, without the quotes the `bash -l -c patch ...` command just hangs and never returns.\r\n\r\nHowever with the quotes, the command completes and returns successfully. `bash -l -c \"patch ...\"`.\r\n\r\nI could not figure out how to hack Bazel to quote the patch command. \r\n\r\nIf you manually patch the png_archive file, you can comment out the `patch_file = clean_dep(\"//third_party:png_fix_rpi.patch\"), ` line  in the tensorflow/tensorflow/workspace.bzl file. Then you are not held back by the patch failure anymore.\r\n\r\nTo comment out a line in bzl file, use `#`.\r\n\r\n", "@shakeel Please find the solution here: https://github.com/tensorflow/tensorflow/issues/22761#issuecomment-428796593\r\n\r\nI'm also submitting a fix for this issue.", "Thanks for fixing the bash command in Bazel and for increasing the timeout. I was getting timed out everywhere on Windows. I thought it was because of my encrypted filesystem."]}, {"number": 19582, "title": "Can any one help me in resolving this issue?? I'm using keras and tensorflow", "body": "Using TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"app.py\", line 32, in <module>\r\n    model, graph = init()\r\n  File \"/home/jitendra/cnn-hand-written-digit-master/model/load.py\", line 12, in init\r\n    loaded_model = model_from_json(loaded_model_json)\r\n  File \"build/bdist.linux-x86_64/egg/keras/engine/saving.py\", line 369, in model_from_json\r\n  File \"build/bdist.linux-x86_64/egg/keras/layers/__init__.py\", line 55, in deserialize\r\n  File \"build/bdist.linux-x86_64/egg/keras/utils/generic_utils.py\", line 145, in deserialize_keras_object\r\n  File \"build/bdist.linux-x86_64/egg/keras/engine/sequential.py\", line 293, in from_config\r\n  File \"build/bdist.linux-x86_64/egg/keras/engine/sequential.py\", line 166, in add\r\n  File \"build/bdist.linux-x86_64/egg/keras/engine/base_layer.py\", line 414, in __call__\r\n  File \"build/bdist.linux-x86_64/egg/keras/engine/base_layer.py\", line 279, in assert_input_compatibility\r\n  File \"build/bdist.linux-x86_64/egg/keras/backend/tensorflow_backend.py\", line 469, in is_keras_tensor\r\n  File \"build/bdist.linux-x86_64/egg/keras/backend/tensorflow_backend.py\", line 477, in is_tensor\r\nAttributeError: 'module' object has no attribute '_TensorLike'\r\n", "comments": ["I guess it has nothing to do with the tensorflow repo. I would recommend to ask this question on stackoverflow. \r\n\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) (thanks @unnir!) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19581, "title": "Can any one help me in resolving this issue?? I'm using keras and tensorflow", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 19580, "title": "Fix typo", "body": "fix typo\r\n\r\nwithin in -> within\r\n\r\na a -> a\r\n", "comments": []}, {"number": 19579, "title": "Turn some sessionmanager/dataset info messages to debug.", "body": "Since TF1.7, many logging seems to have been added and set to info, e.g.:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/session_manager.py\r\n```py\r\nlogging.info(\"Running local_init_op.\")\r\nsess.run(self._local_init_op)\r\nlogging.info(\"Done running local_init_op.\")\r\n```\r\n\r\nAnd \r\nhttps://github.com/tensorflow/tensorflow/blob/b12c3bb1157245adf6230a2e045831348f679b5b/tensorflow/python/training/monitored_session.py\r\n```py\r\nlogging.info('Graph was finalized.')\r\n```\r\n\r\nSame for dataset shuffle buffer:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/shuffle_dataset_op.cc\r\n```cpp\r\nLOG(INFO) << \"Filling up shuffle buffer (this may take a while): \"\r\n  << num_elements_ << \" of \" << dataset()->buffer_size_;\r\n```\r\n\r\nThis leads to a lot of clutter. Should this be cleaned up? The shuffle buffer f.e. might be a nice message, but \"graph was finalized\" and \"running init op\" and \"done running init op\" seem to be should be debug.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19578, "title": "Latest NonMaxSuppression breaks backward compatiblity", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nThe latest changes https://github.com/tensorflow/tensorflow/commit/1a300437cecfae36f7584694dac523851f1cd931 to the NonMaxSuppression op have two issues:\r\n\r\n1. It breaks backward compatibility. It removes all boxes whose scores are below a threshold, and the threshold is default to 0. However, the old API does not require that the scores have to be >= 0. Boxes with negative scores will be removed by default after this change.\r\nOn [my implementation of Mask RCNN](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/FasterRCNN) this decreases mAP by 0.2.\r\n\r\n2. It has a potential bug. This loop:\r\nhttps://github.com/tensorflow/tensorflow/blob/1a300437cecfae36f7584694dac523851f1cd931/tensorflow/core/kernels/non_max_suppression_op.cc#L137-L142\r\n\r\ntriggers an integer underflow when `selected` is empty, because `size()` is unsigned. It happens to cause no bugs in this case, but should better be fixed. If the loop order is reversed like `for (int j = 0; j <= selected.size() - 1; ++j)`, this would become a bug immediately.\r\n\r\nEDIT: From https://en.cppreference.com/w/cpp/language/implicit_conversion:\r\n\r\n> If the destination type is signed, the value does not change if the source integer can be represented in the destination type. Otherwise the result is implementation-defined. (Note that this is different from signed integer arithmetic overflow, which is undefined).\r\n\r\nSo the above code relies on implementation-defined behavior.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "(1) was fixed in https://github.com/tensorflow/tensorflow/commit/d5a71ef8355ebb8d983c5e7f5b1e840cf2d00c17\r\n\r\nClosing since (2) is not an actual bug."]}, {"number": 19577, "title": "Fixed typo in exporter.py", "body": "", "comments": ["Thanks"]}, {"number": 19576, "title": "Fix issue #19575", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Looks already fixed by, https://github.com/tensorflow/tensorflow/pull/19574#pullrequestreview-124077982\r\n\r\nThanks though"]}, {"number": 19575, "title": "[BUG] Failed to build from source because of missing #include directive", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8.0, tf.GIT_VERSION is 'unknown' (I compiled from commit dcb10b1d557168646204239bea6ca5bf1abc40a3)\r\n- **Python version**: Python 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.13.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nWhen compiling tensorflow from source following the instructions here (https://www.tensorflow.org/install/install_sources#build_the_pip_package), I encountered the following C++ error:\r\n```\r\nERROR: /path/to/tensorflow/tensorflow/python/BUILD:310:1: C++ compilation of rule '//tensorflow/python:cpp_python_util' failed (Exit 1)\r\ntensorflow/python/util/util.cc:276:16: error: 'function' in namespace 'std' does not name a template type\r\n     const std::function<int(PyObject*)>& is_sequence_helper,\r\n                ^~~~~~~~\r\ntensorflow/python/util/util.cc:276:24: error: expected ',' or '...' before '<' token\r\n     const std::function<int(PyObject*)>& is_sequence_helper,\r\n                        ^\r\ntensorflow/python/util/util.cc: In function 'bool tensorflow::swig::{anonymous}::FlattenHelper(PyObject*, PyObject*, int)':\r\ntensorflow/python/util/util.cc:280:16: error: 'is_sequence_helper' was not declared in this scope\r\n   int is_seq = is_sequence_helper(nested);\r\n                ^~~~~~~~~~~~~~~~~~\r\ntensorflow/python/util/util.cc:280:16: note: suggested alternative: 'IsSequenceHelper'\r\n   int is_seq = is_sequence_helper(nested);\r\n                ^~~~~~~~~~~~~~~~~~\r\n                IsSequenceHelper\r\n[... other C++ mumbling ...]\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n\r\n### Source code / logs\r\nAdding ``#include <functional>`` to the file solves the issue.\r\n", "comments": ["I also had this issue compiling on an Arch machine with similar software versions as above.", "This has been fixed by commit 2561c4000afcead84823ebead70498533e5ebebb.  Closing."]}]