[{"number": 27957, "title": "Fix tensorrt bazel configuration file typo", "body": "incude -> include\r\n\r\nThis is breaking our CI", "comments": ["@jerryyin can you please check build failures", "There's a fix pending for these failures, they're internal. Will ping here once the fix is landed to retry the tests.", "See also #27901", "Closing because duplicate got merged"]}, {"number": 27956, "title": "All Diagrams Missing", "body": "**System information**\r\n- TensorFlow version: N/A\r\n- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/guide/premade_estimators.md\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/guide/custom_estimators.md\r\n\r\n**Describe the documentation issue**\r\nAll of the diagrams / images are missing from both of the links above. \r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nNo.  I don't know what the diagrams were supposed to be.", "comments": ["@KevlarTheGreat We will take a look at it. Thanks!", "@KevlarTheGreat Thanks for finding these links. I also cannot see the figures in those two links.Thanks!", "THis was updated: https://www.tensorflow.org/guide/estimator\r\n\r\nPlease reopen if image is still not found."]}, {"number": 27955, "title": "TF Nightly 20190418 Crashes on Estimator Import", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below): **tf-nightly-20190418**\r\n- Python version: **3.6**\r\n\r\n**Describe the current behavior**\r\nNightly tensorflow crashes on importing estimator-nightly\r\n\r\n**Describe the expected behavior**\r\nAble to import\r\n\r\n**Code to reproduce the issue**\r\n```\r\npip install tf-nightly==1.14.1.dev20190418\r\nimport tensorflow\r\n```\r\n**Other info / logs**\r\nhttps://colab.research.google.com/drive/1mvVTcAWHwJY1Kko49Ae93iQum6lT9W8D\r\n\r\nCrash:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py in <module>()\r\n     32 from tensorflow._api.v1 import autograph\r\n     33 from tensorflow._api.v1 import bitwise\r\n---> 34 from tensorflow._api.v1 import compat\r\n     35 from tensorflow._api.v1 import config\r\n     36 from tensorflow._api.v1 import data\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v1/compat/__init__.py in <module>()\r\n     19 from __future__ import print_function as _print_function\r\n     20 \r\n---> 21 from tensorflow._api.v1.compat import v1\r\n     22 from tensorflow._api.v1.compat import v2\r\n     23 from tensorflow.python.compat.compat import forward_compatibility_horizon\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v1/compat/v1/__init__.py in <module>()\r\n    641     parent_package_str=__name__,\r\n    642     child_package_str=(\r\n--> 643         'tensorflow_estimator.python.estimator.api._v1.estimator'))\r\n    644 _component_api_helper.package_hook(\r\n    645     parent_package_str=__name__,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/component_api_helper.py in package_hook(parent_package_str, child_package_str, error_msg)\r\n     54   parent_pkg = importlib.import_module(parent_package_str)\r\n     55   try:\r\n---> 56     child_pkg = importlib.import_module(child_package_str)\r\n     57   except ImportError:\r\n     58     if error_msg:\r\n\r\n/usr/lib/python3.6/importlib/__init__.py in import_module(name, package)\r\n    124                 break\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n    128 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/__init__.py in <module>()\r\n      6 from __future__ import print_function as _print_function\r\n      7 \r\n----> 8 from tensorflow_estimator._api.v1 import estimator\r\n      9 _names_with_underscore = []\r\n     10 __all__ = [_s for _s in dir() if not _s.startswith('_')]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/_api/v1/estimator/__init__.py in <module>()\r\n      6 from __future__ import print_function as _print_function\r\n      7 \r\n----> 8 from tensorflow_estimator._api.v1.estimator import experimental\r\n      9 from tensorflow_estimator._api.v1.estimator import export\r\n     10 from tensorflow_estimator._api.v1.estimator import inputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py in <module>()\r\n     33 if not isinstance(_sys.modules[__name__], _deprecation_wrapper.DeprecationWrapper):\r\n     34   _sys.modules[__name__] = _deprecation_wrapper.DeprecationWrapper(\r\n---> 35       _sys.modules[__name__], \"estimator.experimental\", _DEPRECATED_TO_CANONICAL)\r\n\r\nTypeError: __init__() takes 3 positional arguments but 4 were given\r\n```", "comments": ["I have the same error while using tensorflowjs converter!", "I saw the same issue. Wonder why build process did not catch this issue?", "I had thesame problem. For now installing an older version such as: \r\npip install -q tf-nightly-gpu-2.0-preview==2.0.0.dev20190416\r\nsolved allowed me to run my models. ", "I have the same issue on macOS.\r\n\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS 10.14.4**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **tf-nightly-2.0-preview==2.0.0.dev20190418**\r\n- Python version: **3.6.8**", "It's been already fixed by https://github.com/tensorflow/tensorflow/commit/e84d155152ab5a187d2c0e4c5eea6a73df3789e1 but the nightly process for estimator ran with the code before this commit and the nightly process for tensorflow ran with the code after.", "For now, if you want a quick fix, you can edit the `tensorflow/python/util/deprecation_wrapper.py` file to have:\r\n\r\n```python\r\nclass DeprecationWrapper(types.ModuleType):\r\n  \"\"\"Wrapper for TensorFlow modules to support deprecation messages.\"\"\"\r\n\r\n  def __init__(self, wrapped, module_name, _unused_arg={}):  # pylint: disable=super-on-old-class\r\n```\r\n\r\n(just add the `_unused_arg={}` argument). Though this is just patching, and will have to be undone on the next nightly.", "Can we rerun the nightly process for estimator so that user don't have to patch and revert? Also since most of library rely on tf from PIP install, which is hard to patch every time.", "Unfortunately if we launch the nightly process now we will have issues with automatic tooling on the next nightly, so probably it's better to either patch or use an older nightly.", "Fixed in 20190419 nightly. Thanks!"]}, {"number": 27954, "title": "Tensorflow 2.0 has missing features related to checkpointing Dataset", "body": "**System information**\r\n- TensorFlow version: 2.0-alpha.0\r\n- Doc Link:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/make_saveable_from_iterator\r\n\r\n**Describe the documentation issue**\r\nWith 2.0-alpha.0, tf.data.Dataset does not have any make_xx_iterator() methods, so I can't save/restore iterate state while training by using tf.data.experimental.make_saveable_from_iterator().\r\n\r\nHow to save/restore internal state of Dataset?\r\n\r\n", "comments": ["It seems that iterator from Dataset.__iter__() can be serialized with tf.train.Checkpoint(). Even though it is not described in document.\r\n\r\nBut if I used tf.py_function in Dataset chain, it throws error \"Saving stateful functions is not supported yet\". It seems that old tf.py_func operator has stateful flag, but tf.py_function doesn't have.", "Checkpointing of stateful functions is not supported.\r\n\r\nAs far as I can tell from looking at the [implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/script_ops.py) of `py_function` and `numpy_function`. `tf.py_function` is unconditionally stateless while `tf.numpy_function` is unconditionally stateful. So the error you are describing does not match the expected behavior of the code.\r\n\r\nIf you would like further help, please provide a reproducible example.", "Here is reproducible sample.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimage_paths = ['test.png']\r\n\r\n\r\ndef my_map_py(x):\r\n    return x\r\n\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(image_paths)\r\ndataset = dataset.map(lambda x: tf.py_function(my_map_py, [x], [tf.string]))\r\n\r\niterator = dataset.__iter__()\r\n\r\ncheckpoint = tf.train.Checkpoint(iterator=iterator)\r\ncheckpoint.save('checkpoint.tf')\r\n```", "Thank you for the reproducible example.\r\n\r\nI took a closer look at the [implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/script_ops.py) of `py_function` and it turns out it is implemented using the `EagerPyFunc` op, which is also stateful. In other words, you will not be able to checkpoint an input pipeline that contains `numpy_function` or py_function`.", "Thanks for invedtigation. I divided my dataset into multiple datasets and manually save checkpoint via its offset."]}, {"number": 27953, "title": "tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TFLite_Detection_PostProcess' in binary", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 1.9\r\n- Python version: python 3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: GeForce RTX 2080 Ti/PCIe/SSE2  memory:15.6 GiB\r\n\r\nI am trying to convert tflite_graph.pb to .tflite from terminal. Earlier I was using tensorflow 1.3 and used the below command to convert:\r\n\r\n`tflite_convert  --graph_def_file=/home/user/Desktop/FaceDetection/frozen_tflite/tflite_graph.pb --output_file=/home/user/Desktop/FaceDetection/frozen_tflite/coverted_frozen_graph.tflite --input_arrays=normalized_input_image_tensor --output_arrays=\"detection_boxes\",\"detection_scores\",\"detection_classes\",\"num_detections\" --input_shape=1,300,300,3 --allow_custom_ops=True\r\n` \r\n\r\nThat time I got an error that tflite_convert command not found. After digging few git issues and stackover flow, I found that this will work with **tensorflow 1.9**\r\n\r\nI used the pip install -upgrade tensorflow==1.9 to upgrade the tf.\r\n\r\nIt ran with the following error:\r\n`2019-04-18 17:24:39.216037: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 320, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 316, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 91, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 81, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 204, in from_frozen_graph\r\n    import_graph_def(graph_def, name=\"\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py\", line 418, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TFLite_Detection_PostProcess' in binary running on user. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`\r\n\r\n**tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TFLite_Detection_PostProcess' in binary**\r\n\r\nCan anyone help me to solve this issue?\r\n\r\nThank in adv.\r\n", "comments": ["@jyotirmayghosh Can you please upgrade tensorflow to  1.11 as one user mentioned in the  [link](https://github.com/tensorflow/models/issues/5277) that Upgrading to 1.11 worked.", "Yes I have solved this issue.\r\nThanks", "How did you solve it? Did upgrading to tensorflow 1.11 help? I'm currently using tensorflow 1.14.0 and I'm facing a similar issue.", "> How did you solve it? Did upgrading to tensorflow 1.11 help? I'm currently using tensorflow 1.14.0 and I'm facing a similar issue.\r\n\r\nMe too", "> How did you solve it? Did upgrading to tensorflow 1.11 help? I'm currently using tensorflow 1.14.0 and I'm facing a similar issue.\r\n\r\nYep same problem but using 1.15 tensorflow "]}, {"number": 27952, "title": "Array conv_23/Conv_1/BatchNorm/FusedBatchNorm, which is an input to the Conv operator producing the output array conv_3//0/Conv/Relu6, is lacking min/max data, which is necessary for quantization. ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):conda install tensorflow-gpu==1.13\r\n- TensorFlow version (use command below):1.13\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):NO\r\n- GCC/Compiler version (if compiling from source):NO\r\n- CUDA/cuDNN version: CUDA9.0 cuDNN7.1\r\n- GPU model and memory: p40\r\n\r\n\r\n**Describe the current behavior**\r\nI quantized training mobilefacenet,   and want to conver pb file to tflite file but failed\r\n\r\ntoco --output_file=face_freeze.tflite --graph_def_file=face_freeze.pb --input_arrays=x_i\r\nnput  --output_arrays=BatchNorm/Reshape_1 --output_format=TFLITE --inference_typ\r\ne=QUANTIZED_UINT8 --mean_values=127  --std_dev_values=128\r\n\r\n**Code to reproduce the issue**\r\n`def model(is_training , x, y, is_quantize ):\r\n\r\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                        activation_fn = tf.nn.relu6,\r\n                        weights_initializer=slim.variance_scaling_initializer(factor=3.0, mode='FAN_IN', uniform=True),\t\t\t\t\t\r\n                        biases_initializer = tf.constant_initializer(0.0), \t\t\t\t\t\t\r\n                        normalizer_fn=slim.batch_norm,    \r\n                        normalizer_params={'is_training': is_training, 'epsilon':0.001, 'scale': True, 'decay':0.9, 'updates_collections': tf.GraphKeys.UPDATE_OPS}):\r\n\r\n        print('x',x.get_shape())        \r\n        # conv_1        \r\n        conv_1 = slim.conv2d(x, 64, [3, 3], stride=2, padding='SAME',  scope='conv_1')\r\n        print('conv_1',conv_1.get_shape())\r\n\r\n        # conv_2\r\n        conv_2 = Residual(conv_1, num_block=2, num_out=64, _kernel=3,  _stride=1, pad='SAME', num_group=64, name='conv_2')         \r\n        print('conv_2',conv_2.get_shape())\r\n\r\n       # conv_23\r\n        conv_23 = DResidual(conv_2, num_out=128, _kernel=3 , _stride=2, pad='SAME', num_group=128, name='conv_23')\r\n        print('conv_23',conv_23.get_shape())\r\n\r\n       # conv_3\r\n        conv_3 = Residual(conv_23, num_block=6, num_out=128, _kernel=3, _stride=1, pad='SAME', num_group=128, name='conv_3')\r\n        print('conv_3',conv_3.get_shape())\r\n        .....\r\n        .....\r\n\t\t\r\n        #softmax loss\r\n            cross_entropy__ = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n                labels=y, logits=fc7, name='cross_entropy_per_example')\r\n        cross_entropy = tf.reduce_mean(cross_entropy__, name='cross_entropy')\r\n\r\n        all_variable_decay = _all_variables_decapy(weight_decay,1.0)\t\t\r\n        total_loss = cross_entropy + all_variable_decay \r\n\t\t\t\r\n        tf.contrib.quantize.create_training_graph()`\r\n\r\nI use tf.contrib.quantize.create_eval_graph() to create eval graph pb file\r\n\r\n\r\n**Other info / logs**\r\n2019-04-18 19:21:14.747016: F tensorflow/lite/toco/tooling_util.cc:1702] Array c\r\nonv_23/Conv_1/BatchNorm/FusedBatchNorm, which is an input to the Conv operator p\r\nroducing the output array conv_3//0/Conv/Relu6, is lacking min/max data, which i\r\ns necessary for quantization. If accuracy matters, either target a non-quantized\r\n output format, or run quantized training with your model from a floating point\r\ncheckpoint to change the input graph to contain min/max information. If you don'\r\nt care about accuracy, you can pass --default_ranges_min= and --default_ranges_m\r\nax= for easy experimentation.\r\n\r\nLooking forward to receive your reply,  thanks very much", "comments": ["I am facing a similar issue with Cifarnet as well. I have trained the cifarnet model with quantization awareness. The model even converts into frozen model. During the conversion with tflite_converter, I get the following error:\r\n\r\n```\r\n2019-05-05 08:56:39.657151: F tensorflow/lite/toco/tooling_util.cc:1702] Array CifarNet/norm1, which is an input to the Conv operator producing the output array CifarNet/conv2/Relu, is lacking min/max data, which is necessary for quantization. \r\nIf accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. \r\nIf you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nAborted (core dumped)\r\n```\r\nAre there certain ops for which fake quant node needs to be manually added? Any insight into this will be of great help.\r\n\r\nThanks.", "@sxsxsx Is this still an issue?\r\nWe see that you are using an older version of TF(1.x) which is not actively supported.We recommend you to kindly upgrade to TF v2.4 or later and let us know the outcome? Please refer to this [thread ](https://stackoverflow.com/questions/63681168/batch-normalization-quantize-tensorflow-1-x-does-not-have-minmax-information) and let us know if it helps?\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27952\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27952\">No</a>\n"]}, {"number": 27951, "title": "tf.nn.batch_normalization unexpected behavior", "body": "tf.VERSION :  1.13.0-rc2\r\nOS              : MacOS Mojave 10.14.4 on MacBook Pro (13-inch, 2017)\r\n\r\nI encountered a problem trying to implement tf.nn.batch_normalization working on the mnist digits dataset.\r\nFor testing purposes, I implemented a very simple network\r\nL1 : 100 neurons fully connected layer + Batch norm + Sigmoid\r\nL2:   10 neurons fully connected layer + softmax\r\n\r\nWhen I use tf.nn.batch_normalization \r\n`Ybn1   = tf.nn.batch_normalization(Yl1, m1, v1, O1, S1, 1e-5)` it leads to divergence:\r\n\r\n![image](https://user-images.githubusercontent.com/44782534/56353537-df10f380-61d1-11e9-9060-749a04f55cb4.png)\r\n\r\nIf I do the math by myself it does converge:\r\n`Yhat1 = (Yl1 - m1) / tf.sqrt(v1 + 1e-5)`\r\n` Ybn1  = S1 * Yhat1 + O1`\r\n\r\n![image](https://user-images.githubusercontent.com/44782534/56353771-6eb6a200-61d2-11e9-9d4f-08a6096091ea.png)\r\n\r\nIf I implement the content of batch_normalization function within my code, it does not work either.\r\n`inv = math_ops.rsqrt(v1 + 1e-5)`\r\n`inv *= S1`\r\n`Ybn1 =  Yl1 * math_ops.cast(inv, Yl1.dtype) + math_ops.cast(O1 - m1 * inv, Yl1.dtype)`\r\n\r\nbut if I combine the 2 last lines it works correctly:\r\n `inv = math_ops.rsqrt(v1 + 1e-5)`\r\n`Ybn1 =  (Yl1 - m1) * inv * S1+ O1`\r\n\r\nI have certainly done something wrong, but I cannot figure out what, and if it is a real bug I just wanted to let you know.\r\n\r\nHere is the entire code if you want to reproduce the issue:\r\nhttps://github.com/neodelphis/tensorflow-without-a-phd-french/blob/master/mnist_test.ipynb\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not clear that this is an issue with TensorFlow. There is also a larger community that reads questions there. I would, however, recommend that you try to use `tf.keras.layers.BatchNormalization`, as keras layers are the recommended API moving forward.\r\n\r\nIf you think we've misinterpreted a bug, or if you're able to narrow the cause of the errant behavior down and determine that it is indeed an issue with TensorFlow, please comment again with a clear explanation as to why.Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27951\">No</a>\n"]}, {"number": 27950, "title": "Update docstrings in `configure.py`", "body": "HDFS is now supported through bazel build config and not through a flag in this file (`configure.py`).\r\n\r\nHowever, the docstrings still refer to it. Replaced it with `TF_NEED_CUDA`, which is still a flag set in this script", "comments": []}, {"number": 27949, "title": "Tensorflow 2.0: No gradients provided for any variable but only when tf.math.square AND tf.function is used?", "body": "I posted this in SO\r\n\r\nhttps://stackoverflow.com/questions/55730930/tensorflow-2-0-no-gradients-provided-for-any-variable-but-only-when-tf-math-squ\r\n\r\nbut now am wondering if it is actually a bug. \r\n\r\n```\r\nIn [176]: sys.version\r\nOut[176]: '3.7.3 (default, Mar 27 2019, 16:54:48) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]'\r\n\r\nIn [177]: tf.__version__\r\nOut[177]: '2.0.0-alpha0'\r\n```\r\n\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\n\r\nm = 1000\r\nn = 1\r\nX = np.random.randn(m, n).astype(np.float32)\r\ny = (3 + 0 * np.random.randn(m)).astype(np.float32)\r\n\r\ndef create_model():\r\n    a_input = keras.layers.Input(shape=(n,), dtype=np.float32)\r\n    a = K.expand_dims(a_input, axis=2)\r\n    q = keras.layers.Conv1D(1, 1)(a)\r\n    q = - tf.math.square(q) # this breaks things, but only when using tf.function\r\n    model = keras.models.Model(inputs=a_input, outputs=q)\r\n    return model\r\n\r\nmodel = create_model()\r\nmodel.predict(X)\r\n\r\nclass Trainer():\r\n    def __init__(self, epochs=10):\r\n        self.epochs = epochs\r\n        self.model = create_model()\r\n        self.optimizer = tf.optimizers.Adam()\r\n        self.step = 0\r\n    def train(self, X, y, epochs=10):\r\n        X = tf.convert_to_tensor(X, dtype=tf.float32)\r\n        y = tf.convert_to_tensor(y, dtype=tf.float32)\r\n        for epoch in range(epochs):\r\n            l = self._train_one_step(X, y)\r\n        return l\r\n    @tf.function\r\n    def _train_one_step(self, X, y):\r\n        with tf.GradientTape() as tape:\r\n            yp = self.model(X)\r\n            loss = tf.reduce_mean(tf.math.square(y - yp))\r\n        gradients = tape.gradient(loss, self.model.trainable_variables)\r\n        l = self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\r\n        d = dict(loss=loss)\r\n        tf.print(yp[0], loss)\r\n        self.step += 1\r\n\r\ntrainer = Trainer()\r\nl = trainer.train(X, y, epochs=100)\r\n```\r\n\r\n\r\n```\r\nlib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)\r\n    922   if not filtered:\r\n    923     raise ValueError(\"No gradients provided for any variable: %s.\" %\r\n--> 924                      ([v.name for _, v in grads_and_vars],))\r\n    925   if vars_with_empty_grads:\r\n    926     logging.warning(\r\n\r\nValueError: No gradients provided for any variable: ['conv1d_47/kernel:0', 'conv1d_47/bias:0'].\r\n```", "comments": ["@fchollet I think what's happening here is that the tf.math.square not wrapped in a lambda layer is messing the functional model's ability to be called, so self.model(X) leaves unconnected inputs and outputs.", "I had the same issue with the tf.math.pow function as well. But the issue disappears if you put the tf.math operation inside the function with the @tf.function annotation. \r\n", "@robieta @omalleyt12 I think this is related to another issue you're working on, can you check?", "@cottrell could you try this with the 2.0 nightly? i think this should be fixed", "@omalleyt12 Could you please tell me how to install the 2-0 nightly? Should I build it from source or I can just install it just by `pip install` ?", "@jiarenyf I think:\r\n\r\nhttps://pypi.org/project/tf-nightly-2.0-preview/\r\n\r\nbut it might not yet work in new python 3.7 and up. See posts like this one if you hit problems in your python version\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/28691", "@cottrell Thank you, I could install the `tf-nightly-up-2.0-preview` in python 3.7 and ubuntu 18.04 and solve the problem of `No gradients provided for any variable`.", "I have having same issue with `tf.math.sqrt` along with MirroredStrategy and @tf.function", "Hi @mdasadul is this issue occurring with the latest nightly?", "@omalleyt12  yes. I intalled tf nightly-gpu (2.0.0-dev20190522). And my case is quite interesting. It's happening only when I am using mirrored-strategy. I think it's something to do with mirrored strategy and @tf.function ", "@mdasadul thanks for reporting this, could you provide a minimal example that reproduces this?", "I was trying to create a minimal example but couldn't recreate the issue with minimal example. I can share my code if you share your email address \r\n\r\nThanks ", "@omalleyt12 Here is what the minimal example looks like although I am getting **NotImplementedError**  when using distributed strategy. \r\n\r\nThe model looks like this\r\n```\r\n!pip install -q tf-nightly-gpu-2.0-preview\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\r\nfrom tensorflow.keras import Model\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n# Add a channels dimension\r\nx_train = x_train[..., tf.newaxis]\r\nx_test = x_test[..., tf.newaxis]\r\n\r\ntrain_ds = tf.data.Dataset.from_tensor_slices(\r\n    (x_train, y_train)).shuffle(10000).batch(32)\r\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\r\n\r\n\r\nclass MyModel(Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.conv1 = Conv2D(32, 3, activation='relu')\r\n    self.flatten = Flatten()\r\n    self.d1 = Dense(128, activation='relu')\r\n    self.d2 = Dense(10, activation='softmax')\r\n\r\n  def call(self, x):\r\n    x = self.conv1(x)\r\n    x = self.flatten(x)\r\n    x = self.d1(x)\r\n    return self.d2(x)\r\n\r\n```\r\nThe training class without distributed strategy looks like as follows and I was able to save the model without distributed strategy\r\n```\r\nclass Train(object):\r\n  def __init__(self,epochs, enable_function, batch_size, per_replica_batch_size):\r\n    self.epochs = epochs\r\n    self.enable_function = enable_function\r\n    self.batch_size = batch_size\r\n    self.per_replica_batch_size = per_replica_batch_size\r\n    self.learning_rate =  CustomSchedule(10)\r\n    self.model = MyModel()\r\n    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n     reduction=tf.keras.losses.Reduction.SUM)\r\n    self.optimizer = tf.keras.optimizers.Adam()\r\n    self.train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n    self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n\r\n    self.test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n    self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\r\n    \r\n  \r\n  def loss_function(self, real, pred):\r\n      mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n      loss_ = self.loss_object(real, pred)\r\n      mask = tf.cast(mask, dtype=loss_.dtype)\r\n      loss_ *= mask\r\n      return tf.reduce_sum(loss_) * 1./self.batch_size\r\n\r\n  def train_step(self, inputs):\r\n    images, labels = inputs\r\n    with tf.GradientTape() as tape:\r\n      predictions = self.model(images)\r\n      loss = self.loss_function(labels, predictions)\r\n    gradients = tape.gradient(loss, self.model.trainable_variables)\r\n    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\r\n\r\n    self.train_loss(loss)\r\n    self.train_accuracy(labels, predictions)\r\n    \r\n  def test_step(self, inputs):\r\n    images, labels = inputs\r\n    predictions = self.model(images)\r\n    t_loss = self.loss_function(labels, predictions)\r\n\r\n    self.test_loss(t_loss)\r\n    self.test_accuracy(labels, predictions)\r\n    \r\n  def training_loop(self, train_dataset, test_dataset):\r\n    if self.enable_function:\r\n      self.train_step=tf.function(self.train_step)\r\n      self.test_step=tf.function(self.test_step)\r\n    for epoch in range(self.epochs):\r\n      self.train_loss.reset_states()\r\n      self.test_loss.reset_states()\r\n      self.train_accuracy.reset_states()\r\n      self.test_accuracy.reset_states()\r\n\r\n      for images, labels in train_dataset:\r\n        self.train_step((images, labels))\r\n      for test_images,test_labels in test_dataset:\r\n        self.test_step((test_images, test_labels))\r\n\r\n      template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n      print (template.format(epoch+1,\r\n                             self.train_loss.result(),\r\n                             self.train_accuracy.result()*100,\r\n                             self.test_loss.result(),\r\n                             self.test_accuracy.result()*100))\r\n      \r\n      \r\n      \r\n \r\nepochs=5\r\nenable_function=True\r\nbatch_size=128\r\nper_replica_batch_size=128\r\ntrain_obj=Train(epochs, enable_function, batch_size, per_replica_batch_size)\r\ntrain_obj.training_loop(train_ds, test_ds)\r\ntf.saved_model.save(train_obj.model,'model')\r\n```\r\n\r\nWhen I am adding distributed strategy as follows I am getting ** NotImplementedError**\r\n```\r\nclass DistributedTrain(Train):\r\n  def __init__(self,epochs, enable_function, batch_size, per_replica_batch_size):\r\n    Train.__init__(self,epochs, enable_function, batch_size, per_replica_batch_size)\r\n      \r\n  def training_loop(self, train_iterator, test_iterator, \r\n                   num_train_steps_per_epoch, num_test_steps_per_epoch,\r\n                   strategy):\r\n    def distributed_train():\r\n      return strategy.experimental_run(self.train_step, train_iterator)\r\n\r\n    def distributed_test():\r\n      return strategy.experimental_run(self.test_step, test_iterator)\r\n\r\n    if self.enable_function:\r\n      distributed_train = tf.function(distributed_train)\r\n      distributed_test = tf.function(distributed_test)\r\n\r\n    template = 'Epoch: {}, Train Loss: {}, Test Loss: {}'\r\n    for epoch in range(self.epochs):\r\n      self.train_loss.reset_states()\r\n      self.test_loss.reset_states()\r\n      self.train_accuracy.reset_states()\r\n      self.test_accuracy.reset_states()\r\n\r\n      train_iterator.initialize()\r\n      for _ in range(num_train_steps_per_epoch):\r\n        distributed_train()\r\n\r\n      test_iterator.initialize()\r\n      for _ in range(num_test_steps_per_epoch):\r\n        distributed_test()\r\n\r\n      template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n      print (template.format(epoch+1,\r\n                               self.train_loss.result(),\r\n                               self.train_accuracy.result()*100,\r\n                               self.test_loss.result(),\r\n                               self.test_accuracy.result()*100))\r\n\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nnum_replicas = strategy.num_replicas_in_sync\r\n\r\nwith strategy.scope():\r\n  num_train_steps_per_epoch = tf.data.experimental.cardinality(train_ds)\r\n  num_test_steps_per_epoch = tf.data.experimental.cardinality(test_ds)\r\n\r\n  train_iterator = strategy.make_dataset_iterator(train_ds)\r\n  test_iterator = strategy.make_dataset_iterator(test_ds)\r\n  \r\n  train_obj= DistributedTrain(epochs, enable_function, batch_size, per_replica_batch_size)\r\n  train_obj.training_loop(train_iterator,\r\n                          test_iterator,\r\n                                   num_train_steps_per_epoch,\r\n                                   num_test_steps_per_epoch,\r\n                                   strategy)\r\n  tf.saved_model.save(train_obj.model,'dist-model')\r\n```\r\n\r\nThanks so much", "mdasadul@ can you post the stack trace of the error you are seeing? ", "@anj-s @omalleyt12  Here is the stack trace \r\n\r\n```\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-33-a2c05e070639> in <module>()\r\n     55                                    num_test_steps_per_epoch,\r\n     56                                    strategy)\r\n---> 57   tf.saved_model.save(train_obj.model,'dist-model')\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures)\r\n    833   object_saver = util.TrackableSaver(checkpoint_graph_view)\r\n    834   asset_info, exported_graph = _fill_meta_graph_def(\r\n--> 835       meta_graph_def, saveable_view, signatures)\r\n    836   saved_model.saved_model_schema_version = (\r\n    837       constants.SAVED_MODEL_SCHEMA_VERSION)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in _fill_meta_graph_def(meta_graph_def, saveable_view, signature_functions)\r\n    529   resource_initializer_ops = []\r\n    530   with exported_graph.as_default():\r\n--> 531     object_map, resource_map, asset_info = saveable_view.map_resources()\r\n    532     for resource_initializer_function in resource_initializer_functions:\r\n    533       asset_dependencies = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in map_resources(self)\r\n    250         self.captured_tensor_node_ids[obj.resource_handle] = node_id\r\n    251       elif resource_variable_ops.is_resource_variable(obj):\r\n--> 252         new_variable = resource_variable_ops.copy_to_graph_uninitialized(obj)\r\n    253         object_map[obj] = new_variable\r\n    254         resource_map[obj.handle] = new_variable.handle\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in copy_to_graph_uninitialized(var)\r\n   1794       dtype=var.dtype,\r\n   1795       name=var._shared_name,\r\n-> 1796       synchronization=var.synchronization,\r\n   1797       aggregation=var.aggregation,\r\n   1798       extra_handle_data=var.handle)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in synchronization(self)\r\n    512   @property\r\n    513   def synchronization(self):\r\n--> 514     raise NotImplementedError\r\n    515 \r\n    516   @property\r\n\r\nNotImplementedError: \r\n```", "Thanks mdasadul@ seemuch@ is working on adding support for saving a model under distribution strategy scope using the save_model.save() API.  seemuch@ Can you update this issue once the fix is in? Thanks!", "@anj-s can you provide workaround to save model in distribution mode for current tf version? My main goal is convert tf model (subclassed model of tf.keras.Model which was trained using custom loop) to tf lite. How can I do it currently without tf.saved_model.save API?", "I believe this issue is fixed in the latest 2.0 nightly. \r\nAlthough, the `tf.saved_model.save(train_obj.model,'dist-model')` line should be called out side of the distribution scope. ", "I am going to close this issue for now, since it should be fixed. \r\nIf there is any more updates, feel free to let me know. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27949\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27949\">No</a>\n", "@seemuch thank for update! If you can, please, look at this issue https://stackoverflow.com/questions/56739596/how-to-convert-trained-in-custom-loop-subclassed-tf-keras-model-to-tflite ? I can't understand how I can convert subclassed tf.keras.Model to SavedModel due to the following warning: `Skipping full serialization of Keras model, because its inputs are not defined.`", "> Although, the tf.saved_model.save(train_obj.model,'dist-model') line should be called out side of the distribution scope.\r\n\r\nIt's not the true, because I got the next error:\r\n`RuntimeError: Need to be inside \"with strategy.scope()\" for <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fd50d505ef0>`", "Hitting a similar issue with \r\n\r\nValueError: Num gradients 0 generated for op name: \"nvp_layer_5/StatefulPartitionedCall\"\r\nop: \"StatefulPartitionedCall\"\r\ninput: \"S\"\r\ninput: \"B\"\r\ninput: \"nvp_layer_5/StatefulPartitionedCall/args_2\"\r\ninput: \"nvp_layer_5/StatefulPartitionedCall/args_3\"\r\ninput: \"nvp_layer_5/StatefulPartitionedCall/args_4\"\r\nattr {\r\n  key: \"Tin\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n      type: DT_FLOAT\r\n      type: DT_RESOURCE\r\n      type: DT_RESOURCE\r\n      type: DT_RESOURCE\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"Tout\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"_gradient_op_type\"\r\n  value {\r\n    s: \"PartitionedCall-4641\"\r\n  }\r\n}\r\nattr {\r\n  key: \"config\"\r\n  value {\r\n    s: \"\"\r\n  }\r\n}\r\nattr {\r\n  key: \"config_proto\"\r\n  value {\r\n    s: \"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0002\\002J\\0008\\001\"\r\n  }\r\n}\r\nattr {\r\n  key: \"executor_type\"\r\n  value {\r\n    s: \"\"\r\n  }\r\n}\r\nattr {\r\n  key: \"f\"\r\n  value {\r\n    func {\r\n      name: \"__inference_call_4640\"\r\n    }\r\n  }\r\n}\r\n\r\n\r\nIs there some easy workaround beside not using tf.function? I've tried constructing most things inside tf.function call but can't seem to get around this one. I have a feeling some update of tf rc0 or tfp broke it.\r\n\r\nI can try to open a new issue with a minimal example if it will help but I think things might be changing in the versions so not sure it is worth it.", "This is a separate issue. Can you file a separate bug with instructions to\nreproduce?\n\nOn Thu, Aug 29, 2019 at 2:52 PM David Cottrell <notifications@github.com>\nwrote:\n\n> Hitting a similar issue with\n>\n> ValueError: Num gradients 0 generated for op name:\n> \"nvp_layer_5/StatefulPartitionedCall\"\n> op: \"StatefulPartitionedCall\"\n> input: \"S\"\n> input: \"B\"\n> input: \"nvp_layer_5/StatefulPartitionedCall/args_2\"\n> input: \"nvp_layer_5/StatefulPartitionedCall/args_3\"\n> input: \"nvp_layer_5/StatefulPartitionedCall/args_4\"\n> attr {\n> key: \"Tin\"\n> value {\n> list {\n> type: DT_FLOAT\n> type: DT_FLOAT\n> type: DT_RESOURCE\n> type: DT_RESOURCE\n> type: DT_RESOURCE\n> }\n> }\n> }\n> attr {\n> key: \"Tout\"\n> value {\n> list {\n> type: DT_FLOAT\n> }\n> }\n> }\n> attr {\n> key: \"_gradient_op_type\"\n> value {\n> s: \"PartitionedCall-4641\"\n> }\n> }\n> attr {\n> key: \"config\"\n> value {\n> s: \"\"\n> }\n> }\n> attr {\n> key: \"config_proto\"\n> value {\n> s: \"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0002\\002J\\0008\\001\"\n> }\n> }\n> attr {\n> key: \"executor_type\"\n> value {\n> s: \"\"\n> }\n> }\n> attr {\n> key: \"f\"\n> value {\n> func {\n> name: \"__inference_call_4640\"\n> }\n> }\n> }\n>\n> Is there some easy workaround beside not using tf.function? I've tried\n> constructing most things inside tf.function call but can't seem to get\n> around this one. I have a feeling some update of tf rc0 or tfp broke it.\n>\n> I can try to open a new issue with a minimal example if it will help but I\n> think things might be changing in the versions so not sure it is worth it.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27949?email_source=notifications&email_token=AAABHRKJVUBUS2XG2IFJHS3QHBALJA5CNFSM4HG3NY3KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5P5NJA#issuecomment-526374564>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMDZ3IZLRLP3YRBU4LQHBALJANCNFSM4HG3NY3A>\n> .\n>\n\n\n-- \n - Alex\n", " I am getting this error again, I have one encoder model and output of that I am passing to three different decoders and adding loss of all three and then doing backward propagation. But I am getting this error \r\n\r\n`<ipython-input-66-94a2de13802b>:61 train_step  *\r\n        optimizer.apply_gradients(zip(gradients, variables))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:427 apply_gradients\r\n        grads_and_vars = _filter_grads(grads_and_vars)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:1025 _filter_grads\r\n        ([v.name for _, v in grads_and_vars],))\r\n\r\n    ValueError: No gradients provided for any variable: ['encoder/embedding/embeddings:0', 'encoder/gru/kernel:0', 'encoder/gru/recurrent_kernel:0', 'encoder/gru/bias:0', 'decoder_a/embedding_1/embeddings:0', 'decoder_a/gru_1/kernel:0', 'decoder_a/gru_1/recurrent_kernel:0', 'decoder_a/gru_1/bias:0', 'decoder_a/dense_3/kernel:0', 'decoder_a/dense_3/bias:0', 'decoder_a/bahdanau_attention_1/dense_4/kernel:0', 'decoder_a/bahdanau_attention_1/dense_4/bias:0', 'decoder_a/bahdanau_attention_1/dense_5/kernel:0', 'decoder_a/bahdanau_attention_1/dense_5/bias:0', 'decoder_a/bahdanau_attention_1/dense_6/kernel:0', 'decoder_a/bahdanau_attention_1/dense_6/bias:0', 'decoder_b/embedding_2/embeddings:0', 'decoder_b/gru_2/kernel:0', 'decoder_b/gru_2/recurrent_kernel:0', 'decoder_b/gru_2/bias:0', 'decoder_b/dense_7/kernel:0', 'decoder_b/dense_7/bias:0', 'decoder_b/bahdanau_attention_2/dense_8/kernel:0', 'decoder_b/bahdanau_attention_2/dense_8/bias:0', 'decoder_b/bahdanau_attention_2/dense_9/kernel:0', 'decoder_b/bahdanau_attention_2/dense_9/bias:0', 'decoder_b/bahdanau_attention_2/dense_10/kernel:0', 'decoder_b/bahdanau_attention_2/dense_10/bias:0', 'decoder_c/embedding_3/embeddings:0', 'decoder_c/gru_3/kernel:0', 'decoder_c/gru_3/recurrent_kernel:0', 'decoder_c/gru_3/bias:0', 'decoder_c/dense_11/kernel:0', 'decoder_c/dense_11/bias:0', 'decoder_c/bahdanau_attention_3/dense_12/kernel:0', 'decoder_c/bahdanau_attention_3/dense_12/bias:0', 'decoder_c/bahdanau_attention_3/dense_13/kernel:0', 'dec...`\r\n\r\n@seemuch ", "I have same problem.\r\nValueError: No gradients provided for any variable: ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'].\r\n\r\nI have a tf.pow() in the computation. Others are just tf.add, tf.matmul, tf.sigmoid. So I guess this tf.pow() or tf.math.pow() causes the problem. The gradient chain is broken.", "I am also getting the same issue while writing custom layer functionality ", "@hansong999 @UdiBhaskar can you open separate issues with the instructions to reproduce your problems?", "I have the same problem, my model consists of three layers of dnn, and uses a cross-entropy loss for multi-tasks, finally, add these multi-tasks loss for train, but i had just got an error like that:### _ValueError: No gradients provided for any variable_\r\n\r\nclass model():\r\n    .........\r\n    def loss(self, outputs, task_labels):\r\n        losses = []\r\n        ctr_label = tf.cast(task_labels[..., 0], tf.float32)\r\n        ctr = tf.sigmoid(outputs[..., 0])\r\n        ctr_loss = tf.keras.losses.binary_crossentropy(ctr_label, ctr)\r\n        losses.append(ctr_loss)\r\n        for i in range(1, self.num_classes):\r\n            task_label = tf.cast(task_labels[..., i], tf.float32) * ctr_label\r\n            cvr = tf.sigmoid(outputs[..., i])\r\n            task_loss = tf.keras.losses.binary_crossentropy(task_label, ctr * cvr)\r\n            losses.append(task_loss)\r\n        return losses\r\n\r\ntasks_losses = model.loss(tasks_outputs, train_task_labels)\r\ndef loss_all_func():\r\n     return tf.add_n(tasks_losses)\r\njoin_op=tf.keras.optimizers.Adamax(0.001).minimize(loss_all_func,var_list=tf.compat.v1.trainable_variables())", "Please file a new bug for your issue."]}, {"number": 27948, "title": "Automatic conversion from CONV_2D to DEPTHWISE_CONV_2D when input channl is 1 in tflite", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nmacos 10.14.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNone\r\n- TensorFlow installed from (source or binary):\r\nconda install\r\n- TensorFlow version (use command below):\r\ntensorflow 1.13\r\n- Python version:\r\npython 3.6.8\r\n- Bazel version (if compiling from source):\r\nNone\r\n- GCC/Compiler version (if compiling from source):\r\nNone\r\n- CUDA/cuDNN version:\r\nNone\r\n- GPU model and memory:\r\nNone\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nI was trying to implement some super resolution models in tensorflow and tflite, and evaluate their speed using tflite benchmark tool. when i use 1 channel input, e.g. input = tf.placeholder(tf.float32, [1, img_h, img_w, 1]), the first convolution layer automatically became DEPTHWISE_CONV_2D in the benchmark, even if i called \"slim.convolution2d\" function, and the speed was extremely slow. Then i simply changed the input channel to be 3, e.g. input = tf.placeholder(tf.float32, [1, img_h, img_w, 3]), the first convolution in benchmark became CONV_2D as normal and the speed became much faster.\r\n\r\nbelow are some information printed out by the benchmark.\r\n\r\nfirst time, input was \"inputs = tf.placeholder(tf.float32, [1, img_h, img_w, 1])\"\r\n\r\n```\r\nInitialized session in 253.8ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\ncount=1 curr=980043\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=50 first=948543 curr=944078 min=944078 max=955061 avg=947350 std=2060\r\n\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t       DEPTHWISE_CONV_2D\t            0.000\t  538.139\t  536.830\t 56.668%\t 56.668%\t     0.000\t        1\t[srnetwork/Conv/Relu]\r\n\t                 CONV_2D\t          536.833\t   20.081\t   20.083\t  2.120%\t 58.788%\t     0.000\t        1\t[srnetwork/Conv_1/Relu]\r\n\t                 CONV_2D\t          556.918\t   36.430\t   36.419\t  3.844%\t 62.632%\t     0.000\t        1\t[srnetwork/Conv_2/Relu]\r\n\t                 CONV_2D\t          593.339\t   36.507\t   36.473\t  3.850%\t 66.482%\t     0.000\t        1\t[srnetwork/Conv_3/Relu]\r\n\t                 CONV_2D\t          629.814\t   36.657\t   36.581\t  3.862%\t 70.344%\t     0.000\t        1\t[srnetwork/Conv_4/Relu]\r\n\t                 CONV_2D\t          666.397\t   36.532\t   36.718\t  3.876%\t 74.220%\t     0.000\t        1\t[srnetwork/Conv_5/Relu]\r\n\t                 CONV_2D\t          703.117\t   55.629\t   55.648\t  5.874%\t 80.094%\t     0.000\t        1\t[srnetwork/Conv_6/Relu]\r\n\t                 CONV_2D\t          758.768\t  185.664\t  185.694\t 19.602%\t 99.696%\t     0.000\t        1\t[srnetwork/Conv_7/Conv2D]\r\n\t                   SPLIT\t          944.465\t    2.726\t    2.722\t  0.287%\t 99.983%\t     0.000\t        1\t[srnetwork/split, srnetwork/split:1]\r\n\t           CONCATENATION\t          947.188\t    0.089\t    0.092\t  0.010%\t 99.993%\t     0.000\t        1\t[srnetwork/concat]\r\n\t                 RESHAPE\t          947.280\t    0.063\t    0.065\t  0.007%\t100.000%\t     0.000\t        1\t[srnetwork/Reshape]\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t       DEPTHWISE_CONV_2D\t            0.000\t  538.139\t  536.830\t 56.668%\t 56.668%\t     0.000\t        1\t[srnetwork/Conv/Relu]\r\n\t                 CONV_2D\t          758.768\t  185.664\t  185.694\t 19.602%\t 76.270%\t     0.000\t        1\t[srnetwork/Conv_7/Conv2D]\r\n\t                 CONV_2D\t          703.117\t   55.629\t   55.648\t  5.874%\t 82.144%\t     0.000\t        1\t[srnetwork/Conv_6/Relu]\r\n\t                 CONV_2D\t          666.397\t   36.532\t   36.718\t  3.876%\t 86.020%\t     0.000\t        1\t[srnetwork/Conv_5/Relu]\r\n\t                 CONV_2D\t          629.814\t   36.657\t   36.581\t  3.862%\t 89.882%\t     0.000\t        1\t[srnetwork/Conv_4/Relu]\r\n\t                 CONV_2D\t          593.339\t   36.507\t   36.473\t  3.850%\t 93.732%\t     0.000\t        1\t[srnetwork/Conv_3/Relu]\r\n\t                 CONV_2D\t          556.918\t   36.430\t   36.419\t  3.844%\t 97.576%\t     0.000\t        1\t[srnetwork/Conv_2/Relu]\r\n\t                 CONV_2D\t          536.833\t   20.081\t   20.083\t  2.120%\t 99.696%\t     0.000\t        1\t[srnetwork/Conv_1/Relu]\r\n\t                   SPLIT\t          944.465\t    2.726\t    2.722\t  0.287%\t 99.983%\t     0.000\t        1\t[srnetwork/split, srnetwork/split:1]\r\n\t           CONCATENATION\t          947.188\t    0.089\t    0.092\t  0.010%\t 99.993%\t     0.000\t        1\t[srnetwork/concat]\r\n\r\nNumber of nodes executed: 11\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t       DEPTHWISE_CONV_2D\t        1\t   536.830\t    56.668%\t    56.668%\t     0.000\t        1\r\n\t                 CONV_2D\t        7\t   407.612\t    43.028%\t    99.696%\t     0.000\t        7\r\n\t                   SPLIT\t        1\t     2.722\t     0.287%\t    99.984%\t     0.000\t        1\r\n\t           CONCATENATION\t        1\t     0.091\t     0.010%\t    99.993%\t     0.000\t        1\r\n\t                 RESHAPE\t        1\t     0.065\t     0.007%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=948517 curr=944053 min=944053 max=955032 avg=947325 std=2060\r\nMemory (bytes): count=0\r\n11 nodes observed\r\n```\r\n\r\n\r\nsecond time, input was \"inputs = tf.placeholder(tf.float32, [1, img_h, img_w, 3])\"\r\nplease note that nothing else except the input channel was changed.\r\n```\r\nInitialized session in 257.992ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\ncount=1 curr=559357\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=50 first=510275 curr=520996 min=501413 max=591696 avg=519391 std=16622\r\n\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t                 CONV_2D\t            0.000\t   96.923\t   97.608\t 18.794%\t 18.794%\t     0.000\t        1\t[srnetwork/Conv/Relu6]\r\n\t                 CONV_2D\t           97.611\t   19.989\t   20.090\t  3.868%\t 22.662%\t     0.000\t        1\t[srnetwork/Conv_1/Relu6]\r\n\t                 CONV_2D\t          117.703\t   37.846\t   38.136\t  7.343%\t 30.005%\t     0.000\t        1\t[srnetwork/Conv_2/Relu6]\r\n\t                 CONV_2D\t          155.844\t   37.351\t   38.488\t  7.411%\t 37.416%\t     0.000\t        1\t[srnetwork/Conv_3/Relu6]\r\n\t                 CONV_2D\t          194.335\t   37.632\t   37.826\t  7.283%\t 44.699%\t     0.000\t        1\t[srnetwork/Conv_4/Relu6]\r\n\t                 CONV_2D\t          232.163\t   38.839\t   37.950\t  7.307%\t 52.006%\t     0.000\t        1\t[srnetwork/Conv_5/Relu6]\r\n\t                 CONV_2D\t          270.116\t   54.885\t   55.838\t 10.751%\t 62.757%\t     0.000\t        1\t[srnetwork/Conv_6/Relu6]\r\n\t                 CONV_2D\t          325.956\t  183.861\t  190.501\t 36.680%\t 99.437%\t     0.000\t        1\t[srnetwork/Conv_7/Conv2D]\r\n\t                   SPLIT\t          516.461\t    2.701\t    2.727\t  0.525%\t 99.962%\t     0.000\t        1\t[srnetwork/split, srnetwork/split:1]\r\n\t           CONCATENATION\t          519.189\t    0.135\t    0.102\t  0.020%\t 99.982%\t     0.000\t        1\t[srnetwork/concat]\r\n\t                 RESHAPE\t          519.292\t    0.082\t    0.094\t  0.018%\t100.000%\t     0.000\t        1\t[srnetwork/Reshape]\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t                 CONV_2D\t          325.956\t  183.861\t  190.501\t 36.680%\t 36.680%\t     0.000\t        1\t[srnetwork/Conv_7/Conv2D]\r\n\t                 CONV_2D\t            0.000\t   96.923\t   97.608\t 18.794%\t 55.474%\t     0.000\t        1\t[srnetwork/Conv/Relu6]\r\n\t                 CONV_2D\t          270.116\t   54.885\t   55.838\t 10.751%\t 66.225%\t     0.000\t        1\t[srnetwork/Conv_6/Relu6]\r\n\t                 CONV_2D\t          155.844\t   37.351\t   38.488\t  7.411%\t 73.636%\t     0.000\t        1\t[srnetwork/Conv_3/Relu6]\r\n\t                 CONV_2D\t          117.703\t   37.846\t   38.136\t  7.343%\t 80.979%\t     0.000\t        1\t[srnetwork/Conv_2/Relu6]\r\n\t                 CONV_2D\t          232.163\t   38.839\t   37.950\t  7.307%\t 88.286%\t     0.000\t        1\t[srnetwork/Conv_5/Relu6]\r\n\t                 CONV_2D\t          194.335\t   37.632\t   37.826\t  7.283%\t 95.569%\t     0.000\t        1\t[srnetwork/Conv_4/Relu6]\r\n\t                 CONV_2D\t           97.611\t   19.989\t   20.090\t  3.868%\t 99.437%\t     0.000\t        1\t[srnetwork/Conv_1/Relu6]\r\n\t                   SPLIT\t          516.461\t    2.701\t    2.727\t  0.525%\t 99.962%\t     0.000\t        1\t[srnetwork/split, srnetwork/split:1]\r\n\t           CONCATENATION\t          519.189\t    0.135\t    0.102\t  0.020%\t 99.982%\t     0.000\t        1\t[srnetwork/concat]\r\n\r\nNumber of nodes executed: 11\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t                 CONV_2D\t        8\t   516.432\t    99.438%\t    99.438%\t     0.000\t        8\r\n\t                   SPLIT\t        1\t     2.727\t     0.525%\t    99.963%\t     0.000\t        1\r\n\t           CONCATENATION\t        1\t     0.101\t     0.019%\t    99.982%\t     0.000\t        1\r\n\t                 RESHAPE\t        1\t     0.093\t     0.018%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=510244 curr=520968 min=501382 max=591666 avg=519359 std=16622\r\nMemory (bytes): count=0\r\n11 nodes observed\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@achandraa \r\nplease refer to this example code \r\nhttps://drive.google.com/open?id=1JE76oSM6msAykSDz4Lq4GReZLk572EVC\r\nwhen inputs = tf.placeholder(tf.float32, [1, img_h, img_w, 3]), the first convolution layer of generated tflite file is conv2d. but when inputs = tf.placeholder(tf.float32, [1, img_h, img_w, 1]),  the first convolution layer automatically became depthwise_conv2d even if nothing else is changed", "You have to update the ```data_format``` arg depending on the input channel.\r\nhttps://github.com/tensorflow/tensorflow/blob/c03a552e4af852b8fde7d55331dbc3c487e60c6b/tensorflow/contrib/layers/python/layers/layers.py#L968", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27948\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27948\">No</a>\n", "@ymodak would you please tell me how to update the ```data_format``` arg? I remember it has 'NCHW' and 'NHWC' mode, and tflite only support NHWC.", "@SystemErrorWang  have resolve this case?"]}, {"number": 27947, "title": "Update README.md", "body": "Grammar", "comments": ["Thank you for your contribution , changes have been made by different commit."]}, {"number": 27946, "title": "Update README.md", "body": "Grammar", "comments": ["Thanks for your contribution , closing this issue as this change no longer needed."]}, {"number": 27945, "title": "Update RELEASE.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27945) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27945) for more info**.\r\n\r\nI Signed It!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27945) for more info**.\n\n<!-- ok -->", "@Tyorden thank your for your contribution , can you please combine these PRs #27946 & #27947 here ", "> @Tyorden thank your for your contribution , can you please combine these PRs #27946 & #27947 here\r\n\r\nHello, Unfortunately it appears I am not able to merge the pull requests?", "Hello, It appears as though I, unfortunately, do not have the available\npermission to merge the pull requests?\n\nOn Thu, Apr 18, 2019 at 12:52 PM rthadur <notifications@github.com> wrote:\n\n> @Tyorden <https://github.com/Tyorden> thank your for your contribution ,\n> can you please combine these PRs #27946\n> <https://github.com/tensorflow/tensorflow/pull/27946> & #27947\n> <https://github.com/tensorflow/tensorflow/pull/27947> here\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27945#issuecomment-484610149>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEV7DLSRSUVWC763LLWWR4LPRCYOXANCNFSM4HG2LGJA>\n> .\n>\n", "> Hello, It appears as though I, unfortunately, do not have the available permission to merge the pull requests?\r\n> [\u2026](#)\r\n> On Thu, Apr 18, 2019 at 12:52 PM rthadur ***@***.***> wrote: @Tyorden <https://github.com/Tyorden> thank your for your contribution , can you please combine these PRs #27946 <#27946> & #27947 <#27947> here \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#27945 (comment)](https://github.com/tensorflow/tensorflow/pull/27945#issuecomment-484610149)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEV7DLSRSUVWC763LLWWR4LPRCYOXANCNFSM4HG2LGJA> .\r\n\r\nyou can include changes of other PR here", "Changes are already pushed by different commit , so closing this PR, thank you for your contribution."]}, {"number": 27944, "title": "The kernel appears to have died. It will restart automatically. with memory problem?", "body": "1\r\n\r\n\r\nkernel died after running some code \r\nI try to run the code to generate a sample image with the generator I tried to update the conda and Jupiter but none of them worked \r\n\r\nI keep watching the memory usage of GPU but it does not use the GPU that much \r\n\r\ntensorflow2.0 , ubuntu 18.10, cuda 10.0 \r\npython 3.5,\r\n\r\ndef make_generator_model():\r\n    model = tf.keras.Sequential()\r\n    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\r\n    model.add(layers.BatchNormalization())\r\n    model.add(layers.LeakyReLU())\r\n\r\n    model.add(layers.Reshape((7, 7, 256)))\r\n    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\r\n\r\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\r\n    assert model.output_shape == (None, 7, 7, 128)\r\n    model.add(layers.BatchNormalization())\r\n    model.add(layers.LeakyReLU())\r\n\r\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\r\n    assert model.output_shape == (None, 14, 14, 64)\r\n    model.add(layers.BatchNormalization())\r\n    model.add(layers.LeakyReLU())\r\n\r\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\r\n    assert model.output_shape == (None, 28, 28, 1)\r\n\r\n    return model\r\ngenerator = make_generator_model()\r\n\r\nnoise = tf.random.normal([1, 100])\r\ngenerated_image = generator(noise, training=False)\r\n[I 10:20:06.664 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports WARNING:root:kernel 4406ce3b-1b5b-4ef8-aba9-d5fd9ed129e7 restarted 2019-04-18 10:20:21.002451: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1 2019-04-18 10:20:21.081020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1589] Found device 0 with properties: name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582 pciBusID: 0000:42:00.0 totalMemory: 11.91GiB freeMemory: 340.69MiB 2019-04-18 10:20:21.081054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1712] Adding visible gpu devices: 0 2019-04-18 10:20:21.081382: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2019-04-18 10:20:21.107510: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de6ead0990 executing computations on platform CUDA. Devices: 2019-04-18 10:20:21.107562: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): TITAN Xp, Compute Capability 6.1 2019-04-18 10:20:21.127890: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3493050000 Hz 2019-04-18 10:20:21.129460: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de6eed7eb0 executing computations on platform Host. Devices: 2019-04-18 10:20:21.129503: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): , 2019-04-18 10:20:21.129616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1712] Adding visible gpu devices: 0 2019-04-18 10:20:21.129722: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2019-04-18 10:20:21.130785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-04-18 10:20:21.130807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] 0 2019-04-18 10:20:21.130819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1139] 0: N 2019-04-18 10:20:21.131090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1260] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 115 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:42:00.0, compute capability: 6.1) 2019-04-18 10:20:24.168083: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 2019-04-18 10:20:24.331094: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7 2019-04-18 10:20:24.789774: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 2019-04-18 10:20:24.791468: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 2019-04-18 10:20:24.791484: F tensorflow/core/kernels/conv_grad_input_ops.cc:949] Check failed: stream->parent()->GetConvolveBackwardDataAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo(stream->parent()), &algorithms) [I 10:20:27.669 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports WARNING:root:kernel 4406ce3b-1b5b-4ef8-aba9-d5fd9ed129e7 restarted", "comments": ["@SlowMonk This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 27943, "title": "`GetCudaLaunchConfig` configuration of number of blocks", "body": "the definition is:\r\n```\r\ninline CudaLaunchConfig GetCudaLaunchConfig(int work_element_count,\r\n                                            const Eigen::GpuDevice& d) {\r\n  CHECK_GT(work_element_count, 0);\r\n  CudaLaunchConfig config;\r\n  const int virtual_thread_count = work_element_count;\r\n  const int physical_thread_count = std::min(\r\n      d.getNumGpuMultiProcessors() * d.maxGpuThreadsPerMultiProcessor(),\r\n      virtual_thread_count);\r\n  const int thread_per_block = std::min(1024, d.maxGpuThreadsPerBlock());\r\n  const int block_count =\r\n      std::min(DivUp(physical_thread_count, thread_per_block),\r\n               d.getNumGpuMultiProcessors());\r\n\r\n  config.virtual_thread_count = virtual_thread_count;\r\n  config.thread_per_block = thread_per_block;\r\n  config.block_count = block_count;\r\n  return config;\r\n}\r\n```\r\n\r\nwhy is the number of blocks is capped at `getNumGpuMultiProcessors`?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "sorry for the lack of clarity. I am asking the design decision behind only launching as many blocks as `d.getNumGpuMultiProcessors` in this [file](https://github.com/tensorflow/tensorflow/blob/5d34d064d3163c451c90b318a501792dcafee600/tensorflow/core/util/gpu_launch_config.h#L125). ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 27942, "title": "Why \"Don't build XLA with -ffast-math\"?", "body": "The most problem is not as title.\r\n\r\nIt is ridiculous that the error is thrown out after thousands of files were compiled.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n"]}, {"number": 27941, "title": "RNN compatibility issue on unknown batch_size", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colaboratory\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-alpha0 or 1.13.1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: not using GPU\r\n- GPU model and memory: not using GPU\r\n\r\n**Describe the current behavior**\r\n\r\nI sent some data from one model's loss function to another model. The other model starts with a RNN layer.\r\nin tensorflow 1.12.0 and before, the variable y_pred is shape (32, 10, 10) or (20, 10, 10) as first dimension is batch_size, and the model trained normally\r\nin tensorflow 1.13.1 and 2.0.0-alpha0, it shows that the shape is (None, 10, 10) then throws this exception\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in <listcomp>(.0)\r\n   3217     outputs = self._graph_fn(*converted_inputs)\r\n   3218     return nest.pack_sequence_as(self._outputs_structure,\r\n-> 3219                                  [x.numpy() for x in outputs])\r\n   3220 \r\n   3221 \r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n\r\nprobably because it changed the behavior to use a dynamic batch size tensor, then tensors of unknown batch size don't have the numpy method?\r\n\r\nIf the RNN layer is not present, and the data is 2d (like (None, 10) ), the model can train normally.\r\nCNN and Flatten seem to also have the problem.\r\n\r\n**Describe the expected behavior**\r\nI think it should not use $.numpy() there in the backend, probably(?)\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow import keras\r\n\r\n# try-except eager trigger to prevent trouble from ipynb\r\ntry:\r\n    tf.enable_eager_execution();\r\nexcept:\r\n    pass\r\n\r\ndef build_classifier_model():\r\n    model = keras.Sequential([\r\n        keras.layers.SimpleRNN(64, input_shape=(10, 10)),\r\n        keras.layers.Dense(64, activation=tf.nn.relu),\r\n        keras.layers.Dense(1, activation=tf.nn.tanh)\r\n    ])\r\n\r\n    optimizer = tf.optimizers.Adam(0.001) #tf.train.AdamOptimizer(0.001)\r\n\r\n    model.compile(loss='mse',\r\n                optimizer=optimizer,\r\n                metrics=[keras.metrics.mae])\r\n    return model\r\n\r\nsome_model = build_classifier_model();\r\n  \r\ndef build_model():\r\n    def custom_loss(y_true, y_pred):\r\n      # simply tile y_pred as an example\r\n      y_cred = tf.tile(tf.expand_dims(y_pred, axis=2), (1, 1, 10))\r\n      print(y_cred.shape)\r\n      return tf.reduce_sum(some_model(y_cred) ** 2)\r\n    \r\n    model = keras.Sequential([\r\n        keras.layers.Dense(64, input_shape=(12,)),\r\n        keras.layers.Dense(10, activation=tf.nn.tanh)\r\n    ])\r\n\r\n    optimizer = tf.optimizers.Adam(0.001) #tf.train.AdamOptimizer(0.001)\r\n\r\n    model.compile(loss=custom_loss,\r\n                optimizer=optimizer,\r\n                metrics=[keras.metrics.mae])\r\n    return model\r\n        \r\nm = build_model();\r\n# fitting randomness as example\r\nx = np.random.random((500, 12));\r\ny = tf.cast(np.random.randint(0, 2, size=(500,)), tf.float32);\r\nm.fit(x, y, epochs=10);\r\n```\r\n\r\n**Other info / logs**\r\n\r\nfull exception log\r\n```python\r\n(None, 10, 10)\r\n\r\n---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n<ipython-input-3-de77aceb3f93> in <module>()\r\n     38     return model\r\n     39 \r\n---> 40 m = build_model();\r\n     41 # fitting randomness as example\r\n     42 x = np.random.random((500, 12));\r\n\r\n<ipython-input-3-de77aceb3f93> in build_model()\r\n     35     model.compile(loss=custom_loss,\r\n     36                 optimizer=optimizer,\r\n---> 37                 metrics=[keras.metrics.mae])\r\n     38     return model\r\n     39 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    454     self._setattr_tracking = False  # pylint: disable=protected-access\r\n    455     try:\r\n--> 456       result = method(self, *args, **kwargs)\r\n    457     finally:\r\n    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    428       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n    429       #                   layer losses.\r\n--> 430       self.total_loss = self._prepare_total_loss(skip_target_indices, masks)\r\n    431 \r\n    432       # Functions for train, test and predict will\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _prepare_total_loss(self, skip_target_indices, masks)\r\n   1684             loss_fn.reduction = losses_utils.ReductionV2.NONE\r\n   1685             weighted_losses = loss_fn(\r\n-> 1686                 y_true, y_pred, sample_weight=sample_weight)\r\n   1687             loss_fn.reduction = current_loss_reduction\r\n   1688 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py in __call__(self, y_true, y_pred, sample_weight)\r\n     94     with ops.name_scope(scope_name, format(self.__class__.__name__),\r\n     95                         (y_pred, y_true, sample_weight)):\r\n---> 96       losses = self.call(y_true, y_pred)\r\n     97       return losses_utils.compute_weighted_loss(\r\n     98           losses, sample_weight, reduction=self.reduction)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py in call(self, y_true, y_pred)\r\n    156       Loss values per sample.\r\n    157     \"\"\"\r\n--> 158     return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n    159 \r\n    160   def get_config(self):\r\n\r\n<ipython-input-3-de77aceb3f93> in custom_loss(y_true, y_pred)\r\n     24       y_cred = tf.tile(tf.expand_dims(y_pred, axis=2), (1, 1, 10))\r\n     25       print(y_cred.shape)\r\n---> 26       return tf.reduce_sum(some_model(y_cred) ** 2)\r\n     27 \r\n     28     model = keras.Sequential([\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    559       # framework.\r\n    560       if base_layer_utils.needs_keras_history(inputs):\r\n--> 561         base_layer_utils.create_keras_history(inputs)\r\n    562 \r\n    563     # Handle Keras mask propagation from previous layer to current layer.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in create_keras_history(tensors)\r\n    249       operations and need to have Keras metadata assigned to them.\r\n    250   \"\"\"\r\n--> 251   _create_keras_history_helper(tensors, set())\r\n    252 \r\n    253 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops)\r\n    286           # when originating from an eager context\r\n    287           # so can't be supported.\r\n--> 288           constants[i] = backend.function([], op_input)([])\r\n    289       processed_ops = _create_keras_history_helper(layer_inputs, processed_ops)\r\n    290       name = op.name\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3217     outputs = self._graph_fn(*converted_inputs)\r\n   3218     return nest.pack_sequence_as(self._outputs_structure,\r\n-> 3219                                  [x.numpy() for x in outputs])\r\n   3220 \r\n   3221 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in <listcomp>(.0)\r\n   3217     outputs = self._graph_fn(*converted_inputs)\r\n   3218     return nest.pack_sequence_as(self._outputs_structure,\r\n-> 3219                                  [x.numpy() for x in outputs])\r\n   3220 \r\n   3221 \r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```", "comments": ["@kotritrona I could reproduce the issue with TF2.0. However, with little modifications to the above code, I could run in TF-nightly (gist is [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/b24b8695bdf3deb42f5052025bde623f/untitled117.ipynb)) without AtrributeError. Thanks!", "Adding Tom here since it seems to be related to TensorFlowOpLayer that was added recently.", "@kotritrona Looks like this was resolved in recent `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/e945ef2b297613a3181d150d4479285b/untitled117.ipynb) is the gist for your reference.\r\n\r\nCan you please verify once and close the issue if this was resolved for you. Thanks!", "I am closing this issue as this was resolved in recent `tf-nightly`. Please feel free to reopen if this was not resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27941\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27941\">No</a>\n"]}, {"number": 27940, "title": "__name__ becomes 'tensorflow.keras.layers' when import tf.keras.layers", "body": "**System information**\r\nGoogle Colaboratory with tensorflow nightly build 2.0.0.dev20190417.\r\nAny runtime\r\n\r\n**Code to reproduce**\r\n```\r\n!pip install tf-nightly-2.0-preview\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\nprint(__name__)  # output is 'tensorflow.keras.layers'\r\n```\r\n\r\n**Code to avoid the problem**\r\n```\r\n!pip install tf-nightly-2.0-preview\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Dense # Import layers explicitly\r\nprint(__name__)  # output is '__main__'\r\n```", "comments": ["I have tried the same but couldn't see the issue. Please check out here https://github.com/shravankumar147/tensorflow/blob/master/__name___becomes_'tensorflow_keras_layers'_when_import_tf_keras_layers_27940.ipynb", "@coelacanthus We get ouput as _main_ in both of the code scenarios.\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "I have tried a new nightly build(2.0.0.dev20190421), and the issue seems to have been fixed.\r\n\r\nCurrently, how to reproduce the issue is:\r\n```\r\n!echo 'tf-nightly-2.0-preview==2.0.0.dev20190417' > /tmp/requirements.txt\r\n!echo 'tensorflow-estimator-2.0-preview==1.14.0.dev2019041700' >> /tmp/requirements.txt\r\n!echo 'tb-nightly==1.14.0a20190417' >> /tmp/requirements.txt\r\n!pip install -r /tmp/requirements.txt\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\nprint(tf.__git_version__, tf.__version__)\r\nprint(__name__)\r\n```\r\nPlease check out below: https://github.com/coelacanthus/tensorflow/blob/master/__name___becomes_%27tensorflow_keras_layers%27_when_import_tf_keras_layers_27940.ipynb\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-12697-g4a1931ae46 2.0.0-dev20190417\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla T4\r\n\r\nI am sorry for not enough information. Thanks.", "I am closing the issue as it was resolved. Please open a new ticket if you see similar issue again. Thanks!"]}, {"number": 27939, "title": "[CI] Is TensorFlow-jenkins bot died?", "body": "Has Tensoflow-Jenkins bot been running now? We could  get the some messages from the bot when PR was submitted in the past (e.g., PR #13557). However, It seems that we could not see the any activity of the Tensoflow-Jenkins bot recently. What is the CI infrastructure  (e.g., Jenkins, Kokoro, Travis-CI, Circle-CI, and so on)of the Tensorflow deep-learning framework?\r\n\r\n\r\n* Example: PR #24314 (On Dec-13-2019)\r\n![image](https://user-images.githubusercontent.com/82404/56335313-f6061480-61d6-11e9-9118-c8d4ff11c39e.png)\r\n\r\n* Example: PR #20809 (On Jun-4-2019)\r\n![image](https://user-images.githubusercontent.com/82404/56335253-b5a69680-61d6-11e9-9795-045c7b0fb94e.png)\r\n\r\n\r\n* Example: PR #13557 (On Oct-8-2017)\r\n![image](https://user-images.githubusercontent.com/82404/56335169-652f3900-61d6-11e9-9d5f-a6ea642bb606.png)\r\n", "comments": ["Closing this issue since it is not related TensorFlow modules. Thanks!"]}, {"number": 27938, "title": "Read timed out and referenced by '//tensorflow/tools/pip_package:licenses'", "body": "bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package   \r\n Fetching @jpeg; fetching 167s\r\nERROR: /mnt/d/Project/TensorFlow/tensorflow-master/tensorflow/tools/pip_package/BUILD:145:1: no such package '@grpc//third_party/address_sorting': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/grpc/grpc/archive/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz, https://github.com/grpc/grpc/archive/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz] to /home/qili/.cache/bazel/_bazel_qili/f937d3405457c6b857b73c6deab45175/external/grpc/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz: Read timed out and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@grpc//third_party/address_sorting': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/grpc/grpc/archive/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz, https://github.com/grpc/grpc/archive/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz] to /home/qili/.cache/bazel/_bazel_qili/f937d3405457c6b857b73c6deab45175/external/grpc/4566c2a29ebec0835643b972eb99f4306c4234a3.tar.gz: Read timed out\r\nINFO: Elapsed time: 168.452s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)", "comments": ["How can i configure to avoid Time out problem ?", "\u7f51\u901f\u95ee\u9898\uff0c\u5f88\u591a\u5de5\u5177\u65e0\u6cd5\u6210\u529f\u4e0b\u8f7d\uff0c\u5bfc\u81f4\u5b89\u88c5\u4e00\u76f4\u5931\u8d25\uff0c\u5c1d\u8bd5\u8fc7\u7528\u672c\u5730\u4e0b\u8f7d\u5b89\u88c5\u65e0\u6cd5\u5b89\u88c5\u7684\u5de5\u5177\uff0c\u4f46\u662f\u5b89\u88c5\u4e4b\u540e\uff0c\u7528bazel\u5b89\u88c5\u4f9d\u65e7\u4f1a\u81ea\u5df1\u4e0b\u8f7d \r\n\u6700\u540e\u591a\u6b21\u5c1d\u8bd5\u5b89\u88c5\uff0c\u7f51\u7edc\u597d\u7684\u65f6\u5019\u5c31\u80fd\u88c5\u4e0a  \r\n\u6216\u8005\u53ea\u7528\u5b89\u88c5\u67d0\u4e9b\u5de5\u5177\uff0c\u6211\u53ea\u9700\u8981\u5176\u4e2d\u7684\u4e00\u4e2a\u5de5\u5177\uff0c\u6700\u540e\u653e\u5f03\u6574\u4e2apip_package\u5b89\u88c5\uff0c\u53ea\u5b89\u88c5\u4e86\u67d0\u4e9b\u5de5\u5177 \r\nsudo bazel build --config=noaws tensorflow/tools/graph_transforms:summarize_graph ", "@liqi-c  May I request you to post your reply in english and also, Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide steps to reproduce the error. Thanks!\r\n\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27937, "title": "Restoring from checkpoints are broken in TF 1.13.1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary, pip3 install tensorflow-gpu\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: V10.0.130\r\n- GPU model and memory: GTX 1060m, 6GB\r\n\r\n**Describe the current behavior**\r\nI am unable to restore the weights of any of my tf.keras models ONLY when restoring from a new initialization of the model. If I change the weights then restore without reinitializing the model, it will properly restore. Furthermore, a SILENT error is being thrown when this happens, requiring me to print the status of the restore to see it.\r\n\r\n**Describe the expected behavior**\r\nThe weights should restore and not run into an error. And if an error would occur, it should be logged without me having to print it myself.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Disable logging\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\ntf.enable_eager_execution()\r\n\r\n# Create model\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(256, 3, padding=\"same\"),\r\n    tf.keras.layers.Conv2D(3, 3, padding=\"same\")\r\n])\r\nprint(\"Are weights empty before training?\", model.weights == [])\r\n\r\n# Create optim, checkpoint\r\noptimizer = tf.train.AdamOptimizer(0.001)\r\ncheckpoint = tf.train.Checkpoint(model=model)\r\n\r\n# Make fake data\r\nimg = np.random.uniform(0, 255, (32, 32, 3)).astype(np.float32)\r\ntruth = np.random.uniform(0, 255, (32, 32, 3)).astype(np.float32)\r\n# Train\r\nwith tf.GradientTape() as tape:\r\n    logits = model(img[None])\r\n    loss = tf.losses.mean_squared_error(truth[None], logits)\r\n\r\n# Compute/apply gradients\r\ngrads = tape.gradient(loss, model.trainable_weights)\r\ngrads_and_vars = zip(grads, model.trainable_weights)\r\noptimizer.apply_gradients(grads_and_vars)\r\n\r\n# Save model\r\ncheckpoint_path = './ckpt/'\r\ncheckpoint.save('./ckpt/')\r\n\r\n# Check if weights update\r\nprint(\"Are weights empty after training?\", model.weights == [])\r\n\r\n# Reset model\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(256, 3, padding=\"same\"),\r\n    tf.keras.layers.Conv2D(3, 3, padding=\"same\")\r\n])\r\nprint(\"Are weights empty when resetting model?\", model.weights == [])\r\n\r\n# Update checkpoint pointer\r\ncheckpoint = tf.train.Checkpoint(model=model)\r\n# Restore values from the checkpoint\r\nstatus = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\r\n\r\nprint(\"Are weights empty after restoring from checkpoint?\", model.weights == [])\r\nprint(status)\r\nstatus.assert_existing_objects_matched()\r\nstatus.assert_consumed()\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nAre weights empty before training? True\r\nAre weights empty after training? False\r\nAre weights empty when resetting model? True\r\nAre weights empty after restoring from checkpoint? True\r\n<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus object at 0x7f6ac9691f98>\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 56, in <module>\r\n    status.assert_consumed()\r\n  File \"/home/jpatts/Documents/alpha-doom/env/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/util.py\", line 1025, in assert_consumed\r\n    raise AssertionError(\"Unresolved object in checkpoint: %s\" % (node,))\r\nAssertionError: Unresolved object in checkpoint: attributes {\r\n  name: \"VARIABLE_VALUE\"\r\n  full_name: \"sequential/conv2d/kernel\"\r\n  checkpoint_key: \"model/layer-0/kernel/.ATTRIBUTES/VARIABLE_VALUE\"\r\n}\r\n```", "comments": ["`model` hasn't created variables yet in this case:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(256, 3, padding=\"same\"),\r\n    tf.keras.layers.Conv2D(3, 3, padding=\"same\")\r\n])\r\nprint(model.variables)\r\n```\r\nPrints `[]` (or very soon will throw an exception). So restoring at that point defers the restoration; [the checkpoint guide has an explanation](https://www.tensorflow.org/alpha/guide/checkpoints#delayed_restorations). If you restore and then call `model`, `assert_consumed` will pass:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel_1 = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(256, 3, padding=\"same\"),\r\n    tf.keras.layers.Conv2D(3, 3, padding=\"same\")\r\n])\r\n\r\nmodel_1(np.random.uniform(0, 255, (1, 32, 32, 3)))\r\nsave_path = tf.train.Checkpoint(model=model_1).save(\"/tmp/tf_ckpts/\")\r\n\r\nmodel_2 = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(256, 3, padding=\"same\"),\r\n    tf.keras.layers.Conv2D(3, 3, padding=\"same\")\r\n])\r\n\r\nrestore_checkpoint = tf.train.Checkpoint(model=model_2)\r\nstatus = restore_checkpoint.restore(save_path)\r\n#status.assert_consumed()  # Fails! model_2.variables is empty\r\nmodel_2(np.random.uniform(0, 255, (1, 32, 32, 3)))\r\nstatus.assert_consumed()  # Passes\r\n```\r\n\r\nThe output of the model is the correctly restored output even though the variables weren't there when the `restore()` call was made.\r\n\r\nIs that clear? Happy to hear ideas for better documentation. It's a somewhat tricky API I know, but restore-on-create is a bit of a trilemma: we either need to require input shapes to Layer construction so we can create variables immediately, or we can do deferred restoration, or we can require symbolic construction of the computation first (the TF 1.x approach) which gives us enough information to create the variables. We decided not to take the first path since it's annoying to have to specify, and we turned on eager by default so the third path isn't available (although you can optionally specify an input_shape to the first Layer in Sequential and it'll build everything right away).", "Thanks for the quick response. I understand what you are saying, and I think this should be given a line or two of explanation in the TensorFlow Eager tutorial. Since in the tutorial a variable is used, which doesn\u2019t require shape, this problem doesn\u2019t occur until using it in practice. It could definitely use more visibility.\r\n\r\nAlso, given that the weights aren\u2019t restored until model input is provided, I need to compare weights after restoration and input but before gradient step, correct? As at this point they should have taken on the restored values, so I can assert if they are equal.", "Thanks, yes it sounds like the eager guide should have a blurb about deferred restoration and a reference to the checkpointing guide. The main reason it doesn't is presumably the order they were written.\r\n\r\nOn checking that values are restored, yes that makes sense to me. Something like this:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef recreate_model_and_checkpoint():\r\n  model = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(256, 3, padding=\"same\"),\r\n    tf.keras.layers.Conv2D(3, 3, padding=\"same\")\r\n  ])\r\n  return tf.train.Checkpoint(\r\n      optimizer=tf.keras.optimizers.Adam(0.1),\r\n      model=model)\r\n\r\ndef train_step(checkpoint):\r\n  model = checkpoint.model\r\n  optimizer = checkpoint.optimizer\r\n  with tf.GradientTape() as tape:\r\n    output = model(tf.ones([1, 32, 32, 3]))\r\n    loss = tf.reduce_sum(output)\r\n  variables = model.trainable_variables\r\n  gradients = tape.gradient(loss, variables)\r\n  optimizer_weights = []\r\n  before_train_step_weights = [v.numpy() for v in variables]\r\n  optimizer.apply_gradients(zip(gradients, variables))\r\n  return loss, before_train_step_weights\r\n\r\ncheckpoint_one = recreate_model_and_checkpoint()\r\n# Just to create the variables so we have something to save\r\ntrain_step(checkpoint_one)\r\nsave_path = checkpoint_one.save(\"/tmp/tf_ckpts/\")\r\noriginal_loss_1, original_variable_values_1 = train_step(checkpoint_one)\r\noriginal_loss_2, original_variable_values_2 = train_step(checkpoint_one)\r\n\r\ncheckpoint_two = recreate_model_and_checkpoint()\r\nstatus = checkpoint_two.restore(save_path)\r\nnew_loss_1, new_variable_values_1 = train_step(checkpoint_two)\r\nstatus.assert_consumed()\r\nnew_loss_2, new_variable_values_2 = train_step(checkpoint_two)\r\n\r\nnp.testing.assert_allclose(new_loss_1.numpy(), original_loss_1.numpy())\r\nnp.testing.assert_allclose(new_loss_2.numpy(), original_loss_2.numpy())\r\nassert len(original_variable_values_1) == len(new_variable_values_1)\r\nfor original_value, new_value in zip(original_variable_values_1, new_variable_values_1):\r\n  np.testing.assert_allclose(original_value, new_value)\r\nfor original_value, new_value in zip(original_variable_values_2, new_variable_values_2):\r\n  np.testing.assert_allclose(original_value, new_value)\r\n```\r\n\r\nYou could also directly check the optimizer's slot variables rather than running two steps and checking both.", "After running into an issue where I did not know that my model was not being loaded, I think that `status.assert_existing_objects_matched()` and `status.assert_consumed()` should be automatically ran whenever restoring from a checkpoint.\r\n\r\nAs an example, in my case, my model was not loading the weights but it wasn't telling me, and the only check I was doing was that the weights weren't empty. Well I found out that the weights were just random inits, and since the loading error is not fatal, I was debugging for hours why my model was not performing properly. This is especially weird because I was reloading the model in the same file with no changes being made to it. It also passed `status.assert_existing_objects_matched()` and failed on `status.assert_consumed()`, outputting:\r\n```\r\nTraceback (most recent call last):\r\n  File \"simulator.py\", line 131, in <module>\r\n    main()\r\n  File \"simulator.py\", line 128, in main\r\n    model.predict(s0, action)\r\n  File \"simulator.py\", line 117, in predict\r\n    self.status.assert_consumed()\r\n  File \"/home/jpatts/.local/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/util.py\", line 1013, in assert_consumed\r\n    raise AssertionError(\"Unresolved object in checkpoint: %s\" % (node,))\r\nAssertionError: Unresolved object in checkpoint: attributes {\r\n  name: \"VARIABLE_VALUE\"\r\n  full_name: \"beta1_power\"\r\n  checkpoint_key: \"optim/beta1_power/.ATTRIBUTES/VARIABLE_VALUE\"\r\n}\r\n```\r\nLooks like something to do with the optimizer, but it should tell me when this happens, because otherwise (until this experience) I would have just kept assuming it was working.\r\n\r\nAlso, the weirdest part was that some other variables, such as epoch (int), were being restored properly while this wasn't. So there is also inconsistent behaviour happening across what is being restored.\r\nI think not having an all or none approach to restoring variables is not very intuitive.", "When should `assert_consumed` run automatically? It will typically fail right after `restore`.\r\n\r\nCan you share a reproduction for the unchanged file issue you ran into? That sounds like a bug.", "Or how does this sound? We can print a warning on program exit by default if a checkpoint was partially loaded. Status objects will have an \"allow_partial\" which silences the warning.", "I think that\u2019s a good middle of the road solution, as it provides relevant information to the user while also not causing a fatal error, which might be undesirable for some.\r\n\r\nAs for the potential bug I\u2019ll try and create an MVP of it later this week, as it\u2019s in a fairly complicated system.", "Thanks for the feedback. We have a warning for partial checkpoint restores now (in the latest nightly), and the [eager guide now mentions deferred restoration and points to the checkpoint guide](https://github.com/tensorflow/docs/commit/0142563841e83578e5441c7cc62bec571068e753)."]}, {"number": 27936, "title": "Following object-based saving guide on TensorFlow website leads to error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary? pip3 install tensorflow-gpu\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: V10.0.130\r\n- GPU model and memory: GTX 1060m, 6gb\r\n\r\n**Describe the current behavior**\r\nI am having issues saving and loading my models, so I tried the MVP example on the tensorflow website: https://www.tensorflow.org/guide/eager#object-based_saving\r\nIt has an error.\r\n**Describe the expected behavior**\r\nIt should work.\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable(10.)\r\ncheckpoint = tf.train.Checkpoint(x=x)\r\n\r\nx.assign(2.)   # Assign a new value to the variables and save.\r\ncheckpoint_path = './ckpt/'\r\ncheckpoint.save('./ckpt/')\r\n\r\nx.assign(11.)  # Change the variable after saving.\r\n\r\n# Restore values from the checkpoint\r\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\r\n\r\nprint(x)  # => 2.0\r\n```\r\n**Other info / logs**\r\n```\r\npython3 test.py \r\nWARNING:tensorflow:From /home/jpatts/Documents/alpha-doom/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 8, in <module>\r\n    checkpoint.save('./ckpt/')\r\n  File \"/home/jpatts/Documents/alpha-doom/env/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/util.py\", line 1856, in save\r\n    session.run(self.save_counter.initializer)\r\nAttributeError: 'NoneType' object has no attribute 'run'\r\n```", "comments": ["Nvm, I forgot to `tf.enable_eager_execution()`.\r\nI probably should stop jumping between 2.0 and 1.3...."]}, {"number": 27935, "title": "ModuleNotFoundError: No module named 'tensorflow'  in spyder | windows + anaconda", "body": "**System information**\r\n- OS Platform and Distribution : Windows 10 64-bits\r\n- TensorFlow installed from (source or binary): Python 3.6 CPU-only | https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: conda\r\n\r\n\r\n**Describe the problem**\r\n\r\nHi,\r\nI have struggled for hours to install tensorflow to Anaconda and finally succeeded. Hope my experience can help those who have same problem with installation.\r\n\r\nI have tried a lot of methods before, such as:\r\n```\r\npip install tensorflow\r\nconda install tensorflow\r\n```\r\netc,.\r\nThese methods might act as it installed properly but when I tested it in spyder, there were some problems, such as:\r\n\r\n> ImportError: cannot import name 'abs'\r\n> attributeerror: module 'tensorflow' has no attribute '__version__'\r\n\r\nI  just installed tensorflow successfully in Anaconda following the instructions from: https://www.tensorflow.org/install/pip\r\n2. Create a virtual environment (recommended)\r\n``` \r\n(base) C:\\Users\\XXXX>conda create -n venv pip python=3.6\r\n(base) C:\\Users\\XXXX>conda activate venv\r\n(venv) C:\\Users\\XXXX>pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl\r\n```\r\n\r\nThen I test it in terminal:\r\n```\r\n(venv) C:\\Users\\XXXX>python\r\nPython 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.13.1'\r\n```\r\n\r\nIt works!\r\nBut when I tried to import tensorflow in spyder:\r\n> ModuleNotFoundError: No module named 'tensorflow'\r\n\r\n\r\n**Solution**\r\n\r\nThis problem might cause by using virtual environment, and in Anaconda, your spyder or Jupyter Notebook works in default root, but tensorflow is installed in an isolated virtual environment 'venv'. So just install a new spyder or Jupyter Notebook under the virtual enviroment.\r\n- 1. Open Anaconda Navigation\r\n![image](https://user-images.githubusercontent.com/15613656/56325165-c273ad80-6136-11e9-9438-ec8f8eb759c9.png)\r\n- 2. Change 'Applications on' to the virtual environment you just created ('venv')\r\n- 3. Install a new spyder under 'venv'\r\n- 4. Run the new spyder and test\r\n![image](https://user-images.githubusercontent.com/15613656/56325929-c228e180-6139-11e9-9135-9d00715e03bf.png)\r\n\r\n\r\n", "comments": ["@danny1718 Great. Thanks for the detailed instructions. This will really help for people who use Anaconda to install TF in virtual environment. Thanks again.", "Thanks for the detailed instructions. I am closing this issue. Thanks!", "Thank you so much. Very much appreciable. @danny1718 \r\n", "super amazing, this is the most valuable posts I ever saw, directly solve my problem!! THX\r\none more small question, which environment would be better to install? why not install all packages in the base or root environment?", "Thanks a lot for these instructions. I could not find them anywhere else.", "Excelent, clever guy ..", "thank you so much", "I created the another environment \"tf\" from anaconda navigator and installed the Jupyter. Now what happened is that in this new Jupiter my tensoflow works but all other previously installed libraries do not work(like sklearn)m and in the old jupyter tensorflow doesnt work and other libraries work.\r\n\r\nSo how to solve this issue?", "> I created the another environment \"tf\" from anaconda navigator and installed the Jupyter. Now what happened is that in this new Jupiter my tensoflow works but all other previously installed libraries do not work(like sklearn)m and in the old jupyter tensorflow doesnt work and other libraries work.\r\n> \r\n> So how to solve this issue?\r\n\r\n--------------------------------------------------------------------------------------------------------------\r\n\r\nHi Dipen,\r\n\r\nThe libs installed in the default environment would not work in the 'tf' environment because they are two separate environments. And also, tensorflow was installed in the 'tf' environment so it can't be found in the default one.\r\n\r\nTo solve this issue, you can install all the libs you need in 'tf' use pip (recommended) or you can re-install tensorflow in the default environment.", "If you are using anaconda navigator then this might be a alternate solution : \r\ni) First at the initial time when you are installing anaconda navigator you will start with the base_DIR as the environment \r\nii.) Next set up Tensorflow in the base environment by running ''conda install tensoflow '' in base directory \r\niii.) Next as you install Tensorflow over there you can find anather virtual environment Tensorflow_gpu in anaconda navigator \r\n![t1](https://user-images.githubusercontent.com/60361231/87962511-f295c300-cad4-11ea-9508-540d9175f8dd.PNG)\r\nas shown in the image \r\niv.) select it as the default environment run it in terminal and pass the command : \r\n![t2](https://user-images.githubusercontent.com/60361231/87962628-1c4eea00-cad5-11ea-9236-29274e70a55b.PNG)\r\nNext directly download Jupyter Lab or notebook or Spyder over it and launch it with the command : %load_ext tensorboard\r\nIt will work .\r\nThank you ", "Yess finally thank you", "Thanks for the links\r\ni found [anaconda link](https://www.tensorflow.org/install/pip#conda) through which i've installed tensorflow in just few commands\r\n**also its a normal environment**\r\n\r\n```\r\nconda create --name py3-TF2\r\nconda activate py3-TF2\r\nconda install -c anaconda tensorflow\r\n```\r\n\r\nthe second command is a list of sub-commands customized by anaconda community\r\nafter installation i have\r\n```\r\n\r\n> python 3.7.9\r\n> tensorflow 2.1.0\r\n> tensorboard 2.2.1\r\n> numpy 1.19.1\r\n> pip 20.2.2\r\n> scipy 1.5.2\r\n\r\n```", "Anaconda on Windows is such a pain in the ass. If I install Anaconda with all the libraries, then I expect to have all the tools integrated in my `venv`, including TensorFlow, but then that takes 10 minutes to install because I'm on my secondary HDD with Windows 10 installed, and not my primary NVM.e SSD drive where I have my Linux distro installed, because ultimately, **and here is my main rant against whom I have:** NVIDIA!! because they do not release Open Source Drivers for their GPU's and make so many people's lives a pain in the ass! Feel me? Yes, because, [f**k NVIDIA](https://www.youtube.com/watch?v=iYWzMvlj2RQ)", "@danny1718 - I went through the same painful steps and then I saw this post. You saved my interest in tensorflow! All working beautifully now.", "[[Solved] Spyder ModuleNotFoundError In anaconda\r\n](https://exerror.com/spyder-modulenotfounderror-in-anaconda/)"]}, {"number": 27934, "title": " Improve the Cardinality function with tests for WindowDatasetOp", "body": "This PR improves the Cardinality function of `WindowDatasetOp` and add the tests.\r\n\r\ncc: @jsimsa ", "comments": []}, {"number": 27933, "title": "Dataset doc updated", "body": "Following up on issue #25320. This pull request clarifies the documentation by adding runnable code for several methods of the Dataset class (zip, concatenate, interleave, map, flat_map)\r\n\r\n@mrry @jsimsa", "comments": []}, {"number": 27932, "title": "NameError: name 'MyCapper' is not defined", "body": "For gardient clipping in case of Adam optimizer, I'm using below code. \r\n```\r\n# Create an optimizer.\r\nopt = AdamOptimizer(learning_rate=0.1)\r\n\r\n# Compute the gradients for a list of variables.\r\ngrads_and_vars = opt.compute_gradients(loss)\r\n\r\n# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\r\n# need to the 'gradient' part, for example cap them, etc.\r\ncapped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\r\n\r\n# Ask the optimizer to apply the capped gradients.\r\nopt.apply_gradients(capped_grads_and_vars)\r\n```\r\nBut I'm getting error \r\n\r\n> NameError: name 'MyCapper' is not defined\r\n\r\n**Fulltraceback:**\r\n\r\n`grads = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\r\n\r\nNameError: name 'MyCapper' is not defined`\r\n\r\nPleae help.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.10.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): 7.2.0\r\n- CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176\r\n- GPU model and memory: Tesla V100 \r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@monjoybme In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "The above code snippet is taken from the official TensorFlow website [link](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer). `MyCapper` function has been called there. ", "@monjoybme You need to write MyCapper function according to your requirement. If you don't want to constrain your gradients, then comment that line (where `MyCapper` is there) and run. Thanks.\r\n\r\nI am closing this issue. Please post this kind of support questions in Stackoverflow. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27932\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27932\">No</a>\n"]}, {"number": 27931, "title": "GPU build on ARM-64 failing", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04 running on Nvidia Jetson Nano board\r\nLinux jetson-01 4.9.140-tegra #1 SMP PREEMPT Wed Mar 13 00:32:22 PDT 2019 aarch64 aarch64 aarch64 GNU/Linux\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n\r\n- TensorFlow version:\r\nv1.13.1\r\n\r\n- Python version:\r\nPython 2.7.15rc1\r\n\r\n- Installed using virtualenv? pip? conda?:\r\nN/A\r\n\r\n- Bazel version (if compiling from source):\r\n```\r\nINFO: Invocation ID: e7fbbd06-6d2d-4fee-9260-a96c2efc12e9\r\nBuild label: 0.21.0- (@non-git)\r\nBuild target: bazel-out/aarch64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Apr 17 11:17:18 2019 (1555499838)\r\nBuild timestamp: 1555499838\r\nBuild timestamp as int: 1555499838\r\n\r\n```\r\n- GCC/Compiler version (if compiling from source):\r\ngcc (Ubuntu/Linaro 7.3.0-27ubuntu1~18.04) 7.3.0\r\n\r\n- CUDA/cuDNN version:\r\ncuda 10, cuDNN 7\r\n\r\n- GPU model and memory:\r\nTegra based GPU on Jetson Nano bord\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build TensorFlow on ARM64 (Jetson Nano board) for use with GPU. Build is failing.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- Enabled 6GB swap space per: https://www.jetsonhacks.com/2019/04/14/jetson-nano-use-more-memory/\r\n- Built bazel 0.21 from source.\r\n- Checked out v1.13.1 from TF repo\r\n- Applied these changes to build files (diff attached below): https://github.com/tensorflow/tensorflow/issues/21852#issuecomment-477885516\r\n- ran `./configure` saying no to most items except yes for CUDA support\r\n\r\n```\r\nRan following command to start the build\r\nnohup bazel build -c opt --jobs 1 --local_resources 5000,1.0,1.0 --verbose_failures --config=opt --config=cuda --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka //tensorflow:libtensorflow.so &\r\n\r\n```\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n[nohup.txt](https://github.com/tensorflow/tensorflow/files/3091084/nohup.txt)\r\n\r\n\r\n[tf_jetson_nano_build.diff.txt](https://github.com/tensorflow/tensorflow/files/3092069/tf_jetson_nano_build.diff.txt)\r\n", "comments": ["I believe this is the same problem reported in issue https://github.com/tensorflow/tensorflow/issues/25323", "As discussed in #25323 I tried building with gcc-8 instead of gcc-7, however, that produced another error:\r\n$ gcc --version\r\n`gcc (Ubuntu 8.2.0-1ubuntu2~18.04) 8.2.0`\r\n$ bazel build -c opt --verbose_failures --config=opt --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka //tensorflow:libtensorflow.so\r\n```\r\nINFO: Invocation ID: b9c93771-fc51-4edb-b485-09a27f7878fb\r\nINFO: Build option --define has changed, discarding analysis cache.\r\nINFO: Analysed target //tensorflow:libtensorflow.so (0 packages loaded, 8857 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/sdeoras/.cache/bazel/_bazel_sdeoras/6333fd39180a43710cc9e4529b1411f9/external/double_conversion/BUILD.bazel:12:1: undeclared inclusion(s) in rule '@double_conversion//:double-conversion':\r\nthis rule is missing dependency declarations for the following files included by 'external/double_conversion/double-conversion/cached-powers.cc':\r\n  '/usr/lib/gcc/aarch64-linux-gnu/8/include-fixed/limits.h'\r\n  '/usr/lib/gcc/aarch64-linux-gnu/8/include-fixed/syslimits.h'\r\n  '/usr/lib/gcc/aarch64-linux-gnu/8/include/stddef.h'\r\n  '/usr/lib/gcc/aarch64-linux-gnu/8/include/stdarg.h'\r\n  '/usr/lib/gcc/aarch64-linux-gnu/8/include/stdint.h'\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nINFO: Elapsed time: 26.678s, Critical Path: 11.99s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```", "In between builds did you clear the bazel cache?\r\n\r\nif not I would try:\r\n```\r\nbazel clean --expunge_async\r\nrm -rf /home/sdeoras/.cache/\r\n```", "I think gcc-8 or higher is not going to work when building on Jetson Nano per this error:\r\n\r\nERROR: /home/sdeoras/.cache/bazel/_bazel_sdeoras/6333fd39180a43710cc9e4529b1411f9/external/nccl_archive/BUILD.bazel:151:1: error while parsing .d file: /home/sdeoras/.cache/bazel/_bazel_sdeoras/6333fd39180a43710cc9e4529b1411f9/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/external/nccl_archive/_objs/nccl/rings.cu.pic.d (No such file or directory)\r\nIn file included from /usr/local/cuda-10.0/bin/../targets/aarch64-linux/include/cuda_runtime.h:83,\r\n                 from <command-line>:\r\n/usr/local/cuda-10.0/bin/../targets/aarch64-linux/include/crt/host_config.h:129:2: error: #error -- unsupported GNU version! gcc versions later than 7 are not supported!\r\n #error -- unsupported GNU version! gcc versions later than 7 are not supported!", "I am wondering if there is a recommendation on a branch or commit I can try building using gcc-7.", "Reading the other defect you could try any of these options:\r\n\r\n1) Removing `-c opt` and adding `--config=nativeopt` from the bazel command line\r\n2) GCC 7.4\r\n3) Building the v1.12.0 branch", "@wdirons: Thanks for your inputs. v1.12.0 built just fine with cuda 10 on Jetson Nano using gcc 7.3. Please feel free to close this case since the gcc 7.3 related issue with v1.13.1 is being addressed at #25323 ", "@sdeoras Closing this issue since its resolved. Thanks!"]}, {"number": 27930, "title": "[Intel MKL] BFloat16 operator enabling - round 4", "body": "This PR enables BFloat16 version of AddN, FusedBatchNorm, Reshape, Slice,\r\nand Transpose.", "comments": ["@nhasabni can you please resolve conflicts.", "@penpornk thanks for approval!"]}, {"number": 27929, "title": "tf-2.0: AttributeError: module 'tensorflow' has no attribute 'optimizers'", "body": "**System information**\r\n- OS Platform and Distribution Ubuntu 18.04 LTS\r\n- TensorFlow installed from source or binary): source\r\n- TensorFlow version (use command below): v2.0.0-alpha0-4-g2c2d508 2.0.0-alpha0\r\n- Python version: 3.5.7\r\n- Bazel version (if compiling from source): From docker image\r\n- GCC/Compiler version (if compiling from source): From docker image\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nThis documentation suggests there are 4 ways to load an optimizer:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers\r\n\r\nHowever:\r\n\r\n(tf_35) mark@science:~$ python tf_optimizers.py \r\nTraceback (most recent call last):\r\n  File \"tf_optimizers.py\", line 7, in <module>\r\n    opt4 = tf.optimizers.Adagrad\r\nAttributeError: module 'tensorflow' has no attribute 'optimizers'\r\n\r\n**Describe the expected behavior**\r\n\r\nopt = tf.optimizers.Adagrad doesn't work. The first 3 methods do.\r\n\r\n**MINIMAL code to reproduce the issue**\r\n\r\nimport tensorflow as tf\r\ntf.__version__\r\n\r\nopt1 = tf.compat.v2.keras.optimizers.Adagrad\r\nopt2 = tf.compat.v2.optimizers.Adagrad\r\nopt3 = tf.keras.optimizers.Adagrad\r\nopt4 = tf.optimizers.Adagrad\r\n\r\n\r\nI don't know if this is a documentation issue or a tensorflow bug.\r\n", "comments": ["@LGTrader I don't see any issue. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/8cc804d896d0bb5a107635d7566ea708/untitled103.ipynb).\r\n\r\nPlease make sure you have installed TF2.0. When I tried the above gist with TF1.13.1, it has thrown an error. So those four links work well for TF2.0 only. Please let me know what you think. If you agree, please close the issue. Thanks!", "@jvishnuvardhan \r\n\r\nI reworked the code and ran it in Jupyter Notebook on my system. I cannot download the same TF2.0 alpha version that you use because my CPU doesn't have an instruction set (AVX I think) that is now required so my version is built locally from the same code as of a couple of weeks ago. It does identify as 2.0-alpha-0\r\n\r\nAlso, a simple call to help(tf) actually does not show tf.optimizers as an option but does show tf.compat and tf.keras so I think everything is consistent.\r\n\r\nFor now I'm going to assume that possibly I caught the code in a weird state when I built this and will close the report. If I get a chance to rebuild newer code and still see it I think I can reopen it and we'll try again. If you think it's important to keep looking at this I assume you can reopen also?\r\n\r\nThanks!\r\n\r\n\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nopt1 = tf.compat.v2.keras.optimizers.Adagrad\r\nopt2 = tf.compat.v2.optimizers.Adagrad\r\nopt3 = tf.keras.optimizers.Adagrad\r\nprint(opt1)\r\nprint(opt2)\r\nprint(opt3)\r\n2.0.0-alpha0\r\n<class 'tensorflow.python.keras.optimizer_v2.adagrad.Adagrad'>\r\n<class 'tensorflow.python.keras.optimizer_v2.adagrad.Adagrad'>\r\n<class 'tensorflow.python.keras.optimizer_v2.adagrad.Adagrad'>\r\nopt4 = tf.optimizers.Adagrad\r\nprint(opt4)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-15-a356bbac2b8f> in <module>\r\n----> 1 opt4 = tf.optimizers.Adagrad\r\n      2 print(opt4)\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'optimizers'\r\n\r\n\r\n\r\n\r\n\r\nHelp on package tensorflow:\r\n\r\nNAME\r\n    tensorflow - Bring in all of the public TensorFlow interface into this module.\r\n\r\nPACKAGE CONTENTS\r\n    _api (package)\r\n    app (package)\r\n    audio (package)\r\n    autograph (package)\r\n    bitwise (package)\r\n    compat (package)\r\n    compiler (package)\r\n    config (package)\r\n    contrib (package)\r\n    core (package)\r\n    data (package)\r\n    debugging (package)\r\n    distribute (package)\r\n    distributions (package)\r\n    dtypes (package)\r\n    errors (package)\r\n    estimator (package)\r\n    examples (package)\r\n    experimental (package)\r\n    feature_column (package)\r\n    gfile (package)\r\n    graph_util (package)\r\n    image (package)\r\n    initializers (package)\r\n    io (package)\r\n    keras (package)\r\n    layers (package)\r\n    libtensorflow_framework\r\n    linalg (package)\r\n    lite (package)\r\n    logging (package)\r\n    lookup (package)\r\n    losses (package)\r\n    manip (package)\r\n    math (package)\r\n    metrics (package)\r\n    nest (package)\r\n    nn (package)\r\n    profiler (package)\r\n    python (package)\r\n    python_io (package)\r\n    quantization (package)\r\n    queue (package)\r\n    ragged (package)\r\n    random (package)\r\n    raw_ops (package)\r\n    resource_loader (package)\r\n    saved_model (package)\r\n    sets (package)\r\n    signal (package)\r\n    sparse (package)\r\n    spectral (package)\r\n    strings (package)\r\n    summary (package)\r\n    sysconfig (package)\r\n    test (package)\r\n    tools (package)\r\n    tpu (package)\r\n    train (package)\r\n    user_ops (package)\r\n    v1\r\n    version (package)\r\n", "@LGTrader I am reopening it as I noticed Attribute error with tf-nightly. I don't see any issue with TF2.0.0-alpha0. We need to check whether we need to update tf-nightly or the docs on website. Thanks for finding this issue. \r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-a368a9a6e043> in <module>()\r\n      3 opt2 = tf.compat.v2.optimizers.Adagrad\r\n      4 opt3 = tf.keras.optimizers.Adagrad\r\n----> 5 opt4 = tf.optimizers.Adagrad\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation_wrapper.py in __getattr__(self, name)\r\n    104     if name.startswith('_dw_'):\r\n    105       raise AttributeError('Accessing local variables before they are created.')\r\n--> 106     attr = getattr(self._dw_wrapped_module, name)\r\n    107     if (self._dw_warning_count < _PER_MODULE_WARNING_LIMIT and\r\n    108         name not in self._dw_deprecated_printed):\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'optimizers'", "Thanks. You saved me building again. (about 4 hours on my machine) I'll look for feedback in this thread and rebuild to verify fixes later if appropriate.", "I think the latest change to enable tf.optimizers.Adagrad might not be in the alpha0 release, can you install nightly and try it? pip install tf-nightly-gpu-2.0-preview", "I cannot install any of the prebuilt versions of TF as they depend on the AVX instruction set and my Intel i7-980 Extreme processor doesn't have that. This requires I build from source. You can get an overview of what I go through to build here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/26553\r\n\r\nI'm very happy to try out any specific Docker image (to get the GPU support) and version of Tensorflow you'd like me to try but I'm not a coding professional and I sometimes need a little hand holding. ;-) I'll investigate getting the preview version you mention.", "Ah thanks for letting me know your constraint. As an alternative approach, you could easily try your code by running in [Google Colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true), and once verified we could worry about the rebuilding issue.", "I've not used Colab but have been meaning to investigate. I'm not clear if I have to pay for access so any info for individual user types like me is much appreciated but I'll certainly check it out. Thanks!", "OK, I'm running in Colab but it's importing tf-1.13.1. I need to find the right way to import and will get back to you when I am across that threshold. Thanks!", "@LGTrader Google colab is free. Earlier I shared a colab gist [here](https://github.com/tensorflow/tensorflow/issues/27929#issuecomment-486464851). Just open it and run.Thanks!", "I think as long as you have a google account (for example using your GMail), then Colab is free. \r\nA very fun fact is that Colab just starts to support Tesla T4 (though I think you are relying on CPU but not GPU), but just FYI for whoever is interested.\r\n@martinwicke ", "Yes, it's up and running for me. I'm sure Google will shut me down if I do anything too expensive but it stores notebooks on my Google Drive and I've duplicated  jvishnuvardhan 's example.\r\n\r\nI'll continue doing small stuff in Colab so learn more but I think the next step is to possibly rebuild here from source and make sure I don't see the problem anymore. I'm working on setting up the Docker stuff as I write. I need to find the right command to git clone tf-nightly-gpu-2.0-preview and I'll go to work on that 6 hour build.\r\n\r\nThanks!", "Thank you for confirming with us! Closing it now."]}, {"number": 27928, "title": "Fix typos in BatchNormalization doc", "body": "", "comments": ["@drewszurko  thanks for your contribution , did you mean to push this changes to r2.0 or master ?", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}]