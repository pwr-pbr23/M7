[{"number": 54801, "title": "[TF-TRT] Changing Engine Name to TRTEngineOp_ABC_XYZ", "body": "@bixia1 for review\r\n\r\nChange TRTEngineOP name from `TRTEngineOP_X_Y` to `TRTEngineOp_ABC_XYZ`.\r\nThe main objective is to correct `TRTEngineOP_2_1  > TRTEngineOP_17_1`  => shall be false, effectively is true.", "comments": ["@bixia1 any update ?"]}, {"number": 54800, "title": "Add simplification to replace Use(ReduceMin(Arg) with Use(Gte(ReduceArgMin(), 0)).", "body": "Add simplification to replace Use(ReduceMin(Arg) with Use(Gte(ReduceArgMin(), 0)).\n", "comments": []}, {"number": 54799, "title": "Added support of reading short4/ushort4/char4/uchar4 from buffers in Glsl.", "body": "Added support of reading short4/ushort4/char4/uchar4 from buffers in Glsl.\n", "comments": []}, {"number": 54798, "title": "[XLA] Report that real -> complex bitcast_convert is not allowed", "body": "[XLA] Report that real -> complex bitcast_convert is not allowed\n\nThe check as exists is bidirectional: it prevents conversions from complex to real and real to complex alike, but the reported error message was unidirectional.\n", "comments": []}, {"number": 54797, "title": "Disable integration tests in the pip builds", "body": "Disable integration tests in the pip builds\n", "comments": []}, {"number": 54796, "title": "[mhlo] test for reduce_precision", "body": "[mhlo] test for reduce_precision\n", "comments": []}, {"number": 54795, "title": "Fix bug in XlaCompilationCache where an error is raised after calling DisableXlaCompilation but an entry is already compiled.", "body": "Fix bug in XlaCompilationCache where an error is raised after calling DisableXlaCompilation but an entry is already compiled.\n", "comments": []}, {"number": 54794, "title": "Introduce a new PjRtEvent class, and move BlockHostUntilReady and OnReady methods from PjRtBuffer into PjRtEvent, so instead of calling buf->BlockHostUntilReady() you now", "body": "Introduce a new PjRtEvent class, and move BlockHostUntilReady and OnReady methods from PjRtBuffer into PjRtEvent, so instead of calling buf->BlockHostUntilReady() you now\ncall buf->GetEvent()->BlockHostUntilReady().\n\nThe intention is that all PjRt methods that currently enqueue future work but don't return a PjRtBuffer will be modified to return a PjRtEvent, and that all blocking and callbacks on futures will be performed via PjRtEvent.\n", "comments": []}, {"number": 54793, "title": "Tighten access restrictions in GpuInstructionFusion", "body": "Tighten access restrictions in GpuInstructionFusion\n", "comments": []}, {"number": 54792, "title": "Supported more types in TensorDescriptor::GetDataTypeFromTemplateArgs.", "body": "Supported more types in TensorDescriptor::GetDataTypeFromTemplateArgs.\n", "comments": []}, {"number": 54791, "title": "Allow Hierarchical DatasetSpecs", "body": "Allow Hierarchical DatasetSpecs\n", "comments": []}, {"number": 54789, "title": "Decompose ResourceGather CPU ops even if it is a TPU model.", "body": "Decompose ResourceGather CPU ops even if it is a TPU model.\n", "comments": []}, {"number": 54788, "title": "[TF-TRT] Removing tensor name from error message to clean non conversion report", "body": "@bixia1 : for review\r\n\r\nSimplifies the error message generated when a node is converted during segmentation", "comments": ["@bixia1 any update ?", "Would you please simplify the PR description by moving some detail to a conversation block? Other than this, the PR looks fine to me.", "\r\nThe old detail logging can be retrieved using `export TF_CPP_VMODULE=\"segment=1,convert_nodes=2\"\r\n`\r\n\r\nBefore:\r\n```\r\n- Identity -> ##x\r\n\t\t- [Count: ##x] Failed to convert input <tensor_name> to a TRT_TensorOrWeights: Unsupported tensorflow data type bool\r\n\r\n- ConcatV2 -> ##x\r\n        - [Count: ##x] Failed to convert input <tensor_name> to a TRT_TensorOrWeights: Input tensor rank is unknown.\r\n\t\t- [Count: ##x] Failed to convert input <tensor_name> to a TRT_TensorOrWeights: Input tensor with shape [0] is an empty tensor, which is not supported by TRT\r\n\r\n- Split -> ##x\r\n\t\t- [Count: ##x] Failed to convert input <tensor_name> to a TRT_TensorOrWeights: Input tensor rank is unknown.\r\n\r\n- Reshape -> ##x\r\n\t\t- [Count: ##x] Failed to convert input B<tensor_name> to a TRT_TensorOrWeights: Unsupported tensorflow data type int64\r\n```\r\n\r\nAfter:\r\n```\r\n- Identity -> ##x\r\n\t\t- [Count: ##x] Failed to convert at least one input to a TRT_TensorOrWeights: Unsupported tensorflow data type bool\r\n\r\n- ConcatV2 -> ##x\r\n\t\t- [Count: ##x] Failed to convert at least one input to a TRT_TensorOrWeights: Input tensor rank is unknown.\r\n\t\t- [Count: ##x] Failed to convert at least one input to a TRT_TensorOrWeights: Input tensor with shape [0] is an empty tensor, which is not supported by TRT\r\n\r\n- Split -> ##x\r\n\t\t- [Count: ##x] Failed to convert at least one input to a TRT_TensorOrWeights: Input tensor rank is unknown.\r\n\r\n- Reshape -> ##x\r\n\t\t- [Count: ##x] Failed to convert at least one input to a TRT_TensorOrWeights: Unsupported tensorflow data type int64\r\n\t\t- [Count: ##x] Failed to convert at least one input to a TRT_TensorOrWeights: Input tensor rank is unknown.\r\n```", "@bixia1 done", "@DEKHTIARJonathan  would you please rebase?", "@DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted ? Thanks!", "@bixia1 : rebase done"]}, {"number": 54787, "title": "New define for kernel language for dynamic vec4 component selection.", "body": "New define for kernel language for dynamic vec4 component selection.\n", "comments": []}, {"number": 54786, "title": "Parameter server training: Accepting \"Graph execution error\" as worker failure with a flag.", "body": "Parameter server training: Accepting \"Graph execution error\" as worker failure with a flag.\n", "comments": []}, {"number": 54785, "title": "[tf][tfg] Change the TFG Grappler Optimizer to run on ModuleOp", "body": "[tf][tfg] Change the TFG Grappler Optimizer to run on ModuleOp\n\nThe PassManager in the TFG grappler optimizer should be run on ModuleOp, with nested passes targeting functions of the graph directly. Also exposes the PassRegistration target so that the grappler optimizer can actually run the passes.\n", "comments": []}, {"number": 54784, "title": "Could not load library cudnn_ops_infer64_8.dll. Error code 126", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform : Windows 10 Pro\r\n\r\n- TensorFlow installed from (source or binary): Installed via pip\r\n- TensorFlow version: 2.6.2\r\n- Python version:: 3.6.3\r\n- Installed using pip\r\n- CUDA/cuDNN version: Cuda 11.6\r\n- GPU 1080\r\n\r\nBelow are the  Environment Variables\r\n![image](https://user-images.githubusercontent.com/37058769/156252235-8d51862f-0fd7-4ae2-a169-3df8302d33df.png)\r\n\r\nVariable Name   Variable Value\r\nCUDA_PATH   C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\r\nCUDA_PATH_V10_1    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\nCUDA_PATH_V11_6    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\r\nCuDnn   C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\include\r\ncudnn_ops_infer64_8.dll    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\r\nCUPTI   C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\extras\\CUPTI\\lib64\r\n\r\n**Describe the problem**\r\n\r\nTensor flow produces the messages below when beginning the training of the model\r\n2022-03-01 15:02:55.368641: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\nEpoch 1/15\r\nCould not load library cudnn_ops_infer64_8.dll. Error code 126\r\nPlease make sure cudnn_ops_infer64_8.dll is in your library path!\r\n\r\n**There is no cudnn_ops_infer64_8.dll** \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThe python code is below.\r\n\r\nmodel = LeNet.build(width=28, height=28, depth=1, classes=2)\r\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\r\n\tmetrics=[\"accuracy\"])\r\n\r\n# train the network\r\nprint(\"[INFO] training network...\")\r\nH = model.fit(trainX, trainY, validation_data=(testX, testY),\r\n\tclass_weight=classWeight, batch_size=64, epochs=15, verbose=1)\r\n\r\n\r\nI'm totally clueless as to what is going on the all was working well until I installed  v11.6\r\nPlease advise\r\nThanks", "comments": ["@Bstrum36 ,\r\nEvery TensorFlow release is compatible with a certain version, for more information please take a look at the[ tested build configurations](https://www.tensorflow.org/install/source_windows#gpu).In this case, can you please try installing TensorFlow with the latest v2.7 or v2.8 with compatible CUDA and cuDNN  and check if you are facing the same error. Thanks!", "Thanks for the quick response.\nI am using:\ntf version: 2.6.2\nCurrent Python Version- 3.6.3\ntensorflow-2.6 and Python 3.6-3.9 are listed as tested environments\n\nI have CUDA v11.6   in the C:\\Program Files\\NVIDIA GPU Computing\nToolkit\\CUDA path\nI tried a new install of the latest tensorflow and the messages stated\nthat the Requirement already satisfied for each step.\n\nCan you give me a step by step on how to create a test environment.\n\n\n\n\nOn Wed, Mar 2, 2022 at 4:11 AM tilakrayal ***@***.***> wrote:\n\n> @Bstrum36 <https://github.com/Bstrum36> ,\n> Every TensorFlow release is compatible with a certain version, for more\n> information please take a look at the tested build configurations\n> <https://www.tensorflow.org/install/source_windows#gpu>.In this case, can\n> you please try installing TensorFlow with the latest v2.7 or v2.8 with\n> compatible CUDA and cuDNN and check if you are facing the same error.\n> Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/54784#issuecomment-1056637199>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI2XRUOQG6QTOPYLAVPX7YTU54WCRANCNFSM5PVHQ55A>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "Hi,\r\n\r\nI faced the same issue today, and fixed it by removing my currents Cuda and Cndnn installation to replace them by a tested build configuration you can find here : https://www.tensorflow.org/install/source_windows#gpu // for windows.\r\n\r\nthen i just followed the instructions given by tensorflow on this page : https://www.tensorflow.org/install/gpu.\r\n\r\nIf ur facing issues while trying to install cuda, its probably because the installer do not handle the fact that a previous cuda installation has been done on your computer, so u will have to uninstall it first, with all its components.\r\n\r\nGood luck ! \ud83d\udc35 \r\n\r\n", "Thanks for the heads up.  I will give it a try.\n\nOn Wed, Mar 2, 2022 at 5:00 PM Hugo Poidvin ***@***.***>\nwrote:\n\n> Hi,\n>\n> I faced the same issue today, and fixed it by removing my currents Cuda\n> and Cndnn installation to replace them by a tested build configuration you\n> can find here : https://www.tensorflow.org/install/source_windows#gpu //\n> for windows.\n>\n> then i just followed the instructions given by tensorflow on this page :\n> https://www.tensorflow.org/install/gpu.\n>\n> If ur facing issues while trying to install cuda, its probably because the\n> installer do not handle the fact that a previous cuda installation has been\n> done on our computer, so u will have to uninstall it first, with all its\n> components.\n>\n> Good luck ! \ud83d\udc35\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/54784#issuecomment-1057438334>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI2XRUOU7ROFPT5MOYYCV3DU57QHFANCNFSM5PVHQ55A>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "What version of python are you using?\n\nOn Wed, Mar 2, 2022 at 5:00 PM Hugo Poidvin ***@***.***>\nwrote:\n\n> Hi,\n>\n> I faced the same issue today, and fixed it by removing my currents Cuda\n> and Cndnn installation to replace them by a tested build configuration you\n> can find here : https://www.tensorflow.org/install/source_windows#gpu //\n> for windows.\n>\n> then i just followed the instructions given by tensorflow on this page :\n> https://www.tensorflow.org/install/gpu.\n>\n> If ur facing issues while trying to install cuda, its probably because the\n> installer do not handle the fact that a previous cuda installation has been\n> done on our computer, so u will have to uninstall it first, with all its\n> components.\n>\n> Good luck ! \ud83d\udc35\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/54784#issuecomment-1057438334>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI2XRUOU7ROFPT5MOYYCV3DU57QHFANCNFSM5PVHQ55A>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "3.7", "@Bstrum36 ,\r\nPlease take a look at the installation steps from this [link](https://www.tensorflow.org/install) with the compatible tested build [configurations](https://www.tensorflow.org/install/source_windows#gpu) and let us know if you are facing same issue.Thanks", "The issue can be resolved by doing this step: [install zlib for windows](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-zlib-windows)\r\n\r\nJust happened to me a few minutes ago before I stumbled upon the solution. I simply copying `zlibwapi.dll` to the `bin`-directory of the CUDA installation solves the issue. ", "Thanks for the info. I have tried this but here is what I get:\n\nzlib123dllx64.zip\nI unzip and get:\n[image: image.png]\nI copy the zlibwapi.dll   to CUDA/v11.6\n\nThere it is.C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\n[image: image.png]\nStill get  this error:\nCould not load library cudnn_ops_infer64_8.dll. Error code 126\nPlease make sure cudnn_ops_infer64_8.dll is in your library path!\n\nAny ideas\n\nOn Thu, Mar 3, 2022 at 4:59 PM Patrick Levin ***@***.***>\nwrote:\n\n> The issue can be resolved by doing this step: install zlib for windows\n> <https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-zlib-windows>\n>\n> Just happened to me a few minutes ago before I stumbled upon the solution.\n> I simply copying zlibwapi.dll to the bin-directory of the CUDA\n> installation solves the issue.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/54784#issuecomment-1058534320>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI2XRUMWTUHIMUOORRLIYL3U6EY5LANCNFSM5PVHQ55A>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "Are you using cuDNN v8.3.2 (January 10th, 2022), for CUDA 11.5?\r\nThis should be the version compatible with CUDA v11.6.\r\n", "@Bstrum36 ,\r\nAs suggested in the [document](https://www.tensorflow.org/install/source_windows#gpu) please try the re-install the tensorflow stable version 2.7 or 2.8 with the compatible CUDA and Cudnn.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54784\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54784\">No</a>\n"]}, {"number": 54783, "title": "Fix build rules for runtime_client_pybind", "body": "Fix build rules for runtime_client_pybind\n", "comments": []}, {"number": 54782, "title": "internal visibility change only", "body": "internal visibility change only\n", "comments": []}, {"number": 54781, "title": "Keep is_fusion_computation value across ToProto/CreateFromProto HloComputation serialization even for dead code.", "body": "Keep is_fusion_computation value across ToProto/CreateFromProto HloComputation serialization even for dead code.\n", "comments": []}, {"number": 54780, "title": "[JAX] Deprecate .block_host_until_ready() in favor of .block_until_ready().", "body": "[JAX] Deprecate .block_host_until_ready() in favor of .block_until_ready().\n\nJAX kept an older name around (.block_host_until_ready()) in parallel with the new name (.block_until_ready()) to avoid breaking users. Deprecate it so we only have one name.\n", "comments": []}, {"number": 54778, "title": "fix mac OS test.", "body": "fix mac OS test.\n", "comments": []}, {"number": 54777, "title": "[tf][tfg] Fix error message with mismatched results/arguments", "body": "[tf][tfg] Fix error message with mismatched results/arguments\n\nMake the error message less confusing when a functional op is calling a function with the incorrect number of arguments or results.\n", "comments": []}, {"number": 54776, "title": "Fix a build failure.", "body": "Fix a build failure.\n\nRemoved an assert that uses an undefined variable. The problem is introduced by\nPR https://github.com/tensorflow/tensorflow/pull/54301.\n", "comments": []}, {"number": 54775, "title": "Update sonartype Maven repo URL to HTTPS", "body": "Update sonartype Maven repo URL to HTTPS\nFrom Github https://github.com/tensorflow/tensorflow/pull/54425\n", "comments": []}, {"number": 54774, "title": "Update _DirectedInterleaveDataset to validate inputs using element_spec (rather than using legacy methods).", "body": "Update _DirectedInterleaveDataset to validate inputs using element_spec (rather than using legacy methods).\n", "comments": []}, {"number": 54773, "title": "Remove unused `GetFirstWorkingExecutionPlan`.", "body": "Remove unused `GetFirstWorkingExecutionPlan`.\n\nThis became unused in cl/398565269.  Credit to kaixih@nvidia for noticing it\nwas unused in https://github.com/tensorflow/tensorflow/pull/54433.\n", "comments": []}, {"number": 54772, "title": "Update RaggedTensorSpec._to_legacy_output_class to return the class instead of the object.", "body": "Update RaggedTensorSpec._to_legacy_output_class to return the class instead of the object.\n", "comments": []}, {"number": 54771, "title": "[ROCm] fix triangular solve", "body": "The following tests recently began failing for ROCm:\r\n\r\n//tensorflow/compiler/tests:matrix_inverse_op_test_gpu\r\n//tensorflow/compiler/tests:matrix_inverse_op_test_gpu_mlir_bridge_test\r\n//tensorflow/compiler/tests:matrix_solve_op_test_gpu\r\n//tensorflow/compiler/tests:matrix_solve_op_test_gpu_mlir_bridge_test\r\n//tensorflow/compiler/tests:matrix_triangular_solve_op_test_gpu\r\n//tensorflow/compiler/tests:matrix_triangular_solve_op_test_gpu_mlir_bridge_test\r\n//tensorflow/compiler/xla/service/gpu/tests:bef_executable_test_gpu\r\n//tensorflow/compiler/xla/tests:triangular_solve_test_gpu\r\n\r\nThis was due to the introduction of the batched version of the triangular solve operation in cublas in: https://github.com/tensorflow/tensorflow/commit/d797b6d5f09ebde355a58a48b1d8fb8f61f657e2\r\n\r\nThis PR adds the same thing for ROCm and rocblas.\r\n\r\nOne interesting difference is the MakeBatchPointers call, which is implemented in PTX in the CUDA implementation. For ROCm, we add a new helper file with the code for this kernel. This kernel will be usable for other batched operations as well (ex: cholesky).", "comments": ["@cheshire @chsigg @deven-amd", "@cheshire Can you re-approve, please? Thanks! (only change was buildifier/sanity stuff)", "@cheshire  Gentle ping on this one. Thx!", "I pushed one more fix here (for BEF/TFRT) since this is still open.", "@chsigg FYI", "@jayfurmanek Can you please address Ubuntu Sanity errors? Thank you!", "ah, I though I fixed that already. Sorry! Should be fixed now.", "Ugh, note to self - just use buildifier even on small little format things that are easy to fix manually.\r\nSanity *should* pass now.\r\nSorry for the churn."]}, {"number": 54770, "title": "Using templates in upload/download methods of TensorDescriptor to support more types.", "body": "Using templates in upload/download methods of TensorDescriptor to support more types.\n", "comments": []}]