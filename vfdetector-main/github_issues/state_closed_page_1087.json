[{"number": 20661, "title": "tensorflow c++ api session->run() consumes too much time.", "body": "Here are my codes:\r\n```\r\nvoid tensorflow_class_wrapper::load_model(string model_path)\r\n{\r\n\t// set gpu options\r\n\tSessionOptions opts;\r\n\tGraphDef graph_def;\r\n\tConfigProto* config = &opts.config;\r\n\topts.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.3);\r\n\topts.config.mutable_gpu_options()->set_allow_growth(true);\r\n\tassert(_session == NULL);\r\n\tTF_CHECK_OK(NewSession(opts, &_session));\r\n\tprintf(\"Create session done.\\n\");\r\n\r\n\t//read the pb file into the grapdef member\r\n\tTF_CHECK_OK(ReadBinaryProto(Env::Default(), model_path, &graph_def));\r\n\tprintf(\"Load model done.\\n\");\r\n\r\n\t//Add the graph to the session\r\n\tTF_CHECK_OK(_session->Create(graph_def));\r\n\tprintf(\"Create graph done.\\n\");\r\n\treturn;\r\n}\r\n```\r\n\r\n```\r\nvoid tensorflow_class_wrapper::predict()\r\n{\r\n\t// convert data\r\n\tconvert_data();\r\n\tprintf(\"Convert data done.\\n\");\r\n\r\n\tclock_t t1, t2;\r\n\tt1 = clock();\r\n\t//The session will initialize the outputs\r\n\tvector<Tensor> outputs;\r\n\tTF_CHECK_OK( _session->Run(_input_dict,  _output_tensor_names , {}, &outputs));\r\n\tt2 = clock();\r\n\tcout << \"predict consume time: \" << float(t2 - t1) / CLOCKS_PER_SEC << endl;\r\n\r\n\tfor (auto _o : outputs)\r\n\t\tcout << _o.DebugString() << endl;\r\n\tvector<vector<float>> predict_res(outputs.size());\r\n\tfor (int nr = 0; nr < outputs.size(); nr++)\r\n\t\tpredict_res[nr].assign(outputs[nr].flat<float>().data(), outputs[nr].flat<float>().data() + outputs[nr].dims());\r\n\r\n}\r\n```\r\nMy input img size is 512x512x1. PB model is just single convolution and the output shape is 512x512x1. It's confused that session->Run() consumes over 2 seconds even more. By the way, i compile tensorflow1.8 release codes to get tensorflow lib/dll. Is there anything wrong when i compiled the source codes? Or I did not set some options in the right way?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It seems that the model's warm up consumes too much time when run is called for the first time.\r\nyou can refer to https://github.com/tensorflow/serving/issues/385.", "@weberxie  thanks a lot. \r\n\r\n", "@weberxie Thanks for your code and comments. I also met a similar problem. Do you know why the model's warm-up consumes that much time? Is there any way to warm up the model in advance before `_session->Run`?", "@feihugis You can make a dummy call for session->Run after the model was loaded."]}, {"number": 20660, "title": "Attempting to use uninitialized value Variable", "body": "Hi all,\r\n\r\nI had Import a graph with go:\r\n\r\n```go\r\n\t\t\tbmodel := res.GetBytes(0)\r\n\r\n\t\t\tgraph := tf.NewGraph()\r\n\t\t\tif err := graph.Import(bmodel, \"\"); err != nil {\r\n\t\t\t\tlog.Error(err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\tsess, err := tf.NewSession(graph, nil)\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Error(err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\tp.model = &tf.SavedModel{\r\n\t\t\t\tGraph:   graph,\r\n\t\t\t\tSession: sess,\r\n\t\t\t}\r\n```\r\n\r\nuse the model:\r\n\r\n```go\r\n\toutput := tf.Output{\r\n\t\tOp:    p.model.Graph.Operation(\"input\"),\r\n\t\tIndex: 0,\r\n\t}\r\n\ttarget, err := tf.NewTensor([][]float32{})\r\n\r\n\tif err != nil {\r\n\t\tlog.Error(err)\r\n\t\treturn \"accept\", err\r\n\t}\r\n\r\n\ttensor, err := tf.NewTensor([][]float32{preData})\r\n\tif err != nil {\r\n\t\tlog.Error(err)\r\n\t\treturn \"accept\", err\r\n\t}\r\n\r\n\tfeeds := map[tf.Output]*tf.Tensor{\r\n\t\toutput: tensor,\r\n\t\tp.model.Graph.Operation(\"target\").Output(0): target,\r\n\t}\r\n\r\n\tfetches := []tf.Output{\r\n\t\t{\r\n\t\t\tOp:    p.model.Graph.Operation(\"infer\"),\r\n\t\t\tIndex: 0,\r\n\t\t},\r\n\t}\r\n\r\n\tresult, err := p.model.Session.Run(\r\n\t\tfeeds,\r\n\t\tfetches,\r\n\t\tnil,\r\n\t)\r\n```\r\n\r\nbut get error: \r\n\r\n```sh\r\n2018-07-10 14:36:41 ERROR (shield-ai-engine/predict.go:188):20187 Attempting to use uninitialized value Variable_3\r\n\t [[Node: Variable_3/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable_3)]]\r\n```\r\nIt haven't init in NewSession? how to initialize variable?\r\n\r\nenv:\r\n\r\nOS: mac 10.11.6\r\npython version: python2.7\r\ntensorflow install base on https://www.tensorflow.org/install/install_go\r\ntensorflow-go version: v1.9.0-rc2\r\ngo version: go1.9.2 darwin/amd64", "comments": ["save model with python:\r\n\r\n```py\r\n        builder = tf.saved_model.builder.SavedModelBuilder('shield')\r\n        builder.add_meta_graph_and_variables(self.sess, ['login3'])\r\n        builder.save()\r\n```\r\n\r\nhere has all code:\r\n\r\n[https://github.com/tensorflow/tensorflow/issues/20511](https://github.com/tensorflow/tensorflow/issues/20511)", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!"]}, {"number": 20659, "title": "tensorflow", "body": "\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Correct!"]}, {"number": 20658, "title": "How to call object tracking  by C++ Interface ? ", "body": "Object tracking support in the [demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) is provided by libtensorflow_demo.so. I successfully had built it with the cmake options. But I'm not sure how to call the dynamic library correctly, using the C++ interface. The way the JAVA is called in the demo, but I don't really understand the specific process of its work. I hope I can call it in the way of C++ to implement my application, and do not know how to do it?\r\n\r\n@andrewharp ,\r\n\r\nPlease help, Thanks\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", " @WenguoLi \uff0c in the android/src/org/tensorflow/demo/trackingObjectTracker.java,there are some native function  to call c++ intereface, and MultiBoxTracker.java implemented the whole process,  you can refer to it to implement the tracking directly using c++.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20657, "title": "Use unique name for lookup ops", "body": "Add the unique names for lookup ops so that we don't need to specify op names when saving and restoring checkpoints.\r\n\r\nThis PR resolves https://github.com/tensorflow/tensorflow/issues/19528 .", "comments": ["Can you help to review this? @ysuematsu @angersson ", "Need your help to review this? @ysuematsu @qlzh727", "@andreasst, can you also take a look as well?\r\n", "@tobegit3hub, please also rebase and resolve the merge conflict here. Thanks.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Try to fix the conflict by rebasing master and sorry for bring all the reviewer \ud83d\ude1e ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 20656, "title": "please change allocation.cc line 102 copied_buffer_ = std::move(buffer);", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nCentOS 6.9 \r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.9.0-rc2\r\n- **Python version**: \r\n3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n0.14.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc 6.2.0\r\n- **CUDA/cuDNN version**:\r\ncuda 9.0 and cudnn 7.1.1\r\n- **GPU model and memory**:\r\nNVIDIA K80 (12GB/GPU) and P100(16GB/GPU)\r\n- **Exact command to reproduce**:\r\n>   exec env - \\\r\n>     CUDA_TOOLKIT_PATH=/apps/cuda/9.0 \\\r\n>     CUDNN_INSTALL_PATH=/apps/cudnn/7.1.1-cuda9.0 \\\r\n>     GCC_HOST_COMPILER_PATH=/apps/gcc/6.2.0/wrapper/gcc \\\r\n>     LD_LIBRARY_PATH=/apps/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0:/apps/gcc/6.2.0/lib64:/apps/nccl/2.2.13-cuda9.0/lib:/apps/openmpi/3.1.0/lib:/apps/openmpi/3.1.0/lib/profilers:/apps/intel-ct/17.0.1.132/mkl/lib/intel64:/apps/python3/3.6.2/lib:/apps/binutils/2.25/lib:/apps/java/jdk1.8.0_60/lib:/apps/cudnn/7.1.1-cuda9.0/lib64:/apps/cuda/9.0/extras/CUPTI/lib64:/apps/cuda/9.0/lib64 \\\r\n>     NCCL_INSTALL_PATH=/apps/nccl/2.2.13-cuda9.0 \\\r\n>     PATH=/apps/gcc/6.2.0/wrapper:/apps/gcc/6.2.0/bin:/apps/openmpi/wrapper/fortran:/apps/openmpi/3.1.0/bin:/apps/python3/3.6.2/bin:/apps/binutils/2.25/bin:/apps/bazel/0.14.1/bin:/apps/java/jdk1.8.0_60/bin:/apps/cuda/9.0/bin:/home/900/yxs900/.local/bin:/opt/bin:/bin:/usr/bin \\\r\n>     PWD=/proc/self/cwd \\\r\n>     PYTHON_BIN_PATH=/apps/python3/3.6.2/bin/python3 \\\r\n>     PYTHON_LIB_PATH=/apps/python3/3.6.2/lib/python3.6/site-packages \\\r\n>     TF_CUDA_CLANG=0 \\\r\n>     TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0 \\\r\n>     TF_CUDA_VERSION=9.0 \\\r\n>     TF_CUDNN_VERSION=7 \\\r\n>     TF_NCCL_VERSION=2 \\\r\n>     TF_NEED_CUDA=1 \\\r\n>     TF_NEED_OPENCL_SYCL=0 \\\r\n>   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/contrib/lite/_objs/framework/tensorflow/contrib/lite/allocation.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/contrib/lite/_objs/framework/tensorflow/contrib/lite/allocation.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/k8-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote external/gemmlowp -iquote bazel-out/k8-opt/genfiles/external/gemmlowp -iquote external/flatbuffers -iquote bazel-out/k8-opt/genfiles/external/flatbuffers -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/genfiles/third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem tensorflow/contrib/lite/schema -isystem bazel-out/k8-opt/genfiles/tensorflow/contrib/lite/schema -isystem bazel-out/k8-opt/bin/tensorflow/contrib/lite/schema -isystem external/flatbuffers/include -isystem bazel-out/k8-opt/genfiles/external/flatbuffers/include -isystem bazel-out/k8-opt/bin/external/flatbuffers/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-D_GLIBCXX_USE_CXX11_ABI=0' '-march=native' -DFARMHASH_NO_CXX_STRING -c tensorflow/contrib/lite/allocation.cc -o bazel-out/k8-opt/bin/tensorflow/contrib/lite/_objs/framework/tensorflow/contrib/lite/allocation.pic.o\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThe build stops because the variables buffer and copied_buffer have different types,  char*& and const char*&, respectively, and std::swap expects identical types of their inputs and code found no candidate for std::swap, see details in the error message below. Simply change line 102 in tensorflow/contrib/lite/allocation.cc {{ copied_buffer_ = std::move(buffer); }} to {{copied_buffer_.reset(const_cast<char const *>(buffer.release()));}} fix the bug.\r\n\r\n### Source code / logs\r\ntensorflow/contrib/lite/allocation.cc:102:36:   required from here\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: error: no matching function for call to 'swap(const char*&, char*&)'\r\n  swap(std::get<0>(_M_t), __p);\r\n  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:59:0,\r\n                 from /apps/gcc/6.2.0/include/c++/6.2.0/utility:70,\r\n                 from tensorflow/contrib/lite/allocation.cc:27:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:179:5: note: candidate: template<class _Tp> typename std::enable_if<std::__and_<std::is_move_constructible<_Tp>, std::is_move_assignable<_Tp> >::value>::type std::swap(_Tp&, _Tp&)\r\n     swap(_Tp& __a, _Tp& __b)\r\n     ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:179:5: note:   template argument deduction/substitution failed:\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   deduced conflicting types for parameter '_Tp' ('const char*' and 'char*')\r\n  swap(std::get<0>(_M_t), __p);\r\n  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:59:0,\r\n                 from /apps/gcc/6.2.0/include/c++/6.2.0/utility:70,\r\n                 from tensorflow/contrib/lite/allocation.cc:27:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:202:5: note: candidate: template<class _Tp, long unsigned int _Nm> typename std::enable_if<std::__is_swappable<_Tp>::value>::type std::swap(_Tp (&)[_Nm], _Tp (&)[_Nm])\r\n     swap(_Tp (&__a)[_Nm], _Tp (&__b)[_Nm])\r\n     ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:202:5: note:   template argument deduction/substitution failed:\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types '_Tp [_Nm]' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'\r\n  swap(std::get<0>(_M_t), __p);\r\n  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/utility:70:0,\r\n                 from tensorflow/contrib/lite/allocation.cc:27:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:403:5: note: candidate: template<class _T1, class _T2> void std::swap(std::pair<_T1, _T2>&, std::pair<_T1, _T2>&)\r\n     swap(pair<_T1, _T2>& __x, pair<_T1, _T2>& __y)\r\n     ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:403:5: note:   template argument deduction/substitution failed:\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::pair<_T1, _T2>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'\r\n  swap(std::get<0>(_M_t), __p);\r\n  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/vector:64:0,\r\n                 from ./tensorflow/contrib/lite/allocation.h:22,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_vector.h:1557:5: note: candidate: template<class _Tp, class _Alloc> void std::swap(std::vector<_Tp, _Alloc>&, std::vector<_Tp, _Alloc>&)\r\n     swap(vector<_Tp, _Alloc>& __x, vector<_Tp, _Alloc>& __y)\r\n     ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_vector.h:1557:5: note:   template argument deduction/substitution failed:\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::vector<_Tp, _Alloc>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'\r\n  swap(std::get<0>(_M_t), __p);\r\n  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/vector:65:0,\r\n                 from ./tensorflow/contrib/lite/allocation.h:22,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:112:3: note: candidate: void std::swap(std::_Bit_reference, std::_Bit_reference)\r\n   swap(_Bit_reference __x, _Bit_reference __y) noexcept\r\n   ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:112:3: note:   no known conversion for argument 1 from 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}' to 'std::_Bit_reference'\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:120:3: note: candidate: void std::swap(std::_Bit_reference, bool&)\r\n   swap(_Bit_reference __x, bool& __y) noexcept\r\n   ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:120:3: note:   no known conversion for argument 1 from 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}' to 'std::_Bit_reference'\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:128:3: note: candidate: void std::swap(bool&, std::_Bit_reference)\r\n   swap(bool& __x, _Bit_reference __y) noexcept\r\n   ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:128:3: note:   no known conversion for argument 2 from 'char*' to 'std::_Bit_reference'\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/list:63:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:18,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_list.h:1918:5: note: candidate: template<class _Tp, class _Alloc> void std::swap(std::list<_Tp, _Alloc>&, std::list<_Tp, _Alloc>&)\r\n     swap(list<_Tp, _Alloc>& __x, list<_Tp, _Alloc>& __y)\r\n     ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_list.h:1918:5: note:   template argument deduction/substitution failed:\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::list<_Tp, _Alloc>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'\r\n  swap(std::get<0>(_M_t), __p);\r\n  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/string:52:0,\r\n                 from /apps/gcc/6.2.0/include/c++/6.2.0/stdexcept:39,\r\n                 from /apps/gcc/6.2.0/include/c++/6.2.0/array:39,\r\n                 from /apps/gcc/6.2.0/include/c++/6.2.0/tuple:39,\r\n                 from /apps/gcc/6.2.0/include/c++/6.2.0/functional:55,\r\n                 from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/basic_string.h:5287:5: note: candidate: template<class _CharT, class _Traits, class _Alloc> void std::swap(std::basic_string<_CharT, _Traits, _Alloc>&, std::basic_string<_CharT, _Traits, _Alloc>&)\r\n     swap(basic_string<_CharT, _Traits, _Alloc>& __lhs,\r\n     ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/basic_string.h:5287:5: note:   template argument deduction/substitution failed:\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::basic_string<_CharT, _Traits, _Alloc>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'\r\n  swap(std::get<0>(_M_t), __p);\r\n  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/tuple:39:0,\r\n                 from /apps/gcc/6.2.0/include/c++/6.2.0/functional:55,\r\n                 from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/array:275:5: note: candidate: template<class _Tp, long unsigned int _Nm> void std::swap(std::array<_Tp, _Nm>&, std::array<_Tp, _Nm>&)\r\n     swap(array<_Tp, _Nm>& __one, array<_Tp, _Nm>& __two)\r\n     ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/array:275:5: note:   template argument deduction/substitution failed:\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::array<_Tp, _Nm>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'\r\n  swap(std::get<0>(_M_t), __p);\r\n  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/functional:55:0,\r\n                 from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/tuple:1546:5: note: candidate: template<class ... _Elements> void std::swap(std::tuple<_Elements ...>&, std::tuple<_Elements ...>&)\r\n     swap(tuple<_Elements...>& __x, tuple<_Elements...>& __y)\r\n     ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/tuple:1546:5: note:   template argument deduction/substitution failed:\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::tuple<_Elements ...>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'\r\n  swap(std::get<0>(_M_t), __p);\r\n  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/functional:2238:5: note: candidate: template<class _Res, class ... _Args> void std::swap(std::function<_Res(_ArgTypes ...)>&, std::function<_Res(_ArgTypes ...)>&)\r\n     swap(function<_Res(_Args...)>& __x, function<_Res(_Args...)>& __y)\r\n     ^~~~\r\n/apps/gcc/6.2.0/include/c++/6.2.0/functional:2238:5: note:   template argument deduction/substitution failed:\r\nIn file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::function<_Res(_ArgTypes ...)>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'\r\n  swap(std::get<0>(_M_t), __p);\r\n  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\n\r\n", "comments": ["Not sure why the compilers we test with don't catch this. But in any case, feel free to send a pull request!", "Cannot reproduce this on my compiler, @einzigsue  are you using a really old version of the compiler?", "I have approved the pull request #20711. Thank you @SneakyFish5 for your contribution.\r\n"]}, {"number": 20655, "title": "Fix shape fn", "body": "The shape fn used `context->input_tensor(i)` which will result in segfault since the tensor is not available during shape inference. This PR fix the issue as well as making sure that the input/output shape set in the attr match the actual shape before it proceed. The ideal solution would be to run the shape inference using the subgraph, but I'll leave it to the future.", "comments": ["Hi @jjsjann123, would you please help to verify that this fix work with your PR #20350 ?\r\nThanks.", "This fix shouldn't break my code.\r\nWill do a run and report back", "Thanks @jjsjann123. Since you already cherrypick the fix into #20350, I'll close this PR."]}, {"number": 20654, "title": "Required import for workshop notebook", "body": "Required fix for workshop notebook to function correctly (cc @mdanatg)", "comments": ["This is a time-sensitive PR (upcoming workshop presentation). Given that all checks pass, could you please merge ASAP?", "Sorry for the delay, we are in the process of changing PR merge workflow, so there will be some delay this week. We will catch up all the leftovers very soon."]}, {"number": 20653, "title": "Error with Quantizing MNIST with tf.lite", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: bazel\r\n- **TensorFlow version (use command below)**:  commit bcf7e315b4031b3c355af12ca2a4961bcd25c248\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: Build label: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: 1080ti\r\n- **Exact command to reproduce**:\r\n\r\n`bazel build tensorflow/contrib/lite/toco:toco && \\\r\n  ./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=comple_path/frozen_model.pb \\\r\n  --output_file=tflite_model.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shape=\"1,28, 28,1\" \\\r\n  --input_array=\"Inputs\" \\\r\n  --output_array=\"fc2/Relu\"  `\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n`2018-07-09 14:00:04.623621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign\r\n2018-07-09 14:00:04.623726: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign\r\n2018-07-09 14:00:04.623752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd\r\n2018-07-09 14:00:04.623829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.623846: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd\r\n2018-07-09 14:00:04.623879: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.623914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.623930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd\r\n2018-07-09 14:00:04.623961: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624008: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign\r\n2018-07-09 14:00:04.624030: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign\r\n2018-07-09 14:00:04.624098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd\r\n2018-07-09 14:00:04.624146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624193: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd\r\n2018-07-09 14:00:04.624224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign\r\n2018-07-09 14:00:04.624292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign\r\n2018-07-09 14:00:04.624355: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624371: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd\r\n2018-07-09 14:00:04.624403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624436: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624451: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd\r\n2018-07-09 14:00:04.624482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624522: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign\r\n2018-07-09 14:00:04.624547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign\r\n2018-07-09 14:00:04.624606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd\r\n2018-07-09 14:00:04.624652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.624700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd\r\n2018-07-09 14:00:04.624732: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub\r\n2018-07-09 14:00:04.627783: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 229 operators, 362 arrays (0 quantized)\r\n2018-07-09 14:00:04.630029: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 173 operators, 282 arrays (0 quantized)\r\n2018-07-09 14:00:04.632293: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 173 operators, 282 arrays (0 quantized)\r\n2018-07-09 14:00:04.672712: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 107 operators, 201 arrays (1 quantized)\r\n2018-07-09 14:00:04.674373: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 107 operators, 201 arrays (1 quantized)\r\n2018-07-09 14:00:04.676019: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 107 operators, 201 arrays (1 quantized)\r\n2018-07-09 14:00:04.676780: F tensorflow/contrib/lite/toco/tooling_util.cc:1612] Array conv1/act_quant/AssignMinEma/conv1/act_quant/min/Pow, which is an input to the Sub operator producing the output array conv1/act_quant/AssignMinEma/conv1/act_quant/min/sub_2, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\nAborted (core dumped)\r\n`\r\n\r\nI tried it with default_ranges_min, max and it throws the following error\r\n`\r\n2018-07-09 13:56:44.667379: E tensorflow/core/util/command_line_flags.cc:124] Couldn't interpret value  for flag default_ranges_min.\r\n2018-07-09 13:56:44.667451: E tensorflow/core/util/command_line_flags.cc:124] Couldn't interpret value  for flag default_ranges_max.\r\n`\r\n\r\n![screenshot from 2018-07-09 14-03-56](https://user-images.githubusercontent.com/4759327/42475903-ae68adc6-83bb-11e8-9317-fe565127f784.png)\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["It looks like you are trying to convert a training graph, please note that TOCO only supports converting eval graphs.", "Can you expand a bit on it, I did follow the workflow on the documentation page, and did call the create_eval function.", "You should have one graph that is a training graph built with create_training_graph. Then you should create a separate graph that is the corresponding eval graph (just the forward pass, no gradients) that you should create with create_eval_graph.\r\n\r\nThen you should freeze the graph with the freeze_graph tool and provide to tflite_convert.", "Nagging Assignee @raghuraman-k: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@suharshs Could you please tell me what's the correlation with these two graphs?\r\nHow could I use my training graph to generate the eval graph, in other words, what's the point of creating training graph , how should I use it?\r\nThx!"]}, {"number": 20652, "title": "Update cython to 0.28.4", "body": "This fix updates the cython to the latest versioned release of 0.28.4 (released in 07/07/2018).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20651, "title": "MirroredStrategy doesn't support feature embedding across multiple GPUs", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0.176\r\n- **GPU model and memory**: 4 Tesla P40 22919MiB\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI want to train a NN model with 4 GPUs, and this is how I set up my model config in tf.estimator.DNNClassifier:\r\n`distribution = tf.contrib.distribute.MirroredStrategy()\r\n    run_config = tf.estimator.RunConfig(\r\n        train_distribute=distribution,\r\n        log_step_count_steps=1000\r\n    )`\r\n\r\nIt works when I don't have embedding features, and it works if I comment out the \"train_distribute=distribution\" part. When I have embedding features with the distribution config, this is the error I get:\r\n`InternalError: No unary variant device copy function found for direction: 1 and Variant type_name: tensorflow::Tensor\r\n\t [[Node: FunctionBufferingResourceGetNext_1 = FunctionBufferingResourceGetNext[output_types=[DT_INT64, DT_FLOAT, DT_FLOAT, DT_VARIANT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, D`\r\n\r\nI'm wondering whether the current MirroredStrategy doesn't support feature embedding?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I came to the same conclusion, though in my case the error was (using tensorflow-gpu==1.9.0-rc2 on MLE with 4 GPUs):\r\n\r\n    Traceback (most recent call last): \r\n    File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main \"__main__\", fname, loader, pkg_name) \r\n    File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals \r\n    File \"/root/.local/lib/python2.7/site-packages/trainer/controller.py\", line 144, in <module> train(estimator) \r\n    File \"/root/.local/lib/python2.7/site-packages/trainer/controller.py\", line 28, in train throttle_secs=hparams.throttle_secs, \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 454, in train_and_evaluate return executor.run() \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 565, in run getattr(self, task_to_run)() \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 622, in run_master self._start_distributed_training(saving_listeners=saving_listeners) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 776, in _start_distributed_training saving_listeners=saving_listeners) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 375, in train loss = self._train_model(input_fn, hooks, saving_listeners) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1131, in _train_model return self._train_model_distributed(input_fn, hooks, saving_listeners) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1171, in _train_model_distributed self.config) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/python/training/distribute.py\", line 811, in call_for_each_tower return self._call_for_each_tower(fn, *args, **kwargs) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 279, in _call_for_each_tower coord.join(threads) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join six.reraise(*self._exc_info_to_raise) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception yield \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 262, in _call_for_each_tower {t.device: t.merge_args for t in threads}) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 471, in regroup for i in range(len(v0))) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 471, in <genexpr> for i in range(len(v0))) \r\n    File \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 462, in regroup (len(v), len(v0), v, v0)) AssertionError: len(v) == 18, len(v0) == 14, v: [...], v0: [...]\r\n\r\nThe difference between v and v0 is that v contains 4 variables not present in v0. \r\n\r\nv:\r\n\r\n    [(<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fdc4c7b10>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights:0' shape=(868, 10) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights/replica_1:0' shape=(868, 10) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights/replica_2:0' shape=(868, 10) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights/replica_3:0' shape=(868, 10) dtype=float32>})), (<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fdc4c7bd0>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights:0' shape=(291, 6) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights/replica_1:0' shape=(291, 6) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights/replica_2:0' shape=(291, 6) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights/replica_3:0' shape=(291, 6) dtype=float32>})), (<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fdc4c7c10>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights:0' shape=(1001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights/replica_1:0' shape=(1001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights/replica_2:0' shape=(1001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights/replica_3:0' shape=(1001, 8) dtype=float32>})), (<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fdc4c7d10>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights:0' shape=(8001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights/replica_1:0' shape=(8001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights/replica_2:0' shape=(8001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights/replica_3:0' shape=(8001, 8) dtype=float32>})), (<tf.Tensor 'tower_1/deep_exclusive_optimizer/gradients/AddN_4:0' shape=(139, 784) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/0/weights:0' shape=(139, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/0/weights/replica_1:0' shape=(139, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/0/weights/replica_2:0' shape=(139, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/0/weights/replica_3:0' shape=(139, 784) dtype=float32>})), (<tf.Tensor 'tower_1/deep_exclusive_optimizer/gradients/tower_1/deep/layers/0/BiasAdd_grad/tuple/control_dependency_1:0' shape=(784,) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/0/biases:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/0/biases/replica_1:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/0/biases/replica_2:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/0/biases/replica_3:0' shape=(784,) dtype=float32>})), (<tf.Tensor 'tower_1/deep_exclusive_optimizer/gradients/AddN_3:0' shape=(784, 784) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/1/weights:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/1/weights/replica_1:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/1/weights/replica_2:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/1/weights/replica_3:0' shape=(784, 784) dtype=float32>})), (<tf.Tensor 'tower_1/deep_exclusive_optimizer/gradients/tower_1/deep/layers/1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(784,) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/1/biases:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/1/biases/replica_1:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/1/biases/replica_2:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/1/biases/replica_3:0' shape=(784,) dtype=float32>})), (<tf.Tensor 'tower_1/deep_exclusive_optimizer/gradients/AddN_2:0' shape=(784, 784) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/2/weights:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/2/weights/replica_1:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/2/weights/replica_2:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/2/weights/replica_3:0' shape=(784, 784) dtype=float32>})), (<tf.Tensor 'tower_1/deep_exclusive_optimizer/gradients/tower_1/deep/layers/2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(784,) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/2/biases:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/2/biases/replica_1:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/2/biases/replica_2:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/2/biases/replica_3:0' shape=(784,) dtype=float32>})), (<tf.Tensor 'tower_1/deep_exclusive_optimizer/gradients/AddN_1:0' shape=(784, 784) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/3/weights:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/3/weights/replica_1:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/3/weights/replica_2:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/3/weights/replica_3:0' shape=(784, 784) dtype=float32>})), (<tf.Tensor 'tower_1/deep_exclusive_optimizer/gradients/tower_1/deep/layers/3/BiasAdd_grad/tuple/control_dependency_1:0' shape=(784,) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/3/biases:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/3/biases/replica_1:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/3/biases/replica_2:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/3/biases/replica_3:0' shape=(784,) dtype=float32>})), (<tf.Tensor 'tower_1/deep_exclusive_optimizer/gradients/AddN:0' shape=(784, 836) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/exclusive/weights:0' shape=(784, 836) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/exclusive/weights/replica_1:0' shape=(784, 836) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/exclusive/weights/replica_2:0' shape=(784, 836) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/exclusive/weights/replica_3:0' shape=(784, 836) dtype=float32>})), (<tf.Tensor 'tower_1/deep_exclusive_optimizer/gradients/tower_1/deep/exclusive/BiasAdd_grad/tuple/control_dependency_1:0' shape=(836,) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/exclusive/biases:0' shape=(836,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/exclusive/biases/replica_1:0' shape=(836,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/exclusive/biases/replica_2:0' shape=(836,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/exclusive/biases/replica_3:0' shape=(836,) dtype=float32>})), (<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fdc4c7b10>,\r\n\r\n\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights:0' shape=(868, 10) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights/replica_1:0' shape=(868, 10) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights/replica_2:0' shape=(868, 10) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights/replica_3:0' shape=(868, 10) dtype=float32>})), (<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fdc4c7bd0>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights:0' shape=(291, 6) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights/replica_1:0' shape=(291, 6) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights/replica_2:0' shape=(291, 6) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights/replica_3:0' shape=(291, 6) dtype=float32>})), (<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fdc4c7c10>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights:0' shape=(1001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights/replica_1:0' shape=(1001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights/replica_2:0' shape=(1001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights/replica_3:0' shape=(1001, 8) dtype=float32>})), (<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fdc4c7d10>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights:0' shape=(8001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights/replica_1:0' shape=(8001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights/replica_2:0' shape=(8001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights/replica_3:0' shape=(8001, 8) dtype=float32>}))], \r\n\r\nv0:\r\n\r\n    [(<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fe47b59d0>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights:0' shape=(868, 10) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights/replica_1:0' shape=(868, 10) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights/replica_2:0' shape=(868, 10) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/brand_embedding/embedding_weights/replica_3:0' shape=(868, 10) dtype=float32>})), (<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fe47b5a90>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights:0' shape=(291, 6) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights/replica_1:0' shape=(291, 6) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights/replica_2:0' shape=(291, 6) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_category_embedding/embedding_weights/replica_3:0' shape=(291, 6) dtype=float32>})), (<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fe47b5ad0>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights:0' shape=(1001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights/replica_1:0' shape=(1001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights/replica_2:0' shape=(1001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/meta_rawCategory_idx_embedding/embedding_weights/replica_3:0' shape=(1001, 8) dtype=float32>})), (<tensorflow.python.framework.ops.IndexedSlices object at 0x7f9fe47b5bd0>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights:0' shape=(8001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights/replica_1:0' shape=(8001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights/replica_2:0' shape=(8001, 8) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/input_from_feature_columns/input_layer/title_refined_idx_embedding/embedding_weights/replica_3:0' shape=(8001, 8) dtype=float32>})), (<tf.Tensor 'deep_exclusive_optimizer/gradients/AddN_4:0' shape=(139, 784) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/0/weights:0' shape=(139, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/0/weights/replica_1:0' shape=(139, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/0/weights/replica_2:0' shape=(139, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/0/weights/replica_3:0' shape=(139, 784) dtype=float32>})), (<tf.Tensor 'deep_exclusive_optimizer/gradients/deep/layers/0/BiasAdd_grad/tuple/control_dependency_1:0' shape=(784,) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/0/biases:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/0/biases/replica_1:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/0/biases/replica_2:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/0/biases/replica_3:0' shape=(784,) dtype=float32>})), (<tf.Tensor 'deep_exclusive_optimizer/gradients/AddN_3:0' shape=(784, 784) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/1/weights:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/1/weights/replica_1:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/1/weights/replica_2:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/1/weights/replica_3:0' shape=(784, 784) dtype=float32>})), (<tf.Tensor 'deep_exclusive_optimizer/gradients/deep/layers/1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(784,) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/1/biases:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/1/biases/replica_1:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/1/biases/replica_2:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/1/biases/replica_3:0' shape=(784,) dtype=float32>})), (<tf.Tensor 'deep_exclusive_optimizer/gradients/AddN_2:0' shape=(784, 784) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/2/weights:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/2/weights/replica_1:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/2/weights/replica_2:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/2/weights/replica_3:0' shape=(784, 784) dtype=float32>})), (<tf.Tensor 'deep_exclusive_optimizer/gradients/deep/layers/2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(784,) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/2/biases:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/2/biases/replica_1:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/2/biases/replica_2:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/2/biases/replica_3:0' shape=(784,) dtype=float32>})), (<tf.Tensor 'deep_exclusive_optimizer/gradients/AddN_1:0' shape=(784, 784) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/3/weights:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/3/weights/replica_1:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/3/weights/replica_2:0' shape=(784, 784) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/3/weights/replica_3:0' shape=(784, 784) dtype=float32>})), (<tf.Tensor 'deep_exclusive_optimizer/gradients/deep/layers/3/BiasAdd_grad/tuple/control_dependency_1:0' shape=(784,) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/layers/3/biases:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/layers/3/biases/replica_1:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/layers/3/biases/replica_2:0' shape=(784,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/layers/3/biases/replica_3:0' shape=(784,) dtype=float32>})), (<tf.Tensor 'deep_exclusive_optimizer/gradients/AddN:0' shape=(784, 836) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/exclusive/weights:0' shape=(784, 836) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/exclusive/weights/replica_1:0' shape=(784, 836) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/exclusive/weights/replica_2:0' shape=(784, 836) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/exclusive/weights/replica_3:0' shape=(784, 836) dtype=float32>})), (<tf.Tensor 'deep_exclusive_optimizer/gradients/deep/exclusive/BiasAdd_grad/tuple/control_dependency_1:0' shape=(836,) dtype=float32>,\r\n\r\n    MirroredVariable({'/replica:0/task:0/device:GPU:0': <tf.Variable 'deep/exclusive/biases:0' shape=(836,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:1': <tf.Variable 'deep/exclusive/biases/replica_1:0' shape=(836,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:2': <tf.Variable 'deep/exclusive/biases/replica_2:0' shape=(836,) dtype=float32>\r\n    /replica:0/task:0/device:GPU:3': <tf.Variable 'deep/exclusive/biases/replica_3:0' shape=(836,) dtype=float32>}))]\r\n\r\nAll 4 variables are present twice in v, and are `embedding_weights` variables created using `tf.feature_column.input_layer`. My model_fn is similar to the [canned DNN classifier](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/estimator/canned/dnn.py#L86); swapping embedding-based inputs (tf.feature_column.embedding_column) for 1-hot inputs (tf.feature_column.categorical_column_with_identity) bypasses this issue.", "This seems the same root cause as https://github.com/tensorflow/tensorflow/issues/20521, will wait for @rohan100jain's investigation of that issue.", "Nagging Assignee @guptapriya: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looks like the related issue (https://github.com/tensorflow/tensorflow/issues/20521) was fixed. @QianJenLiu / @mattiasarro could you try out your case again and see if it is fixed? ", "Closing for now, please re-open if this is still an issue", "I came across the same problem, how did you solve it?", "I came across the same problem too, just like [here](https://stackoverflow.com/questions/52565923/tensorflow-estimators-mirrorstrategy-assertion-error)"]}, {"number": 20650, "title": "[tf.keras] Bugfix - Add Stateful Metrics to fit_generator", "body": "Description:\r\n\r\nThis should address the 2nd (of 3 issues) in https://github.com/tensorflow/tensorflow/issues/20529.\r\n\r\nThis is just a sync with Keras master branch also: https://github.com/keras-team/keras/blob/master/keras/engine/training_generator.py\r\n\r\n", "comments": ["Nagging Assignee @qlzh727: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @qlzh727: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Ping @fchoolet for review again.\r\n\r\nBtw, the lint is failing for this PR. Please please accordingly.\r\n\r\nhttps://source.cloud.google.com/results/invocations/9ec5867b-8d7f-4d5d-b5a1-13d9cce2271a/targets", "Hopefully fixed the pylinting, ran it locally.\r\n\r\nI changed to ``all_metrics`` to mirror ``training_array.py`` because the linter does not like ``_callbacks``\r\n\r\n@fchollet ", "Looks like this was refactored, going to close."]}, {"number": 20649, "title": "Update 1_basic.ipynb", "body": "", "comments": []}, {"number": 20648, "title": "Fix json notebook", "body": "", "comments": []}, {"number": 20647, "title": "tf-trt converting. support for FC.", "body": "Hello.\r\ntensorflow.contrib.tensorrt.convert will support the fully connected layer?\r\nThx.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@aaroey do you have any comment on this?", "@jjsjann123 any comments?", "Totally missed this one.\r\nWith TRT 4 it is already supported (in fp32 & fp16)\r\nI believe our current PR #21075 should handle fully connected layers (MatMul) between a tensor & a constant.", "This should be fixed by #21075, closing."]}, {"number": 20646, "title": "Fix build for Tensorflow Lite", "body": "- tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:\r\nMove `GetValueType()` to protected because it's called by\r\n`TypedBenchmarkParam`.\r\n\r\n- tensorflow/contrib/lite/profiling/profile_summarizer.cc:\r\n`string` => `std::string`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@googlebot", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @qlzh727   There is already a fix under submission: pull #20330 .", "@shashishekhar OK I'll close this then."]}, {"number": 20645, "title": "sys/sendfile.h does not exist for FreeBSD.", "body": "sys/sendfile.h should be ignored for FreeBSD as it is ignored for Mac OS X.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@kjopek Can u please sign the CLA? Thanks.", "CLA signed, waiting for Google for approval.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 20644, "title": "feature request: Robbins-Monro type learning rate decay", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nI was wondering if there is any appetite for a [Robbins-Monro](https://en.wikipedia.org/wiki/Stochastic_approximation#Robbins%E2%80%93Monro_algorithm) type learning rate decay in tensorflow? The decay would be roughly (a more general solution is implemented at the bottom):\r\n\r\n```python\r\ndecayed_learning_rate = learning_rate * (global_step + 1) ^ (-decay_rate) \r\n```\r\n\r\nAs far as I can tell it is not already implemented in tensorflow, which surprised me since I think this is the learning rate decay rate required for theoretical convergence using the Adam optimizer in Section 4 of [the paper](https://arxiv.org/pdf/1412.6980.pdf), which has:\r\n\r\n```python\r\nalpha_t = alpha / sqrt(t)\r\n```\r\n\r\nwhich is the same as the first equation with `decay_rate = 0.5`, and I assume they start at `t = 1` while tensorflow starts with `global_step = 0`.\r\n\r\nI have an implementation I have been using (mostly copied from the [already implemented ones](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/learning_rate_decay.py)):\r\n\r\n```python\r\ndef robbins_monro_decay(learning_rate,\r\n                        global_step,\r\n                        decay_steps,\r\n                        decay_rate,\r\n                        staircase=False,\r\n                        name=None):\r\n    \"\"\"A Robbins-Monro type decay\"\"\"\r\n\r\n    if global_step is None:\r\n        raise ValueError(\"global_step is required for robbins_monro_decay.\")\r\n\r\n    with tf.name_scope(\r\n            name,\r\n            \"RobbinsMonroDecay\",\r\n            [learning_rate, global_step, decay_steps, decay_rate]) as name:\r\n\r\n        learning_rate = tf.convert_to_tensor(\r\n            learning_rate, name=\"learning_rate\")\r\n        dtype = learning_rate.dtype\r\n        decay_steps = tf.cast(decay_steps, dtype)\r\n        decay_rate = tf.cast(decay_rate, dtype)\r\n        global_step = tf.cast(global_step, dtype)\r\n        p = global_step / decay_steps\r\n\r\n        if staircase:\r\n            p = tf.floor(p)\r\n\r\n        return tf.multiply(\r\n            learning_rate, tf.pow(p + 1, -decay_rate), name=name)\r\n```\r\n\r\nI can make a full pull request if that would be useful? I would be more than happy to add some additional documentation/code or change any of the code/naming/whatever.\r\n\r\nThanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Yes - as far as I am aware this feature is still missing from tensorflow and it would be great for someone to take a quick look and let me know if it is worth me making a pull request.", "Interested to try this out!", "@jeffpollock9,\r\nSorry for the delayed response. Can you please check  [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) is what you are looking for? Thanks!\r\n", "@rmothukuru thanks for getting back on this. The docs for the keras optimizer have:\r\n\r\n``` python\r\ndef decayed_learning_rate(step):\r\n  return initial_learning_rate * decay_rate ^ (step / decay_steps)\r\n```\r\nwhereas the Robbins-Monro one is:\r\n\r\n``` python\r\ndecayed_learning_rate = initial_learning_rate * (step + 1) ^ (-decay_rate) \r\n```\r\n\r\nSo I don't see how they could be the same.", "@jeffpollock9 May be it is easy to implement the above  Robbins-Monro  learning rate decay through [tf.keras.callbacks.LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler). You could also implement a custom callback to schedule the learning rate.\r\n\r\nAs this was an old issue, i guess you might have already implemented Robbins-Monro  learning rate decay method. If you want to contribute, you could raise a PR in TF-addons repo. Thanks!\r\n\r\nPlease let us know what you think. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @jeffpollock9 May be it is easy to implement the above Robbins-Monro learning rate decay through [tf.keras.callbacks.LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler). You could also implement a custom callback to schedule the learning rate.\r\n> \r\n> As this was an old issue, i guess you might have already implemented Robbins-Monro learning rate decay method. If you want to contribute, you could raise a PR in TF-addons repo. Thanks!\r\n> \r\n> Please let us know what you think. Thanks!\r\n\r\nHi @jvishnuvardhan I did indeed implement this learning rate, you can see it if you scroll to the top of the page.\r\n\r\nI'm not really interested in this any more though unfortunately, mostly because it's been 3 years since opened this issue. If you would like to take my implementation and contribute it though, I think that'd be great. Thanks. ", "Development of keras moved to separate [keras-team/keras](https://github.com/keras-team/keras/issues) repository. \r\n\r\nI will close the issue here and open this in keras-team/keras repo and let team evaluate this feature. Thanks!"]}, {"number": 20643, "title": "Custom loss function for computation graph", "body": "I want to evaluate my model in android . But it has a custom loss function similar to the one used in popular siamese neural network. How can I use TensorflowInferenceInterface in android to achieve it.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Os- Android\r\nDistribution - Api 25 Nougat\r\nTensorflow installed from- Gradle dependency\r\nTensorflow version- 1.8\r\n\r\n", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Still waiting ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20642, "title": "init_fn bug in estimator._train_model_distributed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 2.7\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: GTX 980M\r\n\r\n### Describe the problem\r\nThere is a bug in https://github.com/tensorflow/tensorflow/blob/4e883df1eaf52b7c655365eb5f6ecd9e5fe37457/tensorflow/python/estimator/estimator.py#L1327.\r\n\r\ninit_fn should take two arguments. However, the init_fn obtained in https://github.com/tensorflow/tensorflow/blob/4e883df1eaf52b7c655365eb5f6ecd9e5fe37457/tensorflow/python/estimator/estimator.py#L1269 is a lambda function which only takes one argument.\r\n\r\n\r\nCurrently, I am using the following line as a workaround.\r\n```\r\ninit_fn=init_fn and (lambda x, y: init_fn(y)))\r\n```", "comments": ["1. shouldn't init_fn always take two arguments?\r\n2. do you have a code snippet for verifying this?", "@tanzhenyu \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n\r\n  def init_fn(scaffold, session):\r\n    pass\r\n\r\n  scaffold = tf.train.Scaffold(init_fn=init_fn)\r\n  return tf.estimator.EstimatorSpec(\r\n    mode=mode,\r\n    loss=labels[0],\r\n    train_op=tf.no_op(),\r\n    scaffold=scaffold\r\n  )\r\n\r\n\r\ndef input_fn():\r\n  dataset = tf.data.Dataset.from_tensors((np.zeros(10), np.zeros(1)))\r\n  dataset = dataset.repeat()\r\n  return dataset\r\n\r\n\r\ndef main(_):\r\n  distribution_strategy = tf.contrib.distribute.OneDeviceStrategy(\r\n    \"device:GPU:0\")\r\n\r\n  run_config = tf.estimator.RunConfig(train_distribute=distribution_strategy)\r\n\r\n  estimator = tf.estimator.Estimator(\r\n    model_fn=model_fn,\r\n    config=run_config\r\n  )\r\n\r\n  estimator.train(input_fn=input_fn, steps=10)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n\r\nI got the following error when running the above program.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/yfeng23/test/tf/init_fn_test.py\", line 40, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/yfeng23/test/tf/init_fn_test.py\", line 36, in main\r\n    estimator.train(input_fn=input_fn, steps=10)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 978, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1057, in _train_with_estimator_spec\r\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 405, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 816, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 539, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1002, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1007, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 696, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 467, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 287, in prepare_session\r\n    init_fn(sess)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 163, in <lambda>\r\n    self._init_fn = lambda sess: init_fn(self, sess)\r\nTypeError: <lambda>() takes exactly 1 argument (2 given)\r\n```", "@ispirmustafa can you take a look at this?", "@josh11b this is about grouping multiple scaffolds. Based on desired outcome, either scaffold._user_init_fn should be used or a lambda should be created which a dummy first argument. \r\nCould you please take a look?", "I think https://github.com/tensorflow/estimator/commit/1f478bf50e922202e6a378557878bd049d4c9007#diff-d934b9f2c2d9384e077e7ab45e001f69 seems like a fix for this. @fengyang0317 can you try? ", "Yes. The bug is fixed.", "Thanks for verifying! "]}, {"number": 20641, "title": "Fix typos", "body": "This PR fixes some typos: `unecessary`, `parition`, `seperately`, `exectutes`, `ouputs`, `funcion`, `funtions`, and `corresonding`.", "comments": []}, {"number": 20640, "title": "Weird issue when using TextLineReader nested in a function", "body": "### System information\r\n- WIndows 7 enterprise:\r\n- TensorFlow installed from conda:\r\n- TensorFlow version (1.8.0):\r\n- Python version (3.6.5), installed with Anaconda: \r\n- Not using GPU\r\n- Reproduction: unzip the following zip file, two files, test.py and test.txt, under tensorflow environment, \r\nrun test.py.\r\n[tf bug report.zip](https://github.com/tensorflow/tensorflow/files/2179025/tf.bug.report.zip)\r\n\r\n### Describe the problem\r\nI was testing the functionality of TextLineReader, a very simple try. Here is my code, just a few lines:\r\n###### MY CODE#######\r\nimport tensorflow as tf\r\n\r\ndef input_func():\r\n    file_queue = tf.train.string_input_producer([\"D:/tmp_10_data_test.txt\"], num_epochs=5)\r\n    reader = tf.TextLineReader(skip_header_lines=1)\r\n    return reader.read_up_to(file_queue, num_records=5)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(tf.local_variables_initializer())\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n    print(sess.run(input_func()))\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n\r\n\r\n###################\r\nThe weird part is, when I'm running this piece of code, the program keeps running without any error, any output, and it just runs forever. This issue happens when I run the code with PyCharm and Jupyter Notebook. I resolved it by moving the file_queue declaration outside the function as:\r\n#####################\r\nfile_queue = tf.train.string_input_producer([\"D:/tmp_10_data_test.txt\"], num_epochs=5)\r\ndef input_func():\r\n    reader = tf.TextLineReader(skip_header_lines=1)\r\n    return reader.read_up_to(file_queue, num_records=5)\r\n######################\r\nAnd the program runs correctly and outputs the result as expected. But This is just weird, what happened when I declare the file queue inside the very simple function? Shouldn't it be the same regardless of where it is declared? I guess you can replace the file with your own multi-line txt file and the problem should still exist, although I didn't try it...\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I believe the problem is that you can't start the queue runners until you have a queue created or else it will dead lock. That is what your workaround does. You might want to take a look at using the dataset feeding mechanism which is much simpler to use and more intuitive.\r\n ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Sorry for the late response. This is no more an issue for me. Thanks for\nyour kind explanation.\n\nBest regards,\nQingyu\n\n2018-08-08 3:10 GMT+08:00 Alfred Sorten Wolf <notifications@github.com>:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20640#issuecomment-411166936>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALFbPafup2w7rlR1Zfe6I9ad70hO3Dhdks5uOeYdgaJpZM4VHNgl>\n> .\n>\n", "Thanks for responding. Closing for now."]}, {"number": 20639, "title": "Assignment inside while loop affected by concurrency", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n      Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n        Windows 8 (CPU)\r\n- **TensorFlow installed from (source or binary)**:\r\n        Binary\r\n- **TensorFlow version (use command below)**:\r\n       1.2.1\r\n- **Python version**: \r\n      Python 3.6.5 |Anaconda\r\n- **Bazel version**: \r\n    N/A\r\n- **CUDA/cuDNN version**:\r\n      N/A\r\n- **GPU model and memory**:\r\n     N/A\r\n- **Exact command to reproduce**:\r\n\r\nNo gpu\r\n\r\n### Describe the problem\r\n\r\nI wrote this code to answer a SO question but then realized the results are inconsistent probably due to the concurrency involved. The correct answer is returned repeatedly a few times but then very randomly. All other answers are wrong.\r\nI have also read about a switch that turns off concurrent execution but generally the concurrency API and the memory model take care of almost all cases without the involvement of the programmer. In the case of Java it is [JMM](http://download.oracle.com/otndocs/jcp/memory_model-1.0-pfd-spec-oth-JSpec/)\r\n\r\nIt isn't clear whether this is concurrency or parallelism ?\r\n\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nv = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\nn = len(v)\r\na1 = tf.Variable(v, name = 'a')\r\n\r\ndef cond(i, _):\r\n    return i < n\r\n\r\ns = tf.InteractiveSession()\r\ns.run(tf.global_variables_initializer())\r\n\r\ndef body( i, _):\r\n    x = a1[i-1]\r\n    y = a1[i-2]\r\n    z = tf.add(x,y)\r\n    op = a1[i].assign( tf.add(x,y) )\r\n    increment = tf.add(i, 1)\r\n    return increment, op\r\n\r\nprint(s.run(tf.while_loop(cond, body, [2, a1])))\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "That information is updated.", "@skye so you have any comment on this. I was not able to reproduce this on Ubuntu, is it Windows specific?", "```\r\n(11, array([1, 1, 2, 1, 0, 1, 0, 1, 1, 0, 0]))\r\n(11, array([ 1,  1,  2,  3,  5,  8,  5,  5, 10, 15, 10]))\r\n(11, array([ 1,  1,  2,  3,  5,  8, 13, 21, 34, 21,  0]))\r\n(11, array([ 1,  1,  2,  3,  5,  8, 13, 21, 34,  0,  0]))\r\n(11, array([ 1,  1,  2,  3,  5,  8, 13, 21, 13, 13, 13]))\r\n(11, array([ 1,  1,  2,  1,  3,  4,  7, 11, 18, 29, 47]))\r\n(11, array([ 1,  1,  2,  3,  5,  8, 13, 21,  0,  0,  0]))\r\n(11, array([ 1,  1,  2,  3,  5,  8, 13, 21, 34, 55,  0]))\r\n```\r\n\r\nThis set doesn't have the correct result  (11, array([ 1,  1,  2,  3,  5,  8, 13, 21, 34, 55, 89]))\r\n", "Can you try adding a control dependency on `op` to `increment`? Something like:\r\n```\r\nwith tf.control_dependencies([op]):\r\n  increment = tf.add(i, 1)\r\n```", "Yes. That produces the correct result. Is this a thread-safety rule ?", "No, it's about TF's execution model. You could theoretically see this result with a single thread.\r\n\r\nHere's the issue: TF will execute an operation once its inputs are ready, which may not correspond to the order operations are defined in your program. If the inputs to multiple ops are ready, there are no guarantees about which one will run first.\r\n\r\nIn the case of while_loop, you can imagine the loop is unrolled:\r\n```python\r\ni1 = 2\r\n# iteration 1\r\nx1 = a1[i1-1]\r\ny1 = a1[i1-2]\r\nz1 = tf.add(x1,y1)\r\nop1 = a1[i1].assign( tf.add(x1,y1) )\r\nincrement1 = tf.add(i1, 1)\r\ni2 = increment1\r\n# iteration 2\r\nx2 = a1[i2-1]\r\ny2 = a1[i2-2]\r\nz2 = tf.add(x2,y2)\r\nop2 = a1[i2].assign( tf.add(x2,y2) )\r\nincrement2 = tf.add(i2, 1)\r\n# ...\r\n```\r\nGiven that the only data input to iteration 2 is `i2`, which ultimately only depends on `i1`, TF can actually start executing iteration 2 before iteration 1 completes. This means it might run `op2` before `op1` is run, which is what causes the wrong results in your original example. Adding the control dependency forces `op{i}` to complete before `increment{i}` can run, which forces all the assign ops to run in the correct order.\r\n\r\nThe root issue is that the assign ops have visible side effects (i.e. writing to a1, which is then read in subsequent operations), but the execution doesn't take this into account; it only looks at the direct inputs and control deps.\r\n\r\nThis is obviously super confusing and error-prone, so we're working on coming up with better execution semantics. One option is to enable [eager exeuction](https://www.tensorflow.org/guide/eager), which does run everything in program order.\r\n\r\nAnother option is to use [tf.contrib.eager.defun](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun), although this is still being actively developed. This creates a graph, but automatically makes sure that ops with side effects (like variable assignment) happen in the order they're defined. This way you don't have to worry about manually adding control dependencies.\r\n\r\nSorry this is so long, I hope it helps. I'm gonna close this issue since it's not actually a concurrency bug,  but feel free to comment if something is still confusing or you have more questions.", "I got this error too. This error is so vague. if while_loop works in this weird way, then how can anybody even know their program is working properly?  this terribly/disastrously   designed API should be deprecated from using at all.", "Agreed! tf.function (part of TF2) actually fixes this. See https://www.tensorflow.org/alpha/tutorials/eager/tf_function#state_in_tffunction:\r\n\r\n> For example, when writing code which has multiple reads and writes to the same variables, a dataflow graph might not naturally encode the originally intended order of operations. In tf.function, however, because we're converting code which was traced from Python, we know the intended execution order.\r\n> \r\n> This means there's no need to add manual control dependencies; tf.function is smart enough to add the minimal set of necessary and sufficient control dependencies for your code to run correctly.\r\n\r\nThis holds true if you use a while_loop inside a tf.function. So yes, the original tf.Graph behavior will be deprecated in favor of tf.function :)", "> Agreed! tf.function (part of TF2) actually fixes this. See https://www.tensorflow.org/alpha/tutorials/eager/tf_function#state_in_tffunction:\r\n> \r\n> > For example, when writing code which has multiple reads and writes to the same variables, a dataflow graph might not naturally encode the originally intended order of operations. In tf.function, however, because we're converting code which was traced from Python, we know the intended execution order.\r\n> > This means there's no need to add manual control dependencies; tf.function is smart enough to add the minimal set of necessary and sufficient control dependencies for your code to run correctly.\r\n> \r\n> This holds true if you use a while_loop inside a tf.function. So yes, the original tf.Graph behavior will be deprecated in favor of tf.function :)\r\n\r\nI still need to use tf 1.XX, not every company is ready to move to tf 2.0.\r\nFor example, How do I make the output of `t_var` not random? thanks!  \r\nif I use CPU, it's not random, if I GPU, it's random. \r\nhttps://colab.research.google.com/drive/1WcBo8HLIe8Wq9yakNNAYINwiP6GrmAQI\r\n", "tf.function will also be available in TF 1.14 (it's already in the nightly releases, if you're brave).\r\n\r\nI don't have permission to see your colab. Also, I switched teams and am no longer working on TF... I suggest filing a new issue if you're having trouble with something.", "> tf.function will also be available in TF 1.14 (it's already in the nightly releases, if you're brave).\r\n> \r\n> I don't have permission to see your colab. Also, I switched teams and am no longer working on TF... I suggest filing a new issue if you're having trouble with something.\r\n\r\nOhh thanks!, unfortuantely I am still using tf 1.12. : / . new link is here: https://colab.research.google.com/drive/1uz_64qp6rlTQRbDRqOye6s7yddnbCODB .  thanks! : D\r\n", "> > tf.function will also be available in TF 1.14 (it's already in the nightly releases, if you're brave).\r\n> > I don't have permission to see your colab. Also, I switched teams and am no longer working on TF... I suggest filing a new issue if you're having trouble with something.\r\n> \r\n> Ohh thanks!, unfortuantely I am still using tf 1.12. : / . new link is here: https://colab.research.google.com/drive/1uz_64qp6rlTQRbDRqOye6s7yddnbCODB . thanks! : D\r\n\r\nThe \"tf.assign_add\" function has a parameter \"use_locking\", set it to true and you will get fixed result each time. I think this  can make thread safety.", "tf.function still have similar problem.\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(0)\r\n\r\n\r\n@tf.function\r\ndef f():\r\n    dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\r\n    for i in dataset:\r\n        a.assign(i)\r\n        tf.print(a)\r\n    tf.print(a)\r\n\r\n\r\nf()\r\n```\r\nThe code above produce `1 2 3 0` in TF 2.1.\r\nUsing `use_locking=True` doesn't help.\r\n\r\nHowever, TF 2.0 will produce `1 2 3 3`."]}, {"number": 20638, "title": "[Feature Request] Deformable Convolution", "body": "Hi,  TF.\r\n\r\nI wonder if you have any plan to add Deformable Convolution Feature.\r\nhttps://arxiv.org/abs/1703.06211\r\nhttps://github.com/msracver/Deformable-ConvNets\r\n\r\nThank you always, for nice work.\r\n\r\n------------------------", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20637, "title": "Update deploy docs to use Minio distributed mode as well", "body": "Based on discussion here (https://github.com/minio/minio/issues/5797#issuecomment-38089813),\r\ndistributed erasure code deployment works fine with Tensorflow.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "cc @elsonrodriguez ", "Yeah, it looks good. Sorry I forgot to go back and remove this note. The Minio team has long since fixed this bug."]}, {"number": 20636, "title": "[INTEL MKL] Enabled reorder primitive reuse for conv backprop", "body": "", "comments": []}, {"number": 20635, "title": "Unexpected result when load new model versions", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7.0\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**:  4.8.5\r\n- **CUDA/cuDNN version**: 5.1.10\r\n- **GPU model and memory**: p40 etc\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nUsing tensorflow serving with hdfs and serving automatic loading new versions based on 1 second interval. Due to the load of hdfs, variables.index file may appeared later than saved_model.pb.\r\n\r\nSo, the unexpected things appeard, you can see the log in the next few lines.\r\n\r\n### Source code / logs\r\n\r\nwhen predict serving returns :\r\n\r\nAttempting to use uninitialized value conv1d_1_1/kernel\r\n       [[Node: conv1d_1_1/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv1d_1_1/kernel\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv1d_1_1/kernel)]]\r\n       [[Node: dense_1_1/BiasAdd/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_178_dense_1_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nwhen load new version, serving print :\r\n\r\n2018-06-25 16:56:40.483957: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:165] The specified SavedModel has no variables; no checkpoints were restored.\r\n\r\n### Some thoughts\r\n\r\nIt's hard to say who is wrong, but should tensorflow consider and prevent this inconsistency?\r\n\r\n", "comments": ["@nfiedel I can't assign this to you (could you join the TF org?) but what do you think?", "Based on the problem description, it sounds like you are writing the model directly to the directory that TensorFlow Serving is polling for new model versions. Instead, please export (or copy/upload if that is your workflow) to a temporary directory, and then use one of the atomic file operations such as move, to move the saved model directory into place.", "@nfiedel yes, thanks for your reply, your suggestions can avoid this problem. But should we prevent this problem from the code? Or should we make obvious instructions for the user to see?", "@weberxie preventing this in code is particularly challenging across many distributed/inconsistent filesystems. Making this more obvious in the instructions is a great idea \u2013 I'll see if we can do this either now or in coming doc refreshes. Thanks!", "I'll close this for now, thanks @nfiedel."]}, {"number": 20634, "title": "Add pets_labels_list.txt to Android example", "body": "Add pets_labels_list.txt to Android assets directory for TensorFlow Lite Object Detection app.", "comments": ["Closing this as this was added in https://github.com/tensorflow/tensorflow/commit/216887dfab11e3f76b79abb1d4ff04ab5382f509"]}, {"number": 20633, "title": "Delete 3_datasets.ipynb", "body": "", "comments": []}, {"number": 20632, "title": "Input node name correction", "body": "Correct for the error\r\n```\r\nSTARTING!\r\nNum runs: [20]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [-1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nWarmup runs: [1]\r\nGraph: [/data/local/tmp/mobilenet.tflite]\r\nInput layers: [input]\r\nInput shapes: [1,224,224,3]\r\nUse nnapi : [0]\r\nLoaded model /data/local/tmp/mobilenet.tflite\r\nresolved reporter\r\nTensor # 88 is named Placeholder but flags call it input\r\nAborted\r\n```", "comments": ["@aselle could you look please?", "ping for review or reassign @aselle ", "Can one of the admins verify this patch?", "closing this PR as contrib folder will be depricated in 2.0, thank you.\r\nCC @mihaimaruseac"]}]