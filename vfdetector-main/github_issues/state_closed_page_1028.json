[{"number": 22493, "title": "Added the feature to disable MKL support of TensorFlow by environment\u2026", "body": "\u2026al variable TF_DISABLE_MKL=1", "comments": ["Also, what do you plan to do with MKL ops that just replace TF ops (without graph rewrite)? For example, the `Transpose` op?\r\nhttps://github.com/tensorflow/tensorflow/blob/cc83067469bc30bba55932c587f31ef68f15792f/tensorflow/core/kernels/transpose_op.cc#L221-L246", "> Also, what do you plan to do with MKL ops that just replace TF ops (without graph rewrite)? For example, the `Transpose` op?\r\n> [tensorflow/tensorflow/core/kernels/transpose_op.cc](https://github.com/tensorflow/tensorflow/blob/cc83067469bc30bba55932c587f31ef68f15792f/tensorflow/core/kernels/transpose_op.cc#L221-L246)\r\n> \r\n> Lines 221 to 246 in [cc83067](/tensorflow/tensorflow/commit/cc83067469bc30bba55932c587f31ef68f15792f)\r\n> \r\n>  #if defined(INTEL_MKL) && defined(ENABLE_MKL) \r\n>  #define REGISTER(T)                                   \\ \r\n>    REGISTER_KERNEL_BUILDER(Name(\"Transpose\")           \\ \r\n>                                .Device(DEVICE_CPU)     \\ \r\n>                                .TypeConstraint<T>(\"T\") \\ \r\n>                                .HostMemory(\"perm\"),    \\ \r\n>                            MklTransposeCpuOp);         \\ \r\n>    REGISTER_KERNEL_BUILDER(Name(\"ConjugateTranspose\")  \\ \r\n>                                .Device(DEVICE_CPU)     \\ \r\n>                                .TypeConstraint<T>(\"T\") \\ \r\n>                                .HostMemory(\"perm\"),    \\ \r\n>                            MklConjugateTransposeCpuOp); \r\n>   \r\n>  #else  // INTEL_MKL && ENABLE_MKL \r\n>  #define REGISTER(T)                                   \\ \r\n>    REGISTER_KERNEL_BUILDER(Name(\"Transpose\")           \\ \r\n>                                .Device(DEVICE_CPU)     \\ \r\n>                                .TypeConstraint<T>(\"T\") \\ \r\n>                                .HostMemory(\"perm\"),    \\ \r\n>                            TransposeCpuOp);            \\ \r\n>    REGISTER_KERNEL_BUILDER(Name(\"ConjugateTranspose\")  \\ \r\n>                                .Device(DEVICE_CPU)     \\ \r\n>                                .TypeConstraint<T>(\"T\") \\ \r\n>                                .HostMemory(\"perm\"),    \\ \r\n>                            ConjugateTransposeCpuOp); \r\n>  #endif  // INTEL_MKL && ENABLE_MKL\r\n\r\nWe will have another PR to disable those ops, since they are different implementation with MKLDNN based ops.", "I have made all changes as your review, please check if they are ok. We want to check it in ASAP.\r\nThanks!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@penpornk  I pushed changes as you had requested thanks. Also, regarding the CLA,  I am ok with my commits being contributed to this project.", "@penpornk yes, Ramesh's change is ok.", "I am ok with my commits being contributed to this project", "Thank you very much for the PR!", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "Changed `cla` to `yes` because all the authors have confirmed their consent.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I am ok with my commits being contributed to this project", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "I am ok with my commits being contributed to this project"]}, {"number": 22492, "title": "failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS", "body": " when I train keras_retinanet model with tf-gpu, the following errors occured. CUDA_ERROR_ILLEGAL_ADDRESS.\r\nIt's stable reproduction.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nCentOS Linux release 7.4.1708\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nno\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary(pip install tensorflow-gpu)\r\n- **TensorFlow version (use command below)**:\r\n1.10.0-dev20180720\r\nkeras version = 2.2.0\r\n- **Python version**:\r\nPython 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **GCC/Compiler version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\ncuda version = 9.0.176\r\ncudnn version = 7.1.4\r\n- **GPU model and memory**:\r\nTesla P40 ,4 gpu , 22919MiB memory, Driver Version: 390.77\r\n- **Exact command to reproduce**:\r\ndownload open image datasets\r\ngit clone https://github.com/fizyr/keras-retinanet.git\r\npip install . --user\r\npython setup.py build_ext --inplace\r\npython keras_retinanet/bin/train.py --batch-size 16 --gpu 0,1,2,3 --multi-gpu 4 --multi-gpu-force  --steps 100 --snapshot-path oid_snapshots oid /var/oid --parent-label Person\r\n\r\nEpoch 1/50\r\n2018-09-25 14:57:31.436888: E tensorflow/stream_executor/cuda/cuda_driver.cc:1078] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.436888: E tensorflow/stream_executor/cuda/cuda_driver.cc:1078] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.436888: E tensorflow/stream_executor/cuda/cuda_driver.cc:1078] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.436893: E tensorflow/stream_executor/cuda/cuda_driver.cc:1078] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.436980: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x8365350: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.437097: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x89fa330: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.437068: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x7cd6b60: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.437011: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x7622e90: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.437161: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x7cd6b60: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.437173: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x7622e90: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.437113: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x8365350: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.437123: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x89fa330: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-09-25 14:57:31.437207: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n2018-09-25 14:57:31.437211: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n2018-09-25 14:57:31.437315: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n2018-09-25 14:57:31.437303: F tensorflow/stream_executor/cuda/cuda_dnn.cc:211] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n\r\n\r\n\r\ncode address:https://github.com/fizyr/keras-retinanet", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@mahaishou Could you install CUDA9.2? Also, use `pip install tf-nightly-gpu` to install TensorFlow.", "@wt-huang ok, I will try this and update this issue later.", "Is this resolved?"]}, {"number": 22491, "title": "[nGraph] Upgraded to the latest nGraph version.", "body": "1. Updated the nGraph and ngraph-tf version. These latest version contains several bug fixes and feature enhancements. See this for details: https://github.com/NervanaSystems/ngraph-tf/releases/tag/v0.6.0\r\n2. Alphabetically sorted the file list for the bazel files in //thirdparty/ngraph directory", "comments": []}, {"number": 22490, "title": "failed to build tensorfolow lite demo via both bazel and AndroidStudio", "body": "I tried to build the tflite demo apk via bazel.\r\n\r\n\tINFO: SHA256 (https://github.com/bazelbuild/rules_python/archive/8b5d0683a7d878b28fffe464779c8a53659fc645.tar.gz) = 8b32d2dbb0b0dca02e0410da81499eef8ff051dad167d6931a92579e3b2a1d48\r\n\tERROR: error loading package '': Encountered error while reading extension file 'requirements.bzl': no such package '@pip_deps//': Traceback (most recent call last):\r\n\t  File \"/home/local/SPREADTRUM/ben.shi/.cache/bazel/_bazel_spreadtrum.com/1ebbfeac35eb8d759797bb7ba5198a8a/external/io_bazel_rules_python/python/pip.bzl\", line 26\r\n\t  repository_ctx.execute([\"python\", repository_ctx.path(r...(\"\")])\r\n\tFile \"/home/local/SPREADTRUM/ben.shi/.cache/bazel/_bazel_spreadtrum.com/1ebbfeac35eb8d759797bb7ba5198a8a/external/io_bazel_rules_python/python/pip.bzl\", line 29, in repository_ctx.execute\r\n\t  repository_ctx.path(repository_ctx.attr.requirements)\r\n\tNot a regular file: /home/local/SPREADTRUM/ben.shi/Desktop/tensorflow/tensorflow/requirements.txt\r\n\tERROR: error loading package '': Encountered error while reading extension file 'requirements.bzl': no such package '@pip_deps//': Traceback (most recent call last):\r\n\t  File \"/home/local/SPREADTRUM/ben.shi/.cache/bazel/_bazel_spreadtrum.com/1ebbfeac35eb8d759797bb7ba5198a8a/external/io_bazel_rules_python/python/pip.bzl\", line 26\r\n\t  repository_ctx.execute([\"python\", repository_ctx.path(r...(\"\")])\r\n\tFile \"/home/local/SPREADTRUM/ben.shi/.cache/bazel/_bazel_spreadtrum.com/1ebbfeac35eb8d759797bb7ba5198a8a/external/io_bazel_rules_python/python/pip.bzl\", line 29, in repository_ctx.execute\r\n\t  repository_ctx.path(repository_ctx.attr.requirements)\r\n\tNot a regular file: /home/local/SPREADTRUM/ben.shi/Desktop/tensorflow/tensorflow/requirements.txt\r\n\tINFO: Elapsed time: 96.822s\r\n\tINFO: 0 processes.\r\n\tFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\nMy steps\r\n1. git checkout remotes/origin/r1.9\r\n2. bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n\t\r\nI explicitly set NDK and SDK in WORKSPACE as \r\n\r\n\tandroid_sdk_repository(\r\n\t    name = \"androidsdk\",\r\n\t    api_level = 26,\r\n\t    # Ensure that you have the build_tools_version below installed in the\r\n\t    # SDK manager as it updates periodically.\r\n\t    build_tools_version = \"27.0.3\",\r\n\t    # Replace with path to Android SDK on your system\r\n\t    path = \"/home/local/SPREADTRUM/ben.shi/android/sdk\"\r\n\t)\r\n\tandroid_ndk_repository(\r\n\t    name=\"androidndk\",\r\n\t    path=\"/home/local/SPREADTRUM/ben.shi/android/android-ndk-r16b\",\r\n\t    # This needs to be 14 or higher to compile TensorFlow.\r\n\t    # Please specify API level to >= 21 to build for 64-bit\r\n\t    # archtectures or the Android NDK will automatically select biggest\r\n\t    # API level that it supports without notice.\r\n\t    # Note that the NDK version is not the API level.\r\n\t    api_level=28\r\n\t)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "[[Have I written custom code]]\r\n\t    no. pure checkout of branch 1.9\r\n[[OS Platform and Distribution]]\r\n\t    ubuntu 16.04 / amd64\r\n[[TensorFlow installed from]]\r\n\t    git clone https://github.com/tensorflow/tensorflow\r\n[[TensorFlow version]]\r\n\t    git checkout remotes/origin/r1.9\r\n[[Bazel version]]\r\n\t   0.15.2\r\n[[CUDA/cuDNN version]]\r\n\t    not used\r\n[[GPU model and memory]]\r\n\t    not used\r\n[[Exact command to reproduce]]\r\n\t    bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n[[Mobile device]]\r\n\t    pixel 2 with android 9.0", "I also failed to build to Android Studio 3.2, with error message:\r\n\r\n\tCould not find com.android.tools.build:aapt2:3.2.0-4818971.\r\n\tSearched in the following locations:\r\n\t    file:/home/local/SPREADTRUM/ben.shi/android/sdk/extras/m2repository/com/android/tools/build/aapt2/3.2.0-4818971/aapt2-3.2.0-4818971.pom\r\n\t    file:/home/local/SPREADTRUM/ben.shi/android/sdk/extras/m2repository/com/android/tools/build/aapt2/3.2.0-4818971/aapt2-3.2.0-4818971-linux.jar\r\n\t    file:/home/local/SPREADTRUM/ben.shi/android/sdk/extras/google/m2repository/com/android/tools/build/aapt2/3.2.0-4818971/aapt2-3.2.0-4818971.pom\r\n\t    file:/home/local/SPREADTRUM/ben.shi/android/sdk/extras/google/m2repository/com/android/tools/build/aapt2/3.2.0-4818971/aapt2-3.2.0-4818971-linux.jar\r\n\t    file:/home/local/SPREADTRUM/ben.shi/android/sdk/extras/android/m2repository/com/android/tools/build/aapt2/3.2.0-4818971/aapt2-3.2.0-4818971.pom\r\n    \tfile:/home/local/SPREADTRUM/ben.shi/android/sdk/extras/android/m2repository/com/android/tools/build/aapt2/3.2.0-4818971/aapt2-3.2.0-4818971-linux.jar\r\n\t    https://jcenter.bintray.com/com/android/tools/build/aapt2/3.2.0-4818971/aapt2-3.2.0-4818971.pom\t\r\n\t    https://jcenter.bintray.com/com/android/tools/build/aapt2/3.2.0-4818971/aapt2-3.2.0-4818971-linux.jar\r\n\t    https://google.bintray.com/tensorflow/com/android/tools/build/aapt2/3.2.0-4818971/aapt2-3.2.0-4818971.pom\r\n\t\t  https://google.bintray.com/tensorflow/com/android/tools/build/aapt2/3.2.0-4818971/aapt2-3.2.0-4818971-linux.jar\r\n\tRequired by:\r\n\t    project :app\r\n\r\n", "The failure of bazel build is missing when I try to build master branch. But the failure of AndroidStudio still exist. \r\n\r\nDoes it related to the new android studio 3.2? Mine 3.1 automatically upgraded to 3.2 yesterday.", "@benshi001 Please check out Android 3.2 release note:\r\nBeginning with Android Studio 3.2, the source for AAPT2 (Android Asset Packaging Tool 2) is Google's Maven repository.\r\nTo use AAPT2, make sure that you have a google() dependency in your build.gradle file, as shown here:\r\n```\r\n  buildscript {\r\n      repositories {\r\n          google() // here\r\n          jcenter()\r\n      }\r\n      dependencies {\r\n          classpath 'com.android.tools.build:gradle:3.2.0'\r\n      }\r\n  } allprojects {\r\n      repositories {\r\n          google() // and here\r\n          jcenter()\r\n  }\r\n```", "I see. Thank you!"]}, {"number": 22489, "title": "absl/numeric/int128_have_intrinsic.inc: No such file or directory", "body": "Current `tf-nightly-gpu` fails to compile with CUDA-aware custom ops like this:\r\n\r\n```\r\n    cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++\r\n    In file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/absl/strings/numbers.h:37:0,\r\n                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/absl/strings/str_cat.h:62,\r\n                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/stream_executor/lib/strcat.h:21,\r\n                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/stream_executor/launch_dim.h:40,\r\n                     from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/stream_executor/stream.h:36,\r\n                     from horovod/tensorflow/mpi_ops.cc:29:\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/include/absl/numeric/int128.h:698:50: fatal error: absl/numeric/int128_have_intrinsic.inc: No such file or directory\r\n     #include \"absl/numeric/int128_have_intrinsic.inc\"\r\n                                                      ^\r\n    compilation terminated.\r\n```\r\n\r\ncc @yuefengz ", "comments": ["@annarev Do you know how to fix this?", "apparently it just needs an empty file: https://github.com/abseil/abseil-cpp/blob/master/absl/numeric/int128_have_intrinsic.inc", "PR #22719 fixes this", "Nagging Assignee @annarev: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "> apparently it just needs an empty file: https://github.com/abseil/abseil-cpp/blob/master/absl/numeric/int128_have_intrinsic.inc\r\n\r\nThis did not work for me."]}, {"number": 22488, "title": "What's the version of squeezenet, 1.1 or 1.0 in performance part of TFLITE", "body": "link here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/performance.md\r\n\r\nBesides, where can I find the squeezenetV1.1 model of TFLITE with FP32 and no quant. I only found the squeezenetv1.0 [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md#image-classification-float-models), no squeezenetv1.1.\r\n\r\nThanks in advance. \ud83d\ude47 ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@MarkDaoust  @aselle   PTAL", "Nagging Assignees @aselle, @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22487, "title": "Installing Kerras on R/Windows", "body": "Hi All,\r\n\r\nI hope somebody can help.\r\n\r\nI had a well running Keras version running on R, which took me 30 minutes to install. Then I needed to reinstall after more than 20 hours and several workarounds I got everything running except the install_keras() command. See below:\r\n\r\n> install_keras()\r\nUsing r-tensorflow conda environment for TensorFlow installation\r\nDetermining latest release of TensorFlow...done\r\nInstalling TensorFlow...\r\nCollecting tensorflow==1.10.1 from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl\r\n  HTTP error 404 while getting https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl\r\n  Could not install requirement tensorflow==1.10.1 from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl\r\nCould not install requirement tensorflow==1.10.1 from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl for URL https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl\r\nYou are using pip version 10.0.1, however version 18.0 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\r\nError: Error 1 occurred installing packages into conda environment r-tensorflow\r\nIn addition: Warning message:\r\nrunning command '\"C:/MINICO~1/Scripts/activate\" r-tensorflow && pip install --upgrade --ignore-installed \"https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl\"' had status 1 \r\n\r\nIt seems that version 10.10.1 is not available for windows (I use Windows 10), while it is for all other platforms. The file tensorflow-1.10.1-cp36-cp36m-win_amd64.whl seems not to be available anywhere on the Internet (same seems to be true for the 86 version)\r\n\r\nI tested  in a browser \"https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.1-cp36-cp36m-win_amd64.whl\" does not work.\r\nOlder version 1.10.0: \"https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.10.0-cp36-cp36m-win_amd64.whl\" downloads file.\r\n\r\nAlso: \r\n\r\ninstall_keras(tensorflow=\"1.10.1)  gives an HTTP error 404 error\r\n\r\ninstall_keras(tensorflow=\"1.10.0) installs fine, but does not work. \r\nE.g. (below first command works, while second one does not; Flags were set correctly, code used to work in my old installation, I also downgraded numpy and setup tools as requested => no success)\r\nmodelNn <- keras_model_sequential() \r\nmodelNn %>% \r\n  layer_dense(units = FLAGS$dense1Neurons , activation = 'relu', input_shape = c(numInputs)) %>% \r\n  layer_dropout(rate = FLAGS$dense1Dropout)%>%\r\n  layer_dense(units = 1, activation = 'linear')\r\n\r\nI really rely on R tensorflow/keras in my research. Please help!\r\n\r\nThanks\r\n\r\nCarsten", "comments": ["@CarstenLange Just want to make sure. \r\nDid you load keras library with library(keras) before modelNn <- keras_model_sequential()?\r\n Also was that successful for tensorflow version 1.10.0?\r\n", "@ymodak . Thanks for the fast reply.  Yes, Ioaded Keras with library(keras) and the first command executed well. However, only keras_model_sequential() worked other commands did not. E.g.., the second one I mentioned and also \"use_session_with_seed(2018)\" did not work. I was not surprised, because Keras seems to require tensorflow=\"1.10.1\" and I used  tensorflow=\"1.10.0\" instead, which was kind of a hack.", "@CarstenLange You are welcome.Okay. Did you load the libraries such as dplyr or magrittr for the piping operator (%>%) to function? Also can you please paste the error message?", "Yes, dplyr was loaded via tidyverse. I also loaded library(magrittr).  \r\nHere is the error message (this time from the MNIST example from the Tensorflow R webpage: \r\nmodel %>% \r\n+   layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% \r\n+   layer_dropout(rate = 0.4) %>% \r\n+   layer_dense(units = 128, activation = 'relu') %>%\r\n+   layer_dropout(rate = 0.3) %>%\r\n+   layer_dense(units = 10, activation = 'softmax')\r\n Show Traceback\r\n \r\n Rerun with Debug\r\n Error in py_call_impl(callable, dots$args, dots$keywords) : \r\n  ValueError: Layer dense_1 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.engine.se\r\n\r\nHowever, this was tensorflow 1.10.0 since 1.10.1 does not install (see error at beginning of post).\r\nThanks.", "@CarstenLange Which Keras version did you use? Please also provide the information below so we can better help you.\r\n\r\nSystem information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version (use command below): \r\nPython version: \r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:", "I have installed Keras/R on Windows without any errors.", "Hi Wt-Huang,\r\n\r\nI had the same experience. I installed first Anaconda (5.0.1) and then MRO some month ago and all worked fine. Then when I tried to update R nothing worked anymore. I deinstalled everything and started from scratch.\r\nThe latest version of Anconda does not install correctly on Windows. I found a link and an advice to install Minconda instead. However when installing R I ran into new problems. I gave up and now run the Kerras part of my research on Ubunto.\r\n\r\nHowever, it would be interesting which versions of Anaconda, Python, TensorFlow and Keras work well in your case and when you installed.\r\n\r\nThank you\r\nCarsten\r\n\r\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026..\r\nCarsten Lange, Ph.D.\r\nProfessor of Economics and Graduate Coordinator\r\nCalifornia State University\r\n3801 west Temple Ave.\r\nPomona, CA 91768\r\nPhone: +1 (909) 869 3843\r\nEmail: clange@cpp.edu\r\n\r\nFrom: wt-huang <notifications@github.com>\r\nSent: Friday, September 28, 2018 6:37 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Carsten Lange <clange@cpp.edu>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Installing Kerras on R/Windows (#22487)\r\n\r\n\r\nI have installed Keras/R on Windows without any errors.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F22487%23issuecomment-425604925&data=02%7C01%7Cclange%40cpp.edu%7C9d574ba4e0744669e11d08d625ac062c%7C164ba61e39ec4f5d89ffaa1f00a521b4%7C0%7C0%7C636737818329140866&sdata=F1y%2F2mv%2BFN5MgpEL%2F%2F8SOesgt0svWqjYRFTZEkwAtfk%3D&reserved=0>, or mute the thread<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAePQqHYJgVaXZr2u0zxw9BggSLHwjhMIks5ufs6vgaJpZM4W3kye&data=02%7C01%7Cclange%40cpp.edu%7C9d574ba4e0744669e11d08d625ac062c%7C164ba61e39ec4f5d89ffaa1f00a521b4%7C0%7C0%7C636737818329140866&sdata=tfcZjL6CYJz0OpPwB%2BfFCcqG26ZyiRdGbyfl0JyAidk%3D&reserved=0>.\r\n", "@CarstenLange \r\nHi Carten, the previous Anaconda seemingly have less installation issues, however Ubuntu can work just as well if not better. Miniconda is compact but you could miss the benefit of some handy and useful packages. FWIW the versions I have used was Anaconda 3.7.0, Python 3.6, TensorFlow 1.10.0, Keras 2.2.0.  ", "Hi Wt-Huang,\r\n\r\nThanks for the email. I had TensorFlow 1.10.0 installed too when everything still worked. Then when I reinstalled TensorFlow 1.10.0 was not available on the Internet anymore.\r\n\r\nI just recently saw that Anaconda 5.3.0 is out. I will wait for a while until it is at least updated ones and then try to install from scratch.\r\n\r\nBest\r\n\r\nCarsten\r\nFrom: wt-huang <notifications@github.com>\r\nSent: Wednesday, October 3, 2018 1:04 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Carsten Lange <clange@cpp.edu>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Installing Kerras on R/Windows (#22487)\r\n\r\n\r\n@CarstenLange<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FCarstenLange&data=02%7C01%7Cclange%40cpp.edu%7Cc8208f3e1b2d4377f6f508d6296b5949%7C164ba61e39ec4f5d89ffaa1f00a521b4%7C0%7C0%7C636741938373054367&sdata=yMCG0wTd1mFkor0ZYf%2BC4E9rbFUq8MNBSsqMHq0VnKs%3D&reserved=0>\r\nHi Carten, the previous Anaconda seemingly have less installation issues, however Ubuntu can work just as well if not better. Miniconda is compact but you could miss the benefit of some handy and useful packages. FWIW the versions I have used was Anaconda 3.7.0, Python 3.6, TensorFlow 1.10.0, Keras 2.2.0.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F22487%23issuecomment-426777772&data=02%7C01%7Cclange%40cpp.edu%7Cc8208f3e1b2d4377f6f508d6296b5949%7C164ba61e39ec4f5d89ffaa1f00a521b4%7C0%7C0%7C636741938373064380&sdata=pgXMPBN2l9AJdcF7rMf2ioSo0JlDMq3jdFbESsYSGdc%3D&reserved=0>, or mute the thread<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAePQqHg0daFyFzpC7uvVdq5oGVTvEL4sks5uhRgqgaJpZM4W3kye&data=02%7C01%7Cclange%40cpp.edu%7Cc8208f3e1b2d4377f6f508d6296b5949%7C164ba61e39ec4f5d89ffaa1f00a521b4%7C0%7C0%7C636741938373074388&sdata=xzxh0tNNwSRJMOHstRz%2FRkhnwLT13HSXTRI3In%2F%2BDPY%3D&reserved=0>.\r\n", "@CarstenLange \r\nHi Carsten, glad that everything works for you now. Any further issues you may have when using TensorFlow, feel free to let us know.\r\n\r\n "]}, {"number": 22486, "title": "error: Keras-Preprocessing 1.0.3 is installed but keras-preprocessing==1.0.2 is required by {'keras'}", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: pypi\r\n- **TensorFlow version (use command below)**: tensorflow 1.11.0rc2\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: not using\r\n- **GCC/Compiler version (if compiling from source)**: not using\r\n- **CUDA/cuDNN version**: not using\r\n- **GPU model and memory**: not using\r\n- **Exact command to reproduce**: Trying to install keras on Travis-CI (linux-Ubuntu 16.04), report: \r\n\r\n### Describe\r\n\r\nInstalled /home/travis/virtualenv/python3.6.3/lib/python3.6/site-packages/PyYAML-4.2b4-py3.6-linux-x86_64.egg\r\nerror: Keras-Preprocessing 1.0.3 is installed but keras-preprocessing==1.0.2 is required by {'keras'}\r\n\r\nAfter I manually downgrade keras-preprocessing to 1.0.2 in requirements.txt and setup.py, it says:\r\n\r\nerror: Keras-Preprocessing 1.0.2 is installed but keras-preprocessing==1.0.3 is required by {'tensorflow'}\r\n\r\n### Error Logs\r\n\r\nSituation 1: keras-preprocessing 1.0.3 installed: [log](https://travis-ci.org/NTMC-Community/MatchZoo/jobs/429012836).", "comments": ["Same Problem for me and almost the same result with Keras Applications (tf needs 1.0.5 and Keras need 1.0.4)\r\nI have no idea how to work my way around it", "Still same problem here for tf 1.11.0 release. OS is Ubuntu 16.04. Python 3.5, cuda10 tensorflow installed from source. \r\n\r\nSince I build tensorflow in a test directory and install it  with pip3 install --user. pip merely issued a warning and proceeded. \r\n\r\nAfter installed and ran a few tests (ignoring the warning) tensorflow and keras seem to work without problem.", "I have the same problem with inconsistent version requirements between Tensorflow and Keras. This is using `pipenv` to install the mutually latest versions. Are there two versions of Tensorflow and Keras that I could pin that wouldn't have this inconsistency?\r\n\r\n```\r\n$ pipenv install keras tensorflow\r\nCreating a Pipfile for this project...\r\nInstalling keras...\r\nRequirement already satisfied: keras in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (2.2.2)\r\nRequirement already satisfied: numpy>=1.9.1 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from keras) (1.15.2)\r\nRequirement already satisfied: pyyaml in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from keras) (3.13)\r\nRequirement already satisfied: scipy>=0.14 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from keras) (1.1.0)\r\nRequirement already satisfied: six>=1.9.0 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from keras) (1.11.0)\r\nRequirement already satisfied: h5py in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from keras) (2.8.0)\r\nCollecting keras-applications==1.0.4 (from keras)\r\n  Using cached https://files.pythonhosted.org/packages/54/90/8f327deaa37a71caddb59b7b4aaa9d4b3e90c0e76f8c2d1572005278ddc5/Keras_Applications-1.0.4-py2.py3-none-any.whl\r\nCollecting keras-preprocessing==1.0.2 (from keras)\r\n  Using cached https://files.pythonhosted.org/packages/71/26/1e778ebd737032749824d5cba7dbd3b0cf9234b87ab5ec79f5f0403ca7e9/Keras_Preprocessing-1.0.2-py2.py3-none-any.whl\r\nInstalling collected packages: keras-applications, keras-preprocessing\r\n  Found existing installation: Keras-Applications 1.0.5\r\n    Uninstalling Keras-Applications-1.0.5:\r\n      Successfully uninstalled Keras-Applications-1.0.5\r\n  Found existing installation: Keras-Preprocessing 1.0.3\r\n    Uninstalling Keras-Preprocessing-1.0.3:\r\n      Successfully uninstalled Keras-Preprocessing-1.0.3\r\nSuccessfully installed keras-applications-1.0.4 keras-preprocessing-1.0.2\r\n\r\nAdding keras to Pipfile's [packages]...\r\nInstalling tensorflow...\r\nRequirement already satisfied: tensorflow in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (1.11.0)\r\nRequirement already satisfied: grpcio>=1.8.6 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (1.15.0)\r\nRequirement already satisfied: wheel>=0.26 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (0.31.1)\r\nRequirement already satisfied: termcolor>=1.1.0 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (1.1.0)\r\nRequirement already satisfied: six>=1.10.0 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (1.11.0)\r\nRequirement already satisfied: gast>=0.2.0 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (0.2.0)\r\nCollecting keras-preprocessing>=1.0.3 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/b3/bd/796f986980da4d6adc77ffd8b2b11074e7b17a7b74b03789aefac5709c4b/Keras_Preprocessing-1.0.3-py2.py3-none-any.whl\r\nCollecting keras-applications>=1.0.5 (from tensorflow)\r\n  Using cached https://files.pythonhosted.org/packages/3f/9c/6e9393ead970fd97be0cfde912697dafec5800d9191f5ba25352fa537d72/Keras_Applications-1.0.5-py2.py3-none-any.whl\r\nRequirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (1.11.0)\r\nRequirement already satisfied: setuptools<=39.1.0 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (39.1.0)\r\nRequirement already satisfied: absl-py>=0.1.6 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (0.5.0)\r\nRequirement already satisfied: protobuf>=3.6.0 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (3.6.1)\r\nRequirement already satisfied: astor>=0.6.0 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (0.7.1)\r\nRequirement already satisfied: numpy>=1.13.3 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorflow) (1.15.2)\r\nRequirement already satisfied: scipy>=0.14 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from keras-preprocessing>=1.0.3->tensorflow) (1.1.0)\r\nRequirement already satisfied: keras>=2.1.6 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from keras-preprocessing>=1.0.3->tensorflow) (2.2.2)\r\nRequirement already satisfied: h5py in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from keras-applications>=1.0.5->tensorflow) (2.8.0)\r\nRequirement already satisfied: werkzeug>=0.11.10 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow) (0.14.1)\r\nRequirement already satisfied: markdown>=2.6.8 in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow) (3.0.1)\r\nRequirement already satisfied: pyyaml in /home/neil/.local/share/virtualenvs/tmp-DjuGd3hf/lib/python3.6/site-packages (from keras>=2.1.6->keras-preprocessing>=1.0.3->tensorflow) (3.13)\r\nInstalling collected packages: keras-preprocessing, keras-applications\r\n  Found existing installation: Keras-Preprocessing 1.0.2\r\n    Uninstalling Keras-Preprocessing-1.0.2:\r\n      Successfully uninstalled Keras-Preprocessing-1.0.2\r\n  Found existing installation: Keras-Applications 1.0.4\r\n    Uninstalling Keras-Applications-1.0.4:\r\n      Successfully uninstalled Keras-Applications-1.0.4\r\nSuccessfully installed keras-applications-1.0.5 keras-preprocessing-1.0.3\r\n\r\nAdding tensorflow to Pipfile's [packages]...\r\nPipfile.lock not found, creating...\r\nLocking [dev-packages] dependencies...\r\nLocking [packages] dependencies...\r\n\r\nWarning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.\r\n  You can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.\r\n  Hint: try $ pipenv lock --pre if it is a pre-release dependency.\r\nCould not find a version that matches keras-applications==1.0.4,>=1.0.5\r\nTried: 1.0.0, 1.0.0, 1.0.1, 1.0.1, 1.0.2, 1.0.2, 1.0.4, 1.0.4, 1.0.5, 1.0.5\r\nThere are incompatible versions in the resolved dependencies.\r\n```", "https://github.com/tensorflow/tensorflow/issues/22601", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "anyone has solutions?", "@gunan The bug will be fixed in the coming 1.12 version, right?", "The issue was keras_preprocessing was depending on keras. It is fixed in the latest version of keras_preprocessing.\r\nWhile our dependency version is updated, you can always install TF with `--no_deps` and then install the latest version of keras_preprocessing.\r\nOr you can simply install 1.12.0rc1 with `--pre` flag to `pip install tensorflow`", "thanks"]}, {"number": 22485, "title": "Keras eager execution: multi-output model unexpected behavior", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Colab\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: Colab\r\n- **GCC/Compiler version (if compiling from source)**: Colab\r\n- **CUDA/cuDNN version**: Colab\r\n- **GPU model and memory**: Colab\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have a very simple multi-output model. The model is expected to be trained with respect to the first output only.  You can find the code below.  When I run it, first I get a warning message then it crashes.\r\nThe warning message is expected since I am intentionally missing considering the second output from the loss function. But the error message is completely contradicting that : `No data provided for \"out2\". Need data for each key in: ['out1', 'out2']`\r\n\r\n`WARNING:tensorflow:Output \"out2\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"out2\" during training.`\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Dense, Input, Layer\r\nfrom tensorflow.keras.models import Model\r\n\r\ntf.enable_eager_execution()\r\n\r\ninput_tensor = Input(shape=(20,), name=\"input\")\r\nhidden = Dense(100, activation='relu')(input_tensor)\r\nout1 = Dense(10, activation='relu', name=\"out1\")(hidden)\r\nout2 = Dense(5, activation='relu', name=\"out2\")(hidden)\r\nmodel = Model(inputs=input_tensor, outputs=[out1, out2])\r\nmodel.compile(loss={\"out1\": \"mse\"}, optimizer=tf.train.AdamOptimizer(learning_rate=0.001))\r\nmodel.summary()\r\n\r\nnp.random.seed(0)\r\nX = np.random.random((3, 20)).astype(np.float32)\r\nY1 = np.random.random((3, 10)).astype(np.float32)\r\nY2 = np.random.random((3, 5)).astype(np.float32)\r\nmodel.fit(x={'input' : X}, y={'out1' : Y1}, batch_size=1, epochs=10)\r\n\r\n```\r\n\r\n### logs\r\n```\r\nWARNING:tensorflow:Output \"out2\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"out2\" during training.\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    125           if data[x].__class__.__name__ == 'DataFrame' else data[x]\r\n--> 126           for x in names\r\n    127       ]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in <listcomp>(.0)\r\n    125           if data[x].__class__.__name__ == 'DataFrame' else data[x]\r\n--> 126           for x in names\r\n    127       ]\r\n\r\nKeyError: 'out2'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-00a3f18ea48b> in <module>()\r\n     19 Y1 = np.random.random((3, 5)).astype(np.float32)\r\n     20 Y1 = np.random.random((3, 10)).astype(np.float32)\r\n---> 21 model.fit(x={'input' : X}, y={'out1' : Y1}, batch_size=1, epochs=10)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1276         steps_name='steps_per_epoch',\r\n   1277         steps=steps_per_epoch,\r\n-> 1278         validation_split=validation_split)\r\n   1279 \r\n   1280     # Prepare validation data.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\r\n    915           feed_output_shapes,\r\n    916           check_batch_axis=False,  # Don't enforce the batch size.\r\n--> 917           exception_prefix='target')\r\n    918 \r\n    919       # Generate sample-wise weight values given the `sample_weight` and\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    128     except KeyError as e:\r\n    129       raise ValueError('No data provided for \"' + e.args[0] + '\". Need data '\r\n--> 130                        'for each key in: ' + str(names))\r\n    131   elif isinstance(data, list):\r\n    132     if isinstance(data[0], list):\r\n\r\nValueError: No data provided for \"out2\". Need data for each key in: ['out1', 'out2']\r\n```\r\n\r\n", "comments": ["I was investigating the problem and it is related to the eager execution mode. The code runs without any problem in the graph mode.", "@karmel @fchollet - Mind taking a look?\r\n(@allenlavoie may have a quick diagnosis too)", "@omalleyt12 , can you take a look? This seems to be one of the checks run in standardize_input_data.", "Yep will do", "@omalleyt12 I m wondering, is there a workaround while you guys try to fix the bug?", "@nairouz Sure, still looking into this but the easiest workaround would be adding a second loss that always returns zero and passing in fake data for 'out2':\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Dense, Input, Layer\r\nfrom tensorflow.keras.models import Model\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef zero_loss(y_true, y_pred):\r\n  return tf.constant(0.0)\r\n\r\ninput_tensor = Input(shape=(20,), name=\"input\")\r\nhidden = Dense(100, activation='relu')(input_tensor)\r\nout1 = Dense(10, activation='relu', name=\"out1\")(hidden)\r\nout2 = Dense(5, activation='relu', name=\"out2\")(hidden)\r\nmodel = Model(inputs=input_tensor, outputs=[out1, out2])\r\nmodel.compile(loss={\"out1\": \"mse\", \"out2\": zero_loss}, optimizer=tf.train.AdamOptimizer(learning_rate=0.001))\r\nmodel.summary()\r\n\r\nnp.random.seed(0)\r\nX = np.random.random((3, 20)).astype(np.float32)\r\nY1 = np.random.random((3, 10)).astype(np.float32)\r\nY2 = np.random.random((3, 5)).astype(np.float32)\r\nmodel.fit(x={'input' : X}, y={'out1' : Y1, 'out2': Y2}, batch_size=1, epochs=10)\r\n```", "@omalleyt12 Thank you so much. I appreciate your help. "]}, {"number": 22484, "title": "CollectiveAllReduceStrategy \"Out of range: End of sequence\" warnings", "body": "### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04.4 ppc64le\r\nTensorFlow installed from (source or binary):\r\nsource\r\nTensorFlow version (use command below):\r\n1.11\r\nPython version: Python 2.7.15\r\nCUDA/cuDNN version: Cuda 9.2 CuDNN 7.2.1\r\nGPU model and memory: V100 16GB\r\nExact command to reproduce: python keras_model_to_estimator.py /tmp/tfkeras_example\r\n\r\n### Describe the problem\r\nRunning the [ecosystem CollectiveAllReduceStrategy estimator example ](https://github.com/tensorflow/ecosystem/blob/master/distribution_strategy/keras_model_to_estimator.py) on a box with 2GPUs, we get those warning constantly after the last step of an epoch:\r\n```\r\nINFO:tensorflow:loss = 0.6660614, step = 1500 (0.288 sec)\r\n2018-09-21 19:23:01.075834: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,10], [?,1]], output_types=[DT_FLOAT, DT_INT64]](IteratorFromStringHandleV2)]]\r\n\t [[{{node FunctionBufferingResourceGetNext_1}} = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](FunctionBufferingResource_1)]]\r\n2018-09-21 19:23:01.075843: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,10], [?,1]], output_types=[DT_FLOAT, DT_INT64]](IteratorFromStringHandleV2)]]\r\n\t [[{{node FunctionBufferingResourceGetNext}} = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FunctionBufferingResource)]]\r\n\t [[{{node global_step/Read/ReadVariableOp/_171}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_550_global_step/Read/ReadVariableOp\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n2018-09-21 19:23:01.076090: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,10], [?,1]], output_types=[DT_FLOAT, DT_INT64]](IteratorFromStringHandleV2)]]\r\n\t [[{{node FunctionBufferingResourceGetNext}} = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FunctionBufferingResource)]]\r\n```\r\n@yuefengz: Do you also see the same warnings in your test logs?\r\n", "comments": ["Follow up issue after issue #22326 got closed.", "I traced this to [FunctionBufferingResourceGetNextOp](https://github.com/tensorflow/tensorflow/blob/844074c2a8e61b744c3de2718e1c9ea7b1d2edc2/tensorflow/contrib/data/kernels/prefetching_kernels.cc#L382).  Looks like the kernel returns an OutOfRange error to signal completion.  This non-OK status [triggers an abort](https://github.com/tensorflow/tensorflow/blob/844074c2a8e61b744c3de2718e1c9ea7b1d2edc2/tensorflow/core/common_runtime/executor.cc#L2153) of the CollectiveExecutor.\r\n\r\n@mrry @rohan100jain is this expected behavior?  I would have expected the error to be caught somewhere because it is not really an error.", "@dubey That's how the iterators are implemented, yes. The error can be caught in the graph if you use `tf.contrib.data.get_next_as_optional()`, but I think it's library code that's responsible for doing that.", "Thanks Derek.\r\n\r\nI tried changing `get_next()` to `get_next_as_optional()` in [`Estimator._train_model_distributed()`](https://github.com/tensorflow/tensorflow/blob/d0397c3314600da0c9cdc300ae87483331d54298/tensorflow/python/estimator/estimator.py#L1306) but it seems like `PerDeviceDataIterator` does not support `get_next_as_optional`.  @guptapriya suggestions?", "Hm, yes this is something that we want to be able to handle better in the future in distribution strategy code. However, until now we've been relying on the high level API infrastructure to deal with this. For e.g. I think estimator has some logic which stops the training when OutOfRange is encountered.\r\n@yuefengz any idea why this error shows up only in collective ops strategy and not others? \r\n\r\n", "The error probably occurs in all cases, but it shows up in collective strategy because we print a `LOG(WARNING)` when we abort the `CollectiveExecutor`.\r\n\r\nI just chatted with @rohan100jain, who's working on a change in Dataset iterators which should enable a proper fix for this error.  I'll update this issue when those changes are checked in.", "@rohan100jain any update on the iterator changes to help with this issue?", "Some progress made here. Tracked down the bug that was causing issues with Distribution strategies not using MultiDeviceIterator by default. Now Distribution strategies uses MultiDeviceIterator. Now I'll work on getting MultiDeviceIterator to work with get_next_as_optional and expose that API end point.", "@nvcastet Is this still an issue ?\r\nWe see that you are using old version of tensorflow 1.x which is not actively supported, We recommend that you upgrade to 2.4 or later version.Please have a look at the [migration](https://www.tensorflow.org/guide/migrate) guide for reference to migrate from TensorFlow 1.x to TensorFlow 2.Thank you!", "@sushreebarsa I don't know to be honest, it has been more than 3 years. :) `CollectiveAllReduceStrategy` is not used anymore. The new Distribution strategy API for multi-workers is `MultiWorkerMirroredStrategy`.\r\nI can close the issue for now and if someone sees it again, it can get re-opened."]}, {"number": 22483, "title": "Reduce the size of //tensorflow/tools/pip_package:simple_console_windows", "body": "This change reduce the size of `//tensorflow/tools/pip_package:simple_console_windows`'s zip file from 1000027677 bytes to 47690474 bytes for a CPU build. For GPU build, it will avoid going over 4GB when multiple CUDA compatibility are specified.\r\n\r\nTo fix https://github.com/tensorflow/tensorflow/issues/22390\r\n\r\n/cc @gunan ", "comments": ["Thanks for the review!", "@meteorcloudy could you send this internally instead?", "Will do it!"]}, {"number": 22482, "title": " java.lang.IllegalArgumentException: Input to reshape is a tensor with 3072 values, but the requested shape has 9437184", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:0.16.1\r\n- **GCC/Compiler version (if compiling from source)**:NA \r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:NA\r\n\r\n### Describe the problem\r\nI have built a custom classifier model using keras with tensorflow backend and I am trying to inference the model to use it in model. I have successfully created protobuf file which contains inference model. When I am trying to feed the input to the model in android. I am getting this error\r\n```\r\njava.lang.IllegalArgumentException: Input to reshape is a tensor with 3072 values, but the requested shape has 9437184\r\n    \t [[{{node reshape_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_reshape_1_input_0_0, reshape_1/Reshape/shape)]]\r\n```\r\n\r\n### Source code / logs\r\n```\r\n09-24 18:24:52.780 2480-3594/com.appa.iocsv2 E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[reshape_1_input], outputs:[dense_2/Softmax]\r\n    \r\n    --------- beginning of crash\r\n09-24 18:24:52.780 2480-3594/com.appa.iocsv2 E/AndroidRuntime: FATAL EXCEPTION: IntentService[Prediction Service]\r\n    Process: com.appa.iocsv2, PID: 2480\r\n    java.lang.IllegalArgumentException: Input to reshape is a tensor with 3072 values, but the requested shape has 9437184\r\n    \t [[{{node reshape_1/Reshape/eightbit}} = QuantizedReshape[T=DT_QUINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](reshape_1/Reshape_eightbit/reshape_1_input/quantize, reshape_1/Reshape/shape, reshape_1/Reshape_eightbit/reshape_1_input/quantize:1, reshape_1/Reshape_eightbit/reshape_1_input/quantize:2)]]\r\n        at org.tensorflow.Session.run(Native Method)\r\n        at org.tensorflow.Session.access$100(Session.java:48)\r\n        at org.tensorflow.Session$Runner.runHelper(Session.java:314)\r\n        at org.tensorflow.Session$Runner.run(Session.java:264)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)\r\n        at com.appa.iocsv2.analyze.PredictionService.predict(PredictionService.java:151)\r\n        at com.appa.iocsv2.analyze.PredictionService.onHandleIntent(PredictionService.java:103)\r\n        at android.app.IntentService$ServiceHandler.handleMessage(IntentService.java:68)\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n        at android.os.Looper.loop(Looper.java:154)\r\n        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n```\r\n", "comments": ["@maitreya2954 Hi, could you please share a code snippet to reproduce this error.", "Hey @harshini-gadige \r\nHere is the code for which i am facing the issue\r\n## Keras model\r\n```\r\nmodel = Sequential()\r\nmodel.add(Reshape((image_dim, image_dim, 3), input_shape=(image_dim * image_dim * 3,)))\r\nmodel.add(Conv2D(32, (5, 5), padding='same', kernel_regularizer=l2(reg_strength)))\r\nmodel.add(LeakyReLU(alpha=alpha_LReLU))\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\nmodel.add(Dropout(dropout_rate))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(512, kernel_constraint=maxnorm(1), kernel_regularizer=l2(reg_strength)))\r\nmodel.add(LeakyReLU(alpha=alpha_LReLU))\r\nmodel.add(Dropout(dropout_rate))\r\nmodel.add(Dense(64, kernel_constraint=maxnorm(1), kernel_regularizer=l2(reg_strength)))\r\nmodel.add(LeakyReLU(alpha=alpha_LReLU))\r\nmodel.add(Dropout(dropout_rate))\r\nmodel.add(Dense(num_of_classes, activation='softmax', kernel_regularizer=l2(reg_strength)))\r\n```\r\n## Inferencing into protobuf file\r\n```\r\nfrom tensorflow.python.framework import graph_util\r\nfrom tensorflow.python.framework import graph_io\r\nif args.quantize:\r\n    from tensorflow.tools.graph_transforms import TransformGraph\r\n    transforms = [\"quantize_weights\", \"quantize_nodes\"]\r\n    transformed_graph_def = TransformGraph(sess.graph.as_graph_def(), [], pred_node_names, transforms)\r\n    constant_graph = graph_util.convert_variables_to_constants(sess, transformed_graph_def, pred_node_names)\r\nelse:\r\n    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), pred_node_names)    \r\ngraph_io.write_graph(constant_graph, output_fld, args.output_model_file, as_text=False)\r\n```\r\n## Android\r\n```\r\ninferenceInterface = new TensorFlowInferenceInterface(getAssets(), \"CNNMODEL.pb\");                Log.d(\"GETREP\", \"Tensorflow model loaded...Input size: \" + String.valueOf(input.length));\r\n//dimensions of input is (1, 3072)\r\nfloat[] output = predict(input);\r\n\r\nprivate float[] predict(float[] input){\r\n        // model has only 2 output neuron\r\n        float output[] = new float[2];\r\n        inferenceInterface.feed(\"reshape_1_input\", input, 1, input.length);\r\n        Log.d(\"GETREP\", \"Feed completed\");\r\n        inferenceInterface.run(new String[]{\"dense_2/Softmax\"});\r\n        Log.d(\"GETREP\", \"Run completed\");\r\n        inferenceInterface.fetch(\"dense_2/Softmax\", output);\r\n        // return prediction\r\n        return output;\r\n    }\r\n```", "@maitreya2954 You may want to check whether the input is 1D when feeding the graph. Consider reshaping the input to match TFLite model requirements.", "@wt-huang  Currently I am initializing input as \r\n> float[] input = new float[3072];\r\n\r\nSo input is 1D\r\n\r\nAnd I noticed 9437184 is 3072 * 3072. I wonder why?\r\n", "@maitreya2954 In your code snippet it appears that your image size is `32x32`, note that `input.length` should equal input size. If you are using `(1,3072)` as your input, then system is expecting image size of `3072x3072`, which I doubt that is what you want.   ", "@wt-huang The system is throwing an error ```The array of length xxxx cannot be wrapped into tensor of length 3072``` if size of the array is anything else but 3072.\r\n\r\nI have grep'ed the error result and found out it was in [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reshape_op.h](url) line 78\r\n\r\n```\r\nOP_REQUIRES(context, shape.num_elements() == input.NumElements(),\r\n                errors::InvalidArgument(\"Input to reshape is a tensor with \",\r\n                                        input.NumElements(),\r\n                                        \" values, but the requested shape has \",\r\n                                        shape.num_elements()));\r\n```", "@maitreya2954 Please post your full code also attach .pb file. ", "Closing this for now, feel free to reopen if any errors come up."]}, {"number": 22481, "title": "Inconsistent behavior in tf.contrib.distributions.percentile for NaN values", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0\r\n- **Python version**: 3.6.6 (Anaconda)\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\n[`tf.contrib.distributions.percentile`](https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/percentile) behaves inconsistently when NaN values are given. While the documentation does not explicitly states how this case should be handled, one would normally expect to always receive NaN, or maybe ignore them in the computation (compute the percentile of non-NaN values), or even replace them with 0. But the current logic seems to somehow depend on the order of the values in the input: trying to compute the median of `[1.0, NaN]` results in `1.0`, while for `[NaN, 1.0]` it is `NaN`.\r\n\r\nThe behavior seems to be different also between CPU and GPU. I obtained the results above while running on CPU. The same test on GPU (CUDA 9.0, CUDNN 7, Titan V) produced `NaN` in both cases; however, the computed median on GPU for `[1.0, 1.0, NaN]` was again `1.0`.\r\n\r\n### Source code / logs\r\n\r\n```py\r\nimport tensorflow as tf\r\nimport math\r\n\r\nx = tf.placeholder(tf.float32, [None])\r\nm = tf.contrib.distributions.percentile(x, q=50., validate_args=True)\r\nwith tf.Session() as sess:\r\n    print(sess.run(m, feed_dict={ x: [1, math.nan] }))  # Prints 1.0\r\n    print(sess.run(m, feed_dict={ x: [math.nan, 1] }))  # Prints nan\r\n```", "comments": ["`tf.contrib.distributions` are deprecated in TensorFlow; development has migrated to https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/distributions, including `sample_stats.py`.  This issue will be addressed there.\r\n \r\nI presume the source of the problem is that percentile relies on sort+index, but NaN values do not sort consistently: both NaN < x and x < NaN are always false, for all x (including x = NaN).  One candidate resolution would be to explicitly check the input for NaN, and return NaN in that case.  @jvdillon, opinions on setting the interface of `percentile` to that?", "Always tough to decide how to handle NaN.  I think the best thing would be to add `validate_args` which when true would enable an assertion which fires on detection of NaN.\r\n\r\nI think we should be hesitant to do anything more (such as pruning NaN's, masking them with some user specifiable value, etc) since this would potentially incur a cost whether or not the input actually had NaNs. (And this because TFP: avoids `tf.cond` if possible and has a strict \"only pay for what you eat\" policy.)\r\n\r\nThoughts?", "I don't think the cost is that bad: The function already does a complete sort, so the NaN check should be asymptotically cheaper.  There shouldn't be any control flow, either: compute a mask of which batch members have NaNs, then tf.where on that mask to select either NaN or the sort+index result.\r\n\r\nI don't think this is a job for `validate_args` -- having a NaN argument isn't invalid, it just means a NaN answer.  Maybe instead add `allow_nan_stats=True` and raise an exception if there is a NaN in the input and this flag is false?", "`allow_nan_stats=True` sgtm.  However our policy is pretty strict, asymptotic costs or otherwise. We could add another percentile function which masks `NaN`, however that seems weirdly specific. Can you make a case why this should be handled on the TFP side? Ie, a user can do this today:\r\n\r\n     percentile(tfp.where(tf.isnan(x),tf.zeros_like(x),x))\r\n\r\nThe advantage of this is its fully self-documenting and nearly as terse as if percentile took a mask arg.", "Just to make sure we are both on the same page, I am not talking about masking input NaNs (which, I agree, would be weirdly specific) but about propagating them.  The interface I think we should implement is the same as numpy:\r\n```\r\n>>> np.percentile([float(\"nan\"), 1.0], 50)\r\nnan\r\n>>> np.percentile([1.0, float(\"nan\")], 50)\r\nnan\r\n>>> np.percentile([1.0, 2.0, 3.0, 4.0, float(\"nan\")], 20)\r\nnan\r\n>>> np.percentile([1.0, 2.0, 3.0, 4.0], 20)\r\n1.6000000000000001\r\n>>> np.percentile([1.0, float(\"nan\"), 2.0, 3.0, 4.0], 20)\r\nnan\r\n```\r\n\r\nThis can, of course, be done in user-land:\r\n```\r\ndef nan_propagating_percentile(x, q, axis):\r\n  nan_batch_members = tf.reduce_any(tf.isnan(x), axis=axis)\r\n  return tf.where(\r\n    nan_batch_members,\r\n    np.nan * tf.ones(tf.shape(nan_batch_members), dtype=x.dtype),\r\n    percentile(x, q, axis=axis))\r\n```\r\n\r\nbut that seems like something of an imposition.  @jvdillon: How do you value the cost of `is_nan` + `reduce_any` + `where` (for an operation that already includes a sort and slice) in comparison with the benefit of API compatibility, and of avoiding giving non-`NaN` answers on `NaN` inputs?"]}, {"number": 22480, "title": "OSError: SavedModel file does not exist at: saved_model_dir/{saved_model.pbtxt|saved_model.pb}", "body": "I want to optimized the my Tensor flow model (mars-small128.pb) \r\nI have saved_model_dir directory which contain mars-small128.pb file \r\nHere is my code \r\nimport tensorflow as tf\r\nconverter = tf.contrib.lite.TocoConverter.from_saved_model(saved_model_dir)\r\nconverter.post_training_quantize = True\r\ntflite_quantized_model = converter.convert()\r\nopen(\"quantized_model.tflite\", \"wb\").write(tflite_quantized_model)\r\nError\r\nOSError: SavedModel file does not exist at: saved_model_dir/{saved_model.pbtxt|saved_model.pb}\r\nSystem information\r\nOs:linux 16\r\nPython:3.5\r\ntensorflow Installation:pip install -U tf-nightly (latest versiopn)\r\nusing Cpu\r\n\r\nThanks\r\n\r\n\r\n\r\n", "comments": ["@Akhtar303nu Can you guide me with the step by step procedures to be followed for the training to obtain the model file (.pb) in the format ", "> @Akhtar303nu Can you guide me with the step by step procedures to be followed for the training to obtain the model file (.pb) in the format\r\nThanks @devanandd8245 I take  pretrained model of Google (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_modo.md)\r\nit genrate same error in my case\r\ncode:\r\nimport tensorflow as tf\r\nconverter = tf.contrib.lite.TocoConverter.from_saved_model('ssd_mobilenet_v1_coco_2018_01_28')\r\nconverter.post_training_quantize = True\r\ntflite_quantized_model = converter.convert()\r\nopen(\"quantized_model.tflite\", \"wb\").write(tflite_quantized_model)\r\n\r\nError:\r\n\r\n018-09-24 18:56:08.695359: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"t.py\", line 2, in <module>\r\n    converter = tf.contrib.lite.TocoConverter.from_saved_model('ssd_mobilenet_v1_coco_2018_01_28')\r\n  File \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 339, in from_saved_model\r\n    output_arrays, tag_set, signature_key)\r\n  File \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py\", line 239, in freeze_saved_model\r\n    meta_graph = _get_meta_graph_def(saved_model_dir, tag_set)\r\n  File \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py\", line 61, in _get_meta_graph_def\r\n    return loader.load(sess, tag_set, saved_model_dir)\r\n  File \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 192, in load\r\n    loader = SavedModelLoader(export_dir)\r\n  File \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 208, in __init__\r\n    self._saved_model = _parse_saved_model(export_dir)\r\n  File \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 81, in _parse_saved_model\r\n    constants.SAVED_MODEL_FILENAME_PB))\r\nOSError: SavedModel file does not exist at: ssd_mobilenet_v1_coco_2018_01_28/{saved_model.pbtxt|saved_model.pb}\r\n\r\n", "Thanks for the reply.\r\nWhat kind of annotation tool you used for the training and I need some more information on the inputs to be provided to training can you explain me in more clear way that is step by step\r\n\r\n\r\nRegards\r\nDevanandd MG\r\nFrom: Akhtar303nu [mailto:notifications@github.com]\r\nSent: Monday, September 24, 2018 7:45 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Devanandd M G <Devanandd.MG@LntTechservices.com>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] OSError: SavedModel file does not exist at: saved_model_dir/{saved_model.pbtxt|saved_model.pb} (#22480)\r\n\r\n\r\n@Akhtar303nu<https://github.com/Akhtar303nu> Can you guide me with the step by step procedures to be followed for the training to obtain the model file (.pb) in the format\r\nThanks @devanandd8245<https://github.com/devanandd8245> I take pretrained model of Google (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_modo.md)\r\nit genrate same error in my case\r\ncode:\r\nimport tensorflow as tf\r\nconverter = tf.contrib.lite.TocoConverter.from_saved_model('ssd_mobilenet_v1_coco_2018_01_28')\r\nconverter.post_training_quantize = True\r\ntflite_quantized_model = converter.convert()\r\nopen(\"quantized_model.tflite\", \"wb\").write(tflite_quantized_model)\r\n\r\nError:\r\n\r\n018-09-24 18:56:08.695359: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\nFile \"t.py\", line 2, in\r\nconverter = tf.contrib.lite.TocoConverter.from_saved_model('ssd_mobilenet_v1_coco_2018_01_28')\r\nFile \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 339, in from_saved_model\r\noutput_arrays, tag_set, signature_key)\r\nFile \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py\", line 239, in freeze_saved_model\r\nmeta_graph = _get_meta_graph_def(saved_model_dir, tag_set)\r\nFile \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py\", line 61, in _get_meta_graph_def\r\nreturn loader.load(sess, tag_set, saved_model_dir)\r\nFile \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 192, in load\r\nloader = SavedModelLoader(export_dir)\r\nFile \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 208, in init\r\nself._saved_model = _parse_saved_model(export_dir)\r\nFile \"/home/akhtar/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 81, in _parse_saved_model\r\nconstants.SAVED_MODEL_FILENAME_PB))\r\nOSError: SavedModel file does not exist at: ssd_mobilenet_v1_coco_2018_01_28/{saved_model.pbtxt|saved_model.pb}\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/22480#issuecomment-423986224>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ApIcQ1pvV-MPaymcXsCN1FuIDkAbBSKaks5ueOjRgaJpZM4W2fFp>.\r\n\r\nL&T Technology Services Ltd\r\n\r\nwww.LntTechservices.com<http://www.lnttechservices.com/>\r\n\r\nThis Email may contain confidential or privileged information for the intended recipient (s). If you are not the intended recipient, please do not use or disseminate the information, notify the sender and delete it from your system.\r\n", "@sukritiramesh  -  Hi, could you please look into this issue.\r\n", "It sounds like the path to your saved model file is not correct. My guess is you want the full path to `ssd_mobilenet_v1_coco_2018_01_28` rather than a relative path. Can you try the same script with the absolute path specified? ", "Thanks @karmel  I have tried it with full path but genrate same error", "Hi @devanandd8245 , here is the link for step by step guide....hope it may help you\r\nhttps://medium.com/@jsflo.dev/saving-and-loading-a-tensorflow-model-using-the-savedmodel-api-17645576527", "i found it is good to understand the basics but the training steps which means i am in requried of the No.Of.folders and the kind of images and annotations to be used.\n\n\nI am new to tensorflow so i would like to get more depth informations on the training procedures to be followed\n\n\n________________________________\nFrom: yashwantptl7 <notifications@github.com>\nSent: Thursday, September 27, 2018 2:47:13 PM\nTo: tensorflow/tensorflow\nCc: Devanandd M G; Mention\nSubject: Re: [tensorflow/tensorflow] OSError: SavedModel file does not exist at: saved_model_dir/{saved_model.pbtxt|saved_model.pb} (#22480)\n\n\nHi @devanandd8245<https://github.com/devanandd8245> , here is the link for step by step guide....hope it may help you\nhttps://medium.com/@jsflo.dev/saving-and-loading-a-tensorflow-model-using-the-savedmodel-api-17645576527\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/22480#issuecomment-425018032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ApIcQ5eaYJt2S3qLm9UwolANKJVEXER7ks5ufJeZgaJpZM4W2fFp>.\n\nL&T Technology Services Ltd\n\nwww.LntTechservices.com<http://www.lnttechservices.com/>\n\nThis Email may contain confidential or privileged information for the intended recipient (s). If you are not the intended recipient, please do not use or disseminate the information, notify the sender and delete it from your system.\n", "This is hard to debug with the information specified. It seems like you are not passing in your saved_model_dir correctly based on the error message. Can you clarify where the saved model is, and how you are passing in the directory name?", "I'm having the same issue. The follow files we created using a tf.estimator checkpointing on S3.\r\n```                           PRE eval/\r\n2018-11-09 11:49:00        133 checkpoint\r\n2018-11-09 11:49:56   22748845 events.out.tfevents.1541779585.76ea0bf64a92\r\n2018-11-09 10:06:42    4322097 graph.pbtxt\r\n2018-11-09 11:09:04  188348504 model.ckpt-1124.data-00000-of-00001\r\n2018-11-09 11:09:07      16509 model.ckpt-1124.index\r\n2018-11-09 11:09:10    2100949 model.ckpt-1124.meta\r\n2018-11-09 11:42:34  188348504 model.ckpt-1699.data-00000-of-00001\r\n2018-11-09 11:42:37      16509 model.ckpt-1699.index\r\n2018-11-09 11:42:39    2100949 model.ckpt-1699.meta\r\n2018-11-09 11:48:58  188348504 model.ckpt-1814.data-00000-of-00001\r\n2018-11-09 11:49:00      16509 model.ckpt-1814.index\r\n2018-11-09 11:49:02    2100949 model.ckpt-1814.meta\r\n```\r\nAttempting to call \r\n```python\r\nmeta_graph_def = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], 's3://path_to_checkpoints')\r\n```\r\nraises the following error.\r\n```python\r\nOSError: SavedModel file does not exist at: s3://path_to_checkpoints/{saved_model.pbtxt|saved_model.pb}\r\n```\r\nI am able to view the weights but unable to load with this tool\r\n\r\n\r\n\r\n\r\n", "@Akhtar303nu @gdj0nes You are probably passing the path to the .pb file of the **frozen model/graph, which is different from the .pb file of a SavedModel**, and that's why the SavedModel can't be found. Unlike frozen models/graphs, SavedModels are also associated with an auto-generated folder called \"variables\", so be sure your .pb file was generated in the correct way as it is described in the docs: https://www.tensorflow.org/guide/saved_model. \r\n\r\nI was also struggling with that same issue until I figured out I was messing up my .pb files. :)", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "@deangelacgn This looks like my problem or very close. I just don't get it completely. I use uber's framework called ludwig. It use tf under the hood. They say that models generated by ludwig saved by tf's saver.\r\nDirectory with model looks like that:\r\n![model](https://user-images.githubusercontent.com/9881948/56100847-6d614d00-5f26-11e9-8a41-fcbf9d966003.png)\r\nSo in documentation of this saver written:\r\n![tf structure](https://user-images.githubusercontent.com/9881948/56100870-c204c800-5f26-11e9-851f-267d02334c66.png)\r\nLooks like i have variables and don't have .pb file.\r\nSo is there a way to convert model as i have it to tflite model? I'm not sure where is a gap. Is it ludwig, tflite converter or just my understanding?", "hello, I got a similar error. I am running tensorflow on colab.research.google.com. I saved a model to \"drive/Colab/mo_big7_12345789bcdfghijklm\" while the tensorflow version was 1.14. it is just one file.\r\nnow, the tensorflow upgraded to 1.15. when loading the file by \r\ntf.keras.models.load_model( 'drive/Colab/mo_big7_12345789bcdfghijklm' )\r\nit raises: OSError: SavedModel file does not exist at: drive/Colab/mo_big7_12345789bcdfghijklm/{saved_model.pbtxt|saved_model.pb}\r\nI havn't seen the .pb or .pbtxt files before. how to fix it?", "> [odel](https://www.tensorflow.org/guide/saved_model).\r\n> \r\n> I was also struggl\r\n\r\nSo is there a method to convert such frozen pb files to savedModel format??\r\n", "> hello, I got a similar error. I am running tensorflow on colab.research.google.com. I saved a model to \"drive/Colab/mo_big7_12345789bcdfghijklm\" while the tensorflow version was 1.14. it is just one file.\r\n> now, the tensorflow upgraded to 1.15. when loading the file by\r\n> tf.keras.models.load_model( 'drive/Colab/mo_big7_12345789bcdfghijklm' )\r\n> it raises: OSError: SavedModel file does not exist at: drive/Colab/mo_big7_12345789bcdfghijklm/{saved_model.pbtxt|saved_model.pb}\r\n> I havn't seen the .pb or .pbtxt files before. how to fix it?\r\n\r\nI solved this error by remounting the drive path to google drive.", "If you run into this when trying to load a model stored in a .h5-file, e.g. one created using model.save for a tf.keras.Sequential model, the problem might simply be that h5py is not installed. To solve that problem, run `pip3 install h5py`.", "you should save your model in the .pb file.\r\nFirst, load you model if you saved it before and then run YOUR_MODEL.save('NAME.pb').\r\nNow you have a folder that contains saved model.pb and necessary other files and folders.\r\nCreate converter instance: convertor = tensorflow.lite.TFLiteConverter.from_saved_model('P1.pb').\r\nAt the end convert your model and save it: tfmodel = converter.convert()\r\nopen(\"model.tflite\",\"wb\").write(tfmodel)  ", "> If you run into this when trying to load a model stored in a .h5-file, e.g. one created using model.save for a tf.keras.Sequential model, the problem might simply be that h5py is not installed. To solve that problem, run `pip3 install h5py`.\r\n\r\n@ThomasKlein18 Hy Thomas, that appeared to be my error, however, now as I try to run the program I get the following error:\r\nAttributeError(\"type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'\")\r\nAny ideas on what it could be? Thank you", "Hello !!\r\n\r\nI had a similar problem with a saved model that was present in the folders but I could not load because of the same error.\r\nThe solution was to write the absolute path to the model, instead of the generic path that I had on the code.\r\n\r\nI hope that this can help to anyone with the same issue.\r\nRegards.", "Im getting the same error while trying to load the saved_model.pb file while using\r\n converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nIt shows the file when i run \"%ls {saved_model_dir}\"\r\nbut when i run it on converter it gives error : \"SavedModel file does not exist at: saved_model/{saved_model.pbtxt|saved_model.pb}\"", "I'm having the same issue when using ther converter\r\n\r\n```\r\nip-192-168-178-22:toxicity_model loretoparisi$ tensorflowjs_converter --input_format=tf_hub --output_format=tfjs_graph_model ./model.json ./saved\r\nLoading the module using TF 2.X interface from ./model.json.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tensorflowjs_converter\", line 8, in <module>\r\n    sys.exit(pip_main())\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflowjs/converters/converter.py\", line 757, in pip_main\r\n    main([' '.join(sys.argv[1:])])\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflowjs/converters/converter.py\", line 761, in main\r\n    convert(argv[0].split(' '))\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflowjs/converters/converter.py\", line 711, in convert\r\n    experiments=args.experiments)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py\", line 725, in convert_tf_hub_module\r\n    experiments=experiments)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py\", line 481, in convert_tf_saved_model\r\n    model = load(saved_model_dir, saved_model_tags)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 603, in load\r\n    return load_internal(export_dir, tags, options)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 614, in load_internal\r\n    loader_impl.parse_saved_model_with_debug_info(export_dir))\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 56, in parse_saved_model_with_debug_info\r\n    saved_model = _parse_saved_model(export_dir)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 113, in parse_saved_model\r\n    constants.SAVED_MODEL_FILENAME_PB))\r\nOSError: SavedModel file does not exist at: ./model.json/{saved_model.pbtxt|saved_model.pb}\r\nip-192-168-178-22:toxicity_model loretoparisi$ \r\n```", "Hello all, \r\nI am getting the OSError, when trying to access the saved model\r\n\r\ncommands used: \r\n\r\ntf.saved_model.save(model,'./content') - to save the model \r\n\r\nfiles inside content folder:\r\n02/17/2021  01:32 PM    <DIR>          assets\r\n02/17/2021  01:32 PM                         saved_model.pb\r\n02/17/2021  01:32 PM    <DIR>          variables\r\n\r\n!python -m tf2onnx.convert --saved-model './content' --opset 10 --output model.onnx\r\nerror: OSError: SavedModel file does not exist at: './content'/{saved_model.pbtxt|saved_model.pb}.\r\n\r\nI have tried by passing the absolute path, still the error persists, please help me resolve this issue.\r\n\r\nThank you\r\n", "ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'\r\n\r\n\r\nI am using faster_rcnn_inception_v2_coco model and want to convert it into tflite...\r\n\r\n\r\nimport tensorflow as tf\r\n\r\ntflite_model_name=\"TF.lite\"\r\nsaved_model_dir=\"exported_output_graph/saved_model/saved_model.pb\"\r\n #Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\r\ntflite_model = converter.convert()\r\n\r\nwith open(tflite_model_name, 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n\r\nI don't what I am missing here", "I am new to tf but i solved the issue by pointing to the folder with my saved model as opposed to pointing to the .pb file.\r\n\r\nsaved_path = 'models/saved_model1'\r\n\r\nNOT\r\n\r\nsaved_path = 'models/saved_model1/saved_model.pb'\r\n\r\nI hope this helps.", "hello \r\nSimply just give the directory of the file containing all files to the path.\r\nmy first path was : /home/armin/deepSORT/yolov4-deepsort/checkpoints/yolov4-416/saved_model.pb'\r\nwhich i got error No such directory so i changed it to:\r\n/home/armin/deepSORT/yolov4-deepsort/checkpoints/yolov4-416'\r\nand it solved my problem.", "```\r\nimport tensorflow as tf\r\nimport os\r\n \r\n# Convert the model\r\nmypath =\"E:/strokestuff/strokelr\"\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(mypath) \r\ntflite_model = converter.convert()\r\n \r\n# Save the model.\r\nwith open('stroke_lr.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n  ```\r\n\r\n  The error I get:\r\n  OSError: SavedModel file does not exist at: E:/strokestuff/strokelr\\{saved_model.pbtxt|saved_model.pb}\r\n\r\nCan someone help?", "Hii, \r\nI am facing the same error, \r\nIn my case there exists a save_model.pb file but still, it showing me this...\r\n\r\n### Here's my code:\r\n```\r\nMODEL = tf.keras.models.load_model(\"..\\\\training\\\\my_model\")\r\n```\r\n\r\n### Result got while debugging:\r\n```\r\nException has occurred: OSError\r\nSavedModel file does not exist at: ..\\training\\my_model\\{saved_model.pbtxt|saved_model.pb}\r\n  File \"F:\\Projects\\Deep Learning\\Potato Leave\\api\\main.py\", line 9, in <module>\r\n    MODEL = tf.keras.models.load_model(\"..\\\\training\\\\my_model\")\r\n```\r\n\r\nCan someone help?", "Which model are you trying to convert...?", "There are only some model that are tflite supported ", "when using conda environment uninstalling h5py installed from pip and reinstall it using conda solve my problem.\r\n```\r\npip uninstall h5py\r\nconda install h5py\r\n```", "**Commenting incase if someone finds this useful**\r\nJust make sure git is actually downloading the large lfs file `model.h5`\r\n```\r\napt install git-lfs\r\ngit lfs fetch\r\ngit lfs pull\r\n```"]}, {"number": 22479, "title": "Keras: removing layers with model.layers.pop() doesn't work", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Colab\r\n- **TensorFlow version (use command below)**:  1.10.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: Colab\r\n- **GCC/Compiler version (if compiling from source)**: Colab\r\n- **CUDA/cuDNN version**: Colab\r\n- **GPU model and memory**: Colab\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen we delete a layer with model.layers.pop(), the deleted layer reappears. \r\n\r\n### Source code / logs\r\n\r\n    import tensorflow as tf\r\n    import tensorflow.keras as keras\r\n    import tensorflow.keras.backend as K\r\n    from tensorflow.keras.layers import Dense, Input, Layer\r\n    from tensorflow.keras.models import Model\r\n    input_tensor = Input(shape=(10,))\r\n    hidden = Dense(100, activation='relu')(input_tensor)\r\n    out = Dense(10, activation='relu')(hidden)\r\n    model = Model(input_tensor, out)\r\n    model.compile(loss=\"mse\", optimizer=tf.train.AdamOptimizer(learning_rate=0.001))\r\n    model.summary()\r\n    model.layers.pop()\r\n    model.layers.pop()\r\n    model.summary()\r\n    hidden = Dense(120, activation='relu')(model.layers[-1].output)\r\n    out = Dense(5, activation='softmax')(hidden)\r\n    model = Model(input_tensor, out)\r\n    model.summary()\r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 10)                0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 100)               1100      \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                1010      \r\n=================================================================\r\nTotal params: 2,110\r\nTrainable params: 2,110\r\nNon-trainable params: 0\r\n```\r\n```\r\n_________________________________________________________________\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 10)                0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 100)               1100      \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                1010      \r\n=================================================================\r\nTotal params: 2,110\r\nTrainable params: 2,110\r\nNon-trainable params: 0\r\n\r\n```\r\n\r\n```\r\n_________________________________________________________________\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 10)                0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 100)               1100      \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                1010      \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 120)               1320      \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 5)                 605       \r\n=================================================================\r\nTotal params: 4,035\r\nTrainable params: 4,035\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```", "comments": ["@nairouz  Please refer to a similar [issue](https://github.com/keras-team/keras/issues/8909)\r\nTry assigning a new model for the reduced layers model.", "@ymodak thank you for the response. I have created a second model as suggested in the similar thread. \r\nUnfortunately, it is not working. Please note that in [https://github.com/keras-team/keras/issues/8909](url), the user mentioned that `the  model.summary() shows that the layer has been removed`. For me, it is not the case.\r\n\r\nHere is the code. Check it out.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Dense, Input, Layer\r\nfrom tensorflow.keras.models import Model\r\n\r\ninput_tensor = Input(shape=(20,), name=\"input\")\r\nhidden = Dense(100, activation='relu')(input_tensor)\r\nout = Dense(10, activation='relu', name=\"out\")(hidden)\r\n\r\nmodel = Model(inputs=input_tensor, outputs=out)\r\nmodel.compile(loss=\"mse\", optimizer=tf.train.AdamOptimizer(learning_rate=0.001))\r\n\r\nmodel.summary()\r\nmodel.layers.pop()\r\nmodel.summary()\r\n\r\nhidden = Dense(120, activation='relu')(model.layers[-1].output)\r\nout = Dense(5, activation='softmax')(hidden)\r\nmodel2 = Model(input_tensor, out)\r\n\r\nmodel2.summary()\r\n```\r\n\r\n", "@nairouz \r\nThere are two issues in your code:\r\n* You can't use ```model.layers.pop()``` to remove the last layer in the model. In ```tf.keras```, ```model.layers``` will return a shallow copy version of the layers list, so actually you don't remove that layer, just remove the layer in the return value.\r\n* If you want to remove the last dense layer and add your own one, you should use ```hidden = Dense(120, activation='relu')(model.layers[-2].output)```. ```model.layers[-1].output``` means the last layer's output which is the final output, so in your code, you actually didn't remove any layers.\r\n\r\nThanks.", "@yanboliang \r\nThank you very much for the response.\r\nSo basically, there is no need to use `model.layers.pop()` in this case if I understood the point.\r\nActually, I think the `pop` method implementation has changed since I have found many examples where it is used to remove a layer from a model.\r\nYou can check:  \r\n1) [https://stackoverflow.com/questions/41668813/how-to-add-and-remove-new-layers-in-keras-after-loading-weights](https://stackoverflow.com/questions/41668813/how-to-add-and-remove-new-layers-in-keras-after-loading-weights)\r\n2) [https://github.com/keras-team/keras/issues/8909](https://github.com/keras-team/keras/issues/8909)", "Closing this issue since the solution provided by @yanboliang works. ", "Faced the same issue. \r\nTry popping the last layer after loading the model\r\nvgg16_model = keras.applications.vgg16.VGG16()\r\nvgg16_model.layers.pop()\r\nThen, if you want to create a Sequential one just iterrate over it\r\nmodel = Sequential()\r\nfor layer in vgg16_model.layers:\r\n    model.add(layer)\r\nand then add the other layer(s) that you want\r\nmodel.add(Dense(3, activation='softmax'))\r\n\r\n\r\nThat should work", "The purpose of model.layers.pop() is to remove the last layer and fine-tune it reserving the already-trained weights. I resolved the problem simply as below.\r\n\r\nvgg16_model = keras.applications.vgg16.VGG16()\r\nmodel = Sequential()\r\nfor layer in vgg16_model.layers[:-1]:   # just exclude last layer from copying\r\n     model.add(layer)\r\nfor layer in model.layers:       \r\n     layer.trainable = False\r\nmodel.add(Dense(2, activation='softmax'))\r\n\r\nWith model.summary(), you can confirm that the last layer of the model has been replaced.", "@ironmanciti  @AthanasiosTsiaras  This solution gives me\r\n\r\n```\r\nTypeError: The added layer must be an instance of class Layer. Found: <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f9869a25cc0>\r\n```", "I am pretty sure that the original poster was talking about tf.keras NOT Keras.  Some of the responses apply to Keras NOT tf.keras.  (I am just trying to get this clear for myself, don't be offended by the all caps!)  \r\n\r\nI am trying to solve the same issue.  I want to remove the last layer from Inception_V3 in tf.keras.  Then I want to add an additional layer for regression.  I know how to do it in Keras, but not tf.keras.  \r\n\r\nIf you load Inception_V3 in Keras, it shows about 300 layers.  If you load it in tf.keras and print the layer summary you see something completely different.  \r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            (None, 256, 256, 1)  0                                            \r\n__________________________________________________________________________________________________\r\nlambda (Lambda)                 (None, 256, 256, 1)  0           input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\nlambda_1 (Lambda)               (None, 256, 256, 1)  0           input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ninception_v3 (Model)            (None, 11)           21824747    lambda[0][0]                     \r\n                                                                 lambda_1[0][0]                   \r\n__________________________________________________________________________________________________\r\npredictions (Concatenate)       (None, 11)           0           inception_v3[1][0]               \r\n                                                                 inception_v3[2][0]               \r\n==================================================================================================\r\nTotal params: 21,824,747\r\nTrainable params: 21,790,315\r\nNon-trainable params: 34,432", "I got a solution by using\r\n`model._layers.pop(0)`\r\ninstead of\r\n`model.layers.pop(0)`", "Can someone please provide an example of the best way to do this for tf.keras? All I want to do is remove the last layer of a pretrained model\r\n", "@jamescook106 \r\n\r\nThe best way is to create a new model that ends before the last layer of that model. Using the Keras functional model.\r\n\r\nAs far as I'm aware there is **no** method in Keras to delete a specific layer - you need to take apart the entire model and reconstruct it yourself.", "Here is an example of how to remove the last layer of a pretrained model in tf.keras (NOT keras!) and add a new output layer.   This method could be used to change the number of classes for a classification problem, or to change the model to for a regression problem.  The example below changes a 10 class classification model to a 2 output regression model using InceptionV3 in tf.keras.\r\n\r\n### Create InceptionV3 model, load pre-trained weights, find layer you want add on to \r\nnb_classes = 10; # number must match your pre-trained weights\r\nbase_model =InceptionV3(include_top=True, weights=weightFile, input_shape=input_shape, classes=nb_classes)\r\nprint(base_model.summary()) # find layer in the summary that you want to add new output to\r\n\r\nbase_output = base_model.layers[311].output # layer number obtained from model summary above\r\n\r\n### attach new desired output layer to the base model to create your new model\r\nnum_regression_targets = 2 # some point (x, y)\r\nnew_output = tf.keras.layers.Dense(activation=\"sigmoid\", units=num_regression_targets)(base_output)\r\nnew_model = tf.keras.models.Model(inputs=base_model.inputs, outputs=new_output)\r\nprint(new_model.summary())\r\n\r\n### Now the new_model is ready to use", "I re-rad my post above, and realize the explanation could be clearer.  Basically, the strategy is to create a new model starting from a base model that is a standard model included in tf.keras for which you have pre-trained weights.   You can find the standard models are included in tf.keras.keras.applications (tensorflow 1.13) here:\r\n\r\nhttps://github.com/keras-team/keras-applications\r\n\r\nYou create an instance of the base model, then look for the layer which feeds into the final fully connected layer (or other desired layer) by looking at the model summary.  Then you create a new layer which takes as input the desired layer output from the base model.  Finally, you define a new model in tf.keras.model.Models() by defining the inputs and outputs.  Tensorflow will now have a graph of the base model with the new output layer.  ", "Hi, thanks for your response. This makes sense, but from your code it looks like I have to add a new layer. What I want to do is strip the last layer (the softmax layer in my model) and not add a new layer. Is there equivalent way to the old Keras pop method ", "Take a look at the model summary of the new model.  The softmax layer is excluded because it is not needed to produce the final output.  So if you compare the base model to the new model, you will see that you have essentially stripped the last layer.", "@cmwolverton  How does tf.keras know which layers to process?   I see in your solution we are taking the base_output and providing it as input to the new Dense() layer, but the skipped InceptionV3 layers after 311 still exist in the model object, which is where layers 0-310 we are using also exist.   So when model.fit() is running, does the unused layers [311:] still be receiving data and consuming CPU cycles?", "@jamescook106 Did anybody find why pop() does not work with tf.keras() ? I am unable to remove the last layer with .pop() method. My question basically is whether I can rely on the old .pop() method or not. Thanks. G.", "Only thing which worked in my case (with TF 2.0.0) was :\r\n```python\r\nmodel = tf.keras.models.load_model(path_model)\r\nmodel.pop()  # to remove last layer\r\nnew_model = tf.keras.Sequential([\r\n\tmodel,\r\n\ttf.keras.layers.Dense(units=nb_classes, activation=\"softmax\", name=\"new_layer_name\")\r\n])\r\nnew_model.build((None, height, width, 3))\r\n```", "for me following code is a nice workaround. It builds another model without the last layer. Thus effectively being .pop()\r\n\r\n```\r\nmodel_reduced = Sequential()\r\n\r\nfor layer in old_model.layers[:-1]:\r\n  model_reduced.add(layer)\r\n```\r\n", "Previous replies mention iterating over base model and adding to a new model, which seems ok for VGG16.\r\nFor MobileNetV2 it seems to not be possible\r\n\r\nwhen iterating over its layers it will raise an error when reaching an merge.Add layer\r\n<tensorflow.python.keras.layers.merge.Add object at 0x000002BDB32D0F48>\r\n\r\n```ValueError: A merge layer should be called on a list of inputs.```\r\n\r\nI understood from previous conversation that using model.layers will bring a shallow copy from the model and using model._layers would bring the actual layers.\r\nI observed that as @cgebbe mentioned will indeed remove the layers (at least from summary view)\r\n\r\n> \r\n> I got a solution by using\r\n> `model._layers.pop(0)`\r\n> instead of\r\n> `model.layers.pop(0)`\r\n\r\nI didn't found any cons of using this method, but usually messing around the private vars is not a safe thing to do if you don't know the details of the implementation. Does anybody found any problem with this method ?\r\nAnother question would be how I can properly add the mege.Add object layer to my model if I would try the iteration method.\r\n\r\nThanks!\r\n", "> Previous replies mention iterating over base model and adding to a new model, which seems ok for VGG16.\r\n> For MobileNetV2 it seems to not be possible\r\n> \r\n> when iterating over its layers it will raise an error when reaching an merge.Add layer\r\n> <tensorflow.python.keras.layers.merge.Add object at 0x000002BDB32D0F48>\r\n> \r\n> `ValueError: A merge layer should be called on a list of inputs.`\r\n> \r\n> I understood from previous conversation that using model.layers will bring a shallow copy from the model and using model._layers would bring the actual layers.\r\n> I observed that as @cgebbe mentioned will indeed remove the layers (at least from summary view)\r\n> \r\n> > I got a solution by using\r\n> > `model._layers.pop(0)`\r\n> > instead of\r\n> > `model.layers.pop(0)`\r\n> \r\n> I didn't found any cons of using this method, but usually messing around the private vars is not a safe thing to do if you don't know the details of the implementation. Does anybody found any problem with this method ?\r\n> Another question would be how I can properly add the mege.Add object layer to my model if I would try the iteration method.\r\n> \r\n> Thanks!\r\n\r\nI think that's because the structure of mobilenetV2 isn't \"sequential\". For the method that you iterate through all the layers, you assume the model structure passes data through layer by layer. This method is available for VGG since VGG is this kind of model. Yet some model structures aren't like this, such as resnet which contains shortcuts for some data. In these kind of cases, including mobilenetV2, you have to use the functional api solution provided by @cmwolverton [here](https://github.com/tensorflow/tensorflow/issues/22479#issuecomment-487695679).", "pop doesn't help anything here, the correct way to do it is:\r\n\r\n```\r\nmodel2 = Model(model.input, model.layers[-2].output)\r\n```\r\nTested this only for a non-sequential model."]}, {"number": 22478, "title": "Fix for tf.keras.regularizers.{l1,l2}(0.) with tf.get_variable", "body": "This fix tries to address the issue in #22470 where tf.keras.regularizers.{l1,l2}(l=0.) with tf.get_variable returns\r\n```\r\nAttributeError: 'float' object has no attribute 'name'\r\n```\r\n\r\nThe issue only happened when regularization was disabled `l=0.`, as in that case, `regularization = 0.`\r\nwas returned directly.\r\n\r\nThis fix convert `regularization = 0.` to a tensor.\r\n\r\nThis fix fixes #22470.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["@fchollet Could you take a look at this?", "@yongtang Could you look into the failing tests? Probably consider populating more test cases.", "Thanks @fchollet @wt-huang for the review. The PR has been updated, and a unit test has been added as well. Please take a look and let me know if there are any issues.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@yongtang Thanks for the fix! Made a couple of pylint changes.\r\nCLA should be good to go.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@yongtang For formality check, please confirm that you are okay with all the changes made so far. Thanks!", "@wt-huang  Yes I am ok with all the changes. Thanks for the help!", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@wt-huang Looks like all tests passed, except the copybara. Wondering if there is any update or anything else needs to be done?", "Thanks @yongtang LGTM\r\n@fchollet Could you take another look?"]}, {"number": 22477, "title": "failed to query event: CUDA_ERROR_LAUNCH_FAILED", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\nLinux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNA\r\n- **TensorFlow installed from (source or binary)**: \r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.10.1\r\n- **Python version**:\r\n3.6.5\r\n- **Bazel version (if compiling from source)**:\r\nNA\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNA\r\n- **CUDA/cuDNN version**:\r\n9.0 cuDNN 7.2.1\r\n- **GPU model and memory**:\r\nGeForce GTX 1080 Ti 11GB\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nEncountered this error while running a image classification training script. \r\nE tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED\r\n2018-09-23 18:31:38.983015: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:206] Unexpected Event status: 1\r\nAborted (core dumped)\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@monikatomar92 Could you post your tf_env.txt here after environment capture script?", "Hi. Thank you, Following is the tf_env.txt\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2413448/tf_env.txt)\r\n", "@monikatomar92 You may need to install CUDA9.2. Could you post the files under /usr/local/cuda-9.0/lib64? ", "These are the files I have under /usr/local/cuda-9.0/lib64\r\nlibaccinj64.so          libcusolver_static.a    libnppist_static.a\r\nlibaccinj64.so.9.0      libcusparse.so          libnppisu.so\r\nlibaccinj64.so.9.0.176  libcusparse.so.9.0      libnppisu.so.9.0\r\nlibcublas_device.a      libcusparse.so.9.0.176  libnppisu.so.9.0.176\r\nlibcublas.so            libcusparse_static.a    libnppisu_static.a\r\nlibcublas.so.9.0        libnppc.so              libnppitc.so\r\nlibcublas.so.9.0.176    libnppc.so.9.0          libnppitc.so.9.0\r\nlibcublas.so.9.0.282    libnppc.so.9.0.176      libnppitc.so.9.0.176\r\nlibcublas.so.9.0.333    libnppc_static.a        libnppitc_static.a\r\nlibcublas.so.9.0.425    libnppial.so            libnpps.so\r\nlibcublas.so.9.0.480    libnppial.so.9.0        libnpps.so.9.0\r\nlibcublas_static.a      libnppial.so.9.0.176    libnpps.so.9.0.176\r\nlibcudadevrt.a          libnppial_static.a      libnpps_static.a\r\nlibcudart.so            libnppicc.so            libnvblas.so\r\nlibcudart.so.9.0        libnppicc.so.9.0        libnvblas.so.9.0\r\nlibcudart.so.9.0.176    libnppicc.so.9.0.176    libnvblas.so.9.0.176\r\nlibcudart_static.a      libnppicc_static.a      libnvblas.so.9.0.282\r\nlibcudnn.so             libnppicom.so           libnvblas.so.9.0.333\r\nlibcudnn.so.7           libnppicom.so.9.0       libnvblas.so.9.0.425\r\nlibcudnn.so.7.2.1       libnppicom.so.9.0.176   libnvblas.so.9.0.480\r\nlibcudnn_static.a       libnppicom_static.a     libnvgraph.so\r\nlibcufft.so             libnppidei.so           libnvgraph.so.9.0\r\nlibcufft.so.9.0         libnppidei.so.9.0       libnvgraph.so.9.0.176\r\nlibcufft.so.9.0.176     libnppidei.so.9.0.176   libnvgraph_static.a\r\nlibcufft_static.a       libnppidei_static.a     libnvrtc-builtins.so\r\nlibcufftw.so            libnppif.so             libnvrtc-builtins.so.9.0\r\nlibcufftw.so.9.0        libnppif.so.9.0         libnvrtc-builtins.so.9.0.176\r\nlibcufftw.so.9.0.176    libnppif.so.9.0.176     libnvrtc.so\r\nlibcufftw_static.a      libnppif_static.a       libnvrtc.so.9.0\r\nlibcuinj64.so           libnppig.so             libnvrtc.so.9.0.176\r\nlibcuinj64.so.9.0       libnppig.so.9.0         libnvToolsExt.so\r\nlibcuinj64.so.9.0.176   libnppig.so.9.0.176     libnvToolsExt.so.1\r\nlibculibos.a            libnppig_static.a       libnvToolsExt.so.1.0.0\r\nlibcurand.so            libnppim.so             libOpenCL.so\r\nlibcurand.so.9.0        libnppim.so.9.0         libOpenCL.so.1\r\nlibcurand.so.9.0.176    libnppim.so.9.0.176     libOpenCL.so.1.0\r\nlibcurand_static.a      libnppim_static.a       libOpenCL.so.1.0.0\r\nlibcusolver.so          libnppist.so            stubs\r\nlibcusolver.so.9.0      libnppist.so.9.0\r\nlibcusolver.so.9.0.176  libnppist.so.9.0.176\r\n", "@monikatomar92 It appears that you need to install CUDA9.2 for your GPU. Also make sure that `LD_LIBRARY_PATH` is set up correctly.", "@wt-huang I installed CUDA9.2 but I am not able to install/build tensorflow for the same. Any suggestions with if the latest release of tensorflow would work with CUDA9.2? ", "@monikatomar92 Please post the errors here.", "\r\nERROR: /home/monika/tensorflow/tensorflow/core/BUILD:1010:1: C++ compilation of rule '//tensorflow/core:logging_ops_op_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/monika/.cache/bazel/_bazel_monika/c1ba60c7350215d6c41b5c2e155ce675/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/cuda/lib64 \\\r\n    PATH=/home/monika/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/monika/bin:/usr/local/cuda/bin:/home/monika/bin:/home/monika/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/_objs/logging_ops_op_lib/logging_ops.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/_objs/logging_ops_op_lib/logging_ops.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -isystem external/com_google_absl -isystem bazel-out/host/genfiles/external/com_google_absl -isystem bazel-out/host/bin/external/com_google_absl -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread -c tensorflow/core/ops/logging_ops.cc -o bazel-out/host/bin/tensorflow/core/_objs/logging_ops_op_lib/logging_ops.pic.o)\r\nIn file included from external/protobuf_archive/src/google/protobuf/wire_format_lite_inl.h:44:0,\r\n                 from external/protobuf_archive/src/google/protobuf/map_type_handler.h:35,\r\n                 from external/protobuf_archive/src/google/protobuf/map.h:49,\r\n                 from external/protobuf_archive/src/google/protobuf/generated_message_table_driven.h:34,\r\n                 from bazel-out/host/genfiles/tensorflow/core/framework/types.pb.h:25,\r\n                 from ./tensorflow/core/framework/tensor_shape.h:22,\r\n                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:20,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from tensorflow/core/ops/logging_ops.cc:16:\r\nexternal/protobuf_archive/src/google/protobuf/repeated_field.h: In instantiation of 'typename TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add(typename TypeHandler::Type*) [with TypeHandler = google::protobuf::RepeatedPtrField<tensorflow::TensorShapeProto>::TypeHandler; typename TypeHandler::Type = tensorflow::TensorShapeProto]':\r\nexternal/protobuf_archive/src/google/protobuf/repeated_field.h:1977:49:   required from 'Element* google::protobuf::RepeatedPtrField<T>::Add() [with Element = tensorflow::TensorShapeProto]'\r\nbazel-out/host/genfiles/tensorflow/core/framework/attr_value.pb.h:1001:21:   required from here\r\nexternal/protobuf_archive/src/google/protobuf/repeated_field.h:1539:1: internal compiler error: Segmentation fault\r\n }\r\n ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.\r\nThe bug is not reproducible, so it is likely a hardware or OS problem.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 139.522s, Critical Path: 57.15s\r\nINFO: 221 processes: 221 local.\r\nFAILED: Build did NOT complete successfully\r\n", "@monikatomar92 What gcc and bazel version did you use? Try to set up this environment on your system: CUDA 9.2, CuDNN 7.2, gcc: 4.8, Bazel 0.15.0, TensorFlow 1.11.0", "@wt-huang I tried setting up this environment as you mentioned in the last comment. I still couldn't build the tensorflow. Got the following error :\r\nERROR: /home/monika/.cache/bazel/_bazel_monika/c1ba60c7350215d6c41b5c2e155ce675/external/nasm/BUILD.bazel:8:1: C++ compilation of rule '@nasm//:nasm' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/monika/.cache/bazel/_bazel_monika/c1ba60c7350215d6c41b5c2e155ce675/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/lib64 \\\r\n    PATH=/home/monika/anaconda3/bin:/home/monika/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/monika/bin:/usr/local/cuda/bin:/home/monika/bin:/home/monika/bin:/home/monika/bin:/usr/local/cuda/bin:/home/monika/bin:/home/monika/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/nasm/_objs/nasm/external/nasm/asm/nasm.d -DHAVE_SNPRINTF -DHAVE_SYS_TYPES_H -iquote external/nasm -iquote bazel-out/host/genfiles/external/nasm -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/nasm/asm -isystem bazel-out/host/genfiles/external/nasm/asm -isystem bazel-out/host/bin/external/nasm/asm -isystem external/nasm/include -isystem bazel-out/host/genfiles/external/nasm/include -isystem bazel-out/host/bin/external/nasm/include -isystem external/nasm/output -isystem bazel-out/host/genfiles/external/nasm/output -isystem bazel-out/host/bin/external/nasm/output -isystem external/nasm/x86 -isystem bazel-out/host/genfiles/external/nasm/x86 -isystem bazel-out/host/bin/external/nasm/x86 -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -w '-std=c99' -c external/nasm/asm/nasm.c -o bazel-out/host/bin/external/nasm/_objs/nasm/external/nasm/asm/nasm.o)\r\nexternal/nasm/asm/nasm.c: In function \u2018main\u2019:\r\nexternal/nasm/asm/nasm.c:517:1: internal compiler error: in hash_rtx_cb, at cse.c:2533\r\n }\r\n ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.\r\nThe bug is not reproducible, so it is likely a hardware or OS problem.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 25.844s, Critical Path: 1.54s\r\nINFO: 94 processes: 94 local.\r\nFAILED: Build did NOT complete successfully", "@monikatomar92 I was able to install TensorFlow using the said environment. It seems that crosstool/gcc is not installed correctly in your system. You can either follow [this document](https://medium.com/@Oysiyl/install-tensorflow-1-8-0-with-gpu-from-source-on-ubuntu-18-04-bionic-beaver-35cfa9df3600) to install TensorFlow or use [docker](https://www.tensorflow.org/install/docker). ", "Closing this issue for now, feel free to reopen if problem persists."]}, {"number": 22476, "title": "ArchLinux [Optimus-Bumblebee] - Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ArchLinux x64\r\n- **TensorFlow installed from (source or binary)**: via pip\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6.5\r\n- **CUDA/cuDNN version**: CUDA 9.2.148 - cuDNN 7.2.1\r\n- **GPU model and memory**: NVIDIA GTX 970M 3GB\r\n\r\n- **Exact command to reproduce**:\r\n\r\n- Note: I'm using Nvidia Optimus with Bumblebee: https://wiki.archlinux.org/index.php/bumblebee\r\n\r\n1. git clone https://github.com/isseu/emotion-recognition-neural-networks\r\n* Make reqiured configurations...\r\n\r\n2. I removed `tensorflow` and added `tensorflow-gpu 1.10.0` to `requirements.txt`\r\n\r\n3. `sudo optirun python3 emotion_recognition.py train` also tried:\r\n`python3 emotion_recognition.py train`\r\n\r\n### Describe the problem\r\n\r\nTo run Train the data on my GPU, I just edited a few lines on emotion_recognation.py:\r\n\r\n`import tensorflow as tf`\r\n\r\nAdded under `def __init__(self):`\r\n`tf.ConfigProto(allow_soft_placement=True)`\r\n\r\nAdded under `def start_training(self):`\r\n```\r\n        with tf.device('/gpu:0'):\r\n            self.model.fit(bla bla bla)\r\n```\r\n\r\n### Source code / logs\r\n\r\n```\r\nhdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\r\n[+] Dataset found and loaded\r\n[+] Building CNN\r\nWARNING:tensorflow:From /home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\r\nWARNING:tensorflow:From /home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\n2018-09-23 21:31:13.883618: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-23 21:31:14.001876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-23 21:31:14.002292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.95GiB freeMemory: 2.89GiB\r\n2018-09-23 21:31:14.002318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-23 21:31:14.260311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-23 21:31:14.260358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-09-23 21:31:14.260364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-09-23 21:31:14.260525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2595 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n2018-09-23 21:31:14.554680: W tensorflow/core/framework/allocator.cc:108] Allocation of 226492416 exceeds 10% of system memory.\r\n2018-09-23 21:31:14.644729: W tensorflow/core/framework/allocator.cc:108] Allocation of 226492416 exceeds 10% of system memory.\r\n2018-09-23 21:31:14.733250: W tensorflow/core/framework/allocator.cc:108] Allocation of 226492416 exceeds 10% of system memory.\r\n2018-09-23 21:31:14.822594: W tensorflow/core/framework/allocator.cc:108] Allocation of 226492416 exceeds 10% of system memory.\r\n2018-09-23 21:31:14.911661: W tensorflow/core/framework/allocator.cc:108] Allocation of 226492416 exceeds 10% of system memory.\r\n2018-09-23 21:31:15.880266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-23 21:31:15.880304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-23 21:31:15.880313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-09-23 21:31:15.880321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-09-23 21:31:15.880412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2595 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n[+] Training network\r\n---------------------------------\r\nRun id: emotion_recognition\r\nLog directory: /tmp/tflearn_logs/\r\n---------------------------------\r\nTraining samples: 11214\r\nValidation samples: 2804\r\n--\r\nTraceback (most recent call last):\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1261, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1295, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Accuracy/__raw_': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n\r\n\t [[Node: Accuracy/__raw_ = ScalarSummary[T=DT_FLOAT, _device=\"/device:GPU:0\"](Accuracy/__raw_/tags, Accuracy/Mean)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"emotion_recognition.py\", line 110, in <module>\r\n    network.start_training()\r\n  File \"emotion_recognition.py\", line 78, in start_training\r\n    run_id='emotion_recognition'\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/models/dnn.py\", line 216, in fit\r\n    callbacks=callbacks)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/trainer.py\", line 339, in fit\r\n    show_metric)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/trainer.py\", line 816, in _train\r\n    tflearn.is_training(True, session=self.session)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/config.py\", line 95, in is_training\r\n    tf.get_collection('is_training_ops')[0].eval(session=session)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 680, in eval\r\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4951, in _eval_using_default_session\r\n    return session.run(tensors, feed_dict)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Accuracy/__raw_': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n\r\n\t [[Node: Accuracy/__raw_ = ScalarSummary[T=DT_FLOAT, _device=\"/device:GPU:0\"](Accuracy/__raw_/tags, Accuracy/Mean)]]\r\n\r\nCaused by op 'Accuracy/__raw_', defined at:\r\n  File \"emotion_recognition.py\", line 110, in <module>\r\n    network.start_training()\r\n  File \"emotion_recognition.py\", line 78, in start_training\r\n    run_id='emotion_recognition'\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/models/dnn.py\", line 216, in fit\r\n    callbacks=callbacks)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/trainer.py\", line 288, in fit\r\n    self.summ_writer, self.coord)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/trainer.py\", line 794, in initialize_fit\r\n    val_feed_dict)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/trainer.py\", line 937, in create_testing_summaries\r\n    summarize(self.metric, \"scalar\", sname, tr_summ_collection)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/helpers/summarizer.py\", line 98, in summarize\r\n    summaries.get_summary(type, name, value, summary_collection)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tflearn/summaries.py\", line 46, in get_summary\r\n    summ = tf.summary.scalar(tag, value)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/summary/summary.py\", line 90, in scalar\r\n    val = _gen_logging_ops.scalar_summary(tags=tag, values=tensor, name=scope)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 623, in scalar_summary\r\n    \"ScalarSummary\", tags=tags, values=values, name=name)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n    op_def=op_def)\r\n  File \"/home/dentrax/Projects/GitHub/emotion-recognition-neural-networks/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Accuracy/__raw_': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n\r\n\t [[Node: Accuracy/__raw_ = ScalarSummary[T=DT_FLOAT, _device=\"/device:GPU:0\"](Accuracy/__raw_/tags, Accuracy/Mean)]]\r\n```\r\n", "comments": ["I can see that TF can successfully read and recognize your device, because of these logs:\r\n```\r\n2018-09-23 21:31:14.002292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.95GiB freeMemory: 2.89GiB\r\n2018-09-23 21:31:14.002318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-23 21:31:14.260311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-23 21:31:14.260358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-09-23 21:31:14.260364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n```\r\n\r\nYour problem is, the op you are using does not have GPU kernels:\\\r\n```\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Accuracy/__raw_': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n```\r\n\r\nAs you are using a non-official TF model, I recommend reaching out to the maintainers of the repo you downloaded the model from.\r\nhttps://github.com/isseu/emotion-recognition-neural-networks\r\n\r\nAs far as I can see, you are using bumblebee right by prepending the `optirun` command. TF does things right by accessing your GPU. but the model may be using a non-gpu op.\r\nClosing the issue as TF seems to be working correctly.\r\n\r\n", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nMobile device"]}, {"number": 22475, "title": "The tensorflow tested builds advices wrong bazel version for tensorflow-gpu r1.8", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNo\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nr1.8\r\n- **Python version**:\r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n4.8.5\r\n- **CUDA/cuDNN version**:\r\n9.0/7.1\r\n- **GPU model and memory**:\r\nGeForce GTX 960M\r\n- **Exact command to reproduce**:\r\n./configure \r\nThe output is:\r\nYou have bazel 0.9.0 installed.\r\nPlease upgrade your bazel installation to version 0.10.0 or higher to build TensorFlow!\r\nConfiguration finished\r\n\r\n### Describe the problem\r\nI tried to build tf r1.8 with gpu with bazel 0.17 but got errors then I checked [here](https://www.tensorflow.org/install/source) to make sure I got the right version, but saw that I need bazel 0.9.0. When trying it with bazel 0.9.0 I get the error above.\r\nI did not find where I could edit the tf website itself to make a PR, so I just opened this issue. \r\n\r\n### Source code / logs\r\nNone\r\n", "comments": ["I think 0.17 was a typo.\r\n@lamberta the problem is for Linux, tensorflow-gpu, version 1.8.0, we have bazel 0.9.0.\r\nBut it needs to be bazel 0.10.\r\nWhere do we update that?\r\n"]}, {"number": 22474, "title": "[r1.11] Fix MPI build failure caused by StringPiece -> absl::string_view", "body": "**NOTE: This PR is against r1.11, not sure if it is ok as r1.11 is already in rc2.**\r\n\r\n**Please feel free to close if it is not appropriate.**\r\n\r\nThis fix cherry picks #22084 to tries to fix the MPI build failure caused by StringPiece -> absl::string_view.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Assigning to Austin and Gunan since its a cherrypick to RC.", "We will cut 1.12 in 1 week, so I would recommend rejecting this PR for the fix in contrib.\r\n@martinwicke @angersson to make the decision.", "Let's put this into 1.12, we can't really afford 1.11 to slip further.\r\n\r\n@yongtang is this on master already?", "@martinwicke Yes this is in already in master. Given the timeline for r1.11, let me close this PR now. Thanks all for the help!"]}, {"number": 22473, "title": "[Features] Enable partitioned variable assignments", "body": "`PartitionedVariable` cannot use `assign`, `assign_add` and `assign_sub` functions in graph mode. In this pull request, I have enabled these functions in graph mode. This is the first step to enable variables partitioner usage in distribution strategy.", "comments": ["@ebrevdo @theweiho Any coding style and design review comments are welcome. @yuefengz This maybe the first step to enable variable partitioned usage in distribution strategy", "Thank @wangsiyu for the work. \r\n@yuefengz  @ebrevdo @theweiho any feedback from your sides are highly appreciated.\r\n\r\nTalk is cheap, show the code.\r\n\r\nSiyu directly made this PR to ensure the communication could be more efficient.\r\n", "Unrelated failures? @ebrevdo @theweiho I noticed master branch also failed with these unitest especially in  XLA.", "@alextp Hi, Alexandre. The PR seems to be duplicated with tf 2.0 tensorflow/community#11 , right?\r\n\r\n> symbols like tf.assign* will be removed in favor of methods in tf.Variable\r\n\r\nCould you make a comment or reassign? Should we accept the PR or reject?", "Sorry\uff0cI am confused. These assign* methods have been already implemented in PartitionedVariable class and I think it complies with 2.0\uff1f", "@alextp \r\n\r\nHi Alex,\r\n\r\nI am also a little bit puzzled as to your comments.\r\n\r\nIt is appreciated if you could directly comment on our code commit so it would be  easier to ensure we are on the same page.\r\n\r\nThanks.", "I meant this NotImplementedError: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variables.py#L2400\r\n\r\nIf we can replace this error with an implementation of assign, assign_add, etc for PartitionedVariable this will be more 2.0-friendly.", "@alextp I have refined the coding style according to @facaiy .These test failure seems not related to me. Please review again. Thanks.", "Any update for me?  @qlzh727 "]}, {"number": 22472, "title": "Why my tensorflow-gpu runs only on cpu?", "body": "OS Platform and Distribution: windows10\r\n**TensorFlow installed from **: pip install tensorflow-gpu\r\nTensorFlow version: (tensorflow-gpu            1.10.0\uff09\r\nPython version: Python 3.6.5\r\nCUDA/cuDNN version: Cuda compilation tools, release 9.0/cudnn-9.0-windows10-x64-v7.3.0.29.zip\r\n\r\nwhen I run nvidia-smi and nvcc-V,\r\nThe output is following:\r\n(python36) C:\\Windows\\System32>nvidia-smi\r\nSun Sep 23 20:26:38 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 398.82                 Driver Version: 398.82                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080   WDDM  | 00000000:01:00.0  On |                  N/A |\r\n| 42%   30C    P8    11W / 200W |    488MiB /  8192MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1176    C+G   Insufficient Permissions                   N/A      |\r\n|    0      2388    C+G   ...oftEdge_8wekyb3d8bbwe\\MicrosoftEdge.exe N/A      |\r\n|    0      6712    C+G   ...tEdge_8wekyb3d8bbwe\\MicrosoftEdgeCP.exe N/A      |\r\n|    0      7036    C+G   ...t_cw5n1h2txyewy\\ShellExperienceHost.exe N/A      |\r\n|    0      7220    C+G   ...dows.Cortana_cw5n1h2txyewy\\SearchUI.exe N/A      |\r\n|    0      7656    C+G   ...tEdge_8wekyb3d8bbwe\\MicrosoftEdgeCP.exe N/A      |\r\n|    0      8260    C+G   ...hell.Experiences.TextInput.InputApp.exe N/A      |\r\n|    0      9852    C+G   ...tEdge_8wekyb3d8bbwe\\MicrosoftEdgeCP.exe N/A      |\r\n|    0     10308    C+G   ...tEdge_8wekyb3d8bbwe\\MicrosoftEdgeCP.exe N/A      |\r\n|    0     13544    C+G   ...Chrome\\Chrome\\Application\\360chrome.exe N/A      |\r\n|    0     16656    C+G   ...es (x86)\\Internet Explorer\\iexplore.exe N/A      |\r\n+-----------------------------------------------------------------------------+\r\n\r\n(python36) C:\\Windows\\System32>nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n\r\nWhen I use import tensorflow as tf,it's work well \r\nThen I tried to use this code:\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nThe output is following:\r\nDevice mapping: no known devices.\r\n2018-09-23 20:12:06.377265: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\direct_session.cc:288] Device mapping:\r\n\r\nthere is nothing show in the map.Why is this happening? How can I fix it?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nGPU model and memory\nExact command to reproduce\nMobile device", "Thank you for your answer.I'll fill up it at following:\r\n> Have I written custom code:No,I haven't written custom code.\r\n> Bazel version:No,i'm not compiling the source code .\r\n> GPU model and memory:GeForce GTX 1080 WDDM.Memory is 8192MiB \r\n> Exact command to reproduce:My command  Step is as following\r\n>>activate python3.6\uff08a python3.6 enviroment I created on anaconda  \uff09\r\n(python36) C:\\Windows\\System32>python\r\nPython 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n2018-09-24 10:51:55.917673: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nDevice mapping: no known devices.\r\n2018-09-24 10:51:55.926033: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\direct_session.cc:288] Device mapping:\r\n(This Step I can't find my GPU)\r\n> Mobile device:No,It's not works on mobile device\r\n\r\n", "Could you copy-paste the full output of this python command:\r\n`tf.test.is_gpu_available()`", "> Could you copy-paste the full output of this python command:\r\n> `tf.test.is_gpu_available()`\r\n\r\nYes\uff0cI have run the command ,the result is as following\r\n>>> tf.test.is_gpu_available()\r\n2018-09-24 14:39:47.660250: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nFalse\r\n", "I am quite certain that the output you shared comes from non-gpu version of TF.\r\nAre you sure you installed GPU version of TF?", "> I am quite certain that the output you shared comes from non-gpu version of TF.\r\n> Are you sure you installed GPU version of TF?\r\n\r\nyes ,I can found it in my conda list .it's  shown as following \r\n(python36) C:\\Windows\\system32>conda list\r\n# packages in environment at D:\\software\\anaconda\\envs\\tensorflow-gpu:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_tflow_1100_select        0.0.1                       gpu\r\n_tflow_190_select         0.0.1                       gpu\r\nabsl-py                   0.5.0                     <pip>\r\nabsl-py                   0.4.1                    py36_0\r\nastor                     0.7.1                    py36_0\r\nblas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\ncertifi                   2018.8.24                py36_1\r\ncudatoolkit               9.0                           1\r\ncudnn                     7.1.4                 cuda9.0_0\r\ngast                      0.2.0                    py36_0\r\ngrpcio                    1.12.1           py36h1a1b453_0\r\nh5py                      2.7.0               np113py36_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nhdf5                      1.8.15.1                 vc14_4    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nicc_rt                    2017.0.4             h97af966_0\r\nkeras-applications        1.0.4                    py36_1\r\nkeras-base                2.2.2                    py36_0\r\nkeras-gpu                 2.2.2                         0\r\nkeras-preprocessing       1.0.2                    py36_1\r\nlibprotobuf               3.6.0                h1a1b453_0\r\nmarkdown                  2.6.9                    py36_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nMarkdown                  3.0                       <pip>\r\nmkl                       2017.0.3                      0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nnumpy                     1.13.3           py36hb69e940_3\r\nnumpy                     1.14.5                    <pip>\r\npip                       18.0                      <pip>\r\npip                       9.0.1                    py36_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nprotobuf                  3.6.1                     <pip>\r\nprotobuf                  3.6.0            py36he025d50_0\r\npython                    3.6.5                h0c2934d_0\r\npyyaml                    3.12                     py36_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nscipy                     0.19.1              np113py36_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nsetuptools                40.2.0                   py36_0\r\nsetuptools                39.1.0                    <pip>\r\nsix                       1.11.0                   py36_1\r\nsix                       1.11.0                    <pip>\r\ntensorboard               1.10.0           py36he025d50_0\r\ntensorflow                1.10.0          gpu_py36h3514669_0\r\ntensorflow-base           1.10.0          gpu_py36h6e53903_0\r\ntensorflow-gpu            1.10.0               hf154084_0\r\ntermcolor                 1.1.0                    py36_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nvc                        14                            0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nvs2015_runtime            14.15.26706          h3a45250_0\r\nWerkzeug                  0.14.1                    <pip>\r\nwerkzeug                  0.12.2                   py36_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nwheel                     0.31.1                    <pip>\r\nwheel                     0.29.0                   py36_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nwincertstore              0.2                      py36_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\nzlib                      1.2.11                   vc14_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free", "We (tensorflow team) do not maintain the packages available through\nanaconda. Please reach out to anaconda with issues with those packages.\n\nOn Mon, Sep 24, 2018, 12:17 AM li <notifications@github.com> wrote:\n\n> I am quite certain that the output you shared comes from non-gpu version\n> of TF.\n> Are you sure you installed GPU version of TF?\n>\n> yes ,I can found it in my conda list .it's shown as following\n> (python36) C:\\Windows\\system32>conda list\n> packages in environment at D:\\software\\anaconda\\envs\\tensorflow-gpu: Name\n> Version Build Channel\n>\n> _tflow_1100_select 0.0.1 gpu\n> _tflow_190_select 0.0.1 gpu\n> absl-py 0.5.0\n> absl-py 0.4.1 py36_0\n> astor 0.7.1 py36_0\n> blas 1.0 mkl https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> certifi 2018.8.24 py36_1\n> cudatoolkit 9.0 1\n> cudnn 7.1.4 cuda9.0_0\n> gast 0.2.0 py36_0\n> grpcio 1.12.1 py36h1a1b453_0\n> h5py 2.7.0 np113py36_0\n> https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> hdf5 1.8.15.1 vc14_4\n> https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> icc_rt 2017.0.4 h97af966_0\n> keras-applications 1.0.4 py36_1\n> keras-base 2.2.2 py36_0\n> keras-gpu 2.2.2 0\n> keras-preprocessing 1.0.2 py36_1\n> libprotobuf 3.6.0 h1a1b453_0\n> markdown 2.6.9 py36_0\n> https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> Markdown 3.0\n> mkl 2017.0.3 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> numpy 1.13.3 py36hb69e940_3\n> numpy 1.14.5\n> pip 18.0\n> pip 9.0.1 py36_1 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> protobuf 3.6.1\n> protobuf 3.6.0 py36he025d50_0\n> python 3.6.5 h0c2934d_0\n> pyyaml 3.12 py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> scipy 0.19.1 np113py36_0\n> https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> setuptools 40.2.0 py36_0\n> setuptools 39.1.0\n> six 1.11.0 py36_1\n> six 1.11.0\n> tensorboard 1.10.0 py36he025d50_0\n> tensorflow 1.10.0 gpu_py36h3514669_0\n> tensorflow-base 1.10.0 gpu_py36h6e53903_0\n> tensorflow-gpu 1.10.0 hf154084_0\n> termcolor 1.1.0 py36_0\n> https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> vc 14 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> vs2015_runtime 14.15.26706 h3a45250_0\n> Werkzeug 0.14.1\n> werkzeug 0.12.2 py36_0\n> https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> wheel 0.31.1\n> wheel 0.29.0 py36_0\n> https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> wincertstore 0.2 py36_0\n> https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n> zlib 1.2.11 vc14_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22472#issuecomment-423891768>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOa5S98f9xchj1WVaynJKTwJH7tJMks5ueIcGgaJpZM4W1rwb>\n> .\n>\n", "OK,I found there is a tensorflow1.9.0 in my packages ,when I uninstall It, Everything works well. THX  for your answer. I'm appreciate of that .and I will close this issue", "@noticeable how do I check if a previous version is present in my packages or not?", "> @noticeable how do I check if a previous version is present in my packages or not?\r\ni'm using conda  ,in conda ,you can use conda list to see your packages installed\r\n", "> @noticeable how do I check if a previous version is present in my packages or not?\r\n\r\nto solved this problem,you may use pip uninstall tensorflow,if there is a chonse shown. uninstall previous packages may be OK.", "I have installed tensorflow v2.0.0 and tensorflow-gpu v2.0.0 both and keras also, but my tensorflow is still using CPU instead of GPU I don't know how to fix it can somebody help me out", "I'm having a similar issue. The GPU seems to run when running predictions but not when training the model instead it uses CPU.\r\n\r\nA method I tried is this should insure it is running on GPU but the time does not change nor do I see an increase of my GPUs activity as I do with the predictions\r\n\r\n`with tf.device('/device:GPU:0'):\r\n    %time model1.fit(X_train, y_train, batch_size=512, epochs=2, validation_data=(X_test, y_test), verbose = 1)`", "May be the size of your predictions is small that's why you're thinking that in predictions, your system is using GPU. Make sure that you have installed tensorflow-gpu. Also if you've installed keras then sometimes system automatically uses Theano backend for the solution you can set backend from the prompt by simply writing the following code\r\n    set \"KERAS_BACKEND=tensorflow\" \r\n"]}, {"number": 22471, "title": "How to use runStats() and view more detail of debug information in the android image classify demo?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nandroid and Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nXiaomi Max2\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nthe latest\r\n- **Python version**:\r\n3.5\r\n- **Bazel version**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nI am doing a research of tensorflow on mobile devices and I get started in the android image classify demo.\r\nI want to fetch the detail debug information while running pre-trianed model on android devices, and I notice the debug indormation can be fetched through pressing volume button while running the demo as the following:\r\n![20180923190653](https://user-images.githubusercontent.com/14329360/45927296-e46fcd80-bf63-11e8-922c-9c6d2d9695a1.jpg)\r\n\r\nHowever I can not find where to fetch the original debug data listed in the figure. Can someone tell me where to fetch these debug datas? Also the meaning of some of the datas are vague, for example [start], [first], what does these parameters mean? It is important of my research and hope someone could explain these, thx!\r\n\r\n\r\n### Source code / logs\r\n  private void renderDebug(final Canvas canvas) {\r\n    if (!isDebug()) {\r\n      return;\r\n    }\r\n    final Bitmap copy = cropCopyBitmap;\r\n    if (copy != null) {\r\n      final Matrix matrix = new Matrix();\r\n      final float scaleFactor = 2;\r\n      matrix.postScale(scaleFactor, scaleFactor);\r\n      matrix.postTranslate(\r\n          canvas.getWidth() - copy.getWidth() * scaleFactor,\r\n          canvas.getHeight() - copy.getHeight() * scaleFactor);\r\n      canvas.drawBitmap(copy, matrix, new Paint());\r\n\r\n      final Vector<String> lines = new Vector<String>();\r\n      if (classifier != null) {\r\n        String statString = classifier.getStatString();// Here is where the core debug information come from, \r\n                                                                             //but I can not get more information!\r\n        String[] statLines = statString.split(\"\\n\");\r\n        for (String line : statLines) {\r\n          lines.add(line);\r\n        }\r\n      }\r\n\r\n      lines.add(\"Frame: \" + previewWidth + \"x\" + previewHeight);\r\n      lines.add(\"Crop: \" + copy.getWidth() + \"x\" + copy.getHeight());\r\n      lines.add(\"View: \" + canvas.getWidth() + \"x\" + canvas.getHeight());\r\n      lines.add(\"Rotation: \" + sensorOrientation);\r\n      lines.add(\"Inference time: \" + lastProcessingTimeMs + \"ms\");\r\n\r\n      borderedText.drawLines(canvas, 10, canvas.getHeight() - 10, lines);\r\n    }\r\n  }\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "> Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\n> Bazel version\r\n> CUDA/cuDNN version\r\n> GPU model and memory\r\n> Exact command to reproduce\r\n\r\nOK\uff0cI have modified it.", "@jamesdeep  You can set up debugging on your device following the steps [here](https://developer.android.com/training/basics/firstapp/running-app#RealDevice). The on-screen data are for model profiling where you can find the execution time of each operator for benchmarking purposes. Count is the total number of nodes executed, Start is the time stamp for each node and First is the duration.", "Closing this for now, feel free to open a new issue for any additional questions.\r\n"]}, {"number": 22470, "title": "[keras compatibility] tf.keras.regularizers.l2() cannot be used in tf.get_variable()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom code\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6.4\r\n- OS Platform and Distribution: N/A\r\n- Bazel version: N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- Mobile device: N/A\r\n- Exact command to reproduce: Source code below.\r\n\r\n### Describe the problem\r\n`tf.keras.regularizers.l2()` cannot be used in `tf.get_variable()`. Currently I have to use `tf.contrib.layers.l2_regularizer()` in `tf.get_variable()`.  \r\nNote that , `tf.keras.regularizers.l2()` seems to work well in `tf.layers.Dense()`.\r\n\r\nIn general, I expect that all `keras` functions like `regularizers`, `activations`, `initializers` could be used for `tensorflow`.\r\n\r\n### Source code / logs\r\n```\r\nx = tf.get_variable(shape = [dim_in, dim_out],\r\n    initializer=tf.keras.initializers.glorot_uniform(),\r\n    regularizer=tf.keras.regularizers.l2(lambda))\r\n```\r\nTraceback:\r\n>   File \"/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1467, in get_variable\r\n>     aggregation=aggregation)\r\n>   File \"/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1217, in get_variable\r\n>     aggregation=aggregation)\r\n>   File \"/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 527, in get_variable\r\n>     aggregation=aggregation)\r\n>   File \"/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 481, in _true_getter\r\n>     aggregation=aggregation)\r\n>   File \"/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 930, in _get_single_variable\r\n>     loss_name = loss.name\r\n> AttributeError: 'float' object has no attribute 'name'\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Obligated detail updated.", "@tranhungnghiep It looks like the issue only happens when tf.keras.regularizers.l2(l=0.) is used. In that case `regularization = 0.` is returned directly. Otherwise it should be fine:\r\nhttps://github.com/tensorflow/tensorflow/blob/609b2ce3fe8ebecf4031670b8c2186468369b0ba/tensorflow/python/keras/regularizers.py#L56-L62\r\n\r\nCreated a PR #22478 for the fix.", "@yongtang Yes that should fix it. Thanks.", "@yongtang Thanks for the PR!"]}, {"number": 22469, "title": "fix doc bug for per_image_standardization (unit norm vs. variance)", "body": "The first line of the doc states \"Linearly scales `image` to have zero mean and **unit norm.**\"  This is incorrect - \"unit norm\" is calculated by dividing the vector by its L2 norm, and by definition, when you take the L2 norm of the result, it should evaluate to 1.  The result being calculated here is referred to as \"unit variance\".  Reference: https://en.wikipedia.org/wiki/Feature_scaling\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed an individual CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "I signed it\r\n\r\nFrom: googlebot <notifications@github.com>\r\nSent: Saturday, September 22, 2018 7:14 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Roland Fernandez <rfernand@microsoft.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] fix doc bug for per_image_standardization (unit norm vs. variance) (#22469)\r\n\r\n\r\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n\r\n\ud83d\udcdd Please visit https://cla.developers.google.com/<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcla.developers.google.com%2F&data=02%7C01%7Crfernand%40microsoft.com%7C2d4208e751994b011fb108d620fa2ce0%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636732656201260795&sdata=jBj%2Bqsz0yE3XA5n%2F%2FB2f9H2RcSex1TMRlDiRfxYWA00%3D&reserved=0> to sign.\r\n\r\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\r\n\r\n________________________________\r\nWhat to do if you already signed the CLA\r\nIndividual signers\r\n\r\n  *   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcla.developers.google.com%2Fclas&data=02%7C01%7Crfernand%40microsoft.com%7C2d4208e751994b011fb108d620fa2ce0%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636732656201260795&sdata=%2FRR0Mcd3fid21TgA8gQo2Oi4dQP6HZy%2Bg13paz7hSfk%3D&reserved=0> and verify that your email is set on your git commits<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fhelp.github.com%2Farticles%2Fsetting-your-email-in-git%2F&data=02%7C01%7Crfernand%40microsoft.com%7C2d4208e751994b011fb108d620fa2ce0%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636732656201260795&sdata=oTNWFETojMXQp1B0aTec1U%2BuZxe3e7%2FEsMal%2BSOy2ss%3D&reserved=0>.\r\n\r\nCorporate signers\r\n\r\n  *   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot<http://go/cla#troubleshoot> (Public version<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fopensource.google.com%2Fdocs%2Fcla%2F%23troubleshoot&data=02%7C01%7Crfernand%40microsoft.com%7C2d4208e751994b011fb108d620fa2ce0%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636732656201260795&sdata=hEjTumljm4JG90d3hysDaUtjG0BlRWDPNYtjhEor4II%3D&reserved=0>).\r\n  *   The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcla.developers.google.com%2Fclas&data=02%7C01%7Crfernand%40microsoft.com%7C2d4208e751994b011fb108d620fa2ce0%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636732656201260795&sdata=%2FRR0Mcd3fid21TgA8gQo2Oi4dQP6HZy%2Bg13paz7hSfk%3D&reserved=0> and verify that your email is set on your git commits<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fhelp.github.com%2Farticles%2Fsetting-your-email-in-git%2F&data=02%7C01%7Crfernand%40microsoft.com%7C2d4208e751994b011fb108d620fa2ce0%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636732656201260795&sdata=oTNWFETojMXQp1B0aTec1U%2BuZxe3e7%2FEsMal%2BSOy2ss%3D&reserved=0>.\r\n  *   The email used to register you as an authorized contributor must also be attached to your GitHub account<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fsettings%2Femails&data=02%7C01%7Crfernand%40microsoft.com%7C2d4208e751994b011fb108d620fa2ce0%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636732656201260795&sdata=5QUxdFNK22glAdzvw9LpsgRGYTP3JaTuaF%2F4oc3ZD4Q%3D&reserved=0>.\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F22469%23issuecomment-423786277&data=02%7C01%7Crfernand%40microsoft.com%7C2d4208e751994b011fb108d620fa2ce0%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636732656201260795&sdata=8bxpPtgrfx7jyp5eUDokom%2FBQLZqTP3pHFZZUA6NALI%3D&reserved=0>, or mute the thread<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAEGN3oDzYx5JtKxBGIHtDl9SCVqFPaTjks5udu5SgaJpZM4W1hcF&data=02%7C01%7Crfernand%40microsoft.com%7C2d4208e751994b011fb108d620fa2ce0%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636732656201260795&sdata=BztzMbz1n6pU0jTZQ8PM8PiFyg71vj6513IFLp8TuYw%3D&reserved=0>.\r\n"]}, {"number": 22467, "title": "Expose TFOptimizer in tf.keras.optimizers", "body": "This fix tries to address the issue in #22466 where\r\nTFOptimizer was not exposed in tf.keras.optimizers\r\nlike other optimizers.\r\n\r\nThis fix fixes #22466.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Please fix the API compatibility test by generate the pbtxt for the new API.", "Thanks @qlzh727. The PR has been updated with api compatibility tests passed. Please take a look.", "@fchollet, please take a look.", "This class is not meant to be public. The issue linked is unrelated. That issue will be fixed separately with API changes at the level of TF optimizers (in progress)."]}, {"number": 22466, "title": "Unable to use TFOptimizer from keras in tensorflow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: NA\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: Running in colab, chrome\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 2\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\nUsing TFOptimizer from keras inside tensorflow gives error - \r\n```\r\n---> 22 opt = keras.optimizers.TFOptimizer(tf_opt)\r\n     23 opt.lr = lr\r\n     24 \r\n\r\nAttributeError: 'module' object has no attribute 'TFOptimizer'\r\n```\r\n\r\nI manually imported it to make the code work-\r\n```\r\nfrom tensorflow.python.keras.optimizers import TFOptimizer\r\n```\r\n\r\nCould TFOptimizer class be exported from tensorflow in the following file-\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizers.py#L691\r\n\r\n### Source code / logs\r\n\r\n", "comments": ["Added the PR #22467 for the fix.", "@yongtang Thanks for the PR. Looks like this issue will be fixed separately with API changes at the level of TF optimizers which is in progress.\r\n\r\n", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as this will be fixed in API changes.", "Hi, I am facing this error now, could anyone please help me out on how to solve it? Thanks.\r\n\r\n**Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** No\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** NA\r\n**Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:** NA\r\n**TensorFlow installed from (source or binary):** Source\r\n**TensorFlow version (use command below):** 1.14.0\r\n**Python version:** 3.7\r\n**Bazel version (if compiling from source):** NA\r\n**GCC/Compiler version (if compiling from source):** NA\r\n**CUDA/cuDNN version:** 10.1/7.64\r\n**GPU model and memory:** GTX1650 4GB\r\n**Exact command to reproduce:** NA\r\n\r\nError:\r\n`Epoch 00001: loss improved from inf to 2.88872, saving model to weights\\weights-improvement-01-2.8887.hdf5\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-22-50033a1e00fb>\", line 16, in <module>\r\n    history = model.fit(X, y, epochs=10, batch_size=128, callbacks=callbacks_list)\r\n\r\n  File \"C:\\Users\\nrgra\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1239, in fit\r\n    validation_freq=validation_freq)\r\n\r\n  File \"C:\\Users\\nrgra\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\", line 216, in fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n\r\n  File \"C:\\Users\\nrgra\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py\", line 152, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n\r\n  File \"C:\\Users\\nrgra\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py\", line 719, in on_epoch_end\r\n    self.model.save(filepath, overwrite=True)\r\n\r\n  File \"C:\\Users\\nrgra\\anaconda3\\lib\\site-packages\\keras\\engine\\network.py\", line 1152, in save\r\n    save_model(self, filepath, overwrite, include_optimizer)\r\n\r\n  File \"C:\\Users\\nrgra\\anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\", line 449, in save_wrapper\r\n    save_function(obj, filepath, overwrite, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\nrgra\\anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\", line 541, in save_model\r\n    _serialize_model(model, h5dict, include_optimizer)\r\n\r\n  File \"C:\\Users\\nrgra\\anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\", line 163, in _serialize_model\r\n    if isinstance(model.optimizer, optimizers.TFOptimizer):\r\n\r\nAttributeError: module 'tensorflow.keras.optimizers' has no attribute 'TFOptimizer'`", "I am getting the same error too with tf 2.2.0 and kears 2.4.3.\r\nAny help?", "@nrgopalrao @Mahmood-Hoseini  Hi ! I have the same problem than you. Did you find a solution please ?", "@nrgopalrao I did not. It seems like a incompatibility of tf and keras versions. I moved to pytorch.", "Try using this:\r\nInstead of (example)\r\n`from keras.models import Sequential`\r\nuse\r\n`from tensorflow.python.keras.models import Sequential`\r\nThis worked for me.", "Try to replace all the keras to tensorflow.keras"]}, {"number": 22463, "title": "Tensorflow mystique bug with MirroredStrategy freeze on particular local GPU combo", "body": "https://stackoverflow.com/questions/52425485/tensorflow-mirroredstrategy-freeze-on-particular-gpu-combo\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 host Linux Ubuntu 16.04 inside Docker container\r\n- **TensorFlow installed from (source or binary)**: source and binary\r\n- **TensorFlow version (use command below)**: 1.10.1 - 1.11.rc1\r\n- **Python version**: 3.5 - 3.6\r\n- **Bazel version (if compiling from source)**: 0.17.1\r\n- **GCC/Compiler version (if compiling from source)**: 5\r\n- **CUDA/cuDNN version**: 9.0, 9,2/7.2\r\n- **GPU model and memory**: \r\n    (0) Nvidia Titan x 980 12gb\r\n    (1) Nvidia Titan x 980 12gb (same)\r\n    (2) Nvidia 1080ti 11gb\r\n    (3) Nvidia 1080ti 11gb (same)\r\n- **Exact command to reproduce**: example source below\r\n\r\n\r\n### Describe the problem\r\nFour GPUs on the same desktop:\r\n\r\n(0) Nvidia Titan x 980 12gb\r\n(1) Nvidia Titan x 980 12gb (same)\r\n(2) Nvidia 1080ti 11gb\r\n(3) Nvidia 1080ti 11gb (same)\r\n\r\nMirroredStrategy (say from the obvious example code below) goes ok in combinations of the above (1,2), (1,3) but freezes on (2,3), (1,2,3). \r\n(0 - not used at distribution as causes another sort of CUDA_WAIT_TIMEOUT error due to guessed kernel timeouts for being used to render display)\r\n\r\nHave no idea where to go further. Checked with original binary docker images both latest and nightly. Checked with compiled from source TensorFlow r1.10-r1.11 within original nvidia/cuda binary docker image. All the same behaviour.\r\n\r\n\r\n### Source code / logs\r\n```\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\r\nos.environ['CUDA_VISIBLE_DEVICES']=\"2,3\"\r\n\r\nimport tensorflow as tf \r\n\r\ndef model_fn(features, labels, mode):\r\n  layer = tf.layers.Dense(1)\r\n  logits = layer(features)\r\n\r\n  if mode == tf.estimator.ModeKeys.PREDICT:\r\n    predictions = {\"logits\": logits}\r\n    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n  loss = tf.losses.mean_squared_error(\r\n               labels=labels, predictions=tf.reshape(logits, []))\r\n\r\n  if mode == tf.estimator.ModeKeys.EVAL:\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\ndef input_fn():\r\n  features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)\r\n  labels = tf.data.Dataset.from_tensors(1.).repeat(100)\r\n  return tf.data.Dataset.zip((features, labels))\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\nclassifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\n\r\nclassifier.train(input_fn=input_fn)\r\n\r\nclassifier.evaluate(input_fn=input_fn)\r\n```\r\n\r\n[freeze_output_r1.10.txt](https://github.com/tensorflow/tensorflow/files/2407611/freeze_output_r1.10.txt)\r\n[freeze_output_r1.11.txt](https://github.com/tensorflow/tensorflow/files/2407612/freeze_output_r1.11.txt)\r\n[normal_completion_output_r1.10.1.txt](https://github.com/tensorflow/tensorflow/files/2407613/normal_completion_output_r1.10.1.txt)\r\n[normal_completion_output_r1.11.txt](https://github.com/tensorflow/tensorflow/files/2407614/normal_completion_output_r1.11.txt)\r\n[tf_env_r1.10.1.txt](https://github.com/tensorflow/tensorflow/files/2407615/tf_env_r1.10.1.txt)\r\n[tf_env_r1.11.txt](https://github.com/tensorflow/tensorflow/files/2407616/tf_env_r1.11.txt)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "There was mentioned a lot [...:](https://devtalk.nvidia.com/default/topic/994904/linux/peer-to-peer-dma-transfers-bug-under-intel-vt-d-iommu-virtualization-/), [...:](https://bbs.archlinux.org/viewtopic.php?id=230362), [...:](https://stackoverflow.com/questions/42767187/why-does-this-tensorflow-code-crash)\r\n ASUS plx chip and bios related issue.\r\nKernel parameter pci=nommconf has fixed everything.\r\niommu off - should as well but as a last measure."]}, {"number": 22462, "title": "Different behaviors when using  relu activation inside conv2d layer and a standalone ReLU() layer", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3,6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: GTX 1080ti 11gb\r\n- **Exact command to reproduce**: run the code on mnist dataset\r\n\r\n```\r\nfrom tensorflow.python.keras.layers import Conv2D, ReLU, BatchNormalization, Dense, Input, Conv2DTranspose, UpSampling2D, Flatten, Reshape\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.utils import multi_gpu_model\r\nfrom tensorflow.python.keras.optimizers import Nadam\r\nfrom tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow.python.keras.datasets import mnist\r\n\r\n\r\nimg_height, img_width = 28, 28\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nX = np.expand_dims(x_train, axis=-1)\r\nprint(X.shape)\r\n\r\ninput_image = Input(shape=(img_height, img_width, 1), name='image_imput')\r\nx = Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='encoder_conv1', activation='relu')(input_image)\r\nx = Conv2D(filters=64, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='encoder_conv2', activation='relu')(x)\r\nx = Conv2D(filters=128, kernel_size=(3, 3), strides=(2, 2), padding='valid', name ='encoder_conv3', activation='relu')(x)\r\nx = Flatten()(x)\r\nencoded = Dense(units=10)(x)\r\n\r\ny = Dense(units=1152, activation='relu')(encoded)\r\ny = Reshape((3, 3, 128))(y)\r\ny = Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), padding='valid', name ='decoder_deconv1', activation='relu')(y)\r\ny = Conv2DTranspose(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='decoder_deconv2', activation='relu')(y)\r\ndecoded_image = Conv2DTranspose(filters=1, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='decoder_deconv3', activation='relu')(y)\r\n\r\n\r\nCAE = Model(inputs = input_image, outputs = decoded_image, name = 'CAE')\r\n\r\n\r\ntb = TensorBoard(log_dir='logs', write_graph=True)\r\nmc = ModelCheckpoint(filepath='models/top_weights.h5', monitor='acc', save_best_only='True', save_weights_only='True', verbose=1)\r\nes = EarlyStopping(monitor='loss', patience=15, verbose=1)\r\nrlr = ReduceLROnPlateau(monitor='loss')\r\ncallbacks = [tb, mc, es, rlr]\r\nCAE.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\r\n\r\n\r\n# CAE.load_weights('models/top_weights.h5')\r\n# CAE.save('CAE.h5')\r\nCAE.fit(X, X, epochs=1000, batch_size=256, callbacks=callbacks)\r\n ```\r\n\r\nin the above code if i do something like this that is  use a standalone activation layer, the model behaves differently \r\n```\r\nx = Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='encoder_conv1')(input_image)\r\nx = ReLU()(x)\r\n```\r\nIf i include the activation in the conv2d layer, the model converges at 80% accuracy and when i use a standalone activation layer the model is stuck at 11% accuracy.\r\n\r\nI want to know the reason for different behavior ", "comments": ["@srihari-humbarwadi If you don't specify activation argument in the Conv2D layer it will use linear activation by default. Are you specifying separate activation after each convolution layer or just after the first one? ", "I tried adding a separate activation layer after each conv2d layer. And isn't the activation=None by default?", "Okay. Yes it is none by default but according to [keras documentation](https://keras.io/layers/convolutional/) linear activation is applied in the case of absence of activation argument.\r\n> activation: Activation function to use (see activations). If you don't specify anything, no activation is applied ie. \"linear\" activation:", "Yea but a(x) = x  is just an identity mapping shouldn't it do nothing in this case?\n```\nactivation: Activation function to use (see\u00a0activations). If you don't specify anything, no activation is applied (ie. \"linear\" activation:\u00a0a(x) = x).\n```", "To confirm this can you try running your model like this : \r\nx = Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same', name ='encoder_conv1')(input_image)  # run with no activation layer\r\n Remove the  x = ReLU()(x)  layer after conv layer\r\nAfter this can you compare the accuracy and share your findings?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22461, "title": "Disable the logging for OutofRangeError in test", "body": "`tensorflow.python.framework.test_util.ErrorLoggingSession` logs the errors from the tests. However, `OutOfRangeError` is used in many `tf.data` Ops as the signal completion,  which makes the ouput of `tf.data` tests hard to read. This PR disables the loging for `OutOfRnageError` to make the test log easy to read.", "comments": ["@mrry Could you help review this PR when you have time?"]}]