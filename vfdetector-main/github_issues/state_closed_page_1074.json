[{"number": 21055, "title": "Can not install tensorflow", "body": "Hello, \r\n\r\nI am new to python and tensorflow. I have downloaded PyCharm student version and working in virtual env with python 3.5\r\n\r\nI am trying to install tensorflow gpu. I am getting following error:\r\n\"Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '/home/smst/PycharmProjects/Project_Trials/venv/lib/python3.5/site-packages/tfp_nightly-0.0.1.dev20180629.dist-info/METADATA'\"\r\n\r\nnot just tensorflow. for that matter I not able to install any package.tfp nightly is neither getting installed nor uninstalled. I am stuck. \r\n\r\nGPU : NVIDIA TITAN X\r\n\r\ncat /usr/local/cuda-9.2/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 1\r\n#define CUDNN_PATCHLEVEL 4\r\n\r\nThe above shows where cuda is installed and cudnn version.\r\n\r\nPlease help \r\n\r\n\r\n", "comments": ["and this too...\r\n\r\nnvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Wed_Apr_11_23:16:29_CDT_2018\r\nCuda compilation tools, release 9.2, V9.2.88\r\n\r\n", "What command are you using to install tensorflow? Is tensorflow regular (without GPU) working for you?\r\n\r\nAlso StackOverflow might have a better answer for your question.", "pip install  tensorflow-gpu \r\nalso tried pip3 install  tensorflow-gpu\r\n\r\nwithout gpu used to work, even it stopped working now", "The brute force solution would be to try and create a new virtualenv and try simple tensorflow first. If you can send me the output for that I can try and help more. If that works try gpu again in another environment.", "@av8ramit thanks for stepping up!", "It has been 17 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 21054, "title": "gru_ops.so not found", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo using code provided on the website. Here it is the link to it.\r\nhttps://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners\r\n\r\n\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n3.5.5\r\n- **GCC/Compiler version (if compiling from source)**:\r\nUsing Anaconda and Jupyter Notebook\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0\r\ncuDNN 7.0\r\n- **GPU model and memory**:\r\nNvidia GEFORCE 940M\r\n- **Exact command to reproduce**:\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**### Describe the problem**\r\nI am trying to run MNIST (TensorFlow) code basic hand-drawn digit recognition on the Jupyter Notebook. When I run the first cell of it I just give me an error. In the first cell, I actually try to import MNISTR data file this is the error I am getting.\r\n\r\n**This is the code:**\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n\r\n**Error:**\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-15-8bf8ae5a5303> in <module>()\r\n----> 1 from tensorflow.examples.tutorials.mnist import input_data\r\n      2 mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\__init__.py in <module>()\r\n     19 from __future__ import print_function\r\n     20 \r\n---> 21 from tensorflow.examples.tutorials.mnist import input_data\r\n     22 from tensorflow.examples.tutorials.mnist import mnist\r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py in <module>()\r\n     28 from six.moves import xrange  # pylint: disable=redefined-builtin\r\n     29 import tensorflow as tf\r\n---> 30 from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\r\n     31 # pylint: enable=unused-import\r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\contrib\\__init__.py in <module>()\r\n     31 from tensorflow.contrib import copy_graph\r\n     32 from tensorflow.contrib import crf\r\n---> 33 from tensorflow.contrib import cudnn_rnn\r\n     34 from tensorflow.contrib import data\r\n     35 from tensorflow.contrib import deprecated\r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\__init__.py in <module>()\r\n     32 \r\n     33 # pylint: disable=unused-import,wildcard-import\r\n---> 34 from tensorflow.contrib.cudnn_rnn.python.layers import *\r\n     35 # pylint: enable=unused-import,wildcard-import\r\n     36 \r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\layers\\__init__.py in <module>()\r\n     21 \r\n     22 # pylint: disable=unused-import,wildcard-import\r\n---> 23 from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import *\r\n     24 # pylint: enable=unused-import,wildcard-import\r\n     25 \r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\layers\\cudnn_rnn.py in <module>()\r\n     18 from __future__ import print_function\r\n     19 \r\n---> 20 from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n     21 from tensorflow.python.framework import dtypes\r\n     22 from tensorflow.python.framework import ops\r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py in <module>()\r\n     19 \r\n     20 from tensorflow.contrib.eager.python import checkpointable_utils\r\n---> 21 from tensorflow.contrib.rnn.python.ops import lstm_ops\r\n     22 from tensorflow.python.framework import common_shapes\r\n     23 from tensorflow.python.framework import dtypes\r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\contrib\\rnn\\__init__.py in <module>()\r\n     86 \r\n     87 from tensorflow.contrib.rnn.python.ops.fused_rnn_cell import *\r\n---> 88 from tensorflow.contrib.rnn.python.ops.gru_ops import *\r\n     89 from tensorflow.contrib.rnn.python.ops.lstm_ops import *\r\n     90 from tensorflow.contrib.rnn.python.ops.rnn import *\r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\gru_ops.py in <module>()\r\n     31 \r\n     32 _gru_ops_so = loader.load_op_library(\r\n---> 33     resource_loader.get_path_to_datafile(\"_gru_ops.so\"))\r\n     34 \r\n     35 LayerRNNCell = rnn_cell_impl.LayerRNNCell  # pylint: disable=invalid-name\r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\contrib\\util\\loader.py in load_op_library(path)\r\n     54       return None\r\n     55   path = resource_loader.get_path_to_datafile(path)\r\n---> 56   ret = load_library.load_op_library(path)\r\n     57   assert ret, 'Could not load %s' % path\r\n     58   return ret\r\n\r\n~\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py in load_op_library(library_filename)\r\n     54     RuntimeError: when unable to load the library or get the python wrappers.\r\n     55   \"\"\"\r\n---> 56   lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n     57 \r\n     58   op_list_str = py_tf.TF_GetOpList(lib_handle)\r\n\r\nNotFoundError: C:\\Users\\Abdul\\Anaconda3\\envs\\codcus\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\_gru_ops.so not found\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "Bazel version\r\nMobile device\r\nboth are N/A", "Still waiting for your response", "@michaelisard  @tensorflowbutler Waiting to hear back from you", "You seem to be using the 1.0 (very old) version of the tutorial. You haven't filled in the python version you're using, so I can stay anything definitive. \r\n\r\nI know that user op libraries (eg those in contrib) do not work on Windows. That may be the issue. I'd also suggest using a more recent version of TensorFlow (assuming you are indeed using 1.0 as suggested by the tutorial link). ", "![tensorflow](https://user-images.githubusercontent.com/40676809/43325209-0991a8ec-91ae-11e8-8b30-e62bdbbad935.jpg)\r\n", "@meteorcloudy ", "@abdulwahabqurashi as Martin pointed out contrib ops doesn't work on Windows yet. TF is still built with CMake currently.TensorFlow built with Bazel will have support for most ops under contrib, but the Bazel build is still new, it will be enabled from TensorFlow 1.11. I'm working on a documentation for the Windows Bazel build. Hopefully it will help.", "I will close this issue. Thanks @meteorcloudy!"]}, {"number": 21052, "title": "EXA: Fix eager variable calls in notebook", "body": "This PR aims at fixing a RuntimeError happening twice during notebook execution on Colab with TensorFlow 1.9.0.\r\n\r\n```pytb\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-f257cfd7fa84> in <module>()\r\n----> 1 v = tf.Variable(1.0)\r\n      2 assert v.numpy() == 1.0\r\n      3 \r\n      4 # Re-assign the value\r\n      5 v.assign(3.0)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\r\n    237     if context.executing_eagerly():\r\n    238       raise RuntimeError(\r\n--> 239           \"tf.Variable not supported when eager execution is enabled. \"\r\n    240           \"Please use tf.contrib.eager.Variable instead\")\r\n    241     self._in_graph_mode = True\r\n\r\nRuntimeError: tf.Variable not supported when eager execution is enabled. Please use tf.contrib.eager.Variable instead\r\n```\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@aboucaud Can you sign the CLA please? This is a required step for merging this PR.", "@caisq sorry I did not know about that. I signed my commits with a different address than the one I use with Google :/", "tf.Variable now works on nightly with eager execution enabled, so the change to use tf.Variable was deliberate. Thanks for the contribution!"]}, {"number": 21051, "title": "Doc: ctc_beam_search_decoder", "body": "The [TF documentation ](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder) says: _\"Note The ctc_greedy_decoder is a special case of the ctc_beam_search_decoder with top_paths=1 and beam_width=1 (but that decoder is faster for this special case).\"_\r\n\r\nThis implies that the result of the greedy and beam decoder is equal if beam width is set to 1.\r\n\r\nHowever, this is not the case. Even with beam width set to 1, beam search uses more information by distinguishing paths ending with blank and non-blank.\r\nTherefore, it is possible to create inputs from which beam search can extract enough information to produce the correct result, while best path decoding fails.\r\n\r\nHere is such an input matrix with 3 time-steps. It contains a label '0' and the blank '-'. The best path \"0-0\" is marked by a red line:\r\n![matrix_best](https://user-images.githubusercontent.com/15148095/43075266-908f4740-8e80-11e8-85a5-91e9ffd08015.png)\r\n\r\n \r\n\r\n\r\nAnd here is the code which produces different results, depending on the decoder:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nbatchSize=1\r\nnumClasses=2\r\nnumTimesteps=3\r\n\r\n\r\ndef createGraph():\r\n\t\"create tensors\"\r\n\ttinputs=tf.placeholder(tf.float32, [numTimesteps, batchSize, numClasses])\r\n\ttseqLen=tf.placeholder(tf.int32, [None]) # list of sequence length in batch\r\n\ttbeam=tf.nn.ctc_beam_search_decoder(tinputs, tseqLen, beam_width=1, merge_repeated=False)\r\n\ttbest=tf.nn.ctc_greedy_decoder(tinputs, tseqLen, merge_repeated=True)\r\n\r\n\treturn (tinputs, tseqLen, tbeam, tbest)\r\n\r\n\r\ndef getData():\r\n\t\"get data matrix of size TxBxC with T=3, B=1 and C=2 (one label + blank)\"\r\n\tseqLen=[numTimesteps]\r\n\tinputs=np.asarray([ [[0.51, 0.49]], [[0.49, 0.51]], [[0.51, 0.49]] ], np.float32)\r\n\treturn (inputs, seqLen) \r\n\r\n\r\ndef toLabelString(decoderOutput):\r\n\t\"map sparse tensor from decoder to label string\"\r\n\tdecoded=decoderOutput[0][0]\r\n\tidxDict={b:[] for b in range(batchSize)}\r\n\tencodedLabels=[[] for i in range(batchSize)]\r\n\tfor (idxVal, idx2d) in enumerate(decoded.indices):\r\n\t\tvalue=decoded.values[idxVal]\r\n\t\tbatch=idx2d[0]\r\n\t\tencodedLabels[batch].append(value)\r\n\r\n\treturn encodedLabels[0]\r\n\r\ndef main():\r\n\t# initialize\r\n\t(tinputs, tseqLen, tbeam, tbest)=createGraph()\r\n\tsess=tf.Session()\r\n\tsess.run(tf.global_variables_initializer())\r\n\r\n\t# compute decoded result\r\n\t(inputs, seqLen)=getData()\r\n\t[retBeam, retBest]=sess.run([tbeam, tbest], {tinputs:inputs, tseqLen:seqLen } )\r\n\tprint('Beam Search Decoding :', toLabelString(retBeam))\r\n\tprint('Best Path Decoding   :', toLabelString(retBest))\r\n\r\n\r\nif __name__ == '__main__':\r\n\tmain()\r\n```\r\n\r\n\r\nOutput:\r\n```\r\nBeam Search Decoding : [0]\r\nBest Path Decoding   : [0, 0]\r\n```\r\n\r\n---\r\n\r\nHave I written custom code: code see above\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: tested with 1.3 and 1.6\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: code see above, doc is linked\r\nMobile device: N/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "YES", "Nagging Assignee @tatianashp: It has been 119 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "After half a year of inactivity of this issue, I give up, just leave the error in your documentation, I don't care :-)", "I argee with @githubharald that the result of the greedy and beam decoder is different if beam width and top paths is set to 1. I do not have a strong theoretical background but I found the same problem when I optimizing the model, change with the two different decoders.", "After some time spent trying to obtain the same result with the greedy decoder, I ended up finding this issue. This is rather simple: either the documentation needs to be more specific explaining how to obtain the same outputs, which doesn't seem to be possible, or we simply remove this wrong Note to avoid confusion.\r\n\r\nCan someone from the TF core team take a look at this? :pray:  @tatianashp (not sure exactly who should be tagged, trying to ping regular maintainers @mihaimaruseac @yongtang @hawkinsp)\r\n\r\nThis is an easy bug to fix, I'm happy to open a PR if that solves the issue :+1: ", "Unfortunately I'm now working on this specific area but PRs are always welcome", "> Unfortunately I'm now working on this specific area but PRs are always welcome\r\n\r\n@mihaimaruseac I just opened https://github.com/tensorflow/tensorflow/pull/53269 to address this :+1: "]}, {"number": 21050, "title": "[tf.keras] Add missing cast in keras/layers/normalization.py", "body": "`scale` needs to be casted into the right data type", "comments": ["This pull request fixes it as well https://github.com/tensorflow/tensorflow/pull/20809"]}, {"number": 21049, "title": "[XLA] Allow the iota test to be disabled with a manifest file", "body": "This adds the option to disable the iota test using a manifest file.\r\n", "comments": ["Polite inquiry: are you awaiting a response from me, or @nickdesaulniers ? ", "@martinwicke , I believe I LGTM'd the internal review.  Can you or @caisq see this through?", "Does someone need to add a 'do a test build' flag to get the tests to run?\r\n", "Hi.\r\n\r\nIs this now ready to pull, since that 'import/copybara' seems to never report its status for any PR?\r\n"]}, {"number": 21048, "title": "High loss of accuracy after coverting \".pb\" to \".lite\" on Android", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI only modified the ImageClassifier.java to make it compatible with float and quant.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04.3 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nLG G4\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.7.1\r\n- **Python version**:2.7.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nHi,\r\nI retrained a module refer to [tensorflow-for-poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0), and its retrain.py was replaced by [this one](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py).\r\n\r\nI did two experiments:\r\n\r\n**1. Choose the module: mobilenet_1.0_224_quant**\r\nI retrained the module with the command below:\r\n\r\n> python -m scripts.retrain \\\r\n  --architecture=mobilenet_1.0_224_quant \\\r\n  --bottleneck_dir=tf_files/bottlenecks \\\r\n  --how_many_training_steps=500 \\\r\n  --model_dir=tf_files/models/ \\\r\n  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224_quant \\\r\n  --output_graph=tf_files/retrained_graph.pb \\\r\n  --output_labels=tf_files/retrained_labels.txt \\\r\n  --image_dir=tf_files/flower_photos\r\n\r\nThen I converted \".pb\" to \".lite\" with this command:\r\n\r\n> toco \\\r\n  --input_file=tf_files/retrained_graph.pb \\\r\n  --output_file=tf_files/optimized_graph_quant.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shape=\"1,224,224,3\" \\\r\n  --input_array=input \\\r\n  --output_array=final_result \\\r\n  --std_value=128 --mean_value=128\r\n\r\nI tested _optimized_graph_quant.lite_ with the _TfLiteCameraDemo_ which provided by tensorflow-for-poets.\r\nAnd only once inference, I found that the accuracy is very good:\r\n\r\n> 01-01 01:12:37.942 12117 12217 D TfLiteCameraDemo: Timecost to run model inference: 354\r\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: textToShow = 354ms\r\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: roses: 1.00\r\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: sunflowers: 0.00\r\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: tulips: 0.00\r\n\r\n\r\n**2. Choose the module: mobilenet_1.0_224**\r\nI retrained the module with the command below:\r\n\r\n> python -m scripts.retrain \\\r\n  --architecture=mobilenet_1.0_224 \\\r\n  --bottleneck_dir=tf_files/bottlenecks \\\r\n  --how_many_training_steps=500 \\\r\n  --model_dir=tf_files/models/ \\\r\n  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224 \\\r\n  --output_graph=tf_files/retrained_graph.pb \\\r\n  --output_labels=tf_files/retrained_labels.txt \\\r\n  --image_dir=tf_files/flower_photos\r\n\r\nThen I converted \".pb\" to \".lite\" with this command:\r\n\r\n> toco \\\r\n  --input_file=tf_files/retrained_graph.pb \\\r\n  --output_file=tf_files/optimized_graph.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=\"1,224,224,3\" \\\r\n  --input_array=input \\\r\n  --output_array=final_result \\\r\n  --inference_type=FLOAT \\\r\n  --input_data_type=FLOAT\r\n\r\nI also tested optimized_graph.lite with the _TfLiteCameraDemo_ which provided by tensorflow-for-poets.\r\nAnd after multiple inferences, the accuracy increased to an acceptable value :\r\n\r\n> 01-01 01:29:23.221 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 510\r\n01-01 01:29:23.226 13207 13224 D TfLiteCameraDemo: roses: 0.06\r\n01-01 01:29:23.483 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 238\r\n01-01 01:29:23.488 13207 13224 D TfLiteCameraDemo: roses: 0.18\r\n01-01 01:29:23.741 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 221\r\n01-01 01:29:23.746 13207 13224 D TfLiteCameraDemo: roses: 0.31\r\n01-01 01:29:24.162 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 398\r\n01-01 01:29:24.167 13207 13224 D TfLiteCameraDemo: roses: 0.45\r\n01-01 01:29:24.527 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 336\r\n01-01 01:29:24.533 13207 13224 D TfLiteCameraDemo: roses: 0.58\r\n01-01 01:29:24.898 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 339\r\n01-01 01:29:24.907 13207 13224 D TfLiteCameraDemo: roses: 0.68\r\n01-01 01:29:25.274 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 326\r\n01-01 01:29:25.280 13207 13224 D TfLiteCameraDemo: roses: 0.76\r\n01-01 01:29:25.646 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 340\r\n01-01 01:29:25.652 13207 13224 D TfLiteCameraDemo: roses: 0.83\r\n01-01 01:29:26.032 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 354\r\n01-01 01:29:26.041 13207 13224 D TfLiteCameraDemo: roses: 0.87\r\n01-01 01:29:26.433 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365\r\n01-01 01:29:26.438 13207 13224 D TfLiteCameraDemo: roses: 0.91\r\n01-01 01:29:26.831 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365\r\n01-01 01:29:26.838 13207 13224 D TfLiteCameraDemo: roses: 0.93\r\n01-01 01:29:27.288 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 391\r\n01-01 01:29:27.296 13207 13224 D TfLiteCameraDemo: roses: 0.95\r\n01-01 01:29:27.658 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 333\r\n01-01 01:29:27.663 13207 13224 D TfLiteCameraDemo: roses: 0.97\r\n01-01 01:29:28.005 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 313\r\n01-01 01:29:28.010 13207 13224 D TfLiteCameraDemo: roses: 0.97\r\n01-01 01:29:28.381 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 341\r\n01-01 01:29:28.385 13207 13224 D TfLiteCameraDemo: roses: 0.98\r\n\r\n**Please help to analyze that why it needs multiple inferences to get an good accuracy here? Thanks!**\r\n", "comments": ["The demo app applies a low-pass filter to the outputs. \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L134\r\n\r\nWhen outputs of TFLite interpreter are changed, the displayed values are changed gradually. This prevents the case that one bad frame makes the UI jumpy (e.g. 98% -> 0% -> 99%). \r\n\r\nThe logic isn't in TFLite interpreter. You're free to change the code to disable the logic. If you get the values from TFLite interpreter directly is should be fine. \r\n\r\nClosing since this is working as intended. "]}, {"number": 21047, "title": "Adding a callback function register to improve flexibility of Keras", "body": "Hello everyone, I added one member function in `DirectoryIterator` class allow developer to register callback functions when pumping data by `flow_from_directory` of `ImageDataGenerator` class. It's can improve flexibility when have complex data structure of feature and label. It's very useful to register a customize function to process their own logic to integrate the `batch_x` and `batch_y`. It can make the `DirectoryIterator` pump the data structure what the model required.\r\n\r\n> example:\r\n\r\n```\r\ndef process_train_batch_X(raw_x_batch, filenames, index_array):\r\n    tmp_temperature = []\r\n    tmp_time = []\r\n    for i, j in enumerate(index_array):\r\n        tmp_temperature.append(get_temp_from_str(filenames[j]))\r\n        tmp_time.append(get_time_from_str(filenames[j]))\r\n    # get discretization array from the filename\r\n    x_batch_temperature = utils.discretization(tmp_temperature)\r\n    x_batch_time = utils.discretization(tmp_time)\r\n    x_batch = [raw_x_batch, x_batch_temperature, x_batch_time]\r\n    return x_batch\r\n\r\ndef process_batch_Y(raw_x_batch, filenames, index_array):\r\n    .....\r\n\r\ntrain_iterator.register_batch_processor(process_train_batch_X, process_batch_Y)\r\n```", "comments": ["Nagging Reviewer @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Thank you for the PR.\r\n\r\n`preprocessing_function` is an existing argument to `ImageDataGenerator`, which allows you to add a custom processing step for each batch of data. It is called at the start of `standardize`.\r\n\r\nIn you need more advanced types of preprocessing than what can be covered by this built-in setup, I recommend that you subclass `ImageDataGenerator` to add this functionality while reusing as much of the built-in features as possible.\r\n\r\nAfter reviewing your proposal, we decided not to merge it.\r\n\r\nBy the way, this part of the code has migrated to its own repository: https://github.com/keras-team/keras-preprocessing\r\n\r\nIn the future, if you wish to make any PRs to Keras preprocessing, please send your PR to this repository."]}, {"number": 21046, "title": "Adding a callback function to improve flexibility of Keras", "body": "Hello everyone, I added one member function in `DirectoryIterator` class allow developer to register callback functions when pumping data by `flow_from_directory` of `ImageDataGenerator` class. It's can improve flexibility when have complex data structure of feature and label. It's very useful to register a customize function to process their own logic to integrate the `batch_x` and `batch_y`. It can make the `DirectoryIterator` pump the data structure what the model required.\r\n\r\n> example:\r\n\r\n```\r\ndef process_train_batch_X(raw_x_batch, filenames, index_array):\r\n    tmp_temperature = []\r\n    tmp_time = []\r\n    for i, j in enumerate(index_array):\r\n        tmp_temperature.append(get_temp_from_str(filenames[j]))\r\n        tmp_time.append(get_time_from_str(filenames[j]))\r\n    # get discretization array from the filename\r\n    x_batch_wet = utils.discretization(tmp_temperature)\r\n    x_batch_time = utils.discretization(tmp_time)\r\n    x_batch = [raw_x_batch, x_batch_wet, x_batch_time]\r\n    return x_batch\r\n\r\ndef process_batch_Y(raw_x_batch, filenames, index_array):\r\n    .....\r\n\r\ntrain_iterator.register_batch_processor(process_train_batch_X, process_batch_Y)\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "commit author signed it !", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 21045, "title": "Building TensorFlow failed", "body": "### Issue\r\nI'm trying to build the .so file using bazel\r\n`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a`  \r\nGetting an error:\r\n\r\n```\r\ntensorflow/core/BUILD:2306:1: C++ compilation of rule '//tensorflow/core:protos_all_proto_cc_impl' failed (Exit 1)\r\nIn file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.cc:4:\r\nIn file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.h:9:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:52:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/mutex.h:33:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/mutex:35:\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/c++0x_warning.h:32:2: error: This file requires compiler and library support for the ISO C++ 2011 standard. This support is currently experimental, and must be enabled with the -std=c++11 or -std=gnu++11 compiler options.\r\n#error This file requires compiler and library support for the \\\r\n ^\r\nIn file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.cc:4:\r\nIn file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.h:9:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:52:\r\nexternal/protobuf_archive/src/google/protobuf/stubs/mutex.h:58:8: error: no type named 'mutex' in namespace 'std'\r\n  std::mutex mu_;\r\n\r\nIn file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.cc:4:\r\nIn file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.h:9:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:53:\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:346:25: error: no type named 'remove_reference' in namespace 'std'\r\n  typedef typename std::remove_reference<T>::type base_type;\r\n \r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:346:41: error: expected member name or ';' after declaration specifiers\r\n  typedef typename std::remove_reference<T>::type base_type;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:347:17: error: unknown type name 'base_type'\r\n  typedef const base_type& type;\r\n                \r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:401:17: error: no type named 'remove_reference' in namespace 'std'\r\n  typename std::remove_reference<P1>::type p1_;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:401:33: error: expected member name or ';' after declaration specifiers\r\n  typename std::remove_reference<P1>::type p1_;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:402:17: error: no type named 'remove_reference' in namespace 'std'\r\n  typename std::remove_reference<P2>::type p2_;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:402:33: error: expected member name or ';' after declaration specifiers\r\n  typename std::remove_reference<P2>::type p2_;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:403:17: error: no type named 'remove_reference' in namespace 'std'\r\n  typename std::remove_reference<P3>::type p3_;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:403:33: error: expected member name or ';' after declaration specifiers\r\n  typename std::remove_reference<P3>::type p3_;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:404:17: error: no type named 'remove_reference' in namespace 'std'\r\n  typename std::remove_reference<P4>::type p4_;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:404:33: error: expected member name or ';' after declaration specifiers\r\n  typename std::remove_reference<P4>::type p4_;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:405:17: error: no type named 'remove_reference' in namespace 'std'\r\n  typename std::remove_reference<P5>::type p5_;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:405:33: error: expected member name or ';' after declaration specifiers\r\n  typename std::remove_reference<P5>::type p5_;\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:383:9: error: member initializer 'p1_' does not name a non-static data member or base class\r\n        p1_(p1),\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:384:9: error: member initializer 'p2_' does not name a non-static data member or base class\r\n        p2_(p2),\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:385:9: error: member initializer 'p3_' does not name a non-static data member or base class\r\n        p3_(p3),\r\n\r\nexternal/protobuf_archive/src/google/protobuf/stubs/callback.h:386:9: error: member initializer 'p4_' does not name a non-static data member or base class\r\n        p4_(p4)\r\n```\r\n\r\n### System information\r\nOS:\r\n macOS High Sierra 10.13.3\r\n\r\nbazel: \r\n`Build label: 0.15.0-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 26 12:42:27 2018 (1530016947)\r\nBuild timestamp: 1530016947\r\nBuild timestamp as int: 1530016947`\r\n\r\nndk:\r\nandroid-ndk-r16b\r\n\r\nI looked similar problems, like [#3924](https://github.com/bazelbuild/bazel/issues/3924) or [#17046](https://github.com/tensorflow/tensorflow/issues/17046), but unfortunately did not find a solution for himself. \r\n\r\nThank you in advance.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "### Have I written custom code\r\nN/A\r\n\r\n### OS Platform and Distribution\r\nmacOS High Sierra 10.13.3 (17D47)\r\n\r\n### TensorFlow installed from\r\nI used \"native\" pip command for install TensorFlow\r\n\r\n### TensorFlow version\r\n1.5.0\r\n\r\n### Bazel version\r\nBuild label: 0.15.0-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 26 12:42:27 2018 (1530016947)\r\nBuild timestamp: 1530016947\r\nBuild timestamp as int: 1530016947\r\n\r\n### CUDA/cuDNN version\r\nI am not using CUDA/cuDNN\r\n\r\n### GPU model and memory\r\nATI Radeon HD 4670 256 \u041c\u0411\r\n\r\n### Exact command to reproduce\r\n`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a`\r\n\r\n### Mobile device\r\nN/A", "I was trying to make the same build as OP. I got the same errors. I was using ndk14b though.", "Have any solutions about my issue?", "+1", "I am on Linux though, using ndk14b. Also tried with ndk17", "not sure if it helps, but this is the executed commandline: /home/karsten/tensorflow/tensorflow/core/BUILD:2306:1: C++ compilation of rule '//tensorflow/core:protos_all_proto_cc_impl' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/karsten/.cache/bazel/_bazel_karsten/c3148022e5394457e9b87b7ac08211f4/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=27.0.3 \\\r\n    ANDROID_NDK_API_LEVEL=14 \\\r\n    ANDROID_NDK_HOME=/home/karsten/Android/android-ndk-r14b \\\r\n    ANDROID_SDK_API_LEVEL=24 \\\r\n    ANDROID_SDK_HOME=/home/karsten/Android/Sdk \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/tensorflow/core/_objs/protos_all_proto_cc_impl/tensorflow/core/framework/allocation_description.pb.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/tensorflow/core/_objs/protos_all_proto_cc_impl/tensorflow/core/framework/allocation_description.pb.o' -iquote . -iquote bazel-out/armeabi-v7a-opt/genfiles -iquote external/protobuf_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/genfiles/external/bazel_tools -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-v7a-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/src -Wno-unknown-warning-option -Wno-unused-but-set-variable -Wno-sign-compare '--sysroot=external/androidndk/ndk/platforms/android-24/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/allocation_description.pb.cc -o bazel-out/armeabi-v7a-opt/bin/tensorflow/core/_objs/protos_all_proto_cc_impl/tensorflow/core/framework/allocation_description.pb.o)\r\n", "Issue solved, just building with the --cxxopt=-std=c++11 flag ", "Is this fix upstream, or do I have to set it somewhere myself. If so, where do I set this parameter?", "Example \r\n`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=-std=c++11`", "That works, thanks a lot. Should be updated here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android ?", "my issue could not solve with this ....\r\n", "pleas help mee\r\n", "I am still getting error.", "Still getting the error\r\n\r\nCommand: `bazel build -c opt --cxxopt=-std=c++11 --copt=-D__ANDROID_TYPES_FULL__ //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a  --verbose_failures`\r\n\r\nError:\r\n`ERROR: /private/var/tmp/_bazel_ivlab/23994111945cccf7235ecf4d9dc9fbce/external/com_google_absl/absl/numeric/BUILD.bazel:26:1: C++ compilation of rule '@com_google_absl//absl/numeric:int128' failed (Exit 1): clang failed: error executing command \r\n  (cd /private/var/tmp/_bazel_ivlab/23994111945cccf7235ecf4d9dc9fbce/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=28.0.3 \\\r\n    ANDROID_NDK_API_LEVEL=21 \\\r\n    ANDROID_NDK_HOME=/Users/ivlab/Downloads/android-ndk-r14b/ \\\r\n    ANDROID_SDK_API_LEVEL=28 \\\r\n    ANDROID_SDK_HOME=/Users/ivlab/library/Android/Sdk \\\r\n    PATH=/Library/Frameworks/Python.framework/Versions/3.7/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/ivlab/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 \\\r\n    PYTHON_LIB_PATH=/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.o' -iquote external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_absl -D__ANDROID_TYPES_FULL__ '-std=c++11' -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-missing-field-initializers -Wno-sign-compare '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c external/com_google_absl/absl/numeric/int128.cc -o bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/com_google_absl/absl/numeric/int128.cc:139:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?\r\n  uint64_t w0 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));\r\n                                                          ^~~~~~~~~~\r\n                                                          trunc\r\nexternal/androidndk/ndk/platforms/android-21/arch-arm/usr/include/math.h:278:8: note: 'trunc' declared here\r\ndouble  trunc(double);\r\n        ^\r\nexternal/com_google_absl/absl/numeric/int128.cc:141:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?\r\n  uint64_t w1 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));\r\n                                                          ^~~~~~~~~~\r\n                                                          trunc\r\nexternal/androidndk/ndk/platforms/android-21/arch-arm/usr/include/math.h:278:8: note: 'trunc' declared here\r\ndouble  trunc(double);\r\n        ^\r\nexternal/com_google_absl/absl/numeric/int128.cc:143:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?\r\n  uint64_t w2 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));\r\n                                                          ^~~~~~~~~~\r\n                                                          trunc\r\nexternal/androidndk/ndk/platforms/android-21/arch-arm/usr/include/math.h:278:8: note: 'trunc' declared here\r\ndouble  trunc(double);\r\n        ^\r\n3 errors generated.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build`", "dear this is insoloble still we also face this....\n\nOn Fri, 24 May 2019 at 04:02, sodiumLights <notifications@github.com> wrote:\n\n> Still getting the error\n>\n> Command: bazel build -c opt --cxxopt=-std=c++11\n> --copt=-D__ANDROID_TYPES_FULL__\n> //tensorflow/contrib/android:libtensorflow_inference.so\n> --crosstool_top=//external:android/crosstool\n> --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a\n> --verbose_failures\n>\n> Error:\n> ERROR:\n> /private/var/tmp/_bazel_ivlab/23994111945cccf7235ecf4d9dc9fbce/external/com_google_absl/absl/numeric/BUILD.bazel:26:1:\n> C++ compilation of rule '@com_google_absl//absl/numeric:int128' failed\n> (Exit 1): clang failed: error executing command (cd\n> /private/var/tmp/_bazel_ivlab/23994111945cccf7235ecf4d9dc9fbce/execroot/org_tensorflow\n> && \\ exec env - \\ ANDROID_BUILD_TOOLS_VERSION=28.0.3 \\\n> ANDROID_NDK_API_LEVEL=21 \\\n> ANDROID_NDK_HOME=/Users/ivlab/Downloads/android-ndk-r14b/ \\\n> ANDROID_SDK_API_LEVEL=28 \\\n> ANDROID_SDK_HOME=/Users/ivlab/library/Android/Sdk \\\n> PATH=/Library/Frameworks/Python.framework/Versions/3.7/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/ivlab/bin\n> \\ PWD=/proc/self/cwd \\\n> PYTHON_BIN_PATH=/Library/Frameworks/Python.framework/Versions/3.7/bin/python3\n> \\\n> PYTHON_LIB_PATH=/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages\n> \\ TF_CONFIGURE_IOS=0 \\ TF_DOWNLOAD_CLANG=0 \\ TF_NEED_CUDA=0 \\\n> TF_NEED_OPENCL_SYCL=0 \\ TF_NEED_ROCM=0 \\\n> external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang\n> -gcc-toolchain\n> external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64\n> -fpic -ffunction-sections -funwind-tables -fstack-protector-strong\n> -Wno-invalid-command-line-argument -Wno-unused-command-line-argument\n> -no-canonical-prefixes -fno-integrated-as -target\n> armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp'\n> '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF\n> bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.d\n> '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.o'\n> -iquote external/com_google_absl -iquote\n> bazel-out/armeabi-v7a-opt/genfiles/external/com_google_absl -iquote\n> bazel-out/armeabi-v7a-opt/bin/external/com_google_absl\n> -D__ANDROID_TYPES_FULL__ '-std=c++11' -Wall -Wextra -Wcast-qual\n> -Wconversion-null -Wmissing-declarations -Woverlength-strings\n> -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla\n> -Wwrite-strings -Wno-missing-field-initializers -Wno-sign-compare\n> '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem\n> external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem\n> external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include\n> -isystem\n> external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward\n> -c external/com_google_absl/absl/numeric/int128.cc -o\n> bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.o)\n> Execution platform: @bazel_tools//platforms:host_platform\n> external/com_google_absl/absl/numeric/int128.cc:139:59: error: no member\n> named 'trunc' in namespace 'std'; did you mean simply 'trunc'? uint64_t w0\n> = static_cast<uint64_t>(static_cast<double>(std::trunc(v))); ^~~~~~~~~~\n> trunc\n> external/androidndk/ndk/platforms/android-21/arch-arm/usr/include/math.h:278:8:\n> note: 'trunc' declared here double trunc(double); ^\n> external/com_google_absl/absl/numeric/int128.cc:141:59: error: no member\n> named 'trunc' in namespace 'std'; did you mean simply 'trunc'? uint64_t w1\n> = static_cast<uint64_t>(static_cast<double>(std::trunc(v))); ^~~~~~~~~~\n> trunc\n> external/androidndk/ndk/platforms/android-21/arch-arm/usr/include/math.h:278:8:\n> note: 'trunc' declared here double trunc(double); ^\n> external/com_google_absl/absl/numeric/int128.cc:143:59: error: no member\n> named 'trunc' in namespace 'std'; did you mean simply 'trunc'? uint64_t w2\n> = static_cast<uint64_t>(static_cast<double>(std::trunc(v))); ^~~~~~~~~~\n> trunc\n> external/androidndk/ndk/platforms/android-21/arch-arm/usr/include/math.h:278:8:\n> note: 'trunc' declared here double trunc(double); ^ 3 errors generated.\n> Target //tensorflow/contrib/android:libtensorflow_inference.so failed to\n> build\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21045?email_source=notifications&email_token=AIZH7IP244AMYAKNGMS7HSTPW4PBBA5CNFSM4FLJXD52YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWDWRAY#issuecomment-495413379>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIZH7IKITCLZ2CBZYTVKGSLPW4PBBANCNFSM4FLJXD5Q>\n> .\n>\n"]}, {"number": 21044, "title": "Reproducing when setting 'max_to_keep=0', then the get_checkpoint_state(ckpt_dir).all_model_checkpoint_paths only record the recent 1 model path.", "body": "I produced the closed issue #13381, below is my model saving code snippets:\r\n\r\n```python\r\n def do_training(self):\r\n        saver = tf.train.Saver(max_to_keep = 0)\r\n\r\n        epoch_size = (os.path.getsize(self.config.data_path) - 1) // self.config.batch_size // self.config.max_length\r\n        resume_ckpt_dir = self.config.resume_ckpt_dir\r\n        with tf.Session() as sess:\r\n            restored = False\r\n            start_epoch = 0\r\n            if resume_ckpt_dir:\r\n                ckpt = tf.train.get_checkpoint_state(resume_ckpt_dir)\r\n                if ckpt and ckpt.model_checkpoint_path:\r\n                    saver.restore(sess, ckpt.model_checkpoint_path)\r\n                    restored = True\r\n                    print ('Restore from %s ... ' % ckpt.model_checkpoint_path )\r\n                    resume_global_step = sess.run(self.model.global_step)\r\n                    start_epoch = resume_global_step // epoch_size\r\n                    print ('Restore global step is %d' % resume_global_step)\r\n            if not restored:    \r\n                sess.run(tf.global_variables_initializer())\r\n            \r\n            writer = tf.summary.FileWriter(self.config.summary_path, sess.graph)\r\n\r\n            for epoch in range(start_epoch, start_epoch + self.config.num_epochs):\r\n                gs, compl = self.run_epoch(sess, epoch, writer)\r\n                \r\n                if epoch % self.config.save_epoch_interval == 0:\r\n                    print ('Save model at global step (%d) / Epoch (%d)' % (gs, epoch))\r\n                    saver.save(sess, os.path.join(self.config.save_ckpt_dir, 'ckpt'), global_step = gs)\r\n                if compl:\r\n                    print (\"Val loss has reach the expected entropy within %f\" % self.config.comp_stop_width)\r\n                    if (self.config.comp_stop):\r\n                        break\r\n            writer.close()\r\n```\r\n\r\nThe I run the follow code:\r\n![image](https://user-images.githubusercontent.com/12935189/43059769-a6292a3a-8e80-11e8-914b-287082387e71.png)\r\n\r\nI saved the model every 39 iterations, but I can only see the most recent 1 in all_model_checkpoint_paths\r\n\r\nMy versions:\r\n1. tensorflow-gpu         1.3.0\r\n2. Ubuntu 16.04.4 LTS", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@tensorflowbutler \r\nHave I written custom code: no\r\nOS Platform and Distribution: Ubuntu 16.04.4 LTS\r\nTensorFlow installed from: maybe by `pip install tensorflow==1.3.0`, I am quite not sure.\r\nTensorFlow version tensorflow-gpu 1.3.0\r\nBazel version: I don't know how to inspect it, I use a pre-complied pip tensorflow package.\r\nCUDA/cuDNN version: CUDA Version 8.0.61, cuDNN Version 6.0.21\r\nGPU model and memory: GPU 0: GeForce GTX TITAN X (UUID: GPU-005b528e-2b75-4bb2-d828-b518745e9492) and GPU memory is 12204MiB\r\nExact command to reproduce: see above code\r\nMobile device: not tested", "This looks like an intentional decision to avoid leaking memory if `max_to_keep=None`, which seems sensible (the Python list would keep growing, and the `checkpoint` proto would keep growing, probably running into a proto size limit before running out of memory).\r\n\r\nWe could have the Saver hold on to a large but finite number of previous checkpoints if max_to_keep=None (say, 1000000). But that seems like an even weirder API; it still can't be relied on to give you everything, but it looks more like it should.\r\n\r\nSo I'm not sure what's actionable here aside from documentation. For the large-but-finite `checkpoint` proto behavior, I'd suggest setting `max_to_keep` to some very large number. Does that make sense, @sonack ?", "@allenlavoie  Yes! I used very large `max_to_keep` in practice, but I think the docs should make more explanation here.", "Nagging Assignee @allenlavoie: It has been 33 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21043, "title": "Update protobuf to 3 6 0 1", "body": "Fixes issue with std::call_once in visual studio 2017, std::call_once is called twice sometimes", "comments": ["Closing based on discussion in #21035."]}, {"number": 21042, "title": "Templates DataTypeToEnum and associated structs on type properties", "body": "This PR addresses the bug outlined in #20586. Rather than relying on all systems to define the `int64` data type in a manner that is consistent with a `long long`, this PR inspects the properties of the type. For instance, `int64` is a `signed`, `integral` type with a size of `8 bytes`.\r\n\r\nThis is important as not all systems define the basic integral data types in the same way, but they are all defined to meet the same intrinsic properties. On the test system, `int64_t` is not equivalent to a `long long` (which is how the `int64` type is defined in `tensorflow/core/platform/default/integral_types.h`). However, both `long long` and `int64_t` meet the same intrinsic properties on the test system. The following code and output shows this inconsistency, where `int64_t != long long` from the perspective of gcc (Ubuntu 7.3.0-21ubuntu1~16.04) 7.3.0.\r\n\r\n```\r\n#include <cstdint>\r\n#include <iostream>\r\n\r\nint main(void) {\r\n    std::cout << \"int64_t...............: \"\r\n              << \"(\" << typeid(int64_t).name() << \") \" << sizeof(int64_t) << std::endl;\r\n    std::cout << \"long..................: \"\r\n              << \"(\" << typeid(long).name() << \") \" << sizeof(long) << std::endl;\r\n    std::cout << \"long int..............: \"\r\n              << \"(\" << typeid(long int).name() << \") \" << sizeof(long int) << std::endl;\r\n    std::cout << \"long long int.........: \"\r\n              << \"(\" << typeid(long long int).name() << \") \" << sizeof(long long int) << std::endl;\r\n    std::cout << \"long long.............: \"\r\n              << \"(\" << typeid(long long).name() << \") \" << sizeof(long long) << std::endl;\r\n    std::cout << \"uint64_t..............: \"\r\n              << \"(\" << typeid(uint64_t).name() << \") \" << sizeof(int64_t) << std::endl;\r\n    std::cout << \"unsigned long.........: \"\r\n              << \"(\" << typeid(unsigned long).name() << \") \" << sizeof(unsigned long) << std::endl;\r\n    std::cout << \"unsigned long int.....: \"\r\n              << \"(\" << typeid(unsigned long int).name() << \") \" << sizeof(unsigned long int) << std::endl;\r\n    std::cout << \"unsigned long long int: \"\r\n              << \"(\" << typeid(unsigned long long int).name() << \") \" << sizeof(unsigned long long int) << std::endl;\r\n    std::cout << \"unsigned long long....: \"\r\n              << \"(\" << typeid(unsigned long long).name() << \") \" << sizeof(unsigned long long) << std::endl;\r\n    std::cout << \"long long == int64_t? \" << (std::is_same<long long, int64_t>::value ? \"yes\" : \"no\") << std::endl;\r\n    std::cout << \"(sizeof(long long) == 8) && std::is_signed<long long>::value? \"\r\n              << ((sizeof(long long) == 8) && std::is_signed<long long>::value ? \"yes\" : \"no\") << std::endl;\r\n    std::cout << \"(sizeof(int64_t) == 8) && std::is_signed<int64_t>::value? \"\r\n              << ((sizeof(int64_t) == 8) && std::is_signed<int64_t>::value ? \"yes\" : \"no\") << std::endl;\r\n    std::cout << \"unsigned long long == uint64_t? \"\r\n              << (std::is_same<unsigned long long, uint64_t>::value ? \"yes\" : \"no\") << std::endl;\r\n    std::cout << \"(sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value? \"\r\n              << ((sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value ? \"yes\" : \"no\")\r\n              << std::endl;\r\n    std::cout << \"(sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value? \"\r\n              << ((sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value ? \"yes\" : \"no\") << std::endl;\r\n    return 0;\r\n}\r\n```\r\n```\r\nint64_t...............: (l) 8\r\nlong..................: (l) 8\r\nlong int..............: (l) 8\r\nlong long int.........: (x) 8\r\nlong long.............: (x) 8\r\nuint64_t..............: (m) 8\r\nunsigned long.........: (m) 8\r\nunsigned long int.....: (m) 8\r\nunsigned long long int: (y) 8\r\nunsigned long long....: (y) 8\r\nlong long == int64_t? no\r\n(sizeof(long long) == 8) && std::is_signed<long long>::value? yes\r\n(sizeof(int64_t) == 8) && std::is_signed<int64_t>::value? yes\r\nunsigned long long == uint64_t? no\r\n(sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value? yes\r\n(sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value? yes\r\n```\r\n\r\n\r\nNote: `int64` is only used as an example here, this bug applies to other types as well, which this PR addresses.", "comments": ["This looks very precise, which is nice, but I wonder if we really need it, and using tensorflow::int64 and friends when registering kernels isn't just a better place to be. Is there a reason why that doesn't work for you?", "Using `tensorflow::int64` and friends only works when everyone knows that they are supposed to use them (which is either not documented or is poorly documented). As soon as people stop using them (or dont use them) problems will arise and confusion will ensue, especially for people who are writing custom ops. This is shown by this exact situation,  `tensorflow::int64 != std::int64_t`.\r\n\r\nFurther to this, `long long` (which `tensorflow::int64` is defined as) is not guaranteed to be 64 bits, it is guaranteed to be \"at least\" 64 bits. It would be more precise to use the definitions in `cstdint`. However, even in this case it would still be prudent to use this more precise way of matching types to enum values to ensure that further confusion of this sort wont happen again.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think we would like to switch to using std::int64_t, but this would be a big effort, and include changes to much of the C++ code using TensorFlow, not just TensorFlow itself. This may happen as part of switching to https://github.com/abseil/abseil-cpp, assuming they also use (or switch to) std::int64_t.\r\n@martinwicke ", "Sounds like @josh11b suggested an alternative approach to this PR. If there is no objects, I will close this PR. ", "Closing PR. Feel free to comment if you have additional concerns or questions, @Bidski ", "` auto gt_pts_trans = tensorflow::ops::Transpose(root, gt_pts, NULL);  `\r\nI used 'tensorflow::ops::Transpose()' and occurred error despite gt_pts  is  DT_FLOAT\r\n\r\n```\r\nerror: static assertion failed: Specified Data Type not supported\r\n   static_assert(IsValidDataType<T>::value, \"Specified Data Type not supported\");\r\nerror: \u2018v\u2019 is not a member of \u2018tensorflow::DataTypeToEnum<long int>\u2019\r\n       Tensor t(DataTypeToEnum<RealT>::v(), TensorShape());\r\n```\r\n\r\nWhat should I do?\r\n"]}, {"number": 21041, "title": "tensorflow_lite  Prelu unsupport dims->size = 2", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes, I write own model define, loss and training codes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: iPhone 6s\r\n- **TensorFlow installed from (source or binary)**: binary, pip install\r\n- **TensorFlow version (use command below)**:1.3.0\r\n- **Python version**:3.5\r\n- **Bazel version (if compiling from source)**:0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.9.4\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**:GeForce GTX 1060\r\n- **Exact command to reproduce**:\r\n### Describe the problem\r\n In tensorflow_lite demo tflite_simple_example,I use my Rnet.tflite ,I get error as follow\r\n  if (interpreter->AllocateTensors() != kTfLiteOk) {\r\n    NSLog(@\"Failed to allocate tensors.\");\r\n    exit(-1);\r\n  }\r\nerror:tensorflow/contrib/lite/kernels/activations.cc:171 input->dims->size != 4 (2 != 4)\r\n\r\nIn activations.cc I find  PreluPrepare()  TF_LITE_ENSURE_EQ(context, input->dims->size, 4); my  input->dims->size =2\r\n\r\n  TfLiteStatus PreluPrepare(TfLiteContext* context, TfLiteNode* node) {\r\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\r\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\r\n  const TfLiteTensor* input = GetInput(context, node, 0);\r\n  TfLiteTensor* output = GetOutput(context, node, 0);\r\n  const TfLiteTensor* alpha = GetInput(context, node, 1);\r\n\r\n  output->type = input->type;\r\n\r\n  // Currently only Float32 is supported\r\n  // TODO(ycling): Support other data types.\r\n  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);\r\n  TF_LITE_ENSURE_EQ(context, alpha->type, kTfLiteFloat32);\r\n\r\n  // Currently, only support 4D `input` and 3D `alpha` with shape\r\n  // (1, 1, channels).\r\n  // TODO(impjdi): Support other cases where `alpha` is broadcastable\r\n  // to `input`.\r\n  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);\r\n  TF_LITE_ENSURE_EQ(context, alpha->dims->size, 3);\r\n  TF_LITE_ENSURE_EQ(context, alpha->dims->data[0], 1);\r\n  TF_LITE_ENSURE_EQ(context, alpha->dims->data[1], 1);\r\n  TF_LITE_ENSURE_EQ(context, alpha->dims->data[2], input->dims->data[3]);\r\n\r\n  return context->ResizeTensor(context, output,\r\n                               TfLiteIntArrayCopy(input->dims));\r\n}\r\n\r\n### Source code / logs\r\ninput = Input(shape=[24, 24, 3],batch_shape=[1,24, 24, 3])\r\nx = Conv2D(28, (3, 3), strides=1, padding='valid', name='conv1')(input)\r\nx = PReLU(shared_axes=[1, 2], name='prelu1')(x)\r\nx = MaxPool2D(pool_size=3,strides=2, padding='same')(x)\r\n\r\nx = Conv2D(48, (3, 3), strides=1, padding='valid', name='conv2')(x)\r\nx = PReLU(shared_axes=[1, 2], name='prelu2')(x)\r\nx = MaxPool2D(pool_size=3, strides=2)(x)\r\n\r\nx = Conv2D(64, (2, 2), strides=1, padding='valid', name='conv3')(x)\r\nx = PReLU(shared_axes=[1, 2], name='prelu3')(x)\r\nx = Permute((3, 2, 1))(x)\r\nx = Flatten()(x)\r\nx = Dense(128, name='conv4')(x)\r\nx = PReLU( name='prelu4')(x)\r\nclassifier = Dense(2, activation='softmax', name='conv5-1')(x)\r\nbbox_regress = Dense(4, name='conv5-2')(x)\r\nmodel = Model([input], [classifier, bbox_regress])\r\nmodel.load_weights(weight_path, by_name=True)\r\n\r\n\r\nx = PReLU( name='prelu4')(x)    input x->dims->size =2\r\n\r\nHow can I slove this problem?\r\n", "comments": ["Thanks for the report @iChiaGuo We will be tracking this and hopefully have a fix at some point. If you would  like to work around the isse, you can inspect PreluPrepare() and PreluEval() in activations.cc.", "Thanks for reporting. I'll send a fix soon"]}, {"number": 21040, "title": "How to calculate backward pass with feeding custom gradients.", "body": "Currently I am replacing Pythorch's code with tensorflow implementation code.\r\n\r\nThe reference code has the following line.\r\n#####\r\nXXX.backward(grad_output, retain_graph=True)\r\nYYY = input.grad.detach().sum(1).clone().clamp(min=0)\r\n#####\r\nXXX corresponds to ResNet's N*N*C map before fc layers.\r\nIn this code, the directly defined gradient is entered instead of the loss value in the calculation of the backward path.\r\n\r\nHow can I describe this to implement with tensorflow?", "comments": []}, {"number": 21039, "title": "Multiplicative LSTM Cell", "body": "Added Multiplicative LSTM cell based on:\r\nhttps://arxiv.org/pdf/1609.07959.pdf\r\n\r\nThe cell also has the option to implement weight normalization, based on:\r\nhttps://arxiv.org/abs/1602.07868\r\n\r\nThe Multiplicative LSTM with weight normalization is used is this paper by OpenAI:\r\nhttps://arxiv.org/pdf/1704.01444.pdf\r\n\r\nI have also tested the MLSTMCell as a drop in replacement for the BasicLSTMCell in models/tutorials/rnn/ptb/ptb_word_lm.py, and achieved comparable results. ", "comments": ["ping reviewers @ebrevdo @qlzh727 \r\n\r\n@jonnykira can you resolve the conflicts? ", "@jonnykira gentle ping", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 21038, "title": "[BugFix] Fixes #20983 - Store py_func dtypes for correct conversion", "body": "1.  Implement wrapper class `PythonFunc` to store Tout list\r\n    for appropriate dtype conversion from tensorflow types to\r\n    numpy types using the `dtype.as_numpy_dtype` member.\r\n2.  Add py_func_test to illustrate broken behavior (see\r\n    `testConvertEmptyList`). ", "comments": ["^ WIP on the CI errors - they were due to the `FuncRegistry._convert()` being used from `dataset_ops`.  If this is being called this way, should `_convert` (better name?) be exposed from `script_ops` more officially (not through the FuncRegistry)?\r\n\r\nMy fixes are done, I am in-tow for the 9 million bazel files that needed recompilation :) Will squash and push once I confirm things are resolved locally.", "@akshayka - Thanks for the feedback, I think I should have done justice to your comments. Please let me know if this looks better. \r\n\r\nPS: One of the basic tests that I had originally added seemed useless given the py_func_test.py file, so I went ahead and removed it (preserved the test that will *fail* without these changes).", "@akshayka - One of the tests is commented out as (I think) it no longer applies. Once we are all happy with this changeset, I can squash the edits so you have a single commit to merge (if that matters). ", "Great, go ahead and delete the test you commented out, I agree with you that it no longer applies.", "@akshaym - any reservations?", "@sabhiram not from me. I think @akshayka is the best person to review this, so I'll unassign myself. ", "@akshayka - I think this is good to go - might need your blessing before the bot takes over.", "@akshayka - Heh, what did I break? Unfortunately I cannot peer into the CI process enough to guess how this affected what.  Happy to help see it through if needed.", "Hey Sabhiram, sorry for the delay. I didn't have the time to diagnose the issue, but the change caused an internal-only test to segfault."]}, {"number": 21037, "title": "tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory", "body": "can anyone help me??\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/meow/generate_tfrecord.py\", line 99, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/Users/meow/generate_tfrecord.py\", line 85, in main\r\n    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/lib/io/tf_record.py\", line 112, in __init__\r\n    compat.as_bytes(path), compat.as_bytes(compression_type), status)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "i have sloveed it by reduilding everything.THX", "dut now i have about 696 lines of\r\n\"WARNING:root:Variable   [FeatureExtractor/InceptionV2/Mixed_...]   is not available in checkpoint\"\r\n\r\nHave I written custom code  N/A\r\nOS Platform and Distribution  both win10 and mac\r\nTensorFlow installed from pip/brew\r\nTensorFlow version 1.9.0\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce\r\n\r\nssd_inception_v2_coco_2018_01_28\r\nand\r\nssd_inception_v2_coco.config\r\n", "I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please provide a clear description of what you are trying to do, what code you have run, and what problem you are seeing.", "I'm trying to train an object detacor.After I had ready all of my training image and training data, i ran train.py where I got from TensorFlow\\models\\research\\object_detection\\legacy.Then I got lines of WARNING:\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint\r\nWARNING:root:Variable [SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint\r\n\r\nThe detection model I used is \"faster_rcnn_inception_v2_coco_2018_01_28\"  and tha pipeline-config-path is \"faster_rcnn_inception_v2_coco.config\".I all downloaded from https://github.com/tensorflow/models.\r\nThe training still began but the result wasn't good.Can you tell me why i got warning and hows it influence the training.\r\nThank you for your kind assistance. ", "Can you provide a detailed description for:\r\n1. where (and how) did you download the data\r\n2. where (and how) did you download the model (and which model)\r\n3. what script you ran to start the training (and what is the script)\r\n\r\n(i.e., something that we could reproduce on our side would be much helpful)\r\nThanks.", "Here's the files I'm using https://drive.google.com/open?id=1UMpIDh85myaJed0dWvkzlMd9cb3ZHSmx.\r\n\r\n1. I download the data from https://drive.google.com/open?id=1OFx1QpVFx7A0JZA4GTgB4Rm1lIFDemlm\r\n2. I download the model from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md  .Put it in my folder.\r\n3.I start the training with train.py. I put it in google drive.", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Based on the above, there appears to be a mismatch between the checkpoint you are loading and the code you are trying to run. I would recommend starting with the tutorials provided in the Object Detection directory, and, if you have problems with those, please open a question with that repository or on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow). Thanks."]}, {"number": 21036, "title": "No temporal sample weight in metrics", "body": "Hi,\r\n\r\nthe sample weight option is ignored for the metrics. There seems to be a weights option for the metrics, but that does not seem to be able to take a 2D tensor into account for the temporal sample weight mode.\r\n\r\nBest,\r\nNoshaba", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21035, "title": "Clean coding.cc", "body": "Fix of a compile time issue", "comments": ["@kingofthebongo2008 are the changes in the cmake files supposed to be part of this PR?", "Hey,\n\nNo actually.\n\nI just created several pull requests and created local branches after\nfinishing every task, this is why probably they appear one after the other.\nI am relatively new to git so i may have created the branches wrong.\n\n\nOn Tue, 24 Jul 2018 at 06:58, Yifei Feng <notifications@github.com> wrote:\n\n> @kingofthebongo2008 <https://github.com/kingofthebongo2008> are the\n> changes in the cmake files supposed to be part of this PR?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21035#issuecomment-407273025>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABF5cTLmRCC99F70SslPlfEF5_VwDa3Gks5uJptvgaJpZM4VaAoo>\n> .\n>\n", "@kingofthebongo2008 thanks for contributing to TF :). Do you mind double checking your PRs to make sure the commits only contain the changes that are relevant to each PR?", "what should i do in general,\nThere are a couple of pr in groups, split them somehow?\nwhich git commands should i use\n\nOn Tue, 24 Jul 2018 at 23:53, Yifei Feng <notifications@github.com> wrote:\n\n> @kingofthebongo2008 <https://github.com/kingofthebongo2008> thanks for\n> contributing to TF :). Do you mind double checking your PRs to make sure\n> the commits only contain the changes that are relevant to each PR?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21035#issuecomment-407547027>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABF5cZ7zgnfcNS-C1NCF4tD2SKMtyIYiks5uJ4lTgaJpZM4VaAoo>\n> .\n>\n", "@kingofthebongo2008 you might want to create separate branches with [`git checkout`](https://git-scm.com/docs/git-checkout) for each PR, and [`git cherry-pick`](https://git-scm.com/docs/git-cherry-pick) the commits you want to each of the branches.\r\nDo you mind closing the existing stacked PRs and sending us the new ones?", "@kingofthebongo2008 I'm closing this and a bunch of stacked PRs for now. Feel free to send us new ones when you have them ready. Thank you!"]}, {"number": 21034, "title": "`get_weights` will not cover extra variables for subclassed `tf.keras.Model` ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: nightly 07 22\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n`get_weights` will not cover extra variables for subclassed `tf.keras.Model` \r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.resource_variable_ops import ResourceVariable\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(2, use_bias=False)\r\n        self.v = ResourceVariable([1.])\r\n\r\n    def call(self, inputs, training=True, mask=None):\r\n        return self.v + self.d(inputs)\r\n\r\nm = Model()\r\no = m(tf.random_uniform((3, 4)))\r\n# only one element(m.d.kernel), m.v is ignored\r\nprint(m.get_weights())\r\n```\r\n\r\nExtra variables will be ignored by `Model.get_weights` and `Model.set_weights` is it a bug or intention?\r\n\r\nif it is a bug, I would like to send a PR to fix this issue.\r\n", "comments": ["I don't think it's a bug. model itself is only aware of weights that are managed by layers. It would only work here if you use Dense with use_bias=True.\r\nBut, if you really want to manage your own resource variable (which I don't think it's exposed as public as well), you could do subclassed layer instead of subclassed model, i.e.,\r\nclass MyLayer(tf.keras.Layer):\r\n    def __init__(self):\r\n         super(Layer, self).__init__()\r\n    def build(self):\r\n         self.v = ResourceVariable([1.])\r\n         self.built = True\r\n    def call(self, inputs):\r\n         return inputs + self.v\r\n    def compute_output_shape(self):\r\n         something like return self.v.shape\r\n\r\nand use either functional or sequential model,\r\nmodel = tf.keras.Sequential()\r\ninput = tf.keras.Input(???)\r\noutput = tf.keras.layers.Dense(2, use_bias=False)(input)\r\noutput = MyLayer()(output)", "@tanzhenyu \r\nThanks for your reply\r\n> model itself is only aware of weights that are managed by layers\r\n\r\nif so,  then `Model.get_weights` is NOT consistent with `Model.weights` and `Model.save_weights` which contain the extra variables\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.resource_variable_ops import ResourceVariable\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.v = ResourceVariable([1.])\r\n\r\nm = Model()\r\n# not empty\r\nprint(m.weights)\r\n\r\nm.save_weights(\"save/model\")\r\nreader = tf.train.NewCheckpointReader(\"save/model\")\r\n# m.v is also in this list\r\nprint(reader.get_variable_to_shape_map().keys())\r\n```", "yes, model.weights is a combination of layer.weights & _extra_variables. So in this case layer.weights will be empty, and _extra_variable will be v.", "I mean five methods `Model.weights`, `Model.save_weights`, `Model.load_weights`, `Model.get_weights` and `Model.set_weights` should refer to same group of variables, since `Model.weights`, `Model.save_weights` and `Model.load_weights` contain extra variables, `Model.get_weights` and `Model.set_weights` should do too, otherwise the apis are quite confusing to users"]}, {"number": 21033, "title": "\u2018tf.train.shuffle_batch\u2019 problem", "body": "When I train my face_detection model\uff0cI set 0 as my non-face label ,and 1 as face label .After building TFRecord dataset\uff0cI use 'image_batch, label_batch = tf.train.shuffle_batch ' ,it  is clear  that \u2018image_batch \u2019 corresponds to \u2018label_batch\u2019   .However,I found that \u2018image_batch \u2019 is not corresponding to \u2018label_batch\u2019. some non-face pictures'label is 1,and some face pictures' label is 0,why?\r\nhere is my code:`import tensorflow as tf\r\nimport time\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nbegin=time.time()\r\nfilename_queue = tf.train.string_input_producer(['C:\\\\Users\\\\312\\\\Desktop\\\\Face_Detection\\\\ai.tfrecords'],shuffle=False)\r\n\r\nreader = tf.TFRecordReader()\r\n_, serialized_example = reader.read(filename_queue)\r\n\r\nfeatures = tf.parse_single_example(serialized_example,\r\n                               features={\r\n                               'label':tf.FixedLenFeature([],tf.int64),\r\n                               'img_raw':tf.FixedLenFeature([],tf.string),\r\n                               })\r\nimg=tf.decode_raw(features['img_raw'],tf.uint8)\r\nimg = tf.reshape(img, [227,227,3])\r\nlabel=tf.cast(features['label'],tf.int32)\r\n\r\nmin_after_dequeue = 200\r\nbatch_size = 128\r\ncapacity = min_after_dequeue + 10 * batch_size\r\nimage_batch, label_batch = tf.train.batch([img, label], batch_size=batch_size, capacity=capacity)\r\n\r\nimage_batch, label_batch = tf.train.shuffle_batch(\r\n      [img, label],\r\n      batch_size=32,\r\n      num_threads=7,\r\n      capacity=500,\r\n      min_after_dequeue=200)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run((tf.global_variables_initializer(),\r\n              tf.local_variables_initializer()))\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess,coord=coord)\r\n    for i in range(1): \r\n        label=sess.run(label)\r\n        label_batch=sess.run(label_batch)\r\n        image_batch=sess.run(image_batch) \r\n\r\n        for j in range(8):\r\n            plt.imshow(image_batch[j])\r\n            print(label_batch[j])\r\n            plt.show()\r\n    coord.request_stop()\r\n    coord.join(threads)\r\nprint(time.time()-begin)\r\n    `", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21032, "title": "Got different matrix eigenvalues by tensorflow.self_adjoint_eig(A) than by numpy.linalg.eig(A)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: None\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.5\r\n- **TensorFlow installed from (source or binary)**: anaconda\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Python version**: Python 3.6.5 :: Anaconda, Inc.\r\n- **GCC/Compiler version (if compiling from source)**: [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\r\n- **CUDA/cuDNN version**: None \r\n\r\n### Describe the problem\r\nGot different matrix eigenvalues by tensorflow.self_adjoint_eig(A) than by numpy.linalg.eig(A) when A is\r\n[[2., 3.], \r\n[2., 1.]]\r\n\r\n### Source code / logs\r\n```python \r\nimport os\r\nimport tensorflow as tf \r\nimport numpy as np \r\nsess = tf.Session()\r\nA = np.array([[2., 3.],[2., 1.]])\r\ne1,v1=np.linalg.eig(A) \r\nprint(e1)\r\n\r\nB = tf.convert_to_tensor(A, dtype=tf.float64)\r\ne2,v2 = sess.run(tf.self_adjoint_eig(B))\r\nprint(e2)\r\n```\r\n\r\n#### output\r\n```\r\n[ 4. -1.]\r\n[-0.56155281  3.56155281]\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nGPU model and memory\nExact command to reproduce\nMobile device", "tf.self_adjoint_eig is equal to np.linalg.eigh, but not equal to np.linalg.eig. \r\nThe self adjoint algorithm just takes one triangular part and ignores the other triangular part, which explains the difference to np.linalg.eig.\r\nA tf equivalent of np.linalg.eig is not implemented, see https://github.com/tensorflow/tensorflow/issues/22330", "@chenjin3,\r\nSorry for the delayed response. The equivalent of **`np.linalg.eig`** is implemented as [tf.linalg.eigh](https://www.tensorflow.org/api_docs/python/tf/linalg/eigh). Please let us know if this resolves your issue so that we can close it. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 21031, "title": "cmake: add systemlib_GRPC option.", "body": "This is still a work in progress, don't merge.\r\n\r\nI'm trying to add a cmake option to build against `libgrpc++-dev` package provided by system, and the final goal of this PR is #720 . However I'm not sure whether it is working since it fails to build.\r\n\r\nThe build failure looks like this:\r\n```\r\nIn file included from /dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:6:\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:105:19: error: 'RpcMethod' in namespace 'grpc' does not name a type\r\n     const ::grpc::RpcMethod rpcmethod_SendEvents_;\r\n                   ^~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:106:19: error: 'RpcMethod' in namespace 'grpc' does not name a type\r\n     const ::grpc::RpcMethod rpcmethod_SendTracebacks_;\r\n                   ^~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:107:19: error: 'RpcMethod' in namespace 'grpc' does not name a type\r\n     const ::grpc::RpcMethod rpcmethod_SendSourceFiles_;\r\n                   ^~~~~~~~~\r\n[ 15%] Building CXX object CMakeFiles/tf_protos_cc.dir/tensorflow/core/protobuf/saver.pb.cc.o\r\n[ 15%] Building CXX object CMakeFiles/tf_protos_cc.dir/tensorflow/core/protobuf/tensor_bundle.pb.cc.o\r\nIn file included from /dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:6:\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h: In constructor 'tensorflow::EventListener::WithStreamedUnaryMethod_SendTracebacks<BaseClass>::WithStreamedUnaryMethod_SendTracebacks()':\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:247:21: error: 'StreamedUnaryHandler' in namespace 'grpc' does not name a template type\r\n         new ::grpc::StreamedUnaryHandler< ::tensorflow::CallTraceback, ::tensorflow::EventReply>(std::bind(&WithStreamedUnaryMethod_SendTracebacks<BaseClass>::StreamedSendTracebacks, this, std::placeholders::_1, std::placeholders::_2)));\r\n                     ^~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:247:70: error: expected primary-expression before ',' token\r\n         new ::grpc::StreamedUnaryHandler< ::tensorflow::CallTraceback, ::tensorflow::EventReply>(std::bind(&WithStreamedUnaryMethod_SendTracebacks<BaseClass>::StreamedSendTracebacks, this, std::placeholders::_1, std::placeholders::_2)));\r\n                                                                      ^\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:247:96: error: expected primary-expression before '>' token\r\n         new ::grpc::StreamedUnaryHandler< ::tensorflow::CallTraceback, ::tensorflow::EventReply>(std::bind(&WithStreamedUnaryMethod_SendTracebacks<BaseClass>::StreamedSendTracebacks, this, std::placeholders::_1, std::placeholders::_2)));\r\n                                                                                                ^\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h: In constructor 'tensorflow::EventListener::WithStreamedUnaryMethod_SendSourceFiles<BaseClass>::WithStreamedUnaryMethod_SendSourceFiles()':\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:267:21: error: 'StreamedUnaryHandler' in namespace 'grpc' does not name a template type\r\n         new ::grpc::StreamedUnaryHandler< ::tensorflow::DebuggedSourceFiles, ::tensorflow::EventReply>(std::bind(&WithStreamedUnaryMethod_SendSourceFiles<BaseClass>::StreamedSendSourceFiles, this, std::placeholders::_1, std::placeholders::_2)));\r\n                     ^~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:267:76: error: expected primary-expression before ',' token\r\n         new ::grpc::StreamedUnaryHandler< ::tensorflow::DebuggedSourceFiles, ::tensorflow::EventReply>(std::bind(&WithStreamedUnaryMethod_SendSourceFiles<BaseClass>::StreamedSendSourceFiles, this, std::placeholders::_1, std::placeholders::_2)));\r\n                                                                            ^\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:267:102: error: expected primary-expression before '>' token\r\n         new ::grpc::StreamedUnaryHandler< ::tensorflow::DebuggedSourceFiles, ::tensorflow::EventReply>(std::bind(&WithStreamedUnaryMethod_SendSourceFiles<BaseClass>::StreamedSendSourceFiles, this, std::placeholders::_1, std::placeholders::_2)));\r\n                                                                                                      ^\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc: In constructor 'tensorflow::EventListener::Stub::Stub(const std::shared_ptr<grpc::ChannelInterface>&)':\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:30:24: error: class 'tensorflow::EventListener::Stub' does not have any field named 'rpcmethod_SendEvents_'\r\n   : channel_(channel), rpcmethod_SendEvents_(EventListener_method_names[0], ::grpc::RpcMethod::BIDI_STREAMING, channel)\r\n                        ^~~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:30:85: error: 'grpc::RpcMethod' has not been declared\r\n   : channel_(channel), rpcmethod_SendEvents_(EventListener_method_names[0], ::grpc::RpcMethod::BIDI_STREAMING, channel)\r\n                                                                                     ^~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:31:5: error: class 'tensorflow::EventListener::Stub' does not have any field named 'rpcmethod_SendTracebacks_'\r\n   , rpcmethod_SendTracebacks_(EventListener_method_names[1], ::grpc::RpcMethod::NORMAL_RPC, channel)\r\n     ^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:31:70: error: 'grpc::RpcMethod' has not been declared\r\n   , rpcmethod_SendTracebacks_(EventListener_method_names[1], ::grpc::RpcMethod::NORMAL_RPC, channel)\r\n                                                                      ^~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:32:5: error: class 'tensorflow::EventListener::Stub' does not have any field named 'rpcmethod_SendSourceFiles_'\r\n   , rpcmethod_SendSourceFiles_(EventListener_method_names[2], ::grpc::RpcMethod::NORMAL_RPC, channel)\r\n     ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:32:71: error: 'grpc::RpcMethod' has not been declared\r\n   , rpcmethod_SendSourceFiles_(EventListener_method_names[2], ::grpc::RpcMethod::NORMAL_RPC, channel)\r\n                                                                       ^~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc: In member function 'virtual grpc::ClientReaderWriter<tensorflow::Event, tensorflow::EventReply>* tensorflow::EventListener::Stub::SendEventsRaw(grpc::ClientContext*)':\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:36:105: error: 'rpcmethod_SendEvents_' was not declared in this scope\r\n   return new ::grpc::ClientReaderWriter< ::tensorflow::Event, ::tensorflow::EventReply>(channel_.get(), rpcmethod_SendEvents_, context);\r\n                                                                                                         ^~~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:36:105: note: suggested alternative: 'WithAsyncMethod_SendEvents'\r\n   return new ::grpc::ClientReaderWriter< ::tensorflow::Event, ::tensorflow::EventReply>(channel_.get(), rpcmethod_SendEvents_, context);\r\n                                                                                                         ^~~~~~~~~~~~~~~~~~~~~\r\n                                                                                                         WithAsyncMethod_SendEvents\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc: In member function 'virtual grpc::ClientAsyncReaderWriter<tensorflow::Event, tensorflow::EventReply>* tensorflow::EventListener::Stub::AsyncSendEventsRaw(grpc::ClientContext*, grpc::CompletionQueue*, void*)':\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:40:114: error: 'rpcmethod_SendEvents_' was not declared in this scope\r\n   return new ::grpc::ClientAsyncReaderWriter< ::tensorflow::Event, ::tensorflow::EventReply>(channel_.get(), cq, rpcmethod_SendEvents_, context, tag);\r\n                                                                                                                  ^~~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:40:114: note: suggested alternative: 'WithAsyncMethod_SendEvents'\r\n   return new ::grpc::ClientAsyncReaderWriter< ::tensorflow::Event, ::tensorflow::EventReply>(channel_.get(), cq, rpcmethod_SendEvents_, context, tag);\r\n                                                                                                                  ^~~~~~~~~~~~~~~~~~~~~\r\n                                                                                                                  WithAsyncMethod_SendEvents\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc: In member function 'virtual grpc::Status tensorflow::EventListener::Stub::SendTracebacks(grpc::ClientContext*, const tensorflow::CallTraceback&, tensorflow::EventReply*)':\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:44:18: error: 'BlockingUnaryCall' is not a member of 'grpc'\r\n   return ::grpc::BlockingUnaryCall(channel_.get(), rpcmethod_SendTracebacks_, context, request, response);\r\n                  ^~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:44:18: note: suggested alternative:\r\nIn file included from /usr/include/grpc++/impl/codegen/client_unary_call.h:26,\r\n                 from /dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:11:\r\n/usr/include/grpcpp/impl/codegen/client_unary_call.h:38:8: note:   'grpc::internal::BlockingUnaryCall'\r\n Status BlockingUnaryCall(ChannelInterface* channel, const RpcMethod& method,\r\n        ^~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:44:52: error: 'rpcmethod_SendTracebacks_' was not declared in this scope\r\n   return ::grpc::BlockingUnaryCall(channel_.get(), rpcmethod_SendTracebacks_, context, request, response);\r\n                                                    ^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:44:52: note: suggested alternative: 'WithAsyncMethod_SendTracebacks'\r\n   return ::grpc::BlockingUnaryCall(channel_.get(), rpcmethod_SendTracebacks_, context, request, response);\r\n                                                    ^~~~~~~~~~~~~~~~~~~~~~~~~\r\n                                                    WithAsyncMethod_SendTracebacks\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc: In member function 'virtual grpc::ClientAsyncResponseReader<tensorflow::EventReply>* tensorflow::EventListener::Stub::AsyncSendTracebacksRaw(grpc::ClientContext*, const tensorflow::CallTraceback&, grpc::CompletionQueue*)':\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:48:95: error: 'rpcmethod_SendTracebacks_' was not declared in this scope\r\n   return new ::grpc::ClientAsyncResponseReader< ::tensorflow::EventReply>(channel_.get(), cq, rpcmethod_SendTracebacks_, context, request);\r\n                                                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:48:95: note: suggested alternative: 'WithAsyncMethod_SendTracebacks'\r\n   return new ::grpc::ClientAsyncResponseReader< ::tensorflow::EventReply>(channel_.get(), cq, rpcmethod_SendTracebacks_, context, request);\r\n                                                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~\r\n                                                                                               WithAsyncMethod_SendTracebacks\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:48:138: error: 'static void* grpc::ClientAsyncResponseReader<R>::operator new(std::size_t) [with R = tensorflow::EventReply; std::size_t = long unsigned int]' is private within this context\r\n   return new ::grpc::ClientAsyncResponseReader< ::tensorflow::EventReply>(channel_.get(), cq, rpcmethod_SendTracebacks_, context, request);\r\n                                                                                                                                          ^\r\nIn file included from /usr/include/grpc++/impl/codegen/async_unary_call.h:26,\r\n                 from /dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:26,\r\n                 from /dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:6:\r\n/usr/include/grpcpp/impl/codegen/async_unary_call.h:182:16: note: declared private here\r\n   static void* operator new(std::size_t size);\r\n                ^~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc: In member function 'virtual grpc::Status tensorflow::EventListener::Stub::SendSourceFiles(grpc::ClientContext*, const tensorflow::DebuggedSourceFiles&, tensorflow::EventReply*)':\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:52:18: error: 'BlockingUnaryCall' is not a member of 'grpc'\r\n   return ::grpc::BlockingUnaryCall(channel_.get(), rpcmethod_SendSourceFiles_, context, request, response);\r\n                  ^~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:52:18: note: suggested alternative:\r\nIn file included from /usr/include/grpc++/impl/codegen/client_unary_call.h:26,\r\n                 from /dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:11:\r\n/usr/include/grpcpp/impl/codegen/client_unary_call.h:38:8: note:   'grpc::internal::BlockingUnaryCall'\r\n Status BlockingUnaryCall(ChannelInterface* channel, const RpcMethod& method,\r\n        ^~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:52:52: error: 'rpcmethod_SendSourceFiles_' was not declared in this scope\r\n   return ::grpc::BlockingUnaryCall(channel_.get(), rpcmethod_SendSourceFiles_, context, request, response);\r\n                                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:52:52: note: suggested alternative: 'WithAsyncMethod_SendSourceFiles'\r\n   return ::grpc::BlockingUnaryCall(channel_.get(), rpcmethod_SendSourceFiles_, context, request, response);\r\n                                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                                                    WithAsyncMethod_SendSourceFiles\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc: In member function 'virtual grpc::ClientAsyncResponseReader<tensorflow::EventReply>* tensorflow::EventListener::Stub::AsyncSendSourceFilesRaw(grpc::ClientContext*, const tensorflow::DebuggedSourceFiles&, grpc::CompletionQueue*)':\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:56:95: error: 'rpcmethod_SendSourceFiles_' was not declared in this scope\r\n   return new ::grpc::ClientAsyncResponseReader< ::tensorflow::EventReply>(channel_.get(), cq, rpcmethod_SendSourceFiles_, context, request);\r\n                                                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:56:95: note: suggested alternative: 'WithAsyncMethod_SendSourceFiles'\r\n   return new ::grpc::ClientAsyncResponseReader< ::tensorflow::EventReply>(channel_.get(), cq, rpcmethod_SendSourceFiles_, context, request);\r\n                                                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                                                                                               WithAsyncMethod_SendSourceFiles\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:56:139: error: 'static void* grpc::ClientAsyncResponseReader<R>::operator new(std::size_t) [with R = tensorflow::EventReply; std::size_t = long unsigned int]' is private within this context\r\n   return new ::grpc::ClientAsyncResponseReader< ::tensorflow::EventReply>(channel_.get(), cq, rpcmethod_SendSourceFiles_, context, request);\r\n                                                                                                                                           ^\r\nIn file included from /usr/include/grpc++/impl/codegen/async_unary_call.h:26,\r\n                 from /dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.h:26,\r\n                 from /dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:6:\r\n/usr/include/grpcpp/impl/codegen/async_unary_call.h:182:16: note: declared private here\r\n   static void* operator new(std::size_t size);\r\n                ^~~~~~~~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc: In constructor 'tensorflow::EventListener::Service::Service()':\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:60:17: error: expected type-specifier before '::' token\r\n   AddMethod(new ::grpc::RpcServiceMethod(\r\n                 ^~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:65:17: error: expected type-specifier before '::' token\r\n   AddMethod(new ::grpc::RpcServiceMethod(\r\n                 ^~\r\n/dev/shm/tensorflow.orig/tensorflow/contrib/cmake/build/tensorflow/core/debug/debug_service.grpc.pb.cc:70:17: error: expected type-specifier before '::' token\r\n   AddMethod(new ::grpc::RpcServiceMethod(\r\n                 ^~\r\n```\r\n\r\nBuilding with 88e560d6fadc1cf23519b00a9de5ed7c973536fd on Debian unstable, libgrpc++-dev version is `1.13.0-1`\r\n\r\nflags:\r\n```\r\ncmake .. -DCMAKE_BUILD_TYPE=Release \\\r\n\t\t\t -Dtensorflow_VERBOSE=OFF \\\r\n\t\t\t -Dtensorflow_ENABLE_SSL_SUPPORT=ON \\\r\n\t\t\t -Dtensorflow_ENABLE_GRPC_SUPPORT=ON \\\r\n\t\t\t -Dtensorflow_ENABLE_HDFS_SUPPORT=OFF \\\r\n\t\t\t -Dtensorflow_ENABLE_JEMALLOC_SUPPORT=OFF \\\r\n\t\t\t -Dtensorflow_BUILD_CC_EXAMPLE=ON \\\r\n\t\t\t -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF \\\r\n\t\t\t -Dtensorflow_BUILD_ALL_KERNELS=ON \\\r\n\t\t\t -Dtensorflow_BUILD_CONTRIB_KERNELS=ON \\\r\n\t\t\t -Dtensorflow_BUILD_CC_TESTS=OFF \\\r\n\t\t\t -Dtensorflow_BUILD_PYTHON_TESTS=OFF \\\r\n\t\t\t -Dtensorflow_BUILD_MORE_PYTHON_TESTS=OFF \\\r\n\t\t\t -Dtensorflow_BUILD_SHARED_LIB=ON \\\r\n\t\t\t -Dtensorflow_OPTIMIZE_FOR_NATIVE_ARCH=OFF \\\r\n\t\t\t -Dtensorflow_ENABLE_SNAPPY_SUPPORT=ON \\\r\n\t\t\t -Dtensorflow_DISABLE_EIGEN_FORCEINLINE=OFF \\\r\n\t\t\t -Dtensorflow_WIN_CPU_SIMD_OPTIONS=OFF \\\r\n\t\t\t -Dtensorflow_ENABLE_MKL_SUPPORT=OFF \\\r\n\t\t\t -Dtensorflow_ENABLE_MKLDNN_SUPPORT=OFF \\\r\n\t\t\t -Dtensorflow_ENABLE_GPU=OFF \\\r\n\t\t\t -Dtensorflow_ENABLE_POSITION_INDEPENDENT_CODE=ON \\\r\n\t\t\t -Dsystemlib_ZLIB=ON \\\r\n\t\t\t -Dsystemlib_GRPC=ON \\\r\n\t\t\t -DPYTHON_EXECUTABLE=/usr/bin/python3\r\n```", "comments": ["After installing a temporary `libgrpc++-dev 1.13.1` package, this patch is working.", "I'm about to add more similar patches that add `systemlib_XXX` options. Should I submit as different PRs, or continue pushing my commits to this PR?\r\n\r\nWhen all the external dependencies can be switched to use the system provided one, I'm able to make the Debian package.", "@cdluminate Multiple small PRs will be much easier to review and merge, thanks!", "@mrry I see. Then please merge this if acceptable.", "It looks like the Windows CMake build failed due to unresolved symbols. Can you please take a look?\r\n\r\nhttps://source.cloud.google.com/results/invocations/49294506-d799-4c6c-a6ea-e0bed66400c2/log", "@mrry I looked into it but I don't understand how it fails to build.\r\n\r\nThis PR adds an option that is not enabled by default, and the option is tested by me on Debian unstable. Indeed this option may not work on windows, but when we look at the cmake options from build log:\r\n\r\n```\r\nT:\\src\\github\\tensorflow\\cmake_build>\"C:\\Program Files\\cmake\\bin\\cmake.exe\" T:\\src\\github\\tensorflow\\tensorflow\\contrib\\cmake -A x64 -DSWIG_EXECUTABLE=C:\\swig\\swig.exe -DPYTHON_EXECUTABLE=\"C:\\Python35\\python.exe\" -DCMAKE_BUILD_TYPE=Release -DPYTHON_LIBRARIES=\"C:\\Python35\\libs\\python35.lib\" -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_CC_TESTS=OFF -Dtensorflow_TF_NIGHTLY=OFF -Dtensorflow_DISABLE_EIGEN_FORCEINLINE=\"ON\" -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX \r\n```\r\n\r\nWe can find out that neither `-Dsystemlib_GRPC` nor `-Dsystemlib_ALL` is defined. In the rest of the build log, GRPC is cloned and built from scratch, that means the cmake part guarded by `systemlib_GRPC == ON` is not the cause of build failure.", "running again. I'm assuming that if it passes, then we can ignore @mrry comment, otherwise we will need the author to make that change.", "@cdluminate could you take a look?\r\nhttps://source.cloud.google.com/results/invocations/b0d09fcf-b1fb-4e52-aba7-e246ac0c9bcf/targets/%2F%2Ftensorflow%2Ftools%2Fci_build%2Fbuilds:gen_win_out/log", "@drpngx @mrry I'm working on another cmake patchset for building TF on debian unstable. The suggested changes will be made in the mentioned patchset. I'll submit the patchset as another PR soon. At that time this PR will be replaced.", "Sounds good, thanks!", "A new WIP PR https://github.com/tensorflow/tensorflow/pull/21699 has been proposed to replace this one.", "Nagging Assignee @caisq: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this because it has been superseded by #21699."]}, {"number": 21030, "title": "Real time object detection api using tensorflow in android BUT where to change the NDK and SDK version on workspace?", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: none\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**:  3.5.1-32 bit\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: nothing since i have AMD radeon\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n### Describe the problem\r\nIm following this tutorial [https://www.skcript.com/svr/realtime-object-and-face-detection-in-android-using-tensorflow-object-detection-api/](url) since i want to make a realtime detection using tensorflow and when i got to the step where changing the SDK and NDK version in tensorflow workspace file, i couldnt find similar codes like `Uncomment and update the paths in these entries to build the Android demo.\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 23,\r\n    build_tools_version = \"25.0.1\",\r\n     Replace with path to Android SDK on your system\r\n    path = \"<PATH_TO_SDK>\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n   path=\"<PATH_TO_NDK>\",\r\n    api_level=14)` im not sure how the new implementation on android works since it keeps on changing the repository for tensorflow. should i just ignore changing the versions and continue following the steps after it?\r\n\r\n", "comments": ["@achowdhery can answer, but probably WORKSPACE or .bazelrc would probably work.\r\n", "@inakaaay For Bazel build, this is done by running ./configure in Tensorflow directory. \r\nAlternately we also provide a Dockerfile that pre-configures this. https://github.com/tensorflow/models/blob/master/research/object_detection/dockerfiles/android/Dockerfile", "Nagging Assignee @achowdhery: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@inakaaay Did this issue get resolved? Is it still open?"]}, {"number": 21029, "title": "TF 1.8-gpu runs on only one Ubuntu 18.04 account", "body": "About six months ago I succeeded in installing TF 1.4.0-gpu on top of Ubuntu 17.10.  I have multiple user accounts on the system, and I needed all accounts to have TF access.  TF needed to be accessible whether the Python script was launched from a terminal or from a GUI.  The procedure that worked for me was documented [here](https://ubuntuforums.org/showthread.php?t=2386704).\r\n\r\nI decided to upgrade to Ubuntu 18.04 and TF 1.8.0-gpu.  The installation procedure I followed was essentially the same as shown above.  My primary account has full access to TF, and is functioning normally.  However, the secondary account (which has admin privileges) is unable to load TF.  I get this:\r\n\r\n> Python 3.6.5 (default, Apr  1 2018, 05:46:30) \r\n> [GCC 7.3.0] on linux\r\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n> \u02c3\u02c3\u02c3 import tensorflow as tf\r\n> Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import *\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n\r\nI searched for libcublas.so* files on my system.  The screenshot of the search result is attached.  I don't have 8.0 files.  I do have 9.0 and 9.1 files, I'm not sure how I ended up with both.  I have what appear to be manual pages with a version 7 suffix.\r\n\r\nMy main account is not looking for libcublas.so.8.0.  My secondary account is.  Any thoughts as to why?  I reviewed my .bashrc and .profile in each account for discrepancies.  I have none.  The modifications that I make to /etc/profile and ldconfig should be shared between all accounts on the system.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "The \"TensorFlow Butler\" prompt made me check the issue template.  I tried a `pip3 list` in both my active accounts.  The working account showed TF 1.8.0.  The non-working account showed TF 1.4.0, which must have been a hold-over from my Ubuntu 17.10 configuration.  Uninstalling TF 1.4 and then executing `sudo pip3 install tensorflow-gpu==1.8` from the secondary account got me working again.\r\n\r\nPerhaps this is an issue with `pip`, and not with TensorFlow itself?  I thought using `sudo` with `pip3` would install a package for all users.  Perhaps it does not.  Feel free to close this issue if you think there's nothing more to discuss.", "This looks to be an issue with pip.\r\nDid you try \"sudo pip....\" That lets you change system libraries, rather than only your own.\r\nMoreover, it looks like you need to set LD_LIBRARY_PATH for all accounts on your system.\r\nYou can try looking at this:\r\nhttps://www.google.com/search?q=LD_LIBRARY_PATH+ubuntu+system&oq=LD_LIBRARY_PATH+ubuntu+system&aqs=chrome..69i64j0l2j69i57.6390j0j7&sourceid=chrome&ie=UTF-8\r\n\r\nClosing, as this is a pip and CUDA installation issue."]}, {"number": 21028, "title": "Fixed typo in TOCO command line doc", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Submitted a CLA request.", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 21027, "title": "Update Go README with working bazel config flag", "body": "The bazel `--config` flag wasn't working. Had to use `-c`. \r\n\r\nhttps://github.com/tensorflow/serving/issues/517", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 21026, "title": "tried to convert t to a tensor and failed error none values not supported", "body": "When I want to set a trainable variable, and use \r\n```\r\nalpha = tf.Variable(initial_value=1.0)\r\nbeta= tf.Variable(initial_value=1.0)\r\n\r\nloss = alpha*loss_1 + beta*loss_2\r\n```\r\nand use it with a gradient clip, problem happened,\r\nerrors reported like: \r\n```\r\ntried to convert t to a tensor and failed error none values not supported\r\n```\r\nWhat should I do?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Do you have a code snippet?", "Thanks, I have solved this problem by using \r\n```\r\nalpha = tf.Variable(initial_value=tf.Constant(1.0))\r\nbeta= tf.Variable(initial_value=tf.Constant(1.0))\r\nloss = alpha*loss_1 + beta*loss_2\r\n``` "]}, {"number": 21025, "title": "Add C++ gradient for Cast", "body": "- add ops (Ceil, Round and Rint) that return zero gradients\r\n- minor formatting fixes from clang-format\r\n\r\nTested for flakes with:\r\n```\r\n$ bazel test --runs_per_test=100 tensorflow/cc:gradients_math_grad_test\r\n...\r\n//tensorflow/cc:gradients_math_grad_test                                 PASSED in 1.6s\r\n  Stats over 100 runs: max = 1.6s, min = 0.6s, avg = 1.3s, dev = 0.1s\r\n```\r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/14081", "comments": ["@kbsriram can you please resolve conflicts", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]