[{"number": 27648, "title": "Fix an issue in restoring ParallelMapIterator and add tests for ParallelMapDataset ", "body": "This PR fixes two issues and adds tests for `ParallelMapDataset`:\r\n\r\n- https://github.com/tensorflow/tensorflow/commit/ecece7f8976c4da355443d998f5773a0598314a9: Clear the invocation results in the buffer before restoring `ParallelMapIterator`\r\n\r\n- https://github.com/tensorflow/tensorflow/commit/098d891c6d8f3d9d65287efd27a2b47b304b8217: Add tests for `ParallelMapDataset`\r\n\r\n- https://github.com/tensorflow/tensorflow/commit/f254f5c1c4b76287866a68d607664eccead1551f: Change the name of `thread_pool_` from `inter_op` to `intra_op_thread_op`. \r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa \r\n\r\nWhen testing `ParallelMapDataset`, there is a timeout issue for [TestCase7](https://github.com/tensorflow/tensorflow/commit/098d891c6d8f3d9d65287efd27a2b47b304b8217#diff-d1771146ec6106d05f61244c9c8baa81R220). The detailed log is posted [here](https://gist.github.com/feihugis/7c9153d75efd5a000026a7e33a036a39).\r\n\r\nIt seems to be caused by more tasks than threads, which means big `num_parallel_calls` but small `thread_num` of `thread_pool_`. Note that the timeout case uses `use_inter_op_parallelism=false` and `MapFunc = XTimesFour`.\r\n\r\n- When `num_parallel_calls=2` and `thread_num=2`, the test times out ([log](https://gist.github.com/feihugis/7c9153d75efd5a000026a7e33a036a39));\r\n\r\n- When `num_parallel_calls=1` and `thread_num=2`, the test works well ([log](https://gist.github.com/feihugis/d00c9409a34bb2a00f4cd2a4d7255da4)).\r\n\r\nWhen changing `MapFunc = XTimesTwo`, both the above two cases work well.\r\n\r\nI tried debugging to figure out how it happens. It seems to be related to the executor running multiple frames asynchronously\uff0c but I did not find the root cause yet. Do you have any ideas or suggestions?\r\n\r\n", "@feihugis can you please check failed build errors ?", "@rthadur The failures seem to be unrelated:\r\n\r\n- `Linux GPU`: caused by `distribute:multi_worker_test_gpu`\r\n\r\n- `Ubuntu contrib`: caused by `//tensorflow/contrib/cudnn_rnn:cudnn_rnn_ops_test`\r\n\r\n- `Windows Bazel`: caused by `//tensorflow/cc:array_ops_genrule`\r\n\r\n- `Windows Bazel GPU`: caused by `//tensorflow/contrib/resampler:python/ops/_resampler_ops_gpu`\r\n\r\nThe logs for `Android Demo App` and `MacOS Contrib` cannot be accessed. ", "> @rthadur The failures seem to be unrelated:\r\n> \r\n> * `Linux GPU`: caused by `distribute:multi_worker_test_gpu`\r\n> * `Ubuntu contrib`: caused by `//tensorflow/contrib/cudnn_rnn:cudnn_rnn_ops_test`\r\n> * `Windows Bazel`: caused by `//tensorflow/cc:array_ops_genrule`\r\n> * `Windows Bazel GPU`: caused by `//tensorflow/contrib/resampler:python/ops/_resampler_ops_gpu`\r\n> \r\n> The logs for `Android Demo App` and `MacOS Contrib` cannot be accessed.\r\n\r\nGoogle internal checks FAILED. Could you help paste the logs here?", "The failing tests are unrelated. @rthadur can you please re-trigger the internal submission."]}, {"number": 27647, "title": "Kubernetes Charts for Tensorflow 2.0", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWould be nice to have Kubernetes charts which users can deploy and train or server the tf models. \r\n\r\n**Will this change the current API? How?**\r\nNO, but will help users.\r\n\r\n**Who will benefit with this feature?**\r\nThose who are using K8s already...\r\n\r\nWould need 2 charts:\r\n1. CPU Version TF-2.0\r\n2. GPU Version TF-2.0", "comments": ["The right place for this would likely be tensorflow/ecosystem. ", "@ChethanUK,\r\nCan you please refer to the documentation of [Tensorflow's ML Pipeline using Kubeflow](https://www.tensorflow.org/tfx/guide/kubeflow), this Tutorial on [How to Run Tensorflow Pipeline on Kubeflow](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines#7_run_your_first_tfx_pipeline_on_kubeflow) and the [documentation of Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/overview/pipelines-overview/) and let us know if this is what you are looking for?\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 27646, "title": "Training on device with TensorFlow Lite", "body": "This ticket tracks supporting on-device training with TensorFlow Lite. \r\n\r\nA few high-level milestones:\r\n\r\n- [ ] Functional prototype for basic training (e.g. convolution / dense network)\r\n- [ ] Functional prototype for training with control flow (e.g. RNN / LSTM networks)\r\n- [ ] Productionize / optimize basic training\r\n- [ ] Productionize / optimize training with control flow\r\n\r\nSee also #17328", "comments": ["@miaout17 , i will be very interested in this work, please let me know if you can offload some of the work to me for this.\r\n\r\nRegards\r\nAmit", "@miaout17 yes I would also want to know if in the future (near future maybe?) we could have training on the device with TF Lite\r\n\r\nkeep up the good work", "Any news about this feature ?\r\nI'm wondering as it is part of the TFlite 2019 roadmap https://www.tensorflow.org/lite/guide/roadmap", "@miaout17 : Waiting for good news? ", "This is a usefull link to implement federated learning on android: https://vision-air.github.io/federated.html", "@miaout17 \r\nAny news on this? I'm trying to create a product that basically relies on this as part of it's USP since I want users to be able to create their own gestures.", "Any updates on supporting trianing on Android? Especially with GPU support. Thanks!", "> This is a usefull link to implement federated learning on android: https://vision-air.github.io/federated.html\r\n\r\n_might be too greedy, but have you found any opensource code from them or similar ones?\r\nThanks:)_\r\n\r\nEdited:\r\n\r\nFor others who need this, seems it has been solved by:\r\nhttps://github.com/tensorflow/community/pull/124", "@miaout17 ,\r\n\r\nTensorFlow Lite now supports training your models on-device, in addition to running inference. On-device training enables interesting personalization use cases where models can be fine-tuned based on user needs.\r\n\r\nFor more details please refer [this article](https://blog.tensorflow.org/2021/11/on-device-training-in-tensorflow-lite.html). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 27645, "title": "Fix symlinks in lib_package", "body": "As of https://github.com/tensorflow/tensorflow/pull/27493, lib_package\r\nhad the libraries three times instead of symlinks. This uses pkg_tar's\r\nsymlink arg to set things correctly.\r\n\r\nAlso define VERSION_MAJOR for cleanliness and bazel renamed the 'files'\r\narg in pkg_tar to 'srcs'.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["Also relevant: #27556", "@perfinion  can you please address build failures.", "@rthadur fixed the buildifier failure. The windows GPU fail seems unrelated. :)", "> @rthadur fixed the buildifier failure. The windows GPU fail seems unrelated. :)\r\n\r\nThank you "]}, {"number": 27644, "title": "[Feature Request] Need for manual parallelization of a tensorflow graph (or function)", "body": "- TensorFlow version (you are using): 2.0\r\n\r\n**Describe the feature and the current behavior/state.**\r\nToday, there's no easy way to parallelize a single tf graph (or function in 2.0) to different inputs. tf.map_fn doesn't parallelize and tf.while_loop doesn't generate error when the graph is run sequentially. It's strange really what the keyword parallel_iterations even does in tf.while_loop\r\n\r\n**Will this change the current api? How?**\r\nI request a feature named tf.prange (parallel_range) or something equivalent which can be used inside tf.function, as shown below,\r\n\r\n     @tf.function\r\n     def foo():\r\n          for i in tf.prange(100):\r\n                 ...\r\n\r\nwhich will help us to get an error if an op depends on the values of previous flows in a function. Or some other solution to parallelize and raise error in case of no parallelization. As of now, everything happens in the backend and it's difficult to find the problems that lead to such sequential execution of tf graph.\r\n\r\n**Who will benefit with this feature?**\r\nCoders wanting to manually parallelize a particular graph to multiple inputs. (Potentially every ML developer with minimal resources)\r\n\r\nP.S. This would make things extremely convenient.\r\nhttps://github.com/tensorflow/tensorflow/issues/24774 and https://github.com/tensorflow/tensorflow/issues/1984 also state how inconvenient manual parallelization is in tensorflow since quite some time.\r\n\r\nAn exact same issue has been raise before. https://github.com/tensorflow/tensorflow/issues/12492", "comments": ["@caissalover i would love to work on this, can you help me out?", "> @caissalover i would love to work on this, can you help me out?\r\n\r\nAman, you're on your own here, I cannot contribute now for the moment. I'm just a user and write high level code for prototyping using tf. You can seek help of the managers of tf. I have stated the problem we've faced and hint of a solution clearly so the creators solve it, possibly you! :D", "@agarwal-ashish I think this can be implemented somehow with autograph and pfor.\r\n\r\nAssigning to you for triage.", "@caissalover, regarding semantics of parallel_iterations in while_loop, conceptually it is unrolling up to that many loop iterations at any instant and then trying to execute it like a sequential graph. This allows up to that many iterations to run in parallel, but if there are dependencies across iterations, those dependencies are executed sequentially, while still trying to run other parts in parallel.\r\n\r\npfor, under python/ops/parallel_for, uses SPMD semantics for dealing with state accesses across iterations. Hence the semantics are not the same as running all iterations in parallel and bailing out if there are dependencies.\r\n\r\nIf you wrote your own while_loop, than you'd be responsible for checking what loop carried variables you used. Another option is to let autograph convert code with tf.range, and then inspecting the generated graph to check for loop carried state.\r\n", "@caissalover , could you give us a concrete example to make sure we're on the same page about the idea?\r\nI think there are multiple ways in which it can be understood, and my take is annotating the loop somehow to error out if it cannot be trivially parallelized.\r\n\r\nThat would mean, this loop is trivially parallelizable:\r\n\r\n```\r\nta = tf.TensorArray(...)\r\nfor i in tf.range(n):\r\n  tf.autograph.set_loop_options(force_parallel=True)  # Ok\r\n  ta = ta.write(i, ta.read(i) + 1)\r\n```\r\n\r\nWhereas this is not:\r\n\r\n```\r\nta = tf.TensorArray(...)\r\nfor i in tf.range(n):\r\n  tf.autograph.set_loop_options(force_parallel=True)  # Raises an error: ta[i] depends on ta[i-1] - cannot run in parallel\r\n  ta = ta.write(i, ta.read(i - 1) + 1)\r\n```\r\n\r\nDoes this match your suggestion?", "Yes, exactly @mdanatg . So if an error is raised in 2nd case while force_parallel, we'll be able to achieve desired speed more easily by changing the faulty code. I hope that the above syntax structure will not execute all the loops in parallel but just the one here. Otherwise nested loops can be a problem.", "Actually, it seems that `map_fn` is already optimized and does parallelize. @caissalover, have you run any tests that show no performance change when increasing parallel_iterations from 1 to a larger value?", "@mdanatg No, that is not true. Just now I ran my graph at parallel_iterations=1 and 114, and my GPU utilization and time of running the graph didn't change much. It is not optimized.", "Right, scratch that last note, it was referring to a sentence in the original post.\r\n\r\n@agarwal-ashish , could we use the bailing-out logic that you have in pfor and raise an error when this directive was present? AutoGraph can detect it, but statically checking for loop-carried state would be a more involved effort which we wouldn't have time to work on right away.", "@mdanatg, as mentioned earlier in the thread, pfor has SPMD semantics, not strict sequential semantics. Hence it may not raise errors with concurrent state accesses across iterations.", "It turns out that automatically parallelizing arbitrary code is a topic very much open to research and we're unlikely to have a robust general solution any time soon.\r\n\r\nThat said, APIs like `tf.map_fn`, `tf.while_loop`, `tf.data` and `tf.distribute` are quite aggressive in parallelizing loops, even when it might not appear that they aren't. Ultimately it becomes an engineering task to profile and fine tune the code where performance is critical, and given current advancements, that is likely to remain the case for all but very specific applications.\r\n"]}, {"number": 27643, "title": "Java: add support for Alpine Linux (Docker)", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm using the TF Java library to serve prediction requests and I don't have any issue there.\r\nHowever, I'm building a docker image from my server. The size is 343mb from openjdk:8-jdk-alpine, but TF does not work (Error loading shared library ld-linux-x86-64.so.2).\r\nI noticed in the doc that only Ubuntu was supported. Ok.\r\nI created the image from ubuntu:18.04 and installed the jdk over it... no more error as expected, but the size of the image jumps to 500m :(, even after cleaning out everything I could.\r\n\r\nAny plans to make it work from the openjdk:8-jdk-alpine image (or provide a minimalist dockerfile which inherits from that one) ?\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nA lot, I can imagine\r\n\r\n**Any Other info.**\r\n", "comments": ["@neterium Could you check whether the issue persists with latest TF( 1.13.1 and/or 2.0)? Thanks!", "Just tried (1.13.1) , same error:\r\n\r\n> Caused by: java.lang.UnsatisfiedLinkError: /tmp/tensorflow_native_libraries-1555007765207-0/libtensorflow_jni.so: Error loading shared library ld-linux-x86-64.so.2: No such file or directory (needed by /tmp/tensorflow_native_libraries-1555007765207-0/libtensorflow_jni.so)\r\n>         at java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\n>         at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)\r\n>         at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)\r\n>         at java.lang.Runtime.load0(Runtime.java:809)\r\n>         at java.lang.System.load(System.java:1086)\r\n>         at org.tensorflow.NativeLibrary.load(NativeLibrary.java:101)\r\n>         at org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n>         at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)\r\n>         at org.tensorflow.SavedModelBundle.<clinit>(SavedModelBundle.java:170)\r\n> ", "We do not have any plans to support Alpine Linux in our Docker containers. However, we welcome contributions to the tensorflow/tools/dockerfiles directory.", "[This StackOverflow thread](https://stackoverflow.com/questions/50288034/unsatisfiedlinkerror-tmp-snappy-1-1-4-libsnappyjava-so-error-loading-shared-li/51655643#51655643) might be helpful", "@neterium ,\r\nCan you please check this [comment](https://github.com/tensorflow/tensorflow/issues/27643#issuecomment-519159209) and let us know if it helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27643\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27643\">No</a>\n"]}, {"number": 27642, "title": "close", "body": "", "comments": ["It's there, just in a different namespace.  I think in tf.rnn.  I believe\nkeras lstm also provides similar functionally with mask arguments.\n\nOn Thu, Apr 11, 2019, 8:54 PM HuangZhanPeng <notifications@github.com>\nwrote:\n\n> Once I have ask question following:\n> how to implement the api in pytorch with tensorflow 2.0There is two api in\n> pytorch, one is \"nn.utils.rnn.pack_padded_sequence\", other is\n> \"pad_packed_sequence\", I want to ask is there any api or implement to\n> achieve the similar function of pytorch in tensorflow 2.0, thank you~~\n>\n> And @ebrevdo <https://github.com/ebrevdo> give me the solution:\n> If you call tf.nn.{dynamic_,}rnn with sequence_length vector provided and\n> use padded inputs, it will behave the same as if you used pytorch's\n> pack_padded_sequence. Specifically, the final state will properly contain\n> the state of the final step for each batch, and the output values in a\n> batch row b corresponding to time points after sequence_length[b] will be\n> zeroed out.\n>\n> but in TensorFlow 2.0, now there is no tf.nn.{dynamic_,}rnn, how to deal\n> with variable length?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27642>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-NesOybgSdZRpgZX5ZswtxXa5zVks5vgAOOgaJpZM4ciUAC>\n> .\n>\n", "I think this should go to StackOverflow as it is not related to a bug or feature request in TensorFlow.", "@mihaimaruseac I think that is a huge api change, but tf 2.0 did not provide analogous api, I still insist that may be a feature request in TensorFlow, that is the reason I do not ask on StackOverflow", "@ebrevdo [#26974](https://github.com/tensorflow/tensorflow/issues/26974), Yes, there is similar function to achieve that, Although this does not really handle the variable sequence length. And in tf 2.0 may be there is no `tf.rnn` apis.\r\n\r\nMay be in the Effective [TensorFlow 2.0 tutorial](https://www.tensorflow.org/alpha/guide/effective_tf2), it can be soluted with the code sample:\r\n\r\n```\r\nclass DynamicRNN(tf.keras.Model):\r\n\r\n  def __init__(self, rnn_cell):\r\n    super(DynamicRNN, self).__init__(self)\r\n    self.cell = rnn_cell\r\n\r\n  def call(self, input_data):\r\n    # [batch, time, features] -> [time, batch, features]\r\n    input_data = tf.transpose(input_data, [1, 0, 2])\r\n    outputs = tf.TensorArray(tf.float32, input_data.shape[0])\r\n    state = self.cell.zero_state(input_data.shape[1], dtype=tf.float32)\r\n    for i in tf.range(input_data.shape[0]):\r\n      output, state = self.cell(input_data[i], state)\r\n      outputs = outputs.write(i, output)\r\n    return tf.transpose(outputs.stack(), [1, 0, 2]), state\r\n```", "Side note: why did you edit the title and descriptions? You know GitHub stores all edits and anyone can see them, right? It just causes hassles when trying to get back the context"]}, {"number": 27641, "title": "[doc/keras] incorrect comment in `__init__` of `tf.keras.layers.AveragePooling1D` #27633", "body": "#27633 ", "comments": ["Linking [to this comment](https://github.com/tensorflow/tensorflow/pull/27637#issuecomment-480879996) to make PRs look clean and easier to review and merge. Please follow", "@mihaimaruseac  now I have corrected it.", "#27633 also mentions `input_shape` and `output_shape` lists being badly formatted. Can you please make a PR for those too (separate from this one and not including all of these unrelated changes)?", " Input shape:\r\n    - If `data_format='channels_last'`:\r\n      3D tensor with shape `(batch_size, steps, features)`.\r\n    - If `data_format='channels_first'`:\r\n      3D tensor with shape `(batch_size, features, steps)`.\r\n\r\n  Output shape:\r\n    - If `data_format='channels_last'`:\r\n      3D tensor with shape `(batch_size, downsampled_steps, features)`.\r\n    - If `data_format='channels_first'`:\r\n      3D tensor with shape `(batch_size, features, downsampled_steps)`.\r\n@mihaimaruseac  so you want me to edit this input and output shape description in average_pooling function.", "In a __different PR__, if you do it. If you don't do it, that's also ok, just don't close #27633` then.", "You will have to resolve conflicts before this can be merged.", "@mihaimaruseac  removed the merge conflicts.", "ok let's close this PR", "Please reopen from a new fork, as a clean PR."]}, {"number": 27640, "title": "TF Lite conversion of minimal graph with tf.matmul fails on Linux but works on MacOS", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n`16.04.6 LTS (GNU/Linux 4.15.0-47-generic x86_64)` and `macOS Mojave Version 10.14.4`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): `pip install tf-nightly`\r\n- TensorFlow version (use command below): `1.14.1-dev20190408`\r\n- Python version: `2.7.16` (Mac) and `2.7.12` (Linux)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThe example code below creates a minimal TensorFlow graph that computes a `tf.matmul` between two input matrices and exports the graph to TensorFlow Lite from the current session via the Python API. It invokes the TF Lite Interpreter on example input and compares the output to the result of `session.run`.\r\n\r\nThe code works on Mac, but fails on Linux during TF Lite conversion (see logs below).\r\n\r\n**Describe the expected behavior**\r\nIt should work (or at least behave the same) on both operating systems.\r\n\r\nI know that the [TF Lite Operator Compatibility](https://www.tensorflow.org/lite/guide/ops_compatibility#compatible_operations) states:\r\n\r\n> tf.matmul - as long as the second argument is constant and transposition is not used\r\n\r\nThis is not the case here, but it still curious that it works on Mac.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy\r\nimport tensorflow as tf\r\n\r\ndef export_tflite_from_session(session, input_nodes, output_nodes, tflite_filename):\r\n    print(\"Converting to tflite...\")\r\n    converter = tf.lite.TFLiteConverter.from_session(session, input_nodes, output_nodes)\r\n    tflite_model = converter.convert()\r\n    with open(tflite_filename, \"wb\") as f:\r\n        f.write(tflite_model)\r\n    print(\"Converted %s.\" % tflite_filename)\r\n\r\ndef test_tflite_model(tflite_filename, examples):\r\n    print(\"Loading TFLite interpreter for %s...\" % tflite_filename)\r\n    interpreter = tf.lite.Interpreter(model_path=tflite_filename)\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    print(\"input details: %s\" % input_details)\r\n    print(\"output details: %s\" % output_details)\r\n\r\n    for i, input_tensor in enumerate(input_details):\r\n        interpreter.set_tensor(input_tensor['index'], examples[i])\r\n    interpreter.invoke()\r\n    model_output = []\r\n    for i, output_tensor in enumerate(output_details):\r\n        model_output.append(interpreter.get_tensor(output_tensor['index']))\r\n    return model_output\r\n\r\ndef main():\r\n    tflite_filename = \"model.tflite\"\r\n    shape_a = (2, 3, 4)\r\n    shape_b = (2, 4, 5)\r\n\r\n    a = tf.placeholder(dtype=tf.float32, shape=shape_a, name=\"A\")\r\n    b = tf.placeholder(dtype=tf.float32, shape=shape_b, name=\"B\")\r\n    c = tf.matmul(a, b, name=\"output\")\r\n\r\n    numpy.random.seed(1234)\r\n    a_ = numpy.random.rand(*shape_a).astype(numpy.float32)\r\n    b_ = numpy.random.rand(*shape_b).astype(numpy.float32)\r\n    with tf.Session() as session:\r\n        session_output = session.run(c, feed_dict={a: a_, b: b_})\r\n        export_tflite_from_session(session, [a, b], [c], tflite_filename)\r\n\r\n    tflite_output = test_tflite_model(tflite_filename, [a_, b_])\r\n    tflite_output = tflite_output[0]\r\n\r\n    print(\"Input example:\")\r\n    print(a_)\r\n    print(a_.shape)\r\n    print(b_)\r\n    print(b_.shape)\r\n    print(\"Session output:\")\r\n    print(session_output)\r\n    print(session_output.shape)\r\n    print(\"TFLite output:\")\r\n    print(tflite_output)\r\n    print(tflite_output.shape)\r\n    print(numpy.allclose(session_output, tflite_output))\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\nOutput on Mac:\r\n```\r\n2019-04-08 14:46:05.835019: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nConverting to tflite...\r\n2019-04-08 14:46:05.837757: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-04-08 14:46:05.837803: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-04-08 14:46:05.839940: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-04-08 14:46:05.839979: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\nConverted model.tflite.\r\nLoading TFLite interpreter for model.tflite...\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninput details: [{'index': 0, 'shape': array([2, 3, 4], dtype=int32), 'quantization': (0.0, 0L), 'name': 'A', 'dtype': <type 'numpy.float32'>}, {'index': 1, 'shape': array([2, 4, 5], dtype=int32), 'quantization': (0.0, 0L), 'name': 'B', 'dtype': <type 'numpy.float32'>}]\r\noutput details: [{'index': 2, 'shape': array([2, 3, 5], dtype=int32), 'quantization': (0.0, 0L), 'name': 'output', 'dtype': <type 'numpy.float32'>}]\r\nInput example:\r\n[[[0.19151945 0.62210876 0.43772775 0.7853586 ]\r\n  [0.77997583 0.2725926  0.27646425 0.8018722 ]\r\n  [0.95813936 0.87593263 0.35781726 0.5009951 ]]\r\n\r\n [[0.6834629  0.71270204 0.37025076 0.5611962 ]\r\n  [0.50308317 0.01376845 0.7728266  0.8826412 ]\r\n  [0.364886   0.6153962  0.07538124 0.368824  ]]]\r\n(2, 3, 4)\r\n[[[0.9331401  0.65137815 0.39720258 0.78873014 0.31683612]\r\n  [0.56809866 0.8691274  0.4361734  0.8021476  0.14376682]\r\n  [0.70426095 0.7045813  0.21879211 0.92486763 0.44214076]\r\n  [0.90931594 0.05980922 0.18428709 0.04735528 0.6748809 ]]\r\n\r\n [[0.59462476 0.5333102  0.04332406 0.5614331  0.32966843]\r\n  [0.5029668  0.11189432 0.6071937  0.5659447  0.00676406]\r\n  [0.6174417  0.9121229  0.7905241  0.99208146 0.95880175]\r\n  [0.7919641  0.28525096 0.62491673 0.4780938  0.19567518]]]\r\n(2, 4, 5)\r\nSession output:\r\n[[[1.5545473  1.0208298  0.58792216 1.0921113  0.87367964]\r\n  [1.8065444  0.98772776 0.636969   1.1275158  0.94971865]\r\n  [2.0992541  1.6674836  0.93324846 1.812999   0.9258208 ]]\r\n\r\n [[1.437925   0.942041   1.1057518  1.422692   0.69494617]\r\n  [1.4822663  1.226527   1.1926711  1.4789319  1.0796423 ]\r\n  [0.86513305 0.43742114 0.679548   0.8042561  0.26889932]]]\r\n(2, 3, 5)\r\nTFLite output:\r\n[[[1.5545473  1.0208298  0.58792216 1.0921113  0.87367964]\r\n  [1.8065444  0.98772776 0.636969   1.1275158  0.94971865]\r\n  [2.0992541  1.6674836  0.93324846 1.812999   0.9258208 ]]\r\n\r\n [[1.437925   0.942041   1.1057518  1.422692   0.69494617]\r\n  [1.4822663  1.226527   1.1926711  1.4789319  1.0796423 ]\r\n  [0.86513305 0.43742114 0.679548   0.8042561  0.26889932]]]\r\n(2, 3, 5)\r\nTrue\r\n```\r\n\r\nOutput on Linux:\r\n```\r\n2019-04-08 14:47:09.730317: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-04-08 14:47:09.734305: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-04-08 14:47:10.718760: E tensorflow/stream_executor/cuda/cuda_driver.cc:320] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2019-04-08 14:47:10.718805: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:166] retrieving CUDA diagnostic information for host: everest6\r\n2019-04-08 14:47:10.718811: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:173] hostname: everest6\r\n2019-04-08 14:47:10.718867: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 410.104.0\r\n2019-04-08 14:47:10.718890: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 410.104.0\r\n2019-04-08 14:47:10.718896: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:308] kernel version seems to match DSO: 410.104.0\r\n2019-04-08 14:47:10.737178: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4200000000 Hz\r\n2019-04-08 14:47:10.737608: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52d4340 executing computations on platform Host. Devices:\r\n2019-04-08 14:47:10.737622: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-04-08 14:47:10.738962: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1288] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\nConverting to tflite...\r\n2019-04-08 14:47:10.739692: I tensorflow/core/grappler/devices.cc:50] Number of eligible GPUs (core count >= 8): 0\r\n2019-04-08 14:47:10.739747: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-04-08 14:47:10.741001: I tensorflow/core/grappler/devices.cc:50] Number of eligible GPUs (core count >= 8): 0\r\n2019-04-08 14:47:10.741033: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\nTraceback (most recent call last):\r\n  File \"minimal_tflite_test.py\", line 67, in <module>\r\n    main()\r\n  File \"minimal_tflite_test.py\", line 47, in main\r\n    export_tflite_from_session(session, [a, b], [c], tflite_filename)\r\n  File \"minimal_tflite_test.py\", line 9, in export_tflite_from_session\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py\", line 742, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 410, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 176, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-04-08 14:47:11.490702: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1 operators, 3 arrays (0 quantized)\r\n2019-04-08 14:47:11.490766: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1 operators, 3 arrays (0 quantized)\r\n2019-04-08 14:47:11.490876: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 11 operators, 25 arrays (0 quantized)\r\n2019-04-08 14:47:11.490918: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1800] Check failed: axis >= 0 (-323499096 vs. 0)\r\nAborted (core dumped)\r\n```\r\nThe error occurs during TF Lite conversion. The erroneous axis value (`-323499096`) is different every time the script is called. If the value is positive, the error is:\r\n```\r\n2019-04-08 15:14:43.877396: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1801] Check failed: axis < input_shape.dimensions_count() (539352088 vs. 3)\r\n```\r\n", "comments": ["@sklampfl , i ran your code and it works fine on my ubuntu 18.04.2 LTS with python version 3.6.7and tensorflow version 1.13.1. To me this looks like the configuration issue only.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 Thank you for checking this out! I also have the same problem with Python3 (3.5.2) and tensorflow 1.13.1 (with or without gpu). Do you have a specific configuration issue in mind? \r\n\r\nSince the error occurs on C++ level, I believe there might be an issue with system libraries then, rather than Python dependencies.", "I quickly created a small Google Cloud instance with Ubuntu 18.04.2 LTS, Python 3.6.7, and tensorflow 1.13.1, and ran into the same issue there.\r\n\r\n@amitsrivastava78 do you have any specific libraries installed?", "@sklampfl apart from this i have just CUDA 9.2 version installed on my system. which i think should not be the issue try g++ vesion mentioned below : -\r\nand g++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n\r\nRegards\r\nAmit\r\n", "I don't think g++ is relevant either, since I installed tensorflow via pip, but for the sake of completeness this is the result of `g++ --version` on both systems:\r\nMacOS: `Apple LLVM version 10.0.1 (clang-1001.0.46.3) Target: x86_64-apple-darwin18.5.0`\r\nLinux: `g++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609`\r\nOn the cloud instance there was no g++ installed", "@sklampfl , We can try one more thing, lets build the tensorflow from source(and install it) and then try to run the convert_test.py cases can check if you still face the same problem. If not then try to run the example code that you posted, i am pasting my configuration file(.tf_configure.bazelrc) as below , you already know my python and tensorflow configuration : -\r\n\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.2\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda-9.2/lib64:/usr/local/cuda-9.2/extras/CUPTI/lib64:\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 Thank you for investing your time in this.\r\n\r\nI managed to get it to work on a fresh Ubuntu 18.04 instance, with Python 3.6.7 and TensorFlow compiled from source (both `r1.13` and `master`, with gcc 7.3.0).\r\n\r\nIt still does not work on my original Ubuntu 16.04 machine, even if I compile TensorFlow from source (Python 2 or Python 3). I have not yet figured out which configuration breaks it (e.g. gcc version? other libraries? exact Python version?).\r\n\r\nI can work with Ubuntu 18.04 for now, but I think it is still a bug that some TFLite conversions only work under certain conditions.\r\n\r\nPS: Thank you for pointing out the convert_test.py script. These test cases are always successful for me, regardless of whether the minimal test I posted fails or not.", "@sklampfl , Great to hear that things have worked out well on the newer version of Ubuntu for you.\r\n\r\nCheers!\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 @haozha111 I have the same problem on my machine: Debian GNU/Linux 9 (stretch), gcc 4.9.0/gcc 6.3.0 with tensorflow 2.0", "Small code for reproducing:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nroot = tf.train.Checkpoint()\r\nroot.f = tf.function(lambda x, y: tf.matmul(x, y))\r\n\r\nnew_input_data = np.random.randn(2, 100, 100, 3).astype(np.float32)\r\nnew_w = np.random.randn(2, 1, 3, 3).astype(np.float32)\r\n\r\ninput_data = tf.convert_to_tensor(new_input_data)\r\ninput_w = tf.convert_to_tensor(new_w)\r\n\r\nconcrete_func = root.f.get_concrete_function(input_data, input_w)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\ntflite_model = converter.convert()\r\n```\r\n\r\nLog:\r\n```\r\nConverterError: TOCO failed. See console for info.\r\n2019-07-26 11:45:58.103201: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2 operators, 4 arrays (0 quantized)\r\n2019-07-26 11:45:58.103298: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2 operators, 4 arrays (0 quantized)\r\n2019-07-26 11:45:58.120208: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 410 operators, 823 arrays (0 quantized)\r\n2019-07-26 11:45:58.122977: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1812] Check failed: axis >= 0 (-1015774856 vs. 0)\r\nFatal Python error: Aborted\r\n```\r\n\r\nDo we have any workaround now?", "@Oktai15 I'm running Ubuntu 19.04 and building from source did solve the problem on my machine", "Got the same thing working on Ubuntu 18.04 when compiled from source. It doesn't work on 16.04 compiled from source though.", "I also came across this problem on Ubuntu 16.04\u3001python3.7\u3001tensorflow-gpu2.1", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27640\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27640\">No</a>\n"]}, {"number": 27639, "title": "keras `Model.__call__` fails to propagate `Lambda`s with multiple outputs correctly", "body": "\r\n## System Information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (see below)\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version (use command below): ('v1.13.1-0-g6612da8951', '1.13.1')\r\n- Python version: 2.7.12\r\n- GPU model and memory: quadro 620\r\n\r\n## Current Behaviour\r\nCalls to models created using the functional interface fail to appropriately propagate `Lambda` outputs with multiple outputs. Model construction works as expected.\r\n\r\n## Expected Behaviour\r\nOutputs of intermediate layers should have the same structure during model construction as during model call.\r\n\r\n## Code to Reproduce\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nx = tf.keras.layers.Input(shape=(), dtype=tf.float32)\r\ny = tf.keras.layers.Input(shape=(), dtype=tf.float32)\r\n\r\n\r\ndef add_and_mul(args):\r\n    x, y = args\r\n    return x + y, x * y\r\n\r\n\r\ndef add_mul_model():\r\n    x = tf.keras.layers.Input(shape=(), dtype=tf.float32)\r\n    y = tf.keras.layers.Input(shape=(), dtype=tf.float32)\r\n    s = tf.keras.layers.Lambda(lambda a: a[0] + a[1])([x, y])\r\n    p = tf.keras.layers.Lambda(lambda a: a[0] * a[1])([x, y])\r\n    return tf.keras.models.Model(inputs=[x, y], outputs=[s, p])\r\n\r\n\r\nlayer = tf.keras.layers.Lambda(add_and_mul)  # bugged\r\n# layer = add_mul_model()                    # not bugged\r\ns, p = layer([x, y])\r\n# the following gives the same output either way\r\nprint('s = %s' % s)\r\nprint('p = %s' % p)\r\n\r\n\r\ndef double(x):\r\n    print('argument to double: %s' % str(x))\r\n    # using the bugged version, this prints:\r\n    # argument to double:\r\n    #       Tensor(\"lambda/add:0\", shape=(?,), dtype=float32)\r\n    # argument to double: (\r\n    #     <tf.Tensor 'model/lambda/add:0' shape=(1,) dtype=float32>,\r\n    #     <tf.Tensor 'model/lambda/mul:0' shape=(1,) dtype=float32>)\r\n\r\n    # unbugged version prints:\r\n    # argument to double:\r\n    #       Tensor(\"model/lambda/add:0\", shape=(?,), dtype=float32)\r\n    # argument to double:\r\n    #       Tensor(\"model_1/model/lambda/add:0\", shape=(1,), dtype=float32)\r\n    return x * 2\r\n\r\n\r\ns2 = tf.keras.layers.Lambda(double)(s)\r\n\r\n\r\nmodel = tf.keras.models.Model(inputs=[x, y], outputs=s2)\r\n\r\nx = tf.constant([3.0], dtype=tf.float32)\r\ny = tf.constant([4.0], dtype=tf.float32)\r\n\r\nout = model([x, y])\r\nprint(out)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(out))\r\n    # (([7.], [12.]), ([7.], [12.])) for bugged version, 14. for good version\r\n```\r\n", "comments": ["This is resolved by returning a `list` from `add_and_mul` rather than a tuple.\r\n\r\nI believe this can be traced to [generic_utils.to_list](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/generic_utils.py#L545) converting tuples to single-entry lists (i.e. `to_list((x, y)) == [(x, y)]`). This has flow-on effects in `network.engine`, though they seem to have changed in the current master branch.\r\n\r\nIt seems to be resolved in 1.14"]}, {"number": 27638, "title": "Typo in SpaceToBatchND and BatchToSpaceND documentation", "body": "**System information**\r\n- TensorFlow version: 1.13\r\n- Doc Link: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/space-to-batch-n-d, https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/batch-to-space-n-d\r\n\r\n\r\n**Describe the documentation issue**\r\nExample 2 in https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/space-to-batch-n-d says\r\n\r\n> The output tensor has shape `[4, 1, 1, 3]` and value:\r\n\r\n>     [[[1, 2, 3]], [[4, 5, 6]], [[7, 8, 9]], [[10, 11, 12]]]\r\n\r\nHowever, that value's shape is `[4, 1, 3]`. It appears the shape is correct, but the value should be\r\n\r\n    [[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\r\n\r\nThe same issue is present in https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/batch-to-space-n-d example 2.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes.", "comments": ["@alexeyr This was resolved in recent docs on TF website. Please check the [page here](https://www.tensorflow.org/api_docs/python/tf/raw_ops/SpaceToBatchND).\r\n\r\nCurrent example shows as shown below\r\n`[[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]`\r\n\r\nI am closing this issue as this was already resolved. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 27637, "title": "[doc/keras] incorrect comment in the example for `tf.keras.layers.Add` #27632", "body": "Issue #27632 ", "comments": ["Removed typo", "Please make sure the commits on the PR are only those needed for the PR. It makes integrating the code much harder to have commits touching other files.\r\n\r\nYou can create branches for each topic of work you chose to do, push the branches, make PRs with those branches only. Plus, on each branch you can `git squash`, `git rebase`, etc, to make everything look clean and neat and get approval and merging much faster.", "@mihaimaruseac I have removed that unwanted line but when I am trying to rebase my previous commits , I am facing some issues.", "Can you elaborate on the issues?\r\n\r\nFor future references, here's how I would do multiple PRs to the same repo:\r\n\r\n1. fork the repo on github (delete old fork before)\r\n2. `git clone` your fork\r\n3. `git checkout -b $NAME` where `$NAME` would be the name of the branch\r\n4. work only on one thing there, commit often\r\n5. `git rebase -i ...` reorder commits, squash, cleanup\r\n6. `git push`, make PR, go to step 3 with a different branch\r\n\r\nWhenever you have to work on a PR you `git checkout` the corresponding branch, work on it and push and the PR updates automatically", "@shashvatshahi1998 ping on this?", "@shashvatshahi1998 should we also close this and reopen as a clean PR?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 27636, "title": "Fix `cmake` build error in Android example", "body": "1. Remove `gcc`(https://android.googlesource.com/platform/ndk/+/master/docs/ClangMigration.md)\r\n2. Change `gnustl_static` to `c++_static` (https://developer.android.com/ndk/guides/cpp-support.html)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27636) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27636) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27636) for more info**.\n\n<!-- ok -->"]}, {"number": 27635, "title": "Fixed cond with group and summaries, issue #24815", "body": "", "comments": ["@ila96 thanks for your contribution , did you mean to push the changes to tensorflow:master ?", "Thanks @rthadur for the note.\r\n\r\nI created pull request to tensorflow:master, https://github.com/tensorflow/tensorflow/pull/27712\r\n\r\nShould I close this pull request?", "> Thanks @rthadur for the note.\r\n> \r\n> I created pull request to tensorflow:master, #27712\r\n> \r\n> Should I close this pull request?\r\n\r\nI will go ahead and close this PR.Thank you "]}, {"number": 27634, "title": "Missing documentation for variable ops.", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: v1.13\r\n- Doc Link:\r\n\r\n\r\n**Describe the documentation issue**\r\nHi, I am looking for documentation on the ReadVariableOp and VarHandleOp. Specifically as to what are the inputs that they take and what do they produce exactly. I dug around the source code for mentions of these ops and found some calls [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L196) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L864). While this hints at the signature of these ops, I would love to look at the implementation, which probably exists in [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L37) module, but I can not find it in this public repo's master branch. Could someone point me to this?\r\n\r\nAlternately, is there any documentation on these ops written [this](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/fused-batch-norm) way?\r\n\r\nThanks!\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@smr97,\r\nYou can find the documentation of `ReadVariableOp` and `VarHandleOp` with their inputs and output below\r\n\r\n- https://www.tensorflow.org/api_docs/python/tf/raw_ops/ReadVariableOp\r\n- https://www.tensorflow.org/api_docs/python/tf/raw_ops/VarHandleOp\r\n\r\nMarking the issue as closed. Please feel free to re-open if necessary. Thanks!"]}, {"number": 27633, "title": "[doc/keras] incorrect comment in `__init__` of `tf.keras.layers.AveragePooling1D`", "body": "\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling1D#arguments\r\n\r\n\r\n**Describe the documentation issue**\r\nSee the comment below\r\n\r\n```\r\npool_size: Integer, size of the **max** pooling windows.\r\n```\r\n\r\nIt should be **average** instead of **max** since this is average pooling.\r\n\r\nFurthermore, the format of the description for `input shape` and `output shape` is broken.\r\nIt is not rendered in a list format.", "comments": ["Closed via 31be1b7 / #27779"]}, {"number": 27632, "title": "[doc/keras] incorrect comment in the example for `tf.keras.layers.Add`", "body": "\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add#class_add\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nSee the code example (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add#class_add)\r\n\r\n```python\r\n    added = keras.layers.Add()([x1, x2])  # equivalent to added =\r\n    keras.layers.add([x1, x2])\r\n```\r\n\r\nIt should be\r\n\r\n```\r\n    added = keras.layers.Add()([x1, x2])  # equivalent to added = keras.layers.add([x1, x2])\r\n```", "comments": ["@csukuangfj  I am added a pr regarding that issue.", "you're adding too many commits.\r\n", "git squash"]}, {"number": 27631, "title": "[doc/keras] `predict` of `tf.keras.models.Sequential` is rendered incorrectly", "body": "\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#predict\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nThe argument list is not rendered correctly.\r\n\r\nSee it below\r\n\r\n```\r\nArguments:\r\nx: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset or a dataset iterator. - A generator or keras.utils.Sequence instance. * batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size is your data is in the form of symbolic tensors, dataset, dataset iterators, generators, or keras.utils.Sequence instances (since they generate batches). * verbose: Verbosity mode, 0 or 1. * steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None. * max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. * workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. * use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True, use process-based threading. If unspecified, use_multiprocessing will default to False. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes.\r\n```\r\n\r\nIt should be rendered in a list/item format.", "comments": ["PR submitted already #27596", "@csukuangfj I am closing this issue as the PR was already merged and I have looked into the source code that show update. TF website will be updated soon and then you can see correct rendering. Thanks!"]}, {"number": 27630, "title": "Model Start At Different Loss Level After Resuming The Training", "body": "I'm using `tensorflow.keras` in order to build a simple neural network with 3 dense layers. I was able to successfully train the model for 9000 epochs reaching a Mean of Squared Errors (`MSE`) of **0.0496**. However resuming the model, it starts training at about **57** `MSE`.\r\n\r\nThis might indicates that the model weights were not loaded successfully but when restarting the training process from the beginning (without loading previous saved weights), `MSE` starts at about +9000.\r\n\r\nHere is my code:\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport pathlib\r\n\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nimport seaborn as sns\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import model_from_json\r\nfrom tensorflow.keras.models import load_model\r\n\r\nprint(tf.__version__)\r\n\r\ndataset_path = 'D:\\\\FXData\\\\data.csv'\r\ncheckpoint_model_json_path = 'modelBackup/model.json'\r\ncheckpoint_weights_h5_path = 'modelBackup/weights00009000.h5'\r\nresume_from_checkpoint = True\r\n\r\nprint('reading dataset...')\r\ncolumn_names = ['paircode','x1o','x1h','x1l','x1c','x1v','x2o','x2h','x2l','x2c','x2v','x3o','x3h','x3l','x3c','x3v','x4o','x4h','x4l','x4c','x4v','x5o','x5h','x5l','x5c','x5v','x6o','x6h','x6l','x6c','x6v','x7o','x7h','x7l','x7c','x7v','x8o','x8h','x8l','x8c','x8v','x9o','x9h','x9l','x9c','x9v','x10o','x10h','x10l','x10c','x10v','x11o','x11h','x11l','x11c','x11v','x12o','x12h','x12l','x12c','x12v','x13o','x13h','x13l','x13c','x13v','x14o','x14h','x14l','x14c','x14v','x15o','x15h','x15l','x15c','x15v','x16o','x16h','x16l','x16c','x16v','x17o','x17h','x17l','x17c','x17v','x18o','x18h','x18l','x18c','x18v','x19o','x19h','x19l','x19c','x19v','x20o','x20h','x20l','x20c','x20v','x21o','x21h','x21l','x21c','x21v','x22o','x22h','x22l','x22c','x22v','x23o','x23h','x23l','x23c','x23v','x24o','x24h','x24l','x24c','x24v','x25o','x25h','x25l','x25c','x25v','x26o','x26h','x26l','x26c','x26v','x27o','x27h','x27l','x27c','x27v','x28o','x28h','x28l','x28c','x28v','x29o','x29h','x29l','x29c','x29v','x30o','x30h','x30l','x30c','x30v','x31o','x31h','x31l','x31c','x31v','x32o','x32h','x32l','x32c','x32v','x33o','x33h','x33l','x33c','x33v','x34o','x34h','x34l','x34c','x34v','x35o','x35h','x35l','x35c','x35v','x36o','x36h','x36l','x36c','x36v','x37o','x37h','x37l','x37c','x37v','x38o','x38h','x38l','x38c','x38v','x39o','x39h','x39l','x39c','x39v','x40o','x40h','x40l','x40c','x40v','x41o','x41h','x41l','x41c','x41v','x42o','x42h','x42l','x42c','x42v','x43o','x43h','x43l','x43c','x43v','x44o','x44h','x44l','x44c','x44v','x45o','x45h','x45l','x45c','x45v','x46o','x46h','x46l','x46c','x46v','x47o','x47h','x47l','x47c','x47v','x48o','x48h','x48l','x48c','x48v','x49o','x49h','x49l','x49c','x49v','x50o','x50h','x50l','x50c','x50v','nextclose']\r\ndataset = pd.read_csv(dataset_path, names=column_names,\r\n                      na_values = \"?\", comment='\\t',\r\n                      sep=\",\", skipinitialspace=True, skiprows = [0])\r\n\r\nprint('printing dataset tail...')\r\nprint(dataset.tail())\r\n\r\ntrain_dataset = dataset.sample(frac=0.8,random_state=0)\r\ntest_dataset = dataset.drop(train_dataset.index)\r\n\r\ntrain_labels = train_dataset.pop('nextclose')\r\ntest_labels = test_dataset.pop('nextclose')\r\n\r\ndef norm(x):\r\n  return x\r\n#  return (x - train_stats['mean']) / train_stats['std']\r\n\r\nprint('normalizing dataset...')  \r\nnormed_train_data = norm(train_dataset)\r\nnormed_test_data = norm(test_dataset)\r\n\r\ndef build_model():\r\n  print('building the model')\r\n  model = keras.Sequential([\r\n    layers.Dense(512, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\r\n    layers.Dense(512, activation=tf.nn.relu), layers.Dense(256, activation=tf.nn.relu),\r\n    layers.Dense(1, activation='linear')\r\n  ])\r\n  \r\n  model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\r\n  return model\r\n\r\ndef load_model_():\r\n  print('loading the model')\r\n  loaded_model = load_model(checkpoint_weights_h5_path)\r\n  return loaded_model\r\n\r\n\r\nif resume_from_checkpoint:\r\n  model = load_model_()\r\nelse:\r\n  model = build_model()\r\n\r\nmodel.summary()\r\n\r\nprint('testing 10 widthed batch...')\r\nexample_batch = normed_train_data[:10]\r\nexample_result = model.predict(example_batch)\r\nprint(example_result)\r\n\r\nprint('fitting the model...')\r\nmc = keras.callbacks.ModelCheckpoint('weights{epoch:08d}.h5', save_weights_only=False, period=100)\r\n\r\nhistory = model.fit(\r\n  normed_train_data, train_labels,\r\n  epochs=100, validation_split = 0.2, verbose=2,\r\n  batch_size=1000000, callbacks=[mc])\r\n\r\nprint('evaluating the model...')\r\nloss, accuracy = model.evaluate(normed_test_data, test_labels, verbose=0)\r\nprint(\"Testing set MSE: {:5.2f} nextclose\".format(loss))\r\nprint(\"Testing set Accuracy: {:5.2f} nextclose\".format(accuracy))\r\n```", "comments": ["This is a bug fixed currently in [tensorflow-gpu-nighlybuild 2.0](https://github.com/tensorflow/tensorflow/issues/27049)"]}, {"number": 27629, "title": "Tensorflow lite undefined reference", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.12.1\r\n- Bazel version: 0.15.0\r\n- GCC/Compiler version: 7.3.0\r\n(No python related information since I try to compile lite for C/C++ usage)\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFirst of all, I use CMake through a combo  `add_custom_command` and `add_custom_target` to handle the tensorflow build for me. I try to build tensorflow lite for x86 or ARM depending on my needs (I've done it before on the 1.10 tensorflow version).\r\n\r\nThe commands I execute to build tensorflow are in a script: \r\n```\r\nbash configure\r\nbazel --output_base=~/.tmp/tensorflow_lite/1.12.1/.Debug_x86 \\\r\n         build --jobs=4 \\\r\n                 //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so\r\n```\r\nThe build happens fine, I link `libtensorflowlite_c.so` to the custom CMake target I want to use in my own environment and it shows correctly on my make logs. \r\n\r\nHowever I have undefined references for: \r\n```\r\n`tflite::DefaultErrorReporter()'\r\n`tflite::Interpreter::AllocateTensors()'\r\n`tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'\r\n`vtable for tflite::MutableOpResolver'\r\n`tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'\r\n`tflite::InterpreterBuilder::~InterpreterBuilder()'\r\n`tflite::Interpreter::Invoke()'\r\n`tflite::Interpreter::~Interpreter()'\r\n`vtable for tflite::ops::builtin::BuiltinOpResolver'\r\n`tflite::Interpreter::SetNumThreads(int)'\r\n`tflite::FlatBufferModel::~FlatBufferModel()'\r\n`tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n`tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\n```\r\nI quickly checked the bazel workspace and build directives and it seems my issues are related to the `framework` cc_library bazel is creating and that is linked to `libtensorflowlite_c` through the `c_api`.\r\n\r\nI don't know bazel a lot, I'm a bit lost with the dependencies. I would say the dependencies should propagate like this `framework -> c_api & c_api_internal -> libtensorflowlite_c.so`. Is it what happen?\r\nShould I add `//tensorflow/contrib/lite:framework` and other bazel target to be sure they are built? \r\n\r\nHow bazel handles mutual dependency, is this an acceptable situation? It happens between `context` (`//tensorflow/contrib/lite`) and `c_api_internal` (`//tensorflow/contrib/lite/experimental/c`) and between `frawework` (`//tensorflow/contrib/lite`) and `c_api_internal` (`//tensorflow/contrib/lite/experimental/c`).\r\n\r\n```\r\n# context definition, needs c_api_internal\r\ncc_library(\r\n    name = \"context\",\r\n    hdrs = [\"context.h\"],\r\n    deps = [\"//tensorflow/contrib/lite/c:c_api_internal\"],\r\n)\r\n# framework definition, needs c_api_internal (only deps are shown)\r\ncc_library(\r\n    name = \"framework\",\r\n    srcs = [...] + select({...}),\r\n    hdrs = [...],\r\n    copts = tflite_copts(),\r\n    defines = select({...}),\r\n    linkopts = [] + select({...}),\r\n    deps = [\r\n        \":arena_planner\",\r\n        \":graph_info\",\r\n        \":memory_planner\",\r\n        \":schema_fbs_version\",\r\n        \":simple_memory_arena\",\r\n        \":string\",\r\n        \":util\",\r\n        \"//tensorflow/contrib/lite/c:c_api_internal\",\r\n        \"//tensorflow/contrib/lite/core/api\",\r\n        \"//tensorflow/contrib/lite/kernels:eigen_support\",\r\n        \"//tensorflow/contrib/lite/kernels:gemm_support\",\r\n        \"//tensorflow/contrib/lite/nnapi:nnapi_lib\",\r\n        \"//tensorflow/contrib/lite/profiling:profiler\",\r\n        \"//tensorflow/contrib/lite/schema:schema_fbs\",\r\n    ] + select({...}),\r\n)\r\n# c_api_internal definition, needs context & framework\r\ncc_library(\r\n    name = \"c_api_internal\",\r\n    srcs = [\"c_api.h\"],\r\n    hdrs = [\"c_api_internal.h\"],\r\n    copts = tflite_copts(),\r\n    visibility = [\r\n        \"//tensorflow/contrib/lite/experimental/c:__subpackages__\",\r\n    ],\r\n    deps = [\r\n        \"//tensorflow/contrib/lite:context\",\r\n        \"//tensorflow/contrib/lite:framework\",\r\n    ],\r\n)\r\n# \r\n```\r\n\r\nCLOSED: Still have the issue but it's a CMake linkage issue, not related to tensorflow lite build.", "comments": ["@strattist did you resolve with cmake?", "Yes, for x86 I use these bazel commands: \r\n\r\n```\r\nbash configure\r\n  bazel --output_base=/.tmp/tensorflow_lite/1.12.1/.Debug_x86 \\\r\n        build --jobs=4 \\\r\n              --cxxopt='-std=gnu++11' \\\r\n              //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so\r\n```\r\n\r\nFor ARM and AARCH64 I used the Makefile they give in `./tensorflow/tensorflow/contrib/lite/tools/make` although I modified it to add everything within the experimental directory:\r\n- Add `$(wildcard tensorflow/contrib/lite/experimental/c/*.cc)` to the base `CORE_CC_ALL_SRCS` Makefile variable\r\n- Add `$(wildcard tensorflow/contrib/lite/experimental/kernels/*.cc)` to the `CORE_CC_ALL_SRCS` insice the `BUILD_TYPE` condition just below.\r\n- I also deleted the `schema_generated.h` build rule.\r\n\r\nIt worked that way for me. "]}, {"number": 27628, "title": "tensorflow wont start windows 10 python 3.7.3", "body": "\r\n  File \"<ipython-input-2-0786882784e8>\", line 1, in <module>\r\n    runfile('C:/Users/Christophe/.spyder-py3/temp.py', wdir='C:/Users/Christophe/.spyder-py3')\r\n\r\n  File \"e:\\program files\\python3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 824, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"e:\\program files\\python3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/Christophe/.spyder-py3/temp.py\", line 7, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"e:\\program files\\python3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"e:\\program files\\python3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"e:\\program files\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"e:\\program files\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"e:\\program files\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"e:\\program files\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"e:\\program files\\python3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"e:\\program files\\python3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Windows 10 it works in spyder but not in pycharm, installed the modules in pycharm downgraded to python 6, have my scripts folder in documents on other drive, maybe something to do  with environments? kinda new to scripting.  ", "tensorflow version 1.13.1", "can I maybe create environment in script folder, and how would I do that?", "Now I get this:\r\nC:\\Users\\Christophe\\Anaconda3\\python.exe E:/Documents/PythonScripts/tensor.py\r\nTraceback (most recent call last):\r\n  File \"E:/Documents/PythonScripts/tensor.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Christophe\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Christophe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 47, in <module>\r\n    import numpy as np\r\n  File \"C:\\Users\\Christophe\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py\", line 140, in <module>\r\n    from . import _distributor_init\r\n  File \"C:\\Users\\Christophe\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py\", line 34, in <module>\r\n    from . import _mklinit\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nProcess finished with exit code 1", "This is the script i'm using:\r\n\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test, y_test)", "Can you import tensorflow successfully using the terminal? If yes, then you have installed tf correctly and you need to correctly set the environment variables correctly in your ide (spyder,pycharm). If not, then first we need to install tf successfully.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27628\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27628\">No</a>\n"]}, {"number": 27627, "title": "[doc/keras] `__init__` of `tf.keras.losses.BinaryCrossentropy` is not rendered correctly", "body": "\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy#class_binarycrossentropy\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nThe `__init__` function of `tf.keras.losses.BinaryCrossentropy` is not rendered correctly.\r\n\r\nSee the link here https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy#methods", "comments": ["You mean that in documentation `__init__` is not described directly it is started from `__call__`?", "@shashvatshahi1998 \r\n\r\n````\r\n\r\n#### Args:\r\n\r\n* <b>`from_logits`</b>: Whether `output` is expected to be a logits tensor. By default,\r\n    we consider that `output` encodes a probability distribution.\r\n* <b>`label_smoothing`</b>: If greater than `0` then smooth the labels.\r\n* <b>`reduction`</b>: Type of <a href=\"../../../tf/losses/Reduction\"><code>tf.losses.Reduction</code></a> to apply to loss. Default value is\r\n    `SUM_OVER_BATCH_SIZE`.\r\n* <b>`name`</b>: Optional name for the op.\r\n\r\n<h2 id=\"__init__\"><code>__init__</code></h2>\r\n\r\n``` python\r\n__init__(\r\n    from_logits=False,\r\n    label_smoothing=0,\r\n    reduction=losses_impl.ReductionV2.SUM_OVER_BATCH_SIZE,\r\n    name=None\r\n)\r\n\r\n---------\r\nis obviously in its raw format.\r\n---------", "Closing this issue since this is resolved. Feel free to reopen if have further questions. Thanks!"]}, {"number": 27626, "title": "Huawei (android 6.0\uff0cOpenGL3.1 is Mali) GPU slower than CPU", "body": "I have two mobile phones, one is XiaoMi  (Android 9.0\uff0cOpenGL3.1 is Adreno), the other is Huawei (Android 6.0, OpenGL3.1 is Mail). When I run this demo app (Tensor Flow Lite GPU delegate), GPU is faster in the XiaoMi mobile phone than CPU, but in Huawei mobile phone, GPU is slower than CPU. What's the reason, the system version problem? Or the OpenGL ES chip problem?", "comments": ["\uff1f\uff1f\uff1f\uff1f", "It depends on the specific model of your Huawei device.Some Huawei HiSilicon SOCs have very low GPU performance.In addition, the performance of the memory controller of the individual models of the HiSilicon SOC has performance problems.Devices that support NNAPI can use GPU/DSP/TPU to speed up some OPs, perhaps for performance improvements.\r\n", "well,If my device GPU is good enough, but my Android system is less than 8.1 (NNAPI needs to be greater than 8.1), what happens? in tensorflow lite demo app,Will the GPU be faster than the CPU?", "@anneyking \r\n\r\nThanks for reporting.  We also have noticed the performance degradation with Mali devices.  We're actively investigating where the performance regression happened.  I'll update the thread when we find the root cause.", "@impjdi hi, I test the inference time of mobilentv2 on Huawei Kirin seris GPU.  It is really a strange things. It shows same worse performance on  970 and 980, and shows better performance on 960.  It is sure that 980 has a stronger GPU,  but 960 get the best performance. Also, 980 gpu get slight accelerate compared to its cpu (about 30%). But, 960  gpu get  great accelerate compared to its cpu (about 7x). Below are some results on Kirin GPUs. So, can the tensorflow lite optimize for this?\r\n\r\n\r\n|      | Kirin 980 (Mail 76) | Kirin 970 (Mail 72) | Kirin 960(Mail 71) |\r\n| :-: | :-: | :-: | :-: |\r\n| mobilenetv2(224*224) | 19ms | 21ms |14ms |\r\n| ssdlite_mobilenetv2(300*300) | 41ms | 33ms | 29ms |", "@impjdi FYR. Dunno if this is same problem, but it seems there is a problem on some Qualcomm platforms too. I bought a Pixel 3a last week. NNAPI driver doesn't work on it, so I tried GPU Delegate on it. I got GPU numbers much worse than expected with my [quick-and-dirty benchmark](https://github.com/freedomtan/glDelegateBench/). I also got similar GPU numbers for Mobilenet V1 on both [TfLiteCameraDemo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo) apk and command line label_image.\r\n\r\nmodel name | CPU 1 thread (ms) | CPU 4 threads (ms) | GPU delegate (ms)\r\n-- | --: | --:| --:\r\nMobilenet | 113 | 80 | 118\r\nPoseNet | 138 | 96 | 175\r\nDeepLab V3 | 173 | 132 | 270\r\nMobilenet SSD V2 COCO | 200 | 167 | 250", "@freedomtan \r\n\r\nWe have prioritized and have someone working on Mali performance.  Maybe he can find the problem soon.  Hopefully that also addresses the Pixel 3a issue...", "@impjdi FYR. I got more reasonable GPU numbers on Pixel 3a after the GlPersistentBuffer 87ae6fb8ee4.\r\n\r\nmodel name | CPU 1 thread (ms) | CPU 4 threads (ms) | GPU (ms)\r\n-- | --: | --: | --:\r\nMobilenet | 113 | 80 | 52\r\nPoseNet | 138 | 96 | 78\r\nDeepLab V3 | 173 | 132 | 144\r\nMobilenet SSD V2 COCO | 200 | 167 | 113\r\n\r\n", "@freedomtan \r\n\r\nThanks for measuring the timing.  My colleague @NikolayChirkov added those improvements a couple days ago.  It doesn't solve the Mali performance bug, which we're still looking into, but improves some performance issues.", "@impjdi It turns out that I was wrong. The faster numbers are on Pixel 3a running Android Q beta 3 and slower numbers are on Android P. The aforementioned patch doesn't help improve performance on Pixel 3a running Android P. Which means the slower numbers might be caused by GPU driver(s). Maybe the some of the Mali performance issues are caused by driver too?", "@freedomtan \r\n\r\nThanks for the update.  Yeah, Driver versions definitely play a big role in GPU inference.  Thay may be possible.", "@anneyking \r\nCould you please try on the latest tf version and let us know if this is still an issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27626\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27626\">No</a>\n"]}, {"number": 27625, "title": "Update api_def_AudioSpectrogram.pbtxt", "body": "", "comments": []}, {"number": 27624, "title": "CleanUp  & TC added for L2_pool Graph transformation.", "body": "Clean-up: replace FindOperator with FindOp and add missing unit tests\r\nfor the L2_pool graph transformation.", "comments": ["@multiverse-tf , thanks for the review i have updated as per your suggestion, kindly check.\r\n\r\nRegards\r\nAmit", "@multiverse-tf , thanks for approving the PR.\r\n@gbaned , @rthadur please change the PR label to approved.\r\n\r\nRegards\r\nAmit"]}, {"number": 27623, "title": "CUDA 10 / CUDNN 7.5 performance loss (Titan V)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 42.3\r\n- GPU model and memory: Titan V\r\n- GCC/Compiler version (if compiling from source): 4.8\r\n- Nvidia Driver Version: 418.56\r\n\r\n**Problem:** I experience consistent performance losses in TF using CUDA 10.0/CUDNN 7.5. \r\nHere is a minimal example:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\nmatrix_dim = (5120,5120)\r\nnoof_matmul = 512\r\nnum_gpus = 1\r\nfloat_type = tf.float32\r\nc = []\r\n\r\nprint(tf.__file__)\r\nfor d in range(num_gpus):\r\n    with tf.device('/gpu:%s' % d):\r\n        a=tf.random_normal(matrix_dim,dtype=float_type)\r\n        for i in range(noof_matmul):\r\n            a=tf.matmul(a,a)\r\n        c.append(a.op)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(c)\r\n    start=time.time()\r\n    for i in range(10):\r\n        sess.run(c)\r\n\r\nt = (time.time()-start)/10.\r\ntflops = noof_matmul * (2 * matrix_dim[0]**3 - matrix_dim[0]**2) / t / 10**12\r\nprint(t, 'sec, ', tflops, 'TFlops')\r\n```\r\n\r\n**Results Titan V:**\r\nTensorflow 1.13.1 built from source, bazel 0.24.0, CUDA 10.0, CUDNN 7.5.0:\r\n```diff\r\n-FP16: 1.6899 sec, 81.3209 TFlops\r\n-FP32: 10.6873 sec, 12.8588 TFlops\r\n```\r\nTensorflow 1.9 built from source (same configuration besides versions), bazel 0.11.1, CUDA 9.1, CUDNN 7.1.4:\r\n```diff\r\n+FP16: 1.5391 sec, 89.2890 TFlops\r\n+FP32: 10.1137 sec, 13.5881 TFlops\r\n```\r\nPip wheel tensorflow_gpu-1.13.1-cp27-cp27mu-manylinux1_x86_64.whl, CUDA 10.0, CUDNN 7.5.0:\r\n```diff\r\n-FP16: 1.6902 sec, 81.3057 TFlops\r\n-FP32: 10.5577 sec, 13.01656 TFlops\r\n```\r\nConda (Python 3.6) linux-64/tensorflow-gpu-1.13.1-h0d30ee6_0.tar.bz2, CUDA 9.2, CUDNN 7.3.1:\r\n```diff\r\n+FP16: 1.5389 sec, 89.2995 TFlops\r\n+FP32: 10.1150 sec, 13.5863 TFlops\r\n```\r\n\r\nHave you experienced similar performance losses? Do you know any solutions? If necessary, I can run tests on other GPU models as well.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Could I ask a question? When I turn \"tf.float32\" into \"tf.float64\" , this program cannot be stopped. How can I calculate FP64 for our GPUs? Thank you", "This is fixed in tf-nightly (could have been a TF fix or because we're now using CUDA 10.1).\r\n\r\nOn TF 2.0.0:\r\n\r\nfp16: 81.29 tflops\r\nfp32: 13.12 tflops\r\n\r\nOn TF 2.1.0-dev20191226\r\n\r\nfp16: 93.51 tflops\r\nfp32: 13.58 tflops\r\n\r\nAll numbers on Titan-V.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27623\">No</a>\n", "> Could I ask a question? When I turn \"tf.float32\" into \"tf.float64\" , this program cannot be stopped. How can I calculate FP64 for our GPUs? Thank you\r\n\r\n@zkzhu0110 please open a separate github issue for this if your question is still relevant."]}, {"number": 27622, "title": "about TensorFlow Lite GPU delegate", "body": "about TensorFlow Lite GPU delegate,Here's a description\uff1a\r\n\u201cIf some of the ops are not supported by the GPU delegate, the framework will only run a part of the graph on the GPU and the remaining part on the CPU. Due to the high cost of CPU/GPU synchronization, a split execution mode like this will often result in a performance slower than when the whole network is run on the CPU alone\u201d\u3002\r\nI have a question\uff1a\u201cIf some of the ops are not supported by the GPU delegate\u201d\uff0cwhich ops not supported by the GPU delegate  ??????", "comments": []}, {"number": 27621, "title": "Tensorflow speech commands error", "body": "<em>Im getting error when \u0131 convert h5 file to tflite file in speech commands example</em>\r\n\r\n**System information**\r\n- Ubuntu 18.04\r\n\r\n- TensorFlow version:1.10.0\r\n- Python version:3.6.7\r\n- Installed using pip\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**\r\nWhen I try to create tflite file Im getting that\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/speech_commands/android/README.md\r\n\r\n\r\nMy error:\r\n(keras) eco@eco-1Y3-GNB1554A1I3:~/Masa\u00fcst\u00fc/yz/keras/ml/export$ python convert_keras_lite.py\r\nWARNING:tensorflow:From convert_keras_lite.py:28: TocoConverter.from_keras_model_file (from tensorflow.contrib.lite.python.lite) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `lite.TFLiteConverter.from_keras_model_file` instead.\r\nTraceback (most recent call last):\r\n  File \"convert_keras_lite.py\", line 28, in <module>\r\n    output_arrays)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 306, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 556, in from_keras_model_file\r\n    input_shapes, output_arrays)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 368, in from_keras_model_file\r\n    keras_model = _keras.models.load_model(model_file)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py\", line 230, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py\", line 310, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 64, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 173, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1292, in from_config\r\n    process_layer(layer_data)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1278, in process_layer\r\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 64, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 175, in deserialize_keras_object\r\n    return cls.from_config(config['config'])\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1606, in from_config\r\n    return cls(**config)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\", line 330, in __init__\r\n    self.activation = activations.get(activation)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/activations.py\", line 206, in get\r\n    return deserialize(identifier)\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/activations.py\", line 197, in deserialize\r\n    printable_module_name='activation function')\r\n  File \"/home/eco/Masa\u00fcst\u00fc/yz/keras/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 193, in deserialize_keras_object\r\n    function_name)\r\nValueError: Unknown activation function:relu6\r\n\r\n\r\n**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you switch to latest stable TF version 1.13.1 and use [tflite converter](https://www.tensorflow.org/lite/convert/python_api#exporting_a_tfkeras_file_) instead?. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27620, "title": "LD_LIRARY_PATH instead of LD_LIBRARY_PATH ", "body": "Hello, \r\nin tensorflow/stream_executor/platform/default/dso_loader.cc line 50\r\nLD_LIRARY_PATH is called instead of LD_LIBRARY_PATH\r\nBest", "comments": ["I am closing this issue as the PR raised on this issue was merged already. But, please let me know if I'm mistaken. Thanks!"]}, {"number": 27619, "title": "tflite/tools/make: add missing cflags override for linux", "body": "Add missing CC flags for linux makefiles, without this linking tensorflow in a shared library will fail with:\r\n```\r\n/usr/bin/ld: tensorflow-1.13.1/tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a(fftsg.o): relocation R_X86_64_PC32 against symbol `cftmdl1' can not be used when making a shared object; recompile with -fPIC\r\n```\r\n\r\n/cc @dansitu", "comments": ["Can it be backported to r1.13 branch?\r\nProblem is that `gcc` does not use `-fPIC` flag and this is why my application can not link C lang objects (e.g. `c_api_internal.o`) from `libtensorflow-lite.a`\r\n@terryheo "]}]