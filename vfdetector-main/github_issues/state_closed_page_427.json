[{"number": 41069, "title": "Edge TPU Compiler Failed to Compile", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS\r\n- TensorFlow installed from (source or binary): anaconda\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n- Edge TPU Compiler version 2.1.302470888\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nedgetpu_compiler quantized.tflite\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nEdge TPU Compiler version 2.1.302470888\r\nERROR: :129 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.\r\nERROR: Node number 40 (FULLY_CONNECTED) failed to prepare.\r\n```\r\n\r\n**Failure details**\r\nI have used the same quantizing and converting to tflite code before and it works fine on a different structured model. I use it to convert .h5 keras models to tflite models. It works if I create a model based off MobileNet with additional dense layers\r\n```\r\nbase_model = MobileNet(weights='imagenet', include_top=False, input_shape=(200,200,3), dropout=.2)\r\nx = base_model.output\r\nx = GlobalAveragePooling2D()(x)\r\nx = Dense(1024, activation='relu')(x)\r\nx = Dense(1024, activation='relu')(x)\r\nx = Dense(512,activation='relu')(x)\r\ntypes = Dense(20,activation='softmax')(x)\r\nmodel = Model(inputs=base_model.input,outputs=types)\r\n```\r\n\r\nBut if I try to convert model with 2 outputs like this, I get the aformentioned error when compiling.\r\n```\r\nbase_model = MobileNet(weights='imagenet', include_top=False, input_shape=(200,200,3), dropout=.2)\r\nx = base_model.output\r\nx = GlobalAveragePooling2D()(x)\r\nx = Dense(1024, activation='relu')(x)\r\nx = Dense(1024, activation='relu')(x)\r\nx = Dense(512,activation='relu')(x)\r\ntypes = Dense(20,activation='softmax')(x)\r\n\r\ny = base_model.output\r\ny = GlobalAveragePooling2D()(y)\r\ny = Dense(512, activation='relu')(y)\r\ny = Dense(1024, activation='relu')(y)\r\ny = Dense(512, activation='relu')(y)\r\nvalues = Dense(3, activation='sigmoid')(y)\r\n\r\nmodel = Model(inputs=base_model.input, outputs=[types,values])\r\n```\r\n\r\nI can get the same error if I attempt to load the quantized model.\r\n```\r\ninterpreter = tf.lite.Interpreter(model_path='path/to/model/quantized.tflite')\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nprint(\"shape:\", output_details[0]['shape'])\r\nprint(\"shape:\", output_details[1]['shape'])\r\n```\r\nOutput is as expected:\r\n```\r\nshape: [ 1 20]\r\nshape: [1 3]\r\n```\r\n\r\nIf I then\r\n```\r\n#reshape for input of batch size 32\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (32, 200, 200, 3))\r\ninterpreter.resize_tensor_input(output_details[0]['index'], (32, 20))\r\n#interpreter.resize_tensor_input(output_details[1]['index'], (32, 3))\r\ninterpreter.allocate_tensors()\r\n```\r\n**Output:**\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n in \r\n      3 interpreter.resize_tensor_input(output_details[0]['index'], (32, 20))\r\n      4 # interpreter.resize_tensor_input(output_details[1]['index'], (32, 3))\r\n----> 5 interpreter.allocate_tensors()\r\n      6 \r\n\r\n~/Software/anaconda3/envs/Tensorflow2/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py in allocate_tensors(self)\r\n    245   def allocate_tensors(self):\r\n    246     self._ensure_safe()\r\n--> 247     return self._interpreter.AllocateTensors()\r\n    248 \r\n    249   def _safe_to_run(self):\r\n\r\n~/Software/anaconda3/envs/Tensorflow2/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n    108 \r\n    109     def AllocateTensors(self):\r\n--> 110         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n    111 \r\n    112     def Invoke(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/kernel_util.cc:106 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 40 (FULLY_CONNECTED) failed to prepare.\r\n```\r\nAgain, this error does not occur when the model has one output.", "comments": ["@rgbarishian,\r\nOn running the code, I was unable to convert the MobileNet with dense layers model. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/cf8b652ae402bd122591dcb310c3d760/41069.ipynb).\r\n\r\nCould you please provide the complete code to convert the model? Also, please take a look at [this](https://github.com/tensorflow/tensorflow/issues/32496) similar issue and let us know if it helps. Thanks!", "Here is my conversion code. It is based on this tutorial: https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf1.ipynb\r\nLike I mentioned before, this conversion code works fine if the network has one output, it only fails the compiling after this conversion if it has two outputs\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\r\nimport pathlib\r\n\r\nmodel = tf.keras.models.load_model('path/to/file.h5')\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nos.chdir('/directory/of/image/directories')#Where image directories are\r\ndirectory = os.listdir()\r\ndirectory\r\n\r\ndef representative_dataset_gen():\r\n    for i in directory:\r\n        count = 0\r\n        os.chdir(i)\r\n        files = os.listdir()\r\n        print(i)\r\n        for j in files:\r\n            if count<600:\r\n                img = Image.open(j)\r\n                array = np.asarray(img, dtype=np.float32)\r\n                array = preprocess_input(array)\r\n                count=count+1\r\n                yield[np.expand_dims(array, axis=0)]\r\n            else:\r\n                break      \r\n        os.chdir('../')\r\n\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\ntflite_quant_model = converter.convert()\r\n\r\n#Saving file\r\ntflite_model_dir = pathlib.Path('save/path/')\r\ntflite_quant_model_file = tflite_model_dir/'quantized.tflite'\r\ntflite_quant_model_file.write_bytes(tflite_quant_model)\r\n```", "I think I figured out what caused the problem, but I don't know why. When transfer learning, I would freeze one output branch and set the loss to 0. If I compiled with some layers never trained, the compilation fails. Once I train the entire network such that all layers have a loss, it successfully compiles.\r\nHere is the compilation and loss code:\r\n```\r\ndef null_loss(y_true, y_pred):\r\n        return K.zeros_like(y_true)\r\n\r\nlosses = {\r\n        'Output1': 'categorical_crossentropy',\r\n        'Output2': null_loss\r\n        }\r\nmodel.compile(optimizer=optimizer, loss=losses, metrics={Output1':'accuracy'})\r\n```", "Sorry, i am confused by different replies.\r\nAre you having problem to convert when you set 2 outputs or compile the model, or something else ?\r\n\r\nCan you please explain what is the issue you are reporting here exactly ?\r\n\r\nThanks", "That is correct. I am having trouble using the EdgeTPU Compiler with a model with two outputs.\r\nWhen I have a network with one output, I can compile fine. The compiler gave the error in my original post when my two output model was not entirely trained (One branch had its weights frozen and had the output had a `null_loss` (1keras.backend.zeros_like()`). I think the problem was caused because some layers were never trained and possibly the loss was 0. When I finally finished training, and then ran the compiler, it successfully compiled.\r\n\r\nInterestingly, at the end of my training, I freeze the base of the network and the first branch that was originally training. I also set that output's loss to `null_loss` to prevent it from backpropogating. This is when I train the second branch. Once the second branch trains for at least one epoch, I can convert to tflite and compile successfully.", "Do you mind sharing colab link with reproduce\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41069\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41069\">No</a>\n"]}, {"number": 41068, "title": "Remove static string member", "body": "@mihaimaruseac \r\nThis PR removes static std::string", "comments": []}, {"number": 41067, "title": "TypeError: can't pickle _thread.RLock objects while using cross_validate", "body": "Hi,\r\n\r\nI had a keras model and I converted that to scikit model using kerasclassifier but looks like there is some issue with how I am transforming object.\r\n\r\n\r\nI am getting this error while using the cross_validate function @ https://github.com/Neetu162/DeepLearningResearch/blob/677d1ddeb6f345716a457b977c26cbe14efa7bb0/Demo/classify_demo.py#L83\r\n```\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 797, in dispatch_one_batch\r\n    tasks = self._ready_batches.get(block=False)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/queue.py\", line 167, in get\r\n    raise Empty\r\nEmpty\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/home/osboxes/DeepLearningResearch/Demo/classify_demo.py\", line 171, in <module>\r\n    main()\r\n  File \"/home/osboxes/DeepLearningResearch/Demo/classify_demo.py\", line 83, in main\r\n    cv_result = cross_validate(model, perm_inputs_1, cv=5, return_train_score=True, n_jobs=1)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\r\n    return f(**kwargs)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 248, in cross_validate\r\n    for train, test in cv.split(X, y, groups))\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 1004, in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 808, in dispatch_one_batch\r\n    islice = list(itertools.islice(iterator, big_batch_size))\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 248, in <genexpr>\r\n    for train, test in cv.split(X, y, groups))\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\r\n    return f(**kwargs)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/sklearn/base.py\", line 87, in clone\r\n    new_object_params[name] = clone(param, safe=False)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 73, in inner_f\r\n    return f(**kwargs)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/sklearn/base.py\", line 71, in clone\r\n    return copy.deepcopy(estimator)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 280, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 280, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 280, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 280, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/copy.py\", line 169, in deepcopy\r\n    rv = reductor(4)\r\nTypeError: can't pickle _thread.RLock objects\r\n```\r\n\r\nIs there anything incorrect on how I am using the KerasClassifier or cross_validate ?", "comments": ["Was able to get pass that error. Still not able to get cross_validate to work.\r\n\r\nGit Link: https://github.com/Neetu162/DeepLearningResearch/blob/76675a79a4922b8bd0d722ab2e4cad448a8d8c76/Demo/classify_demo.py#L106\r\n\r\nError:\r\n```\r\ncreating the loaded model\r\ncalling the cross_validate method\r\n/home/osboxes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \r\nTraceback (most recent call last):\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\r\n    estimator.fit(X_train, y_train, **fit_params)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\", line 223, in fit\r\n    return super(KerasClassifier, self).fit(x, y, **kwargs)\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\", line 155, in fit\r\n    **self.filter_sk_params(self.build_fn.__call__))\r\n  File \"/home/osboxes/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 800, in __call__\r\n    'The first argument to `Layer.call` must always be passed.')\r\nValueError: The first argument to `Layer.call` must always be passed.\r\n\r\n  FitFailedWarning)\r\n```", "@Neetu162 \r\nPlease share the tensorflow version for us to replicate the issue faced or if possible share a cobal gist with the error faced.", "@Saduf2019\r\n\r\nThis is the list of packages.\r\n\r\n(base) osboxes@osboxes:~$ pip freeze\r\nabsl-py==0.9.0\r\nalabaster==0.7.12\r\nanaconda-client==1.7.2\r\nanaconda-navigator==1.9.12\r\nanaconda-project==0.8.3\r\nargh==0.26.2\r\nasn1crypto==1.3.0\r\nastor==0.8.1\r\nastroid==2.3.3\r\nastropy==4.0\r\nastunparse==1.6.3\r\natomicwrites==1.3.0\r\nattrs==19.3.0\r\nautopep8==1.4.4\r\nBabel==2.8.0\r\nbackcall==0.1.0\r\nbackports.functools-lru-cache==1.6.1\r\nbackports.shutil-get-terminal-size==1.0.0\r\nbackports.tempfile==1.0\r\nbackports.weakref==1.0.post1\r\nbeautifulsoup4==4.8.2\r\nbitarray==1.2.1\r\nbkcharts==0.2\r\nbleach==3.1.0\r\nblis==0.4.1\r\nbokeh==1.4.0\r\nboto==2.49.0\r\nBottleneck==1.3.2\r\ncachetools==4.1.0\r\ncatalogue==1.0.0\r\ncertifi==2020.4.5.1\r\ncffi==1.14.0\r\nchardet==3.0.4\r\nClick==7.0\r\ncloudpickle==1.3.0\r\nclyent==1.2.2\r\ncolorama==0.4.3\r\nconda==4.8.2\r\nconda-build==3.18.11\r\nconda-package-handling==1.6.0\r\nconda-verify==3.4.2\r\ncontextlib2==0.6.0.post1\r\ncryptography==2.8\r\ncycler==0.10.0\r\ncymem==2.0.3\r\nCython==0.29.15\r\ncytoolz==0.10.1\r\ndask==2.11.0\r\ndecorator==4.4.1\r\ndefusedxml==0.6.0\r\ndiff-match-patch==20181111\r\ndistributed==2.11.0\r\ndocutils==0.16\r\nemoji==0.5.4\r\nentrypoints==0.3\r\net-xmlfile==1.0.1\r\nfastcache==1.1.0\r\nfilelock==3.0.12\r\nflake8==3.7.9\r\nFlask==1.1.1\r\nfsspec==0.6.2\r\nfuture==0.18.2\r\ngast==0.3.3\r\ngevent==1.4.0\r\nglob2==0.7\r\ngmpy2==2.0.8\r\ngoogle-auth==1.14.1\r\ngoogle-auth-oauthlib==0.4.1\r\ngoogle-pasta==0.2.0\r\ngplaycli==3.29\r\ngraphviz==0.13.2\r\ngreenlet==0.4.15\r\ngrpcio==1.28.1\r\nh5py==2.10.0\r\nHeapDict==1.0.1\r\nhtml5lib==1.0.1\r\nhypothesis==5.5.4\r\nidna==2.9\r\nimageio==2.6.1\r\nimagesize==1.2.0\r\nimportlib-metadata==1.5.0\r\nintervaltree==3.0.2\r\nipykernel==5.1.4\r\nipython==7.12.0\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.1\r\nisort==4.3.21\r\nitsdangerous==1.1.0\r\njdcal==1.4.1\r\njedi==0.14.1\r\njeepney==0.4.2\r\nJinja2==2.11.1\r\njoblib==0.14.1\r\njson5==0.9.1\r\njsonschema==3.2.0\r\njupyter==1.0.0\r\njupyter-client==5.3.4\r\njupyter-console==6.1.0\r\njupyter-core==4.6.1\r\njupyterlab==1.2.6\r\njupyterlab-server==1.0.6\r\nKeras==2.4.3\r\nKeras-Applications==1.0.8\r\nkeras-models==0.0.7\r\nKeras-Preprocessing==1.1.0\r\nkeyring==21.1.0\r\nkiwisolver==1.1.0\r\nlazy-object-proxy==1.4.3\r\nlibarchive-c==2.8\r\nlief==0.9.0\r\nllvmlite==0.31.0\r\nlocket==0.2.0\r\nlxml==4.5.0\r\nMarkdown==3.2.1\r\nMarkupSafe==1.1.1\r\nmatlink-gpapi==0.4.4.5\r\nmatplotlib==3.1.3\r\nmccabe==0.6.1\r\nmistune==0.8.4\r\nmkl-fft==1.0.15\r\nmkl-random==1.1.0\r\nmkl-service==2.3.0\r\nmock==4.0.1\r\nmore-itertools==8.2.0\r\nmpmath==1.1.0\r\nmsgpack==0.6.1\r\nmultipledispatch==0.6.0\r\nmurmurhash==1.0.2\r\nnavigator-updater==0.2.1\r\nnbconvert==5.6.1\r\nnbformat==5.0.4\r\nnetworkx==2.4\r\nnltk==3.4.5\r\nnose==1.3.7\r\nnotebook==6.0.3\r\nnumba==0.48.0\r\nnumexpr==2.7.1\r\nnumpy==1.19.0\r\nnumpydoc==0.9.2\r\noauthlib==3.1.0\r\nolefile==0.46\r\nopencv-python==4.2.0.34\r\nopenpyxl==3.0.3\r\nopt-einsum==3.2.1\r\npackaging==20.1\r\npandas==1.0.1\r\npandocfilters==1.4.2\r\nparso==0.5.2\r\npartd==1.1.0\r\npath==13.1.0\r\npathlib==1.0.1\r\npathlib2==2.3.5\r\npathtools==0.1.2\r\npatsy==0.5.1\r\npep8==1.7.1\r\npexpect==4.8.0\r\npickleshare==0.7.5\r\nPillow==7.0.0\r\npkginfo==1.5.0.1\r\nplac==1.1.3\r\npluggy==0.13.1\r\nply==3.11\r\npreshed==3.0.2\r\nprometheus-client==0.7.1\r\nprompt-toolkit==3.0.3\r\nprotobuf==3.11.3\r\npsutil==5.6.7\r\nptyprocess==0.6.0\r\npy==1.8.1\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npyaxmlparser==0.3.24\r\npycodestyle==2.5.0\r\npycosat==0.6.3\r\npycparser==2.19\r\npycrypto==2.6.1\r\npycurl==7.43.0.5\r\npydocstyle==4.0.1\r\npydot==1.4.1\r\npyflakes==2.1.1\r\nPygments==2.5.2\r\npylint==2.4.4\r\npyodbc===4.0.0-unsupported\r\npyOpenSSL==19.1.0\r\npyparsing==2.4.6\r\npyrsistent==0.15.7\r\nPySocks==1.7.1\r\npytest==5.3.5\r\npytest-arraydiff==0.3\r\npytest-astropy==0.8.0\r\npytest-astropy-header==0.1.2\r\npytest-doctestplus==0.5.0\r\npytest-openfiles==0.4.0\r\npytest-remotedata==0.3.2\r\npython-dateutil==2.8.1\r\npython-jsonrpc-server==0.3.4\r\npython-language-server==0.31.7\r\npytz==2019.3\r\nPyWavelets==1.1.1\r\npyxdg==0.26\r\nPyYAML==5.3\r\npyzmq==18.1.1\r\nQDarkStyle==2.8\r\nQtAwesome==0.6.1\r\nqtconsole==4.6.0\r\nQtPy==1.9.0\r\nrequests==2.23.0\r\nrequests-oauthlib==1.3.0\r\nrope==0.16.0\r\nrsa==4.0\r\nRtree==0.9.3\r\nruamel-yaml==0.15.87\r\nscikit-image==0.16.2\r\nscikit-learn==0.23.1\r\nscipy==1.4.1\r\nseaborn==0.10.0\r\nSecretStorage==3.1.2\r\nSend2Trash==1.5.0\r\nsimplegeneric==0.8.1\r\nsingledispatch==3.4.0.3\r\nsix==1.14.0\r\nsklearn==0.0\r\nsnowballstemmer==2.0.0\r\nsortedcollections==1.1.2\r\nsortedcontainers==2.1.0\r\nsoupsieve==1.9.5\r\nspacy==2.2.4\r\nSphinx==2.4.0\r\nsphinxcontrib-applehelp==1.0.1\r\nsphinxcontrib-devhelp==1.0.1\r\nsphinxcontrib-htmlhelp==1.0.2\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==1.0.2\r\nsphinxcontrib-serializinghtml==1.1.3\r\nsphinxcontrib-websupport==1.2.0\r\nspyder==4.0.1\r\nspyder-kernels==1.8.1\r\nSQLAlchemy==1.3.13\r\nsrsly==1.0.2\r\nstatsmodels==0.11.0\r\nsympy==1.5.1\r\ntables==3.6.1\r\ntblib==1.6.0\r\ntensorboard==2.2.2\r\ntensorboard-plugin-wit==1.6.0.post3\r\ntensorflow==2.2.0\r\ntensorflow-estimator==2.2.0\r\ntensorflow-gpu==2.2.0\r\ntermcolor==1.1.0\r\nterminado==0.8.3\r\ntestpath==0.4.4\r\nthinc==7.4.0\r\nthreadpoolctl==2.1.0\r\ntoolz==0.10.0\r\ntornado==6.0.3\r\ntqdm==4.42.1\r\ntraitlets==4.3.3\r\nujson==1.35\r\nunicodecsv==0.14.1\r\nurllib3==1.25.9\r\nwasabi==0.6.0\r\nwatchdog==0.10.2\r\nwcwidth==0.1.8\r\nwebencodings==0.5.1\r\nWerkzeug==1.0.1\r\nwidgetsnbextension==3.5.1\r\nwrapt==1.12.1\r\nwurlitzer==2.0.0\r\nxlrd==1.2.0\r\nXlsxWriter==1.2.7\r\nxlwt==1.3.0\r\nxmltodict==0.12.0\r\nyapf==0.28.0\r\nzict==1.0.0\r\nzipp==2.2.0\r\n\r\n\r\nThe dataset to run the program is in https://github.com/Neetu162/DeepLearningResearch/tree/master/Data/badging_med\r\n\r\nThere is no other dependency of the code to any other external interface or file.", "@Neetu162 \r\nI ran the code shared,Please find the [gist here.](https://colab.research.google.com/gist/Saduf2019/1cd0e8c47bea2dc142126783c9c2a7e2/untitled263.ipynb)", "I have raised it to scikit team now https://github.com/scikit-learn/scikit-learn/issues/17874 .. Please keep this issue open in case scikit team comes back with any tensorflow related issue", "@Neetu162 \r\nCan you please update if we may move this to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41066, "title": "Rename tf.nn.swish to tf.nn.silu to give appropriate credit", "body": "The swish was originally coined the \"SiLU\" in https://arxiv.org/pdf/1606.08415.pdf and https://arxiv.org/abs/1702.03118 long before the swish paper. Renaming other peoples' exact same ideas is unacceptable and tensorflow's naming convention implicitly erases the research and work of people outside of Google.\r\nThis request inspired by a [recent discussion](https://www.reddit.com/r/MachineLearning/comments/hkiyir/r_google_has_a_credit_assignment_problem_in/), but this problem has been brought up every few months for the past few years. In light of recent efforts to make the ML community more equitable and _fair_, this is a no-brainer and long overdue.\r\n\r\n**Will this change the current api? How?**\r\ntf.nn.swish ought to be renamed tf.nn.silu and both of the aforementioned papers will be cited in the documentation. At the very least they should be documented in the swish documentation already.", "comments": ["Thank you for bringing this to our attention. Due to backwards compatibility constraints, we cannot remove tf.nn.swish, but we will expose the same functionality as tf.nn.silu to give proper credit to the earlier invention."]}, {"number": 41065, "title": "[TFLite, 16x8] Quantization 16x8, fix for inference_input(output)_type", "body": "This is a small fix for the new options in TFLite converter to make them work with int16 activations:\r\n\r\n`converter.inference_input_type = tf.int16`\r\n`converter.inference_output_type = tf.int16`", "comments": ["Hi @MeghnaNatraj Thanks for the review. I updated except for the test _testPostTrainingIntegerAllowFloatQuantization_\r\nIt uses default full integer conversion without setting target_spec.supported_ops and as result, activations are in int8.\r\n\r\nWhen activations are supposed to be in int8, but inference_input_type is set to int16, what is the expected behaviour ?\r\n1. error\r\n2. quant/dequant nodes from int16 to int8 and back.\r\nThanks", "It should throw as error, but we don't need a test for this. [For reference: int8 quantization supports uint8/int8 input/output types and int16x8 quantization supports int16 input/output types]\r\n\r\n@wwwind For `testPostTrainingIntegerAllowFloatQuantization`, you can make an exact copy of your updated test `testPostTrainingIntegerNoFloatQuantization` and further make the following modifications:\r\n\r\n0. [Avoid copy-paste error] Ensure you re-name it as `testPostTrainingIntegerAllowFloatQuantization`\r\n1. Remove the `use_target_ops_flag` from the test.\r\n2. \r\nReplace \r\n```\r\nif use_target_ops_flag:\r\n    quantized_converter.target_spec.supported_ops = [\r\n        lite.OpsSet.TFLITE_BUILTINS_INT8\r\n    ]\r\n    if quantization_16x8:\r\n      quantized_converter.target_spec.supported_ops = [\r\n          lite.OpsSet.\\\r\n          EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\r\n      ]\r\n    else:\r\n      quantized_converter.target_spec.supported_ops = [\r\n          lite.OpsSet.TFLITE_BUILTINS_INT8\r\n      ]\r\n  else:\r\n    quantized_converter.target_spec.supported_types = [lite.constants.INT8]   \r\n```\r\n\r\nwith \r\n\r\n```\r\nif quantization_16x8:\r\n  quantized_converter.target_spec.supported_ops = [\r\n       lite.OpsSet.\\\r\n          EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\r\n       lite.OpsSet.TFLITE_BUILTINS\r\n      ]\r\n```", "Hi @MeghnaNatraj Thank you for the review!\r\n\r\nI corrected the change so that now it will be an error if there is a mismatch in type of quantization and requested inference_input(output)_type.\r\nI added a separate test for the raised error and for the case \"allow float\" as suggested.", "Hi @MeghnaNatraj Thanks for the review! Corrected.", "@gbaned  Feel free to merge this PR."]}, {"number": 41064, "title": "fix docker envs", "body": "Related https://github.com/tensorflow/tensorflow/issues/4078 https://github.com/tensorflow/tensorflow/issues/4267\r\n\r\n```libcuda.so``` always under ```/usr/lib/x86_64-linux-gnu/```\r\n\r\n![Screenshot from 2020-07-07 12-53-25](https://user-images.githubusercontent.com/17592563/86713719-f0515480-c050-11ea-9992-f1be65355cd4.png)\r\n", "comments": ["@angerson Can you please review this PR ? Thanks!", "@angerson Can you please review this PR ? Thanks!"]}, {"number": 41063, "title": "How to load tensorflow lite during the reboot process ", "body": "Hi, \r\n\r\nI am using Raspberry Pi 4, Model B, 8 GB RAM, Raspbian OS, Python 3.7. \r\nI am running on an object detection script that uses edgetpu.tflite model.  It works perfectly. Now I am trying to run that script automatically when I power up the Pi. \r\n\r\nI have tried this solution: \r\n\r\n    sudo nano /home/pi/.bashrc\r\n\r\nadd \r\n\r\n    echo Running at boot \r\n    sudo python /home/pi/sample.py\r\n\r\nit gives me this error: \r\n    \r\n        Running at boot\r\n    Traceback (most recent call last):\r\n      File \"sample.py\", line 92, in <module>\r\n        from tensorflow.lite.python.interpreter import Interpreter\r\n    ModuleNotFoundError: No module named 'tensorflow'\r\n\r\n\r\nHow can I run the script which uses modules like TensorFlow and TensorFlow Lite during the reboot or at the power-up? \r\n\r\n", "comments": ["@MevadaRavikumar Could you please let us know if this issue still persists ? \r\nCould you please refer to the similar[ tutorial](https://pythonrepo.com/repo/EdjeElectronics-TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi-python-deep-learning) and [link](https://www.tensorflow.org/lite/examples/on_device_training/overview) ,please let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41063\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41063\">No</a>\n"]}, {"number": 41062, "title": "Memory error when Tensorflow 2 tries to allocate memory on GPU", "body": "**System information**\r\n- OS - Linux Ubuntu 20.04 LTS\r\n- TensorFlow installed inside virtualenv using pip\r\n- TensorFlow version = 2.2.0\r\n- Python version = 3.8.2\r\n- CUDA Version = 11.0\r\n- GPU model = NVIDIA GeForce GTX 1080, memory = 11175MiB\r\n\r\n\r\n**Describe the problem**\r\n\r\nMy nvidia driver has crushed recently, so I had to reinstall it. I managed to install cuda (v 11.0), cudnn and the nvidia driver (450.36.06). I run the nvidia tests (on mnist) and it passed (I could follow the GPU utilization via nvidia-smi). \r\nAfter installing the Tensorflow 2.2. in venv using pip, I run into some problems with symbolic links. Doing\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.test.is_gpu_available()\r\n```\r\nreturned ``False``and run into problem such as this\r\n``\r\n2020-07-03 10:24:44.272145: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:\r\n``\r\nI fixed the above by manually adding the missing links (using ``sudo ln -s ``) inside ``/usr/local/cuda-11.0/lib64``. \r\n\r\nHaving fixed the problems with symlinks I run ``sudo ldconfig`` with clean output.\r\nNow I have the following output in python\r\n```python\r\nimport tensorflow as tf\r\n\r\nprint( \"\\n\".join( [str(s) for s in  tf.config.list_physical_devices() ]  ) )\r\n# outputs\r\n# PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\r\n# PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\r\n# PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\r\n# PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\r\n\r\ntf.test.is_gpu_available()\r\n# returns True\r\n```\r\n\r\nThe problem I encounter currently, is with memory allocation on GPU. Doing \r\n```python\r\nimport tensorflow as tf\r\n\r\nt = tf.random.uniform(shape=[10, 20], dtype=tf.float32)\r\n\r\n# crushes with \r\n# 2020-07-03 16:01:39.472212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device #(/job:localhost/replica:0/task:0/device:GPU:0 with 9845 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n#python: malloc.c:2379: sysmalloc: Assertion `(old_top == initial_top (av) && old_size == 0) || ((unsigned long) (old_size) >= #MINSIZE && prev_inuse (old_top) && ((unsigned long) old_end & (pagesize - 1)) == 0)' failed.\r\n#Aborted (core dumped)\r\n```\r\n\r\nAny ideas why Tensorflow cannot utilize GPU when cuda and cudnn are properly installed ?\r\n\r\n\r\n**Logs**\r\n\r\n```\r\nPython 3.8.2 (default, Apr 27 2020, 15:53:34) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> \r\n>>> import tensorflow as tf\r\n>>> \r\n>>> tf.test.is_gpu_available()\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2020-07-03 15:59:14.239096: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-07-03 15:59:14.267287: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3000000000 Hz\r\n2020-07-03 15:59:14.267698: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0c24000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-03 15:59:14.267710: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-03 15:59:14.269133: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-03 15:59:14.360070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 15:59:14.360411: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f783c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-03 15:59:14.360425: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-07-03 15:59:14.360526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 15:59:14.360784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-07-03 15:59:14.360901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-03 15:59:14.362118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-03 15:59:14.362697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-03 15:59:14.362817: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-03 15:59:14.364159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-03 15:59:14.364461: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-03 15:59:14.364529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-03 15:59:14.364584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 15:59:14.364873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 15:59:14.365113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-07-03 15:59:14.365133: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-03 15:59:14.365478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-03 15:59:14.365487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-07-03 15:59:14.365491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-07-03 15:59:14.365544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 15:59:14.365811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 15:59:14.366070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/device:GPU:0 with 9845 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTrue\r\n>>> \r\n>>> t = tf.random.uniform(shape=[10, 20], dtype=tf.float32)\r\n2020-07-03 16:01:39.467677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 16:01:39.468289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-07-03 16:01:39.468335: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-03 16:01:39.468351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-03 16:01:39.468363: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-03 16:01:39.468374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-03 16:01:39.468386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-03 16:01:39.468397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-03 16:01:39.468408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-03 16:01:39.468459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 16:01:39.468843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 16:01:39.469303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-07-03 16:01:39.469608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 16:01:39.470192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-07-03 16:01:39.470212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-03 16:01:39.470222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-03 16:01:39.470232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-03 16:01:39.470241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-03 16:01:39.470250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-03 16:01:39.470260: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-03 16:01:39.470270: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-03 16:01:39.470306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 16:01:39.470799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 16:01:39.471223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-07-03 16:01:39.471245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-03 16:01:39.471250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-07-03 16:01:39.471255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-07-03 16:01:39.471310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 16:01:39.471776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-03 16:01:39.472212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9845 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\npython: malloc.c:2379: sysmalloc: Assertion `(old_top == initial_top (av) && old_size == 0) || ((unsigned long) (old_size) >= MINSIZE && prev_inuse (old_top) && ((unsigned long) old_end & (pagesize - 1)) == 0)' failed.\r\nAborted (core dumped)\r\n```\r\n\r\n\r\n```\r\n$nvidia-smi\r\nFri Jul  3 16:06:39 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  On   | 00000000:01:00.0  On |                  N/A |\r\n| 13%   55C    P0    63W / 250W |    572MiB / 11175MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1225      G   /usr/lib/xorg/Xorg                 35MiB |\r\n|    0   N/A  N/A      1672      G   /usr/lib/xorg/Xorg                193MiB |\r\n|    0   N/A  N/A      1877      G   /usr/bin/gnome-shell              130MiB |\r\n|    0   N/A  N/A      3326      G   ...AAAAAAAAA= --shared-files      191MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n```\r\n$nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Wed_May__6_19:09:25_PDT_2020\r\nCuda compilation tools, release 11.0, V11.0.167\r\nBuild cuda_11.0_bu.TC445_37.28358933_0\r\n```\r\n", "comments": ["@hayk314 \r\nCan you please refer to issue with similar error:\r\n[link](https://github.com/tensorflow/tensorflow/issues/18652) [link1](https://stackoverflow.com/questions/60487683/why-am-i-getting-memory-allocation-error-even-on-batch-size-1) [link2](https://stackoverflow.com/questions/58616804/tensorflow-gpu-2-0-is-throwing-out-of-memory-on-nvidia-rtx-gpu-card) [link3](https://www.kaggle.com/dansbecker/running-out-of-ram-on-gpu-before-cpu) [link3](https://github.com/tensorflow/tensorflow/issues/25807)", "@Saduf2019 \r\n\r\nthanks for the links, but it seems non of them relates to memory allocation problem of TF2 on GPU (there were cases about a model being too big to fit on GPU, but in my case it's about a tiny tensor). \r\nI ended up downgrading my system, in particular cuda from 11.0 to 10.2 (November build), Nvidia driver from 450 to 440, the gcc  compiler from 9 to 8, etc, and now it works.\r\n\r\nThe issue described above, however, remains.", "@hayk314 When you use pip to install TF2.2, it supports CUDA 10.1. I guess that is why it was not working. Please check the tested build configs [here](https://www.tensorflow.org/install/source#gpu). \r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.2.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 2.0.0 | 7.6 | 10.1\r\n\r\nIt is expected not to work with CUDA 11.0 when you install using pip binary. If you want to use CUDA11.0, then try to build TF from source. \r\n\r\nPlease close this issue if this was resolved for you. Thanks!\r\n\r\n", "In fact CUDA 10.2 and GCC 8 work as well with pip installed tensorflow 2.2 . I'm closing this issue in view of the lack of more detailed responses on what breaks down with cuda 11. Thanks for all the comments.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41062\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41062\">No</a>\n"]}, {"number": 41061, "title": "C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1) tensorflow/python/lib/core/bfloat16.cc", "body": "I'm trying to build tf 1.15 from source (cpu-only), in order to convert an object detection model to tflite.\r\n\r\n<pre>bazel build --config=v1 //tensorflow/tools/pip_package:build_pip_package\r\n</pre>\r\n------------------------\r\n\r\n### System information\r\n\r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04\r\n-   **TensorFlow installed from (source or binary)**: source\r\n-   **TensorFlow version (use command below)**: 1.15\r\n-   **Python version**: 3.8\r\n-   **Installed using virtualenv? pip? conda?: conda\r\n-   **Bazel version (if compiling from source)**: 0.26.1\r\n-   **GCC/Compiler version (if compiling from source)**: 7.5.0\r\n\r\n<pre><font color=\"#CC0000\"><b>ERROR: </b></font>/home/user/tensorflow/tensorflow/python/BUILD:329:1: C++ compilation of rule &apos;//tensorflow/python:bfloat16_lib&apos; failed (Exit 1)\r\ntensorflow/python/lib/core/bfloat16.cc: In function &apos;bool tensorflow::{anonymous}::Initialize()&apos;:\r\ntensorflow/python/lib/core/bfloat16.cc:634:36: error: no match for call to &apos;(tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;) (const char [6], &lt;unresolved overloaded function type&gt;, const std::array&lt;int, 3&gt;&amp;)&apos;\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;\r\n                             const std::array&lt;int, 3&gt;&amp; types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from &apos;&lt;unresolved overloaded function type&gt;&apos; to &apos;PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}&apos;\r\ntensorflow/python/lib/core/bfloat16.cc:638:36: error: no match for call to &apos;(tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;) (const char [10], &lt;unresolved overloaded function type&gt;, const std::array&lt;int, 3&gt;&amp;)&apos;\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;\r\n                             const std::array&lt;int, 3&gt;&amp; types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from &apos;&lt;unresolved overloaded function type&gt;&apos; to &apos;PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}&apos;\r\ntensorflow/python/lib/core/bfloat16.cc:641:77: error: no match for call to &apos;(tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;) (const char [5], &lt;unresolved overloaded function type&gt;, const std::array&lt;int, 3&gt;&amp;)&apos;\r\n   if (!register_ufunc(&quot;less&quot;, CompareUFunc&lt;Bfloat16LtFunctor&gt;, compare_types)) {\r\n                                                                             ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;\r\n                             const std::array&lt;int, 3&gt;&amp; types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from &apos;&lt;unresolved overloaded function type&gt;&apos; to &apos;PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}&apos;\r\ntensorflow/python/lib/core/bfloat16.cc:645:36: error: no match for call to &apos;(tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;) (const char [8], &lt;unresolved overloaded function type&gt;, const std::array&lt;int, 3&gt;&amp;)&apos;\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;\r\n                             const std::array&lt;int, 3&gt;&amp; types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from &apos;&lt;unresolved overloaded function type&gt;&apos; to &apos;PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}&apos;\r\ntensorflow/python/lib/core/bfloat16.cc:649:36: error: no match for call to &apos;(tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;) (const char [11], &lt;unresolved overloaded function type&gt;, const std::array&lt;int, 3&gt;&amp;)&apos;\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;\r\n                             const std::array&lt;int, 3&gt;&amp; types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from &apos;&lt;unresolved overloaded function type&gt;&apos; to &apos;PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}&apos;\r\ntensorflow/python/lib/core/bfloat16.cc:653:36: error: no match for call to &apos;(tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;) (const char [14], &lt;unresolved overloaded function type&gt;, const std::array&lt;int, 3&gt;&amp;)&apos;\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::&lt;lambda(const char*, PyUFuncGenericFunction, const std::array&lt;int, 3&gt;&amp;)&gt;\r\n                             const std::array&lt;int, 3&gt;&amp; types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from &apos;&lt;unresolved overloaded function type&gt;&apos; to &apos;PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}&apos;\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n<font color=\"#4E9A06\">INFO: </font>Elapsed time: 1344.879s, Critical Path: 142.68s\r\n<font color=\"#4E9A06\">INFO: </font>765 processes: 765 local.\r\n<font color=\"#CC0000\"><b>FAILED:</b></font> Build did NOT complete successfully\r\n</pre>\r\n\r\n--------\r\nI've already downgraded numpy to <1.19.0, as mentioned in [#40688](https://github.com/tensorflow/tensorflow/issues/40688), but that didn't solve the problem.\r\n\r\n", "comments": ["For TF 1.15 supported Python versions are 2.7, 3.3-3.7\r\nSee https://www.tensorflow.org/install/source#tested_build_configurations", "@\r\n\r\n> For TF 1.15 supported Python versions are 2.7, 3.3-3.7\r\n> See https://www.tensorflow.org/install/source#tested_build_configurations\r\n\r\nmy python version is 3.6.9, however I have the same problem.", "use numpy version < 1.19.0 to resolve this issue under python3.6 environment.\r\npip install 'numpy<1.19.0'", "my python version is 3.6.9, bazel version is 0.19.2,gcc version is 4.8.4,numpy version is 1.18.5,however I have the same problem\u3002i want compile tf1.13.1 ", "> my python version is 3.6.9, bazel version is 0.19.2,gcc version is 4.8.4,numpy version is 1.18.5,however I have the same problem\u3002i want compile tf1.13.1\r\n\r\n**use bazel version 0.26.1 , gcc version 6.3 for better..**\r\n\r\nthese keras version for dependency installation for TF-1.15. \r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.2\r\n\r\nfor TF-1.13:\r\nnumpy=1.15.4\r\nkeras_applications==1.0.4 \r\nkeras_preprocessing==1.0.2\r\n\r\nit should work.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Have tried all above options it is not working for both cpu as well as GPU versions", "I have tried all above options it is not working as well. Noted that the only difference is my ubuntu version is 18.04, python is 3.7", "For TF 1.15 & 1.14 I have no problem, but for TF2 I have exactly the same problem and have tried all of the above", "**Python** 3.6.9\r\n\r\n**TF-1.14:**\r\npip install 'numpy<1.19.0'\r\npip install -U pip six wheel mock\r\npip  install future\r\npip install keras_applications==1.0.4 --no-deps\r\npip install keras_preprocessing==1.0.2 --no-deps\r\npip install pandas\r\n\r\n**bazel version:** bazel-0.25.2-installer-linux-x86_64.sh\r\n\r\n**TF-1.15 & TF-2.0:**\r\npip install 'numpy<1.19.0'\r\npip install -U pip six wheel mock\r\npip  install future\r\npip install keras_applications==1.0.8 --no-deps\r\npip install keras_preprocessing==1.1.2 --no-deps\r\npip install pandas\r\n\r\n**bazel version:** bazel-0.26.1-installer-linux-x86_64.sh\r\n\r\nduring TF-1.15 & 2.0 build you may face problem related to zlib library. for that you can follow this.\r\n**https://github.com/tensorflow/tensorflow/issues/31196**\r\n\r\ni am able to build for all cases.", "@himanshu816. Look like I was missing `pandas`. The build is now complete. Thanks for your help!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41061\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41061\">No</a>\n", "I think this should remain open. Even the latest version has a conflicting dependency with latest versions of numpy, pandas, etc.", "Duplicate to: #40688 (we can keep track there!)", "> my python version is 3.6.9, bazel version is 0.19.2,gcc version is 4.8.4,numpy version is 1.18.5,however I have the same problem\u3002i want compile tf1.13.1\r\n\r\nbazel clean firstly run after downgrading numpy, it worked for me.", "> **TF-1.15 & TF-2.0:**\r\n> pip install 'numpy<1.19.0'\r\n> pip install -U pip six wheel mock\r\n> pip install future\r\n> pip install keras_applications==1.0.8 --no-deps\r\n> pip install keras_preprocessing==1.1.2 --no-deps\r\n> pip install pandas\r\n> \r\n> \r\n> i am able to build for all cases.\r\n\r\nThanks! This approach worked just fine. No problems with zlib. Successfully built TF 2.3.2 on Ubuntu 18.04 LTS (Python 3.6.9, bazel 3.1.0, numpy 1.19.5, gcc 7.5.0)\r\n", "Python 3.6.9\r\n\r\nTF-1.14:\r\npip install 'numpy<1.19.0'\r\npip install -U pip six wheel mock\r\npip install future\r\npip install keras_applications==1.0.4 --no-deps\r\npip install keras_preprocessing==1.0.2 --no-deps\r\npip install pandas\r\n\r\nbazel version: bazel-0.25.2-installer-linux-x86_64.sh\r\n\r\nUseful for me!"]}, {"number": 41060, "title": "UnknownError:    CUDNN_STATUS_BAD_PARAM", "body": "This bug is a continuation of solving this problem:\r\nhttps://github.com/tensorflow/tensorflow/issues/40877\r\n\r\n**System information**\r\n- Have I written custom code with tensorflow.keras.layers.LSTM layer:\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed: from source \"bazel build --config=opt --config=cuda --config=mkl --copt=-march=native //tensorflow/tools/pip_package:build_pip_package\"\r\n- TensorFlow version: v1.12.1-35676-g89798077df 2.4.0\r\n- Python version: 3.6.9\r\n- Bazel version: Python 3.6.9\r\n- GCC/Compiler version: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: 10.2 / 8.0\r\n- GPU model and memory: NVIDIA GeForce RTX 2080 TI 11G\r\n\r\nThe previously described error no longer occurs. No less, when trying to use the LSTM layer, another error appeared\r\n```\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-24-66b029fffc15> in <module>\r\n      1 # train\r\n----> 2 hist = textClassifier.train(X_train[:10000], y_train[:10000])\r\n      3 hist\r\n\r\n<ipython-input-21-d0b8fb2d36c8> in train(self, X, y, w2v_model, verbose, X2)\r\n    308 \r\n    309     def train(self, X, y, w2v_model=None, verbose=0, X2=None):\r\n--> 310         self.fit(X, y, w2v_model, verbose, X2)\r\n    311         return self.hist\r\n    312 \r\n\r\n<ipython-input-21-d0b8fb2d36c8> in fit(self, X, y, w2v_model, verbose, X2)\r\n    301                                   epochs           = self.epochs,\r\n    302                                   batch_size       = self.batch_size,\r\n--> 303                                   shuffle          = False)\r\n    304 \r\n    305         self.model      = model\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1097                 batch_size=batch_size):\r\n   1098               callbacks.on_train_batch_begin(step)\r\n-> 1099               tmp_logs = train_function(iterator)\r\n   1100               if data_handler.should_sync:\r\n   1101                 context.async_wait()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    838         # Lifting succeeded, so variables are initialized and we can run the\r\n    839         # stateless function.\r\n--> 840         return self._stateless_fn(*args, **kwds)\r\n    841     else:\r\n    842       canon_args, canon_kwds = \\\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2842     with self._lock:\r\n   2843       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2844     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2845 \r\n   2846   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)\r\n   1845                            resource_variable_ops.BaseResourceVariable))],\r\n   1846         captured_inputs=self.captured_inputs,\r\n-> 1847         cancellation_manager=cancellation_manager)\r\n   1848 \r\n   1849   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1921       # No tape is watching; skip to running the function.\r\n   1922       return self._build_call_outputs(self._inference_function.call(\r\n-> 1923           ctx, args, cancellation_manager=cancellation_manager))\r\n   1924     forward_backward = self._select_forward_and_backward_functions(\r\n   1925         args,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    548               inputs=args,\r\n    549               attrs=attrs,\r\n--> 550               ctx=ctx)\r\n    551         else:\r\n    552           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nUnknownError:    CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1130): 'cudnnSetRNNDescriptor_v8( rnn_desc.get(), rnn_algo, rnn_mode, bias_mode, direction_mode, input_mode, data_type, compute_type, math_type, input_size, hidden_size, cell_size, num_layers, dropout_desc.handle(), aux_flags)'\r\n\t [[{{node CudnnRNN}}]]\r\n\t [[functional_3/bidirectional_1/forward_lstm_1/PartitionedCall]] [Op:__inference_train_function_13519]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```\r\n\r\n```bash\r\nroot@RedShark:/usr# python3 -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n2020-07-03 00:10:58.059582: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\nv1.12.1-35676-g89798077df 2.4.0\r\n```\r\nIf I turn off cuda ( os.environ['CUDA_VISIBLE_DEVICES'] = '' ), everything works fine on cpu.\r\n", "comments": ["Probably related to #33148 ", "@ApelSYN,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here along with the dataset you are using. Thanks!", "@amahendrakar \r\nI created a small jupyter notebook in which you can reproduce this bug:\r\nhttps://nomeroff.net.ua/tf/lstm_cuda_error/LSTM_cuda_error_example.zip", "@ApelSYN,\r\nI was able to run the code without any issues on [TF v2.2](https://colab.research.google.com/gist/amahendrakar/0663bb997cf9614630f48212b45d67c0/41060.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/33f9cb91121983a628963b8e5b742927/41060-tf-nightly.ipynb). Please find the attached gist. \r\n\r\nPlease take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu) and check if you are running the compatible CUDA and cuDNN version. Thanks! ", "@amahendrakar\r\nI updated cuda to version 11 (from Nvidia https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=debnetwork ) and everything worked for me even without additional rebuilding from the source. Thanks", "@amahendrakar\r\nStill it took tensorflow to rebuild.", "@amahendrakar  Does Tensorflow 2.x support CUDA version 11? In their [documentation](https://www.tensorflow.org/install/gpu#software_requirements) they mention that it supports only version 10.1.", "hi all, I solved it by set \"_could_use_gpu_kernel = False\". I use tensorflowonspark which opened by yahoo, and with the documents says, there are two ways to use GPU, first is cuDNN kernel. second is generic GPU"]}, {"number": 41059, "title": "C++ compilation error when building tensorflow/examples/android:tensorflow.demo", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: source\r\n- Bazel version (if compiling from source): 3.3.0\r\n- GCC/Compiler version (if compiling from source): Java JDK 11, Android ndk 20 and MSVS 2019\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\nBrief description:\r\nI have been trying to get this demo to work all week, getting various errors along the way.  However, I have run into this problem that doesn't have similar previously reported issues.  I get about 15 min into the bazel build command and it throws this error: ERROR: C:/users/adamr/tensorflow/tensorflow/examples/android/BUILD:23:10: C++ compilation of rule '//tensorflow/examples/android:libtensorflow_demo.so' failed (Exit 1): clang failed: error executing command\r\n\r\nI have tried several versions of Android NDK and tried both MSVS 2017 and 2019. I also have msys64 installed and placed in my computers PATH.\r\n\r\nAfter failed bazel build command, I clear the contents of my _bazel_adamr folder and run bazel clean --expunge.\r\n\r\n\r\nSequence of commands (all run on cmd):\r\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n\r\npython configure.py\r\n***\r\nlocation of python: C:\\Users\\adamr\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\r\nPython libraries: C:\\Users\\adamr\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\r\nROCm support? N\r\nCUDA support? N\r\noptimization flags? Default\r\noverride eigen strong inline? n\r\ninteractively configure WORKSPACE? y\r\nAndroid NDK path: C:/Users/adamr/AppData/Local/Android/android-ndk-r20b\r\nNDK API: default\r\nAndroid SDK: C:/Users/adamr/AppData/Local/Android/Sdk\r\nSDK API: 29\r\nBuild tools: 30.0.0-rc4\r\n***\r\n\r\n\r\nbazel build -c opt --fat_apk_cpu=armeabi-v7a --record_rule_instantiation_callstack --experimental_repo_remote_exec --verbose_failures //tensorflow/examples/android:tensorflow_demo --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAttached error Log:\r\n[TensoflowDemoError.txt](https://github.com/tensorflow/tensorflow/files/4869078/TensoflowDemoError.txt)\r\n\r\n", "comments": ["@Regulater27 \r\nCan you please refer to this issue and let us know if it helps:\r\n#38525 ", "That thread is incredibly helpful, I can't believe I missed it!  Thank you, you can close this issue.", "Moving this to closed status with confirmation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41059\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41059\">No</a>\n"]}, {"number": 41058, "title": "not able to reinstall tensorflow", "body": ">>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@harsh3698 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "Thanks my issue is now resolved, I reinstalled everything based on the suggestions !!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41058\">No</a>\n"]}, {"number": 41057, "title": "Build from Souce Failed Dependencies cudnn_stub.cc", "body": "Hi All,\r\n\r\nI have not been able to figure out what I need to do to fix this error, or what I have done wrong:\r\n\r\n> ERROR: C:/users/jason/tensorflow/tensorflow/stream_executor/cuda/BUILD:319:11: undeclared inclusion(s) in rule '//tensorflow/stream_executor/cuda:cudnn_stub': this rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cudnn_stub.cc': 'tensorflow/stream_executor/cuda/cudnn_version.h'\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> INFO: Elapsed time: 1331.616s, Critical Path: 270.82s\r\n> INFO: 5993 processes: 5993 local.\r\n> FAILED: Build did NOT complete successfully\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from: source\r\n- TensorFlow version:\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.3.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: RTX2070\r\n- Intel(R0 Core i9-9980HK CPU\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe last few steps I completed were:\r\n1. cd tensorflow\r\n2. python ./configure/py\r\n  - No to ROCM support\r\n  - Yes to CUDA support\r\n  - Computer capability 6.1,7.5 (because I have an RTZ2070 and GTX1080TI). The second was not attached while running this.\r\n  - Yes to eigen strong line\r\n  - No to andriod workspace builds\r\n3. bazel build //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n> INFO: From Compiling tensorflow/compiler/xla/shape_util.cc:\r\n> C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\include\\numeric(35): warning C4244: '=': conversion from '_Ty' to '_Ty', possible loss of data\r\n>         with\r\n>         [\r\n>             _Ty=tensorflow::int64\r\n>         ]\r\n>         and\r\n>         [\r\n>             _Ty=int\r\n>         ]\r\n> tensorflow/compiler/xla/shape_util.cc(1570): note: see reference to function template instantiation '_Ty std::accumulate<const __int64*,int,std::multiplies<tensorflow::int64>>(const _InIt,const _InIt,_Ty,_Fn)' being compiled\r\n>         with\r\n>         [\r\n>             _Ty=int,\r\n>             _InIt=const __int64 *,\r\n>             _Fn=std::multiplies<tensorflow::int64>\r\n>         ]\r\n> INFO: From Compiling tensorflow/compiler/xla/service/hlo_lexer.cc:\r\n> C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\include\\utility(152): warning C4244: 'initializing': conversion from '_Ty' to '_Ty2', possible loss of data\r\n>         with\r\n>         [\r\n>             _Ty=google::protobuf::uint64\r\n>         ]\r\n>         and\r\n>         [\r\n>             _Ty2=unsigned int\r\n>         ]\r\n> tensorflow/compiler/xla/service/hlo_lexer.cc(415): note: see reference to function template instantiation 'std::pair<unsigned int,unsigned int>::pair<unsigned int&,size_t,0>(_Other1,_Other2 &&) noexcept' being compiled\r\n>         with\r\n>         [\r\n>             _Other1=unsigned int &,\r\n>             _Other2=size_t\r\n>         ]\r\n> tensorflow/compiler/xla/service/hlo_lexer.cc(415): note: see reference to function template instantiation 'std::pair<unsigned int,unsigned int>::pair<unsigned int&,size_t,0>(_Other1,_Other2 &&) noexcept' being compiled\r\n>         with\r\n>         [\r\n>             _Other1=unsigned int &,\r\n>             _Other2=size_t\r\n>         ]\r\n> ERROR: C:/users/jason/tensorflow/tensorflow/stream_executor/cuda/BUILD:319:11: undeclared inclusion(s) in rule '//tensorflow/stream_executor/cuda:cudnn_stub':\r\n> this rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cudnn_stub.cc':\r\n>   'tensorflow/stream_executor/cuda/cudnn_version.h'\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> INFO: Elapsed time: 1331.616s, Critical Path: 270.82s\r\n> INFO: 5993 processes: 5993 local.\r\n> FAILED: Build did NOT complete successfully\r\n", "comments": ["@JSunRae,\r\nAs mentioned in the [guide](https://www.tensorflow.org/install/source_windows) could you please try building TensorFlow from a different branch like `r2.2` or `r2.3.0-rc0`, instead of the master branch.\r\n\r\nAlso, please take a look at the [tested build configuration](https://www.tensorflow.org/install/source_windows#gpu) while building TensorFlow. Thanks!", "same here", "@amahendrakar \r\nsame with 2.3.0rc1", "> As mentioned in the [guide](https://www.tensorflow.org/install/source_windows) could you please try building TensorFlow from a different branch like `r2.2` or `r2.3.0-rc0`, instead of the master branch.\r\n> \r\n> Also, please take a look at the [tested build configuration](https://www.tensorflow.org/install/source_windows#gpu) while building TensorFlow. Thanks!\r\n\r\n@JSunRae,\r\nAny updates regarding this? Please feel free to close the issue if resolved. Thanks!", "> same with 2.3.0rc1\r\n\r\n@alanpurple,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Unfortunately, I tried both of those. Also retried other various versions. I have tried many different ways as well Bazel, Choco, etc erc I have been trying for 2 weeks and have given up. Every time I get a different issue that I cannot seem to get around. I am sticking with pip install for now.", "There was a time I could build tensorflow with windows 10 easily....  the day was gone....long gone.", "Same with `r2.3`", "Same with the latest 1.15.3 (I know it's an old one, but it's the one Nvidia supports for its latest A100 gpu). \r\n\r\nI am trying the following patch: \r\n\r\n```\r\n--- tensorflow/stream_executor/cuda/BUILD.orig\t2020-08-21 20:25:42.789396468 +0200\r\n+++ tensorflow/stream_executor/cuda/BUILD\t2020-08-21 20:28:36.860735091 +0200\r\n@@ -323,6 +323,7 @@\r\n cc_library(\r\n     name = \"cudnn_stub\",\r\n     srcs = if_cuda_is_configured([\"cudnn_stub.cc\"]),\r\n+    hdrs = glob([\"*.h\"]),\r\n     textual_hdrs = glob([\"cudnn_*.inc\"]),\r\n     deps = if_cuda_is_configured([\r\n         \"@local_config_cuda//cuda:cudnn_header\",\r\n```\r\n\r\nThat doesn't help. ", "@surak I added the following line to the deps list the same cc_library entry that you modified in stream_executor/cuda/BUILD and it seems to work for me on Windows for 2.3.0. Sorry that it's not in diff form because I don't have diff set up machine I'm using. My change looks like this:\r\n\r\n    deps = if_cuda_is_configured([\r\n        \"@local_config_cuda//cuda:cudnn_header\",\r\n        \"//tensorflow/stream_executor/lib\",\r\n        \"//tensorflow/stream_executor/platform:dso_loader\",\r\n \t\"//tensorflow/stream_executor/cuda:cudnn_version\",\r\n    ]),\r\n\r\nAny thoughts, comments?", "Thanks @isleong , I tried it, to no avail. I still get a \r\n\r\n```\r\nn file included from tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc:46:\r\nbazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header/third_party/gpus/cudnn/cudnn.h:59:10: fatal error: cudnn_version.h: No such file or directory\r\n   59 | #include \"cudnn_version.h\"\r\n      |          ^~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n```\r\n\r\nIf I check that directory, `ls -la bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header/third_party/gpus/cudnn/`, I have only \r\n\r\n```sh\r\ndrwxrwxr-x 2 itsame mario  21 Aug 21 20:44 .\r\ndrwxrwxr-x 3 itsame mario  19 Aug 21 20:44 ..\r\nlrwxrwxrwx 1 itsame mario 135 Aug 21 20:44 cudnn.h -> /tmp/eb-Hxo8AO/tmp11Wsjp-bazel-build/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cudnn/include/cudnn.h\r\n```\r\n\r\nSo the file wasn't copied with that instruction. ", "Same with me", "@Zhaopudark Did you manage to fix?", "> @Zhaopudark Did you manage to fix?\r\n\r\nnot yet. I will determine to try compiling tf2.2 with cuda10.2 and cudnn 8.0.2", "> @Zhaopudark Did you manage to fix?\r\n\r\nThanks for your guide and this issue's [https://github.com/tensorflow/tensorflow/issues/41446](https://github.com/tensorflow/tensorflow/issues/41446) first method. I have finally compiled tf2.3 on win10(2004)|python3.8.5|cuda11.0(Update 1)|cudnn8.0.2 successfully and test my original algorithms. It seems OK.\r\n\r\nThe problem about \"dependencies of cudnn_stub.cc\"  is in the file \"//tensorflow/stream_executor/cuda/BUILD\". Considering that I had build successfully with tensorflow master branch(Tf 2.4), I try to find the different part about \"cudnn_stub\" in both \"//tensorflow/stream_executor/cuda/BUILD\" file. The result is that tf2.3\u2018s file less the codes \":cudnn_version\".\r\n\r\nSo, I modified \"//tensorflow/stream_executor/cuda/BUILD\" form line 319 to 329, adding \":cudnn_version\" in the \"cc_library\"s member, \"deps\". As follows.\r\n\r\n```C++\r\ncc_library(\r\n    name = \"cudnn_stub\",\r\n    srcs = if_cuda_is_configured([\"cudnn_stub.cc\"]),\r\n    textual_hdrs = glob([\"cudnn_*.inc\"]),\r\n    deps = if_cuda_is_configured([\r\n        \"@local_config_cuda//cuda:cudnn_header\",\r\n        \"//tensorflow/stream_executor/lib\",\r\n        \"//tensorflow/stream_executor/platform:dso_loader\",\r\n        \":cudnn_version\",\r\n    ]),\r\n)\r\n```\r\nAnother problem about \"CUB\" can find methods in this issue [https://github.com/tensorflow/tensorflow/issues/41803](https://github.com/tensorflow/tensorflow/issues/41803)", "@Zhaopudark turns out that it's somewhere else. cudnn 8 has different headers from that cudnn.h and bazel fails to identify the correct version. It's at `third_party/gpus/cuda_configure.bzl` line 1164:\r\n\r\n```python\r\n    if [int(x) for x in cuda_config.cudnn_version.split(\".\")] < [8, 0]:\r\n      cudnn_headers = [\"cudnn.h\"]\r\n    else:\r\n      cudnn_headers = [\"cudnn_adv_infer.h\",\r\n                       \"cudnn_adv_train.h\",\r\n                       \"cudnn_cnn_infer.h\",\r\n                       \"cudnn_cnn_train.h\",\r\n                       \"cudnn_ops_infer.h\",\r\n                       \"cudnn_ops_train.h\",\r\n                       \"cudnn.h\",\r\n                       \"cudnn_version.h\",\r\n                      ]\r\n```\r\n\r\nThis cudnn version is somehow wrong. \r\n\r\nThis is my patch:\r\n\r\n```diff\r\n--- third_party/gpus/cuda_configure.bzl.orig\t2020-08-26 18:55:08.405838123 +0200\r\n+++ third_party/gpus/cuda_configure.bzl\t\t2020-08-26 19:24:47.466666178 +0200\r\n@@ -1136,10 +1136,11 @@\r\n         out_dir = \"cuda/bin\",\r\n     ))\r\n \r\n-    if [int(x) for x in cuda_config.cudnn_version.split(\".\")] < [8, 0]:\r\n-      cudnn_headers = [\"cudnn.h\"]\r\n-    else:\r\n-      cudnn_headers = [\"cudnn_adv_infer.h\",\r\n+    #if [int(x) for x in cuda_config.cudnn_version.split(\".\")] < [8, 0]:\r\n+    #  cudnn_headers = [\"cudnn.h\"]\r\n+    #else:\r\n+    cudnn_headers = [\"cudnn_adv_infer.h\",\r\n+                       \"cudnn_backend.h\",\r\n                        \"cudnn_adv_train.h\",\r\n                        \"cudnn_cnn_infer.h\",\r\n                        \"cudnn_cnn_train.h\",\r\n```\r\n\r\nThis will break with older cuDNNs, but turns out that because check fails, I really don't care. I only need this version of TensorFlow for the A100 gpus, which only support CUDA 11 and cuDNN 8+ anyway. ", "@JSunRae Is this still an issue? \r\nCould you please try with the latest TF v2.7.0 and have a look at the[ build from source](https://www.tensorflow.org/install/source_windows) , [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu) ?Please let us know if it helps. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41057\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41057\">No</a>\n", "I got it working but from memory I just gave up on building from source"]}, {"number": 41056, "title": "Replaced deprecated functions in exmaples", "body": "Replaced deprecated functions `fit_generator` and `predict_generator`  with `fit` and `predict` respectively. \r\nAs `fit` and `predict` support generators.", "comments": []}, {"number": 41055, "title": "APK size increased", "body": "I am using TensorFlow posenet module but it increases my APK size drastically is there any way to reduce model size.", "comments": ["@truptigoqii \r\nCould you please share the code, fill in issue template for us to analyse the issue faced.", "i m using dependency in app gradle file\r\nimplementation 'org.tensorflow:tensorflow-lite:2.2.0'\r\nand imported posenet Module in my app ", "The posenet tflite model size is 12.7 mb\r\nSee https://www.tensorflow.org/lite/models/pose_estimation/overview\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41054, "title": "Could not find a version that satisfies the requirement tensorflow-gpu==1.15.0", "body": "1.15.0 version is available as can be show here:\r\nhttps://pypi.org/project/tensorflow-gpu/1.15.0/\r\n\r\nHowever while running the command:\r\n\r\n> pip install tensorflow-gpu==1.15.0\r\n\r\nI am getting the following output:\r\n\r\n  Could not find a version that satisfies the requirement tensorflow-gpu==1.15.0 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.9.0, 1.10.0, 1.10.1, 1.11.0, 1.12.0, 1.12.2, 1.12.3, 1.13.1, 1.13.2, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\nNo matching distribution found for tensorflow-gpu==1.15.0\r\n", "comments": ["@uday60 \r\n\r\nI tried in colab and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/2050ea3d295e142484d0cb97f8b2c671/untitled78.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ravikyram Somehow it is working now. I guess someone fixed the issue without accepting the bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41054\">No</a>\n"]}, {"number": 41052, "title": "[XLA] Make postorder stack adds a channel once for all predecessors", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nThis pr slightly changes `ComputeInstructionPostOrder` and make the stack for post order traversal not add the whole channel for every predecessor. And example is the `InstructionPostOrderWithAllReduce` test case in `hlo_computation_test.cc`:\r\n```\r\nHloModule Module\r\n\r\nadd {\r\n  lhs = f32[] parameter(0)\r\n  rhs = f32[] parameter(1)\r\n  ROOT add = f32[] add(lhs, rhs)\r\n}\r\n\r\nENTRY entry {\r\n  param = f32[128] parameter(0), sharding={maximal device=0}\r\n  crs0 = f32[128] all-reduce(param),\r\n    replica_groups={{0}}, channel_id=1, to_apply=add,\r\n    sharding={maximal device=0}\r\n  crs1 = f32[128] all-reduce(param),\r\n    replica_groups={{0}}, channel_id=1, to_apply=add,\r\n    sharding={maximal device=1}\r\n  add = f32[128] add(crs0, crs0), sharding={maximal device=0}\r\n  ROOT t = (f32[128], f32[128]) tuple(add, crs1)\r\n})\r\n```\r\nThe `add` instruction would add `crs0` and `crs1` twice in the old implementation. And this pr avoids that.\r\n\r\nThank you for your time on reviewing this pr.", "comments": ["@gbaned \r\nIt seems that @joker-eph seldom reviews pr unrelated to mlir. Shall we add another reviewer for this? Thank you!", "@yunxing @majnemer could you take a look?", "@dimvar \r\nSure :)."]}, {"number": 41051, "title": "Error with the TF Dataset -Plant_Village ", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Google Collab\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version:  3.6\r\n\r\n**Describe the current behavior**\r\n\r\nIt happens when we(@Mtrejo and @DYuusha) try to use tfds.load of 'plant_village, it throws this error: \r\nDownloadError: Failed to get url https://data.mendeley.com/datasets/tywbtsjrjv/1/files/127d0761-7c63-46f0-b08e-d0d9f7cad9da/Plant_leaf_diseases_dataset_without_augmentation.zip. HTTP code: 404.\r\n\r\nAnd it's because, the url is different from the ones in the docs.\r\n**The correct one:** https://data.mendeley.com/datasets/tywbtsjrjv/1/files/d5652a28-c1d8-4b76-97f3-72fb80f94efc/Plant_leaf_diseases_dataset_without_augmentation.zip \r\n\r\n**The one in plant_village.py**: https://data.mendeley.com/datasets/tywbtsjrjv/1/files/127d0761-7c63-46f0-b08e-d0d9f7cad9da/Plant_leaf_diseases_dataset_without_augmentation.zip\r\n\r\n**Describe the expected behavior**\r\nSuccess of the tfds.load\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1aZ4EjqnNa8g0fWg9lccdJLmr0KU4ZL8r?usp=sharing\r\n\r\n", "comments": ["I forgot to add that, before it works just fine.", "@davequinta \r\nPlease confirm if the issue is resolved, so we could move the issue to closed status.", "No @Saduf2019, it keeps throwing 404 in the URL of the dataset", "![TF_PLANT_VILLAGE](https://user-images.githubusercontent.com/11913893/86438167-40cc5900-bcc3-11ea-9454-02c7c0c5ec86.PNG)\r\n", "Thanks for the PR. Will keep this open till we merge the PR.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41051\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41051\">No</a>\n"]}, {"number": 41050, "title": "add gpu test cases", "body": "Related #41049.\r\nAdd GPU test cases.", "comments": ["@fsx950223  Can you please resolve conflicts? Thanks!", "> @fsx950223 Can you please resolve conflicts? Thanks!\r\n\r\nChanged."]}, {"number": 41049, "title": " Failed launching ResizeNearestNeighbor", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.3.0-rc0\r\n- Python version:3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:7.6.5.32-1+cuda10.2\r\n- GPU model and memory:mx150\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\ntensorflow.python.framework.errors_impl.InternalError: Failed launching ResizeNearestNeighbor [Op:ResizeNearestNeighbor]\r\n![Screenshot from 2020-07-03 13-07-02](https://user-images.githubusercontent.com/17592563/86433753-27142b80-bd2e-11ea-8eec-604f6e875fce.png)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/4867906/tf_env.txt)\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\ntf.image.resize(tf.ones([1,416,416,3]),[200,100],tf.image.ResizeMethod.NEAREST_NEIGHBOR, preserve_aspect_ratio=True)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nRelated https://github.com/google/automl/pull/536\r\n\r\nI used GPU\r\n\r\nI can't meet any bug when I ran GPU kernel tests.", "comments": ["@fsx950223 I am not able to reproduce this in colab. Heres my [gist](https://colab.research.google.com/gist/gowthamkpr/5bff9e1a09be1beebd055ff8b8837d70/untitled293.ipynb). Can you try using CUDA 10.1 and see if you are running into the same issue. Thanks!", "> @fsx950223 I am not able to reproduce this in colab. Heres my [gist](https://colab.research.google.com/gist/gowthamkpr/5bff9e1a09be1beebd055ff8b8837d70/untitled293.ipynb). Can you try using CUDA 10.1 and see if you are running into the same issue. Thanks!\n\nIt worked on tf2.2.", "@fsx950223 If you see in my gist, its working on 2.3.0rc0 too. Can you please be specific about this issue as I am not running into the issue on Colab. ", "> @fsx950223 If you see in my gist, its working on 2.3.0rc0 too. Can you please be specific about this issue as I am not running into the issue on Colab.\r\n\r\nI have uploaded my env info. [tf_env.txt](https://github.com/tensorflow/tensorflow/files/4867906/tf_env.txt)", "I am not able to reproduce the issue. How was tensorflow (that is throwing the error) installed?", "> I am not able to reproduce the issue. How was tensorflow (that is throwing the error) installed?\r\n\r\nI installed tf2.3rc0 in two computers via pip binary, they have same problem.", "I was unable to reproduce either.\r\n\r\nCan you please post output of `pip list` and attach `log.txt` obtained after running `python -vvv -c \"import tensorflow as tf; print(tf.image.resize(tf.ones([1,416,416,3]),[200,100],tf.image.ResizeMethod.NEAREST_NEIGHBOR, preserve_aspect_ratio=True))\" &> log.txt`?", "[log.txt](https://github.com/tensorflow/tensorflow/files/4906414/log.txt)\r\n[pip.txt](https://github.com/tensorflow/tensorflow/files/4906417/pip.txt)\r\n", "I fix the bug by ```apt-get install cuda-cudart-10-2```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41049\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41049\">No</a>\n"]}, {"number": 41048, "title": "Can I compile the source code of TF1.10 with gcc8.3\uff1f", "body": "**I use gcc8.3, but when installing TF1.10, the error is as follows. Can I compile the source code of TF1.10 with gcc8.3\uff1f**\r\n\r\ncommand: `python -m joyTf/ --model deepwalk ...`\r\n\r\n```\r\nLog:\r\n_OPS_PATH=/export/home/lijunli/joyGraph/joyOps/lib/libjoy_ops.so\r\nTraceback (most recent call last):\r\n  File \"/export/home/joyGraph-env/python_3.6/lib/python3.6/runpy.py\", line 183, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/export/home/joyGraph-env/python_3.6/lib/python3.6/runpy.py\", line 142, in _get_module_details\r\n    return _get_module_details(pkg_main_name, error)\r\n  File \"/export/home/joyGraph-env/python_3.6/lib/python3.6/runpy.py\", line 109, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/export/home/lijunli/joyGraph/joyTf/__init__.py\", line 18, in <module>\r\n    from joyTf.python.joy_ops import *\r\n  File \"/export/home/lijunli/joyGraph/joyTf/python/joy_ops/__init__.py\", line 22, in <module>\r\n    from joyTf.python.joy_ops.base import initialize_graph\r\n  File \"/export/home/lijunli/joyGraph/joyTf/python/joy_ops/base.py\", line 34, in <module>\r\n    JOY_OPS = tf.load_op_library(_OPS_PATH)\r\n  File \"/export/home/joyGraph-env/python_3.6/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /export/home/lijunli/joyGraph/joyOps/lib/libjoy_ops.so: undefined symbol: _ZN10tensorflow12OpDefBuilder4AttrENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\r\n```\r\n\r\n**It is strange that this error is not reported when I use tf1.10-gpu on the GPU machine, \r\nbut it is still the same when using other tf1.x-gpu versions.\r\nWhat is the cause of this error\uff1f**\r\n\r\nThank you for the answer!\r\n", "comments": ["@AI-Friend,\r\nTensorFlow 1.10 is not actively supported. Please try installing TensorFlow 2.x or 1.15 and let us know if you are facing the same issue.\r\n\r\nAlso, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#cpu) while building TensorFlow. Thanks!", "Supported build configurations during release time are at https://www.tensorflow.org/install/source#cpu", "I re installed tf1.15.0, and then put libtensorflow_ framework.so .1 copy to /usr/lib, then call or report this error. Do I have to compile the source code of TF1.15.0 with gcc8.3?\r\n------------------------------------------------------------------------------------------------------------------\r\nwhen I use \"add_definitions(-D _GLIBCXX_USE_CXX11_ABI=0)\" in CMakeList,  Another error occurred: \r\nTraceback (most recent call last):\r\n  File \"/export/home/joyGraph-env/python_3.6/lib/python3.6/runpy.py\", line 183, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/export/home/joyGraph-env/python_3.6/lib/python3.6/runpy.py\", line 142, in _get_module_details\r\n    return _get_module_details(pkg_main_name, error)\r\n  File \"/export/home/joyGraph-env/python_3.6/lib/python3.6/runpy.py\", line 109, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/export/home/lijunli/joyGraph/joyTf/__init__.py\", line 18, in <module>\r\n    from joyTf.python.joy_ops import *\r\n  File \"/export/home/lijunli/joyGraph/joyTf/python/joy_ops/__init__.py\", line 22, in <module>\r\n    from joyTf.python.joy_ops.base import initialize_graph\r\n  File \"/export/home/lijunli/joyGraph/joyTf/python/joy_ops/base.py\", line 34, in <module>\r\n    JOY_OPS = tf.load_op_library(_OPS_PATH)\r\n  File \"/export/home/joyGraph-env/python_3.6/lib/python3.6/site-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /export/home/lijunli/joyGraph/joyEngine/lib/libjoygraph.so: undefined symbol: _ZNK4Json5Value14getMemberNamesEv\r\n\r\nThank you for your answer!", "This looks like a link issue, you need to find which library provides `_ZNK4Json5Value14getMemberNamesEv` and make sure you link against that/load it. I don't think this comes from TensorFlow (on a quick search, I could not find a `Json::Value::getMemberNames` method in TF code)", "Yes, this problem has been solved. Thank you very much\uff01\r\n", "Perfect. Closing the issue as resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41048\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41048\">No</a>\n"]}, {"number": 41047, "title": "[INTEL MKL] Comparison and cast op fusion", "body": "In this PR, comparison op(bf16/fp32->bool) and cast op(bool->bf16/fp32) is fused into one(bf16/fp32->bf16/fp32). Related eigen functors were added in [eigen MR](https://gitlab.com/libeigen/eigen/-/merge_requests/87). The reason to do such fusion is that comparison functors do not support vectorization, but new added `scalar_cmp_with_cast_op` functor supports vectorization. This can help greatly improve the performance of comparison op, especially for bf16.\r\n\r\nHowever, to show the preformance improvement of BF16, we still need https://github.com/tensorflow/tensorflow/pull/40962 to be merged to get the benefit of vectorization.", "comments": ["@ShengYang1 Can you please resolve conflicts? Thanks!", "Conflicts resolved.", "UT was failed since there is no XLA_CPU kernel registered, so I change the remapper code to make it fuse only for CPU.", "This PR was rolled back in 3f49de584cb7a9de005457006132f5720cc14dd6. May I know the reason? What should I do to bring it back?", "```\r\nNo registered '_GreaterWithCast' OpKernel for 'CPU' devices compatible with node {{node ToFloat}}\r\n\t.  Registered:  <no registered kernels>\r\n```\r\n\r\nRewrite accidentally swapped nodes for unsupported types?", "```\r\n{{node Greater_6}} = Greater[T=DT_FLOAT, _output_shapes=[[?,1,50,1]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Reshape_28, Greater_1/y) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n\r\n{{node ToFloat_12}} = Cast[DstT=DT_FLOAT, SrcT=DT_BOOL, Truncate=false, _output_shapes=[[?,1,50,1]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Greater_6) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n```", "I did a little bit of debugging, and it seems like it's purely internal problem. I'll try to roll it forward tomorrow.", "Thanks! Please let me know If I can provide any help.", "@ezhulenev  Any update on that roll back?", "Still waiting for internal team to fix the problem on their side. The gist is that they use selective op registration, and they are running large integration test to verify that they can register all TF ops in their binary. Hopefully will get some results this week."]}, {"number": 41045, "title": "Error when loading a Subclass model with tf.keras.Sequential blocks inside ", "body": "#### System information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): pip install tensorflow-gpu\r\nTensorFlow version: 2.2.0\r\nPython version: 3.7.0\r\nCUDA/cuDNN version: 11.0 / v8.0.1\r\nGPU model and memory: NVIDIA V100\r\nProblem encountered:\r\n\r\n\r\n#### Problem encountered:\r\n\r\nPlease excuse me if this is not a bug but my lack of understanding on model saving. I will make it down right away if so.\r\n\r\nI've written a code training ResNet-50 model using tf.keras.Model and tf.keras.layers.Layer APIs as below. As you can see, this a very common example of a Subclass model using tf.keras.Sequential() API as part of the model to stack Residual blocks.  \r\n\r\nThe problem is that I can save the model without any issue using \r\n``` model.save(MODEL_DIR, save_format='tf') ``` However, loading the trained model using ``` tf.keras.models.load_model(MODEL_DIR)``` is a pain since it throws errors like the below. I'm wondering whether using tf.keras.Sequential() API inside tf.keras.Model class possibly causes this issue. The error seems to point out that something is missing with sequential blocks. \r\n\r\n\r\n``` \r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-58-ce50dde7e33e> in <module>\r\n----> 1 tf.keras.models.load_model(MODEL_DIR)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)\r\n    188     if isinstance(filepath, six.string_types):\r\n    189       loader_impl.parse_saved_model(filepath)\r\n--> 190       return saved_model_load.load(filepath, compile)\r\n    191 \r\n    192   raise IOError(\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py in load(path, compile)\r\n    114   # TODO(kathywu): Add saving/loading of optimizer, compiled losses and metrics.\r\n    115   # TODO(kathywu): Add code to load from objects that contain all endpoints\r\n--> 116   model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n    117 \r\n    118   # pylint: disable=protected-access\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, loader_cls)\r\n    602       loader = loader_cls(object_graph_proto,\r\n    603                           saved_model_proto,\r\n--> 604                           export_dir)\r\n    605       root = loader.get(0)\r\n    606       if isinstance(loader, Loader):\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py in __init__(self, *args, **kwargs)\r\n    186     self._models_to_reconstruct = []\r\n    187 \r\n--> 188     super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n    189 \r\n    190     # Now that the node object has been fully loaded, and the checkpoint has\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py in __init__(self, object_graph_proto, saved_model_proto, export_dir)\r\n    121       self._concrete_functions[name] = _WrapperFunction(concrete_function)\r\n    122 \r\n--> 123     self._load_all()\r\n    124     self._restore_checkpoint()\r\n    125 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _load_all(self)\r\n    213 \r\n    214     # Finish setting up layers and models. See function docstring for more info.\r\n--> 215     self._finalize_objects()\r\n    216 \r\n    217   @property\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _finalize_objects(self)\r\n    508 \r\n    509     # Initialize graph networks, now that layer dependencies have been resolved.\r\n--> 510     self._reconstruct_all_models()\r\n    511 \r\n    512   def _unblock_model_reconstruction(self, layer_id, layer):\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_all_models(self)\r\n    539       raise ValueError('Error when loading from SavedModel -- the following '\r\n    540                        'models could not be initialized: {}'\r\n--> 541                        .format(uninitialized_model_names))\r\n    542 \r\n    543   def _reconstruct_model(self, model_id, model, layers):\r\n\r\nValueError: Error when loading from SavedModel -- the following models could not be initialized: ['sequential_72', 'sequential_78', 'sequential_63', 'sequential_75', 'sequential_67', 'sequential_73', 'sequential_79', 'sequential_71', 'sequential_62', 'sequential_68', 'sequential_74', 'sequential_66']\r\n```\r\n\r\n\r\nModel construction \r\n\r\n```\r\nclass BottleNeck(tf.keras.layers.Layer):  \r\n    expansion = 4 \r\n\r\n    def __init__(self, in_channels, out_channels, stride=1):\r\n        super(BottleNeck, self).__init__()\r\n        self.conv1 = tf.keras.layers.Conv2D(filters=out_channels, kernel_size=(1, 1), use_bias=False)  \r\n        self.bn1 = tf.keras.layers.BatchNormalization()\r\n        self.pad1 = tf.keras.layers.ZeroPadding2D(padding=(1, 1))\r\n        self.conv2 = tf.keras.layers.Conv2D(filters=out_channels, kernel_size=(3, 3), strides=(stride, stride), use_bias=False) \r\n        self.bn2 = tf.keras.layers.BatchNormalization()\r\n        self.conv3 = tf.keras.layers.Conv2D(filters=out_channels * BottleNeck.expansion, kernel_size=(1, 1), use_bias=False)\r\n        self.bn3 = tf.keras.layers.BatchNormalization()\r\n        \r\n        self.downsample = tf.keras.Sequential()\r\n\r\n        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:            \r\n            self.downsample.add(tf.keras.layers.Conv2D(filters=out_channels * BottleNeck.expansion, kernel_size=(1, 1), strides=(stride, stride), use_bias=False))\r\n            self.downsample.add(tf.keras.layers.BatchNormalization())\r\n  \r\n    def call(self, inputs, training=None):\r\n        out = self.conv1(inputs)\r\n        out = self.bn1(out)\r\n        out = tf.nn.relu(out)\r\n\r\n        out = self.pad1(out)\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = tf.nn.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n        \r\n        down = self.downsample(inputs)\r\n        out += down\r\n        out = tf.nn.relu(out)\r\n\r\n        return out\r\n\r\n    \r\nclass ResNet(tf.keras.Model):\r\n    def __init__(self, dataset, block, num_blocks, num_classes):\r\n        super(ResNet, self).__init__()        \r\n        self.dataset = dataset\r\n        if self.dataset.startswith('cifar'):\r\n            self.in_channels = 64\r\n            self.pad1 = tf.keras.layers.ZeroPadding2D(padding=(1, 1))\r\n            self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), use_bias=False)  \r\n            self.bn1 = tf.keras.layers.BatchNormalization()\r\n            self.relu = tf.nn.relu\r\n\r\n            self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\r\n            self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\r\n            self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2) \r\n            self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2) \r\n            self.avgpool = tf.keras.layers.AveragePooling2D(pool_size=(4, 4))\r\n            self.fc = tf.keras.layers.Dense(num_classes)\r\n\r\n    def _make_layer(self, block, out_channels, num_blocks, stride=1, training=None):     \r\n        strides = [stride] + [1] * (num_blocks - 1)\r\n        layers = tf.keras.Sequential()        \r\n        for stride in strides:\r\n            layers.add(block(self.in_channels, out_channels, stride))\r\n            self.in_channels = out_channels * block.expansion\r\n            \r\n        return layers\r\n\r\n    def call(self, x, training=None):\r\n        if self.dataset == 'cifar10' or self.dataset == 'cifar100':\r\n            \r\n            x = self.pad1(x)\r\n            x = self.conv1(x)\r\n            x = self.bn1(x)\r\n            x = self.relu(x)\r\n            x = self.layer1(x, training=training)\r\n            x = self.layer2(x, training=training)\r\n            x = self.layer3(x, training=training)\r\n            x = self.layer4(x, training=training)\r\n            x = self.avgpool(x)\r\n            x = tf.keras.layers.Flatten()(x)\r\n            x = self.fc(x)\r\n\r\n        return x\r\n\r\n```\r\n\r\n\r\n", "comments": ["@jis478 \r\n\r\nCan you please provide colab link or complete code snippet along with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "> @jis478\r\n> \r\n> Can you please provide colab link or complete code snippet along with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\nHere comes the link:\r\nhttps://colab.research.google.com/drive/1pBpAwl66CJc5wlPVcO-TX7b5F-LCXew4?usp=sharing\r\n\r\nThe last line loading a model causes the initialization error. Thanks! \r\n", "I have tried in colab with TF version 2.2,2.3-rc0,nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/9a742b6398bb2b9ea83d32267f41feda/untitled76.ipynb).Thanks!", "Any updates? I'm looking forward to the solution. Thank you for the support! :)", "Any updates? I'm looking forward to the solution. Thank you for the support! :)", "I have tried in colab with TF version 2.3,nightly version(`2.4.0-dev20200807`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a14593f48e66f8fe8f1e96357bbb72ac/untitled227.ipynb).Thanks!", "@jis478 I tried to reproduce the issue in TF 2.5 and it seems loading the saved model was fixed but getting different error. Please check the gist [here](https://colab.research.google.com/gist/saikumarchalla/8cc5d3a7c4e7630da115adf4ea036d49/untitled227.ipynb#scrollTo=_xaduIxmUkq9&uniqifier=1).", "@jis478  Please let us know the update on this issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41045\">No</a>\n", "anyone has solved the problem?"]}, {"number": 41044, "title": "Update array_ops.py", "body": "### Updated the line 4487 according to issue #41005, previous pull request #41037 \r\n\r\n> **Now documentation says :**\r\n> > `x` is a Tensor which is of same type as `y`, and has a shape broadcastable with `condition` and `y`.\r\n> > `y` is a Tensor which is of  same type as `x`, and has a shape broadcastable with `condition` and `x`.", "comments": ["Nit: Should have used the same PR, just amend the commit / add a new one.\r\n\r\nAlso, note that in general we don't accept a single line non-functional change PR as the amount of CI time needed for the integrate is too large compared to the gains of the change. In the future, when fixing typos, try to fix all typos in a file/directory/API area.", "Sorry for that, I will make sure this will not happen again."]}, {"number": 41043, "title": "TF-Lite Micro: port Silicon Labs STK3701A to Micro Speech example", "body": "This ports the  Micro Speech example to the Silicon Labs STK3701A development board.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41043) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@sldriedler Thank you for your contribution. Can you please sign CLA? Thanks!", "@gbaned My company, Silicon Laboratories signed an agreement. My email is in the group: @silabs-nn-contributors.googlegroups.com\r\n", "@sldriedler Can you use please make sure to use same GitHub username and email-id associated with it. Thanks! ", "@sldriedler  Any update on this PR? Please. Thanks!", "@gbaned Hi, there was an issue with the CLA. This issue should now be resolved. My email: dan.riedler@silabs.com is associated to the silabs-ot-dev Google group.", "@gbaned Are you able to provide any insight as to why the PR is being blocked?\r\n\r\nMy github user email is: dan.riedler@silabs.com\r\nThe committer email of this PR is the same.\r\nMy email is associated with silabs-ot-dev Google group\r\nAnd that google group has a signed CLA.\r\n\r\nIs there anything I'm missing? Any help is very much appreciated. Thanks!\r\n\r\n", "@sldriedler your username and email are not associated , we see a CLA with email dan.riedler@silabs.com from feb 2019, but not the username, can you please make sure both are linked.", "@rthadur Thank you for your response, however, I am still not sure what I need to do. My Github username (sldriedler) has a primary email of dan.riedler@silabs.com. I have just made this email 'public' in the Github account settings. What else do I need to do to associate my username and email?\r\n\r\nThank you for your help", "@sldriedler i am not sure why it still shows no CLA , is it possible to sign the CLA again with all the information ?\r\n\r\ncc @mihaimaruseac ", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41043) for more info**.\n\n<!-- ok -->", "@rthadur rthadur I closed this and opened another PR: https://github.com/tensorflow/tensorflow/pull/41568\r\nThe CLA looks good to go. Thank you for your help."]}, {"number": 41042, "title": "tf.keras.layers.LSTM + tf.function fails to compute jacobian with pfor on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `Yes`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 16.04`\r\n- TensorFlow installed from (source or binary): `Binary`\r\n- TensorFlow version (use command below): `v1.12.1-34938-g99fea8da0d 2.3.0-rc0`\r\n- Python version: `3.7`\r\n- CUDA/cuDNN version:\r\n```\r\n$ conda list | grep cud\r\ncudatoolkit               10.1.243             h6bb024c_0\r\ncudnn                     7.6.5                cuda10.1_0\r\n```\r\n- GPU model and memory: `Nvidia GeForce GTX 1080 Ti`\r\n\r\n**Describe the current behavior**\r\nTensorFlow crashes when computing `GradientTape.jacobian`s for an output of `tf.keras.layers.LSTM` within a `tf.function`, when running on GPU.\r\n\r\n**Describe the expected behavior**\r\nThe graph compiles correctly and efficiently computes the jacobian.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nbatch_size, sequence_length = 2, 3\r\n\r\nx_input = tf.keras.layers.Input(\r\n    shape=(sequence_length, 1),\r\n    name='input',\r\n    dtype=tf.float32)\r\n\r\nmask_input = tf.keras.layers.Input(\r\n    shape=(sequence_length, ),\r\n    name='mask',\r\n    dtype=tf.bool)\r\n\r\n\r\nout = tf.keras.layers.LSTM(\r\n    units=8,\r\n    return_sequences=True,\r\n    return_state=False,\r\n)(x_input, mask=mask_input)\r\nout = tf.keras.layers.Dense(1, activation='linear')(out)\r\nmodel = tf.keras.Model((x_input, mask_input), out)\r\n\r\nx = tf.random.uniform(\r\n    (batch_size, sequence_length, x_input.shape[-1]),\r\n    dtype=x_input.dtype)\r\n\r\nmask = tf.sequence_mask(\r\n    tf.random.uniform(\r\n        (batch_size, ), minval=0, maxval=sequence_length, dtype=tf.int32),\r\n    maxlen=sequence_length,\r\n)[..., ::-1]\r\n\r\n\r\n@tf.function(experimental_relax_shapes=True)\r\ndef compute_jacobian():\r\n    y_true = tf.zeros(batch_size)\r\n    with tf.GradientTape() as tape:\r\n        y = model((x, mask))\r\n        y = tf.reduce_sum(y, axis=1)\r\n        loss = tf.losses.MSE(y_pred=y, y_true=y_true)\r\n\r\n    jacobian = tape.jacobian(\r\n        loss, model.trainable_variables, experimental_use_pfor=True)\r\n\r\n    return jacobian\r\n\r\n\r\njacobian = compute_jacobian()\r\n```\r\n\r\n**Other info / logs**\r\nRunning the above code results in a huge error trace and finally outputs:\r\n> NotImplementedError: Vectorization tried to stack variant tensor Tensor(\"gradients/while_grad/gradients/grad_ys_4/pfor/Identity:0\", shape=(), dtype=variant). This is likely because vectorization of that variant is not fully supported yet.\r\n\r\n\r\n\r\nI know that the pfor flag is experimental, and setting `experimental_use_pfor=False` would make the code run. However, in that case the resulting graph runs so slow that it's effectively unusable even for simple 2-element jacobian. Using `parallel_iterations=10, experimental_use_pfor=False` above results in the following warning, which might have something to do with the slowness:\r\n> 2020-07-03 08:31:26.889383: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node while/enter/_15 was passed bool from functional_1/lstm/PartitionedCall:5 incompatible with expected int32.\r\n2020-07-03 08:31:26.933046: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] layout failed: Out of range: src_output = 26, but num_outputs is only 26\r\n2020-07-03 08:31:26.978710: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node while/enter/_15 was passed bool from functional_1/lstm/PartitionedCall:5 incompatible with expected int32.\r\n2020-07-03 08:31:27.036554: W tensorflow/core/common_runtime/process_function_library_runtime.cc:773] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node while/enter/_15 was passed bool from functional_1/lstm/PartitionedCall:5 incompatible with expected int32.\r\n\r\n\r\nAny workarounds would also be much appreciated and I'd even be happy to contribute a fix for this if one would be doable without much c++ experience.", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/c258822d98863ca2e1f87916c37b7a4c/41042.ipynb), [TF v2.3.0-rc0](https://colab.research.google.com/gist/amahendrakar/0b5ace2e9d091705666e738a358af444/41042.ipynb#scrollTo=_WYaX6Exdta-) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/8b26929993d9afed3253f0c3fa140dfe/41042.ipynb#scrollTo=_WYaX6Exdta-) (i.e. v2.4.0-dev20200703). Please find the attached gist. Thanks!", "Hi @hartikainen, I think this this is the same bug as [#37053](https://github.com/tensorflow/tensorflow/issues/37053)?\r\nSeems like there was a fix but now we're seeing a new error, which is the `NotImplementedError: Vectorization tried to stack variant tensor` message you are reporting.", "@nikitamaia yeah the underlying issue could definitely be the same.", "After some investigation, it looks like this is a known issue and is being worked on. I can update this thread when there is a fix. ", "Hello- is this still being tracked? I think I hit this same issue in `2.4.0-dev20200929`. I'm running on a CPU on Mac OS X.", "Yep, still being tracked. There is currently discussion on the best solution but no complete fix yet.", "Blocked here too. Happy to help contribute towards a fix. Thanks @nikitamaia ", "Hi all, please see the last comment in #37053, which seems to be the same underlying issue and is now fixed. With nightly I was able to run the code originally posted by @hartikainen. Let me know if anyone is still seeing problems when running with tf-nightly.", "@nikitamaia thank you for the update! Confirmed that the fix in #37053 is working for calculating the Jacobian with parallel_for enabled.", "@nikitamaia Thanks a lot for the update! I tried running the above example on `tensorflow/tensorflow:nightly-gpu` docker image and got the following error:\r\n```\r\nroot@e65f2b9ffe99:/# python ./test.py\r\n2020-10-06 07:08:55.053619: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-06 07:08:56.794028: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-06 07:08:56.794903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-06 07:08:56.810727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-06 07:08:56.811586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-06 07:08:56.812436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-06 07:08:56.813253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties:\r\npciBusID: 0000:09:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-06 07:08:56.814075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 4 with properties:\r\npciBusID: 0000:89:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-06 07:08:56.814103: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-06 07:08:56.816613: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-10-06 07:08:56.817725: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-06 07:08:56.818398: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-06 07:08:56.821055: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-06 07:08:56.821640: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\r\n2020-10-06 07:08:56.821804: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-10-06 07:08:56.829746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3, 4\r\n2020-10-06 07:08:56.835858: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-06 07:08:57.382372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-06 07:08:57.383177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-06 07:08:57.383987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-06 07:08:57.384761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties:\r\npciBusID: 0000:09:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-06 07:08:57.385592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 4 with properties:\r\npciBusID: 0000:89:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-06 07:08:57.385629: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-06 07:08:57.385665: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-10-06 07:08:57.385684: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-06 07:08:57.385702: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-06 07:08:57.385720: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-06 07:08:57.385737: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\r\n2020-10-06 07:08:57.385758: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-10-06 07:08:57.393534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3, 4\r\n2020-10-06 07:08:57.393578: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-06 07:08:59.787869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-06 07:08:59.787930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2 3 4\r\n2020-10-06 07:08:59.787943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y Y Y N\r\n2020-10-06 07:08:59.787951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N Y Y N\r\n2020-10-06 07:08:59.787958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   Y Y N Y N\r\n2020-10-06 07:08:59.787966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   Y Y Y N N\r\n2020-10-06 07:08:59.787974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 4:   N N N N N\r\n2020-10-06 07:08:59.794655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7072 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2020-10-06 07:08:59.796640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7598 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2020-10-06 07:08:59.799090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 8433 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2020-10-06 07:08:59.800892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 7072 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\n2020-10-06 07:08:59.803057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 8387 MB memory) -> physical GPU (device: 4, name: GeForce GTX 1080 Ti, pci bus id: 0000:89:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:Using a while_loop for converting Case\r\n2020-10-06 07:09:02.567240: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 1)\r\n2020-10-06 07:09:02.600251: I tensorflow/core/platform/profile_utils/cpu_utils.cc:108] CPU Frequency: 2700110000 Hz\r\n2020-10-06 07:09:02.776378: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:596] layout failed: Invalid argument: Size of values 3 does not match size of permutation 4 @ fanin shape inmodel/dense/BiasAdd-0-TransposeNHWCToNCHW-LayoutOptimizer\r\n2020-10-06 07:09:03.240579: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-10-06 07:09:03.748880: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:596] layout failed: Invalid argument: Size of values 2 does not match size of permutation 4 @ fanin shape inexecute_fn/indexed_case/cond/else/_1/execute_fn/indexed_case/cond/while/body/_107/execute_fn/indexed_case/cond/while/BiasAdd-0-TransposeNHWCToNCHW-LayoutOptimizer\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n```\r\nroot@e65f2b9ffe99:/# python -c \"import tensorflow as tf; print(tf.test.is_gpu_available()); print(tf.__version__)\"\r\nTrue\r\n2.4.0-dev20201005\r\n```\r\n\r\nNot sure if this could be due to some problem in my setup. Or maybe the changes have not yet been picked up by the nightly version in the docker image?", "hmmm this seems to be a different issue? Though a seg fault is never a good thing...does this repro in Colab?", "@nikitamaia it does not because I wasn't able to get the new gpu drivers configured properly on colab, which made the gpus unusable by tensorflow. But I believe it would fail if run on a gpu.", "@hartikainen can you share the code that is resulting in the seg fault? ", "@nikitamaia this was with the same code as in my first message above.", "@nikitamaia I reran the script on my end using the latest `tf-nightly==2.5.0.dev20201106`, and I don't actually see the seg fault anymore. There are a couple of optimization errors and warnings though. Not sure if these are a problem or not, but seems more like these are totally independent of the current issue, which seems to be fixed.\r\n\r\n> 2020-11-07 08:07:15.106824: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like_3/pfor/ZerosLike was passed variant from model/lstm/PartitionedCall:9 incompatible with expected float.\r\n2020-11-07 08:07:15.257045: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like_3/pfor/ZerosLike was passed variant from model/lstm/PartitionedCall:9 incompatible with expected float.\r\n2020-11-07 08:07:15.318805: W tensorflow/core/common_runtime/process_function_library_runtime.cc:807] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/transpose_2_grad/InvertPermutation/pfor/InvertPermutation was passed bool from model/lstm/PartitionedCall:5 incompatible with expected int32.\r\n\r\n<details>\r\n<summary>Full output:</summary>\r\n\r\n```\r\n$ python ./tests/test_gpu_lstm_github_41042.py\r\n2020-11-07 08:06:59.010916: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-07 08:07:01.244734: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-07 08:07:01.246155: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-11-07 08:07:05.463700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 0 with properties:\r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:05.464982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 1 with properties:\r\npciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:05.466219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 2 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:05.467462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 3 with properties:\r\npciBusID: 0000:09:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:05.468704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 4 with properties:\r\npciBusID: 0000:83:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:05.470040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 5 with properties:\r\npciBusID: 0000:84:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:05.472342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 6 with properties:\r\npciBusID: 0000:87:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:05.473593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 7 with properties:\r\npciBusID: 0000:88:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:05.473621: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-07 08:07:05.476765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-07 08:07:05.476824: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-07 08:07:05.478637: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-07 08:07:05.478946: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-07 08:07:05.481011: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-07 08:07:05.481740: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-07 08:07:05.481909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-07 08:07:05.507329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1866] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2020-11-07 08:07:05.509259: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-07 08:07:05.514747: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-07 08:07:06.703321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 0 with properties:\r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:06.705089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 1 with properties:\r\npciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:06.706834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 2 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:06.708577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 3 with properties:\r\npciBusID: 0000:09:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:06.711611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 4 with properties:\r\npciBusID: 0000:83:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:06.714537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 5 with properties:\r\npciBusID: 0000:84:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:06.716320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 6 with properties:\r\npciBusID: 0000:87:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:06.717961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 7 with properties:\r\npciBusID: 0000:88:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-11-07 08:07:06.718009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-07 08:07:06.718059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-07 08:07:06.718090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-07 08:07:06.718120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-07 08:07:06.718149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-07 08:07:06.718179: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-07 08:07:06.718209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-07 08:07:06.718238: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-07 08:07:06.744485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1866] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2020-11-07 08:07:06.744543: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-07 08:07:10.476859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1265] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-07 08:07:10.476916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1271]      0 1 2 3 4 5 6 7\r\n2020-11-07 08:07:10.476928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1284] 0:   N Y Y Y N N N N\r\n2020-11-07 08:07:10.476935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1284] 1:   Y N Y Y N N N N\r\n2020-11-07 08:07:10.476942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1284] 2:   Y Y N Y N N N N\r\n2020-11-07 08:07:10.476949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1284] 3:   Y Y Y N N N N N\r\n2020-11-07 08:07:10.476955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1284] 4:   N N N N N Y Y Y\r\n2020-11-07 08:07:10.476962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1284] 5:   N N N N Y N Y Y\r\n2020-11-07 08:07:10.476968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1284] 6:   N N N N Y Y N Y\r\n2020-11-07 08:07:10.476975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1284] 7:   N N N N Y Y Y N\r\n2020-11-07 08:07:10.492739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10271 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2020-11-07 08:07:10.501169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10271 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2020-11-07 08:07:10.506056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10271 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2020-11-07 08:07:10.513059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10271 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\n2020-11-07 08:07:10.517552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10271 MB memory) -> physical GPU (device: 4, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n2020-11-07 08:07:10.522988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10271 MB memory) -> physical GPU (device: 5, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\n2020-11-07 08:07:10.529496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10271 MB memory) -> physical GPU (device: 6, name: GeForce GTX 1080 Ti, pci bus id: 0000:87:00.0, compute capability: 6.1)\r\n2020-11-07 08:07:10.534045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10271 MB memory) -> physical GPU (device: 7, name: GeForce GTX 1080 Ti, pci bus id: 0000:88:00.0, compute capability: 6.1)\r\n2020-11-07 08:07:14.984163: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-11-07 08:07:15.054012: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2194910000 Hz\r\n2020-11-07 08:07:15.106824: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like_3/pfor/ZerosLike was passed variant from model/lstm/PartitionedCall:9 incompatible with expected float.\r\n2020-11-07 08:07:15.257045: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like_3/pfor/ZerosLike was passed variant from model/lstm/PartitionedCall:9 incompatible with expected float.\r\n2020-11-07 08:07:15.318805: W tensorflow/core/common_runtime/process_function_library_runtime.cc:807] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/transpose_2_grad/InvertPermutation/pfor/InvertPermutation was passed bool from model/lstm/PartitionedCall:5 incompatible with expected int32.\r\n2020-11-07 08:07:15.717394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-07 08:07:16.168825: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n```\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>nvidia-smi</summary>\r\n\r\n```\r\n$ nvidia-smi\r\nSat Nov  7 08:10:23 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\r\n| 26%   33C    P0    58W / 250W |      0MiB / 11178MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A |\r\n| 26%   38C    P0    60W / 250W |      0MiB / 11178MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\r\n| 27%   25C    P0    59W / 250W |      0MiB / 11178MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |\r\n| 27%   31C    P0    61W / 250W |      0MiB / 11178MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |\r\n| 26%   32C    P0    60W / 250W |      0MiB / 11178MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  GeForce GTX 108...  Off  | 00000000:84:00.0 Off |                  N/A |\r\n| 26%   31C    P0    61W / 250W |      0MiB / 11178MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  GeForce GTX 108...  Off  | 00000000:87:00.0 Off |                  N/A |\r\n| 26%   34C    P0    60W / 250W |      0MiB / 11178MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  GeForce GTX 108...  Off  | 00000000:88:00.0 Off |                  N/A |\r\n| 23%   32C    P0    61W / 250W |      0MiB / 11178MiB |      2%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n</details>", "Closing this issue now since original problem has been fixed. @hartikainen if these warnings cause any performance issues for you, feel free to open a new issue as they do not seem to be related. Thanks everyone!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41042\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41042\">No</a>\n"]}, {"number": 41041, "title": "Cuda 11.0 with cuDNN v8.0.1 problem when using GPU : libcudnn.so.7: cannot open shared object file", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Debian GNU/Linux 10 (buster)`\r\n- TensorFlow installed from (source or binary): `binary`\r\n- TensorFlow version: `2.2.0`\r\n- Python version: `3.7.0`\r\n- Installed using virtualenv: from `pyenv` using `pip`\r\n- CUDA/cuDNN version: `11.0 / v8.0.1`\r\n- GPU model and memory: `NVIDIA Corporation TU102 [GeForce RTX 2080 Ti Rev. A]`\r\n\r\n**Problem encountered:**\r\n\r\nI first installed cuda 11.0 using [runfile](https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=runfilelocal):\r\n```bash\r\nwget http://developer.download.nvidia.com/compute/cuda/11.0.1/local_installers/cuda_11.0.1_450.36.06_linux.run\r\nsudo sh cuda_11.0.1_450.36.06_linux.run\r\n```\r\nThen installed cuDNN v8.0.1 (`cuDNN Library for Linux (x86)`) following Nvidia website : [https://developer.nvidia.com/rdp/cudnn-download](https://developer.nvidia.com/rdp/cudnn-download)\r\n\r\n```bash\r\ntar -xzvf cudnn-x.x-linux-x64-v8.x.x.x.tgz\r\n\r\nsudo cp cuda/include/cudnn*.h /usr/local/cuda/include\r\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\r\nsudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*\r\n```\r\n\r\nI configured some PATH variables into my `~/.bashrc` and reload it using `source`:\r\n```bash\r\nexport PATH=/usr/local/cuda-11.0/bin:${PATH}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:${LD_LIBRARY_PATH}\r\nexport CUDA_HOME=/usr/local/cuda\r\n```\r\n\r\n**_Encountered problem_** with `libcudnn.so.7` which is not found: \r\n```bash\r\n2020-07-02 21:06:53.502117: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:/usr/local/cuda/lib64:\r\n2020-07-02 21:06:53.502124: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\n\r\nThe problem was as I installed cuDNN v8.0.1, hence the expected share object file is now (and cannot be found by TensorFlow):\r\n```bash\r\n/usr/local/cuda/lib64/libcudnn.so.8\r\n```\r\nI do not know if I have to reconfigure TensorFlow to tell him the cuDNN version. I fixed the problem (worst fix ever) doing:\r\n\r\n```bash\r\nsudo mv /usr/local/cuda/lib64/libcudnn.so.8 /usr/local/cuda/lib64/libcudnn.so.7\r\n```\r\n\r\nNow, TensorFlow can recognized the share object file... When lauching a neural network training script, now my GPU is used and seems to work well:\r\n```bash\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n...\r\nCreated TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10211 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:2d:00.0, compute capability: 7.5)\r\n```\r\n\r\nCan you please tell me how I can fix this problem following a better way always using my virtualenv ?\r\n", "comments": ["Tensorflow is compiled for a specific CUDA + cuDNN version. Either build from source or down grade these CUDA libraries.\r\n\r\nBy the way people constantly post these kind of questions. Maybe one should train a tensorflow bot to detect questions + auto close and reply to questions like these. ", "Hello,\r\n\r\nThank you for your comment.\r\n\r\nI agree with you and I've never said otherwise before. The problem here is that I want to go through a python virtual environment and unless I'm wrong, it doesn't seem to me that it's possible to retrieve a binary version via the python manager package with the latest version of cuda (or at least target a cuda version). I just wanted to know if it was possible and if so, how.", "@jbuisine \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/39247#issuecomment-628924933) and let us know if it helps.\r\n#40423 [link](https://github.com/tensorflow/tensorflow/issues/40278#issuecomment-643252454)", "I tried to install Tensorflow 2.3.0 as well but the cuDNN 8.0.1 library is always not found.", "As shown in the [documentation](https://www.tensorflow.org/install/source#gpu) for tensorflow 2.2.0 use CUDA 10.1 and CuDNN 7.6 and let me know if the problem still persists. Thanks!", "It seems I need lastest cuda version for my GPU and hence cuDNN 8.0.1, that's why I opened this issue", "@jbuisine As mentioned [here](https://www.tensorflow.org/install/source#gpu), please try using cuDNN 7.6 and let me know if you are running into the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@gowthamkpr Sorry for the delay in answering. My GPU card seems to really need cuda v11.0, hence cuDNN v8.0.1. Compiling tensorflow from scratch works but the issue was using the available binary from pip (it seems to not have v11.0 cuda support as indicated in your link). You can close the issue if you want.", "@jbuisine Glad that its working. I am gonna go ahead and close this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41041\">No</a>\n", "> Tensorflow is compiled for a specific CUDA + cuDNN version. Either build from source or down grade these CUDA libraries.\r\n> \r\n> By the way people constantly post these kind of questions. Maybe one should train a tensorflow bot to detect questions + auto close and reply to questions like these.\r\n\r\nInstead of creating a bot to reply with that answer to people asking this, it would probably be useful if _any_ other place on the internet would mention this. Your comment is the only place where I can find this claim. For example, https://www.reddit.com/r/nvidia/comments/hg45ux/is_cuda_11_compatible_with_tensorflow/ claims that because of backwards compatibility it should work just fine...\r\n\r\nCreating a bot to respond to people is not solving the problem of bad documentation of this fact. It's just solving the symptom of people getting confused by this.", "@jbuisine \r\nRun this command\r\n`source /home/{user_name}/.bashrc`\r\nNote: Replace `{user_name}` with your user name \r\n\r\nYou must have these two environment variables there \"PATH\" & \"LD_LIBRARY_PATH\" and also cuda variable \r\n\r\n```\r\nexport PATH=\"/usr/local/cuda-11.4/bin:$PATH\"\r\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-11.4/lib6\r\n```\r\n\r\nIf you don't have these variables there you must add them. Otherwise, you will face this issue."]}, {"number": 41040, "title": "'Tensor' object has no attribute '_numpy' for `concat`", "body": "Hi! \r\nI'm working on a custom implementation of Physics Informed Deep Learning model from [this paper](https://arxiv.org/abs/1711.10561) and while rewriting a snippet with custom gradient it turns out to be not working for me for latest tensorflow version. \r\n\r\nI've rewritten [this snippet](https://github.com/maziarraissi/PINNs/blob/master/appendix/continuous_time_inference%20(Burgers)/Burgers.py#L105):\r\n\r\n```python3\r\nself.x_u_tf = tf.placeholder(tf.float32, shape=[None, self.x_u.shape[1]])\r\nself.t_u_tf = tf.placeholder(tf.float32, shape=[None, self.t_u.shape[1]])  \r\nself.net_f(self.x_f_tf, self.t_f_tf)\r\n\r\ndef net_u(self, x, t):\r\n    u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\r\n    return u\r\n    \r\ndef net_f(self, x,t):\r\n    u = self.net_u(x,t)\r\n    u_t = tf.gradients(u, t)[0]\r\n    u_x = tf.gradients(u, x)[0]\r\n    u_xx = tf.gradients(u_x, x)[0]\r\n    f = u_t + u*u_x - self.nu*u_xx\r\n```\r\n\r\nto the latest API with GradientTape (**link to collab notebook with error below**) and it throws `AttributeError: 'Tensor' object has no attribute '_numpy'` for `g.gradient(ur, x)`. It turns out that for case without `concat` it works so it looks like `concat` changed behavior.\r\nExample from the paper is operational for older version so I would be thankful for help figuring out this `concat` issue. \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.7\r\n\r\n**Describe the current behavior**\r\n`GradientTape.gradient()` throws `AttributeError: 'Tensor' object has no attribute '_numpy'` for a case that older tensorflow version has been working.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nReproduced error in Colab: \r\n\r\nhttps://colab.research.google.com/drive/1xZ7ZRdtHazS8-gp7uqgdbNE6d-AzUOiS?usp=sharing\r\n", "comments": ["@mtsokol \r\n\r\nI have tried in colab with TF version 2.2 and i am seeing the error message(`NameError: name 'x' is not defined`).please, help me with reproducible code.It helps me in localizing the issue faster.Thanks!", "@ravikyram Hi, thank you for your response! \r\nI'm sorry for this issue, colab looks good to me but I think it will be better if I send gist snippet with exact code: https://gist.github.com/mtsokol/d22adf0637e34a32727f312b9bd8df89\r\n\r\nI've checked it - fails with described error.", "I have tried in colab with TF version 2.2, 2.3-rc0,nightly versions(`2.4.0-dev20200702`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b5fa9ca5e9adcb7ad26e76d393d7fdfd/untitled77.ipynb).Thanks!", "Hi @mtsokol, I think the problem here is the use of the tf.keras placeholder. You'll notice that if you run the code with the newest tf-nightly, even the basic version of GradientTape (ie without concat) also results in an error! The message is:\r\n`ValueError: Passed in object of type <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>, not tf.Tensor\r\nfor g.watch(x)`\r\n\r\nAs you can see [from the docs](https://www.tensorflow.org/api_docs/python/tf/GradientTape#watch), `watch` is expecting a tf.tensor, not KerasTensor.\r\n\r\nIn 2.2 it seems the error you're seeing `'Tensor' object has no attribute '_numpy' for concat` is due to the `_ConcatGradHelper` function ([see source code here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L53)), which is the Gradient for the concat op. You'll notice [in the code here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L118) that  `if context.executing_eagerly()` then it calls the numpy attribute on the tensor, which is not supported for the KerasTensor, hence the error message.\r\n\r\nYou have two options:\r\n1) Not use a placeholder (this is better in the long term since it seems tf-nightly does not allow the placeholder with gradient tape at all)\r\n2) Or to use `tf.compat.v1.disable_eager_execution()` which should allow you to run your code as is without changes.\r\n\r\nHope this helps!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@nikitamaia Hi! Thank you for your answer! Sure, disabling eager execution helped. \r\nI see it now - so as I understand placeholder usage is rather discouraged for 2.0 (as lazy execution)?", "Glad to hear that worked! Yes, in TF 2.x you don't need to use placeholders. If you want your code to execute in graph mode, then you can utilize @tf.function. \r\nDisabling eager execution is a good workaround for now, but I would suggest that for the longer term you consider updating your code to be 2.x friendly! \r\n\r\nI'm closing this issue now since there is no bug and a workaround was found.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41040\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41040\">No</a>\n"]}, {"number": 41039, "title": "Error when building tflite 2.3.0-rc0 metal delegate on macOS", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.2\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0-rc0\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 3.1.0\r\n- GPU model and memory: AMD Radeon R9 M370X 2 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe metal delegate dylib for macOS fails to build using the recommended command from the BUILD file (`tensorflow/lite/delegates/gpu/BUILD:L181`).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFrom `tensorflow-2.3.0-rc0/`:\r\n```\r\nbazel build -c opt --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=default --linkopt -s --strip always --cxxopt=-std=c++14 --apple_platform_type=macos //tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_dylib\r\n```\r\n\r\n**Any other info / logs**\r\nRunning the above command produces error log:\r\n```\r\nbazel build -c opt --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=default --linkopt -s --strip always --cxxopt=-std=c++14 --apple_platform_type=macos //tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_dylib\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=174\r\nINFO: Reading rc options for 'build' from /Users/njassal/dev/tensorflow/tensorflow-2.3.0-rc0/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/njassal/dev/tensorflow/tensorflow-2.3.0-rc0/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /Users/njassal/dev/tensorflow/tensorflow-2.3.0-rc0/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/njassal/.virtualenvs/tensorflow-2.2.0/bin/python3 --action_env PYTHON_LIB_PATH=/Users/njassal/.virtualenvs/tensorflow-2.2.0/lib/python3.6/site-packages --python_path=/Users/njassal/.virtualenvs/tensorflow-2.2.0/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=1\r\nINFO: Found applicable config definition build:v2 in file /Users/njassal/dev/tensorflow/tensorflow-2.3.0-rc0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/njassal/dev/tensorflow/tensorflow-2.3.0-rc0/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:macos in file /Users/njassal/dev/tensorflow/tensorflow-2.3.0-rc0/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /private/var/tmp/_bazel_njassal/c07bdfc7f101779d38a8eaaebedd6122/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nINFO: Repository eigen_archive instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule tf_http_archive defined at:\r\n  /Users/njassal/dev/tensorflow/tensorflow-2.3.0-rc0/third_party/repo.bzl:134:19: in <toplevel>\r\nERROR: /private/var/tmp/_bazel_njassal/c07bdfc7f101779d38a8eaaebedd6122/external/cpuinfo/BUILD.bazel:96:1: Configurable attribute \"srcs\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @cpuinfo//:linux_x86_64\r\n @cpuinfo//:linux_arm\r\n @cpuinfo//:linux_armhf\r\n @cpuinfo//:linux_armv7a\r\n @cpuinfo//:linux_armeabi\r\n @cpuinfo//:linux_aarch64\r\n @cpuinfo//:macos_x86_64\r\n @cpuinfo//:windows_x86_64\r\n @cpuinfo//:android_armv7\r\n @cpuinfo//:android_arm64\r\n @cpuinfo//:android_x86\r\n @cpuinfo//:android_x86_64\r\n @cpuinfo//:ios_x86_64\r\n @cpuinfo//:ios_x86\r\n @cpuinfo//:ios_armv7\r\n @cpuinfo//:ios_arm64\r\n @cpuinfo//:ios_arm64e\r\n @cpuinfo//:watchos_x86_64\r\n @cpuinfo//:watchos_x86\r\n @cpuinfo//:watchos_armv7k\r\n @cpuinfo//:watchos_arm64_32\r\n @cpuinfo//:tvos_x86_64\r\n @cpuinfo//:tvos_arm64\r\nWARNING: Download from https://mirror.bazel.build/github.com/Maratyszcza/FP16/archive/4dfe081cf6bcd15db339cf2680b9281b8451eeb3.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nERROR: Analysis of target '//tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_dylib' failed; build aborted: \r\n\r\n/private/var/tmp/_bazel_njassal/c07bdfc7f101779d38a8eaaebedd6122/external/cpuinfo/BUILD.bazel:96:1: Configurable attribute \"srcs\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @cpuinfo//:linux_x86_64\r\n @cpuinfo//:linux_arm\r\n @cpuinfo//:linux_armhf\r\n @cpuinfo//:linux_armv7a\r\n @cpuinfo//:linux_armeabi\r\n @cpuinfo//:linux_aarch64\r\n @cpuinfo//:macos_x86_64\r\n @cpuinfo//:windows_x86_64\r\n @cpuinfo//:android_armv7\r\n @cpuinfo//:android_arm64\r\n @cpuinfo//:android_x86\r\n @cpuinfo//:android_x86_64\r\n @cpuinfo//:ios_x86_64\r\n @cpuinfo//:ios_x86\r\n @cpuinfo//:ios_armv7\r\n @cpuinfo//:ios_arm64\r\n @cpuinfo//:ios_arm64e\r\n @cpuinfo//:watchos_x86_64\r\n @cpuinfo//:watchos_x86\r\n @cpuinfo//:watchos_armv7k\r\n @cpuinfo//:watchos_arm64_32\r\n @cpuinfo//:tvos_x86_64\r\n @cpuinfo//:tvos_arm64\r\nINFO: Elapsed time: 0.123s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n```\r\n", "comments": ["@teijeong could you check this?", "This was introduced by #34522, but I don't have expertise on this. :(\r\nSimply adding `--cpu=darwin` did not work.\r\n\r\nHi @asus4, do you have some clue?", "hi, I'm facing the same issue #41555 ", "Hi, when I chenged the option `\"cpu\": \"darwin\",` to `\"cpu\": \"darwin_x86_64\"`, the bazel build passed.\r\nhttps://github.com/tensorflow/tensorflow/blob/6a875d47610faeebc95b9875f83330299d141d73/third_party/cpuinfo/BUILD.bazel#L215\r\n\r\nsimilar commit: https://github.com/tensorflow/tensorflow/commit/80567a3468be82ecf992d24052272f5c85d3f567", "Hi @nkjassal ! Could  you try again with above[ suggestion ](https://github.com/tensorflow/tensorflow/issues/41039#issuecomment-664701908)?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41039\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41039\">No</a>\n"]}, {"number": 41038, "title": "Accuracy Benchmarks", "body": "Hi,\r\n\r\nI am trying to find the baseline results for Top-1 accuraciesfor different tensorflow models .\r\nI found more than one link. Could not decide which accuracies are latest and valid results. Below are the links for your reference. Let me know which link to be considered  for baseline/latest.\r\n\r\nhttps://www.tensorflow.org/lite/performance/model_optimization \r\nhttps://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html \r\nhttps://blog.tensorflow.org/2019/06/tensorflow-integer-quantization.html \r\nhttps://www.tensorflow.org/lite/guide/hosted_models \r\n\r\n\r\nThank you,\r\nPraveen.\r\n\r\n", "comments": ["@uu-praveeng \r\nPlease check [link](https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html), as per the latest date on it, and let us know", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]