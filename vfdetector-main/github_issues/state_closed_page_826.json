[{"number": 28750, "title": "add lstm to r1.13", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28750) for more info**.\n\n<!-- need_sender_cla -->", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 28749, "title": "[TF-TRT] Add python test for CombinedNMS", "body": "This test will fail during INT8 calibration until the FakeGPU registration for CombinedNMS in #28745 is merged.", "comments": ["So should I review this after that PR is merged?", "> So should I review this after that PR is merged?\r\n\r\nYes please, or it may help to use this test to validate that #28745 fixes the problem with calibrating CombinedNms.", "@aaroey You can start reviewing it now. It will only compile successfully once #28745 is merged since it uses same nms kernel to do box selection.", "I've confirmed that the changes to `tensorflow/core/kernels/non_max_suppression_op.cc` in\u00a0#28745 fixes the problem with CombinedNMS and allows this test to pass."]}, {"number": 28748, "title": "Counting parameters from Keras model doesn't work correctly with list of layers", "body": "Below is an example that keras model is not able to count correctly the number of parameters when custom keras layers are inside a list that is defined in an another custom layer. \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass TestLayer2(tf.keras.layers.Layer):\r\n    def __init__(self, dim):\r\n        super(TestLayer2, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(dim)\r\n        \r\n    def call(self, x):\r\n        x = self.dense1(x)\r\n        return x\r\n\r\nclass TestLayer1(tf.keras.layers.Layer):\r\n    def __init__(self, dim):\r\n        super(TestLayer1, self).__init__()\r\n        self.list_of_dense = [TestLayer2(dim) for _ in range(2)]\r\n        \r\n    def call(self, x):\r\n        for i in range(len(self.list_of_dense)):\r\n            x = self.list_of_dense[i](x)\r\n        return x\r\n    \r\nclass TestModel(tf.keras.Model):\r\n    def __init__(self, dim):\r\n        super(TestModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(dim)\r\n        self.layer = TestLayer1(dim*2)\r\n        \r\n    def call(self, x):\r\n        x = self.dense1(x)\r\n        x = self.layer(x)\r\n        return x\r\n\r\nt_model = TestModel(512)\r\n\r\ntmp = tf.random.normal([64,512])\r\n\r\nt_model(tmp)\r\n\r\nt_model.summary()\r\n```\r\n\r\nreturns \r\n\r\n```_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_4728 (Dense)           multiple                  262656    \r\n_________________________________________________________________\r\ntest_layer1_13 (TestLayer1)  multiple                  0         \r\n=================================================================\r\nTotal params: 262,656\r\nTrainable params: 262,656\r\nNon-trainable params: 0\r\n```\r\nwhich is not correct. If the nested layers were outside a list then are counted correctly.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass TestLayer2(tf.keras.layers.Layer):\r\n    def __init__(self, dim):\r\n        super(TestLayer2, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(dim)\r\n        \r\n    def call(self, x):\r\n        x = self.dense1(x)\r\n        return x\r\n\r\nclass TestLayer1(tf.keras.layers.Layer):\r\n    def __init__(self, dim):\r\n        super(TestLayer1, self).__init__()\r\n        self.dense1 = TestLayer2(dim)\r\n        self.dense2 = TestLayer2(dim)\r\n        \r\n    def call(self, x):\r\n        x = self.dense1(x)\r\n        x = self.dense2(x)\r\n        return x\r\n\r\nclass TestModel(tf.keras.Model):\r\n    def __init__(self, dim):\r\n        super(TestModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(dim)\r\n        self.layer = TestLayer1(dim*2)\r\n        \r\n    def call(self, x):\r\n        x = self.dense1(x)\r\n        x = self.layer(x)\r\n        return x\r\n\r\nt_model = TestModel(512)\r\n\r\ntmp = tf.random.normal([64,512])\r\n\r\nt_model(tmp)\r\n\r\nt_model.summary()\r\n```\r\n\r\nwhich gives \r\n\r\n```\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_4731 (Dense)           multiple                  262656    \r\n_________________________________________________________________\r\ntest_layer1_14 (TestLayer1)  multiple                  1574912   \r\n=================================================================\r\nTotal params: 1,837,568\r\nTrainable params: 1,837,568\r\nNon-trainable params: 0\r\n```\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0/CuDNN 7.3.1\r\n- GPU model and memory: Nvidia Titan V\r\n", "comments": ["@lioutasb Able to reproduce the issue as mentioned , tested the count of the number of parameters when custom keras layers are inside a list that is defined in an another custom layer.\r\nLayer (type)                 Output Shape              Param #   \r\ndense (Dense)                multiple                  262656    \r\n\r\ntest_layer1 (TestLayer1)     multiple                  0         \r\n\r\nTotal params: 262,656\r\nTrainable params: 262,656\r\nNon-trainable params: 0\r\n\r\nBelow is the output for the nested layers that were outside a list. \r\n\r\nLayer (type)                 Output Shape              Param #   \r\n\r\ndense_3 (Dense)              multiple                  262656    \r\n\r\ntest_layer1_1 (TestLayer1)   multiple                  1574912   \r\n\r\nTotal params: 1,837,568\r\nTrainable params: 1,837,568\r\nNon-trainable params: 0", "@muddham is there any workaround to count fast the number of parameters from a very large network? Manually it would be a pain. ", "@muddham This is not mentioned before but this issue poses a more serious problem because this doesn't let training to be executed for the whole model.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass TestLayer2(tf.keras.layers.Layer):\r\n    def __init__(self, dim):\r\n        super(TestLayer2, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(dim)\r\n        \r\n    def call(self, x):\r\n        x = self.dense1(x)\r\n        return x\r\n\r\nclass TestLayer1(tf.keras.layers.Layer):\r\n    def __init__(self, dim):\r\n        super(TestLayer1, self).__init__()\r\n        self.list_of_dense = [TestLayer2(dim) for _ in range(2)]\r\n        \r\n    def call(self, x):\r\n        for i in range(len(self.list_of_dense)):\r\n            x = self.list_of_dense[i](x)\r\n        return x\r\n    \r\nclass TestModel(tf.keras.Model):\r\n    def __init__(self, dim):\r\n        super(TestModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(dim)\r\n        self.layer = TestLayer1(dim*2)\r\n        \r\n    def call(self, x):\r\n        x = self.dense1(x)\r\n        x = self.layer(x)\r\n        return x\r\n\r\nt_model = TestModel(512)\r\n\r\ntmp = tf.random.normal([64,512])\r\n\r\nt_model(tmp)\r\n\r\nlen(t_model.trainable_variables)\r\n```\r\ngives 2 which is wrong since it only sees the weight and bias matrices from the first dense layer. \r\n\r\nThis means that if you try to train the model using the following code it will only optimize the first dense layer.\r\n\r\n```\r\ngradients = tape.gradient(loss, t_model.trainable_variables)\r\noptimizer.apply_gradients(list(zip(gradients, t_model.trainable_variables)))\r\n```\r\n\r\nI tested the same code on TF2.0 and it works as expected there so it seems only 1.13 has this bug.\r\n", "Keras actually overloads the __setattr__ to check whenever a layer is added. So a list would not trigger it.\r\n\r\nHere is a workaround:\r\n\r\n```\r\nclass TestLayer1(tf.keras.layers.Layer):\r\n    def __init__(self, dim):\r\n        super(TestLayer1, self).__init__()\r\n       \r\n        self.layer_names = ['layer_' + i for i in range(2)]\r\n        for layer_name in self.layer_names:\r\n             self.__setattr__(layer_name, TestLayer2(dim))\r\n                \r\n    def call(self, x):\r\n        for layer_name in self.layer_names:\r\n            x = self.__getattribute__(layer_name)(x)\r\n        return x\r\n```", "@jnd77 I confirm that this workaround works as expected. ", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28748\">No</a>\n"]}, {"number": 28747, "title": "Update array_ops.py", "body": "Added couple of examples for tf.zeros_like", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 28746, "title": "Add RoiAlign Op", "body": "This PR adds RoIAlign op that calculates FPN boxes as described in arXiv:1703.06870", "comments": ["@tfboyd This is part2 of ops that would increase FPN based object detection performance.", "Hi @alextp and @pengchongjin, would you help to review the API part of this PR? Thanks", "@alextp Thanks for the help!", "@alextp  Sorry for the late update. I noticed that grads for the new inputs were missing in earlier commit.", "@pengchongjin, would you help to provide review for the interface from the perspective of object detection?\r\n@reedwm, would you help to review the kernel part of this change?\r\n\r\nThanks.", "@wujingyue just in case you're interested in this op.", "Any progress on this?\r\n", "The ROIAlign Op added here seems to be ported from Detectron and follow its numerics.\r\nI work at vision team at FAIR and I can say that our original version of ROIAlign in Detectron is in fact not aligned perfectly. This issue has no negative impact on accuracy of our models because the convolutional layers will actually compensate for the misalignment. So we did not fix it due to the breaks it will cause. But you might want to use a correct version in tensorflow. The issue can introduce negative impact if the op is used in a different scenario.\r\n\r\nI have written a correct one in tensorflow at [here](https://github.com/tensorpack/tensorpack/blob/bec166ddce24ed9203f68a19c5dc0c1cbf2d5651/examples/FasterRCNN/modeling/model_box.py#L156-L173) with high-level ops. \r\nWhen \"spatial_scale\" ( in the definition of Detectron's ROIAlign) is 1, the incorrect one can produce correct output if rois are offset by 0.5\r\n\r\nAbout this PR itself, it does not make a lot of sense to me that the \"min_level\", \"max_level\" and \"canonical_scale\" are arguments of \"ROIAlign\". They are very specific to the case of applying ROIAlign on FPN. Perhaps fusing all the ROIAlign at different feature levels together into one op can give some speed improvement here, but introducing giant op like this that is specific to one model will make the op less reusable. In my implementation I just call ROIAlign once for each level [at here](https://github.com/tensorpack/tensorpack/blob/bec166ddce24ed9203f68a19c5dc0c1cbf2d5651/examples/FasterRCNN/modeling/model_fpn.py#L103-L130).\r\n\r\nAlso I don't see how using a 5D tensor of [Batch, Level, Channel, Height, Width] as the input will work out. In an actual FPN-based R-CNN models, the features that go into the ROIAlign at each level are of different height and width.", "@ppwwyyxx \r\nCan the single-level ROIAlign be viewed as a special case of this op? \r\n\r\nRegarding the 5D feature tensor [batch, level, channel, height, width], I think height, width here are the padded height and width, not the actual height and width of a particular level. And internally, the op calculates the actual level height and width.\r\nhttps://github.com/tensorflow/tensorflow/pull/28746/files#diff-1c1a01acf243d96e778f7f83bd02f5b2R225", "@pengchongjin \r\nYes, maybe one can achieve a normal ROIAlign by calling this ROIAlign with one level. But my point is this is not the right abstraction for an op named \"ROIAlign\" - by its original definition and name. The op implemented in this PR is an op that applies ROIAlign to FPN features, and with the **heuristics** made by FPN authors to assign rois to FPN levels. \r\n\r\nAn analogy is to write an op called convolution but add an optional pooling option to it, with an heuristics that decides the pooling kernel size from the convolution kernel size, just because some paper does it.\r\n \r\nWriting them in a single op may have some performance benefits, but I'd guess that padding a feature map to a 16x larger one and concatenating all levels may hurt the perf even more.", "Maybe this ROIAlign op should live in addons instead of core TF?", "Sorry for not updating since some time. Vacation and other things got in the way. I will get back to this as soon as possible", "> Maybe this ROIAlign op should live in addons instead of core TF?\r\n\r\nWhat's better is to provide a true ROIAlign in core TF. \"ROIAlign\" is conceptually nothing but \"crop and resize\", so it should be general enough to be added to core, given that `tf.image.crop_and_resize` has been in core for a while.\r\n\r\nIn fact, the existing \"crop_and_resize\" op in TF is close enough to being useful in an object detection model that it only needs to be aligned more perfectly, with some smart coordinate transform. I've discussed this issue a while ago in https://github.com/tensorflow/tensorflow/issues/26278.\r\nIf there is interest, I'm willing to contribute a change to \"crop_and_resize\" to resolve its issues in #26278 and make it become ROIAlign.", "I'd love a change to crop_and_resize, maybe having it named as\ncrop_and_resize_v2 which is a little better to me than ROIAlign.\n\nOn Sat, Jul 20, 2019 at 1:18 AM Yuxin Wu <notifications@github.com> wrote:\n\n> Maybe this ROIAlign op should live in addons instead of core TF?\n>\n> What's better is to provide a true ROIAlign in core TF. \"ROIAlign\" is\n> conceptually nothing but \"crop and resize\", so it should be general enough\n> to be added to core, given that tf.image.crop_and_resize has been in core\n> for a while.\n>\n> In fact, the existing \"crop_and_resize\" op in TF is close enough to being\n> useful in an object detection model that it only needs to be aligned more\n> perfectly, with some smart coordinate transform. I've discussed this issue\n> a while ago in #26278\n> <https://github.com/tensorflow/tensorflow/issues/26278>.\n> If there is interest, I'm willing to contribute a change to\n> \"crop_and_resize\" to resolve its issues in #26278\n> <https://github.com/tensorflow/tensorflow/issues/26278> and make it\n> become ROIAlign.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/28746?email_source=notifications&email_token=AAABHRK6RPD5W7A5SZCH6BLQALC6VA5CNFSM4HNGW5IKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2NJMEQ#issuecomment-513447442>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRN5LYXDTWTVNOUFU7TQALC6VANCNFSM4HNGW5IA>\n> .\n>\n\n\n-- \n - Alex\n", "Can one of the admins verify this patch?", "@samikama Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I believe there is no reason to keep this open anymore. ", "was there a solution?  I think `tf.image.crop_and_resize` is still aligned in the same way?"]}, {"number": 28745, "title": "Add GPU implementation of NMSv2 op", "body": "This PR adds a GPU implementation of NMSv2 Op. It also registers a FakeGPU op for CombinedNonMaxSuppression op to workaround issues encountered due to lack of GPU implementation until a proper GPU implementation can be done based on current GPU kernels.", "comments": ["@tfboyd This is the first part of PRs that would improve performance on object detection networks.", "Test for new op is blocked by the #28744 since GPU tensors are not correctly transferred to host without it.", "Hi @chsigg, could you please help to take a look at this PR?\r\nThanks.", "@aaroey latest changes should address your comments. Please let me know if I missed any.", "Sorry for the delay, there are some internal test failures and I'm still trying to fix them.", "I believe this implementation is wrong: it does not agree with the CPU version of NMS Op.\r\n\r\nIn this implementation, when computing the area in IOU, it uses `(x2-x1+1)*(y2-y1+1)`, as can be seen at: https://github.com/tensorflow/tensorflow/blob/e3062d1924c5d07d07440bc8a7de8d97615a6f52/tensorflow/core/kernels/non_max_suppression_op.cu.cc#L96-L98\r\n\r\nHowever, in the CPU version, it uses `(x2-x1)*(y2-y1)`, as can be seen at: https://github.com/tensorflow/tensorflow/blob/e3062d1924c5d07d07440bc8a7de8d97615a6f52/tensorflow/core/kernels/non_max_suppression_op.cc#L126-L129\r\n\r\nFor many inputs this may not have an effect at all. But for certain inputs the two versions will produce inconsistent results.\r\n\r\nIf your goal is to run object detection models, note that the \"+1\" is a legacy issue and we're trying to avoid the version with \"+1\" in Facebook. See [this PR](https://github.com/pytorch/pytorch/pull/20550) that handles \"+1\" in caffe2.", "@samikama would you please fix the issue @ppwwyyxx mentioned above?", "@samikama I tried to use your kernel from inside python and I am getting a segmentation fault by running this simple script:\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nfrom tensorflow.python.ops import gen_image_ops\r\n\r\nwith tf.device(\"/device:GPU:0\"):\r\n    boxes = tf.constant([[1.0, 1.0, 1.0, 1.0]], dtype=tf.float32)\r\n    scores = tf.constant([1.0], dtype=tf.float32)\r\n    max_output_size = tf.constant(10, dtype=tf.int32)\r\n    iou_threshold = tf.constant(0.7, dtype=tf.float32)\r\n    score_threshold = tf.constant(float('-inf'), dtype=tf.float32)\r\n    print(\"Start\")\r\n    x = gen_image_ops.non_max_suppression_v2(boxes, scores, max_output_size, iou_threshold, score_threshold)\r\n    print(\"End\")\r\n    print(x)\r\n```\r\n\r\n```bash\r\ndocker run --runtime=nvidia -it -v $PWD:/tf -w /tf tensorflow/tensorflow:nightly-gpu-py3\r\n python pyscript.py\r\n```\r\n\r\nThe output is\r\n```\r\nStart\r\nSegmentation fault\r\n```\r\n\r\nCorrect me if I am doing smth wrong", "@ppwwyyxx Thanks for catching that. I made the fixes to support both legacy case and CPU identical implementation in #30893.\r\n@SpaceInvader61 It looks like we missed that some input tensors need to be host tensors when they are changed from attributes to tensors. Can you try with #30893? Also your example is probably not really making use of the nms_v2 since the signature of nms_v2 is \r\n\r\n`non_max_suppression_v2(boxes, scores, max_output_size, iou_threshold, name=None)`\r\n\r\nAnother point is you are passing a box with 0 surface area and that is the only box. Even though there is a single box test in the test suite, we didn't have an invalid box test. I will add the fix for it in an upcoming PR.", "@samikama with .HostMemory() it works, thank you so much! :+1: ", "Has this made it into any of the tensorflow releases? As far as I know, it wasn't included in 1.13 and 1.14. How about in tensorflow 2.0?"]}, {"number": 28744, "title": "Fix ops_testutilcc for GPU tests", "body": "This pr modifies ops_testutil.cc to copy the device tensor to host for test comparisons in GPU tests.", "comments": ["@reedwm can you check this one-liner. It greatly simplifies writing c++ tests for GPU ops.", "Assinging to @chsigg, who wrote the code."]}, {"number": 28743, "title": "Updated TF2.0 examples in logging_ops.py", "body": "Updated all four examples to TF2.0 from TF1.13.1.", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 28742, "title": "TFLite only slightly faster with GPU on Helio P22", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): n/a\r\n- Mobile device: **Xiaomi Redmi 6**\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r1.13\r\n- Python version: n/a\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: **PowerVR GE8320**\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the [TFLite Demo](https://github.com/tensorflow/tensorflow/tree/6d28416/tensorflow/lite/java/demo), average inference time for 1 thread is **480ms**, but it's just **370ms** with GPU. With 4 threads, it's **360ms**.\r\n\r\n**Describe the expected behavior**\r\n\r\nI'd expect GPU performance to be (significantly) better than 4-thread CPU. Granted it's not a great GPU, but I'd expect it to be substantially faster than the CPU.\r\n\r\n**Other info / logs**\r\n\r\nXiaomi Redmi 6:\r\n- OS | Android 8.1 (Oreo), planned upgrade to Android 9.0 (Pie)\r\n- Chipset | Mediatek MT6762 Helio P22 (12 nm)\r\n- CPU | Octa-core 2.0 GHz Cortex-A53\r\n- GPU | PowerVR GE8320\r\n", "comments": ["@munum \r\n\r\nDo you get a warning like\r\n\r\n> WARNING: op code #42 cannot be handled by this delegate.\r\n\r\n?  Also, are you using FP16?", "> @munum\r\n> \r\n> Do you get a warning like\r\n> \r\n> > WARNING: op code #42 cannot be handled by this delegate.\r\n> \r\n> ?\r\n\r\nNo. I'm just using the demo app, the models of which I believe contain only supported ops.\r\n\r\n> Also, are you using FP16?\r\n\r\nI'm using the demo app's default options.\r\n\r\nMine's the Redmi 6 Pro though (Adreno 506).", "Hey :) Were you able to improve the performance with GPU?", "Unfortunately not.  We're making some plumbing to improve parallelism of certain ops.  With a lot of people on vacation, it's going slow.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28742\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28742\">No</a>\n"]}, {"number": 28741, "title": "Add tf.normalize to provide a clean interface for tensor normalization", "body": "**System information**\r\n- TensorFlow version (you are using): 1.3+ or 2.0a\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently there are two \"ways\" to normalize vectors, matrices, and tensors.\r\n1. `tf.math.l2_normalize`\r\n2. `tf.norm`\r\n\r\nThe first one directly normalizes the input tensor and returns it.\r\nThe latter only computes the norm and the user has to do the division to obtain a normalized tensor.\r\n\r\nI propose to add a function called `tf.math.normalize` which has the same/similar `args` as `tf.norm`. It computes the normalized tensor as `tf.math.l2_normalize` does but supports all the normalizations that `tf.norm` supports.\r\n\r\n**Will this change the current api? How?**\r\nNo, it just adds a cleaner interface to do normalization.\r\nIn the long run `tf.math.l2_normalize` would be obsolete, though. It's implementation could be removed and it could use `tf.normalize` internally.\r\n\r\n**Who will benefit with this feature?**\r\nWhen looking for a way to perform l1 normalization I only found `l2_normalize` and `tf.norm`. I then expected there to exist a `l1_normalize` function as well which is not the case.\r\nFrom my point of view this is an inconsistency that people will encounter from time to time.\r\n\r\nI would propose to only add this to tensorflow 2.0 and maybe also remove `tf.math.l2_normalize` in that case.", "comments": ["Sounds reasonable. I don't think we can deprecate the existing endpoint but I'm happy to add a new one, if someone wants to contribute.", "Hey, I'd like to take a crack at this! Do we add this function as tf.normalize, or tf.math.normalize?", "@astropeak, I am also happy to contribute, maybe we can work together on this?", "@sleighsoft , in that case, maybe you could just do it. I haven't started and could always take other tasks.", "But if the cooperation is a better way, I am happy to work together. How to split the tasks? How we commit codes? \r\nAnyway, I am totally OK that only you work on the task if the cooperation isn't a good choice.", "@astropeak It looks like you already have some other parts of tensorflow in your pipeline. I would like to give it a try and would then like to get your advice once I open a PR. If you already have ideas and suggestions feel free to add them here.\r\nMy intial thought was to do it along the lines of `sklearn.preprocessing.normalize` and `numpy.linalg.norm`.", "@sleighsoft @astropeak is someone still working on this? ", "I am, just don't have much spare time right now to make fast progress.", "@sleighsoft I see your PR merged. If this issue was resolved by your PR, Please close. Thanks!"]}, {"number": 28740, "title": "Parallel 2 conv layer but got the same time consumption as serial", "body": "Hi,\r\nI'm trying to study the parallelization of static graphs, so I realize tensorflow codes with 2 convolution layers taking the same input.\r\nHere's my code with tensorflow 1.12.0.\r\n```\r\nimport os\r\nimport time\r\nimport math\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\ndef test(m, session, feed_dict, t):\r\n    for i in range(t):\r\n        session.run([m.y, m.z], feed_dict=feed_dict)\r\n\r\ndef main():\r\n    m = Model()\r\n    session = tf.Session()\r\n    session.run(m.init_op)\r\n    feed_dict = {\r\n        m.x: np.zeros([4, 56, 56, 32]),\r\n    }\r\n    print('hot')\r\n    test(m, session, feed_dict, 100)\r\n    print('start')\r\n    start = time.time()\r\n    test(m, session, feed_dict, 10000)\r\n    print((time.time()-start) / 10000 * 1000)\r\n\r\nclass Model:\r\n    def _conv_layer(self, name, input_var, stride, in_channels, out_channels):\r\n        KERNEL_SIZE = 3\r\n        with tf.variable_scope(name) as scope:\r\n            filter_shape = [KERNEL_SIZE, KERNEL_SIZE, out_channels, in_channels]\r\n            kernel = tf.get_variable(\r\n                'weights',\r\n                shape=filter_shape,\r\n                initializer=tf.truncated_normal_initializer(stddev=math.sqrt(2.0 / KERNEL_SIZE / KERNEL_SIZE / in_channels)),\r\n                dtype=tf.float32\r\n            )\r\n            biases = tf.get_variable(\r\n                'biases',\r\n                shape=[out_channels],\r\n                initializer=tf.constant_initializer(0.0),\r\n                dtype=tf.float32\r\n            )\r\n            output = tf.nn.bias_add(\r\n                tf.nn.conv2d(\r\n                    input_var,\r\n                    kernel,\r\n                    [1, stride, stride, 1],\r\n                    padding='SAME'\r\n                ),\r\n                biases\r\n            )\r\n            return output\r\n\r\n    def __init__(self):\r\n        tf.reset_default_graph()\r\n        self.x = tf.placeholder(tf.float32, [None, 56, 56, 32])\r\n\r\n        self.y = self._conv_layer('layer', self.x, 1, 32, 32)\r\n        self.z = self._conv_layer('layer2', self.x, 1, 32, 32)\r\n\r\n        self.init_op = tf.initialize_all_variables()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nThe time consumption is the same as `self.z = self._conv_layer('layer2', self.y, 1, 32, 32)` (note the `self.y`), so that confuse me about the parallel performance of tensorflow, can any body help to explain that or realize the true parallel way of 2 conv layers (in a single gpu) ?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow cpu/gpu version used. Did you try it from source or binary? How much time it took for parallel and serial time consumption. If you are unclear on the template, please use [template link](https://github.com/tensorflow/tensorflow/issues/new/choose) to fill in the details. Thanks!", "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): python-pip\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: python3.5\r\n- CUDA/cuDNN version: CUDA 9.0.176, cuDNN 7.4.1\r\n- GPU model and memory: NVidia 1080Ti, 11G\r\n- Parallel or Serial time consumption: Same, about 3 ms both", "Tried verifying on Colab with TensorFlow version 1.13.1 (CPU and GPU), found significant time consumption difference between the two. Can you please help me out to know whether the result is consistent with you also or not with the same version. Thanks!  ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28739, "title": "Assigning to ressource variables with varying shapes violates the shape information of the tensor of which is assigned from.", "body": "Hi,\r\nI ran into problems when assigning to variables from tensors with dynamic shape. In the end, it boiled down to using resource variables and having a strided slice tensor from which to assign from, see the upcoming MWE. When assigning to resource variables from a strided slice tensor, the shape information of the tensor becomes corrupt.\r\n\r\n**System information**\r\n- Unexpected behaviour in custom code (minimum working example see below)\r\n- Windows 10\r\n- TensorFlow installed from pip wheels\r\n- TensorFlow version:\r\n  - tf.version.VERSION = 1.13.1\r\n  - tf.version.GIT_VERSION = b'unknown'\r\n  - tf.version.COMPILER_VERSION = MSVC 190024215\r\n  - Sanity check: array([1])\r\n- Python version: Python 3.6.7 (default, Feb 28 2019, 07:28:18) [MSC v.1900 64 bit (AMD64)] on win32\r\n- also occurs without GPU support\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nOutput is\r\n> *results: [1 2] [array([1]), array([1, 2]), array([2])]\r\n> var_value_before [3]\r\n> var_value_after [1 2]\r\n\r\nNote the contradiction that the tensor with elements [1, 2] has shape [1]\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected output\r\n> *results: [1 2] [array([**2**]), array([1, 2]), array([2])]\r\n> var_value_before [3]\r\n> var_value_after [1 2]\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef mwe():\r\n  u = tf.range(3, 4)   # Some tensor of shape (1,)\r\n  v = tf.range(1, 3)   # Some tensor of shape (2,)\r\n\r\n  # Forget about shape of v (Otherwise assign_op will not build with use_resource=True)\r\n  v = tf.placeholder_with_default(v, (None,))\r\n\r\n  # Random stride (leaving this out will not result in error)\r\n  value_to_assign = v[:]\r\n  var = tf.Variable(u, use_resource=True, validate_shape=False)\r\n  assign_op = tf.assign(var, value_to_assign, validate_shape=False)\r\n\r\n  observed_ops = [tf.shape(value_to_assign), value_to_assign, tf.shape(v)]\r\n\r\n  with tf.Session() as sess:\r\n    tf.initializers.global_variables().run()\r\n    var_value_before = sess.run(var)\r\n    results = sess.run([assign_op, observed_ops])\r\n    var_value_after = sess.run(var)\r\n    print('*results:', *results)\r\n    print('var_value_before', var_value_before)\r\n    print('var_value_after', var_value_after)\r\n\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\nmwe()\r\n```\r\n\r\n**Note**\r\nEither `use_resource=False`, leaving out the assign_op when running the session or leaving out the stride op `v[:]` will yield the expected behaviour. This may be tested with the following routine:\r\n\r\n```\r\ndef mwe_options():\r\n  for use_resource in [0, 1]:\r\n    for additional_stride in [0, 1]:\r\n      u = tf.range(3, 4)   # Some tensor of shape (1,)\r\n      v = tf.range(1, 3)   # Some tensor of shape (2,)\r\n\r\n      # Forget about shape of v (Otherwise assign_op will not build with use_resource=True)\r\n      v = tf.placeholder_with_default(v, (None,))\r\n\r\n      # Random stride (leaving this out will not result in error)\r\n      value_to_assign = v[:] if additional_stride else v\r\n      var = tf.Variable(u, use_resource=use_resource, validate_shape=False)\r\n      assign_op = tf.assign(var, value_to_assign, validate_shape=False)\r\n\r\n      observed_ops = [tf.shape(value_to_assign), value_to_assign]\r\n\r\n      with tf.Session() as sess:\r\n        tf.initializers.global_variables().run()\r\n        for add_assign_op in [0, 1]:\r\n          if add_assign_op:\r\n            _, result = sess.run([assign_op, observed_ops])\r\n          else:\r\n            result = sess.run(observed_ops)\r\n          var_value = sess.run(var)\r\n          print(f'use_resource={use_resource}, additional_stride={additional_stride}, add_assign_op={add_assign_op}: {result}, var = {var_value}')\r\n```\r\nOutput:\r\n\r\n> use_resource=0, additional_stride=0, add_assign_op=0: [array([2]), array([1, 2])], var = [3]\r\n> use_resource=0, additional_stride=0, add_assign_op=1: [array([2]), array([1, 2])], var = [1 2]\r\n> use_resource=0, additional_stride=1, add_assign_op=0: [array([2]), array([1, 2])], var = [3]\r\n> use_resource=0, additional_stride=1, add_assign_op=1: [array([2]), array([1, 2])], var = [1 2]\r\n> use_resource=1, additional_stride=0, add_assign_op=0: [array([2]), array([1, 2])], var = [3]\r\n> use_resource=1, additional_stride=0, add_assign_op=1: [array([2]), array([1, 2])], var = [1 2]\r\n> use_resource=1, additional_stride=1, add_assign_op=0: [array([2]), array([1, 2])], var = [3]\r\n> use_resource=1, additional_stride=1, add_assign_op=1: [array([1]), array([1, 2])], var = [1 2]", "comments": ["@ChristianReinbold Able to reproduce the issue and 2 kinds of outputs with the provided code.As this is resolved, please let us know if this issue can be closed.", "@muddham Currently I bypass this issue by exploiting that the shape of `v` still is intact although `value_to_assign` is broken. If you consider the described behaviour as \"expected\", feel free to close the issue.", "@ezhulenev the error disappears if I disable the grappler constant folding with `config.graph_options.rewrite_options.constant_folding = 2`", "@rmlarsen is the constant folding expert.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28739\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28739\">No</a>\n"]}, {"number": 28738, "title": "keras fit_generator ignores verbose parameter for validation_data", "body": "Setting `verbose=0` in `tf.keras.model.fit_generator()` has no effect if `validation_data` is provided.\r\nThe progress is still displayed for each validation step.\r\nThis is annoying if you have a callback that is responsible for printing progress.\r\n\r\n**System information**\r\n- running in docker container tensorflow/tensorflow:latest-py3-jupyter\r\n- `tf.GIT_VERSION`: v1.13.1-0-g6612da8951\r\n- `tf.VERSION`: 1.13.1 \r\n- `tf.keras.__version__`: 2.2.4-tf\r\n\r\n**Describe the current behavior**\r\nProgress-bar is displayed regardless of `verbose=0`\r\n\r\n**Describe the expected behavior**\r\nNothing should be printed\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Dense(1, input_shape=(1,))\r\n])\r\nmodel.compile(loss=\"mae\", optimizer=\"adam\")\r\n\r\ndef generator():\r\n    i=0\r\n    while 1:\r\n        yield (np.array([i]),[i])\r\n        i+=1\r\nvalData = (np.arange(10), np.arange(10))\r\n\r\nhistory = model.fit_generator(generator(), steps_per_epoch=5, verbose=0, validation_data=valData)\r\n```\r\nOutput:\r\n> 10/10 [==============================] - 0s 6ms/sample - loss: 11.3572\r\n\r\nOmitting `validation_data`  stops the output from appearing\r\n\r\nLink to an example notebook:\r\nhttps://colab.research.google.com/drive/1SrTdFXD_SCxu-gvo-h2YjFiAkO1CDaLG", "comments": ["@WaeCo This is duplicate of [#28648](https://github.com/tensorflow/tensorflow/issues/28648). ", "You are right, apparently I did not see this in my search"]}, {"number": 28737, "title": "Minimal ACL Integration", "body": "Adds interfaces to offload kernels to Arm Compute Library.\r\nThis is a minimal integration effort and further steps are needed.\r\nBazel dependency is still not in place, thus the makefile system can be used for testing.\r\nMore details can be found in README.md file under the acl backend.", "comments": ["Hi @GeorgeARM, looks like you've been busy! Let's sync offline about next steps on this, we'd like to patch this in and run some tests on our end.", "Can one of the admins verify this patch?", "@GeorgeARM can you please resolve conflicts ?", "Per offline conversation, this PR is mostly a prototype and proof of concept. We won't be landing directly as-is, however, we will be looking at hosting and developing this externally, and coordinating on integration and use with TFLite.", "Abandoning this PR."]}, {"number": 28736, "title": "Update the flatbuffers md5 checksum to match the update to version 1.11.0", "body": "When flatbuffers was updated to version 1.11 in Tensorflow Lite Micro, the MD5 checksum in third_party_downloads.inc was not updated, so the command\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile test\r\nfails. This fixes that issue.", "comments": []}, {"number": 28735, "title": "Wrong MD5 checksum for flatbuffers when building Tensorflow Lite Micro using make", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: ba63891c8b\r\n- Python version: NA\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen running\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile test\r\nthe command fails when downloading the flatbuffers archive, since there is a mismatch with the MD5 sums.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile test\r\n\r\n**Any other info / logs**\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp                        \r\ndownloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\" \"3811552512049fac3af419130904bc55\" tensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers                           \r\ndownloading http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\r\nChecksum error for 'http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz'. Expected 3811552512049fac3af419130904bc55 but found 02c64880acb89dbd57eebacfd67200d8                                                                                               \r\ntensorflow/lite/experimental/micro/tools/make/Makefile:198: recipe for target 'tensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers' failed                                                                                                                                 \r\nmake: *** [tensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers] Error 1\r\n\r\n", "comments": ["Here's a PR that fixes the issue: https://github.com/tensorflow/tensorflow/pull/28736", "Thank you for the PR. Now that it landed, I think this can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28735\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28735\">No</a>\n"]}, {"number": 28734, "title": "Persistent colors on detected objects when using TFLite demo app", "body": "<em>I would like persisent colors on detected objects when using the TFLite demo app on Android</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nDetected objects in the TFLite demo app show rectangles around them with a confidence number. These rectangles change color and make a certain use case I have rather difficult. I'd like the colors to be persisent, let's say I have 3 labels:\r\nLabel A - GREEN\r\nLabel B - RED\r\nLabel C - BLUE\r\nand so on.\r\n\r\n**Will this change the current api? How?**\r\nI don't want to change the current API but I would appreciate a suggestion on how to change this.\r\n\r\n**Who will benefit with this feature?**\r\nI will.\r\n\r\n**Any Other info.**\r\nNo.\r\n", "comments": ["@Symphonized \r\n\r\nWarning: As this doesn't appear to be a bug with Tensorflow, the better to may ask on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nThis can be achieved on following code changes,\r\n1. [Classifier.java](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/Classifier.java)\r\n```\r\n    public Recognition(\r\n        final String id, final String title, final Float confidence, final RectF location, final int lablePos) {\r\n      this.id = id;\r\n      this.labelPos = lablePos;\r\n      this.title = title;\r\n      this.confidence = confidence;\r\n      this.location = location;\r\n    }\r\n\r\n    private final int labelPos;\r\n    public int getLablePosition() {\r\n      return labelPos;\r\n    }\r\n```\r\n\r\n2. [TFLiteObjectDetectionAPIModel.java](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java) \r\n\r\n```\r\n      recognitions.add(\r\n          new Recognition(\r\n              \"\" + i,\r\n              labels.get((int) outputClasses[0][i] + labelOffset),\r\n              outputScores[0][i],\r\n              detection,\r\n                  (int) outputClasses[0][i] + labelOffset));\r\n``` \r\n3. [MultiBoxTracker.java](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tracking/MultiBoxTracker.java)\r\n\r\n```\r\n      if (potential.second.getLablePosition()  == 87) {//scissors [0 base index]\r\n        trackedRecognition.color = Color.parseColor(\"#0D0068\"); // CYAN for scissors \r\n      } else if (potential.second.getLablePosition()  == 33) {//suitcase\r\n        trackedRecognition.color = Color.parseColor(\"#AA33AA\"); // MAGENTA for suitcase \r\n      } else {\r\n        trackedRecognition.color = COLORS[trackedObjects.size()];\r\n      }\r\n```\r\n\r\n Thanks!", "@Symphonized : Will it be possible to put some light on the feature you are proposing as in APIs that would get affected and the users who will be benefited with this change. Meanwhile you can also have a look on @Dayananda-V's suggestion. Thanks! ", "Thank you @Dayananda-V. Yes I realized this question might not suit the intended goal here. Thanks anyway for helping me find a solution even though it might have seemed trivial to you.\r\n\r\nAll the good.\r\n\r\nThanks."]}, {"number": 28733, "title": "tf.maximum/ tf.nn.relu not giving expected result", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab, CPU versio\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.7 (default)\r\n\r\n**Describe the current behavior**\r\nI was testing Relu and tf.maximum for a transformation, but I did not get the results I expect. \r\n\r\nRunning this code for example,\r\n`with tf.Session() as sess:\r\n    a = tf.random.normal([5])\r\n    print(sess.run(a))\r\n    print(sess.run(tf.nn.relu(a)))\r\n    print(sess.run(tf.maximum(a, tf.zeros_like(a))))\r\n`\r\nNeither maximum nor relu gives the right/expected result (i.e. max(0, a)). Running similar code in either pytorch or numpy does give me the correct result. I've read the documentation but I couldn't find anything describing this. Am I missing something or is maximum/relu broken?", "comments": ["False alarm. Turns out that defining a as tf.random.normal instead of using tf.constant produces a different number each time it's called. "]}, {"number": 28732, "title": "Tensorflow Type error when trying to iterate the Tensors in loop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n Yes. It is mentioned as above in the code. I want to go as it is with loop and conditions. I know there are short hand functions but I want to customize it my way later during the go.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10 Desktop\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo it did not as I am using Desktop system for the development\r\n- TensorFlow installed from (source or binary):\r\nUsing Pip on Python 3.5.0 64 bit\r\n- TensorFlow version (use command below):\r\nTensorflow version is 1.12.0\r\n- Python version:\r\nPython 3.5.0\r\n- Bazel version (if compiling from source): ?\r\n- GCC/Compiler version (if compiling from source):?\r\n- CUDA/cuDNN version: No usng CPU version\r\n- GPU model and memory: There isn't any as I faced issue during the go.\r\nI have the following scenario:\r\n\r\n```\r\ny = tf.placeholder(tf.float32, [None, 1],name=\"output\")\r\nlayers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,activation=tf.nn.leaky_relu, name=\"layer\"+str(layer))\r\n         for layer in range(2)]\r\nmulti_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\r\nrnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\r\nstacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, 100]) \r\nstacked_outputs = tf.layers.dense(stacked_rnn_outputs, 1)\r\noutputs = tf.reshape(stacked_outputs, [-1, 2, 1])\r\noutputs = tf.identity(outputs[:,1,:], name=\"prediction\")\r\nloss = Custom_loss(y,outputs)\r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \r\ntraining_op = optimizer.minimize(loss,name=\"training_op\")\r\n```\r\nThe custom loss function I tried is:\r\n\r\n```\r\ndef Custom_loss(y,outputs):\r\n    hold_loss = []\r\n    for exp,pred in zip(y,outputs):\r\n        if exp >= pred:\r\n            result = tf.pow(pred * 0.5,2) - exp\r\n            hold_loss.append(result)\r\n        else:\r\n            hold_loss.append(tf.subtract(pred-exp))\r\n    return tf.reduce_mean(hold_loss)\r\n```\r\nNow when I am trying to implement this I am getting the following error:\r\n\r\n`TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.`\r\nI have tried implementing the tf.map_fn() but there is the same error I encounter. I have used the following question:\r\n[How to explain the result of tf.map_fn?](https://stackoverflow.com/questions/46096767/how-to-explain-the-result-of-tf-map-fn)\r\n\r\nKindly, help me get through this issue? How I can iterate the tensor? What way is best for the custom loss function implementation?\r\nI am willing to have the custom function become a part of the graph so that I can train it effectively.", "comments": ["The code snippet you provided looks incomplete. Can you please edit the issue and update the minimal code snippet to reproduce the issue reported. Thanks!", "@ymodak Ok lets take a lok at the following scenario with the minimal code snippet.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndef Custom_loss(y,outputs):\r\n    hold_loss = []\r\n    for exp,pred in zip(y,outputs):\r\n        if exp >= pred:\r\n            result = tf.pow(pred * 0.5,2) - exp\r\n            hold_loss.append(result)\r\n        else:\r\n            hold_loss.append(tf.subtract(pred-exp))\r\n    return tf.reduce_mean(hold_loss)\r\nnp_x = np.random.randn(100)\r\nnp_y = np.random.randn(100)\r\n\r\nx = tf.constant(np_x)\r\ny = tf.constant(np_y)\r\n\r\nwith tf.Session() as sess:\r\n   assert sess.run(Custom_loss(x, y))\r\n```\r\nI guess that this will help.", "@JafferWilson your question about map_fn seems to have been answered correctly.\r\n\r\nAgain, to iterate over a tensor either use tf.function (available in nightly, tf2 alpha, and soon-to-be-released 1.14), eager execution, or map_fn.", "@alextp I guess no one answered anything here. Where is the answer? I guess you were in a hurry of closing the issue and forgot to answer.\r\nPlease let me know what is the right way. You see in the above conversation I have received message that my answer is not minimal. Please let me know as I have mention the sample code to ya.", "What I meant is that if you just use tf.map_fn like the code below it works:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndef Custom_loss(y,outputs):\r\n    hold_loss = []\r\n    def f(a):\r\n      exp, pred = a\r\n      return tf.cond(exp >= pred, lambda: tf.pow(pred * 0.5, 2) - exp, lambda: (pred-exp))\r\n    hold_loss = tf.map_fn(f, (y, outputs), dtype=tf.float64)\r\n    return tf.reduce_mean(hold_loss)\r\nnp_x = np.random.randn(100)\r\nnp_y = np.random.randn(100)\r\n\r\nx = tf.constant(np_x)\r\ny = tf.constant(np_y)\r\n\r\nwith tf.Session() as sess:\r\n   assert sess.run(Custom_loss(x, y))\r\n```", "@alextp Thank you for the help."]}, {"number": 28731, "title": "Moved Pending Add integer implementation to its own", "body": "", "comments": ["@talumbau , can you please review this PR and provide the feedback.\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "similar changes have merged, so closing this PR.Thank you for your contribution."]}, {"number": 28730, "title": "use GradientTape to compute gradient cost all memory", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip3\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version:3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0.130/ 7.5.0\r\n- GPU model and memory: GeForce GTX 1060 5GB\r\n\r\n**Describe the current behavior**\r\n\r\nWhen i use tf.GradientTape to compute gradients, it will use all gpu memory even though I use GPUoption in session.\r\n\r\n**Describe the expected behavior**\r\n\r\nGPU memory should be controled\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef mul(x):\r\n    y = tf.layers.dense(x, 3)\r\n    return y\r\n\r\nx = tf.placeholder(tf.float32, [None, 2])\r\n\r\nwith tf.GradientTape() as gg:\r\n    gg.watch(x)\r\n    f_eval = mul(x)\r\n    grad = gg.gradient(f_eval, x)\r\ndel gg\r\n\r\n\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\r\nsess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\r\nsess.run(tf.global_variables_initializer())\r\nimport time\r\ntime.sleep(5)\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n> Observing memory usage, it always use more than 4100MB memory because my GPU only has 5G memory.\r\n", "comments": ["@jjjjjie Can you please update to tf-nightly. I ran the code in tf-nightly, did not receive any out of memory issues", "@muddham What is your GPU memory cost in nvidia-smi when code is running?", "@jjjjjie You need to close the session that you opened. Please check the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/b9f3354f00098b813437902a36166d9c/untitled188.ipynb). Thanks!", "@jvishnuvardhan Thanks for your reply, Maybe I didn\u2019t express my issue clearly, I mean this op always cost all GPU memory although I set this option: \r\n`gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)`\r\nI think   tf.GPUOptions should control this session can take up  less than 20% of the memory but it always take up 100%. This is my confusion, not after session is finish. Thanks!", "It seems to me that tf.GradientTape will use the eager context which contains a default ConfigProto different than the one used in the Session API. \r\n\r\n@jaingaurav, can you comment?", "@aaroey Thanks! So it means if I want to control gpu usage of tf.GradientTape I must use eager mode?", "@jjjjjie Is this still an issue for you? I ran it in colab gpu environment with `TF1.15.5` and I didn't notice full gpu memory. [Here](https://colab.research.google.com/gist/jvishnuvardhan/27f403d734c1e084e87c09dcc4e89819/untitled987.ipynb) is a gist for our reference. Thanks!\r\n\r\nPlease note we are no longer supporting `TF1.x` version. Can you please check and open a new issue if the issue persists with `TF2.x` versions.  Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28730\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28730\">No</a>\n"]}, {"number": 28729, "title": "ValueError: Invalid tensors 'outputs' were found when converting from .pb to .tflite", "body": "I successfully retrained mobilenet quantized model (`architecture=\"mobilenet_1.0_128_quantized\"`) with my own image dataset:\r\n\r\n    python3 -m scripts.retrain \\\r\n      --bottleneck_dir=tf_files/bottlenecks_quant \\\r\n      --how_many_training_steps=50000 \\\r\n      --model_dir=tf_files/models/ \\\r\n      --summaries_dir=tf_files/training_summaries/\"mobilenet_1.0_128_quant\" \\\r\n      --output_graph=tf_files/retrained_graph_50000_1.0_128.pb \\\r\n      --output_labels=tf_files/retrained_labels.txt \\\r\n      --architecture=\"mobilenet_1.0_128_quantized\" \\\r\n      --image_dir=images\r\n\r\nWhen I try to convert .pb file to .tflite using\r\n\r\n    toco \\\r\n      --graph_def_file=tf_files/retrained_graph_50000_1.0_128.pb \\\r\n      --output_file=tf_files/retrained_graph_50000_1.0_128.tflite \\\r\n      --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n      --inference_type=QUANTIZED_UINT8 \\\r\n      --input_shape=\"1,128,128,3\" \\\r\n      --input_array=input \\\r\n      --output_array=outputs \\\r\n      --std_dev_values=127.5 --mean_value=127.5\r\n\r\nIt fails with the next error:\r\n\r\n> ValueError: Invalid tensors 'outputs' were found.\r\n\r\nScript uses this link to download mobilenet model: http://download.tensorflow.org/models/mobilenet_v1_1.0_128_frozen.tgz\r\n\r\n```\r\ndata_url = 'http://download.tensorflow.org/models/mobilenet_v1_'\r\ndata_url += version_string + '_' + size_string + '_frozen.tgz'\r\n```\r\n \r\n", "comments": ["What version of TF are you using? Also can you use ```tflite_convert``` to convert your model?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@anonym24  I'm also facing same issue.\r\n\r\nDid you resolve this issue ?"]}, {"number": 28728, "title": "Error while convert the Tensorflow to tensorflowlite model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\n\r\n```\r\nGraph_def_file = ssdlite_mobilenet_v2_coco_2018_05_09/tflite_graph.pb\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please have a look on #27241, [FAQ](https://www.tensorflow.org/lite/guide/faq#why_are_some_operations_not_implemented_in_tensorflow_lite), [Select operators from TensorFlow](https://www.tensorflow.org/lite/guide/ops_select). Let us know if that resolves the issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28727, "title": "Converting error for quantize aware trained tf.keras.applications.MobileNetV2", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): r1.13\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 2080Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nDuring the converting the quantize aware trained mobilentnetv2 model from tf.keras, it will raise the follow error massage\r\n```\r\nF tensorflow/lite/toco/tooling_util.cc:1702] Array expanded_conv_project_BN/FusedBatchNorm, which is an input to the Conv operator producing the output array block_1_expand_relu/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nAborted (core dumped)\r\n```\r\n**Describe the expected behavior**\r\nIt works for model built with tf.contrib.slim. I feels like the tf.contrib.quantize.create_eval_graph() doesn't support BN layer from tf.keras. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\nworking_path='/tmp/tflite'\r\ntf.keras.backend.set_learning_phase(1)\r\ninputs = tf.keras.Input(shape=(224, 224, 3), name='input')\r\ny_true = tf.keras.Input(shape=[1000], name='label')\r\ny_pred = keras_model.output\r\n\r\nloss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\r\n\r\n# quant aware training\r\ngraph = tf.get_default_graph()\r\ntf.contrib.quantize.create_training_graph(input_graph=graph, quant_delay=0)\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(learning_rate=0.00625).minimize(loss)\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    _input = np.random.rand(10, 224, 224, 3)\r\n    _label = np.zeros([10, 1000])\r\n    _label[:, 2] = np.ones(10)\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(3):\r\n        _, _loss = sess.run([train_step, loss], feed_dict={inputs: _input,\r\n                                                           y_true: _label})\r\n        print(_loss)\r\n    # save\r\n    saver.save(sess, os.path.join(working_path, 'checkpoints/model.ckpt'))\r\n\r\n'''load and convert to TFLite'''\r\ntf.reset_default_graph()\r\ntf.keras.backend.set_learning_phase(0)\r\ninputs = tf.keras.Input(shape=(224, 224, 3), name='input')\r\nkeras_model = tf.keras.applications.MobileNetV2(input_tensor=inputs, alpha=1.0, weights=None, include_top=True)\r\noutput = keras_model.output\r\n\r\n# insert fake quant nodes\r\ngraph = tf.get_default_graph()\r\ntf.contrib.quantize.create_eval_graph(graph)\r\nsaver = tf.train.Saver()\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    saver.restore(sess, tf.train.latest_checkpoint(os.path.join(working_path, 'checkpoints/')))\r\n\r\n    # freeze graph\r\n    graph_def = graph.as_graph_def()\r\n    froze_graph = tf.graph_util.convert_variables_to_constants(sess, graph_def, [output.op.name])\r\n    tf.io.write_graph(froze_graph, working_path, 'freeze_graph.pb')\r\n\r\n# convert to TFLite\r\ngraph_def_file = os.path.join(working_path, 'freeze_graph.pb')\r\ninput_array = [\"input\"]\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_array, [output.op.name],\r\n                                                      input_shapes={\"input\": [1, 224, 224, 3]})\r\n\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\nconverter.inference_input_type = tf.lite.constants.QUANTIZED_UINT8\r\nconverter.quantized_input_stats = {\"input\": (0., 255.)}\r\ntfmodel = converter.convert()\r\nopen(os.path.join(working_path, \"converted_model.tflite\"), \"wb\").write(tfmodel)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mgou/projects/tf-lightweight-yolov3/backbone_pretrain/train_recog_quant.py\", line 91, in <module>\r\n    tfmodel = converter.convert()\r\n  File \"/home/mgou/virtualenv/tf113-py36/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 455, in convert\r\n    **converter_kwargs)\r\n  File \"/home/mgou/virtualenv/tf113-py36/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/mgou/virtualenv/tf113-py36/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-05-15 02:27:33.413703: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1161 operators, 1728 arrays (0 quantized)\r\n2019-05-15 02:27:33.440573: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1161 operators, 1728 arrays (0 quantized)\r\n2019-05-15 02:27:33.608574: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 137 operators, 260 arrays (1 quantized)\r\n2019-05-15 02:27:33.610239: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 137 operators, 260 arrays (1 quantized)\r\n2019-05-15 02:27:33.610999: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 76 operators, 199 arrays (1 quantized)\r\n2019-05-15 02:27:33.611690: F tensorflow/lite/toco/tooling_util.cc:1702] Array expanded_conv_project_BN/FusedBatchNorm, which is an input to the Conv operator producing the output array block_1_expand_relu/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nAborted (core dumped)\r\n```", "comments": ["Hi Mengran,\r\n\r\nYes the contrib.quantize tool may not be able to handle models built by Keras. In the meantime, we are building a replacement that is tightly integrated with tf.keras, as shown on our roadmap (https://www.tensorflow.org/model_optimization/guide/roadmap). This is targeting Q3 and the tf.keras.applications.MobilenetV2 is one of the models we'll ensure support for", "Thank you for your answer! Looking forward for the new quantization toolbox. It's will be very helpful! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28727\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28727\">No</a>\n"]}, {"number": 28726, "title": "Lite: Div Op Int8 support added", "body": "Ref: #21526", "comments": ["Can one of the admins verify this patch?", "Please merge this PR.", "@ANSHUMAN87 Can you please resolve conflicts? Thanks!", "I am working on it!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I will update on this next week. Thanks", "Closing to reopen with latest changes."]}, {"number": 28725, "title": "Autograph fails for keyword-only arguments", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): `pip install tf-nightly-gpu-2.0-preview`\r\n- TensorFlow version (use command below): `v1.12.1-1847-gc095504 2.0.0-dev20190514`\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\nAutograph complains when compiling functions with keyword-only arguments.\r\n\r\nExample output:\r\n`W0515 01:46:22.158518 139635868194560 ag_logging.py:145] Entity <function f at 0x7eff8120c1e0> could not be transformed and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <function f at 0x7eff8120c1e0>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output when filing the bug report. Caused by: inconsistent nodes: None (NoneType) and None (NoneType)`\r\n\r\n**Describe the expected behavior**\r\nAutograph works for keyword-only arguments\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n@tf.function\r\ndef f(*, a):\r\n     return a*2\r\n\r\nf(a=0)\r\n```\r\n", "comments": ["I have reproduce the mentioned output with TensorFlow version 2.0.0-dev20190515 on Colab.", "Thanks for filing the bug! At a glance, it looks related to the lone * arg without a name; probably its name field is None in the AST. We should have fix soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28725\">No</a>\n"]}, {"number": 28724, "title": "BeagleBone Black Tensorflow Lite build scripts", "body": "Added build_bbb_lib.sh and targets/bbb_makefile.inc to tensorflow/tensorflow/lite/tools/make/\r\nBuild tested on Raspberry Pi 3 B+.\r\nStatic Library tested on BeagleBone Black Industrial Edition.\r\n\r\nOnce the library has been generated, rename gen/bbb_armv7l/lib/libtensorflow-lite.a to libtflite.a\r\nand link to your gcc projects with \"-ltflite -lpthread -ldl -lrt -march=armv7-a -mfpu=neon -I . -L .\"\r\ne.g.:\r\n        g++ -c main.cpp -o main.o -I .\r\n\tg++ -o test.run main.o -march=armv7-a -mfpu=neon -L . -l tflite -l pthread -l dl -l rt\r\n\r\nPlease see the Tensorflow Lite documentation for more information:\r\nhttps://www.tensorflow.org/lite/guide/inference#running_a_model\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28724) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28724) for more info**.\n\n<!-- ok -->", "Are there any more reviewers available for this pull request?", "I'll leave it to you as to how you want to add this documentation into the project:\r\n\r\n**Prerequisites**\r\n* These instructions target a BeagleBone Black running its latest Debian image\r\n* You may need to start some commands with `sudo` for them to work properly\r\n* You may need dependency packages to be installed: `sudo apt-get install build-essential make`\r\n\r\n**Instructions**\r\n1. Download the Tensorflow source code: `git clone https://github.com/tensorflow/tensorflow`\r\n2. Change Directory into Tensorflow Lite: `cd tensorflow/tensorflow/lite/tools/make/`\r\n3. Download the dependencies for TF Lite: `sudo ./download_dependencies.sh`\r\n4. Build the TF Lite library for BeagleBone Black: `sudo ./build_bbb_lib.sh`\r\n5. Copy the new static library file: `sudo mv gen/bbb_armv7l/lib/libtensorflow-lite.a ./libtflite.a`\r\n\r\nTo link the library with your gcc projects, copy the libtflite.a file into your project's 'lib' directory.\r\nThe following compiler/linker options may be useful to know for successful compilation:\r\n======{`-ltflite -lpthread -ldl -lrt -march=armv7-a -mfpu=neon -I . -L .`}======\r\n\r\n**Example Makefile**\r\n```\r\nall:\r\n\tg++ -c main.cpp -o main.o -I .\r\n\tg++ -o test.run main.o -march=armv7-a -mfpu=neon -L . -l tflite -l pthread -l dl -l rt\r\n```\r\nWritten by Alastair Cota @TheMindVirus"]}, {"number": 28723, "title": "[INTEL MKL] Remove requantize op fusion from graph_transform.", "body": "The file `tensorflow/tools/graph_transforms/fuse_quantized_convolution.cc` has been outdated, and Intel quantization tool hosts necessary graph transformation for model quantization. Therefore, this PR removes it.", "comments": []}, {"number": 28722, "title": "[INTEL MKL] Enable concat for 2D tensors with MKLDNN.", "body": "This PR is to re-enable ConcatOp for 2D tensors using MKLDNN. Previously, it was reverted (PR: #27359) because of some failure in SSD-Mobile Net. The issue was mishandling of a corner case, when all input tensors were empty. It has been resolved in this PR.", "comments": []}, {"number": 28721, "title": "Failed to load the native TensorFlow runtime", "body": "I'm trying to run tensorflow in a venv managed on PyCharm, but is giving some trouble (I cant even check the version). I have another env, this one on conda, which works fine.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): import tensorflow (for example)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce 1080 Ti - 11Gb. Driver 430.64\r\n\r\n**Traceback**\r\n\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<string>\", line 1, in <module>\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Python\\Python-3.6.8\\lib\\imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n\r\nI have already tried to include `bin`, `include` and `lib` (as in https://github.com/tensorflow/docs/pull/419)", "comments": ["Dumb of me, forgot to install cuDNN. Feel free to delete the post.\r\n\r\nSorry for the inconvenience, and thanks for your time."]}]