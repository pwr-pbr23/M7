[{"number": 41331, "title": "faster rcnn stopped training", "body": "\r\nEverything works fine, just at the end step, training stops.\r\nI user faster rcnn. As soon as I run train.py with all the parameters provided, it stopped training", "comments": ["@Rahul-kateith,\r\nIn order to reproduce the issue reported here, could you please provide the complete code, the dataset and the TensorFlow version you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41330, "title": "AttributeError: module 'keras.backend' has no attribute 'slice'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Windows 10 64 bits\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pypi.douban.com/simple\r\n- TensorFlow version (use command below):2.2\r\n- Python version:3.7.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nAttributeError: module 'keras.backend' has no attribute 'slice'\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://github.com/cyandn/DS/tree/master/NER_Keras\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.", "comments": ["@asd8095075 \r\nPlease refer to [this comment](https://github.com/keras-team/keras/issues/12599#issuecomment-484959748) and let us know if it helps..\r\nLinks for reference:\r\n[Link](https://github.com/keras-team/keras/issues/13352#issuecomment-533915681) [link1](https://www.gitmemory.com/issue/keras-team/keras-contrib/488/503056034) ", "Also please share simple stand lone code for us to replicate the issue faced.", "tensorflow1.13.0+keras2.2.4\uff0cThe problem has been solved\uff0cthanks\u3002", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41330\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41330\">No</a>\n"]}, {"number": 41329, "title": " when using keras, report error: Use Eager execution or decorate this function with @tf.function ", "body": "I do not think currently tensorflow supports building a tf.keras.Model which contains tf.cond or tf.while_loop. \r\n\r\nI have built a model based on tf2.0 which contains while loop, but I want to save it as a pb file, so I need to use tf.keras.Model to rebuild my model.\r\n\r\nI have tried the code below and it reported the error:\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\r\n# the code\r\nimport tensorflow as tf\r\nlayers = tf.keras.layers\r\na = layers.Input(shape=tf.TensorShape([]), dtype=tf.bool)\r\nb = a[0]\r\nc = tf.cond(b, lambda: 1, lambda: 0)\r\nprint (c)\r\n\r\n\r\nthe tensorflow version I used: \r\nv2.0.0-69-g765ac8d 2.0.1", "comments": ["I have tried in colab with TF version 2.2, 2.3-rc1,nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/eb1e01c6e24448e366bb0ad00aa0b885/untitled111.ipynb).Thanks!", "@cookingbear As the error message suggests, you cant use a boolean tensor in tf.cond()\r\n", "> @cookingbear As the error message suggests, you cant use a boolean tensor in tf.cond()\r\n\r\nIf the stop condition of the while loop depends on the input data length, is there any way to use keras.model to build this kind of model?", "The reason I have this concern is that in eager mode I have built the model successfully which contains while loops in which tensors control the flow. However, for some reason, I have to freeze the model into pb format. To accomplish this work, I thought maybe firstly I should use keras.model to rebuild the model. So the problem arised. ", "Hi @cookingbear, taking a look at issue [#31848](https://github.com/tensorflow/tensorflow/issues/31848) might be helpful, as it seems similar to the problem you're facing. The suggested workaround there is to use a keras [lambda layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda). Let me know if this helps.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41329\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41329\">No</a>\n"]}, {"number": 41328, "title": "Error: \"Data adapters should be mutually exclusive for handling inputs. Found multiple adapters to handle\" error when calling `model.fit` with imagenet2012 TFDS.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0 (also tried 2.3 and tf-nightly)\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: RTX 2080 Super, 8 GB\r\n\r\n**Describe the current behavior**\r\nWhen trying to fit a VGG19 model architecture with ImageNet2012 data from TFDS (previously manually downloaded and but with ```download_and_prepare()```), it give the following error:\r\n\r\n> RuntimeError: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [<class 'tensorflow.python.keras.engine.data_adapter.GeneratorDataAdapter'>, <class 'tensorflow.python.keras.engine.data_adapter.CompositeTensorDataAdapter'>] to handle input: <class 'tensorflow.python.data.ops.iterator_ops.OwnedIterator'>, <class 'NoneType'>\r\n\r\n\r\nThis is very similar to issue #33811. However, the error persists even with newer or nightly versions of TF.\r\n\r\n**Describe the expected behavior**\r\nSuccessfully start training.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nbatch_size = 12\r\nn_train = 1281167\r\nds_train,  ds_train_info = tfds.load(name='imagenet2012', download=True, with_info=True,\r\n                                     data_dir='/Data/tfds/', split='train', as_supervised=True,\r\n                                     download_and_prepare_kwargs={'download_dir':'/hdd/Data/tfds/imagenet2012/',})\r\nds_train = ds_train.map(lambda x, y: (tf.image.resize(x, [224, 224], method='bilinear'), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\nds_train = ds_train.batch(batch_size)\r\nds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\nds_train = iter(ds_train)\r\n\r\nmodel = tf.keras.applications.VGG19(include_top=True, weights=None, input_tensor=None, input_shape=None,\r\n                                    pooling=None, classes=1000, classifier_activation='softmax')\r\noptimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\r\nloss_instance = SparseCategoricalCrossentropy(from_logits=False)\r\nmodel.compile(optimizer=optimizer, loss=loss_instance)\r\n\r\nepochs = 500\r\nsteps_per_epoch = n_train//batch_size\r\nhistory = model.fit(x=ds_train, epochs=epochs,\r\n                    steps_per_epoch=steps_per_epoch)\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n~/scratch/imagenet/train_vgg19.py in <module>\r\n    147 # history = model.fit_generator(data_train, epochs=epochs,\r\n    148 history = model.fit(x=ds_train, epochs=epochs,\r\n--> 149                     steps_per_epoch=steps_per_epoch)\r\n    150                     # callbacks=callbacks,\r\n    151                     # validation_data=ds_val,\r\n\r\n~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    813           workers=workers,\r\n    814           use_multiprocessing=use_multiprocessing,\r\n--> 815           model=self)\r\n    816 \r\n    817       # Container that configures and calls `tf.keras.Callback`s.\r\n\r\n~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\r\n   1097     self._insufficient_data = False\r\n   1098 \r\n-> 1099     adapter_cls = select_data_adapter(x, y)\r\n   1100     self._adapter = adapter_cls(\r\n   1101         x,\r\n\r\n~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py in select_data_adapter(x, y)\r\n    967         \"handling inputs. Found multiple adapters {} to handle \"\r\n    968         \"input: {}, {}\".format(\r\n--> 969             adapter_cls, _type_name(x), _type_name(y)))\r\n    970   return adapter_cls[0]\r\n    971 \r\n\r\nRuntimeError: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [<class 'tensorflow.python.keras.engine.data_adapter.GeneratorDataAdapter'>, <class 'tensorflow.python.keras.engine.data_adapter.CompositeTensorDataAdapter'>] to handle input: <class 'tensorflow.python.data.ops.iterator_ops.OwnedIterator'>, <class 'NoneType'>\r\n```", "comments": ["Using `imagenet2012` dataset required registration, so used `imagenet_resized` instead.\r\n\r\nWas able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/d094f9a24f13b64e438e8f5b0d4e9874/41328.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/6341e73feef64c7a4df624fa19b16c53/41328-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@mattroos I changed one line of your code and after that code works without any issue.\r\nThe following line was commented.\r\n#ds_train = iter(ds_train)\r\n\r\nPlease take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/bf5d39f8da3277601c5e77027720efe2/41328-tf-nightly.ipynb).\r\n\r\nPlease note that I have changed `epochs`, `batch_size` etc, to run the code faster. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Thanks, @jvishnuvardhan and @amahendrakar! Yes, that resolved the issue.\r\n\r\nAt some point during my coding I came to think that the tfds.load() was returning a generator, which then had to be converted to an iterator. I can't seem to recreate that condition so perhaps I mixed this up with something else I was coding.\r\n\r\nMany Thanks,\r\nMatt", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41328\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41328\">No</a>\n", "@mattroos, @jvishnuvardhan, @amahendrakar, please, re-open the issue. I suggest it to be handled differently:\r\n ```\r\nimport tensorflow as tf\r\nfeatures =  tf.random.normal(shape=(100, 1, 10))\r\nlabels = tf.random.normal((100,1,1))\r\ndataset = tf.data.Dataset.from_tensor_slices((features,labels))\r\nds_iter = iter(dataset)\r\nx = tf.keras.layers.Input(shape=[10])\r\ny_pred = tf.keras.layers.Dense(1, activation='sigmoid', name=\"L0\")(x)\r\nmodel = tf.keras.Model(x, y_pred)\r\nmodel.compile(optimizer='sgd', loss='mse',)\r\nmodel.fit(ds_iter, epochs=1)\r\n```\r\nThe code above (like the TS code) triggers error `RuntimeError: Data adapters should be mutually exclusive for handling inputs`, however this is a good kind of problem to have. There are 2 adapters that claim ability to  handle such iterator: `CompositeTensorDataAdapter` and `GeneratorDataAdapter`.\r\nIf there is more than single adaptor which can handle certain data - great, just pick the best of them and return it from the function. The adators can be prioritized by changing order in `ALL_ADAPTER_CLS`. \r\n\r\nI suggest not to throw `RuntimeError: Data adapters should be mutually exclusive for handling inputs ` but instead just return `adapter_cls[0]`. The change requires deleting [5 lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/data_adapter.py#L958-L96) in `tf.python.keras.engine.data_adapter.select_data_adapter`\r\n\r\nLater I ran tests with `tf.python.keras.engine.data_adapter.select_data_adapter` forcing it to choose one of the adapters for `tensorflow.python.data.ops.iterator_ops.OwnedIterator`:\r\n1) `GeneratorDataAdapter` works fine with `tensorflow.python.data.ops.iterator_ops.OwnedIterator`,\r\n\r\n2) `CompositeTensorDataAdapter` fails with error `AttributeError: 'IteratorSpec' object has no attribute '_to_batched_tensor_list'` [error stack here](https://pastebin.com/g1RxEhdc) Apparently some change is needed either in `CompositeTensorDataAdapter.can_handle()` or in the handling process itself."]}, {"number": 41327, "title": "MethodChannel#tflite(23624): Node number 8 (FlexConv2D) failed to prepare. ", "body": "Hey I've trained a custom object detector model using pytorch and convert it into ONNX format then into tflite format.\r\nWhen I deploy my model into app using the flutter tflite it shows me this error: \r\n\r\n> MethodChannel#tflite(23624): Node number 8 (FlexConv2D) failed to prepare.\r\n\r\nHow to handle such error?\r\n\r\nPlease any ML and Flutter expert could help me to solve it?\r\n\r\nThis is the code that I have used to done the pipeline conversion:\r\n\r\n```\r\nfrom onnx_tf.backend import prepare\r\nimport onnx\r\n\r\nmodel_onnx = onnx.load('/mydrive/YOLOv5_Model/app/weights/best_fit.onnx')\r\ntf_rep = prepare(model_onnx, device='cpu')\r\ntf_rep.export_graph('/content/model_tf.pb')\r\n\r\ndef wrap_frozen_graph(graph_def, inputs, outputs):\r\n    def _imports_graph_def():\r\n        tf.compat.v1.import_graph_def(graph_def, name=\"\")\r\n\r\n    wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])\r\n    import_graph = wrapped_import.graph\r\n\r\n    return wrapped_import.prune(\r\n        tf.nest.map_structure(import_graph.as_graph_element, inputs),\r\n        tf.nest.map_structure(import_graph.as_graph_element, outputs))\r\n\r\nwith tf.io.gfile.GFile(\"/content/model_tf.pb\", \"rb\") as f:\r\n    graph_def = tf.compat.v1.GraphDef()\r\n    loaded = graph_def.ParseFromString(f.read())\r\n\r\nfrozen_func = wrap_frozen_graph(graph_def=graph_def,\r\n                                inputs=[\"images:0\"],\r\n                                outputs=['output_0:0','output_1:0', 'output_2:0'])\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([frozen_func])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntf_lite_model = converter.convert()\r\nopen('/content/best_fit.tflite', 'wb').write(tf_lite_model)\r\n```", "comments": ["@e-shawakri did you include the aar that enables tf ops?\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_select#android_aar", "> deploy my model into app using the flutter tflite\r\n\r\nCan you elaborate this part? What exact plugin were you using?\r\n\r\nAs @abattery alluded to, since you provided the `tf.lite.OpsSet.SELECT_TF_OPS` to the converter, the runtime on which the model runs should support the select TF ops.", "@abattery\r\nI've added the following into my project \r\n\r\n`dependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    // This dependency adds the necessary TF op support.\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n}`\r\n\r\n\r\n`android {\r\n    defaultConfig {\r\n        ndk {\r\n            abiFilters 'armeabi-v7a', 'arm64-v8a'\r\n        }\r\n    }\r\n}\r\n`\r\n\r\nThese solved the problem but i came up with another problem, the problem is im using Flutter and cant build the Bazel and the MVN into my project. Any recommendation please!", "We can't help you further without knowing what exact method you used for integrating TFLite with Flutter.\r\nAre you using this package? https://pub.dev/packages/tflite_flutter\r\n\r\nAlso, could you share your project, if possible? Is it available on GitHub? Seeing the actual project configuration would be helpful.", "@e-shawakri \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41326, "title": "Differencing Time Series Data - How to Invert Differencing ", "body": "I am working with time series data (non-stationary), I have applied .diff(periods=n) for differencing the data to eliminate trends and seasonality factors from data. \r\n\r\nBy using .diff(periods=n), the observation from the previous time step (t-1) is subtracted from the current observation (t).\r\n\r\nNow I want to invert back the differenced data to its original scale, but I am having issues with that.\r\n\r\nOriginal DataSet before differencing:\r\n\r\n[![original dataset][1]][1]\r\n\r\n\r\nMy code for differencing and output:\r\n\r\n    data_diff = df.diff(periods=1)     \r\n    \r\n    data_diff.head(5) \r\n\r\n[![output differenced data][2]][2]\r\n\r\n\r\nCode for inverting data to original scale and output:\r\n\r\n    cols = df.columns\r\n    x = []\r\n    for col in cols:\r\n        diff_results = df[col] + data_diff[col].shift(-1)\r\n        x.append(diff_results)\r\n    diff_df_inverted = pd.concat(x, axis=1)\r\n    \r\n    diff_df_inverted\r\n\r\n[![data inverted][3]][3]\r\n\r\nAs you can see from last output, I have successfully inverted my data back to its original scale. However, I do not get the inverted data for row 1. It inverts and shifts the values up a row. My question is, why? What am I missing?\r\n\r\nAppreciate your help, thank you!\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/qMwCZ.png\r\n  [2]: https://i.stack.imgur.com/Z5T6B.png\r\n  [3]: https://i.stack.imgur.com/oCXj2.png", "comments": ["@Tyson-Stur \r\nThis question is better asked on StackOverflow since it is not a bug or feature request related to Tensorflow. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n", "Here's a quick function that I defined to take in the differenced series and the first value of the original series that will return the original series. Copy and run the function below and run it. To have the function defined, then copy the codes near bottom to test and see.\r\n\r\ndef diff_inv(series_diff, first_value):\r\n    series_inverted = np.r_[first_value, df_diff].cumsum().astype('float64)\r\n    return series_inverted \r\n\r\nFor a quick example to see how this works, let's make some a dataframe with random numbers and column A\r\nGet its diff() as another dataframe\r\nThen we can use the function to take the diferenced series, which is why it's df_diff.A, we're selecting that column, then input the first value of the original dataframe of that column. The function then returns the restored values in an array format.\r\n\r\ndf = pd.DataFrame({'A': np.random.randint(0, 10, 10)})\r\ndf_diff = df.diff().dropna()\r\ninversed_series = diff_inv(df_diff.A , df.A[0])\r\ndisplay(df)\r\ninversed_series "]}, {"number": 41324, "title": "ValueError: quantized_dimension must be in range [0, 1). Was 3.", "body": "I am trying to load a tflite model using the following python code:\r\n````\r\nimport tensorflow as tf #version=2.0.0\r\ntf.lite.Interpreter(model_path=\"model.tflite\")\r\n````\r\nwith the following error message:\r\n````\r\nValueError: quantized_dimension must be in range [0, 1). Was 3.\r\n````\r\nCan anyone help me to solve this problem?", "comments": ["@IlkayW \r\nPlease refer to below links and let us know if it helps:\r\n[link](https://stackoverflow.com/questions/57234308/edge-tpu-compiler-error-quantized-dimension-must-be-in-range-0-1-was-3) [link1](https://github.com/tensorflow/tensorflow/issues/27880) [link2](https://www.thetopsites.net/article/58082806.shtml) [link3](https://stackoom.com/question/3s9G0/%E8%BE%B9%E7%BC%98TPU%E7%BC%96%E8%AF%91%E5%99%A8-%E9%94%99%E8%AF%AF-quantized-dimension%E5%BF%85%E9%A1%BB%E5%9C%A8-%E8%8C%83%E5%9B%B4%E5%86%85-%E6%98%AF)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41323, "title": "Doc error for `tf.linalg.trace`", "body": "I think the doc-string [here](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/math_ops.py#L2758):\r\n```python\r\n  `output[i, j, k, ..., l] = trace(x[i, j, i, ..., l, :, :])`\r\n```\r\n\r\nshould be:\r\n\r\n```python\r\n  `output[i, j, k, ..., l] = trace(x[i, j, k, ..., l, :, :])`\r\n```\r\n\r\n\r\n", "comments": ["Hi, Can I create a PR to fix this?", "PR is here https://github.com/tensorflow/tensorflow/pull/41348\r\n@jvishnuvardhan ", "Closing this issue as this was resolved. The corresponding PR was merged already. Thanks!"]}, {"number": 41322, "title": "build problems found at runtime for CUDA related issue on laptop with no GPU", "body": "**System information**\r\n- Linux Ubuntu 20.04LTS\r\n- HP Elitebook 820 G3, Intel\u00ae Core\u2122 i7-6600U CPU @ 2.60GHz \u00d7 4.  8GB RAM installed\r\n- TF installed in virtualenv with command \"pip3 install tensorflow\" and keras with \"pip3 install keras\"\r\n- TensorFlow version: 2.2.0.  Keras version 2.4.2\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?:  installed in virtualenv and used pip\r\n- GPU model and memory:  No GPU, installed on laptop.  using Mesa Intel\u00ae HD Graphics 520 (SKL GT2)\r\n\r\n**Describe the problem**\r\nBuild does not produce an error but fails at runtime, see attached log.  Suspect it is related to the building of tensorflow for this laptop without GPU card.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ntrying to train a CNN causes this error\r\n\r\n**Any other info / logs**\r\nsee attached log file\r\n[error_log.txt](https://github.com/tensorflow/tensorflow/files/4908576/error_log.txt)\r\n\r\n", "comments": ["@greencane,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using.\r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/32563#issuecomment-531977227) from a similar issue and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41321, "title": "using 3.2GB ram for the simplest mnist model", "body": "I create the simplest model, but it uses so much ram. Can you help this issue?\r\n\r\n**System information**\r\n- Cuda 10.1\r\n- Gtx 1050 ti\r\n- Windows 10 platform\r\n- Tensorflow-gpu 2.2.0\r\n- keras 2.4.3\r\n\r\n\r\n**Describe the current behavior**\r\nUsing ~3.2GB Video Ram\r\n\r\n**Describe the expected behavior**\r\nUsing ~300MB Video Ram\r\n\r\n**Code**\r\n```\r\nimport numpy as np\r\nimport matplotlib\r\nimport matplotlib.pyplot as plt\r\n\r\nimport keras\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Sequential, load_model\r\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten \r\nfrom keras.utils import np_utils\r\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\r\n\r\n# The first time you run this might be a bit slow, since the\r\n# mnist package has to download and cache the data.\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train = np.array(x_train, dtype=np.float32)\r\nx_test = np.array(x_test, dtype=np.float32)\r\n\r\nx_train /= 255\r\nx_test /= 255\r\ny_train = np_utils.to_categorical(y_train, 10)\r\ny_test = np_utils.to_categorical(y_test, 10)\r\n\r\nx_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\r\nx_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\r\n\r\nprint(x_train.shape) # (60000, 28, 28)\r\nprint(y_train.shape) # (60000,)\r\n\r\nmodel = Sequential()\r\n#model.add(Conv2D(32, kernel_size=(3, 3), padding='valid', activation='relu', kernel_initializer='he_normal', input_shape=(28,28, 1)))\r\n#model.add(MaxPool2D((2, 2)))\r\n#model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))\r\n#model.add(MaxPool2D((2, 2)))\r\n#model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))\r\n#model.add(MaxPool2D((2, 2)))\r\n#model.add(Conv2D(256, kernel_size=(1, 1), activation='relu', kernel_initializer='he_normal'))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(30, activation='relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Dense(10, activation='softmax'))\r\n#model.summary()\r\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/37745467/87245731-7229ef00-c450-11ea-9bcf-cd5703acfbb3.png)\r\n\r\n//EDIT: More info\r\nI had cuda 10.2 installed in my computer. I installed keras and tensorflow. It didn't work at the beginning because tf needs CUDA 10.1. Then I tried to uninstall cuda, reinstalling cuda 10.1. It failed couple times etc. Finally I was able to install cuda 10.1 and tensorflow. I don't know if this is related with the current memory issue. ", "comments": ["@faruknane \r\nPlease confirm if [this gist](https://colab.research.google.com/gist/Saduf2019/8546222f759b6adbb288517166584d17/untitled279.ipynb) confirms your issue.", "Yes, exactly @Saduf2019 \r\n![image](https://user-images.githubusercontent.com/37745467/87550154-6b50e580-c6b7-11ea-917a-827c4c00bf67.png)\r\n", "I tried your example in google colab and I see 1.27 gb ram usage.\r\n<img width=\"212\" alt=\"Screen Shot 2020-07-15 at 12 46 06 PM\" src=\"https://user-images.githubusercontent.com/42785357/87588831-2a49d880-c699-11ea-9df9-25a650446240.png\">\r\n\r\nYou may try again by closing all the applications and rebooting your system.\r\n \r\n", "@ymodak I have cudnn64_7.dll and Cuda 10.1 installed. I don't know what's wrong. But I think it's decreasing the performance. On my PC, it uses 3.2 gib GPU ram. It's not about the apps that are running. When I restart the kernel of the notebook, GPU memory is 77mb. When I run the notebook, the memory usage is 3.2gib. \r\n\r\nI only did install TensorFlow-GPU and Keras packages. Do you have any idea what the issue is about?\r\n\r\n![image](https://user-images.githubusercontent.com/37745467/87661730-c8a56f00-c769-11ea-9e8a-df502ce6ca1f.png)\r\n\r\n![image](https://user-images.githubusercontent.com/37745467/87661794-ea9ef180-c769-11ea-96e1-52608589d59f.png)\r\n\r\nDo I have to install tensorflow package?", "At least can you guide me on how to remove all and reinstall them proper and successfully? ", "I see you are using anaconda to install TF.\r\nFor installing TF with conda you may raise a request on https://github.com/ContinuumIO/anaconda-issues/issues", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41321\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41321\">No</a>\n"]}, {"number": 41320, "title": " Quantization Aware Training with tf.GradientTape gives Error in TensorFlow2.x", "body": "Hey Guys, I am using TensorFlow-2.2, tensorflow_model_optimization and Python 3.8. I am trying to quantize and train a LeNet-300-100 Dense neural network which contains sparsity of 91.3375%. This means that 91.3375% of the weights are zero. I was following the [Quantization TF tutorial][1] and I wanted to train such a sparse network which has been quantized using *tf.GradientTape* rather than *q_aware_model.fit()*.\r\n\r\nIf you look into the [example code][2], the relevant code snippets are:\r\n\r\n\r\n    quantize_model = tfmot.quantization.keras.quantize_model\r\n    \r\n    # q_aware stands for for quantization aware.\r\n    q_aware_model = quantize_model(model)\r\n    \r\n    \r\n    # 'quantize_model' requires recompilation-\r\n    q_aware_model.compile(\r\n        optimizer = tf.keras.optimizers.Adam(lr = 0.0012),\r\n        loss=tf.keras.losses.categorical_crossentropy,\r\n        metrics=['accuracy']\r\n    )\r\n    \r\n    \r\n    # Define 'train_one_step()' and 'test_step()' functions here-\r\n    @tf.function\r\n    def train_one_step(model, mask_model, optimizer, x, y):\r\n        '''\r\n        Function to compute one step of gradient descent optimization\r\n        '''\r\n        with tf.GradientTape() as tape:\r\n            # Make predictions using defined model-\r\n            y_pred = model(x)\r\n    \r\n            # Compute loss-\r\n            loss = loss_fn(y, y_pred)\r\n            \r\n        # Compute gradients wrt defined loss and weights and biases-\r\n        grads = tape.gradient(loss, model.trainable_variables)\r\n        \r\n        # type(grads)\r\n        # list\r\n        \r\n        # List to hold element-wise multiplication between-\r\n        # computed gradient and masks-\r\n        grad_mask_mul = []\r\n        \r\n        # Perform element-wise multiplication between computed gradients and masks-\r\n        for grad_layer, mask in zip(grads, mask_model.trainable_weights):\r\n            grad_mask_mul.append(tf.math.multiply(grad_layer, mask))\r\n        \r\n        # Apply computed gradients to model's weights and biases-\r\n        optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))\r\n    \r\n        # Compute accuracy-\r\n        train_loss(loss)\r\n        train_accuracy(y, y_pred)\r\n    \r\n        return None\r\n        \r\n        \r\n    @tf.function\r\n    def test_step(model, optimizer, data, labels):\r\n        \"\"\"\r\n        Function to test model performance\r\n        on testing dataset\r\n        \"\"\"\r\n        \r\n        predictions = model(data)\r\n        t_loss = loss_fn(labels, predictions)\r\n    \r\n        test_loss(t_loss)\r\n        test_accuracy(labels, predictions)\r\n    \r\n        return None\r\n    \r\n    \r\n    \r\n    # Train model using 'GradientTape'-\r\n        \r\n    # Initialize parameters for Early Stopping manual implementation-\r\n    # best_val_loss = 100\r\n    # loc_patience = 0\r\n        \r\n    for epoch in range(num_epochs):\r\n        \r\n        if loc_patience >= patience:\r\n            print(\"\\n'EarlyStopping' called!\\n\")\r\n            break\r\n            \r\n        # Reset the metrics at the start of the next epoch\r\n        train_loss.reset_states()\r\n        train_accuracy.reset_states()\r\n        test_loss.reset_states()\r\n        test_accuracy.reset_states()\r\n                \r\n        \r\n        for x, y in train_dataset:\r\n            train_one_step(q_aware_model, mask_model, optimizer, x, y)\r\n    \r\n    \r\n        for x_t, y_t in test_dataset:\r\n            test_step(q_aware_model, optimizer, x_t, y_t)\r\n    \r\n        template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f}, Test Accuracy: {4:4f}'\r\n        \r\n        '''\r\n        # 'i' is the index for number of pruning rounds-\r\n        history_main[i]['accuracy'][epoch] = train_accuracy.result() * 100\r\n        history_main[i]['loss'][epoch] = train_loss.result()\r\n        history_main[i]['val_loss'][epoch] = test_loss.result()\r\n        history_main[i]['val_accuracy'][epoch] = test_accuracy.result() * 100\r\n        ''' \r\n    \r\n        print(template.format(\r\n            epoch + 1, train_loss.result(),\r\n            train_accuracy.result()*100, test_loss.result(),\r\n            test_accuracy.result()*100)\r\n             )\r\n        \r\n        # Count number of non-zero parameters in each layer and in total-\r\n        # print(\"layer-wise manner model, number of nonzero parameters in each layer are: \\n\")\r\n        model_sum_params = 0\r\n        \r\n        for layer in winning_ticket_model.trainable_weights:\r\n            # print(tf.math.count_nonzero(layer, axis = None).numpy())\r\n            model_sum_params += tf.math.count_nonzero(layer, axis = None).numpy()\r\n        \r\n        print(\"Total number of trainable parameters = {0}\\n\".format(model_sum_params))\r\n    \r\n        \r\n        # Code for manual Early Stopping:\r\n        if np.abs(test_loss.result() < best_val_loss) >= minimum_delta:\r\n            # update 'best_val_loss' variable to lowest loss encountered so far-\r\n            best_val_loss = test_loss.result()\r\n            \r\n            # reset 'loc_patience' variable-\r\n            loc_patience = 0\r\n            \r\n        else:  # there is no improvement in monitored metric 'val_loss'\r\n            loc_patience += 1  # number of epochs without any improvement\r\n\r\n\r\nGives the following error:\r\n\r\n\r\n> --------------------------------------------------------------------------- InvalidArgumentError                      Traceback (most recent call\r\n> last) <ipython-input-47-bca851ce138d> in <module>\r\n>      19 \r\n>      20     for x, y in train_dataset:\r\n> ---> 21         train_one_step(q_aware_model, mask_model, optimizer, x, y)\r\n>      22 \r\n>      23 \r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\r\n> in __call__(self, *args, **kwds)\r\n>     578         xla_context.Exit()\r\n>     579     else:\r\n> --> 580       result = self._call(*args, **kwds)\r\n>     581 \r\n>     582     if tracing_count == self._get_tracing_count():\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\r\n> in _call(self, *args, **kwds)\r\n>     642         # Lifting succeeded, so variables are initialized and we can run the\r\n>     643         # stateless function.\r\n> --> 644         return self._stateless_fn(*args, **kwds)\r\n>     645     else:\r\n>     646       canon_args, canon_kwds = \\\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\r\n> in __call__(self, *args, **kwargs)    2418     with self._lock:   \r\n> 2419       graph_function, args, kwargs =\r\n> self._maybe_define_function(args, kwargs)\r\n> -> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access    2421     2422   @property\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\r\n> in _filtered_call(self, args, kwargs)    1659       `args` and\r\n> `kwargs`.    1660     \"\"\"\r\n> -> 1661     return self._call_flat(    1662         (t for t in nest.flatten((args, kwargs), expand_composites=True)    1663         \r\n> if isinstance(t, (ops.Tensor,\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\r\n> in _call_flat(self, args, captured_inputs, cancellation_manager)   \r\n> 1743         and executing_eagerly):    1744       # No tape is\r\n> watching; skip to running the function.\r\n> -> 1745       return self._build_call_outputs(self._inference_function.call(    1746       \r\n> ctx, args, cancellation_manager=cancellation_manager))    1747    \r\n> forward_backward = self._select_forward_and_backward_functions(\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\r\n> in call(self, ctx, args, cancellation_manager)\r\n>     591       with _InterpolateFunctionError(self):\r\n>     592         if cancellation_manager is None:\r\n> --> 593           outputs = execute.execute(\r\n>     594               str(self.signature.name),\r\n>     595               num_outputs=self._num_outputs,\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\r\n> in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n>      57   try:\r\n>      58     ctx.ensure_initialized()\r\n> ---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n>      60                                         inputs, attrs, num_outputs)\r\n>      61   except core._NotOkStatusException as e:\r\n> \r\n> InvalidArgumentError:  var and grad do not have the same shape[10]\r\n> [100,10] \t [[node Adam/Adam/update_4/ResourceApplyAdam (defined at\r\n> <ipython-input-37-9c297d161e54>:29) ]]\r\n> [Op:__inference_train_one_step_20360]\r\n> \r\n> Errors may have originated from an input operation. Input Source\r\n> operations connected to node Adam/Adam/update_4/ResourceApplyAdam: \r\n> Mul_4 (defined at <ipython-input-37-9c297d161e54>:26)\t \r\n> sequential/quant_dense_2/BiasAdd/ReadVariableOp/resource (defined at\r\n> /home/arjun/.local/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:162)\r\n> \r\n> Function call stack: train_one_step\r\n\r\n\r\nIs there a way to combine TF model Quantization along with tf.GradientTape?\r\n\r\nThanks!\r\n\r\n\r\n  [1]: https://www.tensorflow.org/model_optimization/guide/quantization/training_example\r\n  [2]: https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/Quantization_LTH_LeNet_300_100_MNIST.ipynb\r\n", "comments": ["@arjun-majumdar \r\n\r\nRequest you to provide colab link or simple standalone code with supporting files to reproduce the issue in our environment(I believe the saved model file is missing) .It helps us in localizing the issue faster.Thanks!", "@ravikyram added the .h5 saved model file to [sub-model](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/LeNet_300_MNIST_Magnitude_Winning_Ticket_Distribution_91.18900266306589.h5)\r\n\r\nPlease let me know if I can help with anything else?", "@arjun-majumdar \r\n\r\nI am not able to open example code given by you.Please, help.Thanks!", "@ravikyram here is the URL to [Google Colab Jupyter Notebook](https://colab.research.google.com/drive/1KloMn8UvIytfjgkkT3DczVs8CYvBPdD5?usp=sharing)\r\n\r\nThe cell you might be interested in the Google Colab Jupyter Notebook is cell number 46.\r\n\r\nLet me know if I can help in anything else. Looking forward to your reply.", "@arjun-majumdar \r\n\r\nPlease, grant me access for Colab notebook.Thanks!", "@ravikyram Granted! Hope it works now.", "I have tried in colab with TF version 2.2 ,nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e5dd1e695a9721caa61eb41a963d6bc3/untitled136.ipynb).Thanks!", "@arjun-majumdar Sorry for the late response. Can you please check with recent TFversion 2.4rc3 and let us know whether the issue persists or not. Thanks!", "@jvishnuvardhan \r\n\r\nI installed TF 2.4rc3 as asked and the error persists. If you navigate to the last cell in the notebook, you can find the error:\r\n\r\n\r\n> ---------------------------------------------------------------------------\r\n> \r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> \r\n> <ipython-input-49-bca851ce138d> in <module>()\r\n>      19 \r\n>      20     for x, y in train_dataset:\r\n> ---> 21         train_one_step(q_aware_model, mask_model, optimizer, x, y)\r\n>      22 \r\n>      23 \r\n> \r\n> 4 frames\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n>      58     ctx.ensure_initialized()\r\n>      59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n> ---> 60                                         inputs, attrs, num_outputs)\r\n>      61   except core._NotOkStatusException as e:\r\n>      62     if name is not None:\r\n> \r\n> InvalidArgumentError:  var and grad do not have the same shape[10] [100,10]\r\n> \t [[node Adam/Adam/update_4/ResourceApplyAdam (defined at <ipython-input-41-9c297d161e54>:29) ]] [Op:__inference_train_one_step_25816]\r\n> \r\n> Errors may have originated from an input operation.\r\n> Input Source operations connected to node Adam/Adam/update_4/ResourceApplyAdam:\r\n>  sequential/quant_dense_2/BiasAdd/ReadVariableOp (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:167)\t\r\n>  Mul_4 (defined at <ipython-input-41-9c297d161e54>:26)\r\n> \r\n> Function call stack:\r\n> train_one_step\r\n> \r\n> \r\n\r\n\r\nIs there any resolution to this problem?\r\n\r\nThanks!", "@arjun-majumdar Sorry for the late response.  Is this still an issue for you?\r\n\r\nIt would be great If you could provide a simple standalone. The current code is too long for us to debug and resolve the issue. We generally suggest to post this kind of questions in Stackoverflow where there is a large community to support this kind of questions. \r\n\r\nThis GitHub repository is mainly for bugs and performance related issues. Thanks! \r\n\r\n", "@jvishnuvardhan Yes, the issue still persists.\r\n\r\nA \"simple\" standalone code is as follows:\r\n\r\n```\r\n# Create a quantized model-\r\nquantize_model = tfmot.quantization.keras.quantize_model\r\n\r\n# q_aware stands for for quantization aware.\r\nq_aware_model = quantize_model(winning_ticket_model)\r\n\r\n# 'quantize_model' requires recompilation-\r\nq_aware_model.compile(\r\n    optimizer = tf.keras.optimizers.Adam(lr = 0.0012),\r\n    loss=tf.keras.losses.categorical_crossentropy,\r\n    metrics=['accuracy']\r\n)\r\n\r\n@tf.function\r\ndef train_one_step(model, mask_model, optimizer, x, y):\r\n    '''\r\n    Function to compute one step of gradient descent optimization\r\n    '''\r\n    with tf.GradientTape() as tape:\r\n        # Make predictions using defined model-\r\n        y_pred = model(x)\r\n\r\n        # Compute loss-\r\n        loss = loss_fn(y, y_pred)\r\n        \r\n    # Compute gradients wrt defined loss and weights and biases-\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    \r\n    # type(grads)\r\n    # list\r\n    \r\n    # List to hold element-wise multiplication between-\r\n    # computed gradient and masks-\r\n    grad_mask_mul = []\r\n    \r\n    # Perform element-wise multiplication between computed gradients and masks-\r\n    for grad_layer, mask in zip(grads, mask_model.trainable_weights):\r\n        grad_mask_mul.append(tf.math.multiply(grad_layer, mask))\r\n    \r\n    # Apply computed gradients to model's weights and biases-\r\n    optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))\r\n\r\n    # Compute accuracy-\r\n    train_loss(loss)\r\n    train_accuracy(y, y_pred)\r\n\r\n    return None\r\n\r\n\r\n# Get one batch of training data and labels-\r\nx, y = next(iter(train_dataset))\r\nprint(f\"x.shape = {x.shape} & y.shape = {y.shape}\")\r\n\r\n# Perform one training iteration/step-\r\ntrain_one_step(q_aware_model, mask_model, optimizer, x, y)\r\n```\r\n\r\nYou can refer to the complete Google Colab jupyter-notebook [here](https://colab.research.google.com/drive/1BH3S6w9n_JtRIBMck-NqC-PKPRe5F4FR?usp=sharing)\r\n\r\nLet me know if you need anything else.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Team, any update on this", "@arjun-majumdar Could you please try on the TF v2.6.0 and let us know if it helps? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41320\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41320\">No</a>\n"]}, {"number": 41319, "title": "Quantization Aware Training with tf.GradientTape gives Error in TensorFlow2.0", "body": "Hey Guys, I am using TensorFlow-2.2, tensorflow_model_optimization and Python 3.8. I am trying to quantize and train a LeNet-300-100 Dense neural network which contains sparsity of 91.3375%. This means that 91.3375% of the weights are zero. I was following the [Quantization TF tutorial][1] and I wanted to train such a sparse network which has been quantized using *tf.GradientTape* rather than *q_aware_model.fit()*.\r\n\r\nIf you look into the [example code][2], the relevant code snippets are:\r\n\r\n\r\n    quantize_model = tfmot.quantization.keras.quantize_model\r\n    \r\n    # q_aware stands for for quantization aware.\r\n    q_aware_model = quantize_model(model)\r\n    \r\n    \r\n    # 'quantize_model' requires recompilation-\r\n    q_aware_model.compile(\r\n        optimizer = tf.keras.optimizers.Adam(lr = 0.0012),\r\n        loss=tf.keras.losses.categorical_crossentropy,\r\n        metrics=['accuracy']\r\n    )\r\n    \r\n    \r\n    # Define 'train_one_step()' and 'test_step()' functions here-\r\n    @tf.function\r\n    def train_one_step(model, mask_model, optimizer, x, y):\r\n        '''\r\n        Function to compute one step of gradient descent optimization\r\n        '''\r\n        with tf.GradientTape() as tape:\r\n            # Make predictions using defined model-\r\n            y_pred = model(x)\r\n    \r\n            # Compute loss-\r\n            loss = loss_fn(y, y_pred)\r\n            \r\n        # Compute gradients wrt defined loss and weights and biases-\r\n        grads = tape.gradient(loss, model.trainable_variables)\r\n        \r\n        # type(grads)\r\n        # list\r\n        \r\n        # List to hold element-wise multiplication between-\r\n        # computed gradient and masks-\r\n        grad_mask_mul = []\r\n        \r\n        # Perform element-wise multiplication between computed gradients and masks-\r\n        for grad_layer, mask in zip(grads, mask_model.trainable_weights):\r\n            grad_mask_mul.append(tf.math.multiply(grad_layer, mask))\r\n        \r\n        # Apply computed gradients to model's weights and biases-\r\n        optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))\r\n    \r\n        # Compute accuracy-\r\n        train_loss(loss)\r\n        train_accuracy(y, y_pred)\r\n    \r\n        return None\r\n        \r\n        \r\n    @tf.function\r\n    def test_step(model, optimizer, data, labels):\r\n        \"\"\"\r\n        Function to test model performance\r\n        on testing dataset\r\n        \"\"\"\r\n        \r\n        predictions = model(data)\r\n        t_loss = loss_fn(labels, predictions)\r\n    \r\n        test_loss(t_loss)\r\n        test_accuracy(labels, predictions)\r\n    \r\n        return None\r\n    \r\n    \r\n    \r\n    # Train model using 'GradientTape'-\r\n        \r\n    # Initialize parameters for Early Stopping manual implementation-\r\n    # best_val_loss = 100\r\n    # loc_patience = 0\r\n        \r\n    for epoch in range(num_epochs):\r\n        \r\n        if loc_patience >= patience:\r\n            print(\"\\n'EarlyStopping' called!\\n\")\r\n            break\r\n            \r\n        # Reset the metrics at the start of the next epoch\r\n        train_loss.reset_states()\r\n        train_accuracy.reset_states()\r\n        test_loss.reset_states()\r\n        test_accuracy.reset_states()\r\n                \r\n        \r\n        for x, y in train_dataset:\r\n            train_one_step(q_aware_model, mask_model, optimizer, x, y)\r\n    \r\n    \r\n        for x_t, y_t in test_dataset:\r\n            test_step(q_aware_model, optimizer, x_t, y_t)\r\n    \r\n        template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f}, Test Accuracy: {4:4f}'\r\n        \r\n        '''\r\n        # 'i' is the index for number of pruning rounds-\r\n        history_main[i]['accuracy'][epoch] = train_accuracy.result() * 100\r\n        history_main[i]['loss'][epoch] = train_loss.result()\r\n        history_main[i]['val_loss'][epoch] = test_loss.result()\r\n        history_main[i]['val_accuracy'][epoch] = test_accuracy.result() * 100\r\n        ''' \r\n    \r\n        print(template.format(\r\n            epoch + 1, train_loss.result(),\r\n            train_accuracy.result()*100, test_loss.result(),\r\n            test_accuracy.result()*100)\r\n             )\r\n        \r\n        # Count number of non-zero parameters in each layer and in total-\r\n        # print(\"layer-wise manner model, number of nonzero parameters in each layer are: \\n\")\r\n        model_sum_params = 0\r\n        \r\n        for layer in winning_ticket_model.trainable_weights:\r\n            # print(tf.math.count_nonzero(layer, axis = None).numpy())\r\n            model_sum_params += tf.math.count_nonzero(layer, axis = None).numpy()\r\n        \r\n        print(\"Total number of trainable parameters = {0}\\n\".format(model_sum_params))\r\n    \r\n        \r\n        # Code for manual Early Stopping:\r\n        if np.abs(test_loss.result() < best_val_loss) >= minimum_delta:\r\n            # update 'best_val_loss' variable to lowest loss encountered so far-\r\n            best_val_loss = test_loss.result()\r\n            \r\n            # reset 'loc_patience' variable-\r\n            loc_patience = 0\r\n            \r\n        else:  # there is no improvement in monitored metric 'val_loss'\r\n            loc_patience += 1  # number of epochs without any improvement\r\n\r\n\r\nGives the following error:\r\n\r\n\r\n> --------------------------------------------------------------------------- InvalidArgumentError                      Traceback (most recent call\r\n> last) <ipython-input-47-bca851ce138d> in <module>\r\n>      19 \r\n>      20     for x, y in train_dataset:\r\n> ---> 21         train_one_step(q_aware_model, mask_model, optimizer, x, y)\r\n>      22 \r\n>      23 \r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\r\n> in __call__(self, *args, **kwds)\r\n>     578         xla_context.Exit()\r\n>     579     else:\r\n> --> 580       result = self._call(*args, **kwds)\r\n>     581 \r\n>     582     if tracing_count == self._get_tracing_count():\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\r\n> in _call(self, *args, **kwds)\r\n>     642         # Lifting succeeded, so variables are initialized and we can run the\r\n>     643         # stateless function.\r\n> --> 644         return self._stateless_fn(*args, **kwds)\r\n>     645     else:\r\n>     646       canon_args, canon_kwds = \\\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\r\n> in __call__(self, *args, **kwargs)    2418     with self._lock:   \r\n> 2419       graph_function, args, kwargs =\r\n> self._maybe_define_function(args, kwargs)\r\n> -> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access    2421     2422   @property\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\r\n> in _filtered_call(self, args, kwargs)    1659       `args` and\r\n> `kwargs`.    1660     \"\"\"\r\n> -> 1661     return self._call_flat(    1662         (t for t in nest.flatten((args, kwargs), expand_composites=True)    1663         \r\n> if isinstance(t, (ops.Tensor,\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\r\n> in _call_flat(self, args, captured_inputs, cancellation_manager)   \r\n> 1743         and executing_eagerly):    1744       # No tape is\r\n> watching; skip to running the function.\r\n> -> 1745       return self._build_call_outputs(self._inference_function.call(    1746       \r\n> ctx, args, cancellation_manager=cancellation_manager))    1747    \r\n> forward_backward = self._select_forward_and_backward_functions(\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\r\n> in call(self, ctx, args, cancellation_manager)\r\n>     591       with _InterpolateFunctionError(self):\r\n>     592         if cancellation_manager is None:\r\n> --> 593           outputs = execute.execute(\r\n>     594               str(self.signature.name),\r\n>     595               num_outputs=self._num_outputs,\r\n> \r\n> ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\r\n> in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n>      57   try:\r\n>      58     ctx.ensure_initialized()\r\n> ---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n>      60                                         inputs, attrs, num_outputs)\r\n>      61   except core._NotOkStatusException as e:\r\n> \r\n> InvalidArgumentError:  var and grad do not have the same shape[10]\r\n> [100,10] \t [[node Adam/Adam/update_4/ResourceApplyAdam (defined at\r\n> <ipython-input-37-9c297d161e54>:29) ]]\r\n> [Op:__inference_train_one_step_20360]\r\n> \r\n> Errors may have originated from an input operation. Input Source\r\n> operations connected to node Adam/Adam/update_4/ResourceApplyAdam: \r\n> Mul_4 (defined at <ipython-input-37-9c297d161e54>:26)\t \r\n> sequential/quant_dense_2/BiasAdd/ReadVariableOp/resource (defined at\r\n> /home/arjun/.local/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:162)\r\n> \r\n> Function call stack: train_one_step\r\n\r\n\r\nIs there a way to combine TF model Quantization along with tf.GradientTape?\r\n\r\nThanks!\r\n\r\nThe .h5 model file used in this code can be accessed in: [sub-model](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/LeNet_300_MNIST_Magnitude_Winning_Ticket_Distribution_91.18900266306589.h5)\r\n\r\nPlease let me know if I can help with anything else?\r\n\r\n  [1]: https://www.tensorflow.org/model_optimization/guide/quantization/training_example\r\n  [2]: https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/Quantization_LTH_LeNet_300_100_MNIST.ipynb\r\n", "comments": ["@arjun-majumdar Closing this issue as this is a duplicate.\r\nDuplicate of https://github.com/tensorflow/tensorflow/issues/41320\r\n\r\nWill follow the progress in the other issue https://github.com/tensorflow/tensorflow/issues/41320. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41319\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41319\">No</a>\n"]}, {"number": 41318, "title": "tensorflow lite example is not working converting file to tflite model. ", "body": "what I'm facing is as same as below link\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/30005\r\n\r\nit worked a couple of month ago, but now, it's not working. \r\nthere's gesture calssification listed on example on github and I opened index.html from web folder then I trained to get file. \r\nbut then, how can I convert it to tflite model without collab? \r\ncollab code is not working as link says \r\n", "comments": ["@kotran88 \r\nPlease share a simple standalone code for us to replicate the issue faced and tf version to replicate it.\r\nThe issue reported is tensorflow js issue, in that case please open it on [respective repo](https://github.com/tensorflow/tfjs/issues).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41318\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41318\">No</a>\n"]}, {"number": 41317, "title": "Undefined behavior in tf.multiply", "body": "**System information**\r\n- OS Platform and Distribution: Linux 5.7.8-5, Manjaro KDE\r\n- TensorFlow installed from: conda-forge\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.3\r\n- GCC/Compiler version: 10.1.0\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory:  GeForce RTX 2070, 7982 MiB\r\n\r\n**Current behavior**\r\n```python\r\nIn [1]: a = np.random.randint(0, 10, (2, 2))                                                                                                                                                                            \r\n\r\nIn [2]: w = np.array([2, 5])                                                                                                                                                                                            \r\n\r\nIn [3]: x = tf.convert_to_tensor(a)                                                                                                                                                                                     \r\n\r\nIn [4]: y = tf.convert_to_tensor(w)                                                                                                                                                                                     \r\n\r\nIn [5]: x                                                                                                                                                                                                               \r\nOut[5]: \r\n<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\r\narray([[8, 0],\r\n       [5, 7]])>\r\n\r\nIn [6]: y                                                                                                                                                                                                               \r\nOut[6]: <tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 5])>\r\n\r\nIn [7]: x * y                                                                                                                                                                                                           \r\nOut[7]: \r\n<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\r\narray([[9, 6],\r\n       [0, 0]])>\r\nIn [8]: tf.multiply(x, y)                                                                                                                                                                                               \r\nOut[8]: \r\n<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\r\narray([[0, 6],\r\n       [7, 3]])>\r\n```\r\nEach time the multiplication operation is performed without any modification to `x` and `y` the results were different. I could not find any plausible explanation of this behavior.\r\n\r\n**Expected behavior**\r\nWhen I performed same multiplication in `numpy` it computed the expected result.\r\n```python\r\nIn [9]: a                                                                                                                                                                                                               \r\nOut[9]: \r\narray([[8, 0],\r\n       [5, 7]])\r\n\r\nIn [10]:  w                                                                                                                                                                                                              \r\nOut[10]: array([2, 5])\r\n\r\nIn [11]: a * w                                                                                                                                                                                                           \r\nOut[11]: \r\narray([[16,  0],\r\n       [10, 35]])\r\n``` \r\n\r\nIs there any explanation behind this unexpected behavior? If it is intended behavior then how do I compute the result I expect in this case?\r\n", "comments": ["Hi, \r\nI tried to replicate the issue and here is the link to my [Notebook]:(https://colab.research.google.com/drive/107mR4pm9OexQEBNDHHpA94LJL84jkZdy?usp=sharing)\r\nI was not able to replicate this. ", "I rechecked the issue and found that this behavior occurs when another instance of `tensorflow` running with considerable GPU memory usage. For example:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      5214      G   /usr/lib/Xorg                                516MiB |\r\n|    0      6005      G   /usr/bin/kwin_x11                            177MiB |\r\n|    0      6066      G   /usr/bin/plasmashell                          68MiB |\r\n|    0      6500      G   /snap/pycharm-community/202/jbr/bin/java       3MiB |\r\n|    0     44308      G   /usr/lib/firefox/firefox                       2MiB |\r\n|    0   1107722      G   /usr/lib/firefox/firefox                       2MiB |\r\n|    0   3337621      C   ...e/xxxx/.conda/envs/Py3Dev/bin/python     6745MiB |\r\n|    0   3400212      C   ...e/xxxx/.conda/envs/Py3Dev/bin/python      407MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```\r\nThe reported behavior occurs. And the behavior is not identical each time. However, once it reported `CUDA_ERROR_OUT_OF_MEMORY` but returned the resulting tensor as zeros.\r\n\r\n```python\r\nIn [1]: import numpy as np                                                                                                                                                                                               \r\n\r\nIn [2]: import tensorflow as tf                                                                                                                                                                                          \r\n\r\nIn [3]: a = np.random.randint(0, 10, (2, 2))                                                                                                                                                                             \r\n\r\nIn [4]: b = np.array([2, 5])                                                                                                                                                                                             \r\n\r\nIn [5]: x = tf.convert_to_tensor(a)\r\n\r\nn [6]: y = tf.convert_to_tensor(b)                                                                                                                                                                                      \r\n\r\nIn [7]: x                                                                                                                                                                                                                \r\nOut[7]: \r\n<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\r\narray([[0, 4],\r\n       [4, 0]])>\r\n\r\nIn [8]: y                                                                                                                                                                                                                \r\nOut[8]: <tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 5])>\r\n\r\nIn [9]: x * y                                                                                                                                                                                                           \r\n2020-07-12 17:07:47.029298: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 187.12M (196214784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\nOut[9]: \r\n<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\r\narray([[0, 0],\r\n       [0, 0]])>\r\n\r\n```\r\nBut it continued to generate garbage values in subsequent tries without any error. My guess is, it is a memory related issue. If if is indeed out of memory shouldn't it throw an exception without returning anything, instead of and warning / info?\r\n\r\nOtherwise, it generates correct result when there is plenty of free GPU memory.  \r\n", "@digital-idiot \r\n\r\nI tried in colab with TF version 2.2 and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d5c442e058d727ffd6a96b5a4e3eafb6/untitled110.ipynb).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41317\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41317\">No</a>\n"]}, {"number": 41316, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [2], [batch]: [5]", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [2], [batch]: [5]`\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n\r\nfrom datasets import PascalVOCDataset\r\n\r\nclass PascalVOCDataset(Dataset):\r\n    \"\"\"\r\n    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\r\n    \"\"\"\r\n\r\n    def __init__(self, data_folder, split, keep_difficult=False):\r\n        \"\"\"\r\n        :param data_folder: folder where data files are stored\r\n        :param split: split, one of 'TRAIN' or 'TEST'\r\n        :param keep_difficult: keep or discard objects that are considered difficult to detect?\r\n        \"\"\"\r\n        self.split = split.upper()\r\n\r\n        assert self.split in {'TRAIN', 'TEST'}\r\n\r\n        self.data_folder = data_folder\r\n        self.keep_difficult = keep_difficult\r\n\r\n        # Read data files\r\n        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\r\n            self.images = json.load(j)\r\n        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\r\n            self.objects = json.load(j)\r\n        print('dataset_path->',os.path.join(data_folder, self.split + '_images.json'))\r\n\r\n        assert len(self.images) == len(self.objects)\r\n\r\n    def __getitem__(self, i):\r\n        # Read image\r\n        #print('get_item')\r\n        image = Image.open(self.images[i], mode='r')\r\n        image = image.convert('RGB')\r\n\r\n        #print('images_shape->',np.array(image).shape)\r\n\r\n        # Read objects in this image (bounding boxes, labels, difficulties)\r\n        objects = self.objects[i]\r\n        #print('image->',image)\r\n        #print('objects[boxes]->',objects['boxes'])\r\n        boxes = torch.FloatTensor(objects['boxes'])  # (n_objects, 4)\r\n        labels = torch.LongTensor(objects['labels'])  # (n_objects)\r\n        difficulties = torch.ByteTensor(objects['difficulties'])  # (n_objects)\r\n\r\n        # Discard difficult objects, if desired\r\n        if not self.keep_difficult:\r\n            boxes = boxes[1 - difficulties]\r\n            labels = labels[1 - difficulties]\r\n            difficulties = difficulties[1 - difficulties]\r\n\r\n        # Apply transformations\r\n        #print('dataset_boxes->',boxes)\r\n        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split)\r\n\r\n        return image, boxes, labels, difficulties\r\n\r\n # Custom dataloaders\r\n    train_dataset = PascalVOCDataset(data_folder,\r\n                                     split='train',\r\n                                     keep_difficult=keep_difficult)\r\n\r\ndef run_train(dataset, num_epochs=1):\r\n    start_time = time.perf_counter()\r\n    print('run_train')\r\n    model = SSD(n_classes=20)\r\n    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy)\r\n    print('dataset->',dataset)\r\n\r\n    for _ in tf.data.Dataset.range(num_epochs):\r\n        for idx,(images,boxes,labels) in enumerate(dataset): # (batch_size (N), 300, 300, 3)\r\n            print('=========================================================')\r\n            images = np.array(images)\r\n            labels = np.array(labels)\r\n            boxes = np.array(list(boxes))\r\n            print('image_shape->', images.shape)\r\n            print('labels_shape->',labels.shape)\r\n            print('boxes_shape->',boxes.shape)\r\n\r\n            if isprint: tf.print(type(images), type(labels),images.shape,labels.shape)\r\n            predicted_locs, predicted_socres = model(images)# (N, 8732, 4), (N, 8732, n_classes)\r\n            loss = criterion(predicted_locs,predicted_socres,boxes,labels)\r\n            print('loss->',loss)\r\n            if idx ==10: break\r\n        pass\r\n    tf.print(\"\uc2e4\ud589 \uc2dc\uac04:\", time.perf_counter() - start_time)\r\ndef train():\r\n    print('train')\r\n    print(tf.__version__)\r\n    batch_size= 8\r\n    images,boxes,labels,difficulties,new_boxes= PascalVOCDataset()\r\n    new_boxes = list(new_boxes)\r\n\r\n\r\n    boxes = tf.ragged.constant(boxes)\r\n    labels = tf.ragged.constant(labels)\r\n    new_boxes = tf.ragged.constant(new_boxes)\r\n\r\n    print('boxes->',boxes.shape)\r\n    print('labels->',labels.shape)\r\n    print('images->', np.array(images).shape)\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices((images,new_boxes,labels))\r\n    run_train(dataset.map(resize_image_bbox, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE))\r\n\r\ndef main():\r\n    train()\r\nif __name__ =='__main__':\r\n    main()\r\n\r\n```\r\n\r\n\r\n\r\n", "comments": ["@SlowMonk,\r\nOn running the code I am facing an error stating `NameError: name 'PascalVOCDataset' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2404b379932ae206f2776e152ad64ea7/41316.ipynb#scrollTo=dDXsAE63p9ld). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. Thanks!\r\n\r\nAlso, please check [this comment](https://github.com/tensorflow/tensorflow/issues/34544#issuecomment-581446568) from a similar issue and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41315, "title": "AttributeError: module 'tensorflow.python.ops.special_math_ops' has no attribute 'bessel_i0e'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: AMD RX-580\r\n\r\n\r\n\r\n**Describe the problem**\r\nAttributeError: module 'tensorflow.python.ops.special_math_ops' has no attribute 'bessel_i0e' arises when trying to use tensorflow's object detection API to fine-tune EfficientDet to detect a custom class I created a dataset and matching test/train TFrecord files.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. I followed the instructions on https://github.com/tensorflow/models/tree/master/official#running-the-models under \"How to get started with the official models\" to install dependencies, clone the models repo, and add the models folder to my python path. The last part I accomplished by adding a sys.path.append(r'C:\\Users\\user\\models') line to the beginning of model_main_tf2.py, which is the file I was under the impression runs the training loop. I also installed protoc-3.12.3-win64.zip and ran \"C:/Program Files/protoc/bin/protoc\" object_detection/protos/*.proto --python_out=. in the models directory.\r\n\r\n2. I downloaded and configured the config files for EfficientDet D7 from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md to point to my TFrecord test/train files, the checkpoint/ckpt-0 file and label_map.txt files.\r\n\r\n3. I ran the following command in an anaconda powershell which prompted the AttributeError: module 'tensorflow.python.ops.special_math_ops' has no attribute 'bessel_i0e':\r\n\r\npython model_main_tf2.py --model_dir=C:\\Users\\user\\efficientdet_d7_coco17_tpu-32\\training --num_train_steps=10000 --sample_1_of_n_eval_examples=1 --pipeline_config_path=C:\\Users\\user\\efficientdet_d7_coco17_tpu-32\\pipeline.config --alsologtostderr \r\n\r\nI also tried it with and without the --checkpoint_dir = C:\\Users\\user\\efficientdet_d7_coco17_tpu-32\\checkpoint flag\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n(base) C:\\Users\\user\\models\\research\\object_detection>python model_main_tf2.py --model_dir=C:\\Users\\user\\efficientdet_d7_coco17_tpu-32\\training --num_train_steps=10000 --sample_1_of_n_eval_examples=1 --pipeline_config_path=C:\\Users\\user\\efficientdet_d7_coco17_tpu-32\\pipeline.config --alsologtostderr\r\n2020-07-12 02:03:31.351897: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-07-12 02:03:31.356039: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"model_main_tf2.py\", line 35, in <module>\r\n    import tensorflow.compat.v2 as tf\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 74, in <module>\r\n    from tensorflow.python.ops.standard_ops import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\standard_ops.py\", line 27, in <module>\r\n    from tensorflow.python.training.experimental import loss_scaling_gradient_tape\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\experimental\\loss_scaling_gradient_tape.py\", line 21, in <module>\r\n    from tensorflow.python.distribute import distribution_strategy_context\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python.distribute.experimental import collective_all_reduce_strategy\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\experimental\\__init__.py\", line 25, in <module>\r\n    from tensorflow.python.distribute import tpu_strategy\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\tpu_strategy.py\", line 28, in <module>\r\n    from tensorflow.compiler.xla.experimental.xla_sharding import xla_sharding\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\compiler\\xla\\experimental\\xla_sharding\\xla_sharding.py\", line 23, in <module>\r\n    from tensorflow.compiler.tf2xla.python import xla as tf2xla\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\compiler\\tf2xla\\python\\xla.py\", line 107, in <module>\r\n    bessel_i0e = _unary_op(special_math_ops.bessel_i0e)\r\nAttributeError: module 'tensorflow.python.ops.special_math_ops' has no attribute 'bessel_i0e'\r\n\r\nPlease let me know if there is any other info I can provide.\r\n", "comments": ["@attianopp \r\nCan you please confirm if this is a build on anaconda.\r\n\r\nPlease refer to these issues and let us know if it helps:\r\n#38589 [link](https://github.com/tensorflow/tensorflow/issues/25517#issuecomment-465844967) [link](https://github.com/keras-team/keras/issues/12118#issuecomment-459333540)", "Yes this is an anaconda build, but installed using pip in the anaconda prompt. The second link seems to have been fruitful, un-installing and re-installing tensorflow/keras. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41315\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41315\">No</a>\n"]}, {"number": 41314, "title": "python program spitting out a tensorflow error of \"ImportError: DLL Load failed while importing\" ", "body": "**System information**\r\n- Windows 10 pro 64bit ver 2004\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.3\r\n- Installed using PIP\r\n- CUDA ver: 10.0\r\n- GPU model and memory: NVIDIA GTX 1070 8GB\r\n-My CPU does not support the AVX Instruction set\r\n\r\n\r\nwhen attempting to use a python program, I am greeted with this error (Copied Verbatim below)\r\n     (side note, I'm not great with python or most of the dev tools I've attempted to use to download all of the required libraries, so forgive me if this has a relatively simple solution.)\r\n\r\nThis occurs when I use the CMD prompt to execute the main.py file for this program. to do this, I typed this command\r\nPython main.py\r\nafter a few moments the error is triggered. the prompt remains blank until the error traceback appears.\r\n(I can give more specifics to the best of my ability, but this is all of the information that I am aware that I have to help diagnose)\r\n\r\n(this is the entire error verbatim)\r\n\r\nError when importing libraries:  Traceback (most recent call last):\r\n  File \"G:\\APPLICATIONS\\Python\\3.8.3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"G:\\APPLICATIONS\\Python\\3.8.3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"G:\\APPLICATIONS\\Python\\3.8.3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"G:\\APPLICATIONS\\Python\\3.8.3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"G:\\APPLICATIONS\\Python\\3.8.3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nSome Python libraries are missing. You can install all required libraries by running in the command line 'pip install -r requirements.txt'\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "is there a way around using the 2.2.0 version of TF without the AVX instruction set? right now I just want to see this work, processing time and efficiency do not matter at the moment.", "@SugoiShades \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\n**TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.**\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\n\r\nIf your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n\r\nTry Google Colab to use TensorFlow.\r\nThe easiest way to use TF will be to switch to google colab. You get pre-installed latest stable TF version. Also you can use pip install to install any other preferred TF version.\r\nIt has an added advantage since you can you easily switch to different hardware accelerators\r\n(cpu, gpu, tpu) as per the task.\r\nAll you need is a good internet connection and you are all set.\r\nTry to build TF from sources by changing CPU optimization flags.\r\nThanks!", "@ravikyram My CPU Definitely doesn't support AVX (its a Core i7 980) I but you answered my question, thank you.", "@SugoiShades \r\n\r\nPlease, close this thread if your query has been answered.Thanks!", "I am closing this issue  since the query has been answered. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41314\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41314\">No</a>\n"]}, {"number": 41313, "title": "Colab TPU not working in 2.3.0-rc1, \"InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified\"", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n\r\n- TensorFlow installed from (source or binary): Installed using `!pip install --upgrade tensorflow==2.3.0-rc1`\r\n\r\n- TensorFlow version (use command below): 2.3.0-rc1\r\n- Python version: 3, colab default \r\n\r\n- GPU model and memory: Colab TPU \r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I try to initialize the TPU using \r\n\r\n```\r\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_cluster(tpu)\r\ntf.tpu.experimental.initialize_tpu_system(tpu)\r\n```\r\n\r\nI get this error \r\n\r\n```\r\nINFO:tensorflow:2.3.0-rc1\r\nINFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.6.63.186:8470\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.6.63.186:8470\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Clearing out eager caches\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-5aaeec42239c> in <module>()\r\n     19 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n     20 tf.config.experimental_connect_to_cluster(tpu)\r\n---> 21 tf.tpu.experimental.initialize_tpu_system(tpu)\r\n     22 strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n     23 print('strategy.num_replicas_in_sync', strategy.num_replicas_in_sync)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)\r\n    109     context.context()._clear_caches()  # pylint: disable=protected-access\r\n    110 \r\n--> 111     serialized_topology = output.numpy()\r\n    112 \r\n    113     # TODO(b/134094971): Remove this when lazy tensor copy in multi-device\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1061     \"\"\"\r\n   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1065 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1029       return self._numpy_internal()\r\n   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1032 \r\n   1033   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nTPU should initialize like in 2.2.0\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\nHere is a github colab gist \r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/a4d5459c13b4bb54ace08993f5b174da/tpu_try_2-3-0-rc1.ipynb\r\n\r\n\r\n", "comments": ["@Santosh-Gupta,\r\nThis query has already been answered in another issue. Please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/40622#issuecomment-654488935) from a member of the TensorFlow team. Thanks!"]}, {"number": 41312, "title": "model.fit() InvalidArgumentError:  indices[28,13] = -2147483648 is not in [0, 1193514) ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Conda\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: Disabled (Hardcoding TensorFlow without GPU)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI try to fit my model and face this problem, for some reason, the largest negative number is being looked up.\r\n**Describe the expected behavior**\r\nThe model should proceed through ftting seamlessly\r\n**Standalone code to reproduce the issue**\r\n\r\n**->Embedding layer**\r\n    \r\n\r\n```\r\ndef pretrained_embedding_layer(word_to_vec_map, word_to_index):\r\n   \r\n\r\n    vocab_len = len(word_to_index) + 1            #1193514      \r\n    emb_matrix = np.zeros((vocab_len,embedding_dim))\r\n    for word, idx in word_to_index.items():\r\n        emb_matrix[idx, :] = word_to_vec_map[word]\r\n\r\n    # Definning a pre-trained Embedding layer\r\n    embedding_layer = layers.Embedding(\r\n                        vocab_len,\r\n                        embedding_dim,\r\n                        trainable = False\r\n                        )\r\n\r\n    # Build the embedding layer, it is required before setting the weights of the embedding layer. \r\n    embedding_layer.build((None,))\r\n    \r\n    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\r\n    embedding_layer.set_weights([emb_matrix])\r\n    \r\n    return embedding_layer\r\n```\r\n\r\n\r\n\r\n**->Model**\r\n\r\n```\r\ndef sentiment_model(input_shape, word_to_vec_map, word_to_index):\r\n\r\n\r\n    sentence_indices =layers.Input(shape=input_shape, dtype='float32')\r\n    \r\n    # Create the embedding layer pretrained with GloVe Vectors\r\n    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\r\n    \r\n    # Propagate sentence_indices through your embedding layer\r\n    # (See additional hints in the instructions).\r\n    embeddings = embedding_layer(sentence_indices)   \r\n\r\n    x = layers.LSTM(128)(embeddings)\r\n    x = layers.Dropout(0.5)(x)\r\n    predictions = layers.Dense(2, activation=\"sigmoid\", name=\"predictions\")(x)\r\n    \r\n    # Create Model instance which converts sentence_indices into X.\r\n    model = keras.Model(inputs=sentence_indices,outputs=predictions)   \r\n    return model\r\n```\r\n\r\n\r\n\r\n```\r\ndef sentences_to_indices(X, word_to_index, max_len):\r\n\r\n    X_indices = np.zeros((m,max_len))\r\n    \r\n    # Assign indices to words\r\n    for i,sentence in enumerate(X):        \r\n        sentence_words = sentence.lower().split()\r\n        for j,word in enumerate(sentence_words):\r\n            X_indices[i, j] = word_to_index[word]\r\n    return X_indices\r\n```\r\n\r\n\r\n```\r\nX_train_indices = sentences_to_indices(X_train, word_to_index, max_features)\r\nY_train_OH = to_categorical(Y_train)\r\nmodel.fit(X_train_indices, Y_train_OH, epochs = 10, batch_size = 32)\r\n```\r\n\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n**Train on 28624 samples\r\nEpoch 1/10\r\n   32/28624 [..............................] - ETA: 15:20**\r\n\r\n**InvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-25-4679097c6578> in <module>\r\n----> 1 model.fit(X_train_indices, Y_train_OH, epochs = 10, batch_size = 32)**\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    630         # Lifting succeeded, so variables are initialized and we can run the\r\n    631         # stateless function.\r\n--> 632         return self._stateless_fn(*args, **kwds)\r\n    633     else:\r\n    634       canon_args, canon_kwds = \\\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n   2365   @property\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~\\Anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\n**InvalidArgumentError:  indices[15,2] = -2147483648 is not in [0, 1193514)\r\n\t [[node model_1/embedding_1/embedding_lookup (defined at <ipython-input-25-4679097c6578>:1) ]] [Op:__inference_distributed_function_6120]**\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node model_1/embedding_1/embedding_lookup:\r\n model_1/embedding_1/embedding_lookup/4992 (defined at C:\\Users\\shash\\Anaconda3\\envs\\sentiment_analysis\\lib\\contextlib.py:81)\r\n\r\nFunction call stack:\r\ndistributed_function\r\n\r\n\r\n\r\n", "comments": ["The problem is when the words are being replaced by their corresponding index. If the word wasn't found in the vocabulary/word_to_index dictionary it was being stored as **nan**.\r\n\r\nThe vocabulary is all the words present in the word embeddings (I have used GloVe twitter embeddings).\r\n\r\nModified function:\r\n\r\n    def sentences_to_indices(X, word_to_index, max_len):\r\n\r\n    X_indices = np.zeros((m,max_len))\r\n    \r\n    # Assign indices to words\r\n    for i,sentence in enumerate(X):        \r\n        sentence_words = sentence.lower().split()\r\n        for j,word in enumerate(sentence_words):\r\n            X_indices[i, j] = word_to_index.get(word,0)\r\n    return X_indices\r\n\r\n**Though, I am not sure if words not present in word embeddings should be stored as zero. What should they be stored as?**", "@shashank1558 \r\nI ran the code shared and face a different error, can you please share complete code or share a colab gist with the error faced.\r\nplease find [the gist](https://colab.research.google.com/gist/Saduf2019/73029ee3ca93a982638955237c91b89e/untitled265.ipynb) or error faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41312\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41312\">No</a>\n"]}, {"number": 41311, "title": "Tensorflow default supports dynamic preprocessing (e.g. masking).", "body": "# Environment\r\n- tensorflow 2.x\r\n- google colab\r\n\r\n# Reproducible code\r\nhttps://colab.research.google.com/gist/MokkeMeguru/8c867b2bbc941cc82e115b9044276afc/test.ipynb\r\n\r\n# Problem\r\nif text data preprocessing using the map function, each epoch resets previous preprocessing information.\r\n\r\n```python\r\ndef encode(text):\r\n    encoded_text = encoder.encode(text.numpy())\r\n    tf.print(\"raw\", encoded_text)\r\n    \r\n    def random_masking(s):\r\n        s = [word if tf.random.uniform([]) > mask_rate else mask_idx for word in s]\r\n        return s\r\n\r\n    encoded_text = random_masking(encoded_text)\r\n    tf.print(\"masked\", encoded_text)\r\n    return [encoded_text\r\n\r\ndef encode_map_fn(text):\r\n    encoded_text = tf.py_function(encode, inp=[text], Tout=(tf.int64))\r\n    encoded_text.set_shape([None])\r\n    return encoded_text\r\n\r\nencoder.encode(next(iter(ds)).numpy()\r\n\r\nencoded_ds = ds.map(encode_map_fn).padded_batch(3)\r\n\r\nprint(\"mask is\", mask_idx)\r\n\r\nfor epoch in range(3):\r\n    print(\"epoch :\", epoch)\r\n    for batch in encoded_ds:\r\n        print(batch)\r\n```\r\n```\r\nmask is 12\r\nepoch : 0\r\nraw [11, 6, 1, 5, 9]\r\nmasked [11, 6, 1, 12, 9] # <---------!\r\n...\r\nepoch : 1\r\nraw [11, 6, 1, 5, 9]\r\nmasked [11, 6, 12, 12, 9] # <------!\r\n...\r\n```\r\n\r\nIn some NLP methods, we know static preprocessing (like BERT) and dynamic preprocessing (like RoBERTa) don't have the same performance.\r\n\r\n**I want to know whether this dynamic preprocessing is the correct attribute or not.**\r\nAnd also\r\n**How to keep the preprocessing information**", "comments": ["@MokkeMeguru \r\n\r\nI have tried in colab with TF 2.2, 2.3-rc1, nightly version(`2.4.0-dev20200712`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/6070d45a1939082f62b9cb8df7a11139/untitled.ipynb).You are seeing the same bahavior?Thanks!", "Yes. It has same problem.", "Tried to reproduce the issue in Tf Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/b483514585d014e12c582a84746f61b0/41311.ipynb) and confirm if this is still an issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41311\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41311\">No</a>\n"]}, {"number": 41310, "title": "Create a Simple Speech Recognition example using TensorFlow Version 2", "body": "A tracking bug for migrating the [TF1 Speech Recognition example](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md) to TF 2", "comments": ["#37302 Would benefit from this update."]}, {"number": 41309, "title": "    AttributeError: 'OwnedIterator' object has no attribute '_get_trainable_state'", "body": "Getting  the error AttributeError: in user code:\r\n\r\nDoes anybody know what is causing the issue ?\r\n\r\nThe code was working fine a couple of days back. \r\n\r\n\r\n```\r\n\r\nnew_input = Input(shape=(128 , 128 , 3))\r\nbase = EfficientNetB0(include_top=False ,\r\n                input_tensor=new_input )\r\n\r\nfor layer in base.layers :\r\n    layer.trainable = False\r\n    \r\nx = base.output\r\nx = Conv2D (64 , (3,3) , activation = 'relu')(x)\r\nx = MaxPooling2D((2,2))(x)\r\nx = GlobalAveragePooling2D ()(x)\r\n\r\nout = Dense (6 , activation = 'softmax')(x)\r\n\r\nmodel = Model (inputs = base.input , outputs = out)\r\n\r\ncce = tf.keras.losses.CategoricalCrossentropy()\r\n\r\nmodel.compile(loss=cce,\r\n              optimizer=Adam(),\r\n              metrics=kappa_score)\r\n\r\nhistory = model.fit_generator(train_generator,\r\n                    validation_data=val_generator, \r\n                    epochs = 100)\r\n\r\n```\r\n\r\nLog is as below - \r\n\r\n**---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-49-3e58d15894cd> in <module>\r\n     24 history = model.fit_generator(train_generator,\r\n     25                     validation_data=val_generator,\r\n---> 26                     epochs = 100)\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1477                                 for t in nest.flatten(outputs)])\r\n   1478           step_outputs = step_function(self, iterator)\r\n-> 1479           outputs = nest.map_structure(lambda t1, t2: concat([t1, t2]), outputs,\r\n   1480                                        step_outputs)\r\n   1481         return outputs\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64 from tensorflow.python.ops import math_ops\r\n     65 from tensorflow.python.ops import sparse_ops\r\n---> 66 from tensorflow.python.ops import summary_ops_v2\r\n     67 from tensorflow.python.ops import variables\r\n     68 from tensorflow.python.ops.ragged import ragged_concat_ops\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846 \r\n    847     Arguments:\r\n--> 848         x: Input data. It could be:\r\n    849           - A Numpy array (or array-like), or a list of arrays\r\n    850             (in case the model has multiple inputs).\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578       target_function = target_function.__func__\r\n    579 \r\n--> 580     if hasattr(target_function, \"__code__\"):\r\n    581       return target_function.__code__\r\n    582 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    625         implements_attr = six.ensure_text(self._implements, \"utf-8\")\r\n    626         attr_value = attr_value_pb2.AttrValue()\r\n--> 627         nameattrlist = attr_value_pb2.NameAttrList()\r\n    628         _text_format.Merge(implements_attr, nameattrlist)\r\n    629         attr_value.func.CopyFrom(nameattrlist)\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    504         conversion options when autograph is set to True.\r\n    505       experimental_relax_shapes: When true, argument shapes may be relaxed to\r\n--> 506         avoid unnecessary retracing.\r\n    507       experimental_compile: If `True`, compiles the function using XLA\r\n    508         (see https://tensorflow.org/xla). XLA performs compiler optimizations,\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2444     self._args_to_indices = {arg: i for i, arg in enumerate(args)}\r\n   2445     self._arg_names = args\r\n-> 2446 \r\n   2447     # A cache mapping from arg index to default value, for canonicalization.\r\n   2448     default_values = fullargspec.defaults\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2775 \r\n   2776   def all_values(self):\r\n-> 2777     \"\"\"A set of all `ConcreteFunction` instances held by this cache.\"\"\"\r\n   2778     return set(self.primary.values()) | set(self.arg_relaxed.values())\r\n   2779 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2665       # CompositeTensors should be flattened instead.\r\n   2666       or isinstance(value, composite_tensor.CompositeTensor))\r\n-> 2667 \r\n   2668 \r\n   2669 def _convert_numpy_inputs(inputs):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    979         converted_func = tf_decorator.make_decorator(original_func, wrapper)\r\n    980         python_func = tf_decorator.rewrap(python_func, original_func,\r\n--> 981                                           converted_func)\r\n    982 \r\n    983       else:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    439 \r\n    440   def __del__(self):\r\n--> 441     try:\r\n    442       func_graph_module.dismantle_func_graph(self.func_graph)\r\n    443     except:  # pylint: disable=bare-except\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966                 options=autograph.ConversionOptions(\r\n    967                     recursive=True,\r\n--> 968                     optional_features=autograph_options,\r\n    969                     user_requested=True,\r\n    970                 ))\r\n\r\nAttributeError: in user code:\r\n\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:576 _reset_compile_cache  *\r\n        self._compiled_trainable_state = self._get_trainable_state()\r\n\r\n    AttributeError: 'OwnedIterator' object has no attribute '_get_trainable_state'\r\n** ", "comments": ["@pn12 \r\nCould you please share the tensorflow version on which the issue was faced, also the code is incomplete.\r\nI ran the code shared and face different error as not all dependencies are shared,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/6428591437f9b09683631feab0c06d5f/untitled272.ipynb).\r\n\r\nWith respect to the error faced, you may refer to below issue for assistance:\r\n[link](https://stackoverflow.com/questions/53925428/attributeerror-sequential-object-has-no-attribute-output-names-not-toco-pr) ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41309\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41309\">No</a>\n", "I had the same error and it was because I had upgraded the tensorflow library from 2.2.0 to 2.3.0 but the jupyter server had not been restarted and was picking up the version 2.2.0. Restarting the server fixed it for me.", "Thanks for sharing."]}, {"number": 41308, "title": "[TFLite] Two consecutive Dequantize OPs are generated in the structure of the model after integer quantization", "body": "## 1. System information\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 x86_64\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): tensorflow==2.3.0-rc1\r\n\r\n## 2. Command used to run the converter, and code using the Python API\r\n**[Here's a minimal Google Colaboratory](https://colab.research.google.com/drive/1Nrf_zZjUgrlp4yi6vJFVj2yBZIDwGxYZ?usp=sharing)** that can reproduce the situation.\r\n\r\n### (1) Integer Quantization\r\nPerforms integer quantization. The resources used for quantization are obtained at **[Google Colaboratory](https://colab.research.google.com/drive/1Nrf_zZjUgrlp4yi6vJFVj2yBZIDwGxYZ?usp=sharing)**. This step will successfully generate an integer quantized .tflite file.\r\n```python\r\n### tensorflow==2.3.0-rc1\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef representative_dataset_gen():\r\n  for image in raw_test_data:\r\n    image = tf.image.resize(image, (512, 512))\r\n    image = image[np.newaxis,:,:,:]\r\n    image = image - 127.5\r\n    image = image * 0.007843\r\n    yield [image]\r\n\r\nraw_test_data = np.load('calibration_data_img_coco_512x512.npy', allow_pickle=True)\r\n\r\n# Integer Quantization - Input/Output=float32\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_quant_model = converter.convert()\r\nwith open('efficientdet_d0_512x512_integer_quant.tflite', 'wb') as w:\r\n    w.write(tflite_quant_model)\r\nprint(\"Integer Quantization complete! - efficientdet_d0_512x512_integer_quant.tflite\")\r\n```\r\n```console\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\n  :\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Issue encountered when serializing global_step.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\nto_proto not supported in EAGER mode.\r\nInteger Quantization complete! - efficientdet_d0_512x512_integer_quant.tflite\r\n```\r\n\r\n### (2) Checking the operation of the model\r\nThe next step is to check the execution of the model with a minimal amount of test code. When you run this test code, you'll see an error in the input geometry mismatch for the Dequantize OP occurs.\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"efficientdet_d0_512x512_integer_quant.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nprint('input:', input_details)\r\nprint('')\r\nprint('output:', output_details)\r\n\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint('output_data.shape:', output_data.shape)\r\n```\r\n```console\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-7-c9ae37dddd9b> in <module>()\r\n      3 \r\n      4 interpreter = tf.lite.Interpreter(model_path=\"efficientdet_d0_512x512_integer_quant.tflite\")\r\n----> 5 interpreter.allocate_tensors()\r\n      6 input_details = interpreter.get_input_details()\r\n      7 output_details = interpreter.get_output_details()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)\r\n    241   def allocate_tensors(self):\r\n    242     self._ensure_safe()\r\n--> 243     return self._interpreter.AllocateTensors()\r\n    244 \r\n    245   def _safe_to_run(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 782 (DEQUANTIZE) failed to prepare.\r\n```\r\n\r\n## 3. Links to saved models, GraphDef, and test datasets\r\nThe links below contain EfficientDet D0's GraphDef and saved_model and test datasets.\r\n**https://drive.google.com/file/d/1ymLnGUebTPTRtGLPy539w0P0r3UZfcA_/view?usp=sharing**\r\nThe integer quantized .tflite file can be obtained from the link below.\r\n**https://drive.google.com/file/d/12S6GgRn4I2jl1c5omP023i4zgKACyDe5/view?usp=sharing**\r\n## 4. Failure details\r\nIn some places, a double Dequantize OP is generated in duplicate. Due to this issue, running the Python API **`interpreter.allocate_tensors()`** causes an error indicating that the structure of the model is flawed.\r\n```console\r\nRuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 782 (DEQUANTIZE) failed to prepare.\r\n```\r\n![Screenshot 2020-07-11 21:57:04](https://user-images.githubusercontent.com/33194443/87227676-9eedf000-c3d7-11ea-952c-5375a67b4f36.png)\r\nThe content of the error message and the INDEX number of the OP indicates that it is the second Dequantize OP that is causing this problem.\r\n![Screenshot 2020-07-12 00:38:44](https://user-images.githubusercontent.com/33194443/87227799-276c9080-c3d8-11ea-948a-ce7799cdc3a5.png)\r\n## 5. Related Issues\r\n- [tflite: Slicing isn't compatible with quantisation #29571](https://github.com/tensorflow/tensorflow/issues/29571)\r\n- [TFLite Interpreter, allocate_tensors() failed to prepare, not kTFLiteInt8/Uint8 #31053](https://github.com/tensorflow/tensorflow/issues/31053)", "comments": ["Same error for me, Have you find the way to fix this error @PINTO0309 ", "@Julius-ZCJ \r\nNo. I am waiting for the issue to be fixed.", "@liyunlu0618 Do you have any idea when we would expect to fix this issue? Thanks", "The problem seems to be resolved in 2.4.0-dev20200829. Inference performed correctly using Integer Quantized models.\r\n![Screenshot 2020-08-29 21:52:39](https://user-images.githubusercontent.com/33194443/91637339-b42fe400-ea42-11ea-8073-7c9ce1a0c147.png)\r\n\r\n![Screenshot 2020-08-29 21:52:49](https://user-images.githubusercontent.com/33194443/91637298-7d59ce00-ea42-11ea-8d3f-d8266009057e.png)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41308\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41308\">No</a>\n"]}, {"number": 41307, "title": "[DLPack] DLPack doesn't work with int32 on gpu", "body": "Reproduce code: https://colab.research.google.com/drive/1kwR_ZVutSot7yA7FqPUvCsKgXcirmNLh?pli=1#scrollTo=UI-LuV7EUick\r\n\r\nError message on my machine:\r\n```python\r\n2020-07-11 14:42:25.598880: E tensorflow/stream_executor/cuda/cuda_driver.cc:1037] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE: invalid argument; host dst: 0x1abfd2c0; GPU src: 0x1a027e80; size: 12=0xc\r\n2020-07-11 14:42:25.598954: F tensorflow/core/common_runtime/gpu/gpu_util.cc:291] GPU->CPU Memcpy failed\r\n```\r\n\r\nIt seems the int32 tensor's data pointer is on cpu instead of being on gpu, althought its device is \"gpu\". Previously as I know there's special case handling for int32 tensor(https://github.com/tensorflow/tensorflow/issues/34071), not sure whether this is related.\r\n\r\nAnd I just found I used constant_op.constant to test dlpacks, which means now it only test the case on cpu since tf.constant always place data on host memory\r\n\r\nDo you have any idea on this issue? @sanjoy @alextp ", "comments": ["Looks like `.device` returns the [device of the operation](https://github.com/tensorflow/tensorflow/blob/0d7e40f0dba759a54a8ee4e4c2a96e05937e14c2/tensorflow/python/framework/ops.py#L420) which is not correct for operations with `HostMemory` outputs, like [int32 identity](https://github.com/tensorflow/tensorflow/blob/0d7e40f0dba759a54a8ee4e4c2a96e05937e14c2/tensorflow/core/kernels/identity_op.cc#L162).\r\n\r\nWe can probably add a method that returns the true devices for a given `TFE_TensorHandle` (`Tensor`s know how to deallocate themselves so this information is in principle already present), but this is a high impact API change and we need to think this through.\r\n\r\nAlso, @VoVAllen , what should be the behavior of `to_dlpack` if it is passed a non-GPU tensor?", "CC @jaingaurav ", "For DLPack, there's a ctx field indicating which device this pointer belongs to. \r\n\r\nTwo things I think needed:\r\n- Be able to get the real device of the pointer\r\n  - I believe there should be such API in tensorflow now, because operator needs to know the pointer's location for further computation.\r\n- Be able to explicitly copy int32 tensor to gpu\r\n  - Currently our workaround is `cast it to uint32 -> copy to gpu ->cast it back to int32`\r\n\r\nI understand why int32 tensor are on cpu if using tf.constant, but why it needs exception in `tf.identity` also that still not copying it to gpu?", "> * I believe there should be such API in tensorflow now, because operator needs to know the pointer's location for further computation.\r\n\r\nI agree, I have filed a Google-internal bug about this.\r\n\r\n> but why it needs exception in `tf.identity` also that still not copying it to gpu?\r\n\r\nThe `int32` `Identity` kernel [explicitly places](https://github.com/tensorflow/tensorflow/blob/ac3456f3ad9ab8af38d933f823aa665358f7c4fe/tensorflow/core/kernels/identity_op.cc#L134) its input and output on the host, via a `HostMemory` annotation.", "Any updates on this issue?", "Internally we decided that this is a documentation bug.  The physical device of a `Tensor` can be accessed via the `backing_device` property, not the `device` property.", "Is there any API to get the `backing_device` I can use to fix the bug in dlpack for now? Currently I use https://github.com/tensorflow/tensorflow/blob/3ace29966b79711b848a76135248c743b2fdeace/tensorflow/c/eager/dlpack.cc#L112, which seems the op device instead of backing_devicce", "Just call BackingDeviceName instead of DeviceName", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41307\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41307\">No</a>\n"]}, {"number": 41306, "title": "ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., `tf.Variable(lambda : tf.truncated_normal([10, 40]))`) when building functions.", "body": "### Version ###\r\n\r\nOS Platform\r\nPython 3.7.6\r\ntensorflow 2.2.0\r\nkeras 2.3.0-tf\r\nmem 64247.95703125\r\ncpu 16\r\n\r\n### Try to reproduce this work ###\r\nhttps://www.kaggle.com/devang/transfer-learning-with-keras-and-mobilenet-v2\r\n\r\n### Issues ###\r\n## The error occurs when I try to define layers and compile model using: ##\r\n---------------------------------------------------------------------------------------------------------------------------------------------\r\nepochs = 100\r\nbatch_size = 150\r\ntestsplit = .2\r\ntargetx = 224\r\ntargety = 224\r\nlearning_rate = 0.0001\r\nclasses = 120\r\nseed = random.randint(1, 1000)\r\n\r\nshape=(targetx, targety, 3))\r\n\r\nx = base_model.output\r\nx = GlobalAveragePooling2D()(x)\r\n\r\nx = Dropout(rate = .2)(x)\r\nx = BatchNormalization()(x)\r\nx = Dense(1280, activation='relu', kernel_initializer=glorot_uniform(seed), bias_initializer='zeros')(x)\r\n\r\nx = Dropout(rate = .2)(x)\r\nx = BatchNormalization()(x)\r\npredictions = Dense(classes, activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros')(x)\r\n\r\nmodel = Model(inputs=base_model.input, outputs=predictions)\r\n\r\n---------------------------------------------------------------------------------------------------------------------------------------------\r\n### Error report ###\r\n\r\n---------------------------------------------------------------------------------------------------------------------------------------------\r\nValueError Traceback (most recent call last)\r\nin\r\n12 \"\"\"\r\n13\r\n---> 14 x = Dense(1280, activation='relu', kernel_initializer=glorot_uniform(seed), bias_initializer='zeros')(x)\r\n15 x = Dropout(rate = .2)(x)\r\n16 x = BatchNormalization()(x)\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py in symbolic_fn_wrapper(*args, **kwargs)\r\n73 if _SYMBOLIC_SCOPE.value:\r\n74 with get_graph().as_default():\r\n---> 75 return func(*args, **kwargs)\r\n76 else:\r\n77 return func(*args, **kwargs)\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/base_layer.py in call(self, inputs, **kwargs)\r\n461 'You can build it manually via: '\r\n462 'layer.build(batch_input_shape)')\r\n--> 463 self.build(unpack_singleton(input_shapes))\r\n464 self.built = True\r\n465\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/layers/core.py in build(self, input_shape)\r\n893 name='kernel',\r\n894 regularizer=self.kernel_regularizer,\r\n--> 895 constraint=self.kernel_constraint)\r\n896 if self.use_bias:\r\n897 self.bias = self.add_weight(shape=(self.units,),\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\r\n280 dtype=dtype,\r\n281 name=name,\r\n--> 282 constraint=constraint)\r\n283 if regularizer is not None:\r\n284 with K.name_scope('weight_regularizer'):\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py in variable(value, dtype, name, constraint)\r\n618 \"\"\"\r\n619 v = tf_keras_backend.variable(\r\n--> 620 value, dtype=dtype, name=name, constraint=constraint)\r\n621 if hasattr(value, 'tocoo'):\r\n622 v._keras_shape = value.tocoo().shape\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in variable(value, dtype, name, constraint)\r\n843 dtype=dtypes_module.as_dtype(dtype),\r\n844 name=name,\r\n--> 845 constraint=constraint)\r\n846 if isinstance(value, np.ndarray):\r\n847 v._keras_shape = value.shape\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in call(cls, *args, **kwargs)\r\n259 return cls._variable_v1_call(*args, **kwargs)\r\n260 elif cls is Variable:\r\n--> 261 return cls._variable_v2_call(*args, **kwargs)\r\n262 else:\r\n263 return super(VariableMetaclass, cls).call(*args, **kwargs)\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\r\n253 synchronization=synchronization,\r\n254 aggregation=aggregation,\r\n--> 255 shape=shape)\r\n256\r\n257 def call(cls, *args, **kwargs):\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in (**kws)\r\n234 shape=None):\r\n235 \"\"\"Call on Variable class. Useful to force the signature.\"\"\"\r\n--> 236 previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n237 for _, getter in ops.get_default_graph()._variable_creator_stack: # pylint: disable=protected-access\r\n238 previous_getter = _make_getter(getter, previous_getter)\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)\r\n2645 synchronization=synchronization,\r\n2646 aggregation=aggregation,\r\n-> 2647 shape=shape)\r\n2648\r\n2649\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in call(cls, *args, **kwargs)\r\n261 return cls._variable_v2_call(*args, **kwargs)\r\n262 else:\r\n--> 263 return super(VariableMetaclass, cls).call(*args, **kwargs)\r\n264\r\n265\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in init(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\r\n1432 aggregation=aggregation,\r\n1433 shape=shape,\r\n-> 1434 distribute_strategy=distribute_strategy)\r\n1435\r\n1436 def _init_from_args(self,\r\n\r\n/apps/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\r\n1515 if isinstance(initial_value, ops.Tensor) and hasattr(\r\n1516 initial_value, \"graph\") and initial_value.graph.building_function:\r\n-> 1517 raise ValueError(\"Tensor-typed variable initializers must either be \"\r\n1518 \"wrapped in an init_scope or callable \"\r\n1519 \"(e.g., `tf.Variable(lambda : \"\r\n\r\nValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., tf.Variable(lambda : tf.truncated_normal([10, 40]))) when building functions. Please file a feature request if this restriction inconveniences you.\r\n\r\n---------------------------------------------------------------------------------------------------------------------------------------------\r\nI had a few go on changing the 'seed' to the following:\r\n\r\n- [1, seed]\r\n- tf.constant(np.random.rand(2, 2))\r\n- tf.keras.Variable(lambda : tf.truncated_normal([1, seed]))\r\n\r\nHowever, I still can't manage to convert 'seed' to a tensor.\r\n\r\nCan anyone help me please?\r\n\r\nAny suggestions/feedback will be much appreciated!", "comments": ["@Isabellaleesln \r\nI ran the code shared, it seems incomplete.Please provide with code such that it can replicate the issue faced, [gist of the code shared](https://colab.research.google.com/gist/Saduf2019/134b453e41309630ac0f0052d330fe8d/untitled272.ipynb).\r\n\r\nWith respect to the error faced please refer to these issues and let us know if it helps:\r\n[link](https://stackoverflow.com/questions/59229898/tensor-typed-variable-initializers-must-either-be-wrapped-in-an-init-scope-or-ca) [link1](https://github.com/shamangary/LOUPE_Keras/issues/1) ", "Hi @Saduf2019 Thank you for response =)\r\nPlease find the full code here: https://www.kaggle.com/devang/transfer-learning-with-keras-and-mobilenet-v2. \r\n\r\nRegards,\r\n", "Hi @Saduf2019  I have looked at your links. [link1](https://stackoverflow.com/questions/59229898/tensor-typed-variable-initializers-must-either-be-wrapped-in-an-init-scope-or-ca) seems not the same as mine. \r\n\r\nYou can find the remaining code of your first [link](https://colab.research.google.com/gist/Saduf2019/134b453e41309630ac0f0052d330fe8d/untitled272.ipynb) at [Here](https://www.kaggle.com/devang/transfer-learning-with-keras-and-mobilenet-v2)\r\n\r\nThank you for looking at this issue!", "@Isabellaleesln\r\nPlease share simple stand alone indented code to replicate the issue or please share a colab gist with the error faced for us to analyse.", "@Saduf2019 \r\nPlease find the _**full mobilenet_v2 model code**_ and **_related weights_** in the attached below. Thanks.\r\n\r\n[mobilenet_v2_model_weights.zip](https://github.com/tensorflow/tensorflow/files/4915590/mobilenet_v2_model_weights.zip)\r\n\r\n", "@Isabellaleesln Were you able to solve this problem?\r\nI think the problem is that you were not setting the initializers in the right fashion. Please take a look at this [doc](https://keras.io/api/layers/initializers/) on how to intialize variables\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41306\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41306\">No</a>\n"]}, {"number": 41305, "title": "[RNN] Converting network with LSTM layer to int8 sefaults whole python", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu or Google Colab\r\n- TensorFlow installed from (source or binary): pip install tf-nightly\r\n- TensorFlow version (or github SHA if from source): 2.4.0-dev20200710\r\n\r\n\r\nTrying out conversion, I managed to reproducibly segfault. I am not sure whether I am supplying wrong inputs to representative_dataset but in any case, the code should not be segfaulting and at max throwing python errors. The issue is reproducible also on colab.\r\n\r\n```\r\n!pip install tf-nightly\r\n```\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.__version__\r\n```\r\n\r\n```\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\nx_train = x_train.astype(np.float32)\r\nx_test = x_test.astype(np.float32)\r\n\r\n_EPOCHS = 1\r\n_TRAINING_DATA_COUNT = 1000\r\nx_train = x_train[:_TRAINING_DATA_COUNT]\r\ny_train = y_train[:_TRAINING_DATA_COUNT]\r\n\r\nmodel.fit(x_train, y_train, epochs=_EPOCHS)\r\nmodel.evaluate(x_test, y_test, verbose=0)\r\n```\r\n> [1.8708466291427612, 0.531000018119812]\r\n\r\n```\r\nrun_model = tf.function(lambda x: model(x))\r\n# This is important, let's fix the input size.\r\nBATCH_SIZE = 1\r\nSTEPS = 28\r\nINPUT_SIZE = 28\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\r\n\r\n# model directory.\r\nMODEL_DIR = \"keras_lstm\"\r\nmodel.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f69aa50b730> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: <module '__main__'> is a built-in module\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f69aa50b730> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: <module '__main__'> is a built-in module\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function <lambda> at 0x7f69aa50b730> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: <module '__main__'> is a built-in module\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nINFO:tensorflow:Assets written to: keras_lstm/assets\r\nINFO:tensorflow:Assets written to: keras_lstm/assets\r\n\r\n```\r\ndef representative_dataset_gen():\r\n    return [[x_train[:1]]] # Not sure why I need to wrap this in two lists (otherwise converter complains)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\nconverter.experimental_new_converter = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        \r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n```\r\n\r\n**Running this last cell now segfaults python interpreter / crashes jupyter kernel**\r\n```\r\n# This segfaults\r\ntflite_model = converter.convert()\r\n```\r\n\r\n", "comments": ["fused unidirectional_lstm does not support int8 yet (WIP), sorry about that.\r\n\r\nif you use float, it should work.", "@ppershing \r\n\r\nCan you try with float instead of int8 and see if that helps to resolve the issue. Thanks!\r\n", "Hmm, I can try but\r\n1) will it even use representative_dataset if it is going to for float conversion?\r\n2) in any case, shouldn't segfault be distinct from \"not supported\"? At minimum I would expect a clean rejection instead of crashing whole python interpreter\r\n3) I guess I am out of luck because I need int8 for Coral", "fused unidirectional lstm does not support integer-only just yet.\r\n\r\nJian is working on it.", "I can reproduce the issue. [Here](https://colab.research.google.com/gist/jvishnuvardhan/09b4600f647eff8bea6d9963772af53f/untitled.ipynb) is the gist for our reference. Thanks!\r\n\r\n@ppershing In future, please try to share full code so that it is easy to reproduce and find root-cause of the issue. Thanks!", "@ppershing Is this still an issue ? Could you please try on the latest stable version of TF 2.6.0 & please refer to the[ link](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) .Please  let us know if it helps ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41305\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41305\">No</a>\n"]}, {"number": 41304, "title": "tf.py_function fails on the tutorial code when running tensorflow-gpu 2.2 on Windows 10", "body": "**System information**\r\n- OS Platform: Windows 10\r\n- TensorFlow installed: using pip\r\n- TensorFlow version: tensorflow gpu 2.2\r\n- Python version: 3.8.3\r\n- CUDA/cuDNN version: CUDA 10.1\r\n- GPU model and memory: GeForce GTX 1080, 8GB\r\n\r\n**Describe the current behavior**\r\nThe serialized result without tf.py_function was normal.\r\nBut there were exception errors from eager_py_func when trying to get the serialized result with tf.py_function.\r\n**Describe the expected behavior**\r\nThe serialized result without/with tf.py_fuction should be the same.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe issue can be reproduced by following code (from tensoflow tutorial, \"TFRecord and tf.Example\", https://www.tensorflow.org/tutorials/load_data/tfrecord). \r\nThe code is running well on  Colab on the tutorial website but there are exception errors when running on Windows 10 with cmd or pycharm.\r\n Here is the code:\r\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\n\r\nimport tensorflow as tf\r\n\r\ndef _bytes_feature(value):\r\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n  if isinstance(value, type(tf.constant(0))):\r\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\r\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\ndef _float_feature(value):\r\n  \"\"\"Returns a float_list from a float / double.\"\"\"\r\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n\r\ndef _int64_feature(value):\r\n  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\r\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\ndef serialize_example(feature0, feature1, feature2, feature3):\r\n  feature = {\r\n      'feature0': _int64_feature(feature0),\r\n      'feature1': _int64_feature(feature1),\r\n      'feature2': _bytes_feature(feature2),\r\n      'feature3': _float_feature(feature3),\r\n  }\r\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\r\n  return example_proto.SerializeToString()\r\n\r\ndef tf_serialize_example(f0,f1,f2,f3):\r\n  tf_string = tf.py_function(\r\n    serialize_example,\r\n    (f0,f1,f2,f3),\r\n    tf.string)\r\n  return tf.reshape(tf_string, ())\r\n\r\nresult_without_py_function = serialize_example(False, 4, b'goat', 0.9876)\r\nprint(\"result_without_py_function =\",result_without_py_function)\r\n\r\nresult_with_py_function = tf_serialize_example(False, 4, b'goat', 0.9876)\r\nprint(\"result_with_py_function=\",result_with_py_function)\r\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\n\r\nI only wrote extra lines to print out results from serialize_example() and tf_serialize_example() for comparison.\r\nRunning the above code on windows cmd or pycharm will get the exception errors when executing:\r\nresult_with_py_function = tf_serialize_example(False, 4, b'goat', 0.9876)\r\n\r\nAll the output including error messages:\r\n\r\n020-07-10 20:36:22.730441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-10 20:36:25.159732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-07-10 20:36:25.298922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.847GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-07-10 20:36:25.299515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.847GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-07-10 20:36:25.299855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-10 20:36:25.330094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-07-10 20:36:25.345517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-10 20:36:25.375238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-10 20:36:25.392428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-10 20:36:25.412082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-07-10 20:36:25.430892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-10 20:36:25.432276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-07-10 20:36:25.432747: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-07-10 20:36:25.439470: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18e19723790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-10 20:36:25.439669: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-10 20:36:25.594336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.847GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-07-10 20:36:25.594839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.847GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-07-10 20:36:25.595149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-10 20:36:25.595311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-07-10 20:36:25.595476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-10 20:36:25.595637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-10 20:36:25.595793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-10 20:36:25.595953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-07-10 20:36:25.596121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-10 20:36:25.596974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-07-10 20:36:26.501843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-10 20:36:26.502016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 \r\n2020-07-10 20:36:26.502118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N N \r\n2020-07-10 20:36:26.502217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   N N \r\n2020-07-10 20:36:26.503174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6280 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-07-10 20:36:26.504471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 6280 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2020-07-10 20:36:26.507161: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18e8a13a5e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-10 20:36:26.507331: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n2020-07-10 20:36:26.507460: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080, Compute Capability 6.1\r\nresult_without_py_function = b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?'\r\n\r\n2020-07-10 20:36:26.534597: W tensorflow/core/framework/op_kernel.cc:1741] Invalid argument: TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 43, in eager_py_func\r\n    _result = pywrap_tfe.TFE_Py_FastPathExecute(\r\n\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 241, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 130, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 309, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py\", line 20, in serialize_example\r\n    'feature0': _int64_feature(feature0),\r\n\r\n  File \"F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py\", line 15, in _int64_feature\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 542, in init\r\n    copy.extend(field_value)\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\", line 282, in extend\r\n    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\", line 282, in <listcomp>\r\n    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\", line 171, in CheckValue\r\n    raise TypeError(message)\r\n\r\nTypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 43, in eager_py_func\r\n    _result = pywrap_tfe.TFE_Py_FastPathExecute(\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py\", line 40, in <module>\r\n    result_with_py_function = tf_serialize_example(False, 4, b'goat', 0.9876)\r\n  File \"F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py\", line 30, in tf_serialize_example\r\n    tf_string = tf.py_function(\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 454, in eager_py_func\r\n    return _internal_py_func(\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 336, in _internal_py_func\r\n    result = gen_script_ops.eager_py_func(\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 50, in eager_py_func\r\n    return eager_py_func_eager_fallback(\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 99, in eager_py_func_eager_fallback\r\n    _result = _execute.execute(b\"EagerPyFunc\", len(Tout), inputs=_inputs_flat,\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 43, in eager_py_func\r\n    _result = pywrap_tfe.TFE_Py_FastPathExecute(\r\n\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 241, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 130, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 309, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py\", line 20, in serialize_example\r\n    'feature0': _int64_feature(feature0),\r\n\r\n  File \"F:/python_projects/deep_learning/workspace/programs/dlcls/tf_py_function_test.py\", line 15, in _int64_feature\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 542, in init\r\n    copy.extend(field_value)\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\", line 282, in extend\r\n    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\", line 282, in <listcomp>\r\n    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\r\n\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\", line 171, in CheckValue\r\n    raise TypeError(message)\r\n\r\nTypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)\r\n\r\n [Op:EagerPyFunc]\r\n\r\nProcess finished with exit code 1\r\n\r\n", "comments": ["@tom970  Could you please try running this code on virtual environment and let us know if you are facing the same issue.Thanks!", "Hi Saikumarchalla,\r\n\r\nThe code is running well on Colab on the tutorial website but there are the exception errors when running on Windows 10 with cmd or pycharm. (Assuming that the virtual environment you meant is Colab which I can only test on).\r\n\r\nIt seems that the problem only exists in the tensorflow-gpu 2.2 for Windows.\r\n\r\nThanks!\r\n", "Hi Saikumarchalla,\n\nThe code is running well on Colab on the tutorial website but there are the\nexception errors when running on Windows 10 with cmd or pycharm. (Assuming\nthat the virtual environment you meant is Colab which I can only test on).\n\nIt seems that the problem only exists in the tensorflow-gpu 2.2 for Windows.\n\nThanks!\n\nOn Mon, Jul 13, 2020 at 5:12 AM saikumarchalla <notifications@github.com>\nwrote:\n\n> @tom970 <https://github.com/tom970> Could you please try running this\n> code on virtual environment and let us know if you are facing the same\n> issue.Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41304#issuecomment-657523818>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQH3KMP2IEL34PEVVTZSLALR3L2ZNANCNFSM4OXE3UOQ>\n> .\n>\n", "@tom970 Can you please check with `tf-nightly` and let us know whether the issue persists with the newer TF version? Is this an issue with `tensorflow-gpu 2.2` or with `cpu` also? Thanks!", "Hi\r\nI checked tf-nightly-gpu (2.4.0-dev20200721) and tencorflow-cpu\u00a02.2.\u00a0 The same error came out from both of them.\u00a0\r\nThanks,", "Hi ,\n\nI checked tf-nightly-gpu (2.4.0-dev20200721) and tencorflow-cpu 2.2.  The\nsame error came out from both of them.\n\nThanks,\n\n\nOn Mon, Jul 20, 2020 at 2:31 PM Vishnuvardhan Janapati <\nnotifications@github.com> wrote:\n\n> @tom970 <https://github.com/tom970> Can you please check with tf-nightly\n> and let us know whether the issue persists with the newer TF version? Is\n> this an issue with tensorflow-gpu 2.2 or with cpu also? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41304#issuecomment-661344010>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQH3KML4O4N2ZD2E6FUHBSDR4SZR5ANCNFSM4OXE3UOQ>\n> .\n>\n", "@tom970 I ran your code in my Windows10 without any issues.. Output is as follows. Please note that mine is `tensorflow-cpu` (don't have gpu). Thanks!\r\n\r\n```\r\n2020-07-22 13:06:15.442389: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-07-22 13:06:15.452911: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2364545cca0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-22 13:06:15.458004: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nbytes_list {\r\n  value: \"test_string\"\r\n}\r\n\r\nbytes_list {\r\n  value: \"test_bytes\"\r\n}\r\n\r\nfloat_list {\r\n  value: 2.7182817459106445\r\n}\r\n\r\nint64_list {\r\n  value: 1\r\n}\r\n\r\nint64_list {\r\n  value: 1\r\n}\r\n\r\ntf.Tensor(False, shape=(), dtype=bool)\r\ntf.Tensor(0, shape=(), dtype=int32)\r\ntf.Tensor(b'cat', shape=(), dtype=string)\r\ntf.Tensor(-0.3667049183424323, shape=(), dtype=float64)\r\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xbf\\xc0\\xbb\\xbe'>\r\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xaa\\x81x>'>\r\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04a\\x91c\\xbf'>\r\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03dog\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04A\\x81K\\xbf'>\r\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nS\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x03\\n\\x15\\n\\x08feature2\\x12\\t\\n\\x07\\n\\x05horse\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xb8\\xd5\\x80\\xbf'>\r\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nS\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x03\\n\\x15\\n\\x08feature2\\x12\\t\\n\\x07\\n\\x05horse\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x0b\\x10R\\xbf'>\r\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xf4\\x0eV?'>\r\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04:j\\x12\\xbf'>\r\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xb1\\x93@?'>\r\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x041\\x12\\x93\\xbe'>\r\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.3667049>}\r\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.24268213>}\r\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.88893706>}\r\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'dog'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.794941>}\r\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'horse'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-1.0065222>}\r\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'horse'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.8205573>}\r\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'goat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.83616567>}\r\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.5719334>}\r\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'goat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.7522536>}\r\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.28724816>}\r\nfeatures {\r\n  feature {\r\n    key: \"feature0\"\r\n    value {\r\n      int64_list {\r\n        value: 0\r\n      }\r\n    }\r\n  }\r\n  feature {\r\n    key: \"feature1\"\r\n    value {\r\n      int64_list {\r\n        value: 0\r\n      }\r\n    }\r\n  }\r\n  feature {\r\n    key: \"feature2\"\r\n    value {\r\n      bytes_list {\r\n        value: \"cat\"\r\n      }\r\n    }\r\n  }\r\n  feature {\r\n    key: \"feature3\"\r\n    value {\r\n      float_list {\r\n        value: -0.36670491099357605\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nfeatures {\r\n  feature {\r\n    key: \"depth\"\r\n    value {\r\n      int64_list {\r\n        value: 3\r\n      }\r\n    }\r\n  }\r\n  feature {\r\n    key: \"height\"\r\n    value {\r\n      int64_list {\r\n        value: 213\r\n      }\r\n...\r\n\r\n```", "It seems that you didn't run the same code as mine since I have two printouts, \"result_without_py_function\" and \"rusult_with_py_function\" for comparison,  but I didn't find them in your outputs. Maybe I missed something.\r\n\r\nBy the way, what version of tensorflow-cpu did you run? I tested on tensoflow-cpu-2.2 and still have the issue. Here is the output:\r\n\r\n2020-07-22 15:32:33.724032: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-07-22 15:32:33.729841: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x275ce3b9790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-22 15:32:33.730128: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nresult_without_py_function = b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?'\r\n2020-07-22 15:32:33.815474: W tensorflow/core/framework/op_kernel.cc:1741] Invalid argument: TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)\r\nTraceback (most recent call last):\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 42, in eager_py_func\r\n    _result = pywrap_tfe.TFE_Py_FastPathExecute(\r\n\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 241, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 130, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 309, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line 20, in serialize_example\r\n    'feature0': _int64_feature(feature0),\r\n\r\n  File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line 15, in _int64_feature\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 542, in init\r\n    copy.extend(field_value)\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\", line 282, in extend\r\n    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\", line 282, in <listcomp>\r\n    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\", line 171, in CheckValue\r\n    raise TypeError(message)\r\n\r\nTypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 42, in eager_py_func\r\n    _result = pywrap_tfe.TFE_Py_FastPathExecute(\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line 40, in <module>\r\n    result_with_py_function = tf_serialize_example(False, 4, b'goat', 0.9876)\r\n  File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line 30, in tf_serialize_example\r\n    tf_string = tf.py_function(\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 454, in eager_py_func\r\n    return _internal_py_func(\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 336, in _internal_py_func\r\n    result = gen_script_ops.eager_py_func(\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 49, in eager_py_func\r\n    return eager_py_func_eager_fallback(\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 98, in eager_py_func_eager_fallback\r\n    _result = _execute.execute(b\"EagerPyFunc\", len(Tout), inputs=_inputs_flat,\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)\r\nTraceback (most recent call last):\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\", line 42, in eager_py_func\r\n    _result = pywrap_tfe.TFE_Py_FastPathExecute(\r\n\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 241, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 130, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 309, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line 20, in serialize_example\r\n    'feature0': _int64_feature(feature0),\r\n\r\n  File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line 15, in _int64_feature\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 542, in init\r\n    copy.extend(field_value)\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\", line 282, in extend\r\n    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\", line 282, in <listcomp>\r\n    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\r\n\r\n  File \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\", line 171, in CheckValue\r\n    raise TypeError(message)\r\n\r\nTypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of: (<class 'int'>,)\r\n\r\n [Op:EagerPyFunc]\r\n\r\nProcess finished with exit code 1", "It seems that you didn't run the same code as mine since I have two\nprintouts, \"result_without_py_function\" and \"rusult_with_py_function\" for\ncomparison, but I didn't find them in your outputs. Maybe I missed\nsomething.\n\nBy the way, what version of tensorflow-cpu did you run? I tested on\ntensoflow-cpu-2.2 and still have the issue. Here is the output:\n\n2020-07-22 15:32:33.724032: I\ntensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports\ninstructions that this TensorFlow binary was not compiled to use: AVX2\n2020-07-22 15:32:33.729841: I\ntensorflow/compiler/xla/service/service.cc:168] XLA service 0x275ce3b9790\ninitialized for platform Host (this does not guarantee that XLA will be\nused). Devices:\n2020-07-22 15:32:33.730128: I\ntensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0):\nHost, Default Version\nresult_without_py_function =\nb'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?'\n2020-07-22 15:32:33.815474: W tensorflow/core/framework/op_kernel.cc:1741]\nInvalid argument: TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False>\nhas type <class 'tensorflow.python.framework.ops.EagerTensor'>, but\nexpected one of: (<class 'int'>,)\nTraceback (most recent call last):\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\",\nline 42, in eager_py_func\n_result = pywrap_tfe.TFE_Py_FastPathExecute(\n\ntensorflow.python.eager.core._FallbackException: This function does not\nhandle the case of the path where all inputs are not already EagerTensors.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\nline 241, in call\nreturn func(device, token, args)\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\nline 130, in call\nret = self._func(*args)\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\",\nline 309, in wrapper\nreturn func(*args, **kwargs)\n\nFile \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n20, in serialize_example\n'feature0': _int64_feature(feature0),\n\nFile \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n15, in _int64_feature\nreturn tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\",\nline 542, in init\ncopy.extend(field_value)\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\",\nline 282, in extend\nnew_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\",\nline 282, in\nnew_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\",\nline 171, in CheckValue\nraise TypeError(message)\n\nTypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class\n'tensorflow.python.framework.ops.EagerTensor'>, but expected one of:\n(<class 'int'>,)\n\nTraceback (most recent call last):\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\",\nline 42, in eager_py_func\n_result = pywrap_tfe.TFE_Py_FastPathExecute(\ntensorflow.python.eager.core._FallbackException: This function does not\nhandle the case of the path where all inputs are not already EagerTensors.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\nFile \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n40, in\nresult_with_py_function = tf_serialize_example(False, 4, b'goat', 0.9876)\nFile \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n30, in tf_serialize_example\ntf_string = tf.py_function(\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\nline 454, in eager_py_func\nreturn _internal_py_func(\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\nline 336, in _internal_py_func\nresult = gen_script_ops.eager_py_func(\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\",\nline 49, in eager_py_func\nreturn eager_py_func_eager_fallback(\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\",\nline 98, in eager_py_func_eager_fallback\n_result = _execute.execute(b\"EagerPyFunc\", len(Tout), inputs=_inputs_flat,\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\",\nline 59, in quick_execute\ntensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\ntensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError:\n<tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class\n'tensorflow.python.framework.ops.EagerTensor'>, but expected one of:\n(<class 'int'>,)\nTraceback (most recent call last):\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\",\nline 42, in eager_py_func\n_result = pywrap_tfe.TFE_Py_FastPathExecute(\n\ntensorflow.python.eager.core._FallbackException: This function does not\nhandle the case of the path where all inputs are not already EagerTensors.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\nline 241, in call\nreturn func(device, token, args)\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\nline 130, in call\nret = self._func(*args)\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\",\nline 309, in wrapper\nreturn func(*args, **kwargs)\n\nFile \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n20, in serialize_example\n'feature0': _int64_feature(feature0),\n\nFile \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n15, in _int64_feature\nreturn tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\",\nline 542, in init\ncopy.extend(field_value)\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\",\nline 282, in extend\nnew_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\",\nline 282, in\nnew_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]\n\nFile\n\"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\",\nline 171, in CheckValue\nraise TypeError(message)\n\nTypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class\n'tensorflow.python.framework.ops.EagerTensor'>, but expected one of:\n(<class 'int'>,)\n\n[Op:EagerPyFunc]\n\nProcess finished with exit code 1\n\nOn Wed, Jul 22, 2020 at 1:10 PM Vishnu <notifications@github.com> wrote:\n\n> @tom970 <https://github.com/tom970> I ran your code in my Windows10\n> without any issues.. Output is as follows. Please note that mine is\n> tensorflow-cpu (don't have gpu). Thanks!\n>\n> 2020-07-22 13:06:15.442389: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n> 2020-07-22 13:06:15.452911: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2364545cca0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n> 2020-07-22 13:06:15.458004: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n> bytes_list {\n>   value: \"test_string\"\n> }\n>\n> bytes_list {\n>   value: \"test_bytes\"\n> }\n>\n> float_list {\n>   value: 2.7182817459106445\n> }\n>\n> int64_list {\n>   value: 1\n> }\n>\n> int64_list {\n>   value: 1\n> }\n>\n> tf.Tensor(False, shape=(), dtype=bool)\n> tf.Tensor(0, shape=(), dtype=int32)\n> tf.Tensor(b'cat', shape=(), dtype=string)\n> tf.Tensor(-0.3667049183424323, shape=(), dtype=float64)\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xbf\\xc0\\xbb\\xbe'>\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xaa\\x81x>'>\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04a\\x91c\\xbf'>\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03dog\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04A\\x81K\\xbf'>\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nS\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x03\\n\\x15\\n\\x08feature2\\x12\\t\\n\\x07\\n\\x05horse\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xb8\\xd5\\x80\\xbf'>\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nS\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x03\\n\\x15\\n\\x08feature2\\x12\\t\\n\\x07\\n\\x05horse\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x0b\\x10R\\xbf'>\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xf4\\x0eV?'>\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04:j\\x12\\xbf'>\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xb1\\x93@?'>\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x041\\x12\\x93\\xbe'>\n> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.3667049>}\n> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.24268213>}\n> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.88893706>}\n> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'dog'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.794941>}\n> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'horse'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-1.0065222>}\n> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'horse'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.8205573>}\n> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'goat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.83616567>}\n> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.5719334>}\n> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'goat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.7522536>}\n> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.28724816>}\n> features {\n>   feature {\n>     key: \"feature0\"\n>     value {\n>       int64_list {\n>         value: 0\n>       }\n>     }\n>   }\n>   feature {\n>     key: \"feature1\"\n>     value {\n>       int64_list {\n>         value: 0\n>       }\n>     }\n>   }\n>   feature {\n>     key: \"feature2\"\n>     value {\n>       bytes_list {\n>         value: \"cat\"\n>       }\n>     }\n>   }\n>   feature {\n>     key: \"feature3\"\n>     value {\n>       float_list {\n>         value: -0.36670491099357605\n>       }\n>     }\n>   }\n> }\n>\n> features {\n>   feature {\n>     key: \"depth\"\n>     value {\n>       int64_list {\n>         value: 3\n>       }\n>     }\n>   }\n>   feature {\n>     key: \"height\"\n>     value {\n>       int64_list {\n>         value: 213\n>       }\n> ...\n>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41304#issuecomment-662670928>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQH3KMI5QOOLCYLEPDDTSR3R45BUFANCNFSM4OXE3UOQ>\n> .\n>\n", "I just fresh installed the tensorflow-cpu 2.2.0 into another PC with windows 10 and tested the code. The same error came out. ", "I just fresh installed the tensorflow-cpu 2.2.0 into another PC with\nwindows 10 and tested the code. The same error came out.\n\nOn Wed, Jul 22, 2020 at 3:51 PM Tom Huang <tomhuang970@gmail.com> wrote:\n\n> It seems that you didn't run the same code as mine since I have two\n> printouts, \"result_without_py_function\" and \"rusult_with_py_function\" for\n> comparison, but I didn't find them in your outputs. Maybe I missed\n> something.\n>\n> By the way, what version of tensorflow-cpu did you run? I tested on\n> tensoflow-cpu-2.2 and still have the issue. Here is the output:\n>\n> 2020-07-22 15:32:33.724032: I\n> tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports\n> instructions that this TensorFlow binary was not compiled to use: AVX2\n> 2020-07-22 15:32:33.729841: I\n> tensorflow/compiler/xla/service/service.cc:168] XLA service 0x275ce3b9790\n> initialized for platform Host (this does not guarantee that XLA will be\n> used). Devices:\n> 2020-07-22 15:32:33.730128: I\n> tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0):\n> Host, Default Version\n> result_without_py_function =\n> b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?'\n> 2020-07-22 15:32:33.815474: W tensorflow/core/framework/op_kernel.cc:1741]\n> Invalid argument: TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False>\n> has type <class 'tensorflow.python.framework.ops.EagerTensor'>, but\n> expected one of: (<class 'int'>,)\n> Traceback (most recent call last):\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\",\n> line 42, in eager_py_func\n> _result = pywrap_tfe.TFE_Py_FastPathExecute(\n>\n> tensorflow.python.eager.core._FallbackException: This function does not\n> handle the case of the path where all inputs are not already EagerTensors.\n>\n> During handling of the above exception, another exception occurred:\n>\n> Traceback (most recent call last):\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\n> line 241, in call\n> return func(device, token, args)\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\n> line 130, in call\n> ret = self._func(*args)\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\",\n> line 309, in wrapper\n> return func(*args, **kwargs)\n>\n> File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n> 20, in serialize_example\n> 'feature0': _int64_feature(feature0),\n>\n> File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n> 15, in _int64_feature\n> return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\",\n> line 542, in init\n> copy.extend(field_value)\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\",\n> line 282, in extend\n> new_values = [self._type_checker.CheckValue(elem) for elem in\n> elem_seq_iter]\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\",\n> line 282, in\n> new_values = [self._type_checker.CheckValue(elem) for elem in\n> elem_seq_iter]\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\",\n> line 171, in CheckValue\n> raise TypeError(message)\n>\n> TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class\n> 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of:\n> (<class 'int'>,)\n>\n> Traceback (most recent call last):\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\",\n> line 42, in eager_py_func\n> _result = pywrap_tfe.TFE_Py_FastPathExecute(\n> tensorflow.python.eager.core._FallbackException: This function does not\n> handle the case of the path where all inputs are not already EagerTensors.\n>\n> During handling of the above exception, another exception occurred:\n>\n> Traceback (most recent call last):\n> File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n> 40, in\n> result_with_py_function = tf_serialize_example(False, 4, b'goat', 0.9876)\n> File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n> 30, in tf_serialize_example\n> tf_string = tf.py_function(\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\n> line 454, in eager_py_func\n> return _internal_py_func(\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\n> line 336, in _internal_py_func\n> result = gen_script_ops.eager_py_func(\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\",\n> line 49, in eager_py_func\n> return eager_py_func_eager_fallback(\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\",\n> line 98, in eager_py_func_eager_fallback\n> _result = _execute.execute(b\"EagerPyFunc\", len(Tout), inputs=_inputs_flat,\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\",\n> line 59, in quick_execute\n> tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError:\n> <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class\n> 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of:\n> (<class 'int'>,)\n> Traceback (most recent call last):\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\",\n> line 42, in eager_py_func\n> _result = pywrap_tfe.TFE_Py_FastPathExecute(\n>\n> tensorflow.python.eager.core._FallbackException: This function does not\n> handle the case of the path where all inputs are not already EagerTensors.\n>\n> During handling of the above exception, another exception occurred:\n>\n> Traceback (most recent call last):\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\n> line 241, in call\n> return func(device, token, args)\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\",\n> line 130, in call\n> ret = self._func(*args)\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\",\n> line 309, in wrapper\n> return func(*args, **kwargs)\n>\n> File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n> 20, in serialize_example\n> 'feature0': _int64_feature(feature0),\n>\n> File \"F:/python_projects/test_tesorflow_cpu/tf_py_function_test.py\", line\n> 15, in _int64_feature\n> return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\",\n> line 542, in init\n> copy.extend(field_value)\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\",\n> line 282, in extend\n> new_values = [self._type_checker.CheckValue(elem) for elem in\n> elem_seq_iter]\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\",\n> line 282, in\n> new_values = [self._type_checker.CheckValue(elem) for elem in\n> elem_seq_iter]\n>\n> File\n> \"F:\\python_projects\\test_tesorflow_cpu\\venv\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\",\n> line 171, in CheckValue\n> raise TypeError(message)\n>\n> TypeError: <tf.Tensor: shape=(), dtype=bool, numpy=False> has type <class\n> 'tensorflow.python.framework.ops.EagerTensor'>, but expected one of:\n> (<class 'int'>,)\n>\n> [Op:EagerPyFunc]\n>\n> Process finished with exit code 1\n>\n> On Wed, Jul 22, 2020 at 1:10 PM Vishnu <notifications@github.com> wrote:\n>\n>> @tom970 <https://github.com/tom970> I ran your code in my Windows10\n>> without any issues.. Output is as follows. Please note that mine is\n>> tensorflow-cpu (don't have gpu). Thanks!\n>>\n>> 2020-07-22 13:06:15.442389: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n>> 2020-07-22 13:06:15.452911: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2364545cca0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n>> 2020-07-22 13:06:15.458004: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n>> bytes_list {\n>>   value: \"test_string\"\n>> }\n>>\n>> bytes_list {\n>>   value: \"test_bytes\"\n>> }\n>>\n>> float_list {\n>>   value: 2.7182817459106445\n>> }\n>>\n>> int64_list {\n>>   value: 1\n>> }\n>>\n>> int64_list {\n>>   value: 1\n>> }\n>>\n>> tf.Tensor(False, shape=(), dtype=bool)\n>> tf.Tensor(0, shape=(), dtype=int32)\n>> tf.Tensor(b'cat', shape=(), dtype=string)\n>> tf.Tensor(-0.3667049183424323, shape=(), dtype=float64)\n>> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xbf\\xc0\\xbb\\xbe'>\n>> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xaa\\x81x>'>\n>> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04a\\x91c\\xbf'>\n>> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03dog\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04A\\x81K\\xbf'>\n>> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nS\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x03\\n\\x15\\n\\x08feature2\\x12\\t\\n\\x07\\n\\x05horse\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xb8\\xd5\\x80\\xbf'>\n>> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nS\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x03\\n\\x15\\n\\x08feature2\\x12\\t\\n\\x07\\n\\x05horse\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x0b\\x10R\\xbf'>\n>> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xf4\\x0eV?'>\n>> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04:j\\x12\\xbf'>\n>> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xb1\\x93@?'>\n>> <tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x041\\x12\\x93\\xbe'>\n>> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.3667049>}\n>> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.24268213>}\n>> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.88893706>}\n>> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'dog'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.794941>}\n>> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'horse'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-1.0065222>}\n>> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'horse'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.8205573>}\n>> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'goat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.83616567>}\n>> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.5719334>}\n>> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'goat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.7522536>}\n>> {'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.28724816>}\n>> features {\n>>   feature {\n>>     key: \"feature0\"\n>>     value {\n>>       int64_list {\n>>         value: 0\n>>       }\n>>     }\n>>   }\n>>   feature {\n>>     key: \"feature1\"\n>>     value {\n>>       int64_list {\n>>         value: 0\n>>       }\n>>     }\n>>   }\n>>   feature {\n>>     key: \"feature2\"\n>>     value {\n>>       bytes_list {\n>>         value: \"cat\"\n>>       }\n>>     }\n>>   }\n>>   feature {\n>>     key: \"feature3\"\n>>     value {\n>>       float_list {\n>>         value: -0.36670491099357605\n>>       }\n>>     }\n>>   }\n>> }\n>>\n>> features {\n>>   feature {\n>>     key: \"depth\"\n>>     value {\n>>       int64_list {\n>>         value: 3\n>>       }\n>>     }\n>>   }\n>>   feature {\n>>     key: \"height\"\n>>     value {\n>>       int64_list {\n>>         value: 213\n>>       }\n>> ...\n>>\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/41304#issuecomment-662670928>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AQH3KMI5QOOLCYLEPDDTSR3R45BUFANCNFSM4OXE3UOQ>\n>> .\n>>\n>\n", "I just ran one more time in my windows10 and I ran both printouts. Please check the output below.\r\n\r\n```\r\n2020-07-23 10:33:37.248345: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-07-23 10:33:37.281316: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e89bc25120 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-23 10:33:37.286097: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nresult_without_py_function = b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?'\r\nresult_with_py_function= tf.Tensor(b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?', shape=(), dtype=string)\r\n```", "What version of tensorflow-cpu are you using?\r\nI fresh installed python and tensorflow-cpu in another machine and the issue still there. \r\nHere is my environment:\r\nOS: Windows10 64 bit\r\nPython: 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]\r\nTensorflow-cpu: 2.2.0\r\n\r\n", "What version of tensorflow-cpu are you using?\nI fresh installed python and tensorflow-cpu in another machine and the\nissue still there.\nHere is my environment:\nOS: Windows10 64 bit\nPython: 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64\nbit (AMD64)]\nTensorflow-cpu: 2.2.0\n\nOn Thu, Jul 23, 2020 at 10:37 AM Vishnu <notifications@github.com> wrote:\n\n> I just ran one more time in my windows10 and I ran both printouts. Please\n> check the output below.\n>\n> 2020-07-23 10:33:37.248345: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n> 2020-07-23 10:33:37.281316: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e89bc25120 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n> 2020-07-23 10:33:37.286097: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n> result_without_py_function = b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?'\n> result_with_py_function= tf.Tensor(b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?', shape=(), dtype=string)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41304#issuecomment-663138488>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQH3KMMEO67G34MN4R3IIF3R5BYMBANCNFSM4OXE3UOQ>\n> .\n>\n", "Mine is also tensoflow-cpu, TF2.2, Window10 64bit, Python 3.6,6. Thanks!\r\n", "Thanks for your testing. I did more testing with different versions of Python including your Python 3.6.6. And finally narrowed down the issue. Here are my test results for  tensorflow-cpu-2.2.0:\r\nPython 3.6.6 --> worked (your setting)\r\nPython 3.7.6 --> worked\r\nPython 3.7.8 --> worked (latest Python 3.7)\r\nPython 3.8.0 --> failed (first Python 3.8)\r\nPython 3.8.3 --> failed \r\nPython 3.8.5 --> failed (latest Python 3.8)\r\n\r\nI also tested on tesnorflow-gpu-2.2.0\r\nPython 3.7.8 --> worked\r\nPython 3.8.3 --> failed\r\n\r\nObviously this issue was introduced from Python 3.8.0. \r\n\r\nBut in Tensorflow official web page https://www.tensorflow.org/install/pip\r\nThe \"System requirements\" says \"Python 3.8 support requires TensorFlow 2.2 or later.\" . \r\nSo I believed that Tensorflow 2.2 requires Python 3.8 and then installed it.\r\nHowever this seems opposite for this issue. Python 3.8 doesn't work while 3.7 and 3.6 works.\r\nAt this moment, I can use Python 3.7 to walk around this issue, but not sure something else in TF2.2 requires Python 3.8.\r\n\r\nThanks,\r\n", "Thanks for your testing. I did more testing with different versions of\nPython including your Python 3.6.6. And finally narrowed down the issue.\nHere are my test results for  tensorflow-cpu-2.2.0:\nPython 3.6.6 --> worked (your setting)\nPython 3.7.6 --> worked\nPython 3.7.8 --> worked (latest Python 3.7)\nPython 3.8.0 --> failed (first Python 3.8)\nPython 3.8.3 --> failed\nPython 3.8.5 --> failed (latest Python 3.8)\n\nI also tested on tesnorflow-gpu-2.2.0\nPython 3.7.8 --> worked\nPython 3.8.3 --> failed\n\nObviously this issue was introduced from Python 3.8.0.\n\nBut in Tensorflow official web page https://www.tensorflow.org/install/pip\nThe \"System requirements\" says \"Python 3.8 support requires TensorFlow 2.2\nor later.\" .\nSo I believed that Tensorflow 2.2 requires Python 3.8 and then installed it.\nHowever this seems opposite for this issue. Python 3.8 doesn't work while\n3.7 and 3.6 works.\nAt this moment, I can use Python 3.7 to walk around this issue, but not\nsure something else in TF2.2 requires Python 3.8.\n\nThanks,\n\nOn Thu, Jul 23, 2020 at 12:28 PM Vishnu <notifications@github.com> wrote:\n\n> Mine is also tensoflow-cpu, TF2.2, Window10 64bit, Python 3.6,6. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41304#issuecomment-663190505>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQH3KMMGVLLOFH2ACGE7C4TR5CFNXANCNFSM4OXE3UOQ>\n> .\n>\n", "@tom970 Thanks for testing different version. Is this problem related to this particular code or any other code also fails when Python3.8 is used? Can you please test any simple tutorial on TF website with Python 3.8 (cpu and gpu)? Thanks!", "This problem is only related to \"tf.py_function\" as I mentioned in the title. So far I didn't see other issue related to Python3.8 when I ran simple tutorials on TF website. ", "This problem is only related to \"tf.py_function\" as I mentioned in the\ntitle. So far I didn't see other issue related to Python3.8 when I ran\nsimple tutorials on TF website.\n\nOn Fri, Jul 24, 2020 at 12:49 PM Vishnuvardhan Janapati <\nnotifications@github.com> wrote:\n\n> @tom970 <https://github.com/tom970> Thanks for testing different version.\n> Is this problem related to this particular code or any other code also\n> fails when Python3.8 is used? Can you please test any simple tutorial on TF\n> website with Python 3.8 (cpu and gpu)? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41304#issuecomment-663705064>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQH3KMKVC4BOSRAUQZZRVKDR5HQVZANCNFSM4OXE3UOQ>\n> .\n>\n", "For python 3.8, does this issue only reproduce on Windows, or is this a problem for ubuntu, mac os as well?", "I don't know if the issue is on linux or mac as well since I only have\nWindows\n\nOn Mon, Jul 27, 2020 at 10:41 AM Rohan Jain <notifications@github.com>\nwrote:\n\n> For python 3.8, does this issue only reproduce on Windows, or is this a\n> problem for ubuntu, mac os as well?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41304#issuecomment-664539241>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQH3KMNRMMYL54NB4ALARI3R5W335ANCNFSM4OXE3UOQ>\n> .\n>\n", "Okay I ran this on a mac and ubuntu machine and they both seem to succeed (tensorflow 2.3.1) [with python 3.8]\r\n", "@tom970 Can you please check with recent `TF2.4rc3` and let us know how it progresses. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41304\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41304\">No</a>\n"]}, {"number": 41302, "title": "up", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41302) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 41301, "title": "Resnet weights link ", "body": "Where can I find the Resnet50 pretrained weights to download ?\r\n\r\nThanks", "comments": ["ResNet 50 weights can be downloaded from [here](https://github.com/fchollet/deep-learning-models/releases/tag/v0.2)", "These are old versions, prompting errors while code run.\r\nPlease check.", "You may try https://github.com/tensorflow/models/blob/master/research/slim/README.md#pre-trained-models\r\n[ResNet 50 weights](https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5)", "\r\nSwitching to pytorch, keras seems to have quite a few version issues while installing effnet as well.\r\nThanks everybody for quick fixes.\r\n\r\n> You may try https://github.com/tensorflow/models/blob/master/research/slim/README.md#pre-trained-models\r\n> [ResNet 50 weights](https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5)\r\n\r\n\r\n\r\n> You may try https://github.com/tensorflow/models/blob/master/research/slim/README.md#pre-trained-models\r\n> [ResNet 50 weights](https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5)\r\n\r\nThey are prompting size shape errorrs.\r\n", "@pn12 \r\nCan you please share simple stand alone code for us to replicate the issue faced or if possible please share a colab gist for us to analyse.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41300, "title": "Confusing documentation for tf.image.rgb_to_yuv", "body": "The documentation for tf.image.rgb_to_yuv says \"Outputs a tensor of the same shape as the images tensor, containing the YUV value of the pixels. The output is only well defined if the value in images are in [0,1].\" Does that mean the RGB values should be [0,1]?\r\n\r\nIf so, the usage example added confusion:\r\n```\r\nx = [[[1.0, 2.0, 3.0],\r\n      [4.0, 5.0, 6.0]],\r\n    [[7.0, 8.0, 9.0],\r\n      [10.0, 11.0, 12.0]]]\r\ntf.image.rgb_to_yuv(x)\r\n```\r\n\r\nClearly, x does not lie in [0,1]\r\n", "comments": ["There are two ways of representing an image: \r\n1. [0, 255] pixel values range.\r\n2. [0, 1] (as float) pixel values range\r\n\r\nSo you need to convert the input image into a float [0, 1] range. ", "@tonychenxyz [Here](https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_yuv?version=nightly) is the link to the `tf-nightly` doc. The usage example was updated in the `tf-nightly`. It clearly says that \r\n\r\n> The output is only well defined if the value in images are in [0,1].\r\n\r\nPlease verify and close the issue if this was resolved for you. thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]