[{"number": 32525, "title": "invalid_creator_scope", "body": "", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32525\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32525\">No</a>\n"]}, {"number": 32524, "title": "unicode decode error when import tensorflow.contrib.slim", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed via pip install tensorflow-gpu==1.2\r\n- TensorFlow version 1.2.0\r\n- Python version 3.6\r\n- CUDA/cuDNN version CUDA8.0 cuDNN5.1\r\n- GPU model and memory: GTX1080TI\r\n\r\nThere is something wrong when I import slim\r\n`import tensorflow.contrib.slim as slim`\r\n\r\n`Traceback (most recent call last):\r\n  File \"finetune-sintel.py\", line 22, in <module>\r\n    from model_pwcnet import ModelPWCNet, _DEFAULT_PWCNET_FINETUNE_OPTIONS\r\n  File \"/home/sxl/ext4-2T-3/code/tfoptflow-master/tfoptflow/model_pwcnet.py\", line 21, in <module>\r\n    from model_base import ModelBase\r\n  File \"/home/sxl/ext4-2T-3/code/tfoptflow-master/tfoptflow/model_base.py\", line 15, in <module>\r\n    import tensorflow.contrib.slim as slim\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib import bayesflow\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.contrib.bayesflow.python.ops import entropy\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/entropy.py\", line 23, in <module>\r\n    from tensorflow.contrib.bayesflow.python.ops.entropy_impl import *\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/entropy_impl.py\", line 30, in <module>\r\n    from tensorflow.contrib.bayesflow.python.ops import variational_inference\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/variational_inference.py\", line 26, in <module>\r\n    from tensorflow.contrib.bayesflow.python.ops.variational_inference_impl import *\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/variational_inference_impl.py\", line 29, in <module>\r\n    from tensorflow.contrib.bayesflow.python.ops import stochastic_graph_impl as sg\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/stochastic_graph_impl.py\", line 28, in <module>\r\n    from tensorflow.contrib.bayesflow.python.ops import stochastic_tensor_impl\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/stochastic_tensor_impl.py\", line 51, in <module>\r\n    from tensorflow.contrib.distributions.python.ops import distribution\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/distributions/__init__.py\", line 91, in <module>\r\n    from tensorflow.contrib.distributions.python.ops.bernoulli import *\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/bernoulli.py\", line 21, in <module>\r\n    from tensorflow.contrib.distributions.python.ops import distribution\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/distribution.py\", line 29, in <module>\r\n    from tensorflow.contrib import framework as contrib_framework\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/framework/__init__.py\", line 81, in <module>\r\n    from tensorflow.contrib.framework.python.ops import *\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/__init__.py\", line 26, in <module>\r\n    from tensorflow.contrib.framework.python.ops.variables import *\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 26, in <module>\r\n    from tensorflow.contrib.framework.python.ops import gen_variable_ops\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/gen_variable_ops.py\", line 40, in <module>\r\n    _ops.RegisterShape(\"ZeroInitializer\")(None)\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1697, in __call__\r\n    self._op_type)\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\", line 67, in register\r\n    stack = traceback.extract_stack()\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/traceback.py\", line 211, in extract_stack\r\n    stack = StackSummary.extract(walk_stack(f), limit=limit)\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/traceback.py\", line 364, in extract\r\n    f.line\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/traceback.py\", line 286, in line\r\n    self._line = linecache.getline(self.filename, self.lineno).strip()\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/linecache.py\", line 16, in getline\r\n    lines = getlines(filename, module_globals)\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/linecache.py\", line 47, in getlines\r\n    return updatecache(filename, module_globals)\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/linecache.py\", line 137, in updatecache\r\n    lines = fp.readlines()\r\n  File \"/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/codecs.py\", line 321, in decode\r\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xcf in position 4307: invalid continuation byte\r\n`\r\n", "comments": ["@Checkmate986212 ,\r\nCan you please try using latest versions of TF- like 1.14 and 1.15-rc0? it works fine with these versions.\r\n```\r\nimport tensorflow.cntrib.slim as slim\r\n#prints\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n```", "Thank you \r\n\r\n> @Checkmate986212 ,\r\n> Can you please try using latest versions of TF- like 1.14 and 1.15-rc0? it works fine with these versions.\r\n> \r\n> ```\r\n> import tensorflow.cntrib.slim as slim\r\n> #prints\r\n> WARNING:tensorflow:\r\n> The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\n> For more information, please see:\r\n>   * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n>   * https://github.com/tensorflow/addons\r\n>   * https://github.com/tensorflow/io (for I/O related ops)\r\n> If you depend on functionality not listed there, please file an issue.\r\n> ```\r\n\r\nThank you very much, I changed to TF 1.10, and it works\r\n\r\nWell, I was really confused about the problem that it is called 'unicode error'", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32524\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32524\">No</a>\n"]}, {"number": 32523, "title": "iOS `TensorFlowLiteC` using old C API", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 13\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 6\r\n- TensorFlow installed from (source or binary): binary (TensorFlowLiteC)\r\n- TensorFlow version: 1.14.0\r\n\r\n**Describe the problem**\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.15.0-rc0/tensorflow/lite/experimental/ios/TensorFlowLiteC.podspec does not have the pre-compiled binary yet, considering we have a significant backward compatible API change introduce by https://github.com/tensorflow/tensorflow/commit/4cc4425aee000b6358fbf219fa3dd64710b7aed4\r\n\r\nCurrently, it's a problem for us at https://github.com/dart-lang/tflite_native/pull/22", "comments": ["Is this still an issue for you? We have recently published both v2.0.0 and v1.15.0 of the TensorFlowLiteC framework via CocoaPods. Let me know if there's anything I can help with.", "@yyoon looking at https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/lite/experimental/ios/TensorFlowLiteC.podspec, it's still pointing to `1.14.0`. So does https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/lite/experimental/ios/TensorFlowLiteC.podspec\r\n\r\nMeanwhile, neither https://dl.google.com/dl/cpdc/0e27bc28472e2519/TensorFlowLiteC-1.15.0.tar.gz nor https://dl.google.com/dl/cpdc/0e27bc28472e2519/TensorFlowLiteC-2.0.0.tar.gz is available", "I see how that could confuse you, but the actual process we take for releasing TensorFlowLiteC is:\r\n - Build the TensorFlowLiteC binary by checking out the release branch\r\n - Publish the binary to a publicly downloadable place\r\n - Update the podspec to bump up the version and point to the new binary, and publish the new podspec to CocoaPods *after the fact*.\r\n\r\nSo, it's somewhat irrelevant to what the podspec file is pointing to, in the specific release branch. Instead, I think you should be looking at the actual history of the C API directory in both branches.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commits/v2.0.0/tensorflow/lite/experimental/c\r\nhttps://github.com/tensorflow/tensorflow/commits/v1.15.0/tensorflow/lite/experimental/c\r\n\r\nYou can see that the API style change commit is included in v1.15.0.\r\nIf you want to download v1.15.0 without going through CocoaPods, you can use this link directly:\r\nhttps://dl.google.com/dl/cpdc/559f7884827d4af4/TensorFlowLiteC-1.15.0.tar.gz", "ah thanks, I can see v2.0.0 on https://cocoapods.org/pods/TensorFlowLiteObjC now"]}, {"number": 32522, "title": " Config value android_arm is not defined in any .rc file", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version: Tensorflow 1.13.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.19.2\r\n\r\nWhen I convert my TensorFlow model into TFLite model, as suggested from:https://github.com/tensorflow/tensorflow/issues/32112, I use tf.lite.OpsSet.SELECT_TF_OPS to covert it, then according to \r\n[running the model](https://www.tensorflow.org/lite/guide/ops_select#running_the_model), i installed bazel to get Android AAR:\r\n```\r\nbazel build --cxxopt='--std=c++11' -c opt             \\\r\n  --config=android_arm --config=monolithic          \\\r\n  //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops\r\n```\r\nBut a error occurs although I have already set up my environment by `export PATH=\"$PATH:$HOME/bin\"`:\r\n```\r\nERROR: Config value android_arm is not defined in any .rc file\r\n```\r\nThanks for you reading\r\n", "comments": ["The `--config=android_arm` config should be defined in [tensorflow/.bazelrc. ](https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L6). Are you not seeing that in your checkout?", "> The `--config=android_arm` config should be defined in [tensorflow/.bazelrc. ](https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L6). Are you not seeing that in your checkout?\r\n\r\nThanks for your response, but how can i find the file if i installed through conda\uff1f", "I'm not familiar with the conda build, but as a workaround you can manually supply the appropriate build flags listed from @ https://github.com/tensorflow/tensorflow/blob/539de53a75d9daebda8f3fc8e5ad0e04d1dfa06e/.bazelrc#L6.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32522\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32522\">No</a>\n", "The following command is working for me.\r\nbazel build -c opt --config=android_arm tensorflow/lite:libtensorflowlite.so"]}, {"number": 32521, "title": "can not tflite squeezedet graph by freeze_graph.py", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version (1.15.0-dev20190707):\r\n\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, CAST, CONCATENATION, CONV_2D, DIV, EXP, FLOOR, GREATER, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, REDUCE_MAX, RESHAPE, SOFTMAX, STRIDED_SLICE, SUB, TRANSPOSE, UNPACK. Here is a list of operators for which you will need custom implementations: RandomUniform.\r\n\r\n\r\n", "comments": ["Hi, there is one op 'RandomUniform' that is not supported by tflite as built-in op. TF Lite can also support ops from tensorflow. Could you follow instructions here (https://www.tensorflow.org/lite/guide/ops_select) ?", "Update:\r\n\r\nwe are adding `RandomUniform` as a built-in op. Jaeyoo, could you help follow up on this issue once your code is checked-in?", "Hi @Davari393 ,\r\n\r\nI am sorry for the belated updates. As the op is not yet supported and the support in builtin ops are delayed, as @haozha111 suggested, the following conversion will work for now.\r\n\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([model])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32521\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32521\">No</a>\n"]}, {"number": 32520, "title": "Training stuck on Allocation exceeds 10% of system memory", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.9.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: running on CPU\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe training is stuck after throwing the following warning: \r\n2019-09-13 23:25:57.221146: W tensorflow/core/framework/allocator.cc:108] Allocation of 603979776 exceeds 10% of system memory.\r\nstart training iteration: 0 elapse: 1568442370.016824\r\n2019-09-13 23:26:10.230583: W tensorflow/core/framework/allocator.cc:108] Allocation of 2147483648 exceeds 10% of system memory.\r\n2019-09-13 23:26:11.860365: W tensorflow/core/framework/allocator.cc:108] Allocation of 2147483648 exceeds 10% of system memory.\r\n\r\n**Describe the expected behavior**\r\nExpect the training to slowly progress given the memory is not big enough to hold the tensors then the swap should be able to help. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nMN_REDUCED.py\r\n    \r\n    import tensorflow as tf\r\n    from tensorflow.python.framework import ops\r\n    from tensorflow.python.framework import dtypes\r\n    import random\r\n    import numpy as np\r\n      \r\n    BATCH_SIZE = 128\r\n    OUTPUT=4096\r\n    NUM_CLASSES = OUTPUT\r\n    \r\n    class MN_REDUCED(object):\r\n     \r\n          def __init__(self, trainable=True, dropout=0.5):\r\n              self.trainable = trainable\r\n              self.dropout = dropout\r\n              self.parameters = []\r\n    \r\n          def build(self,rgb,train_mode=None):\r\n              with tf.name_scope('conv_1') as scope:\r\n                  kernel = tf.Variable(tf.truncated_normal([3, 3, 3, OUTPUT], dtype=tf.float32,\r\n                                          stddev=1e-2), name='weights')\r\n                  conv = tf.nn.conv2d(rgb, kernel, [1, 1, 1, 1], padding='SAME')\r\n                  biases = tf.Variable(tf.constant(0.0, shape=[OUTPUT], dtype=tf.float32),\r\n                                           trainable=True, name='biases')\r\n                  conv1 = tf.nn.bias_add(conv, biases)\r\n               #   shape = int(np.prod(out.get_shape()))\r\n               #   flat = tf.reshape(out, [BATCH_SIZE, -1])\r\n               #   self.out_0 = flat[:,0:OUTPUT]\r\n              with tf.name_scope('conv_2') as scope:\r\n                  kernel = tf.Variable(tf.truncated_normal([3, 3, OUTPUT, OUTPUT], type=tf.float32,\r\n                                          stddev=1e-2), name='weights')\r\n                  conv = tf.nn.conv2d(conv1, kernel, [1, 1, 1, 1], padding='SAME')\r\n                  biases = tf.Variable(tf.constant(0.0, shape=[OUTPUT], dtype=tf.float32),\r\n                                           trainable=True, name='biases')\r\n                  out = tf.nn.bias_add(conv, biases)\r\n                  shape = int(np.prod(out.get_shape()))\r\n                  flat = tf.reshape(out, [BATCH_SIZE, -1])\r\n                  self.out_0 = flat[:,0:OUTPUT]\r\n     \r\n          def loss(self, labels):\r\n              labels = tf.cast(labels, tf.int32)\r\n              oneHot = tf.one_hot (labels, NUM_CLASSES)\r\n              loss = tf.reduce_mean(tf.square(self.out_0 - oneHot), name='loss')\r\n              return loss\r\n     \r\n          def training(self, loss):\r\n              optimizer = tf.train.GradientDescentOptimizer(0.001)\r\n              train_op = optimizer.minimize(loss);\r\n              return train_op\r\n\r\n\r\nmain.py\r\n\r\n     import tensorflow as tf\r\n     import random\r\n     import os\r\n     from DataInput import DataInput\r\n     #from CNN_FULL_CPU import CNN_FULL_CPU\r\n     from MN_REDUCED import MN_REDUCED\r\n     import pdb\r\n     from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\r\n     import time\r\n     import numpy as np\r\n     \r\n     dataset_path = \"./\"\r\n     train_labels_file = \"dataset.txt\"\r\n     \r\n     IMAGE_HEIGHT = 32\r\n     IMAGE_WIDTH = 32\r\n     NUM_CHANNELS = 3\r\n     BATCH_SIZE = 128\r\n     NUM_ITERATIONS = 1000\r\n     #NUM_ITERATIONS = 10\r\n     LEARNING_RATE = 0.001\r\n     SUMMARY_LOG_DIR=\"./summary-log\"\r\n     lasttime = 0\r\n   \r\n       def placeholder_inputs(batch_size):\r\n               images_placeholder = tf.placeholder(tf.float32,\r\n                                                                       shape=(batch_size, IMAGE_HEIGHT,\r\n                                                                                  IMAGE_WIDTH, NUM_CHANNELS))\r\n               labels_placeholder = tf.placeholder(tf.int32,\r\n                                                                       shape=(batch_size))\r\n       \r\n               return images_placeholder, labels_placeholder\r\n   \r\n     def fill_feed_dict(images_pl, labels_pl, sess):\r\n               #images_feed, labels_feed = sess.run([data_input.example_batch, data_input.label_batch])\r\n       \r\n       \r\n               #feed_dict = {\r\n               #       images_pl: images_feed,\r\n               #       labels_pl: labels_feed,\r\n               #}\r\n               n = BATCH_SIZE * IMAGE_WIDTH * IMAGE_HEIGHT * NUM_CHANNELS\r\n               k = IMAGE_WIDTH\r\n               a = np.empty(n, dtype=np.float32)\r\n               np.random.seed(0)\r\n       \r\n           for i in range(0, n, k):\r\n                   a[i:i+k] = np.random.normal(loc=0, scale=1, size=k)\r\n               rand = np.reshape(a, (BATCH_SIZE, IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS))\r\n       \r\n       \r\n               n1 = BATCH_SIZE\r\n               a1 = np.empty(n1, dtype=np.float32)\r\n               for i in range(0, n1):\r\n                   a1[i] = np.random.normal(loc=0, scale=1)\r\n       \r\n       \r\n               feed_dict = {\r\n                       images_pl: rand,\r\n                       labels_pl: a1,\r\n               }\r\n       \r\n               return feed_dict\r\n   \r\n       def do_eval(sess,\r\n                           eval_correct,\r\n                           logits,\r\n                           images_placeholder,\r\n                           labels_placeholder,\r\n                           dataset):\r\n   \r\n           true_count = 0\r\n           # // is flooring division\r\n           steps_per_epoch = dataset.num_examples // BATCH_SIZE\r\n           num_examples = steps_per_epoch * BATCH_SIZE\r\n   \r\n           for step in xrange(steps_per_epoch):\r\n                   #feed_dict = fill_feed_dict(dataset, images_placeholder,        labels_placeholder)\r\n                   feed_dict = fill_feed_dict(dataset, images_placeholder, labels_placeholder,sess)\r\n                   count = sess.run(eval_correct, feed_dict=feed_dict)\r\n                   true_count = true_count + count\r\n   \r\n           precision = float(true_count) / num_examples\r\n           print ('  Num examples: %d, Num correct: %d, Precision @ 1: %0.04f' %\r\n                           (num_examples, true_count, precision))\r\n\r\n    def placeholder_inputs(batch_size):\r\n        images_placeholder = tf.placeholder(tf.float32,\r\n        shape=(batch_size, IMAGE_HEIGHT,\r\n        IMAGE_WIDTH, NUM_CHANNELS))\r\n        labels_placeholder = tf.placeholder(tf.int32,\r\n        shape=(batch_size))  \r\n       def main():\r\n               with tf.Graph().as_default():\r\n   \r\n                   #data_input = DataInput(dataset_path, train_labels_file, BATCH_SIZE)\r\n                   images_placeholder, labels_placeholder = placeholder_inputs(BATCH_SIZE)\r\n                   cnn_full_cpu = MN_REDUCED()\r\n                   cnn_full_cpu.build(images_placeholder)\r\n   \r\n                   summary = tf.summary.merge_all()\r\n                   saver = tf.train.Saver()\r\n                   sess = tf.Session()\r\n                   #sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n                   summary_writer = tf.summary.FileWriter(SUMMARY_LOG_DIR, sess.graph)\r\n                   coord = tf.train.Coordinator()\r\n                   threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n   \r\n                   loss = cnn_full_cpu.loss(labels_placeholder)\r\n                   train_op = cnn_full_cpu.training(loss)\r\n   \r\n                   init = tf.global_variables_initializer()\r\n                   sess.run(init)\r\n                   eval_correct = evaluation(cnn_full_cpu.out_0, labels_placeholder)\r\n   \r\n                   try:\r\n                           for i in range(NUM_ITERATIONS):\r\n                                   feed_dict = fill_feed_dict(images_placeholder,\r\n                                                                   labels_placeholder, sess)\r\n                                   _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\r\n   \r\n                                   print ('Step %d: loss = %.6f' % (i, loss_value))\r\n   \r\n                           coord.request_stop()\r\n                           coord.join(threads)\r\n                   except Exception as e:\r\n                           print(e)\r\n                   infer = sess.run([cnn_full_cpu.out_0], feed_dict=feed_dict)\r\n           sess.close().   \r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI checked the memory utilization of the virtual machine. \r\nAlthough the memory is almost fully utilized, but there is still swap area. The program should run at a cost of performance. \r\n              total        used        free      shared  buff/cache   available\r\nMem:           3945        3712         178           0          54          79\r\nSwap:          4092        2169        1923\r\n\r\nIn order to simulate a memory constrained environment, I setup a VirtualBox with 4GB memory and 4 GB swap size. Only one cpu core. \r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "Those two files I put in the question (MN_REDUCED.py and main.py) are the code for reproducing the error. You should be able to run it as long as the TensorFlow version is the same. ", "@ychen404 \r\nI tried to reproduce the issue. However i am getting the below error `ModuleNotFoundError: No module named 'DataInput'`.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/1dafc3287f80876abd20efd34b48149f/untitled189.ipynb).Kindly, help me to reproduce the issue.Thanks!", "@ravikyram \r\nThe code is running now from the link you sent me. I don't know how to import module in Colab so I just merge the two files i posted above into one. \r\n\r\nThe idea is to change the global variable OUTPUT and see how the training behaves.\r\nThe OUTPUT defines the output dimensions of both the convolution layers in the model. \r\n\r\n\r\n ", "@ychen404 \r\nAny changes you have made to the gist file, i have shared. I am not seeing any changes in that file. Thanks!", "```\r\nimport tensorflow as tf\r\nimport random\r\nimport os\r\nimport pdb\r\nfrom tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\r\nimport time\r\nimport numpy as np\r\n\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.framework import dtypes\r\n\r\nimport random\r\n\r\ndataset_path = \"./\"\r\ntrain_labels_file = \"dataset.txt\"\r\n\r\nIMAGE_HEIGHT = 32\r\nIMAGE_WIDTH = 32\r\nNUM_CHANNELS = 3\r\nBATCH_SIZE = 128\r\nNUM_ITERATIONS = 1000\r\n#NUM_ITERATIONS = 10\r\nLEARNING_RATE = 0.001\r\nSUMMARY_LOG_DIR=\"./summary-log\"\r\nlasttime = 0\r\n#IMAGE_HEIGHT = 100\r\n#IMAGE_HEIGHT = 224\r\n#IMAGE_WIDTH = 100\r\n#IMAGE_WIDTH = 224\r\n#BATCH_SIZE = 25\r\n#NUM_CHANNELS = 3\r\n#LEARNING_RATE = 0.0001\r\nBATCH_SIZE = 128\r\nOUTPUT=4096\r\nNUM_CLASSES = OUTPUT\r\n\r\n\r\nclass MN_REDUCED(object):\r\n\r\n    def __init__(self, trainable=True, dropout=0.5):\r\n        self.trainable = trainable\r\n        self.dropout = dropout\r\n        self.parameters = []\r\n\r\n\r\n    def build(self,rgb,train_mode=None):\r\n        with tf.name_scope('conv_1') as scope:\r\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, OUTPUT], dtype=tf.float32,\r\n                                    stddev=1e-2), name='weights')\r\n            conv = tf.nn.conv2d(rgb, kernel, [1, 1, 1, 1], padding='SAME')\r\n            biases = tf.Variable(tf.constant(0.0, shape=[OUTPUT], dtype=tf.float32),\r\n                                     trainable=True, name='biases')\r\n            conv1 = tf.nn.bias_add(conv, biases)\r\n         #   shape = int(np.prod(out.get_shape()))\r\n         #   flat = tf.reshape(out, [BATCH_SIZE, -1])\r\n         #   self.out_0 = flat[:,0:OUTPUT]\r\n\r\n        with tf.name_scope('conv_2') as scope:\r\n            kernel = tf.Variable(tf.truncated_normal([3, 3, OUTPUT, OUTPUT], dtype=tf.float32,\r\n                                    stddev=1e-2), name='weights')\r\n            conv = tf.nn.conv2d(conv1, kernel, [1, 1, 1, 1], padding='SAME')\r\n            biases = tf.Variable(tf.constant(0.0, shape=[OUTPUT], dtype=tf.float32),\r\n                                     trainable=True, name='biases')\r\n            out = tf.nn.bias_add(conv, biases)\r\n            shape = int(np.prod(out.get_shape()))\r\n            flat = tf.reshape(out, [BATCH_SIZE, -1])\r\n            self.out_0 = flat[:,0:OUTPUT]\r\n\r\n    def loss(self, labels):\r\n        labels = tf.cast(labels, tf.int32)\r\n        oneHot = tf.one_hot (labels, NUM_CLASSES)\r\n        loss = tf.reduce_mean(tf.square(self.out_0 - oneHot), name='loss')\r\n        return loss\r\n\r\n    def training(self, loss):\r\n        optimizer = tf.train.GradientDescentOptimizer(0.001)\r\n        train_op = optimizer.minimize(loss);\r\n        return train_op\r\n\r\n\r\ndef placeholder_inputs(batch_size):\r\n        images_placeholder = tf.placeholder(tf.float32,\r\n                                                                shape=(batch_size, IMAGE_HEIGHT,\r\n                                                                           IMAGE_WIDTH, NUM_CHANNELS))\r\n        labels_placeholder = tf.placeholder(tf.int32,\r\n                                                                shape=(batch_size))\r\n\r\n        return images_placeholder, labels_placeholder\r\n\r\ndef fill_feed_dict(images_pl, labels_pl, sess):\r\n        #images_feed, labels_feed = sess.run([data_input.example_batch, data_input.label_batch])\r\n\r\n\r\n        #feed_dict = {\r\n        #       images_pl: images_feed,\r\n        #       labels_pl: labels_feed,\r\n        #}\r\n        n = BATCH_SIZE * IMAGE_WIDTH * IMAGE_HEIGHT * NUM_CHANNELS\r\n        k = IMAGE_WIDTH\r\n        a = np.empty(n, dtype=np.float32)\r\n        np.random.seed(0)\r\n\r\n        for i in range(0, n, k):\r\n            a[i:i+k] = np.random.normal(loc=0, scale=1, size=k)\r\n        rand = np.reshape(a, (BATCH_SIZE, IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS))\r\n\r\n\r\n        n1 = BATCH_SIZE\r\n        a1 = np.empty(n1, dtype=np.float32)\r\n        for i in range(0, n1):\r\n            a1[i] = np.random.normal(loc=0, scale=1)\r\n\r\n\r\n        feed_dict = {\r\n                images_pl: rand,\r\n                labels_pl: a1,\r\n        }\r\n\r\n        return feed_dict\r\n\r\ndef do_eval(sess,\r\n                        eval_correct,\r\n                        logits,\r\n                        images_placeholder,\r\n                        labels_placeholder,\r\n                        dataset):\r\n\r\n        true_count = 0\r\n        # // is flooring division\r\n        steps_per_epoch = dataset.num_examples // BATCH_SIZE\r\n        num_examples = steps_per_epoch * BATCH_SIZE\r\n\r\n        for step in xrange(steps_per_epoch):\r\n                #feed_dict = fill_feed_dict(dataset, images_placeholder,        labels_placeholder)\r\n                feed_dict = fill_feed_dict(dataset, images_placeholder, labels_placeholder,sess)\r\n                count = sess.run(eval_correct, feed_dict=feed_dict)\r\n                true_count = true_count + count\r\n\r\n        precision = float(true_count) / num_examples\r\n        print ('  Num examples: %d, Num correct: %d, Precision @ 1: %0.04f' %\r\n                        (num_examples, true_count, precision))\r\n\r\ndef evaluation(logits, labels):\r\n        correct = tf.nn.in_top_k(logits, labels, 1)\r\n        pred = tf.argmax(logits, 1)\r\n\r\n        return tf.reduce_sum(tf.cast(correct, tf.int32))\r\n\r\n\r\ndef myTimer( str,iteration ):\r\n        global lasttime\r\n        start = time.time()\r\n        elapse = start -lasttime\r\n        lasttime = time.time()\r\n        print (\"%s iteration: %d elapse: %f\" % (str,iteration, elapse))\r\n        return;\r\n\r\n\r\ndef main():\r\n        with tf.Graph().as_default():\r\n\r\n                #data_input = DataInput(dataset_path, train_labels_file, BATCH_SIZE)\r\n                images_placeholder, labels_placeholder = placeholder_inputs(BATCH_SIZE)\r\n                cnn_full_cpu = MN_REDUCED()\r\n                cnn_full_cpu.build(images_placeholder)\r\n\r\n                summary = tf.summary.merge_all()\r\n                saver = tf.train.Saver()\r\n                sess = tf.Session()\r\n                #sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n                summary_writer = tf.summary.FileWriter(SUMMARY_LOG_DIR, sess.graph)\r\n                coord = tf.train.Coordinator()\r\n                threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n                loss = cnn_full_cpu.loss(labels_placeholder)\r\n                train_op = cnn_full_cpu.training(loss)\r\n\r\n                init = tf.global_variables_initializer()\r\n                sess.run(init)\r\n                eval_correct = evaluation(cnn_full_cpu.out_0, labels_placeholder)\r\n\r\n                try:\r\n                        for i in range(NUM_ITERATIONS):\r\n                                feed_dict = fill_feed_dict(images_placeholder,\r\n                                                                labels_placeholder, sess)\r\n\r\n                                myTimer(\"start training\", i)\r\n                                _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\r\n                                myTimer(\"end training\", i)\r\n\r\n                                print ('Step %d: loss = %.6f' % (i, loss_value))\r\n\r\n                        coord.request_stop()\r\n                        coord.join(threads)\r\n                except Exception as e:\r\n                        print(e)\r\n                myTimer(\"start inference\", i)\r\n                infer = sess.run([cnn_full_cpu.out_0], feed_dict=feed_dict)\r\n                myTimer(\"end inference\", i)\r\n        sess.close()\r\n\r\nif __name__ == '__main__':\r\n        main()\r\n```\r\n\r\nI did modify the code yesterday, but the modification is all gone today. \r\nIt seems that I can't directly save the changes on gist under your account. It will automatically create a copy connects with my github account. \r\nAnyway, you can just copy the above code and paste in a notebook. It should work. \r\nThe notebook does not allow me to call modules from other file so I merge them into one. \r\n\r\n\r\n\r\n", "I tried on colab with TF version 1.9 ,1.15.0-rc0 and able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9ce500f74fa9d12d83206dc24da08caf/untitled204.ipynb#scrollTo=uI17CXoWzAai).Thanks!", "@ychen404 Can you try `TF1.15.0rc1` and let us know whether the issue persists? Another request to you is \"please provide small standalone code\". Thanks!", "@jvishnuvardhan \r\nSorry for the delay. I was working on some deadlines in the past week. \r\nI tried on TF1.15.0rc1 as well. The issue persists. \r\nNot quite sure what did you mean by \"small standalone code\". The code that shared in the gist can run out-of-the-box and everything is in one python fie. ", "@ychen404 I checked the code and ran it for 1 iteration with `BATCH_SIZE = 32`. Code runs without any issues. Please check the output after 1 iteration. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/751adf3a7e73b3395db80ac03a5e3832/untitled204.ipynb) is the gist with 1 iteration.\r\n\r\n```\r\nWARNING:tensorflow:From <ipython-input-1-2b91e330c2c9>:173: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\r\nstart training iteration: 0 elapse: 1569524116.256695\r\nend training iteration: 0 elapse: 641.337531\r\nStep 0: loss = 0.002209\r\nstart inference iteration: 0 elapse: 0.000627\r\nend inference iteration: 0 elapse: 153.733750\r\n```\r\n\r\nYou need to select optimum parameters for your configuration. I don't thing this is related to bug in TF. Please let us know what you think? As it is not related to TF, it is better to post in Stackoverflow where the issues like these will get lot of attention. Thanks!", "@jvishnuvardhan \r\nI understand that too big a network model may not run given a certain configuration. \r\nBut should tensorflow let users know that the model is too big to run if that's the case? \r\nFor the case of batch 128, I still don't know if  1) the code is stuck somewhere because the memory allocation cannot go through or 2) the code is actually fine just taking too much time. \r\nIs there a way that at least I can narrow down the possibilities from the above two options? ", "I had the same error message.\r\n\"Allocation of 18717081600 exceeds 10% of system memory.\"\r\n\r\nIt didn't happen when i didn't use tf.distribute.mirroredstrategy() as a context when creating and compiling a model. But when i used the context(mirrorstrategy), the message displayed and training process was stuck.\r\n\r\np.s.) eager execution have been disabled.\r\n\r\n\r\n", "The minimum requirements for such training about the RAM was 16 GB (at least). I have increased the total RAM amount to 25 after that, and it started working successfully.\r\n\r\n", "> The minimum requirements for such training about the RAM was 16 GB (at least). I have increased the total RAM amount to 25 after that, and it started working successfully.\r\n\r\nHi woosal I'm currently having the same kind of issue with error message Allocation of 33867777024 exceeds 10% of system memory.\r\n\r\nI'm trying to compile a two hidden layers LSTM model with a single time series variable, with training set size = 46 008, and I get this error message. I'm using the R keras interface, with an average laptop (8 GB RAM, Intel i5 2.1GHz, standard GPU etc..)\r\n\r\nI already tried to reduce the batch_size to 1 and got the same issue. Since I already tested to compile the model with less input data (below 6 000 training data it works perfectly), I strongly suspect the model is just to big for my memory, but I wonder how big it has to be to fit. So do you know is there's a way to know how much memory is needed to handle the model ?\r\n\r\nI can provide code if needed. \r\n\r\nThanks in advance !", "We had the same problem. We need to add a new dimension, as the 3D tensor receives 4D. \r\nlist_x.append (image [:, np.newaxis])\r\n\r\nInsert a new axis that will appear at the axis position in the expanded array shape.\r\n\r\nSo we managed to solve the memory failure.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 32519, "title": "Failed to get convolution algorithm", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes / no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 LTS Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 2.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version:  9.0\r\n- GPU model and memory: Quadro GP100\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI have a simple sequential `tf.keras` model with convolution that runs on \r\na tfrecords Dataset. Suddenly, convolutions stopped working. \r\nNo known changes made to system.\r\n\r\n**Describe the expected behavior**\r\nIt runs as per usual\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nWon't help reproduce what may be system's issue.\r\n```\r\nds = tf.data.TFRecordsDataset(...).shuffle(...)\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(...),\r\n     ...\r\n])\r\n\r\nmodel.compile(...)\r\nmodel.fit(...)\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n name: Quadro GP100, pci bus id: 0000:af:00.0, compute capability: 6.0)\r\n2019-09-14 08:13:43.313823: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56244eb0db50 executing computations on platform CUDA. Devices:\r\n2019-09-14 08:13:43.313866: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Quadro GP100, Compute Capability 6.0\r\n\r\n2019-09-14 08:14:35.273134: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:111] Filling up shuffle buffer (this may take a while): 4255 of 129446\r\n....\r\n\r\n\r\n2019-09-14 08:14:36.878088: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:162] Shuffle buffer filled.\r\n2019-09-14 08:14:48.210416: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-09-14 08:14:49.011180: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-09-14 08:14:49.016488: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-09-14 08:14:49.016616: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv1d/conv1d}}]]\r\n```\r\n\r\nno warning message was printed above.", "comments": ["@SumNeuron ,\r\nHi,looks like the issue is with CUDA, please refer similar issue #24828 and [solution](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-457425190).Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32519\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32519\">No</a>\n"]}, {"number": 32518, "title": "DefaultLogger Internal error: could not find any implementation for node (Unnamed Layer* 1) [TopK], try increasing the workspace size with IBuilder::setMaxWorkspaceSize()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.14\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory: RTX 2080 , 8 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nSee the following error.  \r\n\r\n2019-09-13 20:44:30.298493: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Internal error: could not find any implementation for node (Unnamed Layer* 1) [TopK], try increasing the workspace size with IBuilder::setMaxWorkspaceSize()\r\n2019-09-13 20:44:30.301481: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger ../builder/tacticOptimizer.cpp (1330) - OutOfMemory Error in computeCosts: 0\r\n\r\n**Describe the expected behavior**\r\nWork around  so as to not see this error. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "Looks like  the error goes away when the workpspace memory size is increased.   Wondering, if the native segment fallback  works well transparently.  Preliminary observation is that it does not. \r\n\r\n```\r\n2904:2019-09-16 16:22:44.847590: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Internal error: could not find any implementation for node (Unnamed Layer* 1) [TopK], try increasing the workspace size with IBuilder::setMaxWorkspaceSize()                                                                                                                                                  \r\n2911:2019-09-16 16:22:44.850941: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger ../builder/tacticOptimizer.cpp (1330) - OutOfMemory Error in computeCosts: 0 \r\n2912:2019-09-16 16:22:44.851015: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:647] Engine creation for xxx/TRTEngineOp_4 failed. The native segment will be used instead. Reason: Internal: Failed to build TensorRT engine   \r\n```\r\n", "Please, provide simple standalone code, then it is easy for localizing the issue faster. Thanks again.!", "That is easier said than done, when dealing with complex networks.  As a general observation, it is  not possible to create a sample code for every issue.   A  suggestion  would be to collect more metrics,  and log more and better  traces.  Most other complex pieces  of software uses such techniques rather than relying on  samples codes from developers. ", "@sgambient as you said please try to increase the workspace size. Also @pooyadavoodi .\r\nIf it fails to build the engine, the fallback should work transparently. If not please reopen or file another issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32518\">No</a>\n"]}, {"number": 32517, "title": "undeclared inclusion(s) in rule '//tensorflow/contrib/lite/kernels:eigen_support'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r1.12\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: Cuda 9.0/ cuDNN 7\r\n- GPU model and memory: Mobile GTX 1070\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nsudo bazel --output_base=/opt/tensorflow build --config=cuda --config=opt --define framework_shared_object=false tensorflow:libtensorflow_cc.so //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n`./configure`\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/ryan/catkin_ws/devel/lib/python2.7/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild:ignite --define with_ignite_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda-9.0/lib64:/home/ryan/catkin_ws/devel/lib:/opt/ros/kinetic/lib:/opt/ros/kinetic/lib/x86_64-linux-gnu\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\n\r\n`build log`\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nDEBUG: /opt/tensorflow/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nWARNING: /opt/tensorflow/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /opt/tensorflow/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /opt/tensorflow/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /opt/tensorflow/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /opt/tensorflow/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /opt/tensorflow/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/ryan/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/ryan/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/ryan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/ryan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/ryan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/ryan/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/ryan/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/ryan/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed 2 targets (0 packages loaded).\r\nINFO: Found 2 targets...\r\nERROR: /home/ryan/tensorflow/tensorflow/contrib/lite/kernels/BUILD:57:1: undeclared inclusion(s) in rule '//tensorflow/contrib/lite/kernels:eigen_support':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/lite/kernels/eigen_support.cc':\r\n  '/opt/tensorflow/external/eigen_archive/Eigen/Core'\r\nINFO: Elapsed time: 3.144s, Critical Path: 2.54s\r\nINFO: 18 processes: 18 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["There's not much we can do for TF 1.12 at this point. Are you able to repro the issue in 1.14 or a more recent build?", "I tried with r1.11 and r1.14 and it builds.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32517\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32517\">No</a>\n"]}, {"number": 32516, "title": "keras.fit not working with conv2D layer", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am using the fashion_mnist dataset to train a convolutional network,\r\n`fashion_mnist = keras.datasets.fashion_mnist\r\n\r\n(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()`\r\nwhen I use a model with conv2D as first layer, more specifically:\r\n`model = keras.models.Sequential([\r\n    keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, input_shape=(28,28,1)),\r\n...`\r\nthen use the fit function:\r\n`model.fit(train_images, train_labels, batch_size=1, epochs=5)`\r\nan error pops up:\r\n`\r\nValueError: Error when checking input: expected conv2d_2_input to have 4 dimensions, but got array with shape (60000, 28, 28)`\r\n\r\nso I changed the input shape to (28,28,1,0), but another error pops up:\r\n`ValueError: Input 0 of layer conv2d_6 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [None, 28, 28, 1, 0]`\r\n\r\n**Describe the expected behavior**\r\nThe conv nn should have worked just fine\r\n\r\n**Code to reproduce the issue**\r\n`\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n\r\n(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, input_shape=(28,28,1)),\r\n    keras.layers.MaxPooling2D(2,2),\r\n    keras.layers.Flatten(),\r\n    keras.layers.Dense(128, activation=tf.nn.relu),\r\n    keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(),\r\n              loss='sparse_categorical_crossentropy')\r\n\r\nmodel.fit(train_images, train_labels, epochs=5)\r\n'\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n`\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-16-4abc8fa24c0d> in <module>()\r\n     10     keras.layers.Flatten(),\r\n     11     keras.layers.Dense(128, activation=tf.nn.relu),\r\n---> 12     keras.layers.Dense(10, activation=tf.nn.softmax)\r\n     13 ])\r\n     14 \r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)\r\n    108       tf_utils.assert_no_legacy_layers(layers)\r\n    109       for layer in layers:\r\n--> 110         self.add(layer)\r\n    111 \r\n    112   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)\r\n    172           # and create the node connecting the current layer\r\n    173           # to the input layer we just created.\r\n--> 174           layer(x)\r\n    175           set_inputs = True\r\n    176 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    584         # the corresponding TF subgraph inside `backend.get_graph()`\r\n    585         input_spec.assert_input_compatibility(self.input_spec, inputs,\r\n--> 586                                               self.name)\r\n    587         graph = backend.get_graph()\r\n    588         with graph.as_default(), backend.name_scope(self._name_scope()):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)\r\n    121                          'expected ndim=' + str(spec.ndim) + ', found ndim=' +\r\n    122                          str(ndim) + '. Full shape received: ' +\r\n--> 123                          str(x.shape.as_list()))\r\n    124     if spec.max_ndim is not None:\r\n    125       ndim = x.shape.ndims\r\n\r\nValueError: Input 0 of layer conv2d_6 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [None, 28, 28, 1, 0]\r\n`\r\n", "comments": ["apparently it was an issue with the input, so I did some reshaping and now it works just fine."]}, {"number": 32515, "title": "add rocm into build info flags", "body": "This is a bug fix from JIZHI, the AI platform in Tencent.\r\n\r\n@deven-amd\r\nBelow bug is caused by build_config_info changed from \"--build_config cuda\" to \"--is_config_cuda True\", but miss the file tensorflow/contrib/cmake/CMakeLists.txt\r\n \r\n \r\nERROR: /tensorflow-rep/tensorflow/python/BUILD:199:1: Executing genrule //tensorflow/python:py_build_info_gen failed (Exit 2): bash failed: error executing command \r\n  (cd /xxxxxxxxxx/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/cuda/lib64:/usr/local/nccl_2.4.7-1+cuda10.0_x86_64/lib:/usr/local/lib64 \\\r\n    PATH=/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/ibutils/bin:/root/.ft:/root/bin:/root/.ft \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/tools/build_info/gen_build_info --raw_generate \"bazel-out/host/bin/tensorflow/python/platform/build_info.py\"  --is_config_cuda True --is_config_rocm False --key_value  cuda_version_number=${TF_CUDA_VERSION:-} cudnn_version_number=${TF_CUDNN_VERSION:-} ')\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nusage: gen_build_info.py [-h] [--build_config BUILD_CONFIG]\r\n                         [--raw_generate RAW_GENERATE]\r\n                         [--key_value [KEY_VALUE [KEY_VALUE ...]]]\r\ngen_build_info.py: error: unrecognized arguments: --is_config_cuda True --is_config_rocm False\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32515) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32515) for more info**.\n\n<!-- ok -->", "closing this PR as contrib folder will be depricated in 2.0, thank you.\r\nCC @mihaimaruseac", "Should I submit the PR in r1.15 again?\r\n@mihaimaruseac and @rthadur ", "Please make it against master, without anymore contrib (contrib is removed anyway).\r\n\r\nThe window for cherry-picks for 1.15 is ended"]}, {"number": 32514, "title": "[r2.0 CherryPick]: [INTEL MKL] Fixing spurious omp thread spawning", "body": "This PR fixes the core over-utilization issue when Eigen and OpenMP threadings crash.\r\n\r\nFrom PR #32485:\r\nIn some models that use eager/imperative code where we don\u2019t rewrite a few operators to MKL, a lot (intra_op_threads*OMP_NUM_THREADS) of threads are spawned by Eigen matmul code. This is because -fopenmp flag is passed to Eigen as part of the build configuration. This PR fixes the configuration to not pass -fopenmp to Eigen under --config=mkl.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32514) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32514) for more info**.\n\n<!-- cla_yes -->", "Manually setting CLA to yes because all the co-authored commits are from PR #32485 that is already merged in master."]}, {"number": 32513, "title": "Update release notes for tensorrt and mixed precision", "body": "", "comments": []}, {"number": 32512, "title": "Expand checkpoint + SavedModel file formats to include provenance (origin) meta data", "body": "I wrote a really nice issue description. Then github made it all disappear when I clicked their shiny new \"Similar issues - Try it!\" feature. I'm too frustrated to retype it. The gist is really in the title: current provenance practice is basically in the filename of the model, which sucks. \r\n\r\nWe should add a field (that should get as auto-populated as possible) to the model outputs to keep track of the story behind the model. ", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "While I'm on Windows 10, Intel 64-bit, TF 1.14, `conda install tensorflow-gpu`, this feature request applies across platforms and future versions. \r\n\r\nWhat I'm thinking is basically expanding on the data that goes into the `history` object returned by `model.fit`, and to retain that information when doing checkpoint saves/restores and when exporting a SavedModel. \r\n\r\nThe expanded information would include:\r\n1. The `optimizer` settings: \r\n    1. Type\r\n    1. Parameters\r\n    1. Any epoch-specific information the optimizer implementer deems relevant\r\n1. The ability to provide a note for what the training data is / where it came from\r\n    1. This probably has to be manually entered as part of the training script, could I guess try to include any path/directory type information automatically? \r\n1. Any log entries collected from any `model.fit` `callbacks`\r\n1. Some general field(s) where additional notes about the data can be entered. \r\n\r\nWith detailed training information like this included in the file, and with e.g. a command line tool to allow inspection + presumably tensorboard support, then there would be much less uncertainty about how a specific set of weights came to be, and best practice would become the effortless default rather than an uphill battle. ", "I don't think we have access to most of this from `tf.saved_model.save` without special-casing Keras there, which is something we don't want to do. But some of this sounds reasonable for `Model.save`.\r\n\r\nCC @k-w-w ; any interest/guidance?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 32511, "title": "[r2.0-CherryPick]:Fix bug when cloning functional models that use Tensor keyword argume\u2026", "body": "\u2026nts.\r\n\r\nAligned the cloning implementation to be similar to Model.from_config(model.get_config()), with a few minor differences.\r\n\r\nPiperOrigin-RevId: 268969706", "comments": []}, {"number": 32510, "title": "Build doesn't respect LD_LIBRARY_PATH necessary for some rules.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nRHEL 6\r\n- TensorFlow installed from (source or binary):\r\nr1.14 from git\r\n- TensorFlow version:\r\n1.14\r\n- Python version:\r\nPython 3.5.6 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?:\r\nUsing conda's python\r\n- Bazel version (if compiling from source):\r\n0.26.1\r\n- GCC/Compiler version (if compiling from source):\r\n7.3.0\r\n- CUDA/cuDNN version:\r\nCUDA - 10.1\r\ncuDNN - 7.6\r\n- GPU model and memory:\r\nNvidia V100 32GB\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to build TensorFlow on a RHEL 6 system, and run into the following error:\r\n\r\n```\r\nERROR: /scratch/users/mkrafcz2/bazel_output/external/nccl_archive/BUILD.bazel:67:1: nvlink external/nccl_archive/device_dlink_hdrs_register_sm_70.h failed (Exit 1): nvlink failed: error ex 1uting command\r\n  (cd /scratch/users/mkrafcz2/bazel_output/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/local_config_cuda/cuda/cuda/bin/nvlink '--cpu-arch=X86_64' '--arch=sm_70' '--register-link-binaries=bazel-out/k8-opt/bin/external/nccl_archive/device_dlink_hdrs_register_sm_70.h' '--output-file=bazel-out/k8-opt/bin/external/nccl_archive/device_dlink_hdrs_sm_70.cubin' bazel-out/k8-opt/bin/external/nccl_archive/libdevice_lib.pic.a)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n/ui/ncsa/mkrafcz2/.cache/bazel/_bazel_mkrafcz2/install/316fe1c1cb66caaa37fdaa8c2ebb1ffd/_embedded_binaries/process-wrapper: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /ui/ncsa/mkrafcz2/.cache/bazel/_bazel_mkrafcz2/install/316fe1c1cb66caaa37fdaa8c2ebb1ffd/_embedded_binaries/process-wrapper)\r\n```\r\n\r\nBecause the native gcc is 4.4.7, I need to use a custom compiler which I get by running \r\n`module load gcc/7.3.0`.\r\n\r\nI then run the bazel build with --action_env=PATH and --action_env=LD_LIBRARY_PATH thinking that this will be appropriately passed into tests and such through the build, but from the error above, this is not the case.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nEnvironment script:\r\n\r\n```\r\nsource ~/conda3/bin/activate\r\n\r\nconda activate tf-build-test\r\n\r\nmodule load gcc/7.3.0\r\nmodule load cuda-10.0\r\nmodule load java-1.8.112\r\n\r\nexport PATH=\"${PWD}/bazel-0.26.1/bin:${PATH}\"\r\n\r\nexport CUDNN_PATH=\"/usr/apps/deeplearning/cuDNNv7.6.3.30_10.1\"\r\nexport SRC_DIR=\"${PWD}/tensorflow-git\"\r\nexport BAZEL_OUTPUT=${HOME}/scratch-global/bazel_outpu\r\n```\r\n\r\nBuild script:\r\n\r\n```\r\n# This build script is populated initially from the conda tensorflow recipe\r\n\r\nsource scripts/env.sh\r\n\r\ncd tensorflow-git\r\n\r\nmkdir -p ${BAZEL_OUTPUT}\r\n\r\nexport BAZEL_OPTS=\"--batch --output_base=${BAZEL_OUTPUT}\"\r\n\r\n# Compile tensorflow from source\r\nexport PYTHON_BIN_PATH=$(which python)\r\nexport PYTHON_LIB_PATH=$(python -c 'import sys; print([path for path in sys.path if \"site-packages\" in path][0])')\r\nexport USE_DEFAULT_PYTHON_LIB_PATH=1\r\n\r\n# additional settings\r\n# disable jemmloc (needs MADV_HUGEPAGE macro which is not in glib <= 2.12)\r\nexport TF_NEED_JEMALLOC=0\r\n\r\n# do not build with MKL support\r\nexport TF_NEED_MKL=0\r\n\r\n# Ivybridge\r\nexport CC_OPT_FLAGS=\"-march=ivybridge -mtune=intel\"\r\n\r\nexport TF_NEED_GCP=1\r\nexport TF_NEED_HDFS=1\r\nexport TF_NEED_S3=1\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NEED_ROCM=0\r\nexport TF_NEED_MPI=0\r\nexport TF_NEED_TENSORRT=0\r\n\r\n# CUDA details\r\nexport TF_NEED_CUDA=1\r\nexport CUDA_PATH=$(which nvcc | rev | cut -d '/' -f 3- | rev)\r\nexport TF_CUDA_PATHS=\"${CUDA_PATH},${CUDNN_PATH}\"\r\nexport TF_CUDA_VERSION=$(cat ${CUDA_PATH}/version.txt | cut -d ' ' -f 3 | cut -d '.' -f -2)\r\nexport CUDNN_MAJOR=$(cat ${CUDNN_PATH}/include/cudnn.h | grep CUDNN_MAJOR | grep -v CUDNN_VERSION | cut -d ' ' -f 3)\r\nexport CUDNN_MINOR=$(cat ${CUDNN_PATH}/include/cudnn.h | grep CUDNN_MINOR | grep -v CUDNN_VERSION | cut -d ' ' -f 3)\r\nexport CUDNN_PATCHLEVEL=$(cat ${CUDNN_PATH}/include/cudnn.h | grep CUDNN_PATCHLEVEL | grep -v CUDNN_VERSION | cut -d ' ' -f 3)\r\nexport TF_CUDNN_VERSION=\"${CUDNN_MAJOR}\"\r\n\r\nexport TF_NCCL_VERSION=\"\"\r\n\r\n# libcuda.so.1 needs to be symlinked to libcuda.so\r\n# ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1\r\n# on a \"real\" system the so.1 library is typically in /usr/local/nvidia/lib64\r\n# add the stubs directory to LD_LIBRARY_PATH so libcuda.so.1 can be found\r\n#export LD_LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs/:${LD_LIBRARY_PATH}\"\r\n\r\nyes \"\" | ./configure\r\n\r\nbazel ${BAZEL_OPTS} build \\\r\n    --copt=-march=ivybridge \\\r\n    --copt=-mtune=ivybridge \\\r\n    --copt=-ftree-vectorize \\\r\n    --copt=-fPIC \\\r\n    --copt=-fstack-protector-strong \\\r\n    --copt=-O3 \\\r\n    --cxxopt=-fvisibility-inlines-hidden \\\r\n    --cxxopt=-fmessage-length=0 \\\r\n    --verbose_failures \\\r\n    --config=opt \\\r\n    --config=cuda \\\r\n    --color=yes \\\r\n    --curses=yes \\\r\n    --action_env=PATH \\\r\n    --action_env=LD_LIBRARY_PATH \\\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n\r\n# build a whl file\r\nmkdir -p $SRC_DIR/tensorflow_pkg\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package $SRC_DIR/tensorflow_pkg\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["After some googling, it seems my issue may actually be an issue with bazel instead.\r\nThe issues bazelbuild/bazel#4137 and bazelbuild/bazel#3320 seem to describe my issue perfectly.\r\n\r\nEither --action_env is not being passed to subordinate actions, or bits like process_wrapper are ", "Thanks for updating the issue! Unfortunately, I do think this is a Bazel problem, so I'll close this issue here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32510\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32510\">No</a>\n"]}, {"number": 32509, "title": "[Intel MKL] Fixing _MklQuantizeV2 rewrite issue", "body": "This fixes _MklQuantizeV2 rewrite issue by adding 2 new attributes that were added recently to the original QuantizeV2 Op.", "comments": ["Thanks @penpornk for the review. I have addressed your comment."]}, {"number": 32508, "title": "[Intel MKL] Supporting MatMul, Transpose and Softmax with BFloat16 type", "body": "This PR enables _MklMatMul, _MklTranspose and _MklSoftmax with BFloat16 type.\r\nSome of the changes are suggested by Clang format checker.", "comments": ["hi @penpornk, thanks for review. I have addressed your points. Pls take a look.", "Thank you a quick review @penpornk!"]}, {"number": 32507, "title": "2.0.0-rc2 cherry-pick request: Use relative imports only in TensorFlow.", "body": "Estimator's autocomplete works without relative imports and relative imports cause some issue.\r\n\r\nPiperOrigin-RevId: 268940330", "comments": []}, {"number": 32506, "title": "UnboundLocalError: local variable `packed` referenced before assignment", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\nSome specific set of circumstances skip over assigning `packed` to a value.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/551b67c8dff2d3656cb1858f1cb11beb049dafaf/tensorflow/python/util/nest.py#L472\r\n\r\n**Describe the expected behavior**\r\nThe local variable `packed` should have a value set.\r\n\r\n**Code to reproduce the issue**\r\nThrow an `IndexError` in [this call](https://github.com/tensorflow/tensorflow/blob/551b67c8dff2d3656cb1858f1cb11beb049dafaf/tensorflow/python/util/nest.py#L461) and make sure the [if statement](https://github.com/tensorflow/tensorflow/blob/551b67c8dff2d3656cb1858f1cb11beb049dafaf/tensorflow/python/util/nest.py#L467) is not entered.\r\n\r\nI wish I had a portable example for you but I'm not quite sure why my code triggering this path and I don't have much time to investigate the root cause, but I can confirm that is an error that I have encountered naturally. It isn't just hypothetical.\r\n\r\n**Other info / logs**\r\nTF portion of the stack trace:\r\n```\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 516, in map_structure\r\n    expand_composites=expand_composites)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 450, in pack_sequence_as\r\n    return _sequence_like(structure, packed)\r\nUnboundLocalError: local variable 'packed' referenced before assignment\r\n```\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@gehring \r\nPlease, let us know is this still an issue?. If so please provide a minimal standalone code to reproduce the issue reported here.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 32505, "title": "Keras ModelCheckpoint callback fails to create directory when saving as SavedModel", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10 Pro Version 1903\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nTF 2.0.0rc1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Using CPU\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nUsing tf.keras.callbacks.ModelCheckpoint results in Failed to create a directory. I believe it's an issue with how paths are created in Tensorflow. Error does not occur if I use double backslash to join checkpoint_folder and model_filename instead of os.path.join. \r\n**Describe the expected behavior**\r\nDirectory for saved model is created and saved model is saved correctly\r\n**Code to reproduce the issue**\r\n```\r\nmodel_filename = str(args.net) + \"-Epoch-{epoch:02d}-Loss-{val_loss:.2f}\"\\\r\nif args.save_format == 'h5':\r\n    model_filename += '.h5'\r\ncheckpoint_path = os.path.join(args.checkpoint_folder, model_filename)\r\n\r\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\r\n        filepath=checkpoint_path,\r\n        monitor='val_loss',\r\n        verbose=1,\r\n        save_best_only=True,\r\n        save_weights_only=False,\r\n        mode='auto')\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Users/iden/PycharmProjects/PytorchSSD/tf_translate/train_ssd.py\", line 307, in <module>\r\n    workers=args.num_workers, max_queue_size=args.max_queue_size)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1297, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\", line 332, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\", line 297, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\", line 964, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\", line 1000, in _save_model\r\n    self.model.save(filepath, overwrite=True)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 1189, in save\r\n    signatures, options)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\", line 115, in save_model\r\n    signatures, options)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save.py\", line 74, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\", line 899, in save\r\n    utils_impl.get_or_create_variables_dir(export_dir)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\utils_impl.py\", line 183, in get_or_create_variables_dir\r\n    file_io.recursive_create_dir(variables_dir)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\", line 438, in recursive_create_dir\r\n    recursive_create_dir_v2(dirname)\r\n  File \"C:\\Users\\iden\\PycharmProjects\\PytorchSSD\\tf2venv\\lib\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\", line 453, in recursive_create_dir_v2\r\n    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(path))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: models/mb1-ssd-Epoch-01-Loss-32.98\\variables; No such file or directory\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["@idenc,\r\nIn order to expedite the trouble-shooting process, please provide a standalone code to reproduce the issue reported here. Thanks!\r\n", "This code should reproduce the issue.\r\n```\r\nimport os\r\n\r\nimport numpy as np\r\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint\r\nfrom tensorflow.python.keras.layers import Dense, Activation\r\nfrom tensorflow.python.keras.models import Sequential\r\n\r\nmodel_filename = \"test-Epoch-{epoch:02d}\"\r\ncheckpoint_path = os.path.join('models/', model_filename)\r\n\r\nmodel_checkpoint = ModelCheckpoint(\r\n    filepath=checkpoint_path,\r\n    monitor='val_loss',\r\n    verbose=1,\r\n    save_best_only=False,\r\n    save_weights_only=False,\r\n    mode='auto')\r\n\r\nx_train, y_train = np.ones([1, 784]), np.ones([1, 10])\r\nmodel = Sequential()\r\nmodel.add(Dense(10, input_dim=784, kernel_initializer='uniform'))\r\nmodel.add(Activation('softmax'))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\r\nmodel.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[model_checkpoint])\r\n```", "@idenc, I tried replicating the issue on colab but executed without any error. Please see the [gist](https://colab.sandbox.google.com/gist/gadagashwini/421b3c03826a7b346b85e0d8703bfca9/untitled156.ipynb). Thanks!", "@gadagashwini I think it may only occur on Windows. Running on my Windows 10 machine I get the error if I use a forward slash with the checkpoint directory. However, just removing any slashes and using os.path.join throws no errors. Can be closed.", "I bumped into this issue as well. In my case the problem was that the directory where the checkpoints were saved was also monitored by microsoft onedrive client and this seemed to have caused the problem. Possibly OneDrive trying to read  the checkpoint file when keras attempted to update it, hence the conflict. Not sure if this is the case of the problem mentioned in this issue, however possibly worth examining for conflicts with 3rd party application. Is it possible to have interference with source control software like git, as well?"]}, {"number": 32504, "title": "Fix performance regression issue by reusing metrics property.", "body": "PiperOrigin-RevId: 268827158", "comments": []}, {"number": 32503, "title": "Pin tensorflow_estimator to 1.15.1", "body": "As this is the last TF release in 1.15 series and Estimator 1.15.1 is\r\nthe corresponding last Estimator release, we pin it using `==`.", "comments": []}, {"number": 32502, "title": "Plain English explanation of CLA?", "body": "\r\n## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#contributor-license-agreements\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI have no idea what any of this legal mumbo-jumbo actually entails.  \"Grant of Patent License\", \"Grant of Copyright License\".  Do I own my contributions?  Does Google own my contributions?  What does all of this mess mean?  If I create something and \"give it away\", I want to make it free as in gratis and as in libre widely, and not just to Google.  Is that happening here?  Does Google charge/restrict people (e.g. corporations) using tensorflow?  *Can* Google charge/restrict people using tensorflow and/or my contributions? \r\n\r\n### Submit a pull request?\r\nNo.\r\n", "comments": ["> I have no idea what any of this legal mumbo-jumbo actually entails. \"Grant of Patent License\", \"Grant of Copyright License\". Do I own my contributions?\r\n\r\nYes, you own your contributions. The preamble of the agreement addresses this.\r\n\r\n> Does Google own my contributions? \r\n\r\nNo, Google only receives a license to your contribution, as detailed in Sections 2 and 3 of the agreement, which enables Google to incorporate the contribution into the project.\r\n\r\n> What does all of this mess mean? If I create something and \"give it away\", I want to make it free as in gratis and as in libre widely, and not just to Google. Is that happening here?\r\n\r\nYou are entirely welcome to make your contributions as free as possible. The Contributor License Agreement grants Google broad permission to use the contribution, which Google then uses to release your contribution as part of TensorFlow under the Apache 2 license. However, since you retain ownership of your contributions, you are also free to publish those contributions (and/or anything else that you personally hold the rights to) under the most liberal terms possible by publishing it on GitHub (or elsewhere) under a license such as the CC0.\r\n\r\n> Does Google charge/restrict people (e.g. corporations) using tensorflow? Can Google charge/restrict people using tensorflow and/or my contributions?\r\n\r\nTensorFlow is released under the Apache 2 license which is a perpetual open source license. This means that Google cannot prevent people, corporations, you, or anyone from ever using TensorFlow under the terms of the Apache 2 license. \r\n\r\nLet me know if you have more questions.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32502\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32502\">No</a>\n"]}, {"number": 32501, "title": "Error when using stateful RNN with multiple inputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0rc0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0.130/7.6.0\r\n- GPU model and memory: GTX 980 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nThe stock example of RNNs with multiple inputs from here https://www.tensorflow.org/beta/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs produces an error if you set `stateful=True`.  This seems to be a problem with any multi-input RNN with stateful=True.\r\n\r\n**Describe the expected behavior**\r\n\r\nThere should be no error, multi-input RNNs with stateful=True should work the same as with stateful=False (other than preserving state).\r\n\r\n**Code to reproduce the issue**\r\n\r\nNote, this code is copied from https://www.tensorflow.org/beta/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs, with the exception that I changed the line\r\n```\r\nrnn = tf.keras.layers.RNN(cell)\r\n```\r\nto\r\n```\r\nrnn = tf.keras.layers.RNN(cell, stateful=True)\r\n```\r\n\r\n``` python\r\nimport collections\r\n\r\nimport tensorflow as tf\r\n\r\nNestedInput = collections.namedtuple(\"NestedInput\", [\"feature1\", \"feature2\"])\r\nNestedState = collections.namedtuple(\"NestedState\", [\"state1\", \"state2\"])\r\n\r\n\r\nclass NestedCell(tf.keras.layers.Layer):\r\n    def __init__(self, unit_1, unit_2, unit_3, **kwargs):\r\n        self.unit_1 = unit_1\r\n        self.unit_2 = unit_2\r\n        self.unit_3 = unit_3\r\n        self.state_size = NestedState(\r\n            state1=unit_1, state2=tf.TensorShape([unit_2, unit_3])\r\n        )\r\n        self.output_size = (unit_1, tf.TensorShape([unit_2, unit_3]))\r\n        super(NestedCell, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shapes):\r\n        # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\r\n        input_1 = input_shapes.feature1[1]\r\n        input_2, input_3 = input_shapes.feature2[1:]\r\n\r\n        self.kernel_1 = self.add_weight(\r\n            shape=(input_1, self.unit_1), initializer=\"uniform\", name=\"kernel_1\"\r\n        )\r\n        self.kernel_2_3 = self.add_weight(\r\n            shape=(input_2, input_3, self.unit_2, self.unit_3),\r\n            initializer=\"uniform\",\r\n            name=\"kernel_2_3\",\r\n        )\r\n\r\n    def call(self, inputs, states):\r\n        # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\r\n        # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\r\n        input_1, input_2 = tf.nest.flatten(inputs)\r\n        s1, s2 = states\r\n\r\n        output_1 = tf.matmul(input_1, self.kernel_1)\r\n        output_2_3 = tf.einsum(\"bij,ijkl->bkl\", input_2, self.kernel_2_3)\r\n        state_1 = s1 + output_1\r\n        state_2_3 = s2 + output_2_3\r\n\r\n        output = [output_1, output_2_3]\r\n        new_states = NestedState(state1=state_1, state2=state_2_3)\r\n\r\n        return output, new_states\r\n\r\n\r\nunit_1 = 10\r\nunit_2 = 20\r\nunit_3 = 30\r\n\r\ninput_1 = 32\r\ninput_2 = 64\r\ninput_3 = 32\r\nbatch_size = 64\r\nnum_batch = 100\r\ntimestep = 50\r\n\r\ncell = NestedCell(unit_1, unit_2, unit_3)\r\nrnn = tf.keras.layers.RNN(cell, stateful=True)\r\n\r\ninp_1 = tf.keras.Input((None, input_1))\r\ninp_2 = tf.keras.Input((None, input_2, input_3))\r\n\r\noutputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))\r\n\r\nmodel = tf.keras.models.Model([inp_1, inp_2], outputs)\r\n\r\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \".../tmp2.py\", line 70, in <module>\r\n    outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))\r\n  File \"...\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\", line 623, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"...\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 777, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"...\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 2099, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"...\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\", line 561, in build\r\n    self.reset_states()\r\n  File \"...\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\", line 809, in reset_states\r\n    spec_shape = None if self.input_spec is None else self.input_spec[0].shape\r\nAttributeError: 'NestedInput' object has no attribute 'shape'\r\n```\r\n", "comments": ["Issue replicating for TF version-2.0rc0 and also 2.0rc1, please find the [gist](https://colab.sandbox.google.com/gist/oanush/8b07187ad22ba68f2bdd7f45a058d851/32501.ipynb) of the colab.Thanks!", "This issue is present in 1.14.0 as well (and probably earlier, but the RNN api was different so the example doesn't run for other reasons).", "Thanks for reporting the issue. Let me take a look.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32501\">No</a>\n"]}, {"number": 32500, "title": "Memory continues to grow after repeated calls to model.predict(tf.one_hot(states, dtype='float32', depth=3))", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tensorflow==2.0.0-rc1\r\n- TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566 2.0.0-rc1\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Intel Iris Plus Graphics 640 1536 MB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the below code in Docker (version 19.03.2) causes the memory to grow without limit. This is visible in `docker stats`, eventually crashing docker.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe memory should not grow indefinitely\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python3\r\nimport tensorflow as tf\r\n\r\nrows = 6\r\ncolumns = 7\r\n\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=[rows * columns, 3]),\r\n  tf.keras.layers.Dense(7, input_shape=[rows * columns * 3]),\r\n])\r\n\r\nmodel.compile(\r\n  optimizer=tf.keras.optimizers.SGD(lr=0.01),\r\n  loss='mean_squared_error',\r\n  metrics=['accuracy']\r\n)\r\n\r\nstates = [ [ 1 ] * rows * columns for i in range(20) ]\r\n\r\nfor iteration in range(1000000):\r\n    print('iteration', iteration)\r\n    model.predict(tf.one_hot(states, dtype='float32', depth=3))\r\n\r\n```\r\n\r\nThe aforementioned code runs in an image generated by the following Dockerfile\r\n\r\n```Dockerfile\r\nFROM centos:7\r\n\r\nENV SOURCE_DIRECTORY /tmp/tf-connect4\r\n\r\nENV PYTHON_VERSION 3.7.4\r\n\r\nRUN yum -y groupinstall -y \"Development Tools\" && \\\r\n    yum -y update && \\\r\n    yum -y install openssl-devel zlib-devel libffi libffi-devel wget && \\\r\n    wget https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tar.xz && \\\r\n    tar -xJf Python-$PYTHON_VERSION.tar.xz && \\\r\n    cd Python-$PYTHON_VERSION && \\\r\n    ./configure && \\\r\n    make && \\\r\n    make install && \\\r\n    pip3 install --upgrade pip && \\\r\n    pip3 install tensorflow==2.0.0-rc1 tensorflow_probability==0.8.0-rc0 numpy falcon jsonschema\r\n```", "comments": ["I replicated the issue on colab with Tf 2.0.0.rc1. Please take a look at colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/cc35a1c57a228661157480b1ba307116/untitled150.ipynb). Thanks!", "I can reproduce this issue with the same setting. If I add keras.backend.clear_session() after each call to model.predict(), it is getting super slow.", "still a bug after upgrading to `tensorflow==2.0.0`", "@LuisSaybe Thank you for the repro case- I added some memory tracking to it and found a pretty clear linear increase in memory utilization of the model object you've defined. It does seem to be fixed in TF 2.1.0, so I guess I'll be waiting until that's released to move off of 1.14.\r\n![memoryleak](https://user-images.githubusercontent.com/16674595/68467202-65773c00-01db-11ea-907d-d15d133654b7.png)\r\n![memoryleak21](https://user-images.githubusercontent.com/16674595/68467208-6a3bf000-01db-11ea-97e5-c9cc00d5eb61.png)\r\n", "This issue has been fixed recently. @LuisSaybe Could you try tf-nightly to verify it? Thanks!", "@LuisSaybe Is this still an issue. I ran it in colab for 25000 iterations without any issue. Thanks!", "will check today, sorry was on vacation", "@yhliang2018 yes it is fixed now in tf-nightly, thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32500\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32500\">No</a>\n"]}, {"number": 32499, "title": "fix variable reference in docs", "body": "", "comments": []}, {"number": 32498, "title": "Implement test for avg. pool into TFLu - Int8", "body": "This patch adds tests in TFLu for average pooling Int8. Five more tests have been integrate into pooling_test.cc:\r\n\r\nThese tests will cover:\r\n\r\n- Different fused activation (Relu, Relu1 and Relu6)\r\n- Different output shapes\r\n- Different strides\r\n- Different paddings\r\n\r\nThe min and max value used for quantizing (-15.9375, 15.8130 respectively) have been chosen in order to have the dequantized output equal to the floating point one.\r\n\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32498) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32498) for more info**.\n\n<!-- ok -->", "Many thanks for the review!"]}, {"number": 32497, "title": "ModuleNotFoundError: No module named 'official.wide_deep'", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/estimators/linear\r\n\r\n## Description of issue (what needs changing):\r\n`https://github.com/tensorflow/models` has `wide_deep` in `official.r1.wide_deep` but the documentation says it's in `official.wide_deep`.\r\n", "comments": ["Issue fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32497\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32497\">No</a>\n"]}, {"number": 32496, "title": "[r2.0.0-rc1] Converting to TFLite format: <type == kTfLiteInt32/64 condition> was not true. ONE_HOT failed to prepare", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.2 LTS**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): **Source**\r\n- TensorFlow version (use command below): **2.0.0-rc1 commit 59bf33**\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0.130\r\n- GPU model and memory: NVidia 1080Ti / 11G\r\n\r\n**Describe the current behavior**\r\n\r\nConversion of TF2.0 function containing `reshape` and `one_hot` ops to TFLite format fails with the following RuntimeError. Source code of the program is listed in the `Code` section below.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-2-ec9775ede022> in <module>\r\n----> 1 run()\r\n\r\n~/mironov/hbtest/tflite_one_hot_bug_v2.py in run()\r\n     25\r\n     26   interpreter = tf.lite.Interpreter(model_path=model_file)\r\n---> 27   interpreter.allocate_tensors()\r\n     28\r\n     29 if __name__ == '__main__':\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter.py in allocate_tensors(self)\r\n    242   def allocate_tensors(self):\r\n    243     self._ensure_safe()\r\n--> 244     return self._interpreter.AllocateTensors()\r\n    245\r\n    246   def _safe_to_run(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)                                                                                         \r\n    104\r\n    105     def AllocateTensors(self):\r\n--> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n    107\r\n    108     def Invoke(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/one_hot.cc:141 op_context.indices->type == kTfLiteInt32 || op_context.indices->type == kTfLiteInt64 was not true.Node number 3 (ONE_HOT) failed to prepare.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n`converter.convert()`  finishes without errors\r\n\r\n**Code to reproduce the issue**\r\n\r\nFile: `tflite_one_hot_bug_v2.py`\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n@tf.function(input_signature=[\r\n    tf.TensorSpec(shape=[10,10], dtype=tf.int32, name='inp0')])\r\ndef model(inp0):\r\n  res = tf.reshape(inp0, [100])\r\n  res = tf.one_hot(res, depth=10)\r\n  print(res.shape)\r\n  return res\r\n\r\ndef run():\r\n  def _representative_dataset_gen():\r\n    for i in range(10):\r\n      yield [np.random.random_integers(0, 9, size=[10,10]).astype('int32')]\r\n\r\n  cfunc = model.get_concrete_function()\r\n  converter = tf.lite.TFLiteConverter.from_concrete_functions([cfunc])\r\n  converter.representative_dataset = _representative_dataset_gen\r\n  converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n  tflite_model = converter.convert()\r\n  model_file = \"/tmp/tflite_one_hot_bug_v2.tflite\"\r\n  open(model_file, \"wb\").write(tflite_model)\r\n\r\n  interpreter = tf.lite.Interpreter(model_path=model_file)\r\n  interpreter.allocate_tensors()\r\n\r\nif __name__ == '__main__':\r\n  run()\r\n```\r\n\r\n**Other info / logs**\r\nN/A", "comments": ["Hey! As you can see from the error, types other than int32/int64 are not currently supported yet for the one_hot op.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/one_hot.cc#L140\r\n\r\nIf it's a index type supported in TF, I suggest you take a look since we won't have immediate cycles to look into this.", "Closing per last comment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32496\">No</a>\n"]}]