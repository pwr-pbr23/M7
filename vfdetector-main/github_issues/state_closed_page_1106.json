[{"number": 20077, "title": "Some enhancement on S3 file system error processing", "body": "This fix is an enhancement on S3 file system error processing.\r\n\r\nAs TF's errors API allows taking  variadic arguments, this fix removed the unneeded string concatenation for error message generation.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Reviewer @jhseu: It has been 16 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20076, "title": "New function recursive_fill in image_ops", "body": "Implementation of a very common post-processing operation in image processing in a efficient manner embedded in a tensorflow graph. This function will iteratively fill the \"holes\" of a binary 2D or 3D image until it cannot fill more. It is also possible to set the sensitivity of the method by establishing a threhsold and a maximum number of iterations. For instance, let the threshold be 8 (or actually any number below or equal to 8)\r\n\r\nThen from this:\r\n0 0 0 0 0\r\n0 1 1 1 0\r\n0 1 0 1 0\r\n0 1 1 1 0\r\n0 0 0 0 0\r\n\r\nWe will obtain this:\r\n0 0 0 0 0\r\n0 1 1 1 0\r\n0 1 1 1 0\r\n0 1 1 1 0\r\n0 0 0 0 0", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Ping @jmlipman . Can you address the comments from @ringw?", "@caisq sorry, I actually fixed the first one and I'm stuck in the second one that refers to give the function enough flexibility to handle batches of images. I understand that that's the standard but I can't come up with a way to make it efficiently and I've tried to add another while loop wrapping the already existing while loop to iterate over each image, but since I don't have much free time at the moment I couldn't figure out how to do it.", "@jmlipman Hmm, I actually think conv2d and conv3d will already treat the 0th axis as a batch of independent images, so you shouldn't really need to modify the loop. You can treat a 4-D input as a batch of 3-D voxel masks if you just skip `array_ops.expand_dims(mask, 0)` in that case. Please also just copy `test_random_3d_fills` and create a 4-D test case!", "pinging PR author @jmlipman ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 20075, "title": "error in running a LSTM programme", "body": "i am writing a basic LSTM program for text generation. when i am writing line:\r\nmodel.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\r\nfollowing error occurring, may i know what is wrong with this:\r\npython /home/srinath/char_gen.py \r\nUsing TensorFlow backend.\r\nTotal Characters 144413\r\nTotal Vocab 45\r\nTotal Patterns: 144313\r\nTraceback (most recent call last):\r\n  File \"/home/srinath/char_gen.py\", line 63, in <module>\r\n    model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py\", line 166, in add\r\n    layer(x)\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 500, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 460, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 2112, in call\r\n    initial_state=initial_state)\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 609, in call\r\n    input_length=timesteps)\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2957, in rnn\r\n    maximum_iterations=input_length)\r\nTypeError: while_loop() got an unexpected keyword argument 'maximum_iterations'\r\nERROR:tensorflow:==================================\r\nObject was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\r\n<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f75882d7da0>\r\nIf you want to mark it as used call its \"mark_used()\" method.\r\nIt was originally created here:\r\n['File \"/home/srinath/char_gen.py\", line 63, in <module>\\n    model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py\", line 166, in add\\n    layer(x)', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 500, in __call__\\n    return super(RNN, self).__call__(inputs, **kwargs)', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 460, in __call__\\n    output = self.call(inputs, **kwargs)', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 2112, in call\\n    initial_state=initial_state)', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 609, in call\\n    input_length=timesteps)', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2877, in rnn\\n    input_ta = input_ta.unstack(inputs)', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 170, in wrapped\\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 413, in unstack\\n    indices=math_ops.range(0, num_elements), value=value, name=name)', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 170, in wrapped\\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 139, in _add_should_use_warning\\n    wrapped = TFShouldUseWarningWrapper(x)', 'File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 96, in __init__\\n    stack = [s.strip() for s in traceback.format_stack()]']\r\n==================================\r\nplease help me if possible.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Getting the same error. No idea why, please let me know if you find a solution", "Have I written custom code - i have written code as per the tutorial.\nOS Platform and Distribution - Ubuntu -16.04\nTensorFlow installed from - sources per tutorial.\nTensorFlow version - 1.2 GPU\nBazel version - Not installed\nCUDA/cuDNN version - 7.5\nGPU model and memory - 16gb memory\nExact command to reproduce -  N/A\n\nOn Sun, Jun 17, 2018 at 6:17 AM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> Thank you for your post. We noticed you have not filled out the following\n> field in the issue template. Could you update them if they are relevant in\n> your case, or leave them as N/A? Thanks.\n> Have I written custom code\n> OS Platform and Distribution\n> TensorFlow installed from\n> TensorFlow version\n> Bazel version\n> CUDA/cuDNN version\n> GPU model and memory\n> Exact command to reproduce\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20075#issuecomment-397846971>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AXgS2sgOOJUTuozeMWM-2B-7Dli3xqT4ks5t9ac-gaJpZM4UqevE>\n> .\n>\n\n\n\n-- \nSri Nath Dwivedi\nAsstt. Professor\n(Computer Science & Engg.)\nDr. Ambedkar Institute of Technology for Handicapped, U.P. Kanpur\n", "Use tensorflow gpu 1.4.1 with keras 2.1.5, CUDA 8, CUDNN 7\r\nIf your GPU is not compatible with CUDA 8 just try using keras 2.1.5, it solved my problem!", "i don't have GPU but have a laptop with NVIDIA graphic card.\r\nwhen i tried to upgrade tensorflow==1.4.1 from previous version 1.2.0 after that during running a programme following problem occurred:\r\npython /home/srinath/practice_LSTM.py Traceback (most recent call last):\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/srinath/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/srinath/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcudnn.so.6: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/srinath/practice_LSTM.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/srinath/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/srinath/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/srinath/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcudnn.so.6: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nsrinath@srinath-HP-Pavilion-Notebook:~$ python /home/srinath/char_gen.py Traceback (most recent call last):\r\n", "in the mentioned site, i tried to understand the problem, but i am not getting what should i do. in that site following thing is relevant, so should i change all things means CUDA, CUDnn etc.\r\n\r\ntensorflow_gpu-1.4.0 | GPU | 2.7, 3.3-3.6 | GCC 4.8 | Bazel 0.5.4 | 6 | 8\r\n-- | -- | -- | -- | -- | -- | --\r\n\r\n\r\n", "Which nvidia graphics card do you have? A graphics card is a GPU.\r\nYou need to install tensorflow-gpu on your system to utilize it\r\nAccording to your graphics card you must install a specific version of CUDA and CUDNN (cudnn version is wrong that's why the new error)", "i have installed tensorflow 1.2 and it is working well, CUDA8.0GA2 and CUDnn8.0\r\nNVIDIA driver version: 384.130\r\nNVIDIA server string:TheX.orgFoundation\r\nNVIDIA serverversion:11.0\r\n\r\n\r\n", "It's working? Great", "now i installed following\r\ntensorflow-gpu==1.4.0 /1.4.1\r\nCUDA8.0\r\ncuDNN6.08 because in cudann site there was no option of cuDNN7 with CUDA8.0, there were 2 options cuDNN5.1 with CUDA8 or 6 so i installed 6, then in running every programme following warning shows:\r\n/home/snd/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\nhow to resolve this\r\nthis error does not appear in tensorflow-gpu==1.2\r\n", "Is this only a warning or an error? If it's a warning I think you could ignore it as you probably won't be using fast tensor utility ", "okay thanks, it is only warning\r\nthanks for support\r\n", "Happy to help!", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "problem solved\n\nOn Mon, Jul 9, 2018 at 12:07 AM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> Nagging Assignee @shivaniag <https://github.com/shivaniag>: It has been\n> 14 days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20075#issuecomment-403306860>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AXgS2h_KU87uv_D3ANqKflOl1Lc9VbwMks5uElFygaJpZM4UqevE>\n> .\n>\n\n\n\n-- \nSri Nath Dwivedi\nAsstt. Professor\n(Computer Science & Engg.)\nDr. Ambedkar Institute of Technology for Handicapped, U.P. Kanpur\n"]}, {"number": 20074, "title": "Colocation bug for distributed training(NMT model)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.6.1\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**: \r\n9/7\r\n- **GPU model and memory**:\r\nTitanXP/ 12G\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI saw the lines in [NMT](https://github.com/tensorflow/nmt/blob/365e7386e6659526f00fa4ad17eefb13d52e3706/nmt/model_helper.py#L310). If embedding parameter is set in parameter server, embedding_lookup must follow the device placement but the result is opposite(embedding parameter follows embedding_lookup). I think the placement of colocation group in `placer.cc` is wrong so I tried to fix it but it's complicated. Do you have any plan to fix it? or Can you give a suggestion?\r\n\r\n### Source code / logs\r\n", "comments": ["@azaks2 any thoughts?", "Any easy way to reproduce the problem? Also what are the downsides for your use case to have > 1 partitions. ", "Nagging Assignee @azaks2: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm building an automatic parallelization module on TensorFlow.(https://github.com/snuspl/parallax). However,  users must remember the number of partitions > 1 for NMT model. I want to provide an automatic way that users don't need to care about additional stuff for distributed environment.  Even though I assigned the embedding parameters on a PS, it goes away to worker by the default placer. Is there any way that I can force to assign the parameters to a PS? ", "model_device_fn = tf.train.replica_device_setter(10)\r\n with tf.device(model_device_fn):\r\n   ...\r\n   tf.get_variable()\r\n\r\nshould work. I also tried tf.fixed_size_partitioner(1) and it worked with nmt (tf.get_default_graph() shows expected devices). ", "Yes, the device in tf.get_default_graph() is correct but the device placement is changed when running the graph in a session. I checked it using log_device_placement=True for session config. The problem is controlling the final device placement using tf.device() API is impossible in this case.", "I poked a little more. I could not reproduce the problem. Meaning I always get lookup and the embedding var to be on ps. I did however observed the difference in the placement of dynamic_seq2seq/encoder/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 op, which moves from worker to ps whenever we have 1 partition. The reason is that the embedding var is on ps, lookup is collocate with the embedding var, and the lookup result is fed to the tensor array op that wants to be collocated with its input (this can be a performance bug on its own). When num_partitions > 1 there is an intermediate node after the lookup (namely ParallelDynamicStitch) that has no collocation constraints and is placed on the worker. So now tensor array ops can be on the worker and lookup on ps.\r\n\r\nAgain, I did not witness with tf.device assignments being ignored though internally added collocation constraints can be very intuitive and potentially harmful. To keep things more consistent between having 1 partition or more I would try wrapping embedding lookup with tf.identity. Indeed I am thinking to change embedding_lookup op itself to just always do it on its own.", "First of all, thank you for digging the problem. I met the problem when training the below German model.\r\n`python -m nmt.nmt \\\r\n    --src=de --tgt=en \\\r\n    --ckpt=/path/to/checkpoint/translate.ckpt \\\r\n    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \\\r\n    --out_dir=/tmp/deen_gnmt \\\r\n    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \\\r\n    --inference_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \\\r\n    --inference_output_file=/tmp/deen_gnmt/output_infer \\\r\n    --inference_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en` \r\n\r\nI can wrap embedding lookup with `tf.Identity` as you suggested but I'm still not sure it's a general solution because collocation constraints and `tf.device` are always stuck together. In my case, I spent for a long time to find the reason for the failure of the distributed training. \r\nHowever, if TensorFlow prioritizes internal collocation constraints more than user device placement by `tf.device` as its policy, maybe I must follow the policy and find another solution.", "> I can wrap embedding lookup with tf.Identity\r\nI did that already and the submitted the code . So there should no longer be any asymmetry between 1 and many partitions. I would not be surprised if  your code works now. \r\n> In my case, I spent for a long time to find the reason for the failure of the distributed training.\r\nIt does seem a common occurrence. Note The graphdef has all collocation constraints explicitly mentioned which can be useful for debugging.\r\n\r\n>However, if TensorFlow prioritizes internal collocation constraints more \r\nI do not think we do. If device placement and collocation cant be reconciled the placement should fail.\r\n\r\n\r\n\r\n\r\n", "Thank you. I'll try with the new code. "]}, {"number": 20073, "title": "Can't load hdf5 files when using tf.keras.SequentialModel.add() and ModelCheckpoint()", "body": "Bug.  Compiled Tensorflow 1.8 and 1.9rc0 both fail. Linux Mint 18.2.  Python-3.5.2  CUDA 9.0/CuDNN 7.0\r\n\r\nHave I written custom code: I've attached a file\r\nOS Platform and Distribution: Linux Mint 18.2\r\nTensorFlow installed from: source\r\nTensorFlow version: 1.8 and 1.9rc0\r\nBazel version: 0.13.1\r\nCUDA/cuDNN version: 9.0/7.0\r\nGPU model and memory: Nvidia 1080Ti\r\nExact command to reproduce: run the attached script.\r\n\r\n\r\nWhen using tf.keras.SequentialModel to build a network, and using tf.keras.callbacks.ModelCheckpoint to save the model/weights during training, the saved hdf5 file cannot be loaded into the same model.\r\n\r\nI believe the same problem was observed in keras-1.0\r\nSee here: https://github.com/keras-team/keras/issues/2281\r\nAnd reportedly fixed here: https://github.com/keras-team/keras/commit/1206120d1084cbe45dc2876f002cb572a97e3844\r\n\r\nI've attached a minimal script which will readily reproduce the problem here: [save-bug.txt](https://github.com/tensorflow/tensorflow/files/2107894/save-bug.txt)\r\n\r\nThe output error I receive when attempting to load the model weights into the same model which I saved them from is:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./save-bug.py\", line 38, in <module>\r\n    predict()\r\n  File \"./save-bug.py\", line 31, in predict\r\n    model.load_weights('test.hdf5');\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/network.py\", line 1190, in load_weights\r\n    saving.load_weights_from_hdf5_group(f, self.layers)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/saving.py\", line 697, in load_weights_from_hdf5_group\r\n    ' layers.')\r\nValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "updated description", "@fchollet Can you take a look at this?", "Got the same error while trying to load saved Sequential model. Tensorflow built from r1.9 branch yesterday.", "I get a similar error-message in [this notebook](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb) which worked perfectly with TensorFlow v. 1.4.0 and Keras v. 2.0.8-tf. I have just upgraded to TensorFlow 1.9 and Keras 2.1.6-tf and now I get an error.\r\n\r\nI have done an internet search and found many people experiencing different variations of this bug.\r\n\r\nThis is the complete error message I get when calling `load_model()` in Keras:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-38-53ab8ddbb1a4> in <module>()\r\n----> 1 model = load_model(path_best_model)\r\n\r\n~/anaconda3/envs/tf-gpu-skopt/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py in load_model(filepath, custom_objects, compile)\r\n    230 \r\n    231     # set weights\r\n--> 232     load_weights_from_hdf5_group(f['model_weights'], model.layers)\r\n    233 \r\n    234     if compile:\r\n\r\n~/anaconda3/envs/tf-gpu-skopt/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py in load_weights_from_hdf5_group(f, layers)\r\n    730                      'containing ' + str(len(layer_names)) +\r\n    731                      ' layers into a model with ' + str(len(filtered_layers)) +\r\n--> 732                      ' layers.')\r\n    733 \r\n    734   # We batch weight value assignments in a single backend call\r\n\r\nValueError: You are trying to load a weight file containing 7 layers into a model with 0 layers.\r\n```", "I have a similar error with TF 1.9 and some models (containing Lambda and Conv2D layers), but other models seem to load correctly", "The weights are in the HDF5 file, and are saved correctly at https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/keras/engine/saving.py#L145 (I added logs in the load / save) but when loading from https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/keras/engine/saving.py#L716 they appear empty\r\n\r\nI guess it comes from the fact that the model is not initialized with placeholder weights when creating at https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/keras/engine/saving.py#L229 so the loader assumes that the layers don't have weights\r\n", "Same for me, tf.keras models saved and loaded fine, when I added\r\na reshape layer they still saved but I can't load them anymore.\r\n\r\nModel:\r\n`model.add(tf.keras.layers.Reshape([76832]))`\r\n\r\nOf course this is just flattening the input and someone can argue that tf.keras.layers.Flatten() does the same thing, but this is just needed because of a TensorRT Bug so reshape is my only option here.\r\n\r\nHope that somebody can take a closer look into that.\r\n", "This is still happening with tf 1.11.0 keras 2.1.6-tf.\r\n\r\nRepro:\r\n\r\n```python\r\nFN = 'tmp.model.h5'\r\nmodel = keras.Sequential([\r\n  keras.layers.InputLayer(input_shape=(8, 8, 1)),\r\n  keras.layers.Conv2D(1, 5),\r\n])\r\nkeras.models.save_model(model, FN)\r\nkeras.models.load_model(FN)\r\n```\r\n\r\nThis errors out with:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./saveload.py\", line 24, in <module>\r\n    keras.models.load_model(FN)\r\n  File \"tensorflow/python/keras/engine/saving.py\", line 233, in load_model\r\n    load_weights_from_hdf5_group(f['model_weights'], model.layers)\r\n  File \"tensorflow/python/keras/engine/saving.py\", line 784, in load_weights_from_hdf5_group\r\n    ' layers.')\r\nValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.\r\n```\r\n\r\nIt appears to work without the InputLayer, e.g.:\r\n\r\n```python\r\nmodel = keras.Sequential([                                                        \r\n  keras.layers.Conv2D(1, 5, input_shape=(8, 8, 1)),                               \r\n])                                                                                \r\n```", "In the working case, `model.layers[0].get_config()` has `'batch_input_shape': (None, 8, 8, 1)`\r\n\r\nIn the non working case, `batch_input_shape` is absent.", "For those still struggling, here is my workaround - instead of saving whole model, i just save and load weights. Obviously in this case you need exact same model defined in learning and inference code.\r\nE.g.:\r\n```python\r\ndef makeModel(env, shape, fname, lr):\r\n\r\n    # does not work due to https://github.com/tensorflow/tensorflow/issues/20073\r\n    # if os.path.isfile(fname):\r\n    #    print(\"Loading model\")\r\n    #    return load_model(fname)\r\n\r\n    m = Sequential()\r\n    m.add(InputLayer(input_shape=shape+(4,)))\r\n    m.add(Conv2D(32, kernel_size=8, strides=4, activation='relu'))\r\n    m.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))\r\n    m.add(Conv2D(64, kernel_size=3, activation='relu'))\r\n    m.add(Dense(512, activation='relu', activity_regularizer=\"l2\"))\r\n    m.add(Flatten())\r\n    m.add(Dense(env.action_space.n, activation=\"linear\"))\r\n    m.compile(loss=huber_loss, optimizer=tf.keras.optimizers.RMSprop(lr))\r\n    if os.path.isfile(fname):\r\n       m.load_weights(fname)\r\n    return m\r\n```", "This is fixed with latest tf-nightly build version '1.15.0-dev20190808'. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=20073\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=20073\">No</a>\n"]}, {"number": 20071, "title": "Tensorflow failed when build with CMake", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 7\r\n- **TensorFlow installed from (source or binary)**:Source\r\n- **TensorFlow version (use command below)**:1.8\r\n- **Python version**: Anaconda 5.1.0 (Python 3.6 64-bit)\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:VS2015\r\n- **CUDA/cuDNN version**:9.0\r\n- **GPU model and memory**:GeFore GTX 1080\r\n- **Exact command to reproduce**:N/A\r\n\r\n\r\n### Describe the problem\r\n I compile according to the document and report an error when \"MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj\"is executed: \r\n\r\nCUSTOMBUILD : error : RPC failed;curl 18 transfer closed with outstanding read data remaining [E:\\Tensorflow\\tensorflow\\tensorflow\r\ncontrib\\cmake\\build\\grpc.vcxproj]\r\n\r\n[screenshot](https://note.youdao.com/yws/public/resource/103710ae2f5c53643adf49949dd427b2/xmlnote/B89834353E21430BBB709FD8777C0414/3698)\r\ncould you please help take a look at this?Thanks!\r\n\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20070, "title": "tf.set_shape bug for 3D data space?", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 9.0/cuDNN 7.1.4\r\n- **GPU model and memory**: NVIDIA Tesla K80 on GCP\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nHave bugs in TF_GraphSetTensorShape_wrapper function when dealing with high-dimensional shapes.\r\n\r\n<img width=\"858\" alt=\"error1\" src=\"https://user-images.githubusercontent.com/19757981/41492650-59506ac0-70f8-11e8-95dd-9f0d3c29d071.png\">\r\n\r\nIn my code, the images_in is a tensor which represents a 3D model. I use tf.set_shape but after I print out the result, the shape is wrong. I set num_channels as 1 and it also shows 1 when I print it out. But the second shape of the images_in is 8 which not equals to num_channels. \r\n\r\n![error4](https://user-images.githubusercontent.com/19757981/41492752-1109bd7e-70f9-11e8-80bf-926b8582a223.png)\r\n\r\nI looked into tf.set_shape() below and found if I use unknown shape, it will call a function called: TF_GraphSetTensorShape_wrapper. \r\n\r\n<img width=\"505\" alt=\"error2\" src=\"https://user-images.githubusercontent.com/19757981/41492836-a849466e-70f9-11e8-9ce1-7b4af79ccb66.png\">\r\n\r\nBut I cannot see what this function does exactly since it might be a c-embedded function.\r\n\r\n<img width=\"742\" alt=\"error3\" src=\"https://user-images.githubusercontent.com/19757981/41492962-a1e25d32-70fa-11e8-9271-4f38627f61c3.png\">\r\n\r\nI have tried this function in 2D data space if I use images_in.set_shape([None, num_channels, resolution, resolution]) and the shapes are correct.\r\n\r\nSo I assume there might be something wrong with TF_GraphSetTensorShape_wrapper function when dealing with high-dimensional shapes? Or does it hard code somewhere and cause the problem?\r\n\r\nThanks.\r\n", "comments": ["Can you post a self-contained example the reproduces the issue? The example does appear to be a bug, but it's difficult to reproduce without a self-contained example.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20068, "title": "streaming data from google cloud storage to tensorflow input pipeline is slow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04 and MacOS High Sierra 10.13.4\r\n- **TensorFlow installed from (source or binary)**: docker and virtualenv\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: N/A\r\n\r\nDescribe the problem:\r\nMy data is stored on google cloud storage (millions of small audios); my tensorflow input pipeline sample code is below.\r\n \r\ntf.data.Dataset.from_generator(generator_filename_func).\\\r\nmap(signal_processing_func, num_parallel_calls=CPU_NUM).\\\r\nbatch(100).\\\r\nmake_one_shot_iterator()\r\n\r\nThe pipeline starts with a generator (a generator that generates audio filenames from google cloud storage); then run signal processing with map function and multithread. \r\n\r\nThe input pipeline is fast when reading data from local disk (download audios to cloud compute engine VM), however, reading data from mounted folder (mount google cloud storage to the folder with gcsfuse command) is slow (about 5-10 times slower). I know google cloud supports using python multithread to read data, but it does not seem to be compatible with tensorflow input pipeline. I also tried google.cloud.storage to stream data, it's also very slow for small files. How should I stream data from google cloud storage to tensorflow input pipeline with low latency? What tools/library is recommended and compatible with tensorflow input pipeline?\r\n\r\nThe other weird thing is that the pipeline speed does not change much as I add more CPU and SSD. I believe the bottleneck is reading files, how can I optimize reading many small files in the tensorflow input pipeline?\r\n\r\n", "comments": ["Reassigning this to @saeta since he has vastly more experience with GCS than I do (and he may have a better suggestion for how to achieve this using other GCP services).", "Nagging Assignee @saeta: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @hbi-tianyi !\r\n\r\nThere are a number of things that can be done to optimize. (1) I highly recommend batching up your training samples into larger TFRecord files (e.g. 100-300 MB files). GCS has relatively high latency compared to local disk, but extremely high throughput. (2) Make sure you're using Parallel Interleave to read from multiple TFRecord files simultaneously, and also `.prefetch(tf.contrib.data.AUTOTUNE)` as the final step of your `tf.data.Dataset`. (3) If you still have performance problems, or don't want to batch up your data into large TFRecord files, you can consider using the new TensorFlow / Cloud Bigtable integration, which can achieve even higher performance than Cloud Storage (GCS).\r\n\r\nIf you'd like an example for a high performance input pipeline, check out the [Cloud TPU example for ResNet-50](https://github.com/tensorflow/tpu/tree/master/models/official/resnet). That input pipeline is able to drive a full Cloud TPU, and loads data from GCS at over 4 Gbps!\r\n\r\nIf you're still running into issues, I recommend checking out [Training Performance](https://www.youtube.com/watch?v=SxOsJPaxHME&vl=en) which has an emphasis on input pipelines, and try using the [Cloud TPU profiling tools](https://cloud.google.com/tpu/docs/cloud-tpu-tools#profile_tab) which have good instrumentation and visualization of input pipelines and on-device execution.\r\n\r\nHope this helps!\r\n-Brennan\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "I have a similar issue. I have many small files that I read in a Dataset.map function. I expected to be able to mask GCS latency by setting num_parallel_calls high enough, but I don't get the expected throughput. Am I missing something?", "Hi @jpambrun If you cannot change your file format to have larger bundle files, I recommend using `tf.contrib.data.parallel_interleave`, as that has some nice prefetching knobs that can help tune your input pipeline. As an alternative, [Cloud Bigtable](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/bigtable/README.md) can work very well for many small files that are stored as individual rows. That said, in my experience tuning input pipelines on GCS, the fixed per-file overheads are large enough (and TPUs (and 8xV100 GPUs) fast enough across most models) that you often need to use larger files that can be loaded in larger sequential reads.\r\n\r\nAll the best,\r\n-Brennan", "> I have a similar issue. I have many small files that I read in a Dataset.map function. I expected to be able to mask GCS latency by setting num_parallel_calls high enough, but I don't get the expected throughput. Am I missing something?\r\n\r\nI used python multiprocessing/threading to solve the problem. I found map function of dataset api works very well on large tfrecord files; for small files, I found python multiprocessing is faster (you can also push more threads to get better results if hardware is not the bottleneck).", "Thanks for the insightful response. \r\n\r\nI will look into bundling images into records, but I still think it could/should have better performance. In node, I could go through 100s of these images per second from GCS on a single thread with little overhead. I find the parallel_interleave proposition hard to understand as it maps datasets to datasets and I already have a dataset with filenames and labels.\r\n\r\nIn essence, I expected (like the OP I think) that I could read, decode and preprocess images in a Dataset.map() efficiently when configured with a high thread count. At any given moment many threads would be locked in IO due to GCS latency, but others would be hard at work decoding and preprocessing thus masking the latency completely. \r\n\r\nLooking at parallel_interleave documentation, I can see that is allows to return results out of order and that is missing from map, but I cannot figure out how to use it and I can't seem to find good example.", "> Hi @jpambrun If you cannot change your file format to have larger bundle files, I recommend using `tf.contrib.data.parallel_interleave`, as that has some nice prefetching knobs that can help tune your input pipeline. As an alternative, [Cloud Bigtable](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/bigtable/README.md) can work very well for many small files that are stored as individual rows. That said, in my experience tuning input pipelines on GCS, the fixed per-file overheads are large enough (and TPUs (and 8xV100 GPUs) fast enough across most models) that you often need to use larger files that can be loaded in larger sequential reads.\r\n> \r\n> All the best,\r\n> -Brennan\r\n\r\nI did a little bit research on tf.contrib.data.parallel_interleave, it seems it works mostly on tfrecord and text data. I tried to use it for audio data, it did not work, not sure if I used it correctly. Is there anyway to use it for audio or image data? If so, would you please share a naive example? Thank you very much!", "> Hi @hbi-tianyi !\r\n> \r\n> There are a number of things that can be done to optimize. (1) I highly recommend batching up your training samples into larger TFRecord files (e.g. 100-300 MB files). GCS has relatively high latency compared to local disk, but extremely high throughput. (2) Make sure you're using Parallel Interleave to read from multiple TFRecord files simultaneously, and also `.prefetch(tf.contrib.data.AUTOTUNE)` as the final step of your `tf.data.Dataset`. (3) If you still have performance problems, or don't want to batch up your data into large TFRecord files, you can consider using the new TensorFlow / Cloud Bigtable integration, which can achieve even higher performance than Cloud Storage (GCS).\r\n> \r\n> If you'd like an example for a high performance input pipeline, check out the [Cloud TPU example for ResNet-50](https://github.com/tensorflow/tpu/tree/master/models/official/resnet). That input pipeline is able to drive a full Cloud TPU, and loads data from GCS at over 4 Gbps!\r\n> \r\n> If you're still running into issues, I recommend checking out [Training Performance](https://www.youtube.com/watch?v=SxOsJPaxHME&vl=en) which has an emphasis on input pipelines, and try using the [Cloud TPU profiling tools](https://cloud.google.com/tpu/docs/cloud-tpu-tools#profile_tab) which have good instrumentation and visualization of input pipelines and on-device execution.\r\n> \r\n> Hope this helps!\r\n> -Brennan\r\n\r\nThank you very much for your suggestion!\r\n(1) that's what I did. I used python threading/multiprocessing to write large tfrecord (2) same question as I asked in previous post (3) Is there a better way to copy certain small files from GCS to VM faster? If so, how can I use it? \r\nOnce again, thank you very much for your suggestion!\r\n", "@hbi-tianyi did you come up with a good solution for your data pipeline? We are having similar problems and also work on audio (reading random audio chunks)"]}, {"number": 20067, "title": "Fail to find the dnn implementation.", "body": "Last keras, tensorflow.\r\nUsing anaconda with jupiter notebook.\r\nIt sometimes breaks then I use CuDNNGRU.\r\nRestarting jupiter helps.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nUnknownError: Fail to find the dnn implementation.\r\n\t [[Node: cu_dnngru_1/CudnnRNN = CudnnRNN[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\"gru\", seed=87654321, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](cu_dnngru_1/transpose, cu_dnngru_1/ExpandDims_1, cu_dnngru_1/Const_1, cu_dnngru_1/concat)]]\r\n\t [[Node: loss/mul/_73 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_618_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'cu_dnngru_1/CudnnRNN', defined at:\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\r\n    self.io_loop.start()\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\asyncio\\base_events.py\", line 421, in run_forever\r\n    self._run_once()\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\asyncio\\base_events.py\", line 1431, in _run_once\r\n    handle._run()\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\r\n    handler_func(fileobj, events)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-982a486f943a>\", line 44, in <module>\r\n    i = CuDNNGRU(128)(i)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 500, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 460, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py\", line 90, in call\r\n    output, states = self._process_batch(inputs, initial_state)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py\", line 297, in _process_batch\r\n    is_training=True)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\", line 1552, in __call__\r\n    seed=self._seed)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\", line 941, in _cudnn_rnn_no_input_c\r\n    direction, dropout, seed, name)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\", line 855, in _cudnn_rnn\r\n    name=name)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py\", line 143, in cudnn_rnn\r\n    is_training=is_training, name=name)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3309, in create_op\r\n    op_def=op_def)\r\n  File \"e:\\neuro_projects\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1669, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nUnknownError (see above for traceback): Fail to find the dnn implementation.\r\n\t [[Node: cu_dnngru_1/CudnnRNN = CudnnRNN[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\"gru\", seed=87654321, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](cu_dnngru_1/transpose, cu_dnngru_1/ExpandDims_1, cu_dnngru_1/Const_1, cu_dnngru_1/concat)]]\r\n\t [[Node: loss/mul/_73 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_618_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Windows 10\r\ntensorflow tf-nightly-gpu\r\ncuda 9 cudnn 7\r\nnvidia gtx 760\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport os\r\nfrom time import *\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\n\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, Dense, Reshape, Dropout, SimpleRNN, CuDNNGRU, CuDNNLSTM, LeakyReLU\r\nfrom keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau\r\n\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom sklearn.utils import shuffle\r\n\r\nwork_dir = r'E:\\neuro_projects\\morda'\r\n\r\ndata = pd.read_csv(os.path.join(work_dir, 'train.tsv'), delim_whitespace=True, names=[str(i) for i in range(101)])\r\ndata = shuffle(data)\r\nscaler = MinMaxScaler()\r\ndata = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\r\nx = data[data.columns[:-1]]\r\ny = data['100']\r\nx_train = x[:8000]\r\nx_test = x[8000:]\r\ny_train = y[:8000]\r\ny_test = y[8000:]\r\n\r\nlr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9,\r\n                               cooldown=1,\r\n                               patience=10,\r\n                               verbose=1,\r\n                               min_lr=1e-8)\r\nnan = TerminateOnNaN()\r\nstop = EarlyStopping(monitor='val_loss', min_delta=0, patience=200, verbose=1, mode='auto')\r\ncallbacks = [lr_reducer, nan, stop]\r\n#test = pd.read_csv(os.path.join(work_dir, 'test.tsv'), delim_whitespace=True, names=[str(i) for i in range(101)])\r\n#x_test = test[test.columns[:-1]]\r\n#y_test = test['100']\r\n\r\ndef test_model():\r\n    y_pred = model.predict(x_test)\r\n    y_pred = y_pred/scaler.scale_[-1]\r\n    from sklearn.metrics import mean_absolute_error\r\n    print(mean_absolute_error(data['100'][8000:], y_pred))\r\n    \r\n\r\ninp = Input(shape=(x_test.shape[1],))\r\ni = inp\r\ni = Reshape((1, x_test.shape[1]))(i)\r\n\r\ni = CuDNNGRU(256)(i)\r\ni = LeakyReLU()(i)\r\n#i = Dropout(0.3)(i)\r\n\r\ni = Dense(1)(i)\r\n\r\nmodel = Model(inp, i)\r\n\r\nmodel.compile(loss='mae', optimizer='adam')\r\n#model.summary()\r\ntimer = time()\r\nhistory = model.fit(x_train, y_train,\r\n                      epochs=10000,\r\n                      batch_size=64,\r\n                      verbose=2,\r\n                      validation_data=(x_test, y_test),\r\n                      callbacks=callbacks\r\n                   )\r\nplt.plot(history.history['loss'][10:])\r\nplt.plot(history.history['val_loss'][10:])\r\nplt.title('model loss')\r\nplt.ylabel('loss')\r\nplt.xlabel('epoch')\r\nplt.legend(['train', 'test'], loc='upper left')\r\nplt.show()\r\nprint(history.history['loss'][-1], history.history['val_loss'][-1])\r\nprint((time()-timer)/60)\r\ntest_model()", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi, there. I also met the same problem with CuDNN GRU.\r\n\r\nI know that working example of cudnn lstm exists (below link), and modified it to use gru instead of lstm.\r\nhttps://gist.github.com/protoget/9b45881f23c96e201a90581c8f4b692d\r\n\r\n```\r\n// tested gru version\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nshape = [2, 2, 2]\r\nn_cell_dim = 2\r\n\r\nclass TestCuDNN(object) :\r\n    def init_vars(self, sess):\r\n      sess.run(tf.global_variables_initializer())\r\n\r\n\r\n    def train_graph(self):\r\n      with tf.Graph().as_default(), tf.device('/gpu:0'):\r\n        with tf.Session() as sess :\r\n          is_training = True\r\n\r\n          inputs = tf.random_uniform(shape, dtype=tf.float32)\r\n\r\n          with tf.variable_scope('A', reuse=tf.AUTO_REUSE) as scope:\r\n              gru = tf.contrib.cudnn_rnn.CudnnGRU(\r\n                  num_layers=1,\r\n                  num_units=n_cell_dim,\r\n                  direction='bidirectional',\r\n                  dtype=tf.float32)\r\n              gru.build(inputs.get_shape())\r\n              outputs, output_states = gru(inputs, training=is_training)\r\n\r\n          with tf.variable_scope('B', reuse=tf.AUTO_REUSE) as scope:\r\n              gru2 = tf.contrib.cudnn_rnn.CudnnGRU(\r\n                  num_layers=1,\r\n                  num_units=n_cell_dim,\r\n                  direction='bidirectional',\r\n                  dtype=tf.float32)\r\n              gru2.build(inputs.get_shape())\r\n              outputs2, output_states2 = gru2(inputs, training=is_training)\r\n\r\n          with tf.device('/cpu:0'):\r\n            saver = tf.train.Saver()\r\n\r\n          self.dump_graph('before save')\r\n        \r\n          self.init_vars(sess)\r\n          saver.save(sess, '/tmp/cudnn_gru_test')\r\n\r\n\r\n    def inf_graph(self):\r\n      with tf.Graph().as_default(), tf.device('/cpu:0'):\r\n        with tf.Session() as sess:\r\n          inputs = tf.random_uniform(shape, dtype=tf.float32)\r\n          with tf.variable_scope('A', reuse=tf.AUTO_REUSE) as scope:\r\n                outputs = self.do_inference(inputs)\r\n\r\n          with tf.variable_scope('B', reuse=tf.AUTO_REUSE) as scope:\r\n                outputs2 = self.do_inference(inputs)\r\n                \r\n          # debug graphs\r\n          g = tf.get_default_graph()\r\n          print('default tf graph :', g)\r\n          keys = g.get_all_collection_keys()\r\n          print('current name scope :', g.get_name_scope())\r\n          for key in keys :\r\n                print('all graph (', key, ')  :', g.get_collection(key))\r\n          print('current name scope :', g.get_name_scope())\r\n            \r\n          reader = tf.train.NewCheckpointReader('/tmp/cudnn_test')\r\n          for var_name in reader.get_variable_to_shape_map() :\r\n                print(var_name)\r\n   \r\n                \r\n          saver = tf.train.Saver()\r\n\r\n          saver.restore(sess, '/tmp/cudnn_gru_test')\r\n          print(sess.run([outputs, outputs2]))\r\n\r\n    def do_inference(self, inputs) :\r\n      single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(\r\n          n_cell_dim, reuse=tf.get_variable_scope().reuse)\r\n\r\n      gru_fw_cell = [single_cell() for _ in range(1)]\r\n      print('gru fw cell graph :', gru_fw_cell[0].graph)\r\n      gru_bw_cell = [single_cell() for _ in range(1)]\r\n      (outputs, output_state_fw,\r\n       output_state_bw) = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\r\n           gru_fw_cell,\r\n           gru_bw_cell,\r\n           inputs,\r\n           dtype=tf.float32,\r\n           time_major=True)\r\n      return outputs\r\n\r\n    def dump_graph(self, where) :\r\n        print('')\r\n\r\n        print('--- dumping tensorflow graph [', where, '] ---')\r\n        g = tf.get_default_graph()\r\n        print('default tf graph :', g)\r\n\r\n        # debug graphs\r\n        keys = g.get_all_collection_keys()\r\n        print('current name scope :', g.get_name_scope())\r\n        for key in keys :\r\n            print('all graph (', key, ')  :', g.get_collection(key))\r\n        print('') \r\n        print('')\r\n    \r\n    def main(self):\r\n        with tf.Graph().as_default() :\r\n          print('default graph :', tf.get_default_graph())\r\n          self.train_graph()\r\n        with tf.Graph().as_default() :\r\n          print('default graph :', tf.get_default_graph())\r\n          self.inf_graph()\r\n        print('finishes')  \r\n\r\n\r\n```\r\n\r\nThe result is as follows : \r\n\r\ndefault graph : <tensorflow.python.framework.ops.Graph object at 0x000001D9A04742E8>\r\n\r\n--- dumping tensorflow graph [ before save ] ---\r\ndefault tf graph : <tensorflow.python.framework.ops.Graph object at 0x000001D9A0474518>\r\ncurrent name scope : \r\nall graph ( trainable_variables )  : [<tf.Variable 'A/cudnn_gru/opaque_kernel:0' shape=<unknown> dtype=float32_ref>, <tf.Variable 'B/cudnn_gru/opaque_kernel:0' shape=<unknown> dtype=float32_ref>]\r\nall graph ( variables )  : [<tf.Variable 'A/cudnn_gru/opaque_kernel:0' shape=<unknown> dtype=float32_ref>, <tf.Variable 'B/cudnn_gru/opaque_kernel:0' shape=<unknown> dtype=float32_ref>]\r\nall graph ( saveable_objects )  : [<tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops.CudnnGRUSaveable object at 0x000001D9E5472390>, <tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops.CudnnGRUSaveable object at 0x000001D99E51F550>]\r\nall graph ( update_ops )  : []\r\n\r\n\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1277     try:\r\n-> 1278       return fn(*args)\r\n   1279     except errors.OpError as e:\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1262       return self._call_tf_sessionrun(\r\n-> 1263           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1264 \r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1349         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1350         run_metadata)\r\n   1351 \r\n\r\nUnknownError: Fail to find the dnn implementation.\r\n\t [[Node: A/cudnn_gru_1/CudnnRNNCanonicalToParams = CudnnRNNCanonicalToParams[T=DT_FLOAT, direction=\"bidirectional\", dropout=0, input_mode=\"linear_input\", num_params=12, rnn_mode=\"gru\", seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_layers, A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_units, A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_units, A/cudnn_gru_1/random_uniform, A/cudnn_gru_1/random_uniform_1, A/cudnn_gru_1/random_uniform_2, A/cudnn_gru_1/random_uniform_3, A/cudnn_gru_1/random_uniform_4, A/cudnn_gru_1/random_uniform_5, A/cudnn_gru_1/random_uniform_6, A/cudnn_gru_1/random_uniform_7, A/cudnn_gru_1/random_uniform_8, A/cudnn_gru_1/random_uniform_9, A/cudnn_gru_1/random_uniform_10, A/cudnn_gru_1/random_uniform_11, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n...\r\n", "I was also using Keras/Tensorflow inside Jupyter notebooks. The first notebook worked OK. The second gave the UnknownError: Fail to find the dnn implementation.\r\nAfter closing the first notebook (Save + Close/Halt), the second notebook started to work again.\r\nNote: both notebooks used the same CuDNNLSTM layer.", "\"CudnnCompatibleGRUCell. A GRU impl akin to tf.nn.rnn_cell.GRUCell to use along with tf.contrib.cudnn _rnn.CudnnGRU. The latter's params can be used by it seamlessly.\"\r\n--How can CudnnCompatibleGRUCell get weights and biases from CudnnGRU's params ???", "I encountered a similar issue as riklmr. I had to restart Jupyter Notebook to get the 2nd notebook's model to overcome this error. I rather do that at this point than upgrading versions of Keras/Tensorflow as recommended in other pages. Moving on with my life, for now...", "@riklmr : I have a similar issue. I have two processes using CuDNNLSTM. One process trains model on some data and another process predicts using a previously saved model. When i run both the process i get this error. When i change cudnnLSTM to LSTM cell it works fine. But i need to use CuDNNLSTM. Does anyone have any resolution.", "The solution here seemed to work\r\nhttps://stackoverflow.com/questions/54473254/cudnnlstm-unknownerror-fail-to-find-the-dnn-implementation\r\n\r\nadding this worked for me\r\n ```\r\nimport tensorflow as tf\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\r\n```\r\n\r\nSome other recommendations were to do a full install of CUDA including the display driver its shipped with, it may be your current driver is outdated", "@dmitrievanthony That worked for me for a while, but the same error came back even with growth enabled.\r\n\r\n**EDIT** OK, there was some other python process running that must have also been using the GPU. When I killed it everything was fine again."]}, {"number": 20066, "title": "[Intel-MKL] Support for N-D Transpose using MKL-DNN", "body": "This PR adds support for N dimensional transpose using MKL-DNN.\r\nSince MKL-DNN requires exception support enabled for compilation,\r\nwe create a new build rule for mkl_transpose_op, and thus the related\r\nchanges in other build files. Also, since we can eliminate MKL binary\r\nblob for Transpose, we add corresponding preprocessor macros around\r\ntranspose code.", "comments": ["Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi, when will this PR be merged into master?", "@nhasabni Thanks for your patience. Attempting to merge now."]}, {"number": 20065, "title": "[Intel MKL] Fixing MKL graph layout pass test", "body": "This PR fixes the MKL graph layout pass test which was failing because the order\r\nin which nodes in the graph are printed seems to have changed.", "comments": []}, {"number": 20064, "title": "Branch 200727545", "body": "Manual merge in \r\n```\r\ntensorflow/tools/api/generator/create_python_api.py\r\ntensorflow/tools/ci_build/install/install_pip_packages.sh\r\ntensorflow/tools/ci_build/install/install_python3.5_pip_packages.sh\r\ntensorflow/tools/ci_build/install/install_python3.6_pip_packages.sh\r\ntensorflow/tools/pip_package/BUILD\r\n```", "comments": ["@akshaym Can you take a look at the test failure in \"Ubuntu Python2\"? Looks like something related to the golden API files.", "Thanks @caisq. I believe it should be fixed now. ", "Great! Thanks, @akshaym "]}, {"number": 20063, "title": "Fix: DepthwiseConv2D fails when bias is enabled", "body": "Replaced `if self.bias` with `if self.use_bias`. bias is a tensor and using it as a bool will result in error.\r\nFixes #20023 ", "comments": []}, {"number": 20062, "title": "Memory leak using loss in Eager Execution", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Xubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.8.0-3211-g1aea422 1.9.0-rc0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.1 / 7.1\r\n- **GPU model and memory**: GeForce GTX 960M - 2GB\r\n- **Exact command to reproduce**: The following code:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nlayer = tf.keras.layers.Dense(5)\r\ninputs = tf.zeros([32, 100], tf.float32)\r\nlabels = tf.zeros([32], tf.int64)\r\n\r\nfor i in range(100000):\r\n  with tf.GradientTape() as tape:\r\n    logits = layer(inputs)\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\r\n```\r\n\r\n### Describe the problem\r\n\r\nI observe a memory leak when computing the loss using the code above. I also tried using other losses, such as `tf.losses.mean_squared_error` and `tf.losses.hinge_loss`, and I observed similar leaks. Using memory_profiler produces the following plot:\r\n![mplot_leak_entropy](https://user-images.githubusercontent.com/24420973/41473326-53606eb2-708f-11e8-9618-2f7696781828.png)\r\n\r\nI tried to use the workaround suggested by @allenlavoie in #19671, but it does not seem to work in this case.\r\n\r\n### Source code / logs\r\n\r\nI used pympler to track the memory usage, by adding it to the code:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom pympler.tracker import SummaryTracker\r\ntf.enable_eager_execution()\r\n\r\nlayer = tf.keras.layers.Dense(5)\r\ninputs = tf.zeros([32, 100], tf.float32)\r\nlabels = tf.zeros([32], tf.int64)\r\n\r\ntracker = SummaryTracker()\r\n\r\nfor i in range(100000):\r\n  with tf.GradientTape() as tape:\r\n    logits = layer(inputs)\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\r\n  if i % 1000 == 0:\r\n    tracker.print_diff()\r\n```\r\n\r\nAfter a few rounds, it starts to produce the following output:\r\n```bash\r\n                types |   # objects |   total size\r\n===================== | =========== | ============\r\n  <class 'EagerTensor |        1000 |    156.25 KB\r\n         <class 'list |           0 |      7.14 KB\r\n```\r\n\r\nSo it seems an EagerTensor is being created and not released at every iteration, which may be the cause of the leak.\r\n\r\nI thank @allenlavoie and @akshaym for solving my previous issues in #19385. I think this problem is related to those.", "comments": ["I see, this one is much simpler:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nlayer = tf.keras.layers.Dense(5)\r\ninputs = tf.zeros([32, 100], tf.float32)\r\nlabels = tf.zeros([32], tf.int64)\r\n\r\nfor i in range(100000):\r\n  tf.losses.sparse_softmax_cross_entropy(labels, inputs)\r\n  print(len(tf.get_default_graph().get_collection(tf.GraphKeys.LOSSES)))\r\n```\r\n\r\nYou can `tf.reset_default_graph()` to work around the issue. The fix will probably be to not add to a global collection. Thank you for the report!", "Thank you for the reply @allenlavoie . This workaround fixes the problem for the loss, but it seems to create a leak with `tf.contrib.summary.record_summaries_every_n_global_steps`. This code:\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nfor i in range(100000):\r\n  tf.reset_default_graph() \r\n  with tf.contrib.summary.record_summaries_every_n_global_steps(100):\r\n    pass\r\n```\r\nproduces the following memory plot:\r\n![mprof_summary_leak](https://user-images.githubusercontent.com/24420973/41495224-d8ae5a7a-70f8-11e8-88e0-0c22ef9c2c00.png)\r\n\r\nStrangely, this leak only happens if we reset the graph. If we remove `tf.reset_default_graph()` from the loop, there is no leak at all. Removing the summary from the loop also stops the leak, so it is not the reset operation which leaks. \r\n\r\nEDIT: replacing `tf.contrib.summary.record_summaries_every_n_global_steps` by `tf.contrib.summary.always_record_summaries`fixes the leak and the code also runs more than 10 times faster.", "Summaries may be another case related to https://github.com/tensorflow/tensorflow/issues/19671. I think they both create resources with unique names, so they'll both hit our kernel cache. You could probably reset the graph and set the random seed to work around them both.\r\n\r\nI am planning to fix the kernel caching issues (hopefully relatively soon), and I have a fix out for review on the losses collection issue. Thank you for the reports!"]}, {"number": 20061, "title": "Wide Deep model erased all my files?", "body": "I downloaded the models-master folder to my harddrive and ran the wide-deep.py script.  Next thing I know, all the files on my harddrive are wiped out.  Is there a script that deletes the directory? ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "U need to run a custom ROM drive id if you wanna have any luck with that... And evwrytime it will wipe ur drive .. With a ROM its secluded away on the drive to be left alone use backe partition code 17.773.76... It works nice if you run a version 7.1 or above..\r\n ", "Hi @mklebanamc, we are really sorry to hear that. The wide deep model used to clear out the directory set by the `--model_dir` flag, to allow training to start from a blank slate. We recently pushed out a change to clear the directory only when the user has specified a `--clean` flag. In general, Estimator expects free reign over the model directory. Again, sincere apologies that this happened."]}, {"number": 20060, "title": "quantized model throw exception: max_y must be larger than min_y", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nTensorflow docker image 1.7.1-devel-gpu\r\n- **TensorFlow installed from (source or binary)**:\r\nTensorflow docker image 1.7.1-devel-gpu\r\n- **TensorFlow version (use command below)**:\r\nTensorflow docker image 1.7.1-devel-gpu\r\n- **Python version**: \r\nPython 2.7 from Tensorflow docker image 1.7.1-devel-gpu\r\n- **Bazel version (if compiling from source)**:\r\nBazel from Tensorflow docker image 1.7.1-devel-gpu\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC from Tensorflow docker image 1.7.1-devel-gpu\r\n- **CUDA/cuDNN version**:\r\n9.0/7.5\r\n- **GPU model and memory**:\r\nGTX 1080Ti\r\n- **Exact command to reproduce**:\r\nSee below\r\n\r\n### Describe the problem\r\n\r\nInference with quantized graph(transform_graph tool with below command). \r\n```\r\n/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=../transformer/train/freeze_model_fixed.pb --out_graph=../transformer/train/quantized_graph.pb --inputs='inputs' --outputs='strided_slice_9,Select_1' --transforms='add_default_attributes remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes  sort_by_execution_order'\r\n```\r\n\r\n### Source code / logs\r\n\r\nThe exception:\r\n>>> InvalidArgumentError (see above for traceback): max_y must be larger than min_y.\r\n[[Node: while/add_1/eightbit = QuantizedAdd[T1=DT_QUINT8, T2=DT_QUINT8, Toutput=DT_QINT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](while/add_1_eightbit/while/Min/quantize, while/mul_1/eightbit/requantize, while/add_1_eightbit/while/Min/quantize:1, while/add_1_eightbit/while/Min/quantize:2, while/mul_1/eightbit/requantize:1, while/mul_1/eightbit/requantize:2)]] \r\n[[Node: while/body/parallel_0/body/decoder/layer_4/encdec_attention/multihead_attention/Reshape/_3663 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_12502...on/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](^_cloopwhile/truediv/_6)]]", "comments": ["Can you please share more details on your model to look into this further ?", "Hi @lmatt-bit,\r\n\r\nQuantization efforts have been prioritized in TensorFlow Lite so you should try to use that https://www.tensorflow.org/performance/quantization \r\n\r\nCurrently we support quantized training and conversion to TFLite, but in the next few weeks we will be releasing some easier to use tools that should be more robust than transform_graph. Will update this issue when that is ready.", "Thanks @suharshs . \r\n\r\nTried that link before, but there is another issue when use \"quantize_graph.py\". (\"No op named convert_gradient_to_tensor_cc661786 in defined operations.\") \r\n\r\nI thought transform_graph may be robust than it, because i can get the graph successfully. \r\n\r\nWaiting for the new tool to do the conversion. We are trying to do quantization on the tensor2tensor transformer NMT model. Maybe you can test the tool with that model. :)", "Hi lmatt-bit, that quantize_graph error is still not the latest tools described in https://www.tensorflow.org/performance/quantization\r\n\r\nDid you try using the contrib/quantize library https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize and then converting to TensorFlow Lite?\r\n\r\nWe will track support for t2t as well for the new tools. Thanks!", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20059, "title": "map_and_batch slower than map + batch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Docker (tensorflow/tensorflow:1.8.0-gpu-py3)\r\n- **TensorFlow version (use command below)**: `v1.8.0-0-g93bc2e2072`\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: GTX 1080 Ti 11 Gb\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\n\r\nUsing `map_and_batch` in my use case results in a slower input pipeline than using normal `map` followed by `batch`. `batch_size=512`\r\n\r\nHere is my code. The `augment_data` and `padding_inputs_width` are [quite heavy](https://github.com/cipri-tom/tf-crnn/blob/15b9aa5cce440a4e4f4df0dcffc77ce4cd9b913d/src/data_handler.py#L92)\r\n\r\n```python\r\ndef parse_example(serialized_example, output_shape=None):\r\n    features = tf.parse_single_example(serialized_example, feature_spec)\r\n    label = features.pop('label')\r\n\r\n    # Replace image_raw with the decoded & preprocessed version\r\n    image = features.pop('image_raw')\r\n    image = tf.image.decode_png(image, channels=1)\r\n    image = augment_data(image)\r\n    image, orig_width = padding_inputs_width(image, output_shape, ...)\r\n    features['image'] = image\r\n    features['image_width'] = orig_width\r\n    return features, label\r\n\r\n\r\ndef make_input_fn(files_pattern, batch_size, output_shape):\r\n    shaped_parse_example = partial(parse_example, output_shape=output_shape)\r\n\r\n    def input_fn():\r\n        files = tf.data.Dataset.list_files(files_pattern, shuffle=True)\r\n        ds = files.apply(tf.contrib.data.parallel_interleave(\r\n            tf.data.TFRecordDataset,\r\n            cycle_length=4, block_length=16, sloppy=True))\r\n\r\n        # NOTE: using map_and_batch seems to decrease performance\r\n        ds = (ds.shuffle(buffer_size=128) # small buffer since files were also shuffled\r\n                .apply(tf.contrib.data.map_and_batch(\r\n                    shaped_parse_example, batch_size,\r\n                    num_parallel_batches=4, drop_remainder=True))\r\n\r\n                # separate calls version, comment the above apply\r\n                # .map(shaped_parse_example, num_parallel_calls=4)\r\n                # .apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\r\n              )\r\n        features, labels = ds.prefetch(2).make_one_shot_iterator().get_next()\r\n        return features, labels\r\n\r\n    return input_fn\r\n```\r\n\r\n\r\nWhile I use the same number of _parallel_ stuff, I think the difference comes from the fact that the map function is heavy and when using `map_and_batch` only one thread is used for producing each batch.\r\n\r\n### How much slower ?\r\n\r\nIt is hard to quantify. With `map_and_batch` I just see lower numbers for GPU utilisation and even reaching zero at times. I tried increasing the `prefetch` to 4 to make up for this, but no improvement.\r\n\r\nHere I ran with the first input pipeline for a bit and then with the `map_and_batch`. You can see a difference of about 30%.\r\n\r\n<img width=\"361\" alt=\"screen shot 2018-06-15 at 14 13 36\" src=\"https://user-images.githubusercontent.com/2991890/41467345-733f37ec-70a6-11e8-89f7-21be1d8eda66.png\">\r\n\r\n\r\n### Feature request\r\n\r\nThe reason for this issue is that the documentation for `map_and_batch` says it will be done automatically in future versions. I think that in its current version, this can be a regression, as shown above. I believe (though I'm most probably wrong) that there should be a parameter in `map_and_batch` controlling the number of threads for the `map` operation, and another one for the `num_parallel_batches`. Or along those lines...\r\n\r\nEdit: Python version is 3.5.2", "comments": ["Can you try setting `num_parallel_calls=4` rather than `num_parallel_batches=4`? Currently your program will execute 4 * `batch_size` functions in parallel, which might be leading to contention.\r\n\r\n/cc @jsimsa", "Thanks for the fast reply! \r\n\r\n`map_and_batch() got an unexpected keyword argument 'num_parallel_calls'`\r\n\r\nThis is TF 1.8. Do I have to try the RC 1.9 ?", "Ah yes, that argument was only added in bf228e1435da0032d2529de93661b742ee8a7048, so you'd need to upgrade to use it.\r\n\r\nAs a proxy however, does it speed up your program if you cut `num_parallel_batches` down to 1?\r\n\r\n(Incidentlly, the reason we added `num_parallel_calls` was because on some platforms and with some batch sizes, kicking off `batch_size` computations would slow things down. The prototype automatic optimizer for `map().batch()` -> `map_and_batch()` (bab05a2191383b3c66e9ea9ee192aef0aa36c218) uses `num_parallel_calls` to keep the degree of parallelism the same before and after the rewrite.)", "Thanks @mrry for the reply! It took me a while to come back with new results as there were other things running on the machine, so benchmarking was not feasible.\r\n\r\nI tried with `num_parallel_batches=1` in v1.8, but didn't get anything out of it. It even seemed a bit slower, since I don't think there was any parallelism left (no `num_parallel_calls` in v1.8).\r\n\r\nI also tried with v1.9 RC `map_and_batch(num_parallel_calls=4)`, which is supposed to replicate the `.map(num_parallel_calls=4).batch()`. While it is faster than the above, 1.15 steps/sec (instead of 1/s), it is not as fast as map+batch, which gets me 1.3 steps/sec. I find this a bit strange", "That is surprising. I'll assign this to @jsimsa, since he has been working on the performance of `map_and_batch()` most recently (and since our working hypothesis is that converting parallel `map().batch()` to `map_and_batch()` with the same degree is always at worst performance-neutral, we'll need to get to the bottom of this).\r\n\r\n@jsimsa The only thing I can think of here is that the `ParallelConcat()` we use here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b7300de4ef75bdd9373bccc4ae5b98135a70287b/tensorflow/core/kernels/data/map_and_batch_dataset_op.cc#L309-L310\r\n\r\n...might be slower than sequential concat in some cases. For example, we might be using too many threads to perform each copy, and they could be contending. I'm not convinced that multithreading that copy is always a good idea when we'd expect to have `num_parallel_calls` copies issuing in parallel anyway.\r\n\r\n@cipri-tom We might have some difficulty reproducing your workload without more details. As a proxy, would you be able to capture a performance trace using a tool like pprof and share the results when running each version? Also, could you try running with `intra_op_parallelism=1`, by adding a `tf.ConfigProto` to your `tf.Session` arguments? That would help to test my hypothesis about `ParallelConcat()`.\r\n\r\nThanks!", "Hi @cipri-tom, I evaluated the performance of `map().batch()` vs `map_and_batch()` across a wide range of configurations (varying transformation parallelism, batch size, map function cost, threadpool size) and didn't come across any configuration for which `map_and_batch()` would perform worse than `map().batch()`.\r\n\r\nThis is the program that I used for my evaluation:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\nbatch_size = 1024\r\nnum_calls = 16\r\ninter_op = 16\r\nelement_size = 1\r\nnum_iters = 16\r\nk = 1024 * 1024\r\n\r\ndataset = tf.data.Dataset.from_tensors((np.random.rand(\r\n    element_size, 4 * k), np.random.rand(4 * k, 1))).repeat()\r\n\r\nchained_dataset = dataset.map(\r\n    tf.matmul, num_parallel_calls=num_calls).batch(batch_size=batch_size)\r\nchained_iterator = chained_dataset.make_one_shot_iterator()\r\nchained_get_next = chained_iterator.get_next()\r\n\r\nchained_deltas = []\r\nwith tf.Session(config=tf.ConfigProto(\r\n    inter_op_parallelism_threads=inter_op)) as sess:\r\n\r\n  with tf.Session(\r\n      config=tf.ConfigProto(\r\n          inter_op_parallelism_threads=inter_op)) as sess:\r\n    for _ in range(5):\r\n      sess.run(chained_get_next.op)\r\n    for _ in range(num_iters):\r\n      start = time.time()\r\n      sess.run(chained_get_next.op)\r\n      end = time.time()\r\n      chained_deltas.append(end - start)\r\n\r\nfused_dataset = dataset.apply(\r\n    tf.contrib.data.map_and_batch(\r\n        tf.matmul, num_parallel_calls=num_calls, batch_size=batch_size))\r\nfused_iterator = fused_dataset.make_one_shot_iterator()\r\nfused_get_next = fused_iterator.get_next()\r\n\r\nfused_deltas = []\r\nwith tf.Session(config=tf.ConfigProto(\r\n    inter_op_parallelism_threads=inter_op)) as sess:\r\n\r\n  with tf.Session(\r\n      config=tf.ConfigProto(\r\n          inter_op_parallelism_threads=inter_op)) as sess:\r\n\r\n    for _ in range(5):\r\n      sess.run(fused_get_next.op)\r\n    for _ in range(num_iters):\r\n      start = time.time()\r\n      sess.run(fused_get_next.op)\r\n      end = time.time()\r\n      fused_deltas.append(end - start)\r\n\r\nprint(\r\n    \"batch size: %d, num parallel calls: %d, inter-op parallelism: %d, \"\r\n    \"element size: %d\\nchained wall time: %f (median), %f (mean), %f \"\r\n    \"(stddev), %f (min), %f (max)\\n  fused wall time: %f (median), %f \"\r\n    \"(mean), %f (stddev), %f (min), %f (max)\\n    chained/fused: \"\r\n    \"   %.2fx (median),    %.2fx (mean)\" %\r\n    (batch_size, num_calls, inter_op, element_size, np.median(chained_deltas),\r\n     np.mean(chained_deltas), np.std(chained_deltas), np.min(chained_deltas),\r\n     np.max(chained_deltas), np.median(fused_deltas), np.mean(fused_deltas),\r\n     np.std(fused_deltas), np.min(fused_deltas), np.max(fused_deltas),\r\n     np.median(chained_deltas) / np.median(fused_deltas),\r\n     np.mean(chained_deltas) / np.mean(fused_deltas)))\r\n```\r\n\r\nSee if you can use it as a starting point to generate an example that reproduces the issue you have encountered.\r\n\r\nAs a side note, since you seem to care about performance, I recommend you build TensorFlow from source with AVX, AVX2, or FMA enabled (assuming your CPU supports these). Doing so will likely benefit the performance of your pipeline.", "@jsimsa  thank you for getting back! We have things to keep the GPUs busy until next week, so I can't try anything before that. I'll get back when I get any conclusive results", "@jsimsa Thank you for the tests and the benchmarking program! Indeed, running it with various configurations doesn't reflect any troubles with either pipeline.\r\n\r\nOn my side, there are no conclusive results. I still see the mentioned slow-down, but the causes are very weird and most probably tied to my system/program and not to TF.\r\n\r\nThis is because I ran one very long training on a separate and more performant machine, and during the training I saw the `global_step/sec` measure fluctuating by about the same amount as I previously attributed to the difference between `.map().batch()` and `map_and_batch()`.\r\n\r\n<img width=\"358\" alt=\"graph of global steps per second during continuous training\" src=\"https://user-images.githubusercontent.com/2991890/43446724-e32eb9fc-94a9-11e8-9899-bec46e933afb.png\">\r\n\r\nIt is interesting that the intervals of performance drop/increase are synchronised with the epochs. In other words, each lasts ~4000 steps which is the size of one epoch, and I trained for 10 epochs. If you have any suggestions for this, they would be very welcome. Otherwise, it is safe to let this issue die \ud83d\ude00 \r\n", "@cipri-tom thank you for reporting your findings ... my best guess this is related to either I/O or memory alignment ... to better understand what is going on, I would collect and compare pprof traces for epochs that are performing differently"]}, {"number": 20058, "title": "[C++ API] tf.reset_default_graph() equivalent", "body": "Does the C++ API has an equivalent method to `tf.reset_default_graph()` ? \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n\r\nThat said, in the C++ API the graph is always explicit - the `Graph` object or some representative of it is always provided to the API functions.\r\n\r\nIn other words, there is no notion of a \"default graph\", so there is no `tf.reset_default_graph()`.\r\nHope that helps."]}, {"number": 20057, "title": "Feature request: Add \"warm_start_from\" to tf.keras.estimator.model_to_estimator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI can create an estimator either direct or from a keras model using the initialisers:\r\n```\r\ntf.estimator.estimator(\r\n    model_fn,\r\n    model_dir=None,\r\n    config=None,\r\n    params=None,\r\n    warm_start_from=None\r\n)\r\n```\r\n```\r\ntf.keras.estimator.model_to_estimator(\r\n    keras_model=None,\r\n    keras_model_path=None,\r\n    custom_objects=None,\r\n    model_dir=None,\r\n    config=None\r\n)\r\n```\r\nHowever with the keras approach there is no option to warm start the model from a previously saved checkpoint. I am requesting a feature that makes it possible to do this with a call such as:\r\n```\r\ntf.keras.estimator.model_to_estimator(\r\n    keras_model=None,\r\n    keras_model_path=None,\r\n    custom_objects=None,\r\n    model_dir=None,\r\n    config=None,\r\n    warm_start_from=None\r\n)\r\n```\r\n\r\nWithout this I cannot see how to start an estimator in a pre-trained state.\r\n\r\nI have also tried to do this by just calling an untrained estimator to do .predict which causes it to attempt to load from a checkpoint. When I provide the checkpoint from previous estimator training I get an error as not all data seems to be available. This is described in more detail at stackoverflow here https://stackoverflow.com/questions/50855256/keras-estimator-model-to-estimator-cannot-warm-start-or-load-previous-checkpoi\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "A duplicate of https://github.com/tensorflow/tensorflow/issues/19295?\r\nAnother related problem is at https://github.com/tensorflow/tensorflow/issues/18696", "/cc @ispirmustafa for https://github.com/tensorflow/tensorflow/issues/14713#issuecomment-396422797", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It is a duplicate.", "Closing this since its a duplicate."]}, {"number": 20056, "title": "Handle checking of all export fields in single function", "body": "", "comments": ["Thanks, @msamogh , for this nice refactor. There are a few tests failing, mostly due to the pluralization of error messages. Can you run the tests and fix as necessary? I hadn't done this in the past to avoid changing the language of the error, but I think it should be fine at this point.", "@karmel  I'll get on it. The errors seem to be related to the style and not the pluralization, though. The pluralization errors were part of the last PR I'd submitted, which I fixed in this one.", "@karmel Any update on this?", "Sorry, I didn't realize changes had been pushed. I think we still need to update the tests to match the new error message-- @msamogh , want to give that a shot and ping when you have tests passing? Many thanks--\r\n\r\n```\r\n======================================================================\r\nFAIL: test_serving_input_receiver_receiver_tensors_invalid (__main__.ServingInputReceiverTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/estimator/export_test.runfiles/org_tensorflow/tensorflow/python/estimator/export/export_test.py\", line 114, in test_serving_input_receiver_receiver_tensors_invalid\r\n    1: array_ops.placeholder(dtypes.string, name=\"example0\")})\r\nAssertionError: \"receiver_tensors keys must be strings\" does not match \"receiver_tensor keys must be strings: 1.\"\r\n======================================================================\r\nFAIL: test_input_receiver_receiver_tensors_invalid (__main__.SupervisedInputReceiverTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/estimator/export_test.runfiles/org_tensorflow/tensorflow/python/estimator/export/export_test.py\", line 279, in test_input_receiver_receiver_tensors_invalid\r\n    1: array_ops.placeholder(dtypes.string, name=\"example0\")})\r\nAssertionError: \"receiver_tensors keys must be strings\" does not match \"receiver_tensor keys must be strings: 1.\"\r\n======================================================================\r\nFAIL: test_serving_input_receiver_receiver_tensors_invalid (__main__.TensorServingReceiverTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/estimator/export_test.runfiles/org_tensorflow/tensorflow/python/estimator/export/export_test.py\", line 747, in test_serving_input_receiver_receiver_tensors_invalid\r\n    1: array_ops.placeholder(dtypes.string, name=\"example0\")})\r\nAssertionError: \"receiver_tensors keys must be strings\" does not match \"receiver_tensor keys must be strings: 1.\"\r\n----------------------------------------------------------------------\r\n```", "@karmel Oops, my bad. I let that one get away. Fixed it now.", "Thanks, @msamogh . Tests are still failing; are you able to run them locally to test as you go? Errors are below, but I think the best solution here is to revert your last commit, and update the tests themselves, since we don't want to guess at the pluralization of future export types. \r\n\r\n```\r\n======================================================================\r\nFAIL: test_serving_input_receiver_features_invalid (__main__.ServingInputReceiverTest)\r\n----------------------------------------------------------------------\r\nValueError: features keys must be strings: 1.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/estimator/export_test.runfiles/org_tensorflow/tensorflow/python/estimator/export/export_test.py\", line 87, in test_serving_input_receiver_features_invalid\r\n    receiver_tensors=receiver_tensors)\r\nAssertionError: \"feature keys must be strings\" does not match \"features keys must be strings: 1.\"\r\n======================================================================\r\nFAIL: test_input_receiver_features_invalid (__main__.SupervisedInputReceiverTest)\r\n----------------------------------------------------------------------\r\nValueError: features keys must be strings: 1.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/estimator/export_test.runfiles/org_tensorflow/tensorflow/python/estimator/export/export_test.py\", line 228, in test_input_receiver_features_invalid\r\n    receiver_tensors=receiver_tensors)\r\nAssertionError: \"feature keys must be strings\" does not match \"features keys must be strings: 1.\"\r\n```", "@karnel Updated tests", "Something seems to have changed in the squashing of commits above? I will have to re-review in more detail, but there are now lots of errors that indicate param mismatch:\r\n\r\n```\r\n======================================================================\r\nERROR: test_serving_input_receiver_features_invalid (__main__.TensorServingReceiverTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/estimator/export_test.runfiles/org_tensorflow/tensorflow/python/estimator/export/export_test.py\", line 731, in test_serving_input_receiver_features_invalid\r\n    receiver_tensors=receiver_tensors)\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/estimator/export_test.runfiles/org_tensorflow/tensorflow/python/estimator/export/export.py\", line 206, in __new__\r\n    _check_tensor(features, None)\r\nTypeError: _check_tensor() missing 1 required positional argument: 'error_label'\r\n```", "@karmel Hey, it should hopefully be fixed now. Unfortunately, I lost my Bazel build and it takes really long for me to build Tensorflow on my (pretty old) system, so I'm having to resort to ad-hoc testing on my machine. Hence, the multiple test case mismatches. Sorry about that.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@caisq Could you merge this if everything is fine?", "You're in! Thanks @msamogh for your help and patience."]}, {"number": 20055, "title": "Speech_commands conv model depthwise_conv operation slow in tensorflow lite", "body": "System information\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nTensorFlow installed from (source): tensorflow v1.8.0\r\nPython version: 3.6\r\nBazel version (if compiling from source): 0.11.1\r\nGCC/Compiler version (if compiling from source): 5.4\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n\r\n#**Describe the problem**\r\n\r\nI am trying to use Speech_commands conv model (conv_actions_frozen.pb from \"http://download.tensorflow.org/models/speech_commands_v0.01.zip\")\r\nto inference on tensorflow lite. \r\nI use TOCO to convert conv_actions_frozen.pb to conv_actions_frozen.tflite, and it convert the conv to \r\ndepthwise_conv  + conv operation.\r\n\r\nBelow is inference log (depthwise_conv  + conv ) :\r\n\r\nLoaded model conv_actions_frozen.tflite\r\nresolved reporter\r\nResizing input tensor\r\nAUDIO_SPECTROGRAM computing time: 1.42 ms \r\nMfcc computing time: 2.143 ms \r\nReshape computing time: 0.003 ms \r\nDepthWise_Conv computing time: 68.609 ms \r\nMaxPool computing time: 0.484 ms \r\nEvalFloat \r\nEvalFloat kMultithreadOptimized \r\nConv computing time: 21.343 ms \r\ninvoked \r\naverage time: 102.171 ms \r\n\r\nI found  that DepthWise_Conv operation took so long to compute , so i modify TOCO (toco_tooling.cc) to \r\nlet converting process without ConvToDepthwise transformation. \r\n//transformations.Add(new ConvertPureConvToDepthwise);\r\n\r\nHere is the inference log (conv + conv ) :\r\n\r\nLoaded model noDSconv.tflite\r\nresolved reporter\r\nResizing input tensor\r\nAUDIO_SPECTROGRAM computing time: 1.083 ms \r\nMfcc computing time: 1.551 ms \r\nReshape computing time: 0.002 ms \r\nEvalFloat \r\nEvalFloat kMultithreadOptimized \r\nConv computing time: 10.334 ms \r\nMaxPool computing time: 0.454 ms \r\nEvalFloat \r\nEvalFloat kMultithreadOptimized \r\nConv computing time: 18.243 ms \r\ninvoked \r\naverage time: 32.628 ms \r\n\r\n\r\nI would expect the inference on tflite should be fast after TOCO converting the model.\r\nIs there any reason or clue can check this?\r\nThanks.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow version", "System information :\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nTensorFlow installed from (source): Anaconda\r\nTensorFlow version (use command below): tensorflow v1.8.0\r\nPython version: 3.6\r\nBazel version (if compiling from source): 0.11.1\r\nGCC/Compiler version (if compiling from source): 5.4\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20054, "title": "Doc feature request: How to build Tensorflow Slim", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIn https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim I see no steps on how to compile this module. In the case of Tensorflow serving I see the complete set of commands to build it. Could you add some steps on how to build Slim? Thanks in advance.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Never mind, I saw that it is part of Tensorflow, not a separate module"]}, {"number": 20053, "title": "error : tf.nn.ctc_beam_search_decoder", "body": "if classes size is small \uff0ctf.nn.ctc_beam_search_decoder can run normally.\r\n\r\nif classes size is big (example: 3800), tf.nn.ctc_beam_search_decoder Will not end\r\n\r\ncpu: 8\r\nmemory: 32\r\ngpu: 1080ti\r\n\r\nPlease anyone help me\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "running time of beam search is O(T * BW * C * log(BW * C)), with BW=beam width, C=number of characters, T=input sequence length. Increasing the number of characters C increases the running time more than linearly. So, maybe you just have to wait a little longer?"]}, {"number": 20052, "title": "tflite: output tensor size is not variable.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: tensorflow 1.8\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: 6.4\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: GeForce GTX 750\r\n- **Exact command to reproduce**: run mtcnn pnet model in tflite android demo of tflitecamerademo.\r\n\r\n### Describe the problem\r\nUse mtcnn pnet source code to generate pnet.pb file, \r\nthen convert pnet.pb to pnet.tflite file.\r\nwhen run pnet.tflite in  tflitecamerademo and input size is not the same as the number of when converting the tflite, the error log is \"num_input_elements != num_output_elements (42050 != 48050)\".\r\nI guess output tensor need resize because I resize the input tensor.\r\nBut I can't find the API of output tensor resize.\r\n\r\n### Source code / logs\r\nThe pnet code as below:\r\n    (self.feed('data')  #pylint: disable=no-value-for-parameter, no-member\r\n    .conv(3, 3, 10, 1, 1, padding='VALID', relu=False, name='conv1')\r\n     .prelu(name='PReLU1')\r\n     .max_pool(2, 2, 2, 2, name='pool1')\r\n     .conv(3, 3, 16, 1, 1, padding='VALID', relu=False, name='conv2')\r\n     .prelu(name='PReLU2')\r\n     .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv3')\r\n     .prelu(name='PReLU3')\r\n     .conv(1, 1, 2, 1, 1, relu=False, name='conv4-1')\r\n      .softmax(3,name='prob1'))\r\n\r\n    (self.feed('PReLU3') #pylint: disable=no-value-for-parameter\r\n    .conv(1, 1, 4, 1, 1, relu=False, name='conv4-2'))\r\ngenerate pb file as below:\r\n   data = tf.placeholder(name=\"input\", dtype=tf.float32, shape=(None, None, None, 3))\r\n   pnet = PNet({'data':data})\r\n   pnet.load(os.path.join(model_path, 'det1.npy'), sess)\r\n\r\n    frozen_graphdef = tf.graph_util.convert_variables_to_constants(\r\n            sess, sess.graph_def, ['pnet/conv4-2/BiasAdd', 'pnet/prob1'])\r\n   model_f = tf.gfile.GFile(\"/home/hwh/pnet_none.pb\", \"wb\")\r\n\r\nconvert to tflite file as below:\r\n./bazel-bin/tensorflow/contrib/lite/toco/toco    --input_file=/home/hwh/pnet.pb   --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE   --output_file=/home/hwh/pnet.tflite --inference_type=FLOAT   --input_type=FLOAT --input_arrays=pnet/input   --output_arrays=pnet/prob1,pnet/conv4-2/BiasAdd --input_shapes=1,320,320,3\r\n\r\nthen I run this tflite file in tflitecamerademo, and before tflite run, \r\nI resize input tensor to (1, 299, 299, 3).\r\n\r\nwhen run the error log as below:\r\n    06-15 16:14:36.500 12080-12275/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n    Process: android.example.com.tflitecamerademo, PID: 12080\r\n    java.lang.NullPointerException: Internal error: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/reshape.cc:68 num_input_elements != num_output_elements (42050 != 48050)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:123)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:166)\r\n        at com.example.android.tflitecamerademo.ImageClassifierFloatInception.runInference(ImageClassifierFloatInception.java:121)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:117)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:697)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Camera2BasicFragment.java)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:592)\r\n        at android.os.Handler.handleCallback(Handler.java:815)\r\n        at android.os.Handler.dispatchMessage(Handler.java:104)\r\n        at android.os.Looper.loop(Looper.java:194)\r\n        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n\r\n", "comments": ["It looks like the model you are using has a Reshape op with a fixed size. The easiest workaround is to reconvert using --input_shapes=1,299,299,3. Does that work? Otherwise you'll need to change the resize code in the java app.", "Yes, when input_shapes = 1, 299, 299, 3 is ok.\r\nbut my pb is generated by input tensor of (None, None, None, 3),\r\nI hope the input tensor size is variable.\r\nAnd I can resize input size in java code,\r\nand my output tensor size is fit for (1, 299, 299, 3) input,\r\nbut output is reported error.\r\nI made an experiment that mean op, input tensor size is variable, output tensor is fixed, it is ok.\r\nBut now my net output size will change with input tensor.\r\nSo I think it is perfect that the output is auto fit for input tensor,\r\nAnd my java code need allocate an enough memory.\r\nThanks.", "For now you will have to fix your shape, this is a known bug that we are tracking and plan to fix.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any updates on this issue? Is this fixed?", "https://github.com/tensorflow/tensorflow/issues/22377#issuecomment-444550814", "Hi @suharshs\r\n\r\nwhen will you fix this problem?\r\n\r\nthanks", "Was it fixed in a newer version?", "is this fixed?"]}, {"number": 20051, "title": "Add mirror for nasm in workspace.bzl", "body": "", "comments": []}, {"number": 20050, "title": "tf.contrib.rnn.LSTMCell wrong documentation and unclear naming  ", "body": "### Describe the problem\r\n1.\r\nIn the documentation at [https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell](url) it says: \r\n\r\n> Long short-term memory unit (LSTM) recurrent network cell. The default non-peephole implementation is based on: http://www.bioinf.jku.at/publications/older/2604.pdf S. Hochreiter and J. Schmidhuber. \"Long Short-Term Memory\". Neural Computation, 9(8):1735-1780, 1997\r\n\r\nThis is not true since the implementation already contains the forget gate which was not mentioned in the original paper but in this one: \r\n\r\n> \"Learning to Forget: Continual Prediction with LSTM\" by Gers et al.\r\n\r\n2.\r\nThe naming of the parameter `num_units` or the `LSTMcell` is  is unclear and causes a lot of confusion. (google \"num_units lstm cell\" and you will find countless posts with the same question: what does it mean?)\r\n\r\nSince the number of units within an LSTM cell is fixed, `num_units` should either be `memory_cell_block_size` as it is described in the \"Long Short-Term Memory\" paper and/or rename `LSTMcell` into `LSTMMemoryCellBlock`\r\n\r\n### Request\r\n1. Add the Paper:\r\n> \"Learning to Forget: Continual Prediction with LSTM\" by Gers et al.\r\n\r\n to the Documention and mention that the implementation already contains forget gates.\r\n\r\n2. Rename `num_units` and/or  `LSTMcell` or at the very least add a comment", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I don't see how those fields are relevant, since it is more a documentation problem.", "Nagging Assignee @MarkDaoust: It has been 125 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Your suggestions sound like a pretty concrete proposal for some doc-updates. Any chance you can submit them as a PR?\r\n\r\nIt's too late to change the name of the num_units parameter.\r\nBut clarifying the description in the doc-string could be helpful.", "I see that the correct paper has been referenced. I will see if I can find some time to issue and PR, regarding the num_units parameter.", "Hi @oldsqlwnb ! Contrib has been removed from 2.x version.  Have you checked latest [LSTMcelldocument](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell) from 2.7 yet?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20049, "title": "Failed to Load the Native Tensorflow runtime", "body": "I am using WINDOWS10 64bit, Anaconda 4.5.4 and PyCharm as an IDE. I have NVIDIA Quadro4000m GPU installed. Now I want to use Keras-GPU with tensorflow as backend and I am getting the following error?\r\nOne more thing I could not create environment with tensorflow versionb 1.8.0 with keras-gpu all versions as it shows unsatisfiable error due to conflicts in dependency. It is related to mkl file version  I dont know how to fix the dependancy.\r\n\r\nThanks\r\n\r\nUsing TensorFlow backend.\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/pav7rt/PycharmProjects/CADImage_Classification/CAD_Classification.py\", line 5, in <module>\r\n    import keras\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\keras\\backend\\__init__.py\", line 84, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n```\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\pav7rt\\AppData\\Local\\Continuum\\anaconda3\\envs\\CADImageClassification\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Technically, I think this should go to the keras issues.\r\n\r\nYour problem is probably that you don't have the right libraries installed. Please follow the installation procedure carefully. You can use the windows dependency walker to see what libraries you're missing. Typically it's a problem with the CUDA version.", "I am using c onda environment version 3.5.2\r\nGpu capacity 6gb nvidiaqudro4000m \r\nTensorflow installed from Fonda\r\nCudnn 8.0 installed from vonda\r\nThough I wanted upgrade but could not allowed because conflict error I mentioned \r\nIn my post\r\nBazel I dont know \r\nOS windows 10 enterprise version\r\nIntel Lenovo core i5 processor", "I feel this problem is related to anaconda virtual environment conflict error \r\nAs I am not able to install specific version in it because of conflict error I mentioned", "Right, you are probably correct. What was the exact message about mkl?\r\n\r\n/CC @tatianashp ", "Error: Unsatisfiable package specifications.\r\n\r\n.mkl versions for two different libraries are conflicting. Check Conda info <package name > for the dependency. \r\n\r\nWhen I check I found dependance in deadlock so it's not possible but can you help in this case", "/CC @agramesh1 ", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "Closing this issue due to staleness. Please use the latest version of TensorFlow and build again.\r\nFeel free to report any issues you encounter with latest TensorFlow. Thanks!"]}, {"number": 20048, "title": "fix TF_GraphImportGraphDefWithResults and TF_GraphImportGraphDefWithR\u2026", "body": "These two C APIs fail when pb file is > 64 MB. The fix is to use wrapper function tensorflow::ParseProtoUnlimited, which doesn't have the arbitrary 64MB restriction.", "comments": []}, {"number": 20047, "title": "[bug]Using \"tensorflow/models/research/object_detection/sample.config/faster_rcnn_resnet50_coco.config\" produce \".pb\" to \"sess.run\"", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nAlso, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win7\r\n- **TensorFlow installed from (source or binary)**:source \r\n- **TensorFlow version (use command below)**:tensorflow-1.8.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nUsing \"faster_rcnn_resnet50_coco.config\" to train model by my own dataset,then i freeze model.ckpt to pb.When i used pb and tensorlfow-1.8.0 and tensorflow-1.7.0 to derive result,it reported error:\r\n![image](https://user-images.githubusercontent.com/10041362/41454685-ec935d34-70ac-11e8-96ca-6f0d34cb3ef1.png)\r\nBut when i used pb and tensorflow-1.4.0,tensorflow-1.5.0,tensorflow-1.6.0 to derive result,it was ok.And I used \"faster_rcnn_inception_resnet_v2_atrous_coco.config\" to train model,then freeze it.Tensorflow-1.8.0 and Tensorflow-1.4.0 were useful,it did't report error.\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 20046, "title": "R1.6", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "cc gunan@"]}]