[{"number": 22648, "title": "v1.11 breaking tf.Variable changes: no inheritance possible", "body": "Cannot inherit from `tf.Variable` anymore. `__init__` only raises `NotImplementedError`.\r\n\r\nI've noticed the MetaClass structure that is new (and the VariableV1/V2 WIP on Master)\r\n\r\nSo should anything different be subclassed? I found no documentation nor notes in the changelog for this (breaking) change.\r\n\r\nRequest: any kind of documentation or changelog notes on this. Hack: use only < 1.11 version\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: irrelevant\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary, pip\r\n- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: irrelevant\r\n- **GPU model and memory**: irrelevant\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nclass TestClass(tf.Variable):\r\n    def __init__(self, *args, **kwargs):\r\n        super(TestClass, self).__init__(*args, **kwargs)\r\n\r\ninstance1 = TestClass()\r\n```\r\n\r\n", "comments": ["@mayou36 Thanks for reporting this. ", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Assuming you restructure a lot anyway internally about the variables, I'll look at them again once it's stabilized.\r\n\r\nCan be closed from my side", "@mayou36 Yes, you're right. You can find more details here: [ RFC: Variables in TensorFlow 2.0 #11 ](https://github.com/tensorflow/community/pull/11/files)", "@facaiy Thanks! And feel free to close\r\n", "You'd need to inherit from the private underlying implementation types, RefVariable (deprecated) or ResourceVariable.\r\n\r\nIf you describe your use-case for inheriting from Variable we can discuss how best to accomplish it.", "@alextp thanks! ResourceVariable was what I also looked at. Our use-case is:\r\nwe are building a fitting library in High Energy Physics based on TF. We would really appreciate your opinion on the variables. As it's use-case specific, I'd propose to move it [here](https://github.com/zfit/zfit/issues/28) (and leave it as a reference). "]}, {"number": 22647, "title": "Unable to build Tensorflow on Windows 8", "body": "I am not able to build Tensorflow on windows 8 using the following command.\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nTensorflow 1.7 \r\nCuda 9.0\r\n\r\n1 error detected in the compilation of \"C:/Users/imrans/AppData/Local/Temp/nvcc_inter_files_tmp_dir/gru_ops_gpu.cu.compute_70.cpp1.ii\".\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 985.757s, Critical Path: 177.12s\r\nINFO: 466 processes: 466 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@imrsayyed29 Please complete the template so we can better help you.", "Hello @wt-huang,\r\n\r\nFollowing are the details:\r\n\r\nHave I written custom code : **N/A**\r\nOS Platform and Distribution : **Windows 8**\r\nTensorFlow installed from : **https://www.tensorflow.org/install/**\r\nTensorFlow version : **1.2**\r\nBazel version : **0.17.2**\r\nCUDA/cuDNN version: **9.0/7.3.1.20**\r\nGPU model and memory: **N/A**\r\nExact command to reproduce: **bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package**\r\nMobile device : **N/A**\r\n\r\nThanks", "@imrsayyed29 Please find build configurations for TensorFlow [here](https://www.tensorflow.org/install/source_windows). Best to upgrade to Windows 10 and use the latest TensorFlow version 1.11.0.", "This should work on Windows 10, feel free to reopen if any issue comes up."]}, {"number": 22646, "title": "Tensorflow 1.10 C++ project errors : logging.h error in line 229", "body": "I have generated ` libtensorflow_cc.so`(V1.9, V1.10 and V1.11) file successully with bazel in Win10 enviroment,  but when I use this TensorFlow API in VS2017, there are errors as bellow:\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/39480728/46284482-96ce1300-c5aa-11e8-90f1-48f6384eb254.png)\r\n\r\n![image](https://user-images.githubusercontent.com/39480728/46296842-dc500780-c5cd-11e8-8ba6-718c2535f5ea.png)\r\n\r\nI also build TensorFlow1.8 C++ version API, and it used  OK without these errors in VS2017.\r\n\r\nC:\\tf\\tensorflow1.10\\bazel-tensorflow1.10\\tensorflow\\core\\platform\\default\\logging.h\r\nI don not know why it occur these errors. Tensorflow1.8 did not have these errors.\r\nI build tensorflow1.10 by bazel with Windows10-gpu version.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@EdwardVincentMa Hi, have you tried with tensorflow V 1.11 ? Let us know if you are facing similar issue with V 1.11", "@harshini-gadige  Yes, V1,11 has the same issues.", "@harshini-gadige  And where is the TensorFlow's lib(build by bazel) ?  ", "@harshini-gadige Hello, how to solve this problem ?", "@aselle Hi, could you please look into this ?", "@aselle Hello, how to solve this problem\uff1f", "That file has not changed for a while. @meteorcloudy any clue? Also cc @gunan.", "I think in visual studio, you may need to add search directories for include paths.\r\nOur integration tests that use custom ops has been passing on this, and other users have not reported similar issues. So I am pretty sure this is not a bug in TF.\r\n\r\nI would recommend reaching out to stackoverflow, or using google for looking for how to set up visual studio header include directories.", "@EdwardVincentMa How did you build the TensorFlow1.8 C++ version API? With CMake? Can you be more specific so I can understand what's missing in the bazel build.", "@gunan  I check the project. The problem is always exist. ", "@gunan: Getting the similar issue. I using Qt creator insted of visual studio ", "Add NOMINMAX in Preprocessor Definitions (/DNOMINMAX) when compiling.\r\n![image](https://user-images.githubusercontent.com/6142053/48608641-bcc64000-e9bd-11e8-8165-087ee5fd06f3.png)\r\n", "> Add NOMINMAX in Preprocessor Definitions (/DNOMINMAX) when compiling.\r\n> ![image](https://user-images.githubusercontent.com/6142053/48608641-bcc64000-e9bd-11e8-8165-087ee5fd06f3.png)\r\n\r\n\u52a0\u4e0a\u8fd9\u884c\u4ee3\u7801\uff0c\u53ef\u4ee5work\u3002\u4e0d\u8fc7\u7c7b\u4f3c\u4e8e\u975e\u6781\u5927\u503c\u6291\u5236\u4e4b\u7c7b\u7684\u64cd\u4f5c\u4f1a\u51fa\u73b0\u95ee\u9898", "Hi @EdwardVincentMa !                                                                                                                                                                      \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Have you tried [latest version ](https://www.tensorflow.org/install/pip)yet ? You can use[ tested configurations](https://www.tensorflow.org/install/source_windows#tested_build_configurations) to build older version using [Bazel](https://www.tensorflow.org/install/source_windows).Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 22645, "title": "Problem with CUDA 10 and Tensorflow 1.11", "body": "Hello,\r\n\r\nI have Windows 10 , a Geforce 1060 6GB and I have installed CUDA 10. What I would like is to use tensorflow gpu for deep learning purposes. I have installed tensorflow-gpu 1.11\r\n\r\nI installed some C++ compilers in Microsoft Visual basic 2017 prior to CUDA 10 installation.\r\nI tried CUDA which was compatible with the graphic card but not CUDA 9. \r\nThen , I copied the cudnn 7.3.1 (latest) .dll into the bin located in CUDA/bin.\r\n\r\n\r\nProblem is..tensorflow doesn't still recognize my GPU (recognize only the CPU but printed \"CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\").\r\n\r\nVersion of keras 2.2 are still not compatible with tensorflow 1.11.\r\n\r\nI checked te version of CUDA and it worked ,it is the 10 version printed out.\r\n\r\nHowever I tried the C++ compiling tests (I tried to compile the bandwidth project in Microsoft Visual Studio loacted in CUDA Samples ) but it seemed to fail.\r\n\r\n\r\nCan someone help me please , I am a bit desperate !\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Hello,\r\n\r\nHere is the filled template:\r\n\r\nOS Platform and Distribution : Windows 10\r\n\r\nTensorFlow installed from : Conda\r\n\r\nTensorFlow version :    1.11\r\n\r\nBazel version: NA\r\n\r\nCUDA/cuDNN version : CUDA  v10 / CUDNN v7.3.1\r\n\r\nGPU model and memory : Geforce 1060 6B Max-Q\r\n\r\nExact command to reproduce : \r\n-Microsoft Visual studio 2017 installation : C++ complier box checked\r\n-CUDA 10 installation : all ok , version is good\r\n-CUDNN version for CUDA 10 . .dll placed in the bin of NVIDIA Toolkits/CUDA/v10/bin\r\n-pip install tensorflow-gpu\r\nAnd it still doesn't see my GPU\r\nI clicked on \"repair\" in Microsoft Visual Studio 2017 to try but it still doesn't see it.\r\nWhen i do nvidia-smi in the Windows command , it says either it doesn't work or my GPU isn't in WDDM mode. I don't understand that...\r\n\r\nI also tried to install cuda and cudnn via     \"conda -c install tensorflow-gpu\"   in the conda prompt , it installed  automatically cuda toolkit v9 directly and cudnn version accordingly (so , without installing manually CUDA) in the python libraries but not in C:\\Program... and not in the environment variable  . But it still doesn't work.\r\n\r\nI tried to install CUDA v9 manually but the installation said that my system is incompatible as my gpu muight be newer than the version of CUDA (which is surprising) , and therefore I could not use CUDA fully.\r\n\r\nMobile device: NA", "TensorFlow requires CUDA 9.0 according to: https://www.tensorflow.org/install/gpu", "Just like @adampl shared, prebuilt binaries for TF on pypi use CUDA 9.0\r\nthe binaries you installed from conda are supported by the anaconda community, we have no documentation on those.\r\n\r\nFor using TF with cuda 10, you have to build it from sources yourself. Here is the documentation for that:\r\nhttps://www.tensorflow.org/install/source_windows"]}, {"number": 22644, "title": "Is there some way to decode bytes string back to unicode which is decoded using string.encode('utf-8') in tensorflow pipline.", "body": "I am using tfRecord for storing my data. I have a Hindi text which is basically Unicode text and to convert it into bytes I have encoded into using string.encode('utf-8').\r\nAt the time of taking out data using tensorflow dataset API, I am facing trouble in getting my data in original form, Since I did not find any working method that could convert my byte string back to Unicode string.\r\nI have Tried \r\n`tf.compat.as_text(\r\n    byte_tensor,\r\n    encoding='utf-8'\r\n)`\r\nalso, raw_decode which should not work and it definitely not worked. Though I can convert my text back to original using string.decode('utf-8') by taking data out of the graph. But definitely, I don't want to do that since it will defeat the whole purpose and make my operation slow.\r\nCan anyone suggest any functionality that could help in this matter?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@lifeisshubh did you find a solution? I have this same need.", "In my case i have encode the data i have in alphanumeric form.\n\nOn Sun, 12 Apr, 2020, 9:57 pm Celso Fran\u00e7a, <notifications@github.com>\nwrote:\n\n> @lifeisshubh <https://github.com/lifeisshubh> did you find a solution? I\n> have this same need.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22644#issuecomment-612641484>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AB5QL3R6MGCKA4225ZN7Y2TRMHTXLANCNFSM4FYGNRSA>\n> .\n>\n"]}, {"number": 22643, "title": "r1.11: Build instructions for tensorflow lite as static library for android", "body": "Is there any? I followed https://github.com/yesmung/tensorflow approach to build 1.9 and it worked. But now I see structure was changed a bit and I would like to avoid porting original solution to 1.11 (especially if there is better way)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code\r\nN/A\r\n\r\nOS Platform and Distribution\r\nWindows 10 x64\r\n\r\nTensorFlow installed from\r\nN/A\r\n\r\nTensorFlow version\r\nr1.11\r\n\r\nBazel version\r\n0.15.0\r\n\r\nCUDA/cuDNN version\r\nN/A\r\n\r\nGPU model and memory\r\nN/A\r\n\r\nExact command to reproduce\r\nN/A\r\n\r\nMobile device\r\nN/A", "Also interesting in tflite static lib on android without Java api", "@ElderOrb I'm also interested in this, can you share the instructions you used to build 1.9 statically?", "@nicolaerosia no instructions but there is a branch https://github.com/ElderOrb/tensorflow/tree/r1.9", "any progress here?", "@ElderOrb I find a makefile under tensorflow/lite/tools/make at the latest master branch. And if we make it under tensorflow root path, like: make -f tensorflow/lite/tools/make/Makefile TARGET=android TARGET_ARCH=armv7 TARGET_TOOLCHAIN_PREFIX=/ndk_standalone_tc/p19/arm/bin/arm-linux-androideabi- all -j4.\r\nIt should generate a static library under tensorflow/lite/tools/make/gen/android_armv7/lib/ named libtensorflow-lite.a .\r\nHowever, we found this static library has some performance issue. We somehow managed to bazel build //tensorflow/lite/tools/benchmark:benchmark_model. We examine a tflite by this binary program, then examine the same tflite by the static library we built above. And we see big inference time difference, like 400ms(libtensorflow-lite.a) and 80ms(benchmark_model). So, not so sure if this makefile creates a desirable library or not.\r\nAll in all, I think we need a official way to generate static library for tflite. So, help us~@ TF developers~A big thanks! :+1: ", "@fengxueem I've tried your command with  both ndk18 and ndk19-beta but build failed in both cases. Moreover, your tensorflow path looks wrong: tensorflow/lite/tools/make/Makefile vs tensorflow/contrib/lite/tools/make/Makefile (for tensorflow 1.11 / 1.12). Also, toolchain prefix is non-standard too... Could you please provide a bit more details on how to create such environment? What are exact versions of gcc/clang/NDK/whatever which allow to build static library using this approach. Thanks!", "Makefile is a current way, but if you add a new target like android you need to make sure to enable the right optimization flags.\r\n\r\nAssigning @jdduke . We'd like to make an easy  path for this with bazel in the future.", "Can I ask why you explicitly need a static native library (.a), rather than a share native library (.so)? Unless all intermediate static libraries are using the same compiler and build options, it can be tricky to guarantee compatibility between them.", "Well.. in fact I'd like to have both variants (and then select the one working faster). But ok. Any instructions for dynamic library? ", "For now, you can build the C shared library, but remove the linker script option so that C++ symbols are preserved (i.e., remove the `linkopts` entry for the [libtensorflowlite_c.so](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/c/BUILD#L19) target). From there, you can use the usual Android build command:\r\n\r\n```\r\nbazel build -c opt --cxxopt=--std=c++11 \\\r\n  --config=android_arm \\\r\n  //tensorflow/lite/experimental/c:libtensorflowlite_c.so\r\n```", "Great, I need to check, thanks! What NDK should I use?", "I've got c:libtensorflowlite_c.so compiled using the approach suggested, but on attempt to link it I'm getting the following errors: \r\n\r\nerror: undefined reference to 'tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'\r\n..\\app\\tensorflowlite/tensorflow/contrib/lite/error_reporter.h:36: error: undefined reference to 'vtable for tflite::ErrorReporter'\r\n\r\nwhat I'm missing? ", "Also tried r1.12, doesnt compile at all: \r\n\r\nbazel --output_user_root=./build build -c opt --cxxopt=--std=c++11 --config=android_arm //tensorflow/lite/experimental/c:libtensorflowlite_c.so --verbose_failures\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/mnt/d/ai/tensorflow-wus-r1.12/tools/bazel.rc\r\nINFO: Invocation ID: fe404c80-e9c9-4bfc-a1f3-8c1029d687f3\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\nUse --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\nUse --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\nINFO: Elapsed time: 0.194s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)", "Which version of Bazel are you using? I believe there are issues with the latest public release, so you might need to downgrade.", "Okay, I'll downgrade bazel and retry. Any hints on missing symbols issue (undefined reference to 'vtable for tflite::ErrorReporter') ? ", "I've added a proper shared library target for the C/C++ API. Have a look [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/BUILD#L404).\r\n\r\nFor static libraries, bazel does not generally produce monolithic static libraries. Each cc_library will produce a static library, but only for the sources of that target; dependencies will remain in separate static libraries. There is a way to transitively walk all dependencies and manually assemble a single static library, but you'd have to write your own custom rule/script to do so.", "After building this shared library from the latest master (using bazel 0.21 and NDK 18b) and trying to link against it, I'm still getting errors: \r\n\r\n(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'\r\n..\\app\\tensorflowlite/tensorflow/contrib/lite/error_reporter.h:36: error: undefined reference to 'vtable for tflite::ErrorReporter'\r\n\r\nbuild cmd I used: \r\n\r\nbazel --output_user_root=./build build -c opt --cxxopt=--std=c++11 --config=android_arm //tensorflow/lite:libtensorflowlite.so\r\n\r\nAlso, is it possible to build it for windows ?", "Okay, finally I fixed the build by commenting out my subclass for ErrorReporter, but now it crashes on querying inputs. Does it mean model format was changed? \r\n\r\nAlso, windows build is failing with the following errors: \r\n\r\nINFO: From Linking tensorflow/lite/libtensorflowlite.so:\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/ldl'; ignored\r\nLINK : warning LNK4044: unrecognized option '/ldl'; ignored\r\nD:/tensorflow_master/tensorflow/lite/BUILD:406:1: output 'tensorflow/lite/libtensorflowlite.so.if.lib' was not created\r\nERROR: D:/tensorflow_master/tensorflow/lite/BUILD:406:1: not all outputs were created or valid\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\n\r\nP.S. Maybe I used wrong command line for windows build: \r\n\r\nbazel.bat --output_user_root=./builddir build //tensorflow/lite:libtensorflowlite.so", "> but now it crashes on querying inputs. Does it mean model format was changed?\r\n\r\nWe need more details about the crash to say more. Is there an unresolved symbol error when the library is opened? What does the crash stack look like?", "Seems like the issue was related to master instability. 1.13rc works just fine for android / linux (although I still can't build for windows)", "> Also tried r1.12, doesnt compile at all:\r\n> \r\n> bazel --output_user_root=./build build -c opt --cxxopt=--std=c++11 --config=android_arm //tensorflow/lite/experimental/c:libtensorflowlite_c.so --verbose_failures\r\n> WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n> /mnt/d/ai/tensorflow-wus-r1.12/tools/bazel.rc\r\n> INFO: Invocation ID: fe404c80-e9c9-4bfc-a1f3-8c1029d687f3\r\n> ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\n> Use --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\n> ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\n> Use --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\n> INFO: Elapsed time: 0.194s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\ntry old version bazel, like 0.18.0 or 0.19.0", "This script should help those who are trying to build tensorflow lite for android as a static library: https://github.com/tensorflow/tensorflow/issues/20905#issuecomment-542810510"]}, {"number": 22642, "title": "Make colocations be compatible with DistributionStrategy in SyncReplicasOptimizer", "body": "This is a compatibility improvement related to `DistributionStrategy`.  The `ops.colocate_with` in 'SyncReplicasOptimizer' should be replaced with `distribution.colocate_with_vars` to support `DistributionStrategy`. Otherwise, this `colocation` will be ignored and the `self._local_step` will be placed on `PS`.\r\n\r\nThis feature can be reffered to `parameter_server_strategy_test.py`.\r\n\r\n```\r\n# The device scope is ignored for variables but not for normal ops.\r\nwith ops.device('/job:worker/task:0'):\r\n  x = variable_scope.get_variable(\r\n        'x', initializer=10.0,\r\n         aggregation=variable_scope.VariableAggregation.SUM)\r\n```\r\n\r\n```\r\n# The colocate_vars_with can override the distribution's device.\r\nwith d.colocate_vars_with(x):\r\n  y = variable_scope.get_variable(\r\n        'y', initializer=20.0,\r\n        aggregation=variable_scope.VariableAggregation.SUM)\r\n```\r\n\r\n", "comments": ["Hi @yuefengz .\r\n  This is a small code change related to `DistributionStrategy`. Please check it. \r\nThanks.", "Hi, @rmlarsen \r\n  These UT failures seem not related to me. Could you start it again?\r\nThanks.", "Hi @rmlarsen @yuefengz ,\r\n  All CI test has passed. Could you please help review this PR? It has been pending for long time.\r\nThanks.", "@yuefengz @rmlarsen @anj-s \r\n\r\nCould you please kindly take a look at this CR? This has been pending for more than two weeks.\r\n\r\nThanks.", "@wangsiyu Sorry for the delayed response. What is the use case for using SyncReplicaOptimizer with DistributionStrategy? You should be able to use a TensorFlow optimizer with DistributionStrategy without requiring to wrap it in a SyncReplicasOptimizer.", "@anj-s Thank you for response. My use case is to use `ParameterServerStrategy` with `sync` mode between workers. So the `SyncReplicasOptimizer` should be inserted into the model to wrap the original optimizer. Is there any other way to use `sync` mode between workers in `DistributionStrategy`? ", "@wangsiyu I guess this should work but I am not sure whether @anj-s is planning anything for colocation. Also prefer to use CollectiveAllReduceStrategy for synchronous training.", "@yuefengz `CollectiveAllReduceStrategy ` is in sync mode but it is not based on Parameter Server architecture. Sometimes the users pick `ParameterServerStrategy` for distributing the training variables across servers.  ", "@wangsiyu We currently don't support sync mode with ParameterServerStrategy. It is definitely a use case that we want to support in the future but we are not actively working on it. We want to explore other solutions that don't depend on SyncReplicaOptimizer going forward. \r\n\r\nDoes the CollectiveAllReduceStrategy not work for your use case? What issues do you run into?", "@anj-s This is because many businesses use sync mode based on Parameter Server architecture in Alibaba including CNN and Seq2Seq models. CNN may works well with `CollectiveAllReduceStrategy ` because we usually don't need to partition variables across different servers. However, some Seq2Seq tasks may involved with large variables but update in sparse.  Although async mode is also suitable for computation in this case but sync mode may be better for convergence. \r\n\r\nYou mentioned that you are not actively working on sync mode in PS strategy and will explore other solutions which is not depend on `SyncReplicasOptimizer`. Does it mean PS strategy will deprecat `SyncReplicasOptimizer` in the future? Why ? Is it because we cannot avoid modifying models code to wrap common optimizer\uff1f", "This change looks good to me and we will have a better story for `SyncReplicaOptimizer` soon.", "Nagging Assignee @rmlarsen: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22641, "title": "[BUG] Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)", "body": "ERROR: /home/tensor/tensorflow/tensorflow/BUILD:566:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/tensor/.cache/bazel/_bazel_tensor/6576bf81f142261c3a81cca43642eb08/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/tensor/.cache/bazel/_bazel_tensor/6576bf81f142261c3a81cca43642eb08/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 127, in <module>\r\n    from tensorflow.python.platform import test\r\n  File \"/home/tensor/.cache/bazel/_bazel_tensor/6576bf81f142261c3a81cca43642eb08/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/platform/test.py\", line 47, in <module>\r\n    import mock                # pylint: disable=g-import-not-at-top,unused-import\r\nImportError: No module named mock\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3283.850s, Critical Path: 90.89s\r\nINFO: 3418 processes: 3418 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Thank you very much. I want to build tensorflow by using its source code. And it is cup-only", "OS Platform:ubutun16.4\r\nTensorFlow installed from:https://github.com/tensorflow/tensorflow/\r\nBazel version:I have tried many versions,but I can not figure it out now , I will send you later\r\ndevice : x86_64 withoutGPU\r\n\r\ncomond is : bazel build -c opt //tensorflow/tools/pip_package:build_pip_package", "the Bazel version I used is 0.17.2", "See\r\n```ImportError: No module named mock```\r\n\r\n@SummerLife Try installing the module mock", "@SummerLife Did this solve your issue?\r\n> See\r\n> `ImportError: No module named mock`\r\n> \r\n> @SummerLife Try installing the module mock\r\n\r\n", "Thank you very much! I truely solved this problem through installing the module! ", "    $ bazel build --verbose_failures tensorflow/tools/pip_package:build_pip_package\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/james/.cache/bazel/_bazel_james/eb428e77df4edf260b1a35afd0cdd799/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 776, in <module>\r\n    main()\r\n  File \"/home/james/.cache/bazel/_bazel_james/eb428e77df4edf260b1a35afd0cdd799/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 755, in main\r\n    importlib.import_module(package)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/james/.cache/bazel/_bazel_james/eb428e77df4edf260b1a35afd0cdd799/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/keras/datasets/imdb.py\", line 25, in <module>\r\n    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n  File \"/home/james/.cache/bazel/_bazel_james/eb428e77df4edf260b1a35afd0cdd799/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py\", line 23, in <module>\r\n    import keras_preprocessing\r\nModuleNotFoundError: No module named 'keras_preprocessing'\r\n\r\n\r\nHowever, I do have `keras_preprocessing` installed.", "I am also facing same issue.", "+1\r\n", "+1"]}, {"number": 22640, "title": "Fix warning for format specifier", "body": "tensorflow/core/util/command_line_flags.cc:73:37: warning:\r\nformat specifies type 'long *' but the argument has type 'int64_t *' (aka 'long long *') [-Wformat]\r\n    if (sscanf(arg.data(), \"%ld%c\", &parsed_int64, &extra) != 1) {\r\n                            ~~~     ^~~~~~~~~~~~~\r\n                            %lld\r\n1 warning generated.", "comments": ["@rmlarsen I didn't mean to dismiss your review. I saw another warning covering similar type of fix, so I added another commit to the same PR.", "@nehaljwani no worries. Thanks for fixing!", "@nehaljwani could you rebase and push again?", "This seems to have been fixed in f02251190f5908d2078e9fc11b92375dfc3a3054"]}, {"number": 22639, "title": "R1.11", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@DRoghanian Was this PR submitted by mistake?", "Yes, please reject it, my apologies.\n\nOn Mon, Oct 1, 2018 at 13:02 Rasmus Munk Larsen <notifications@github.com>\nwrote:\n\n> @DRoghanian <https://github.com/DRoghanian> Was this PR submitted by\n> mistake?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/22639#issuecomment-425981284>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ApJ1ePz_RRpvX-x_VH2s2OionV8gYCN3ks5ugkqbgaJpZM4XBRkN>\n> .\n>\n-- \n\nDavid Roghanian\nStevens Institute of Technology '18\nSystems Eng. | Mech Eng. | International Business\ndroghani@stevens.edu | (973) 207 - 4086\n", "@DRoghanian no problem :-)\r\n"]}, {"number": 22638, "title": "[BUG] tf.batch_gather error when feeding int64 indices", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: using CPU\r\n- **GPU model and memory**: using CPU\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nx = [[0, 1, 2], [3, 4, 5]]\r\ny = tf.convert_to_tensor([[1, 2, 0], [2, 0, 1]], dtype=tf.int64)\r\ntf.batch_gather(x, y)\r\n```\r\n### Describe the problem\r\nWhen feeding an int64 Tensor as `indices` to `tf.batch_gather`, an error occurs.\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1144, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 981, in _TensorTensorConversionFunction\r\n    (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(\"Reshape:0\", shape=(2, 1), dtype=int32)'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"a.py\", line 4, in <module>\r\n    tf.batch_gather(x, y)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 2726, in batch_gather\r\n    batch_indices += reshape(dim_indices, dim_shape)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 862, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 301, in add\r\n    \"Add\", x=x, y=y, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 546, in _apply_op_helper\r\n    inferred_from[input_arg.type_attr]))\r\nTypeError: Input 'y' of 'Add' Op has type int32 that does not match type int64 of argument 'x'.\r\n```\r\n\r\nThe error occurs [here](https://github.com/tensorflow/tensorflow/blob/5fa4e1ac928b0512b28e955c588c5a7eab2ea046/tensorflow/python/ops/array_ops.py#L2726) at the first `for` iteration. I found `batch_indices` is an int64 Tensor, while `dim_indices` is an int32 Tensor.\r\nObviously, a simple fix is to cast `dim_indices` to the dtype of the input `indices`. I wonder if I can directly send a pull request.", "comments": ["@ebrevdo Hi, could you please look into this one. The datatype of y is int64 in the above example, but it gives \"_Input 'y' of 'Add' Op has type int32_\" in the error message.", "Unfortunately I don't have cycles to look at this.\n\nOn Wed, Oct 3, 2018, 2:46 PM harshini-gadige <notifications@github.com>\nwrote:\n\n> Assigned #22638 <https://github.com/tensorflow/tensorflow/issues/22638>\n> to @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22638#event-1883142924>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_hbB8K9VDrxI1FKJpFIJAlVX2APks5uhTAvgaJpZM4XBM_H>\n> .\n>\n", "I'd love to approve a pull request with the cast you suggest (and a unit test which fails without it)!", "I found this issue has been solved in [this commit](https://github.com/tensorflow/tensorflow/commit/6cc738da1748e819b9c8ee92dc2f1a7bdb291b50)."]}, {"number": 22637, "title": "[Feature request] Refactor gFile out into separate library?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI really love gfile's API/ abstraction layer for wrapping GCS, and it's my primary method of scripting file manipulations (as opposed to, say, using a subprocessed gsutil and scraping stdout). However, TensorFlow's long import time imposes a high overhead on these scripts because `from tensorflow import gfile` will import all of TensorFlow. It'd be nice if gfile were refactored into a separate python module.\r\n\r\n", "comments": ["@mrry Hi, can this be added as a new feature request ?", "@martinwicke Is this something you'd be interested in having done? Or is it something we might eventually get from Abseil?", "ABSL doesn't presently contain gFile. I am not a fan of gFile being part of the TF API at all -- I'd much rather have it live somewhere else. ABSL seems to be the right spot, not sure what the plans are, I'll ask.", "We have no plans to refactor it in this manner. However, opensource is awesome! If you're willing to do the work, you could certainly move the code out and maintain it separately. I think a filesystem abstraction library would be a great addition to the Python ecosystem, and if a well maintained option emerges, we would consider depending on it as well. ", "I'll close this issue as we have no plans on working on this. ", "It might be worth taking a look at [GCSFS](https://gcsfs.readthedocs.io/)/[S3Fs](https://s3fs.readthedocs.io/en/latest/) or [PyFilesystem](https://www.pyfilesystem.org/)."]}, {"number": 22636, "title": "Query regarding 1.11.0 release", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: Binary\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nTwo questions:\r\n- Are the wheels uploaded on PyPI for Windows made using bazel or cmake?\r\n- The wheels uploaded on PyPI for macOS claim that they target 10.11, however, the function at https://github.com/tensorflow/tensorflow/blob/v1.11.0/tensorflow/core/platform/posix/env_time.cc#L31 uses `clock_gettime`, which wasn't available till 10.12 was released. So, how was it built? \ud83e\udd14 ", "comments": ["@nehaljwani 1.11.0 for Windows uses bazel, cmake is deprecated. You are correct that `clock_gettime` was not built prior to 10.12, it requires custom implementation in `libzmq`. The wheels targeting 10.11 use custom implementation. ", "How is the custom implementation for `clock_gettime` being used? I don't see it in the source code anywhere. I am asking that because the build failed for me and I had to apply this patch:\r\n```patch\r\ndiff --git a/tensorflow/core/platform/posix/env_time.cc b/tensorflow/core/platform/posix/env_time.cc\r\nindex 59a67b1..88f8a6c 100644\r\n--- a/tensorflow/core/platform/posix/env_time.cc\r\n+++ b/tensorflow/core/platform/posix/env_time.cc\r\n@@ -18,6 +18,23 @@ limitations under the License.\r\n\r\n #include \"tensorflow/core/platform/env_time.h\"\r\n\r\n+// Slightly pruned version of https://gist.github.com/alfwatt/3588c5aa1f7a1ef7a3bb\r\n+// Copyright (c) 2015-2018 Alf Watt - Open Source - https://opensource.org/licenses/MIT\r\n+#if defined __APPLE__\r\n+#include <mach/clock.h>\r\n+#include <mach/mach.h>\r\n+int alt_clock_gettime (int clock_id, timespec *ts) {\r\n+  clock_serv_t cclock;\r\n+  mach_timespec_t mts;\r\n+  host_get_clock_service (mach_host_self (), clock_id, &cclock);\r\n+  clock_get_time (cclock, &mts);\r\n+  mach_port_deallocate (mach_task_self (), cclock);\r\n+  ts->tv_sec = mts.tv_sec;\r\n+  ts->tv_nsec = mts.tv_nsec;\r\n+  return 0;\r\n+}\r\n+#endif\r\n+\r\n namespace tensorflow {\r\n\r\n namespace {\r\n@@ -28,7 +45,11 @@ class PosixEnvTime : public EnvTime {\r\n\r\n   uint64 NowNanos() override {\r\n     struct timespec ts;\r\n+#if defined __APPLE__ && __MAC_OS_X_VERSION_MIN_REQUIRED < 101200 // less than macOS 10.12\r\n+    alt_clock_gettime(CALENDAR_CLOCK, &ts);\r\n+#else\r\n     clock_gettime(CLOCK_REALTIME, &ts);\r\n+#endif\r\n     return (static_cast<uint64>(ts.tv_sec) * kSecondsToNanos +\r\n             static_cast<uint64>(ts.tv_nsec));\r\n   }\r\n--\r\n2.5.4 (Apple Git-61)\r\n```", "Also, for the wheels on Windows (GPU), was https://github.com/tensorflow/tensorflow/pull/22483 cherry-picked before building them?", "All of our wheels are not using bazel to be built, including windows.\r\nFor macos, @av8ramit @yifeif what is our build infra using?\r\nI am thinking we use el capitan to build for macos?\r\n\r\nAnd the final question, no. even at head, that PR is partially rolled back.", "I think we have 10.12.6  sierra?"]}, {"number": 22635, "title": "Tensorflow image classification", "body": "I've been trying to build a neural network model that will be able to classify images. But there is this one consistent error that keeps on popping up. Can anyone please help me with this?Here the error below\r\n\r\nTypeError: Expected bool for argument 'transpose_a' not <tf.Variable 'Variable_6:0' shape=(1024,) dtype=float32_ref>.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22634, "title": "Cmake can't handle deprecated files (assert_next_dataset_op.cc...)", "body": "Have I written custom code - No\r\n\r\nOS Platform and Distribution - Windows 10\r\n\r\n\r\nCmake cannot find deprecated files \r\n\r\nCMake Error at tf_core_kernels.cmake:221 (add_library):\r\n  Cannot find source file:\r\n\r\n    C:/tensorflow/tensorflow/contrib/data/kernels/assert_next_dataset_op.cc\r\n\r\n  Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm\r\n  .hpp .hxx .in .txx\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:525 (include)\r\n\r\n\r\nCMake Error at tf_core_ops.cmake:73 (add_library):\r\n  Cannot find source file:\r\n\r\n    C:/tensorflow/tensorflow/contrib/data/ops/dataset_ops.cc\r\n\r\n  Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm\r\n  .hpp .hxx .in .txx\r\nCall Stack (most recent call first):\r\n  tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)\r\n  CMakeLists.txt:523 (include)\r\n\r\n\r\nCMake Error at tf_core_ops.cmake:73 (add_library):\r\n  No SOURCES given to target: tf_contrib_data_dataset_ops\r\nCall Stack (most recent call first):\r\n  tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)\r\n  CMakeLists.txt:523 (include)\r\n\r\n\r\nCMake Error at tf_core_kernels.cmake:221 (add_library):\r\n  No SOURCES given to target: tf_core_kernels\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:525 (include)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code - No\r\nOS Platform and Distribution - Windows 10\r\nTensorFlow installed from - Git\r\nTensorFlow version - Last\r\nBazel version - Last\r\nCUDA/cuDNN version NA\r\nGPU model and memory NA\r\nExact command to reproduce NA\r\nMobile device NA", "@azaks2 Hi, could you please look into this.", "The files have been recently moved around that broke cmake. Since the latter is no longer supported you should switch to bazel.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "still can not find C:/tensorflow/tensorflow/contrib/data/ops/dataset_ops.cc on master branch. \r\n\r\nmissing version_info.cc", "The weird part in all of this is that when you use Bazel on windows to build tensorflow libs, it keeps failing.\r\nWhen you search for  those errors you only find people telling you NOT to use bazel because there is no support on windows so far.\r\nThen when we use Cmake you guys tell us to use bazel because you broke Cmake...", "Hi, \r\nA similar error here:\r\n````\r\n(base) C:\\projects\\External\\tensorflow\\tensorflow\\contrib\\cmake\\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:\\Users\\Tino\\bin\\swigwin-3.0.12\\swig.exe -DPYTHON_EXECUTABLE=C:\\Users\\Tino\\Anaconda3\\python.exe -DPYTHON_LIBRARIES=C:\\Users\\Tino\\Anaconda3\\libs\\python37.lib -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF\r\nCMake Warning at CMakeLists.txt:9 (message):\r\n  Your current cmake generator is set to use 32 bit toolset architecture.\r\n  This may cause \"compiler out of heap space\" errors when building.  Consider\r\n  using the flag -Thost=x64 when running cmake.\r\n\r\n\r\n-- Configuring done\r\nCMake Error at tf_core_framework.cmake:328 (add_library):\r\n  Cannot find source file:\r\n\r\n    C:/projects/External/tensorflow/tensorflow/core/util/version_info.cc\r\n\r\n  Tried extensions .c .C .c++ .cc .cpp .cxx .m .M .mm .h .hh .h++ .hm .hpp\r\n  .hxx .in .txx\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:518 (include)\r\n\r\n\r\nCMake Error at tf_core_ops.cmake:73 (add_library):\r\n  Cannot find source file:\r\n\r\n    C:/projects/External/OLD/tensorflow/tensorflow/contrib/data/ops/dataset_ops.cc\r\n\r\n  Tried extensions .c .C .c++ .cc .cpp .cxx .m .M .mm .h .hh .h++ .hm .hpp\r\n  .hxx .in .txx\r\nCall Stack (most recent call first):\r\n  tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)\r\n  CMakeLists.txt:524 (include)\r\n\r\n\r\nCMake Error at tf_core_kernels.cmake:218 (add_library):\r\n  Cannot find source file:\r\n\r\n    C:/projects/External/tensorflow/tensorflow/contrib/data/kernels/assert_next_dataset_op.cc\r\n\r\n  Tried extensions .c .C .c++ .cc .cpp .cxx .m .M .mm .h .hh .h++ .hm .hpp\r\n  .hxx .in .txx\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:526 (include)\r\n\r\n\r\nCMake Error: CMake can not determine linker language for target: tf_core_framework\r\nCMake Error: CMake can not determine linker language for target: tf_contrib_data_dataset_ops\r\nCMake Error: CMake can not determine linker language for target: tf_core_kernels\r\n-- Generating done\r\nCMake Warning:\r\n  Manually-specified variables were not used by the project:\r\n\r\n    SWIG_EXECUTABLE\r\n\r\n\r\n-- Build files have been written to: C:/projects/External/tensorflow/tensorflow/contrib/cmake/build\r\n```\r\nI have Windows 10 x64\r\nAnaconda 3\r\nSwig\r\nBut trying to compile TF and TFLite for using as C++ API on Windows.", "@Arritmic Using release v1.12.0, some contrib/data/kernels files moved to core/kernels/data/experimental. You can search for the filenames that are missing and change the filepaths in the tf_core_framework.cmake and tf_core_kernels.cmake files.", "Thank you so much. :)", "Still can't find `tensorflow\\contrib\\data\\ops\\dataset_ops.cc`, after I copied that file to `contrib\\data\\ops`, it works. starting compiling...", "@Steroes Thank you, it helped.", "@Hema414 You can also build tensorflow using bazelon windows or linux using this [guide](https://github.com/guikarist/tensorflow-windows-build-script).", "@Steroes\uff0cit is work,thanks", "> @Hema414 You can also build tensorflow using bazelon windows or linux using this [guide](https://github.com/guikarist/tensorflow-windows-build-script).\r\n\r\nHow could I use libtensorflow_cc.so in Windows system? ", "@shoutOutYangJie libtensorflow_cc.so is actually a dll file. When you follow the guide in the link i provided earlier, then it will eventually provide you with a ddl (just a rename of the .so file) and a .lib file. With CMake you can link to the dll by linking to the .lib file. If you then include the header files that are required (see also the guide) then you can link it to you example code.", "you means i can use cmake to transfer .so file to .dll file ?\r\n\r\n\r\n \r\n---Original---\r\nFrom: \"Steroes\"<notifications@github.com>\r\nDate: 2019/6/26 14:27:01\r\nTo: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com>;\r\nCc: \"Mention\"<mention@noreply.github.com>;\"shoutOutYangJie\"<312358434@qq.com>;\r\nSubject: Re: [tensorflow/tensorflow] Cmake can't handle deprecated files (assert_next_dataset_op.cc...) (#22634)\r\n\r\n\r\n\r\n@shoutOutYangJie libtensorflow_cc.so is actually a dll file. When you follow the guide in the link i provided earlier, then it will eventually provide you with a ddl (just a rename of the .so file) and a .lib file. With CMake you can link to the dll by linking to the .lib file. If you then include the header files that are required (see also the guide) then you can link it to you example code.\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or mute the thread.", "Hi,\r\nCMake is an open-source, cross-platform family of tools designed to build, test and package software. That means if you are working on Windows, you can compile tensorflow for Windows, and it will be something like libtensorflow_cc.dll (dynamic lib) or libtensorflow_cc.lib (static lib). Then, you will be able to link it in your cmake project, qt project or whatever other build system.\r\n\r\nAlso, if you may compile tensorflow library inside of your cmake project. It would be something like this:\r\n\r\n```\r\nif(NOT TARGET tensorflow) \r\n        get_filename_component(tf_build_DIR ${CMAKE_BINARY_DIR}/intermediates/tensorflow REALPATH)\r\n        file(MAKE_DIRECTORY ${tf_build_DIR})\r\n        add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/../../path/to/tensorflow/ ${tf_build_DIR})\r\nendif()\r\n```\r\n\r\nAnd finally if you are on Windows:\r\n```\r\n\r\nif(CMAKE_SYSTEM_NAME STREQUAL Windows)\r\n    set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} /MD\")\r\n    set_property(TARGET tensorflow PROPERTY CXX_STANDARD 17)\r\n    set_property(TARGET tensorflow PROPERTY CXX_STANDARD_REQUIRED ON)\r\n    target_link_libraries(MyProject  tensorflow)\r\nendif()\r\n```"]}, {"number": 22633, "title": "calibration for tensorRT INT8 in tensorflow failed?", "body": "when I use the resnetV150_frozen.pb model\uff0c I can calibrate the INT8 graph.     However, I try to use another pb model I download from web, I find the INT8 calibration does not work.   I just don't know why?  I doubt the problem may lie in the memory allocation. And when I use data to calibrate the INT8 graph, it always reports the following error.  \u201cERROR:tensorflow:Not a calib graph. Doesn't seem to contain any calibration nodes.\u201d  please help me!\r\n\r\nif f.INT8:\r\n    calibGraph=getINT8CalibGraph(f.batch_size,wsize)\r\n    \r\n    timings,comp,_,mdstats=timeGraph(calibGraph,1,1,dummy_input)\r\n    int8Graph=getINT8InferenceGraph(calibGraph)\r\n    del calibGraph\r\n    timings,comp,valint8,mdstats=timeGraph(int8Graph,f.batch_size,\r\n                                   f.num_loops,dummy_input)\r\n    \r\n  vals=[valnative,valfp32,valfp16,valint8]\r\n\r\n\r\ndef timeGraph(gdef,batch_size=128,num_loops=100,dummy_input=None,timelineName=None):\r\n  tf.logging.info(\"Starting execution\")\r\n  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\n  tf.reset_default_graph()\r\n  g = tf.Graph()\r\n  if dummy_input is None:\r\n    dummy_input = np.random.random_sample((batch_size,3,224,224))\r\n  outlist=[]\r\n  with g.as_default():\r\n\r\n    next_element=tf.constant(dummy_input, dtype= tf.float32)\r\n    out = tf.import_graph_def(\r\n      graph_def=gdef,\r\n      input_map={f.input_node:next_element},\r\n      return_elements=[f.output_node]\r\n    )\r\n    out = out[0].outputs[0]\r\n    outlist.append(out)\r\n    \r\n  timings=[]\r\n  \r\n  with tf.Session(graph=g,config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    tf.logging.info(\"Starting Warmup cycle\")\r\n    \r\n    \r\n\r\n\r\n\r\n    num_iters=1\r\n    for i in range(num_loops):\r\n      tstart=time.time()\r\n      for k in range(num_iters):\r\n        val = sess.run(outlist)\r\n      timings.append((time.time()-tstart)/float(num_iters))\r\n      print(\"iter \",i,\" \",timings[-1])\r\n    comp=sess.run(tf.reduce_all(tf.equal(val[0],val[0])))\r\n    print(\"Comparison=\",comp)\r\n    sess.close()\r\n    tf.logging.info(\"Timing loop done!\")\r\n    return timings,comp,val[0],None\r\n\r\n\r\ndef getINT8InferenceGraph(calibGraph):\r\n  trt_graph=trt.calib_graph_to_infer_graph(calibGraph)\r\n  return trt_graph\r\n\r\ndef getINT8CalibGraph(batch_size=128,workspace_size=1<<25):\r\n  trt_graph = trt.create_inference_graph(getResnet50(f.frozen_graph), [ f.output_node],\r\n                                         max_batch_size=batch_size,\r\n                                         max_workspace_size_bytes=workspace_size,\r\n                                         precision_mode=\"INT8\"\r\n                                       )  \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/2537426/model.zip)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Thank you !  the configuration of the env is:\r\nbazel release 0.17.1\r\ntensorflow installed from source code \r\ncuda9.2 \r\ncudnn 7.2.1\r\nGTX 1070 8G\r\nthe demo is download from https://github.com/tensorflow/models/tree/master/research/tensorrt", "@tilaba Try to reduce batch_size to 1 and run the above code. It depends on how different your model is from the one downloaded from the web. Please compare those two models first. Make sure your inference data is representative of the training data. ", "I tired, but it does not work.\r\n\r\n", "the testing pb model  is attached here . it seems that no node in graph was converted to \"TRTengineops\"\r\nnodes, that's weird. \r\n\r\nhttps://github.com/tilaba/pb_download-link", "Any update on how to resolve this error? I am receiving the same error with tensorflow 1-13-1, \r\nCUDA - 10.0.130 version, NVIDIA driver -  Driver Version: 430.09 \r\nTensorRT version 5.0.2.6 - Stable version for CUDA 10.0"]}, {"number": 22632, "title": "Sometimes Deadlock from TF Dataset using Generator", "body": "Seems to be a normal generator code, but 50% possibility to hang on exit / or print deconstruction exceptions, and it is also possible to exit successfully.\r\nThe Tensorflow version I use v1.10\r\n\r\n```sh\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata_dir, batch_size, depth, height, width = '/tmp/dataset/flowers_jpegs', 64, 3, 224, 224\r\n\r\ndef generator():\r\n  from keras.preprocessing.image import ImageDataGenerator\r\n  from keras.utils import OrderedEnqueuer\r\n  gen = ImageDataGenerator(data_format='channels_first', rescale=1./255, fill_mode='nearest').flow_from_directory(\r\n                           data_dir + '/train', target_size=(height, width), batch_size=batch_size)\r\n  enqueuer = OrderedEnqueuer(gen, use_multiprocessing=False)\r\n  enqueuer.start(workers=16)\r\n  n_classes = gen.num_classes\r\n\r\n  while True:\r\n    batch_xs, batch_ys = next(enqueuer.get())\r\n    yield batch_xs, batch_ys\r\n\r\nds = tf.data.Dataset.from_generator(generator, (tf.float32, tf.float32))\r\nds = ds.prefetch(buffer_size=batch_size)\r\nds_iter = ds.make_one_shot_iterator()\r\n\r\n\r\nwith tf.Session() as sess:\r\n  images, labels = ds_iter.get_next()\r\n  images = tf.reshape(images, (-1, 3, height, width))\r\n  labels = tf.reshape(labels, (-1, 2))\r\n  print(sess.run(images).reshape([-1])[0:8])\r\n\r\n  images, labels = ds_iter.get_next()\r\n  images = tf.reshape(images, (-1, 3, height, width))\r\n  labels = tf.reshape(labels, (-1, 2))\r\n  print(sess.run(images).reshape([-1])[0:8])\r\n```\r\n\r\n**Environment:**\r\n**Have I written custom code:** The code above the the complete single python3 file, and no other libraries include customized changes.\r\n**OS Platform and Distribution: Ubuntu 16.04 x86-64 LTS;\r\n**TensorFlow installed from:** clone `tensorflow:v1.10.1` from github and build for CUDA 1.10 and CUDNN 7.3;\r\n**Bazel version:** 0.15.0\r\n**CUDA/cuDNN version:** CUDA = 10.0; CUDNN = 7.3;\r\n**GPU model and memory:** Tesla V100 (each GPU 16 GB memory)\r\n**Exact command to reproduce:** `python3 above-code.py` (note that \"/tmp/dataset/flowers_jpegs\" is a standard directory including original jpeg images inside serveral subfolders (as different classes) which satisfies Keras ImageData directory input format.)\r\n**Mobile device:** N/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Thanks you, I have already updated these environment information.", "Can you please provide a link to download data_dir as well?\r\nThanks", "You can create a simple data_dir by the following steps:\r\n\r\n```sh\r\n1) mkdir -p /tmp/dataset/flowers_jpegs/train/{rose,daisy}\r\n2) then copy at least 1 or 2 image file (*.jpg) to each sub directory\r\n3) done.\r\n```", "Seems like Tensorflow is easy to throw an unexpected exception when the tensorflow session exits.", "Nagging Assignee @ymodak: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I close this issue because tf.data.Dataset.from_generator is not a good choice for HP training."]}, {"number": 22631, "title": "\"Not found: Resource does not exist\" exception thrown in runtime", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: .\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.5.0\r\n- **Bazel version (if compiling from source)**: /\r\n- **GCC/Compiler version (if compiling from source)**: /\r\n- **CUDA/cuDNN version**: 9.1\r\n- **GPU model and memory**: Tesla M40 24GB, Tesla P100 16GB\r\n- **Exact command to reproduce**: /\r\n\r\n### Describe the problem\r\nAn exception is produced when I try to run a slightly modified version of a implementation of Tacotron-2(https://github.com/Rayhane-mamah/Tacotron-2.git). The problem can be reproduced with batch size=32 or 48, but not with batch size=64, either on single or multiple GPUs.\r\nExact the same exception may occur on another model of my own, when the batch size is large enough.\r\nIn my implementation variables (VariableV2 ops) are placed on CPU0, while other ops are placed on GPU towers.\r\nThe problem looks similar with https://github.com/tensorflow/tensorflow/issues/22094\r\n\r\n**Update**\r\nThe problems only occurs when colocate_gradients_with_ops=True in gradients computation.\r\n### Source code / logs\r\n2018-09-29 05:10:28.440837: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at tensor_array_ops.cc:121 : Not found: Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.\r\nExiting due to exception: Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.\r\n\t [[Node: model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[\"loc:@model...yScatterV3\"], source=\"model/tower_2/gradients\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/tower_2/tacotron/decoder/TensorArray/_231, model/tower_2/tacotron/decoder/while/Exit_1/_1161)]]\r\n\t [[Node: model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch/_1982 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_14621...amicStitch\", _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch)]]\r\n\r\nCaused by op 'model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3', defined at:\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 369, in <module>\r\n    main()\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 365, in main\r\n    train(log_dir, args)\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 231, in train\r\n    model.add_gradients()\r\n  File \"/tmp/apprunner/.working/runtime/app/models/tacotron.py\", line 256, in add_gradients\r\n    grad_vars = optimizer.compute_gradients(self.loss, colocate_gradients_with_ops=True)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 511, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 532, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 701, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 396, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 701, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 161, in _TensorArrayGatherGrad\r\n    .grad(source=grad_source, flow=flow))\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 812, in grad\r\n    return self._implementation.grad(source, flow=flow, name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 239, in grad\r\n    handle=self._handle, source=source, flow_in=flow, name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6229, in tensor_array_grad_v3\r\n    name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\r\n    op_def=op_def)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3', defined at:\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 369, in <module>\r\n    main()\r\n[elided 0 identical lines from previous traceback]\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 365, in main\r\n    train(log_dir, args)\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 229, in train\r\n    feeder.target_lengths, global_step=global_step)\r\n  File \"/tmp/apprunner/.working/runtime/app/models/tacotron.py\", line 115, in initialize\r\n    maximum_iterations=max_iters)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 329, in dynamic_decode\r\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/util/nest.py\", line 374, in map_structure\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/util/nest.py\", line 374, in <listcomp>\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 329, in <lambda>\r\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 856, in stack\r\n    return self._implementation.stack(name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 289, in stack\r\n    return self.gather(math_ops.range(0, self.size()), name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 303, in gather\r\n    element_shape=element_shape)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6018, in tensor_array_gather_v3\r\n    flow_in=flow_in, dtype=dtype, element_shape=element_shape, name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\r\n    op_def=op_def)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.\r\n\t [[Node: model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[\"loc:@model...yScatterV3\"], source=\"model/tower_2/gradients\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/tower_2/tacotron/decoder/TensorArray/_231, model/tower_2/tacotron/decoder/while/Exit_1/_1161)]]\r\n\t [[Node: model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch/_1982 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_14621...amicStitch\", _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch)]]\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.\r\n\t [[Node: model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[\"loc:@model...yScatterV3\"], source=\"model/tower_2/gradients\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/tower_2/tacotron/decoder/TensorArray/_231, model/tower_2/tacotron/decoder/while/Exit_1/_1161)]]\r\n\t [[Node: model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch/_1982 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_14621...amicStitch\", _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 283, in train\r\n    step, loss, _, target_lengths = sess.run([global_step, avg_loss, train_op, feeder.target_lengths])\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.\r\n\t [[Node: model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[\"loc:@model...yScatterV3\"], source=\"model/tower_2/gradients\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/tower_2/tacotron/decoder/TensorArray/_231, model/tower_2/tacotron/decoder/while/Exit_1/_1161)]]\r\n\t [[Node: model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch/_1982 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_14621...amicStitch\", _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch)]]\r\n\r\nCaused by op 'model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3', defined at:\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 369, in <module>\r\n    main()\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 365, in main\r\n    train(log_dir, args)\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 231, in train\r\n    model.add_gradients()\r\n  File \"/tmp/apprunner/.working/runtime/app/models/tacotron.py\", line 256, in add_gradients\r\n    grad_vars = optimizer.compute_gradients(self.loss, colocate_gradients_with_ops=True)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 511, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 532, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 701, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 396, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 701, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 161, in _TensorArrayGatherGrad\r\n    .grad(source=grad_source, flow=flow))\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 812, in grad\r\n    return self._implementation.grad(source, flow=flow, name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 239, in grad\r\n    handle=self._handle, source=source, flow_in=flow, name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6229, in tensor_array_grad_v3\r\n    name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\r\n    op_def=op_def)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3', defined at:\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 369, in <module>\r\n    main()\r\n[elided 0 identical lines from previous traceback]\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 365, in main\r\n    train(log_dir, args)\r\n  File \"/tmp/apprunner/.working/runtime/app/train.py\", line 229, in train\r\n    feeder.target_lengths, global_step=global_step)\r\n  File \"/tmp/apprunner/.working/runtime/app/models/tacotron.py\", line 115, in initialize\r\n    maximum_iterations=max_iters)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 329, in dynamic_decode\r\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/util/nest.py\", line 374, in map_structure\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/util/nest.py\", line 374, in <listcomp>\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 329, in <lambda>\r\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 856, in stack\r\n    return self._implementation.stack(name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 289, in stack\r\n    return self.gather(math_ops.range(0, self.size()), name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 303, in gather\r\n    element_shape=element_shape)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6018, in tensor_array_gather_v3\r\n    flow_in=flow_in, dtype=dtype, element_shape=element_shape, name=name)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\r\n    op_def=op_def)\r\n  File \"/tmp/apprunner/.working/runtime/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Resource __per_step_11/_tensor_arraysmodel/tower_2/tacotron/decoder/TensorArray_3/N10tensorflow11TensorArrayE does not exist.\r\n\t [[Node: model/tower_2/gradients/model/tower_2/tacotron/decoder/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[\"loc:@model...yScatterV3\"], source=\"model/tower_2/gradients\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/tower_2/tacotron/decoder/TensorArray/_231, model/tower_2/tacotron/decoder/while/Exit_1/_1161)]]\r\n\t [[Node: model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch/_1982 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_14621...amicStitch\", _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](model/tower_1/gradients/model/tower_1/tacotron/postnet_conv/conv1d_3/batch_normalization/moments/mean_grad/DynamicStitch)]]\r\n", "comments": ["@alextp wdyt?  it seems like maybe the tensorarray op is being executed on the wrong device.  but i thought the resource inputs force everything on the correct device?", "Resource inputs force everything to run on the correct device. We can rule this out by running the code with tf.ConfigProto(log_device_placement=True) to see where the ops are executing.\r\n\r\nI do not understand though how changing the batch size can affect whether resources are present or not.", ">           Resource inputs force everything to run on the correct device. We can rule this out by running the code with tf.ConfigProto(log_device_placement=True) to see where the ops are executing.\r\n> I do not understand though how changing the batch size can affect whether resources are present or not.\r\n\r\nI've enabled allow_soft_placement, and the problem only occurs when colocate_gradients_with_ops=True in gradients computation. So I guess this colocation option breaks some ops' placement?", "This definitely looks like a placer bug.\n\nCan you find a minimal example to reproduce this?\n\nOn Fri, Oct 5, 2018 at 12:30 AM Mutian He <notifications@github.com> wrote:\n\n>       Resource inputs force everything to run on the correct device. We can rule this out by running the code with tf.ConfigProto(log_device_placement=True) to see where the ops are executing.\n>\n> I do not understand though how changing the batch size can affect whether\n> resources are present or not.\n>\n> I've enabled allow_soft_placement, and the problem only occurs when\n> colocate_gradients_with_ops=True in gradients computation. So I guess this\n> colocation option breaks some ops' placement?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22631#issuecomment-427270497>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxUWBOsAELwIWxkBRGWvtxPhhDNJrks5uhwp8gaJpZM4XA1f2>\n> .\n>\n\n\n-- \n - Alex\n", "Sorry I didn't managed to make a minimal example..", "Can you provide exact steps to reproduce?", "I am facing a similar error mentioned above. I will try my best to help resolve this issue as it benefits my work as well. Please reopen the issue @ebrevdo .\r\n\r\nOS Platform and Distribution: Linux Ubuntu x86_64 - 4.15.0-52-generic (kernel)\r\nTensorFlow installed from: conda 4.7.5\r\nTensorFlow version: 1.13.1\r\nBazel version: N/A\r\nCUDA/cuDNN version: 10.0\r\nGPU model and memory: Tesla V100-SXM2-16GB\r\nExact command to reproduce:\r\nMobile device: N/A\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\ndef discriminative_loss(y_true, y_pred):\r\n    \"\"\"Computes loss for a batch of images\r\n    Args:\r\n        y_true: (n, h, w) where each elements contains the ground truth instance id\r\n        y_pred: (n, h, w, d) d-dimensional vector for each pixel for each image in the batch\r\n    Returns:\r\n        loss\r\n    \"\"\"\r\n    # Compute the loss for each image in the batch\r\n    def compute_loss(input):\r\n        prediction = input[1]\r\n        label = input[0]\r\n\r\n        # Number of clusters in ground truth\r\n        clusters,_ = tf.unique(tf.reshape(label, [-1]))\r\n\r\n        # Compute cluster means and variances for each cluster\r\n        def compute_mean(c):\r\n            mask = tf.equal(label[:,:,0], c)\r\n            masked_pixels = tf.boolean_mask(prediction, mask)\r\n            cluster_mean = tf.reduce_mean(masked_pixels, axis=0)\r\n\r\n            return cluster_mean\r\n\r\n        cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))\r\n        return tf.reduce_mean(cluster_means)\r\n\r\n    # We want to know the loss for each image in the batch\r\n    losses = tf.map_fn(compute_loss, (y_true,y_pred), dtype=(tf.float32))\r\n    return losses\r\n\r\ndef discriminative_loss_working(y_true, y_pred):\r\n    # Compute the loss for only the first image in the batch\r\n\r\n    prediction = y_pred[0]\r\n    label = y_true[0]\r\n\r\n    # Number of clusters in ground truth\r\n    clusters,_ = tf.unique(tf.reshape(label, [-1]))\r\n\r\n    # Compute cluster means and variances for each cluster\r\n    def compute_mean(c):\r\n        mask = tf.equal(label[:,:,0], c)\r\n        masked_pixels = tf.boolean_mask(prediction, mask)\r\n        cluster_mean = tf.reduce_mean(masked_pixels, axis=0)\r\n\r\n        return cluster_mean\r\n\r\n    cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))\r\n    return tf.reduce_mean(cluster_means)\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, input_shape):\r\n        super(MyModel, self).__init__()\r\n        self.conv = tf.keras.layers.Conv2D(filters=4, kernel_size=(1,1))\r\n\r\n    def call(self, input):\r\n        return self.conv(input)\r\n\r\ninput_shape = (1,128,128,3)\r\ndef my_gen():\r\n    while True:\r\n        x = np.random.rand(1,input_shape[1], input_shape[2],3)\r\n        y = np.random.randint(11000, 11015, (input_shape[1], input_shape[2],1))\r\n        yield x,y\r\n\r\ntrain_dataset = tf.data.Dataset.from_generator(\r\n                    my_gen,\r\n                    (tf.float32, tf.float32),\r\n                    (tf.TensorShape([1,128,128,3]),\r\n                     tf.TensorShape([128,128,1])))\r\ntrain_dataset = train_dataset.batch(1)\r\ntrain_dataset = train_dataset.repeat()\r\n\r\nmodel = MyModel(input_shape=input_shape)\r\n\r\n# This is a fix to make loading weights possible\r\n# x = tf.zeros((1,) + input_shape)\r\nx = tf.zeros(input_shape)\r\ny = model(x)\r\n\r\nwith tf.Session(config=config):\r\n    optimizer = tf.keras.optimizers.SGD(lr=0.0001)\r\n    model.compile(loss=discriminative_loss,optimizer=optimizer)\r\n    model.fit(train_dataset, epochs=5, steps_per_epoch=2)\r\n```\r\n[tf_error.log](https://github.com/tensorflow/tensorflow/files/3415129/tf_error.log)\r\n", "@mutiann \r\nWe see that you are using old version of tensorflow 1.x which is out of support window, We recommend that you upgrade to 2.6.0  and let us know if the issue still persists in newer versions .Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22631\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22631\">No</a>\n"]}, {"number": 22630, "title": "Failure [INSTALL_FAILED_NO_MATCHING_ABIS: Failed to extract native libraries, res=-113]", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:1.10\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:0.17\r\n- **GCC/Compiler version (if compiling from source)**:N/a\r\n- **CUDA/cuDNN version**: N/a\r\n- **GPU model and memory**: N/a\r\n- **Exact command to reproduce**: adb install bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI'm trying tensorflow demo app on my computer using the bazel build in the ubuntu terminal. The project has built successfully. When I run **adb install bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk** I got this error\r\n**Failure [INSTALL_FAILED_NO_MATCHING_ABIS: Failed to extract native libraries, res=-113]**\r\n\r\nHow can I solve this problem? Any help, please\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["The is built and is installed successfully, but it give me a run time error after replace the detect.tflife with my detect.tflite and my label map (after making some changes in the build and the DetectorActivity)", "@Elites2017 Try adding this to build.gradle under Android to create apk for each type:\r\n\r\n```\r\nsplits {\r\n        abi {\r\n            enable true\r\n            reset()\r\n            include 'x86', 'armeabi-v7a', 'x86_64'\r\n            universalApk true\r\n        }\r\n    }\r\n```", "I have used that but the app runs and crashes each time I launch it. Runtime Error, I don't know where that error comes from? Before setting my model on the app, at the step to convert my .pb file (graph from the ssd_mobilenet v1_1.0_224) to .tflite file. The model was converted with this warning:\r\n\r\n**the (tflite_graph.pb) file is converted to .tflite with the following warning.\r\nIgnoring unsupported attribute type with key '_output_types'  should I have worried about it ?\r\n\r\nOr do you think this warning can cause the runtime error in the app? \r\n\r\nNB: But the model from tensorflow works in the app, but my own model doesn't**\r\n\r\nExact comand used to convert my .pb file to .tflite file:\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/david/pro/tflite_graph.pb --output_file=/home/david/pro/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess --output_arrays=TFLite_Detection_PostProcess:1 --output_arrays=TFLite_Detection_PostProcess:2 --output_arrays=TFLite_Detection_PostProcess:3 --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6", "@Elites2017 You would need to look into the details of your model. Please check input and output data type. Also make sure all ops used in the model are supported by tflite. Any other error messages did you observe?", "I gave the following model a try [](download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz)\r\n it stiils give me the warning of unsupported types.\r\n\r\nMy model details :Found 1 possible inputs: (name=normalized_input_image_tensor, type=float(1), shape=[1,300,300,3]) \r\nNo variables spotted.\r\nFound 1 possible outputs: (name=TFLite_Detection_PostProcess, op=TFLite_Detection_PostProcess) \r\nFound 5595395 (5.59M) const parameters, 0 (0) variable parameters, and 0 control_edges\r\nOp types used: 451 Const, 389 Identity, 105 Mul, 94 FakeQuantWithMinMaxVars, 70 Add, 35 Sub, 35 Relu6, 35 Rsqrt, 34 Conv2D, 25 Reshape, 13 DepthwiseConv2dNative, 12 BiasAdd, 2 ConcatV2, 1 TFLite_Detection_PostProcess, 1 Squeeze, 1 Sigmoid, 1 RealDiv, 1 Placeholder\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=converting/tflite_graph.pb --show_flops --input_layer=normalized_input_image_tensor --input_layer_type=float --input_layer_shape=1,300,300,3 --output_layer=TFLite_Detection_PostProcess\r\n\r\nBut it still gives me the error **W tensorflow/contrib/lite/toco/tflite/operator.cc:879] Ignoring unsupported attribute type with key '_output_types'** when trying to converting the exported graph (.pb file) to .tflite file although it's converted to .tflite file\r\n\r\nExact command:\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=/home/david/tensorflow/converting/tflite_graph.pb \\\r\n--output_file=/home/david/tensorflow/converting/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays=TFLite_Detection_PostProcess \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops \\\r\n--default_ranges_min=0 \\\r\n--default_ranges_max=6 ", "@Elites2017 \r\nGlad the above approach works for the TensorFlow model. [Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md) is link to tflite supported ops. To tweak your model, please post your code snippet so we can provide guidelines. Since this is not a bug nor feature request, the question is best asked at StackOverflow.\r\n\r\n", "Good that it worked out.", "Are all your questions answered?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "It works without config option while building\r\n\r\nbazel build -c opt  --cxxopt='--std=c++11' //tensorflow/lite/examples/android:tflite_demo"]}, {"number": 22629, "title": "Building tensorflow on Aarch64 (bazel newer than 0.16.0) has no default_toolchain provided.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04 on NVIDIA Jetson AGX Xavier (Aarch64)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n1.10.1\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n0.17.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\nNVIDIA Jetson Xavier\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nCompiling on a Jetson AGX Xavier board yields an error `ERROR: No default_toolchain found for cpu 'aarch64'. Valid cpus are`...\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n### Commentary\r\n\r\nIt looks like the problem is that recent (newer than 0.16.0) versions of bazel provide a named cpu for aarch64 rather than wrapping it in to ARM. The relevent bazel commit is https://github.com/bazelbuild/bazel/commit/886d01c89fca32e46f5841081eb3288e5b4f313b#diff-1ec7b345acf6aff694c52be93dc2f3d1. Using bazel 0.15.2 on aarch64 works (although needs the patch https://github.com/JasonAtNvidia/JetsonTFBuild/blob/master/jetson.patch)", "comments": ["I was able to build and test TF Lite, see https://github.com/tensorflow/tensorflow/pull/16175, with Bazel 0.16.1 on an internal board running Debian arm64 rootfs.", "Perfect. That looks like the same issue. I don't really know bazel all that well-- would tensorflow/BUILD be the file to make a similar change to to get the full build running? My interest is actually in tensorflow_cc.so", "@n-west I didn't have problem building non-TFLite part for CPU only. Maybe you ran into some CUDA configuration problem.", "@n-west  Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22628, "title": "tf-1.10.1 freeze_graph 'list index out of range'", "body": "See [here](https://github.com/tensorflow/tensorflow/issues/22029), I wonder why no one answer my question ???", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "closed as it is duplication of #22626."]}, {"number": 22627, "title": "tf-1.10.1 freeze_graph 'list index out of range'", "body": "See [here](https://github.com/tensorflow/tensorflow/issues/22029), I wonder why no one answer my question ???", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "closed as it is duplication of #22626."]}, {"number": 22626, "title": "tf-1.10.1 freeze_graph 'list index out of range'", "body": "See [here](https://github.com/tensorflow/tensorflow/issues/22029), I wonder why no one answer my question ???", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device"]}, {"number": 22625, "title": "The scope of application for  Post-training quantization", "body": "If I want to optimize tensorflow model running on linux,  can I use Post-training quantization? or can a tensorflow lite model run on linux?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@mingrenbuke Yes. For more information about Post training quantization, please refer [this.](https://www.tensorflow.org/performance/post_training_quantization)", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22624, "title": "The problem of   quantifying model by tf.keras", "body": "I can use the format of h5 to convert my model to tflite, but i can't quantize the model to int8", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@dlml You should be able to quantize your model to int8. Please provide more details about the issue and post any error messages here."]}, {"number": 22623, "title": "Almost 2GB GPU memory missing in Tensorflow as compared to what nvidia-smi reports", "body": "I have a 11GB 1080Ti GPU, NVidia-smi reports 11264MiB memory, Tensorflow reports 9.1GiB memory only. \r\n\r\nI understand that stackoverflow may be a better option to raise this question, but I believe this issue could be a bug or incompatibility among Windows 10, NVidia driver and Tensorflow. \r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nProblem starts to happen when enabling the GPU in python:\r\n    import tensorflow as tf\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)\r\n    config = tf.ConfigProto(gpu_options=gpu_options)\r\n    session = tf.Session(config=config)\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nTensorflow installed from binary\r\n- **TensorFlow version (use command below)**:\r\nprint(tf.__version__)\r\n1.10.0\r\n\r\n- **Python version**:\r\npython --version\r\nPython 3.6.6\r\n\r\n- **CUDA/cuDNN version**:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCuda compilation tools, release 9.0, V9.0.176\r\nNVIDIA-SMI 388.13                 Driver Version: 388.13\r\n\r\n\r\n- **GPU model and memory**:\r\nEVGA 1080 Ti 11GB memory\r\n\r\n- **Exact command to reproduce**:\r\n    import tensorflow as tf\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)\r\n    config = tf.ConfigProto(gpu_options=gpu_options)\r\n    session = tf.Session(config=config)\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI couldn't find similar situation online. After a fresh boot, with the 1080Ti connecting to NO monitor (Intel 4600HD act as the primary display), NVIDIA-SMI reports no process occupying the memory, Tensorflow still reports much less memory than NVIDIA-SMI.\r\n\r\nBelow are the output after issuing the tensorflow-start script with GPU support, and the outputs of the nvidia-smi command. The free GPU memory is 9.1GiB only in tensorflow as compared to 11GB by nvidia-smi. Tensorflow does attempt to allocate 11GiB memory because of per_process_gpu_memory_fraction=1, but cuda reports out of memory error. Using the Allow_growth option won't break the 9.1GiB limits. After launching tensorflow with GPU support, nvidia-smi reports 9460MiB / 11264MiB of memory usage.\r\n\r\n*************************************Output from tensorflow ***********************************\r\nT:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-09-30 03:14:36.523917: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.00GiB freeMemory: 9.10GiB\r\n2018-09-30 03:14:36.528039: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-30 03:14:37.821700: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-30 03:14:37.824541: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971]      0 \r\n2018-09-30 03:14:37.826779: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0:   N \r\n2018-09-30 03:14:37.828695: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11264 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-09-30 03:14:37.835373: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:903] failed to allocate 11.00G (11811160064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2018-09-30 03:14:37.838382: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:903] failed to allocate 9.90G (10630043648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n\r\n**************************nvidia-smi output before launching tensorflow **************************\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 388.13                 Driver Version: 388.13                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108... WDDM  | 00000000:01:00.0 Off |                  N/A |\r\n|  9%   55C    P0    60W / 250W |    132MiB / 11264MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n**************************nvidia-smi output after launching tensorflow **************************\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 388.13                 Driver Version: 388.13                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108... WDDM  | 00000000:01:00.0 Off |                  N/A |\r\n| 10%   55C    P2    57W / 250W |   9460MiB / 11264MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     18556      C   ...cal\\Programs\\Python\\Python36\\python.exe N/A      |\r\n+-----------------------------------------------------------------------------+", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "Hi,\r\n\r\nThe Bazel version should be unrelated to this issue and the problem happens on desktop. It is ok to leave them as N/A. Thanks. \r\n\r\nA strange update:\r\nI could allocate slightly more GPU memory by using the following 0.82 memory fraction:\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.82)\r\n\r\nBut the script would be terminated by the following error even if memory is successfully allocated\r\n2018-09-30 23:44:44.818714: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@maxlawwk You may need to reduce `per_process_gpu_memory_fraction` to a lower value depending upon the model used. Also, make sure to terminate all processes that consume GPU memory. You can refer to this [link](https://medium.com/@lmoroney_40129/installing-tensorflow-with-gpu-on-windows-10-3309fec55a00) to install TensorFlow on Windows. \r\n", "Thank you for you reply. After an excursion to the rendering software community, I found that WDDM 2.x driver required by Windows 10 only tells all processes at most 90% of actual video ram of a GPU, even if it is a secondary, no-monitor-attached GPU. Turn out the available VRAM which can be allocated by a tensorflow script is roughly about 90%*90% = 81%. Only Tesla and some Quadro GPUs support TCC mode which completely skips WDDM driver. This is a common problem for all GPU-memory hungry applications working in Windows 10.\r\n\r\nBack to Windows 7 using WDDM 1.x, applications can use about 95% of video memory. Such a problem also does not exist in Linux. Even worse, both NVidia and Microsoft do not have much motivation about fixing this bug or feature or whatever it is.", "@maxlawwk glad that you are aware of possible solutions. Most drivers reserve a block of memory for tasks other than supporting main applications. Windows for example, unused RAM is always allocated for disk read\\write cache. You may not want to go beyond 90% where performance issues and slowdowns will likely occur.\r\n\r\n\r\n", "Closing as a solution is identified", "@wt-huang I know it is closed but is there workaround for this other than downgrading to W7 or switching to ubuntu?", "@maxlawwk @wt-huang  I am not sure if the numbers on my system fit the calculation and explaination here. I have RTX 2060 having 6 GB VRAM even reported by tensorflow but only 3961 MB is used while creating tensorflow device. Just ~64% of the total.\r\n\r\nAttaching screenshot for reference:\r\n\r\n![image](https://user-images.githubusercontent.com/8385414/128591233-0c3a3592-5e90-44fe-b9e1-8f775634b307.png)\r\n\r\nBelow is what I got as free memory stats from nvidia management library which is consistent with what's reported by nividia-smi(below-2)\r\n\r\n### Nvidia Management Library Output\r\n\r\n![image](https://user-images.githubusercontent.com/8385414/128591694-670a40e1-cdb1-48ad-911f-adfb421edcb9.png)\r\n\r\n### nvidia-smi Output\r\n\r\n![image](https://user-images.githubusercontent.com/8385414/128591724-f181e4bd-3ec1-4329-98dc-444b42b30d91.png)\r\n\r\n\r\nSo, I have hard time believing that 35% of my VRAM is reserved by WDDM driver.", "@skumarlabs , could you manage to solve this issue? Right now I'm facing something similar on my Legion 5 latop with a Geforce GTX 1650 card, Tensorflow see only 2.3 GB from the full 4 GB vram.", "> @skumarlabs , could you manage to solve this issue? Right now I'm facing something similar on my Legion 5 latop with a Geforce GTX 1650 card, Tensorflow see only 2.3 GB from the full 4 GB vram.\r\n\r\n@qspi unfortunately No.", "thanks, good to know. I will post here if I can do any progress on my side.", "any solution found for this? so far I was not even able to use the GPU (I think) until I installed cuda and cudnn libraries from anaconda navigator. But now I found out only 2gb out of 4gb is being used by tensorflow.. That doesn't seem right. \r\n\r\n### JupyterLab Output\r\n![image](https://user-images.githubusercontent.com/46588637/147851135-28babedd-b3bc-4a73-921c-68b2eb803102.png)\r\n\r\n### Anaconda Terminal Output\r\n![image](https://user-images.githubusercontent.com/46588637/147851147-a216bb77-3028-4c7a-a218-5f999d0dd665.png)\r\n\r\n", "Seeing same issue on my end. I have a 4GB GTX 3050Ti with only about 2GB being reported by TensorFlow. Strange that this issue hasn't been resolved, or at least an explanation provided for at this point? "]}, {"number": 22622, "title": "Fixed bug in tflitecamerademo", "body": "It seems that the top K labels should be printed after the inference has been run and the filter has been applied. \r\n\r\nI therefore moved the call to printTopKLabels after this happens, into a block labeled \"Print the results.\"", "comments": ["Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@rogerthat94 could you run `clang-format-3.6.0` and push again? Thanks."]}, {"number": 22621, "title": "FailedPreconditionError in of tf.keras TF1.11 but not TF1.10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 RS4 x64\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 10/7.3\r\n- **GPU model and memory**: GTX1060 6GB\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nget_session = tf.keras.backend.get_session\r\na = tf.Variable([[0.5, 0., 1.], [2., 0., -1.]])\r\ntf.add(a, a).eval(session=get_session())\r\n```\r\n\r\n### Describe the problem\r\nA code that run with TF1.10 but not TF1.11\r\n\r\n### Source code / logs\r\n```\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_1\r\n\t [[{{node Variable_1/read}} = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable_1)]]\r\n```\r\n", "comments": ["I also have this issue on Ubuntu 16.04, Python 3.5.2 and can reproduce it.", "You can initialize the variables using function [global_variables_initializer()](https://www.tensorflow.org/api_docs/python/tf/Variable).\r\nAlso refer [this](https://www.tensorflow.org/api_docs/python/tf/initializers/global_variables).\r\nThis should pass:\r\nimport tensorflow as tf\r\nget_session = tf.keras.backend.get_session\r\na = tf.Variable([[0.5, 0., 1.], [2., 0., -1.]])\r\nwith tf.Session() as sess: \r\n    sess.run(tf.global_variables_initializer())\r\n    print(tf.add(a, a).eval(session=get_session()))", "thank you this is a solution but in the past  (tf 1.10=<) we don't have to do that with tf.keras get_session. Should I adapt this new way to do things or this is a bug?", "You are welcome. The default values for tf.keras have been changed to match those in external Keras. This is a breaking change so yes you will have to adapt to this new way.", "Thanks", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 22620, "title": "Why using Camera2 API for Android Tensorflow Lite Demo?", "body": "Many manufactures didn't implemented Camera2 that well https://github.com/googlesamples/android-Camera2Basic/issues/123\r\n\r\nSo it's better to use legacy Camera API even for Android API >= 25", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@shashishekhar  so you have any insight on this? I suspect we just used the new API arbitrarily.", "@anonym24 : Which manufacturers are you talking about? Do you have an example where the API is correctly implemented by one manufacturer and not implemented by another manufacturer.  \r\n\r\n@aselle: Camera2 API is the official API, Camera 1 API is deprecated.", "The main issue with Camera2 API that I can't set Full HD resolution for preview (1920x1080), though it can be done with Legacy Camera API (TextureView can be set maximum to 1440x1080)\r\n\r\nProofs:\r\n\r\nhttps://github.com/googlesamples/android-Camera2Basic/issues/123\r\nhttps://stackoverflow.com/questions/31362202/android-camera2-output-sizes\r\nhttps://stackoverflow.com/questions/52374888/camera2-1440x1080-is-maximum\r\n\r\nI tried some Xiaomi devices and some users reported this problem on Samsung devices (see stackoverflow links) and some others\r\n\r\nSeems many phone manufactures (Samsung, Xiaomi, Sony and so on) didn't care about Camera2 that much", "@anonym24 : Thank you for the links. This seems like a bad implementation on some particular manufacturers. I hope in future the manufacturers will fix these bugs. Most of our samples are forward looking so we want to support Camera 2 API, rather than Camera 1. This is certainly not a problem with AOSP.\r\n\r\nI agree for production use cases, you may have to do workarounds against bugs and may need to use Camera 1 API for your use case."]}, {"number": 22619, "title": "Unable to use multiple CPU cores in TensorFlow", "body": "This issue was previously asked here on StackOverflow (with no answers at the time of this issue): https://stackoverflow.com/q/52507748/188046\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code**: custom Python code, but no custom ops (Python is linked below)\r\n- **OS Platform and Distribution**: Ubuntu 16.04.5\r\n- **Mobile device**: N/A\r\n- **TensorFlow installed from**: binary, from `tensorflow` PyPI package via pip (also tried from conda with same result)\r\n- **TensorFlow version**:  v1.11.0-0-gc19e29306c 1.11.0\r\n- **Python version**: 3.6.6 from conda\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version**: N/A\r\n- **CUDA/cuDNN version**: N/A, problem occurs on CPU\r\n- **GPU model and memory**: N/A, problem occurs on CPU\r\n- **CPU model**: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz (2 sockets, 10 cores per socket)\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\n\r\nI am unable to configure TensorFlow to use multiple CPU cores for inter-op parallelism on my machine. As described in my [StackOverflow question](https://stackoverflow.com/q/52507748/188046), I have read other answers extensively, and scrubbed the first page of Google search results for several keywords, and tried everything I've seen suggested, and I just can't get this to work.\r\n\r\nI have included a program below that demonstrates the problem. The program calls `matmul` once per core (i.e. weak scaling). I would expect that as the number of cores increases, the running time would stay roughly constant. Instead the running time seems to increase linearly with the core count, indicating that the `matmul` ops are running sequentially, not in parallel.\r\n\r\nI have also confirmed via `htop` that there is only one core on my CPU that is in use when the program is running. The system is otherwise idle. `htop` has the capability to show multiple threads within a process, but I do not even see these (or they are not using enough CPU to show up on the first page of results when sorted by CPU usage).\r\n\r\nHow can I get TensorFlow to execute different operations on different cores in parallel?\r\n\r\nNote:\r\n\r\n  * I am creating a session with multiple CPU devices. I have also tried only creating a single CPU device, and relying entirely on `inter_op_parallelism_threads`. Nothing I have tried has been able to use multiple cores.\r\n  * I can comment out the line `with tf.device(d):`, and it makes no difference.\r\n  * I have tried tracing (see the commented out lines), and the trace seems to reflect what I'd expect. Ops are being assigned to the CPUs like I want them to be. However, they still don't run in parallel.\r\n  * I have also tried generating a Chrome trace (commented out lines at the very bottom). The Chrome trace doesn't seem to be working properly, or at least the reported running times are way off what they should be. So I'm not sure how much this information can be relied upon. Perhaps I'm doing something wrong.\r\n\r\n### Source code / logs\r\n\r\nSource for test_cores.py: https://gist.github.com/elliottslaughter/750a27c832782f4daec8686281027de8\r\n\r\nSample output:\r\n\r\n```\r\n$ python test_cores.py \r\n2018-09-29 09:40:34.489657: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary \r\nwas not compiled to use: AVX2 FMA\r\nRunning on 1 cores\r\n  Assigning matmul to /cpu:0\r\n  Duration (via time.perf_counter()): 3.209691 (693823.014392 - 693819.804701)\r\n  Clock (via time.clock()): 3.205479 (8.912035 - 5.706556)\r\nRunning on 2 cores\r\n  Assigning matmul to /cpu:0\r\n  Assigning matmul to /cpu:1\r\n  Duration (via time.perf_counter()): 6.452124 (693829.493906 - 693823.041782)\r\n  Clock (via time.clock()): 6.449224 (15.388567 - 8.939343)\r\n```", "comments": ["@elliottslaughter Can you try using intra_op_parallelism_threads=2 when you use 2 cpus. i.e\r\n\r\n## When working on 1 cpu\r\n with tf.Session(config=tf.ConfigProto(\r\n            device_count={ \"CPU\": n_cpus },\r\n            inter_op_parallelism_threads=n_cpus,\r\n            intra_op_parallelism_threads=1,\r\n    )) as sess:\r\n\r\n## When working on 2 cpus\r\n with tf.Session(config=tf.ConfigProto(\r\n            device_count={ \"CPU\": n_cpus },\r\n            inter_op_parallelism_threads=n_cpus,\r\n            intra_op_parallelism_threads=2,\r\n    )) as sess:\r\n", "@harshini-gadige Thanks for the suggestion. Though in my real use case, I'm not especially interested in intra-op parallelism, you're right that this is an interesting data point for debugging purposes.\r\n\r\nUnfortunately, changing the value of `intra_op_parallelism` doesn't seem to make a difference. E.g. when I set `intra_op_parallelism_threads=n_cpus` I still get the exact same running times as when I set `intra_op_parallelism_threads=1`. I also get the same running times if I hard-code the value to e.g. `4`.", "@elliottslaughter Hi, thanks for trying it out. When I tried at my end, I got below results. Please check.\r\n\r\nWhen intra_op_parallelism_threads=1,\r\n**Result :** \r\nRunning on 1 cores\r\n  Assigning matmul to /cpu:0\r\n  Duration (via time.perf_counter()): 2.494752 (638917.741260 - 638915.246508)\r\n  Clock (via time.clock()): 2.491805 (5.819835 - 3.328030)\r\n\r\nWhen  intra_op_parallelism_threads=2,\r\n**Result :** \r\nRunning on 2 cores\r\n  Assigning matmul to /cpu:0\r\n  Assigning matmul to /cpu:1\r\n  Duration (via time.perf_counter()): 2.647655 (638947.747454 - 638945.099799)\r\n  Clock (via time.clock()): 5.101251 (7.604016 - 2.502765)\r\n\r\n\r\n", "@elliottslaughter You can see the difference in the Duration from both the results when  intra_op_parallelism_threads changed from 1 to 2.", "Sorry, you're right. I didn't realize that `time.clock` was processor, not wall-time, so I wasn't expecting it to diverge from `time.perf_counter`.\r\n\r\nI uploaded a new version of the script to my gist to make it easier to measure the differences between the different parallelism:\r\n\r\nhttps://gist.github.com/elliottslaughter/750a27c832782f4daec8686281027de8\r\n\r\nUsing the new version makes it clear that I am able to use intra-parallelism, but not inter- or device parallelism. My results look like:\r\n\r\n```\r\n$ time python test_cores.py 1\r\n2018-10-02 10:03:39.670640: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nRunning on 1 CPU devices with 1 inter- and 1 intra-parallelism\r\n  Assigning matmul to /cpu:0\r\n  Duration (via time.perf_counter()): 23.035862 (954428.021369 - 954404.985507)\r\n  Clock (via time.clock()): 23.033090 (28.836996 - 5.803906)\r\n\r\nreal\t0m24.255s\r\nuser\t0m24.341s\r\nsys\t0m4.684s\r\n$ time python test_cores.py 2 --use-intra\r\n2018-10-02 10:05:40.475097: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nRunning on 1 CPU devices with 1 inter- and 2 intra-parallelism\r\n  Assigning matmul to /cpu:0\r\n  Assigning matmul to /cpu:0\r\n  Duration (via time.perf_counter()): 25.217650 (954551.012769 - 954525.795119)\r\n  Clock (via time.clock()): 48.820185 (54.550306 - 5.730121)\r\n\r\nreal\t0m26.358s\r\nuser\t0m48.323s\r\nsys\t0m6.395s\r\n$ time python test_cores.py 2 --use-inter\r\n2018-10-02 10:06:23.214802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nRunning on 1 CPU devices with 2 inter- and 1 intra-parallelism\r\n  Assigning matmul to /cpu:0\r\n  Assigning matmul to /cpu:0\r\n  Duration (via time.perf_counter()): 46.085013 (954614.619281 - 954568.534268)\r\n  Clock (via time.clock()): 46.082734 (51.454584 - 5.371850)\r\n\r\nreal\t0m47.329s\r\nuser\t0m46.348s\r\nsys\t0m5.282s\r\n$ time python test_cores.py 2 --use-dev\r\n2018-10-02 10:07:20.181342: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nRunning on 2 CPU devices with 1 inter- and 1 intra-parallelism\r\n  Assigning matmul to /cpu:0\r\n  Assigning matmul to /cpu:1\r\n  Duration (via time.perf_counter()): 50.084812 (954675.586213 - 954625.501400)\r\n  Clock (via time.clock()): 49.951666 (55.752782 - 5.801116)\r\n\r\nreal\t0m51.306s\r\nuser\t0m50.315s\r\nsys\t0m5.631s\r\n```\r\n\r\nAre you aware of any reasons why I might not be able to use inter- or device parallelism? I really wanted to try model parallelism on some of my programs.\r\n\r\nThanks.", "@azaks2 Hi, could you please look into this issue ?", "@harshini-gadige @azaks2 Is there anything I can do to help debug this? Thanks.", "Take a look at https://github.com/tensorflow/tensorflow/issues/22546. It seems like you running the constant folding pass not the actual graph.", "@elliottslaughter Hi, please keep us posted on your findings based on azaks2 comment.", "I can confirm that the constant folding pass was the issue. Using `tf.placeholder` as suggested does fix the problem. For anyone who comes here later, I've updated my gist, and you can see the difference with the new `--no-const-fold` option:\r\n\r\n```\r\n$ time python test_cores.py 2 --use-inter\r\n2018-10-18 10:07:53.755221: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nRunning on 1 CPU devices with 2 inter- and 1 intra-parallelism\r\n  Assigning matmul to /cpu:0\r\n  Assigning matmul to /cpu:0\r\n  Duration (via time.perf_counter()): 46.047578 (1190217.583074 - 1190171.535495)\r\n  Clock (via time.clock()): 46.045166 (51.374167 - 5.329001)\r\n\r\nreal\t0m47.237s\r\nuser\t0m46.413s\r\nsys\t0m5.146s\r\n$ time python test_cores.py 2 --use-inter --no-const-fold\r\n2018-10-18 10:08:49.194386: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nRunning on 1 CPU devices with 2 inter- and 1 intra-parallelism\r\n  Assigning matmul to /cpu:0\r\n  Assigning matmul to /cpu:0\r\n  Duration (via time.perf_counter()): 25.088182 (1190255.414810 - 1190230.326628)\r\n  Clock (via time.clock()): 48.275543 (57.226300 - 8.950757)\r\n\r\nreal\t0m29.652s\r\nuser\t0m49.870s\r\nsys\t0m7.621s\r\n```\r\n\r\nThe updated gist shows how to use `tf.placeholder` to achieve this:\r\n\r\nhttps://gist.github.com/elliottslaughter/750a27c832782f4daec8686281027de8\r\n\r\nThanks @azaks2 for your help!", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "with tf.Session(config=tf.ConfigProto(\r\nIt doen't help me. I want to restrict tensorflow to only 1 out of 8 available cores but can't do that.\r\ndevice_count={ \"CPU\": 1 },\r\ninter_op_parallelism_threads=1,\r\nintra_op_parallelism_threads=1,\r\nand still count 8 threads. I want only 1 thread.", "Does [this](https://www.tensorflow.org/api_docs/python/tf/config/set_logical_device_configuration) help?"]}]