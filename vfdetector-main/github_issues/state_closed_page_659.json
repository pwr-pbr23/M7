[{"number": 33838, "title": "pip missing all versions beyond 1.13.1", "body": "- Rasbian 9 buster\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 1.14.1\r\n- Python version: 3.7\r\n- Installed using pip\r\n\r\nI need tensorflow version 1.14.0 specifically. Windows has the version I'm looking for. But when i run it on my pi pip says that 1.14.0 is missing and it only lists possible version numbers of 1.11.0, 1.12.0, and 1.13.1.\r\n\r\n**sudo python3.7 -m pip install tensorflow==1.14.0**\r\n\r\n\r\n**Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0 (from versions: 0.11.0, 1.12.0, 1.13.1)\r\nERROR: No matching distribution found for tensorflow==1.14.0**", "comments": ["I tried it in my windows virtualenv.\r\n\r\n```python -m pip install tensorflow==1.14.0```\r\n**Collecting tensorflow==1.14.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==1.14.0**\r\nIt shows versions:none", "I am able to install successfully with the command ```python3 -m pip install tensorflow==1.14.0``` in my ubuntu1804 wsl. \r\n", "Try running the command ```python -m pip search tensorflow``` from your raspbian.\r\nIt should probably be some sort of compatibility issues. \r\nAre you able to install from the links provided in https://www.tensorflow.org/install/pip at the bottom of the page? \r\n\r\nPython 3, Pi0 or Pi1 | https://storage.googleapis.com/tensorflow/raspberrypi/tensorflow-1.14.0-cp34-none-linux_armv6l.whl\r\n-- | --\r\n\r\nPython 3, Pi2 or Pi3 | https://storage.googleapis.com/tensorflow/raspberrypi/tensorflow-1.14.0-cp34-none-linux_armv7l.whl\r\n-- | --\r\n\r\n\r\n\r\n\r\n\r\n", "Try `python -m pip install --upgrade pip` first.\r\n\r\nNewest TF pips are manylinux2010 compliant but old versions of pip don't know about this format.", "> Try running the command `python -m pip search tensorflow` from your raspbian.\r\n> It should probably be some sort of compatibility issues.\r\n> Are you able to install from the links provided in https://www.tensorflow.org/install/pip at the bottom of the page?\r\n> \r\n> Python 3, Pi0 or Pi1\thttps://storage.googleapis.com/tensorflow/raspberrypi/tensorflow-1.14.0-cp34-none-linux_armv6l.whl\r\n> Python 3, Pi2 or Pi3\thttps://storage.googleapis.com/tensorflow/raspberrypi/tensorflow-1.14.0-cp34-none-linux_armv7l.whl\r\n\r\nDo these precompiled wheels work for python 3.7? It says 34", "Official python 3.7 support was added only in 1.14 and later.", "im getting this error when running `sudo python3 -m pip install https://storage.googleapis.com/tensorflow/raspberrypi/tensorflow-1.14.0-cp34-none-linux_armv7\r\nl.whl`:\r\n\r\n\r\n`ERROR: tensorflow-1.14.0-cp34-none-linux_armv7l.whl is not a supported wheel on this platform.`", "Doe `cp34-none-linux_armv71` show up in `pip debug --verbose`?", "this was what was printed. Something similar but not exactly.\r\n\r\n> WARNING: This command is only meant for debugging. Do not use this with automation for parsing and getting these details, since the output and options of this command may change without notice.\r\npip version: pip 19.3.1 from /home/pi/.local/lib/python3.7/site-packages/pip (python 3.7)\r\nsys.version: 3.7.3 (default, Apr  3 2019, 05:39:12) \r\n[GCC 8.2.0]\r\nsys.executable: /usr/bin/python3.7\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: UTF-8\r\nsys.platform: linux\r\nsys.implementation:\r\n  name: cpython\r\nCompatible tags: 29\r\n  cp37-cp37m-manylinux2014_armv7l\r\n  cp37-cp37m-linux_armv7l\r\n  cp37-abi3-manylinux2014_armv7l\r\n  cp37-abi3-linux_armv7l\r\n  cp37-none-manylinux2014_armv7l\r\n  cp37-none-linux_armv7l\r\n  cp36-abi3-manylinux2014_armv7l\r\n  cp36-abi3-linux_armv7l\r\n  cp35-abi3-manylinux2014_armv7l\r\n  cp35-abi3-linux_armv7l\r\n  cp34-abi3-manylinux2014_armv7l\r\n  cp34-abi3-linux_armv7l\r\n  cp33-abi3-manylinux2014_armv7l\r\n  cp33-abi3-linux_armv7l\r\n  cp32-abi3-manylinux2014_armv7l\r\n  cp32-abi3-linux_armv7l\r\n  py3-none-manylinux2014_armv7l\r\n  py3-none-linux_armv7l\r\n  cp37-none-any\r\n  cp3-none-any\r\n  py37-none-any\r\n  py3-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any", "This means that that specific pip won't be able to load/install. Due to toolchain differences.\r\n\r\nYou might need to compile from source", "if i compile from source, wouldn't that just give me the same .whl that is downloadable from the links in the website? What difference would it make?", "i have been able to install it by simply wget'ing the whl and renaming from tensorflow-1.14.0-cp34-none-linux_armv7l.whl to tensorflow-1.14.0-cp37-none-linux_armv7l.whl. with the only needed difference being the python version in the filename, as it would comply with pip. But I'm now experiencing a new issue when importing that i will make a separate issue on. #33872 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33838\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33838\">No</a>\n", "The tags in the filename specify C++ ABI compatibility. Compiling from source will ensure you get all binaries that will run on your system.\r\n\r\nYes, you can install it (that is, unzip the files in the wheel) if you rename. But you can't use it without crashes and errors.", "https://github.com/tensorflow/tensorflow/issues/33872#issuecomment-548476622", "I have mac and I doing a fresh install and I have the same error\r\n```\r\npip3  install tensorflow==1.14\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==1.14\r\n\r\npip3 install --upgrade pip\r\nRequirement already up-to-date\r\n\r\n\r\n```\r\nI have to downgrade to python version 3.7.4\r\nit doesn't works on python 3.8.0", "We haven't released any pip package for 3.8", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "The issue is still relevant since there are no working Tensorflow 2.x builds for Python 3.7 (or newer) for the Raspberry Pi.", "Please open a new issue for that. At the moment we no longer have anyone working on the RPi builds, instead we recommend using TFLite.\r\n\r\nClosing this issue as it seems to have been caused by a very old `pip` and is old.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33838\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33838\">No</a>\n"]}, {"number": 33837, "title": "Using intermediate layer outputs in custom loss function causes CUDA_ERROR_OUT_OF_MEMORY", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n**Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Ubuntu 19.10**\r\n- TensorFlow installed from (source or binary):\r\n**Binary*\r\n- TensorFlow version (use command below):\r\n**2.1.0-dev20191027 (same results in 2.0)**\r\n- Python version:\r\n**3.7.5rc1**\r\n- CUDA/cuDNN version:\r\n**10.1**\r\n- GPU model and memory:\r\n**2080 TI 12gb + driver 430.50**\r\n\r\n**Describe the current behavior**\r\nI'm trying to add a kl divergence regularizer which relies on 2 intermediate layers of the encoder model. This custom loss in Keras model returns a tensor instead of a scalar when using outputs of intermediate layers. This appears to be adding operators to the graph at each iteration until I get a CUDA out of memory error.  \r\n\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \r\n__________________________________________________________________________________________________\r\nconv2d (Conv2D)                 (None, 32, 32, 32)   1568        input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\nconv2d_1 (Conv2D)               (None, 16, 16, 32)   16416       conv2d[0][0]                     \r\n__________________________________________________________________________________________________\r\nconv2d_2 (Conv2D)               (None, 8, 8, 32)     16416       conv2d_1[0][0]                   \r\n__________________________________________________________________________________________________\r\nconv2d_3 (Conv2D)               (None, 4, 4, 32)     16416       conv2d_2[0][0]                   \r\n__________________________________________________________________________________________________\r\nflatten (Flatten)               (None, 512)          0           conv2d_3[0][0]                   \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 256)          131328      flatten[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 256)          65792       dense[0][0]                      \r\n__________________________________________________________________________________________________\r\ndense_2 (Dense)                 (None, 32)           8224        dense_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_3 (Dense)                 (None, 32)           8224        dense_1[0][0]                    \r\n__________________________________________________________________________________________________\r\nreparameterize (Reparameterize) (None, 32)           0           dense_2[0][0]                    \r\n                                                                 dense_3[0][0]                    \r\n__________________________________________________________________________________________________\r\nmodel_1 (Model)                 (None, 64, 64, 3)    256611      reparameterize[0][0]             \r\n==================================================================================================\r\nTotal params: 520,995\r\nTrainable params: 520,995\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n2019-10-29 20:54:07.009002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-29 20:54:07.911330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n |----------------------------------------| 0.0%  recon: 163.58 kl: 0.0 capacity (nats): 0.0 Epoch: 1/10 Loss: Tensor(\"add_2:0\", shape=(), dtype=float32) SAMPLING FRAME 1\r\n |----------------------------------------| 0.1%  recon: 183.64 kl: 0.0 capacity (nats): 0.0 Epoch: 1/10 Loss: Tensor(\"add_6:0\", shape=(), dtype=float32) SAMPLING FRAME 2\r\n |----------------------------------------| 0.3%  recon: 171.7 kl: 0.0 capacity (nats): 0.01 Epoch: 1/10 Loss: Tensor(\"add_18:0\", shape=(), dtype=float32) SAMPLING FRAME 3\r\n |----------------------------------------| 0.6%  recon: 180.35 kl: 0.0 capacity (nats): 0.02 Epoch: 1/10 Loss: Tensor(\"add_38:0\", shape=(), dtype=float32) SAMPLING FRAME 4\r\n |----------------------------------------| 1.1%  recon: 179.76 kl: 0.0 capacity (nats): 0.03 Epoch: 1/10 Loss: Tensor(\"add_66:0\", shape=(), dtype=float32) SAMPLING FRAME 5\r\n |\u2588---------------------------------------| 1.7%  recon: 182.14 kl: 0.0 capacity (nats): 0.04 Epoch: 1/10 Loss: Tensor(\"add_102:0\", shape=(), dtype=float32) SAMPLING FRAME 6\r\n |\u2588---------------------------------------| 2.4%  recon: 175.29 kl: 0.0 capacity (nats): 0.05 Epoch: 1/10 Loss: Tensor(\"add_142:0\", shape=(), dtype=float32)2019-10-29 20:54:13.957278: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.18G (1263714304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n |\u2588---------------------------------------| 2.5%  recon: 182.22 kl: 0.0 capacity (nats): 0.05 Epoch: 1/10 Loss: Tensor(\"add_146:0\", shape=(), dtype=float32) SAMPLING FRAME 7\r\n |\u2588---------------------------------------| 2.7%  recon: 168.09 kl: 0.0 capacity (nats): 0.06 Epoch: 1/10 Loss: Tensor(\"add_158:0\", shape=(), dtype=float32)2019-10-29 20:54:14.610518: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 120.52M (126371328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-10-29 20:54:14.610906: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 120.52M (126371328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n```\r\nNotice that \"loss: \" prints something like Tensor(\"add_146:0\", shape=(), dtype=float32) when I call train_on_batch. Previously this was just a scalar. \r\n\r\n**Describe the expected behavior**\r\nI expect kl_divergence(X, X_pred) in my code to return a scalar, and no out of memory errors.\r\n\r\n**Code to reproduce the issue**\r\n``` bash\r\ngit clone https://github.com/alexbooth/Beta-VAE-Tensorflow-2.0.git  \r\ncd Beta-VAE-Tensorflow-2.0  \r\npython3 train.py --batch_size=512\r\n```\r\nModel + Loss\r\n``` python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense\r\nfrom tensorflow.keras.layers import Flatten, Reshape, Input\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n\r\ndef Conv(n_filters, filter_width, strides=2, activation=\"relu\", name=None):\r\n    return Conv2D(n_filters, filter_width, \r\n                  strides=strides, padding=\"same\", activation=activation, name=name)\r\n\r\n\r\ndef Deconv(n_filters, filter_width, strides=2, activation=\"relu\", name=None):\r\n    return Conv2DTranspose(n_filters, filter_width, \r\n                  strides=strides, padding=\"same\", activation=activation, name=name)\r\n\r\n\r\nclass Reparameterize(tf.keras.layers.Layer):\r\n    \"\"\"\r\n    Custom layer.\r\n     \r\n    Reparameterization trick, sample random latent vectors Z from \r\n    the latent Gaussian distribution which has the following parameters \r\n\r\n    mean = Z_mu\r\n    std = exp(0.5 * Z_logvar)\r\n    \"\"\"\r\n    def call(self, inputs):\r\n        Z_mu, Z_logvar = inputs\r\n        epsilon = tf.random.normal(tf.shape(Z_mu))\r\n        sigma = tf.math.exp(0.5 * Z_logvar)\r\n        return Z_mu + sigma * epsilon\r\n\r\n\r\nclass BetaVAE:\r\n    def __init__(self, input_shape, latent_dim=32, loss_type=\"mse\", learning_rate=0.0005):\r\n        self.latent_dim = latent_dim\r\n        self.C = 0\r\n        self.gamma = 100\r\n\r\n        channels = input_shape[2]\r\n\r\n        # create encoder\r\n        encoder_input = Input(shape=input_shape)\r\n        X = Conv(32, 4)(encoder_input)\r\n        X = Conv(32, 4)(X)\r\n        X = Conv(32, 4)(X)\r\n        X = Conv(32, 4)(X)\r\n        X = Flatten()(X)\r\n        X = Dense(256, activation=\"relu\")(X)\r\n        X = Dense(256,  activation=\"relu\")(X)\r\n        Z_mu = Dense(self.latent_dim)(X)\r\n        Z_logvar = Dense(self.latent_dim, activation=\"relu\")(X)\r\n        Z = Reparameterize()([Z_mu, Z_logvar])\r\n\r\n        # create decoder\r\n        output_activation = \"sigmoid\" if channels == 1 else None\r\n        decoder_input = Input(shape=(self.latent_dim,))\r\n        X = Dense(256,  activation=\"relu\")(decoder_input)\r\n        X = Dense(256,  activation=\"relu\")(X)\r\n        X = Dense(512,  activation=\"relu\")(X)\r\n        X = Reshape((4, 4, 32))(X)\r\n        X = Deconv(32, 4)(X)\r\n        X = Deconv(32, 4)(X)\r\n        X = Deconv(32, 4)(X)\r\n        decoder_output = Deconv(channels, 4, activation=output_activation)(X)\r\n\r\n        # define vae losses\r\n        def reconstruction_loss(X, X_pred):\r\n            if loss_type == \"bce\":\r\n                bce = tf.losses.BinaryCrossentropy() \r\n                return bce(X, X_pred) * np.prod(input_shape)\r\n            elif loss_type == \"mse\":\r\n                mse = tf.losses.MeanSquaredError()\r\n                return mse(X, X_pred) * np.prod(input_shape)\r\n            else:\r\n                raise ValueError(\"Unknown reconstruction loss type. Try 'bce' or 'mse'\")\r\n\r\n        def kl_divergence(X, X_pred):\r\n            self.C += (1/1440) # TODO use correct scalar\r\n            self.C = min(self.C, 35) # TODO make variable\r\n            kl = -0.5 * tf.reduce_mean(1 + Z_logvar - Z_mu**2 - tf.math.exp(Z_logvar))\r\n            return self.gamma * tf.math.abs(kl - self.C)\r\n\r\n        def loss(X, X_pred):\r\n            return reconstruction_loss(X, X_pred) + kl_divergence(X, X_pred)\r\n\r\n        # create models\r\n        self.encoder = Model(encoder_input, [Z_mu, Z_logvar, Z])\r\n        self.decoder = Model(decoder_input, decoder_output)\r\n        self.vae = Model(encoder_input, self.decoder(Z))\r\n        self.vae.compile(optimizer='adam', loss=loss, metrics=[reconstruction_loss, kl_divergence])\r\n\r\n    def predict(self, inputs, mode=None):\r\n        if mode == \"encode\":\r\n            _, _, self.Z = self.encoder.predict(inputs)\r\n            return self.Z\r\n        if mode == \"decode\":\r\n            return self.decoder.predict(inputs)\r\n        if mode == None:\r\n            return self.vae.predict(inputs) \r\n        raise ValueError(\"Unsupported mode during call to model.\") \r\n```", "comments": ["@alexbooth, \r\nThis issue shows when the GPU resources are already used by another resources or process. Please do check if any parallel python process is being used?.\r\nAnd also, check which programs take memory on your GPU using the `nvidia-smi` command in a terminal. Let us know how it progresses. Thanks!\r\n", "Hi @gadagashwini , thanks for the reply. Here is the output of ```nvidia-smi``` at two points during execution.  \r\nJust after training begins:\r\n```\r\nFri Nov  1 19:48:56 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A |\r\n| 14%   55C    P0    73W / 250W |   3834MiB / 10997MiB |     18%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1242      G   /usr/lib/xorg/Xorg                            36MiB |\r\n|    0      1737      G   /usr/lib/xorg/Xorg                           341MiB |\r\n|    0      1946      G   /usr/bin/gnome-shell                         224MiB |\r\n|    0      2344      G   ...quest-channel-token=2209957141405069853   238MiB |\r\n|    0      4386      G   /home/alex/.steam/ubuntu12_32/steam           80MiB |\r\n|    0      4413      G   ./steamwebhelper                              34MiB |\r\n|    0      4452      G   ...quest-channel-token=8933237957317349096    71MiB |\r\n|    0     31754      C   python3                                     2741MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nJust before the script crashes:\r\n```\r\nFri Nov  1 19:49:01 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A |\r\n| 14%   55C    P0    53W / 250W |   9978MiB / 10997MiB |     15%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1242      G   /usr/lib/xorg/Xorg                            36MiB |\r\n|    0      1737      G   /usr/lib/xorg/Xorg                           341MiB |\r\n|    0      1946      G   /usr/bin/gnome-shell                         224MiB |\r\n|    0      2344      G   ...quest-channel-token=2209957141405069853   238MiB |\r\n|    0      4386      G   /home/alex/.steam/ubuntu12_32/steam           80MiB |\r\n|    0      4413      G   ./steamwebhelper                              34MiB |\r\n|    0      4452      G   ...quest-channel-token=8933237957317349096    71MiB |\r\n|    0     31754      C   python3                                     8885MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nSo it is indeed an out of memory error. It seems like new ops are being added to the graph until the oom error.\r\n\r\nThe only parallel process I think could be tf.data.Dataset which I had set to prefetch with an autotuned buffer size and also generate data in a ```while True``` loop. But I replaced this with a generic serial batch function and still had the same issue\r\n```python\r\nself.dataset = tf.data.Dataset.from_generator(self.generate, tf.float32, output_shapes=self.train_input_shape)\r\nself.dataset = self.dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\nself.dataset = self.dataset.batch(batch_size=self.batch_size)\r\nself.dataset_iterator = iter(self.dataset)\r\n```\r\n\r\nAlso wanted to share something interesting. When I compile the model with the flag ```experimental_run_tf_function=False``` I get the expected output from my custom loss function (scalar value instead of tensor). But my hyper parameter (```self.C```) in ```kl_divergence(X, X_pred)``` does not update. \r\n\r\nUsage of flag inspired from this tfp issue: https://github.com/tensorflow/probability/issues/519\r\n\r\nOutput from that:\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \r\n__________________________________________________________________________________________________\r\nconv2d (Conv2D)                 (None, 32, 32, 32)   1568        input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\nconv2d_1 (Conv2D)               (None, 16, 16, 32)   16416       conv2d[0][0]                     \r\n__________________________________________________________________________________________________\r\nconv2d_2 (Conv2D)               (None, 8, 8, 32)     16416       conv2d_1[0][0]                   \r\n__________________________________________________________________________________________________\r\nconv2d_3 (Conv2D)               (None, 4, 4, 32)     16416       conv2d_2[0][0]                   \r\n__________________________________________________________________________________________________\r\nflatten (Flatten)               (None, 512)          0           conv2d_3[0][0]                   \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 256)          131328      flatten[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 256)          65792       dense[0][0]                      \r\n__________________________________________________________________________________________________\r\ndense_2 (Dense)                 (None, 32)           8224        dense_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_3 (Dense)                 (None, 32)           8224        dense_1[0][0]                    \r\n__________________________________________________________________________________________________\r\nreparameterize (Reparameterize) (None, 32)           0           dense_2[0][0]                    \r\n                                                                 dense_3[0][0]                    \r\n__________________________________________________________________________________________________\r\nmodel_1 (Model)                 (None, 64, 64, 3)    256611      reparameterize[0][0]             \r\n==================================================================================================\r\nTotal params: 520,995\r\nTrainable params: 520,995\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n2019-11-01 19:38:24.141589: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-01 19:38:24.298499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n |----------------------------------------| 0.0%  recon: 174.61 kl: 0.06 capacity (nats): 0.0 Epoch: 1/10 Loss: 174.74608 SAMPLING FRAME 1\r\n |----------------------------------------| 0.1%  recon: 172.69 kl: 0.0 capacity (nats): 0.0 Epoch: 1/10 Loss: 172.76025 SAMPLING FRAME 2\r\n |----------------------------------------| 0.3%  recon: 169.32 kl: 0.03 capacity (nats): 0.0 Epoch: 1/10 Loss: 169.3567 SAMPLING FRAME 3\r\n |----------------------------------------| 0.6%  recon: 165.3 kl: 0.06 capacity (nats): 0.0 Epoch: 1/10 Loss: 165.30199 SAMPLING FRAME 4\r\n |----------------------------------------| 1.1%  recon: 163.67 kl: 0.09 capacity (nats): 0.0 Epoch: 1/10 Loss: 163.6917 SAMPLING FRAME 5\r\n |\u2588---------------------------------------| 1.7%  recon: 153.0 kl: 0.41 capacity (nats): 0.0 Epoch: 1/10 Loss: 153.34195 SAMPLING FRAME 6\r\n |\u2588---------------------------------------| 2.5%  recon: 157.61 kl: 0.95 capacity (nats): 0.0 Epoch: 1/10 Loss: 158.49252 SAMPLING FRAME 7\r\n |\u2588---------------------------------------| 3.4%  recon: 142.18 kl: 4.65 capacity (nats): 0.0 Epoch: 1/10 Loss: 146.75726 SAMPLING FRAME 8\r\n |\u2588\u2588--------------------------------------| 4.4%  recon: 120.83 kl: 13.64 capacity (nats): 0.0 Epoch: 1/10 Loss: 134.39842 SAMPLING FRAME 9\r\n |\u2588\u2588--------------------------------------| 5.6%  recon: 106.34 kl: 18.35 capacity (nats): 0.0 Epoch: 1/10 Loss: 124.62601 SAMPLING FRAME 10\r\n |\u2588\u2588\u2588-------------------------------------| 6.9%  recon: 100.26 kl: 16.07 capacity (nats): 0.0 Epoch: 1/10 Loss: 116.25942 SAMPLING FRAME 11\r\n```", "Issue is replicating with Tf 2.0.\r\nPlease see the colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/1d01d8742c17caaca5de509e82de3263/untitled232.ipynb). Thanks!", "Thanks. Please let me know if you need any more info on my end. ", "Any updates?", "Hello! This issue feels like have the same root cause https://github.com/tensorflow/tensorflow/issues/41200", "@alexbooth @RomanGirin I tried to reproduce this issue using latest version of tf-nightly and ran into this issue. You can find the gist [here](https://colab.research.google.com/gist/gowthamkpr/278d021807df0870d9d2d6911a90fefc/untitled317.ipynb). Can you please try reproducing it and let us know if the issue  still persists. Thanks!", "@gowthamkpr I used simplified example to model the issue. And it seems it's design issue. I described fix for my simple example in this comment https://github.com/tensorflow/tensorflow/issues/41200#issuecomment-695779647\r\nPlease, take a look it may apply to this issue also", "CUDA_ERROR_OUT_OF_MEMORY error is not seen in the latest 2.3 and the tf-nightly version. Also with reference to [#41200 (comment)](https://github.com/tensorflow/tensorflow/issues/41200#issuecomment-695779647) Please let us know if it is ok to close this issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33837\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33837\">No</a>\n", "Closing the issue. Please feel free to reopen it..", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33837\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33837\">No</a>\n"]}, {"number": 33836, "title": "Support CUDNN depthwise convolution", "body": "This PR adds the CUDNN depthwise convolution as the default implementations of DepthwiseConv2dNative, DepthwiseConv2dNativeBackpropInput, DepthwiseConv2dNativeBackpropFilter.\r\n\r\nCuDNN 7.6.3 improves the performance of some cases for the depthwise convolution. Details at:\r\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_763.html#rel_763\r\n\r\n@nluehr ", "comments": ["Very cool! @benbarsdell does this mean the depthwise convolutions can be enabled in the auto mixed precision graph rewrite?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/552d6a22f66f6f2c7eb749ac3e79d83a9476b0e2/tensorflow/core/grappler/optimizers/auto_mixed_precision_lists.h#L76-L80", "Thanks for the contribution @houtoms!   LGTM but I haven't worked on TF in like 3 years so I'm not sure there's some kind of special release process nowadays.   Adding @tatianashp to redirect as needed.", "Christian - can you take a look?\r\n\r\n/cc @aaroey @sanjoy ", "Any updates on this?  Fast CUDNN depthwise convolutions would be a great improvement for TF users.", "@sanjoy Tx for the review. More changes are made. PTAL.", "> Any updates on this? Fast CUDNN depthwise convolutions would be a great improvement for TF users.\r\n\r\nTx for the comment. We are working on it. For the auto mixed precision support, we might still need to wait for a more universal support from CUDNN and then we can enable it in AMP.", "@sanjoy Yes, I think we need to put those filter dim restrictions into the conditions. It would make things clearer. Codes are updated. PTAL.\r\n\r\nAlso for the autotune part, the TF_CUDNN_USE_AUTOTUNE controls the cudnn autotuning and is set to be true by default. And we can find that it turns on the autotune here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/depthwise_conv_op.cc#L298.", "@sanjoy PTAL.", "@sanjoy Just get some feedback from CuDNN and CuDNN depwise convolution actually assumes the multiplier needs to be one. So, we need to make sure the `in_depth == out_depth` to trigger the fast CuDNN depwise conv path. I've made further changes, PTAL.", "The CPU tests fail in compilation and I think I need to put the CUDNN_VERSION back to macros, since the CPU version won't include the cudnn.h and cannot recognize CUDNN_VERSION. This commit should be able to fix those tests. @sanjoy ", "@houtoms I found that using NHWC layout will turn on cuDNN depth wise convolution in backward prop but not forward (the speedup is great but could be better withcuDNN enabled in forward pass).\r\n\r\nSee the following log:\r\n\r\n```\r\n2020-02-11 17:54:40.120856: I tensorflow/core/kernels/depthwise_conv_op.cc:394] DepthwiseConv2dNative:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 0\r\n2020-02-11 17:54:40.121049: I tensorflow/core/kernels/depthwise_conv_op.cc:394] DepthwiseConv2dNative:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 0\r\n2020-02-11 17:54:40.121305: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:604] DepthwiseConv2d: DepthwiseConv2DBackpropInput Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n2020-02-11 17:54:40.121323: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:625] DepthwiseConv2dNativeBackpropInput:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 0\r\n2020-02-11 17:54:40.121345: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1100] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n2020-02-11 17:54:40.121354: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1120] DepthwiseConv2dNativeBackpropFilter:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n2020-02-11 17:54:40.121643: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1100] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; stride = 1, pad_rows = 0, pad_cols = 0, output: [512, 64, 64, 6]\r\n2020-02-11 17:54:40.121660: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1120] DepthwiseConv2dNativeBackpropFilter:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 1\r\n2020-02-11 17:54:40.365886: I tensorflow/core/kernels/depthwise_conv_op.cc:394] DepthwiseConv2dNative:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 0\r\n2020-02-11 17:54:40.366915: I tensorflow/core/kernels/depthwise_conv_op.cc:394] DepthwiseConv2dNative:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 0\r\n2020-02-11 17:54:40.367243: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:604] DepthwiseConv2d: DepthwiseConv2DBackpropInput Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n2020-02-11 17:54:40.367262: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:625] DepthwiseConv2dNativeBackpropInput:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 0\r\n2020-02-11 17:54:40.367286: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1100] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n2020-02-11 17:54:40.367297: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1120] DepthwiseConv2dNativeBackpropFilter:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n2020-02-11 17:54:40.367630: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1100] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; stride = 1, pad_rows = 0, pad_cols = 0, output: [512, 64, 64, 6]\r\n2020-02-11 17:54:40.367646: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1120] DepthwiseConv2dNativeBackpropFilter:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 1\r\n```\r\n\r\nTurning on universally with NGC TF:\r\n\r\n```\r\n2020-02-11 09:58:16.110166: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1087] DepthwiseConv2dNativeBackpropFilter:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n2020-02-11 09:58:16.110603: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1073] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; stride = 1, pad_rows = 0, pad_cols = 0, output: [512, 64, 64, 6]\r\n2020-02-11 09:58:16.110620: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1087] DepthwiseConv2dNativeBackpropFilter:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 1\r\n2020-02-11 09:58:16.284146: I tensorflow/core/kernels/depthwise_conv_op.cc:377] DepthwiseConv2dNative:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 1\r\n2020-02-11 09:58:16.284413: I tensorflow/core/kernels/depthwise_conv_op.cc:377] DepthwiseConv2dNative:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n2020-02-11 09:58:16.284958: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:591] DepthwiseConv2d: DepthwiseConv2DBackpropInput Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n2020-02-11 09:58:16.284980: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:606] DepthwiseConv2dNativeBackpropInput:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n2020-02-11 09:58:16.285029: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1073] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n2020-02-11 09:58:16.285042: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1087] DepthwiseConv2dNativeBackpropFilter:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n2020-02-11 09:58:16.285488: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1073] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; stride = 1, pad_rows = 0, pad_cols = 0, output: [512, 64, 64, 6]\r\n2020-02-11 09:58:16.285507: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1087] DepthwiseConv2dNativeBackpropFilter:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 1\r\n```\r\n\r\nIs this behavior intended?", "@byronyi Yes, I think this is intended. As https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_763.html#rel_763 mentions, only the wgrad is supported when NHWC is used. In the future cuDNN release, there will be more faster NHWC kernels and then the conditions would be relaxed.", "@houtoms Thanks for the explanation! \r\n\r\nThis patch along with #31597 by @AyanmoI greatly helps us training MobileNet/ShuffleNet using Volta.", "Turns out the performance gain is sometimes negated by layout optimizer, as shown in the following logs:\r\n\r\n```\r\n2020-03-26 18:31:36.956273: I tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc:659] GenericLayoutOptimizer: transforming node 'title_match/separable_conv2d/depthwise' with op 'DepthwiseConv2dNative' from data format 'NCHW' to 'NHWC'\r\n2020-03-26 18:31:36.957056: I tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc:659] GenericLayoutOptimizer: transforming node 'title_match/separable_conv2d_1/depthwise' with op 'DepthwiseConv2dNative' from data format 'NCHW' to 'NHWC'\r\n2020-03-26 18:31:36.957293: I tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc:659] GenericLayoutOptimizer: transforming node 'summary_match/separable_conv2d/depthwise' with op 'DepthwiseConv2dNative' from data format 'NCHW' to 'NHWC'\r\n2020-03-26 18:31:36.957461: I tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc:659] GenericLayoutOptimizer: transforming node 'summary_match/separable_conv2d_1/depthwise' with op 'DepthwiseConv2dNative' from data format 'NCHW' to 'NHWC'\r\n```\r\n\r\nPing @ezhulenev; might related to #23847.", "cc @andyly: 48140fa and f917d99 might be related, too. ", "> @houtoms I found that using NHWC layout will turn on cuDNN depth wise convolution in backward prop but not forward (the speedup is great but could be better withcuDNN enabled in forward pass).\r\n> \r\n> See the following log:\r\n> \r\n> ```\r\n> 2020-02-11 17:54:40.120856: I tensorflow/core/kernels/depthwise_conv_op.cc:394] DepthwiseConv2dNative:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 0\r\n> 2020-02-11 17:54:40.121049: I tensorflow/core/kernels/depthwise_conv_op.cc:394] DepthwiseConv2dNative:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 0\r\n> 2020-02-11 17:54:40.121305: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:604] DepthwiseConv2d: DepthwiseConv2DBackpropInput Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n> 2020-02-11 17:54:40.121323: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:625] DepthwiseConv2dNativeBackpropInput:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 0\r\n> 2020-02-11 17:54:40.121345: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1100] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n> 2020-02-11 17:54:40.121354: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1120] DepthwiseConv2dNativeBackpropFilter:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n> 2020-02-11 17:54:40.121643: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1100] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; stride = 1, pad_rows = 0, pad_cols = 0, output: [512, 64, 64, 6]\r\n> 2020-02-11 17:54:40.121660: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1120] DepthwiseConv2dNativeBackpropFilter:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 1\r\n> 2020-02-11 17:54:40.365886: I tensorflow/core/kernels/depthwise_conv_op.cc:394] DepthwiseConv2dNative:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 0\r\n> 2020-02-11 17:54:40.366915: I tensorflow/core/kernels/depthwise_conv_op.cc:394] DepthwiseConv2dNative:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 0\r\n> 2020-02-11 17:54:40.367243: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:604] DepthwiseConv2d: DepthwiseConv2DBackpropInput Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n> 2020-02-11 17:54:40.367262: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:625] DepthwiseConv2dNativeBackpropInput:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 0\r\n> 2020-02-11 17:54:40.367286: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1100] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n> 2020-02-11 17:54:40.367297: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1120] DepthwiseConv2dNativeBackpropFilter:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n> 2020-02-11 17:54:40.367630: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1100] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; stride = 1, pad_rows = 0, pad_cols = 0, output: [512, 64, 64, 6]\r\n> 2020-02-11 17:54:40.367646: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1120] DepthwiseConv2dNativeBackpropFilter:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 1\r\n> ```\r\n> \r\n> Turning on universally with NGC TF:\r\n> \r\n> ```\r\n> 2020-02-11 09:58:16.110166: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1087] DepthwiseConv2dNativeBackpropFilter:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n> 2020-02-11 09:58:16.110603: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1073] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; stride = 1, pad_rows = 0, pad_cols = 0, output: [512, 64, 64, 6]\r\n> 2020-02-11 09:58:16.110620: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1087] DepthwiseConv2dNativeBackpropFilter:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 1\r\n> 2020-02-11 09:58:16.284146: I tensorflow/core/kernels/depthwise_conv_op.cc:377] DepthwiseConv2dNative:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 1\r\n> 2020-02-11 09:58:16.284413: I tensorflow/core/kernels/depthwise_conv_op.cc:377] DepthwiseConv2dNative:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n> 2020-02-11 09:58:16.284958: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:591] DepthwiseConv2d: DepthwiseConv2DBackpropInput Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n> 2020-02-11 09:58:16.284980: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:606] DepthwiseConv2dNativeBackpropInput:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n> 2020-02-11 09:58:16.285029: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1073] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [8, 512, 512, 104]\r\n> 2020-02-11 09:58:16.285042: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1087] DepthwiseConv2dNativeBackpropFilter:  Input: [8, 512, 512, 104]; Filter: [3, 3, 104, 1]; Output: [8, 512, 512, 104], stride = 1, pad_rows = 1, pad_cols = 1, Use cuDNN: 1\r\n> 2020-02-11 09:58:16.285488: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1073] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; stride = 1, pad_rows = 0, pad_cols = 0, output: [512, 64, 64, 6]\r\n> 2020-02-11 09:58:16.285507: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1087] DepthwiseConv2dNativeBackpropFilter:  Input: [512, 66, 66, 6]; Filter: [3, 3, 6, 1]; Output: [512, 64, 64, 6], stride = 1, pad_rows = 0, pad_cols = 0, Use cuDNN: 1\r\n> ```\r\n> \r\n> Is this behavior intended?\r\n\r\nwhen I train the ssd_mobilenet_v2, it compute slow ,and  I find that op `DepthwiseConv2DBackpropFilter`  doesn't use cudnn\r\n`2020-04-14 09:45:43.122841: I tensorflow/core/kernels/depthwise_conv_grad_op.cc:1074] DepthwiseConv2d: DepthwiseConv2DBackpropFilter Input: [1, 150, 150, 32]; Filter: [3, 3, 32, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [1, 150, 150, 32]`  but I am not find the `NHWC layout` ,could you tell me how to turn on cuDNN depth wise convolution? ", "@XuanBaby For the `NHWC layout`, do you mean you didn't find the keyword in log?\r\n\r\nAs for the DepthwiseConv2DBackpropFilter, it should fulfill some criteria to use NHWC (like CUDNN Ver.> 7.6.3, input=half, etc.). More is here: https://github.com/tensorflow/tensorflow/blob/1155fbd26bafa1dd02eb1fe96313297ec5e84732/tensorflow/core/kernels/depthwise_conv_grad_op.cc#L1092 "]}, {"number": 33835, "title": "A bug in TF2.0 that prevents tf.int32 tensor to be placed on GPU (but tf.int16/64 or tf.float32 is fine)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 'v2.0.0-rc2-26-g64c3d38' and '2.0.0' (GPU version)\r\n- Python version: 3.x\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Colab GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nCannot put an int32 tensor on a GPU. But can put an int16/int64/float32 on a GPU.\r\n\r\n**Describe the expected behavior**\r\nShould be able to place an int32 tensor on a GPU.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n#####################################\r\nwith tf.device('GPU:0'):\r\n&nbsp;&nbsp;&nbsp;&nbsp; var = tf.constant([1,2,3], dtype=tf.int32) #doesn't work\r\n&nbsp;&nbsp;&nbsp;&nbsp; print(var.device) #wrongly puts on CPU\r\n&nbsp;&nbsp;&nbsp;&nbsp; var = tf.constant([1,2,3], dtype=tf.int16) #works fine\r\n&nbsp;&nbsp;&nbsp;&nbsp; print(var.device) #correctly puts on GPU\r\n\r\nprints the following:\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\n#####################################\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Yes, there's a reason for that. I'm afraid it might be intended behavior.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4eb3e36d1b0cd511e1677e740bd093f42365cf9f/tensorflow/python/eager/pywrap_tensor.cc#L352-L354", "@gtg162y ,\r\nCan you please check the feedback from @ppham27 ?Thanks!", "I was not aware that most int32 kernels are placed in host memory. I understand it now, but it's just that it is restrictive to the developer who wants to put an int32 tensor on a GPU. It'd be nice if there is any way around it. ", "@gtg162y I agree that this behavior is not ideal. You can avoid that codepath in the eager runtime by using `tf.function`. It's a bit hacky but maybe this workaround is acceptable.\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_v2_behavior()\r\n\r\n@tf.function(autograph=False)\r\ndef f():\r\n  with tf.device('/device:GPU:0'):\r\n    return tf.constant([1, 2], tf.int32)\r\n\r\ny = f()\r\ny.dtype, y.device  # Returns (tf.int32, '/job:localhost/replica:0/task:0/device:GPU:0')\r\n```\r\n\r\nhttps://colab.research.google.com/drive/1HhNCQs5UureoQ5Iql3VvhzUMgnkzCwRR", "Closing this issue since its resolved. Feel free to reopen if have further questions. thanks!"]}, {"number": 33834, "title": "Embedding Layer's mask operation with LSTM Layer Gives Wrong Results When Using a GPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0 GPU version\r\n- Python version: 3.x\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen using a GPU, the mask operation of the embedding layer (with LSTM layer) gives wrong result. With mask_zero = True, anytime there is an input of 0 in one of the time steps, the output should be same as the previous time step's output. Which is what it does with a CPU. However, using a GPU, it just gives a zero output (which is not the expected behavior).\r\n\r\n**Describe the expected behavior**\r\nWith mask_zero = True, anytime there is an input of 0 in one of the time steps, the output should be same as the previous time step's output. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n########################################\r\nimport tensorflow as tf\r\ntf.random.set_seed(0)\r\n\r\ninputs = tf.keras.layers.Input(shape=(None,), dtype='int32')\r\nx = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=True)(inputs)\r\noutputs = tf.keras.layers.LSTM(2, return_sequences=True)(x)\r\n\r\nmodel = tf.keras.Model(inputs, outputs)\r\npadded_inputs = tf.constant([[1,3,0,0]])\r\nprint(model(padded_inputs))\r\n\r\nIt prints the following:\r\ntf.Tensor(\r\n[[[0.00727878 0.00290126]\r\n  [0.00217528 0.00565582]\r\n  [0.         0.        ]\r\n  [0.         0.        ]]] \r\n\r\ninstead of \r\n\r\ntf.Tensor(\r\n[[[0.00727878 0.00290126]\r\n  [0.00217528 0.00565582]\r\n  [0.00217528 0.00565582]\r\n  [0.00217528 0.00565582]]] (which is what it is supposed to print). \r\nNote that when only using a CPU, it gives the correct result.\r\n########################################\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Issue is replicating with Tf 2.0.0-gpu on colab. Please take a look at the [gist](https://colab.sandbox.google.com/gist/gadagashwini/c8f7774683b56740684e55d9100ffcda/untitled227.ipynb). Thanks!", "Just wanted to check if and when will this be resolved in the future? Thanks alot for listening!", "Sorry for the late reply, this might take some time on my end. \r\n\r\nAlso I think even the numerical result is different here, those different value actually belong to masked timesteps which will be auto ignored by keras. Are you somehow using those values?", "@gtg162y,\r\nThis issue is fixed in Tf==2.1.\r\nPlease  find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/8bc5e750d90639563d669741602fccb0/untitled459.ipynb) . \r\nPlease close the issue if it was already resolved for you. Thanks!", "I think this issue is not fixed yet, the gist was run on a CPU env, which doesn't reproduce the issue.", "Was able to reproduce the issue with Tf 2.1 GPU.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/8abe03c1fea7aac3540225b99b121f68/untitled462.ipynb). Thanks!", "I am able to reproduce the issue with Tf nightly (2.4.0-dev20200809)GPU.\r\nPlease find the [gist here]((https://colab.research.google.com/gist/Saduf2019/c466f8f21b07450a8438d89e352cc248/untitled346.ipynb) ). Thanks!", "@amitp-ai \r\nThis issue has been fixed in tf-nightly and tf 2.5, please refer tot he [gist here](https://colab.research.google.com/gist/Saduf2019/e5e66d55e18434207cb0f3f699e9f5e7/untitled590.ipynb), can you please upgrade your tf and move this issue to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33834\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33834\">No</a>\n"]}, {"number": 33833, "title": "Config value opt is not defined in any .rc file", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution windows 10:\r\n- TensorFlow installed from source:\r\n- Python version 3.7.5:\r\n- Installed using virtualenv? pip? conda?:\r\n\r\n\r\n- Bazel version 0.24.1:\r\n- Visual studio 2019\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nit always report CUSTOMBUILD : error : Config value opt is not defined in any .rc file.\r\nI did use -c opt instead of -config=opt\r\nI attached cmake file and building information below\r\n\r\n\r\nINFO: Options provided by the client:\r\n10>  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\n10>INFO: Options provided by the client:\r\n10>  'build' options: --python_path=C:/Users/Lin.Xiang/AppData/Local/Microsoft/WindowsApps/python.exe\r\n10>INFO: Reading rc options for 'build' from c:\\users\\lin.xiang\\fast\\build\\external\\tensorflow\\src\\tensorflow_download\\.bazelrc:\r\n10>  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\n10>CUSTOMBUILD : error : Config value opt is not defined in any .rc file\r\n10>Done building project \"tensorflow_CPU.vcxproj\" -- FAILED.\r\n\r\n", "comments": ["here is cmake file, if it helps\r\n\r\n\r\n# Download and set up Tensorflow\r\n\r\ninclude(${PROJECT_SOURCE_DIR}/cmake/Externals.cmake)\r\n\r\nif(WIN32)\r\n    set(GIT_EXECUTABLE \"git.exe\")\r\n    # Use CMake to build tensorflow on windows\r\n    if(FAST_BUILD_TensorFlow_CPU OR FAST_BUILD_TensorFlow_CUDA)\r\n        ExternalProject_Add(tensorflow_download\r\n            PREFIX ${FAST_EXTERNAL_BUILD_DIR}/tensorflow\r\n            BINARY_DIR ${FAST_EXTERNAL_BUILD_DIR}/tensorflow\r\n            GIT_REPOSITORY \"https://github.com/smistad/tensorflow.git\"\r\n            GIT_TAG \"fast-updated\"\r\n            UPDATE_COMMAND \"\"\r\n            CONFIGURE_COMMAND \"\"\r\n            BUILD_COMMAND \"\"\r\n            INSTALL_COMMAND \"\"\r\n        )\r\n    endif()\r\n    if(FAST_BUILD_TensorFlow_CPU)\r\n\r\n    ExternalProject_Add(tensorflow_CPU\r\n            DEPENDS tensorflow_download\r\n            PREFIX ${FAST_EXTERNAL_BUILD_DIR}/tensorflow\r\n            BINARY_DIR ${FAST_EXTERNAL_BUILD_DIR}/tensorflow\r\n            DOWNLOAD_COMMAND \"\"\r\n            UPDATE_COMMAND \"\"\r\n            CONFIGURE_COMMAND\r\n                echo \"Configuring TensorFlow...\" COMMAND\r\n                cd ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/ COMMAND\r\n                ${PROJECT_SOURCE_DIR}/cmake/TensorflowConfigureCPU.bat COMMAND\r\n                echo \"Done TF configure\"\r\n            BUILD_COMMAND\r\n                echo \"Building tensorflow with bazel for CPU..\" COMMAND\r\n                cd ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/ COMMAND\r\n                bazel build --config=opt //tensorflow:tensorflow_cc.dll\r\n            INSTALL_COMMAND\r\n                echo \"Installing tensorflow binary\"  COMMAND\r\n                ${CMAKE_COMMAND} -E copy ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-bin/tensorflow/tensorflow_cc.dll.if.lib ${FAST_EXTERNAL_INSTALL_DIR}/lib/tensorflow_CPU.lib COMMAND\r\n                ${CMAKE_COMMAND} -E copy ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-bin/external/protobuf_archive/protobuf.lib ${FAST_EXTERNAL_INSTALL_DIR}/lib/protobuf.lib COMMAND\r\n                ${CMAKE_COMMAND} -E copy ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-bin/tensorflow/tensorflow_cc.dll ${FAST_EXTERNAL_INSTALL_DIR}/bin/tensorflow_CPU.dll COMMAND\r\n                echo \"Installing tensorflow headers\"  COMMAND\r\n                ${CMAKE_COMMAND} -E copy_directory ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/tensorflow ${FAST_EXTERNAL_INSTALL_DIR}/include/tensorflow/ COMMAND\r\n                echo \"Installing tensorflow generated headers\" COMMAND\r\n                ${CMAKE_COMMAND} -E copy_directory ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-genfiles/tensorflow ${FAST_EXTERNAL_INSTALL_DIR}/include/tensorflow/  COMMAND\r\n                echo \"Installing tensorflow third party headers\"  COMMAND\r\n                ${CMAKE_COMMAND} -E copy_directory ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/third_party/ ${FAST_EXTERNAL_INSTALL_DIR}/include/third_party/  COMMAND\r\n                #echo \"Installing protobuf headers\"  COMMAND\r\n                #${CMAKE_COMMAND} -E copy_directory ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-tensorflow_download/external/protobuf_archive/src/google/ ${FAST_EXTERNAL_INSTALL_DIR}/include/google/ COMMAND\r\n                #echo \"Installing nsync headers\"  COMMAND\r\n                #xcopy ${src} ${dest} /y COMMAND\r\n                echo \"Installing absl headers\"  COMMAND\r\n                ${CMAKE_COMMAND} -E copy_directory ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-tensorflow_download/external/com_google_absl/absl/ ${FAST_EXTERNAL_INSTALL_DIR}/include/absl/\r\n    )\r\n    endif()\r\nelse(WIN32)\r\n    # Use bazel to build tensorflow on linux\r\n    set(GIT_EXECUTABLE \"git\")\r\n    if(FAST_BUILD_TensorFlow_CPU OR FAST_BUILD_TensorFlow_CUDA)\r\n        ExternalProject_Add(tensorflow_download\r\n            PREFIX ${FAST_EXTERNAL_BUILD_DIR}/tensorflow\r\n            BINARY_DIR ${FAST_EXTERNAL_BUILD_DIR}/tensorflow\r\n            GIT_REPOSITORY \"https://github.com/smistad/tensorflow.git\"\r\n            GIT_TAG \"fast-updated\"\r\n            UPDATE_COMMAND \"\"\r\n            CONFIGURE_COMMAND \"\"\r\n            BUILD_COMMAND \"\"\r\n            INSTALL_COMMAND \"\"\r\n        )\r\n    endif()\r\n    if(FAST_BUILD_TensorFlow_ROCm)\r\n        # Need a seperate repo for rocm atm\r\n        ExternalProject_Add(tensorflow_download_rocm\r\n            PREFIX ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm\r\n            BINARY_DIR ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm\r\n            GIT_REPOSITORY \"https://github.com/ROCmSoftwarePlatform/tensorflow-upstream\"\r\n            GIT_TAG \"r1.14-rocm\"\r\n            UPDATE_COMMAND \"\"\r\n            CONFIGURE_COMMAND \"\"\r\n            BUILD_COMMAND \"\"\r\n            INSTALL_COMMAND \"\"\r\n        )\r\n        ExternalProject_Add(tensorflow_ROCm\r\n            DEPENDS tensorflow_download_rocm\r\n            PREFIX ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm\r\n            BINARY_DIR ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm\r\n            DOWNLOAD_COMMAND \"\"\r\n            UPDATE_COMMAND \"\"\r\n            # Run TF configure in the form of a shell script.\r\n            CONFIGURE_COMMAND\r\n                cd ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm/src/tensorflow_download_rocm/ && sh ${PROJECT_SOURCE_DIR}/cmake/TensorflowConfigureROCm.sh\r\n            # Build using bazel\r\n            BUILD_COMMAND\r\n                echo \"Building tensorflow with bazel and ROCm (AMD) GPU support\" &&\r\n                cd ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm/src/tensorflow_download_rocm/ && bazel build -c opt --config=rocm //tensorflow:libtensorflow_cc.so\r\n            INSTALL_COMMAND\r\n                echo \"Installing tensorflow binary\" &&\r\n                cp -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm/src/tensorflow_download_rocm/bazel-bin/tensorflow/libtensorflow_cc.so.1.14.0 ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_ROCm.so &&\r\n                cp -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm/src/tensorflow_download_rocm/bazel-bin/tensorflow/libtensorflow_framework.so.1.14.0 ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_framework_ROCm.so &&\r\n                chmod a+w ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_ROCm.so &&\r\n                chmod a+w ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_framework_ROCm.so &&\r\n                patchelf --set-soname libtensorflow_cc_ROCm.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_ROCm.so &&\r\n                patchelf --set-soname libtensorflow_framework_ROCm.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_framework_ROCm.so &&\r\n                patchelf --replace-needed libtensorflow_framework.so.1 libtensorflow_framework_ROCm.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_ROCm.so &&\r\n                echo \"Installing tensorflow headers\" &&\r\n                cp -rf ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm/src/tensorflow_download_rocm/tensorflow/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ &&\r\n                echo \"Installing tensorflow generated headers\" &&\r\n                cp -rf ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm/src/tensorflow_download_rocm/bazel-genfiles/tensorflow/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ &&\r\n                echo \"Installing tensorflow third_party headers\" &&\r\n                cp -rf ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm/src/tensorflow_download_rocm/third_party/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ &&\r\n                echo \"Installing protobuf headers\" &&\r\n                bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm/src/tensorflow_download_rocm/bazel-out/)/../../../external/protobuf_archive/src/google/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ -Rf\" &&\r\n                #echo \"Installing nsync headers\" &&\r\n                #bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm/src/tensorflow_download_rocm/bazel-out/)/../../../external/nsync/public/*.h ${FAST_EXTERNAL_INSTALL_DIR}/include/ -Rf\" &&\r\n                echo \"Installing absl headers\" &&\r\n                bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow_rocm/src/tensorflow_download_rocm/bazel-out/)/../../../external/com_google_absl/absl/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ -Rf\"\r\n    )\r\n    endif()\r\n    if(FAST_BUILD_TensorFlow_CUDA)\r\n    ExternalProject_Add(tensorflow_CUDA\r\n            DEPENDS tensorflow_download\r\n            PREFIX ${FAST_EXTERNAL_BUILD_DIR}/tensorflow\r\n            BINARY_DIR ${FAST_EXTERNAL_BUILD_DIR}/tensorflow\r\n            DOWNLOAD_COMMAND \"\"\r\n            UPDATE_COMMAND \"\"\r\n            # Run TF configure in the form of a shell script. CUDA should be installed in /usr/local/cuda\r\n            CONFIGURE_COMMAND\r\n                cd ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/ && sh ${PROJECT_SOURCE_DIR}/cmake/TensorflowConfigureCUDA.sh\r\n            # Build using bazel\r\n            BUILD_COMMAND\r\n                echo \"Building tensorflow with bazel and CUDA GPU support\" &&\r\n                cd ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/ && bazel build -c opt --config=cuda --copt=-mfpmath=both --copt=-march=core-avx2 //tensorflow:libtensorflow_cc.so\r\n            INSTALL_COMMAND\r\n                echo \"Installing tensorflow binary\" &&\r\n                cp -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-bin/tensorflow/libtensorflow_cc.so.1.14.0 ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_CUDA.so &&\r\n                cp -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-bin/tensorflow/libtensorflow_framework.so.1.14.0 ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_framework_CUDA.so &&\r\n                chmod a+w ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_CUDA.so &&\r\n                chmod a+w ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_framework_CUDA.so &&\r\n                patchelf --set-soname libtensorflow_cc_CUDA.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_CUDA.so &&\r\n                patchelf --set-soname libtensorflow_framework_CUDA.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_framework_CUDA.so &&\r\n                patchelf --replace-needed libtensorflow_framework.so.1 libtensorflow_framework_CUDA.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_CUDA.so &&\r\n                #echo \"Installing mkl binaries\" &&\r\n                #bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-out/)/../../../external/mkl/lib/libmklml_intel.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/ -Rf\" &&\r\n                #bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-out/)/../../../external/mkl/lib/libiomp5.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/ -Rf\" &&\r\n                echo \"Installing tensorflow headers\" &&\r\n                cp -rf ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/tensorflow/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ &&\r\n                echo \"Installing tensorflow generated headers\" &&\r\n                cp -rf ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-genfiles/tensorflow/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ &&\r\n                echo \"Installing tensorflow third_party headers\" &&\r\n                cp -rf ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/third_party/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ &&\r\n                echo \"Installing protobuf headers\" &&\r\n                bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-out/)/../../../external/protobuf_archive/src/google/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ -Rf\" &&\r\n                #echo \"Installing nsync headers\" &&\r\n                #bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-out/)/../../../external/nsync/public/*.h ${FAST_EXTERNAL_INSTALL_DIR}/include/ -Rf\" &&\r\n                echo \"Installing absl headers\" &&\r\n                bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-out/)/../../../external/com_google_absl/absl/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ -Rf\"\r\n    )\r\n    endif()\r\n    if(FAST_BUILD_TensorFlow_CPU)\r\n    ExternalProject_Add(tensorflow_CPU\r\n            DEPENDS tensorflow_download\r\n            PREFIX ${FAST_EXTERNAL_BUILD_DIR}/tensorflow\r\n            BINARY_DIR ${FAST_EXTERNAL_BUILD_DIR}/tensorflow\r\n            DOWNLOAD_COMMAND \"\"\r\n            UPDATE_COMMAND \"\"\r\n            # Run TF configure in the form of a shell script.\r\n            CONFIGURE_COMMAND\r\n                cd ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/ && sh ${PROJECT_SOURCE_DIR}/cmake/TensorflowConfigureCPU.sh\r\n            # Build using bazel\r\n            BUILD_COMMAND\r\n                echo \"Building tensorflow with bazel for CPU\" &&\r\n                cd ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/ && bazel build --config=opt --copt=-mfpmath=both --copt=-march=core-avx2 //tensorflow:libtensorflow_cc.so\r\n            INSTALL_COMMAND\r\n                echo \"Installing tensorflow binary\" &&\r\n                cp -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-bin/tensorflow/libtensorflow_cc.so.1.14.0 ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_CPU.so &&\r\n                cp -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-bin/tensorflow/libtensorflow_framework.so.1.14.0 ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_framework_CPU.so &&\r\n                chmod a+w ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_CPU.so &&\r\n                chmod a+w ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_framework_CPU.so &&\r\n                patchelf --set-soname libtensorflow_cc_CPU.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_CPU.so &&\r\n                patchelf --set-soname libtensorflow_framework_CPU.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_framework_CPU.so &&\r\n                patchelf --replace-needed libtensorflow_framework.so.1 libtensorflow_framework_CPU.so ${FAST_EXTERNAL_INSTALL_DIR}/lib/libtensorflow_cc_CPU.so &&\r\n                echo \"Installing tensorflow headers\" &&\r\n                cp -rf ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/tensorflow/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ &&\r\n                echo \"Installing tensorflow generated headers\" &&\r\n                cp -rf ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-genfiles/tensorflow/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ &&\r\n                echo \"Installing tensorflow third_party headers\" &&\r\n                cp -rf ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/third_party/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ &&\r\n                echo \"Installing protobuf headers\" &&\r\n                bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-out/)/../../../external/protobuf_archive/src/google/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ -Rf\" &&\r\n                #echo \"Installing nsync headers\" &&\r\n                #bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-out/)/../../../external/nsync/public/*.h ${FAST_EXTERNAL_INSTALL_DIR}/include/ -Rf\" &&\r\n                echo \"Installing absl headers\" &&\r\n                bash -c \"cp $(readlink -f ${FAST_EXTERNAL_BUILD_DIR}/tensorflow/src/tensorflow_download/bazel-out/)/../../../external/com_google_absl/absl/ ${FAST_EXTERNAL_INSTALL_DIR}/include/ -Rf\"\r\n    )\r\n    endif()\r\nendif(WIN32)", "@xiang123 \r\nThe TensorFlow team does not officially support cmake, sorry. Please try out building from source with Bazel only.Please, follow the steps mentioned in the [Tensorflow build ](https://www.tensorflow.org/install/source_windows)official website.Please, let us know how it progresses. Thanks!", "the \"opt\" config option gets created when you run configure.\r\nI will not look into the cmake code you have, but before running `bazel build`, are you running `./configure` as instructed in https://www.tensorflow.org/install/source#configure_the_build", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33833\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33833\">No</a>\n"]}, {"number": 33832, "title": "How can I convert outputs or tflite tensors to swift multi-dimensional arrays to be able to perform mathematical computations on them?", "body": "I am running posenet on swift with tflite.\r\nThe model has multiple output arrays with the following dimensions:\r\n1x14x14x17, 1x14x14x34, 1x14x14x32, 1x14x14x32\r\n\r\nHow can I convert outputs or tflite tensors to swift ios multi-dimensional arrays to be able to perform mathematical computations on them?\r\n\r\n[There was a similar question for java](https://github.com/tensorflow/tensorflow/issues/25841)", "comments": ["**help me please**", "is it possible now?", "Hi, @joker2017 \r\nSorry for late answer.\r\n\r\nIn Swift, `Tensor` is implemented like [this][Swift Tensor].\r\nTo perform mathematical computations on the data of a `Tensor`, you can access to the [`Data`] of a tensor.\r\n\r\nHowever, dealing with [`Data`] type is a bit tricky.\r\nInstead, you can convert the [`Data`] type to the other data type, such as [`Array`].\r\n\r\nYou can convert [`Data`] to the single dimensional [`Array`], as well as multi dimensional [`Array`].\r\nIf you choose to convert this as a single dimensional [`Array`], you can take a look at [PoseNet] example.\r\nIn [the example][PoseNet], it implemented [FlatArray] structure to convert a [`Tensor`][Swift Tensor].\r\nYou can access to the each element like `tensorData[1, 13, 12, 15]`.\r\nThe use case is in the [`ModelDataHandler.swift`][PoseNet FlatArray].\r\n\r\n\r\n[Swift Tensor]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/swift/Sources/Tensor.swift\r\n[`Data`]: https://developer.apple.com/documentation/foundation/data\r\n[`Array`]: https://developer.apple.com/documentation/swift/array\r\n[PoseNet]: https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/ios\r\n[PoseNet FlatArray]: https://github.com/tensorflow/examples/blob/master/lite/examples/posenet/ios/PoseNet/ModelDataHandler/ModelDataHandler.swift#L187\r\n[FlatArray]: https://github.com/tensorflow/examples/blob/master/lite/examples/posenet/ios/PoseNet/Extensions/TFLiteExtension.swift#L43"]}, {"number": 33831, "title": "TF2.0.0 Error on InceptionV3", "body": "Last Edit: This is too hard to reproduce.  It relies on a custom 'pylib' module.  Closing the issue.\r\n\r\nI have updated this post to have a more user-friendly piece of code.  You need to run it with ipython or paste it in to a cell on jupyter notebook because the first three lines are bash script executed using the (!) magic command.\r\nI have seen this code execute properly with Python 3.6 running on a CPU.\r\nI can't get it to run on Python 3.7.  Not sure if it has anything to do with my GPU.\r\nSorry, I can't be more certain about this.\r\n[code_inception.txt](https://github.com/tensorflow/tensorflow/files/3787021/code_inception.txt)\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7 conda environment\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: CUDA 10/ cuDNN 7.6.4\r\n- GPU model and memory: NVidia RTX 2080 TI and RTX 2080 MaxQ\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nError produced on python 3.7 on GPU (not on python 3.6 TF2.0.0 on a CPU):\r\n\r\n      1/Unknown - 0s 71ms/step\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-992b5c7b95cf> in <module>\r\n      2 # BUMP EPOCHS to 50 for true training\r\n      3 EPOCHS = 1  # 50\r\n----> 4 model.fit(dataset, epochs=EPOCHS)\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    783         max_queue_size=max_queue_size,\r\n    784         workers=workers,\r\n--> 785         use_multiprocessing=use_multiprocessing)\r\n    786 \r\n    787   def evaluate(self,\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    335                 mode=ModeKeys.TRAIN,\r\n    336                 training_context=training_context,\r\n--> 337                 total_epochs=epochs)\r\n    338             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    339 \r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    125         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    126       try:\r\n--> 127         batch_outs = execution_function(iterator)\r\n    128       except (StopIteration, errors.OutOfRangeError):\r\n    129         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    613       # This is the first call of __call__, so we have to initialize.\r\n    614       initializers = []\r\n--> 615       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    616     finally:\r\n    617       # At this point we know that the initialization is complete (or less\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    495     self._concrete_stateful_fn = (\r\n    496         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 497             *args, **kwds))\r\n    498 \r\n    499     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2363       args, kwargs = None, None\r\n   2364     with self._lock:\r\n-> 2365       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2366     return graph_function\r\n   2367 \r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2671 \r\n   2672       self._function_cache.missed.add(call_context_key)\r\n-> 2673       graph_function = self._create_graph_function(args, kwargs)\r\n   2674       self._function_cache.primary[cache_key] = graph_function\r\n   2675       return graph_function, args, kwargs\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2561             arg_names=arg_names,\r\n   2562             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2563             capture_by_value=self._capture_by_value),\r\n   2564         self._function_attributes,\r\n   2565         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    956                                           converted_func)\r\n    957 \r\n--> 958       func_outputs = python_func(*func_args, **func_kwargs)\r\n    959 \r\n    960       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    438         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    440     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    441 \r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)\r\n     71     strategy = distribution_strategy_context.get_strategy()\r\n     72     outputs = strategy.experimental_run_v2(\r\n---> 73         per_replica_function, args=(x, y, sample_weights))\r\n     74     # Out of PerReplica outputs reduce or pick values to return.\r\n     75     all_outputs = dist_utils.unwrap_output_dict(\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)\r\n    761       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\r\n    762                                 convert_by_default=False)\r\n--> 763       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    764 \r\n    765   def reduce(self, reduce_op, value, axis):\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   1817       kwargs = {}\r\n   1818     with self._container_strategy().scope():\r\n-> 1819       return self._call_for_each_replica(fn, args, kwargs)\r\n   1820 \r\n   1821   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n   2162         self._container_strategy(),\r\n   2163         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n-> 2164       return fn(*args, **kwargs)\r\n   2165 \r\n   2166   def _reduce_to(self, reduce_op, value, destinations):\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)\r\n    262       y,\r\n    263       sample_weights=sample_weights,\r\n--> 264       output_loss_metrics=model._output_loss_metrics)\r\n    265 \r\n    266   if reset_metrics:\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)\r\n    310           sample_weights=sample_weights,\r\n    311           training=True,\r\n--> 312           output_loss_metrics=output_loss_metrics))\r\n    313   if not isinstance(outs, list):\r\n    314     outs = [outs]\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n    251               output_loss_metrics=output_loss_metrics,\r\n    252               sample_weights=sample_weights,\r\n--> 253               training=training))\r\n    254       if total_loss is None:\r\n    255         raise ValueError('The model cannot be run '\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n    125     inputs = nest.map_structure(ops.convert_to_tensor, inputs)\r\n    126 \r\n--> 127   outs = model(inputs, **kwargs)\r\n    128   outs = nest.flatten(outs)\r\n    129 \r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    776                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    777                 else:\r\n--> 778                   outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    779 \r\n    780             except errors.OperatorNotAllowedInGraphError as e:\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    715     return self._run_internal_graph(\r\n    716         inputs, training=training, mask=mask,\r\n--> 717         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n    718 \r\n    719   def compute_output_shape(self, input_shape):\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask, convert_kwargs_to_constants)\r\n    871 \r\n    872           # Compute outputs.\r\n--> 873           output_tensors = layer(computed_tensors, **kwargs)\r\n    874 \r\n    875           # Update tensor_dict.\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    776                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    777                 else:\r\n--> 778                   outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    779 \r\n    780             except errors.OperatorNotAllowedInGraphError as e:\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    715     return self._run_internal_graph(\r\n    716         inputs, training=training, mask=mask,\r\n--> 717         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n    718 \r\n    719   def compute_output_shape(self, input_shape):\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask, convert_kwargs_to_constants)\r\n    871 \r\n    872           # Compute outputs.\r\n--> 873           output_tensors = layer(computed_tensors, **kwargs)\r\n    874 \r\n    875           # Update tensor_dict.\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    776                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    777                 else:\r\n--> 778                   outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    779 \r\n    780             except errors.OperatorNotAllowedInGraphError as e:\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py in call(self, inputs)\r\n    191     # behavior.\r\n    192     call_input_shape = inputs.get_shape()\r\n--> 193     call_input_channel = self._get_input_channel(call_input_shape)\r\n    194     if call_input_channel != self._build_input_channel:\r\n    195       raise ValueError(\r\n\r\n~/anaconda3/envs/tfgpu/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py in _get_input_channel(self, input_shape)\r\n    299     channel_axis = self._get_channel_axis()\r\n    300     if input_shape.dims[channel_axis].value is None:\r\n--> 301       raise ValueError('The channel dimension of the inputs '\r\n    302                        'should be defined. Found `None`.')\r\n    303     return int(input_shape[channel_axis])\r\n\r\nValueError: The channel dimension of the inputs should be defined. Found `None`.\r\n\r\n**Describe the expected behavior**\r\nNo error.  It trains.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nSee code_inception.txt attached\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nNil\r\n\r\n\r\n", "comments": []}, {"number": 33830, "title": "Typo in TensorFlow Core Tutorials - Image classification", "body": "\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/classification.ipynb\r\n\r\n## Description of issue (what needs changing):\r\nTypo\r\n\r\n### Clear description\r\n```\r\nCreate the model\r\nThe model consists of three convolution blocks with a max pool layer in each of them. There's a fully connected layer with 512 units on top of it thatr is activated by a relu activation function. \r\n```\r\nIn Create the model section,\r\n```thatr``` should be ```that```\r\n\r\n```\r\nCreate the model\r\nThe model consists of three convolution blocks with a max pool layer in each of them. There's a fully connected layer with 512 units on top of it that is activated by a relu activation function. \r\n```\r\n### Submit a pull request?\r\n", "comments": ["Thanks for the report.\r\nCan you please make a pull request? This doc lives here: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/classification.ipynb\r\nIf not familiar, check out this section in the [TensorFlow doc contributor guide](https://www.tensorflow.org/community/contribute/docs#interactive_notebooks).", "Thank you for review the issue.\r\nCan you please check out the pull request I made? https://github.com/tensorflow/docs/pull/1148", "This was merged. Thanks"]}, {"number": 33829, "title": "Train in Keras with Stateless CuDNN GRU and infer using tensorflow with stateful CuDNN GRU", "body": "Hi\r\n\r\nI am facing a problem with using Stateful and Steteless CuDNN GRU\r\n\r\nI trained my model in Keras with CuDNN GRU's using let's say 1000 time steps and stateful=False. \r\n\r\nDuring inference, to keep the model real-time, I changed the time_steps to 1, and stateful=true to load the model and it's weights. I converted this model to tensorflow protobuf using convert_variables_to_constants() function to freeze the graph and write_graph() to write it to a protobuf file.\r\n\r\nThe problem I am facing currently is that I am getting same results whether or not stateful = True or False during inference. which means I am missing something while converting keras model to tensorflow model. Is there any special consideration needed with the conversion process?\r\n\r\nWhile running tensorflow session, along with the output, do I need to get states of all the CuDNN GRU layers and pass in these states with the next input? If yes, how can I get these states from the graph. It's not clear from the nodes of cudnn layer, which of it might contain state information. \r\n\r\nOr is there something very fundamental I am missing here? Any help is very much appreciated. \r\n\r\nTensorflow version : 1.14.0\r\nKeras version : 2.2.4\r\n\r\nThanks\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!"]}, {"number": 33828, "title": "Questions about the example \"Neural machine translation with attention\"", "body": "There is one questions when I learned the  example \"Neural machine translation with attention\"(https://www.tensorflow.org/tutorials/text/nmt_with_attention).\r\n       Why the attention weight is calculated by encoder_output and encoder_hiiden and context vector is contacted with decoder_embedding.  In my opinion, the attention weight should be calculated by encoder_output and every single hiiden of decoder_output, and context vector should be contacted with decoder_output. Maybe I have not understood the seq2seq with attention completely?\r\n", "comments": ["@RyanPeking ,\r\n\r\nHello,This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](http://stackoverflow.com/questions/tagged//tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@RyanPeking ,\r\nany update on the issue ?Thanks!", "no, thanks, it has been resolved", "Closing since issue is resolved,Thanks!"]}, {"number": 33827, "title": "lowerbound not implemented (from searchsorted)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version (or github SHA if from source): tensorflow           2.0.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: . Here is a list of operators for which you will need custom implementations: LowerBound.\r\nTraceback (most recent call last):\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/bin/toco_from_protos\", line 8, in \r\n    sys.exit(main())\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\n```\r\n", "comments": ["@aptly-io,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "This is a python snippit edited and invoked from within visual studio code. Hope this clarifies. Thanks a lot to look into it!\r\n\r\nI tried to use the `searchsorted`. `searchsorted` depends on `lower_bound` to do the actual work when the `side` parameter is `left`. Note that for a full working version of searchsorted, also `upper_bound` might need to be supported (in this snippet it is not needed). See here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L4771\r\n\r\n```\r\n# This runs directly from within Visual Studio Code\r\n\r\n#%%\r\n\r\n# This example only works with # pip3 install tensorflow==1.14.0\r\nimport tensorflow as tf\r\n\r\n# TF 2.0 does not support the low level api compared to 1.x version\r\n# It continuously fails Interpreter.set_tensor() or Interpreter.invoke()\r\n# import tensorflow.compat.v1 as tf\r\n# tf.disable_v2_behavior()\r\n\r\nprint(tf.__version__)\r\n\r\n#%%\r\n\r\n# Dummy values taken from tensorflow searchsorted() man. page (https://www.tensorflow.org/api_docs/python/tf/searchsorted)\r\nsorted_sequence = [[0, 3, 9, 9, 10], [1, 2, 3, 4, 5]]\r\nimposter_table = tf.constant(sorted_sequence)\r\n\r\ninput_avg_scores = tf.placeholder(tf.int32, shape = (2, 3), name = 'input_avg_scores')\r\n\r\nscores = tf.searchsorted(imposter_table, input_avg_scores)\r\n\r\n#%%\r\nwith tf.Session() as session:\r\n    # Dummy values taken from tensorflow searchsorted() man. page\r\n    values = [[2, 4, 9], [0, 2, 6]]\r\n    print(session.run(scores, feed_dict = {input_avg_scores: values}))    # should return [[1 2 2] [0 1 5]]\r\n\r\n    converter = tf.lite.TFLiteConverter.from_session(session, [input_avg_scores], [scores])\r\n\r\n    # since LowerBounds is not supported, try including whitelisted TensorFlow operations: it is not whitelisted\r\n    # converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n    # converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\n    tflite_model = converter.convert()\r\n    open(\"fm_search_scores.tflite\", \"wb\").write(tflite_model)\r\n#%%\r\n```\r\n\r\nThis is the full output when running the 3rd cell:\r\n```\r\n\r\n\r\nwith tf.Session() as session:...\r\n[[1 2 2]\r\n [0 1 5]]\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n~/dev/projects/cognisance/tensorflow/tf_averager.py in \r\n     10     # converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n     11 \r\n---> 12     tflite_model = converter.convert()\r\n     13     open(\"fm_search_scores.tflite\", \"wb\").write(tflite_model)\r\n\r\n~/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    896           input_tensors=self._input_tensors,\r\n    897           output_tensors=self._output_tensors,\r\n--> 898           **converter_kwargs)\r\n    899     else:\r\n    900       result = _toco_convert_graph_def(\r\n\r\n~/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)\r\n    402   data = toco_convert_protos(model_flags.SerializeToString(),\r\n    403                              toco_flags.SerializeToString(),\r\n--> 404                              input_data.SerializeToString())\r\n    405   return data\r\n    406 \r\n\r\n~/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    170       stderr = _try_convert_to_unicode(stderr)\r\n    171       raise ConverterError(\r\n--> 172           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    173   finally:\r\n    174     # Must manually cleanup files.\r\n\r\nConverterError: TOCO failed. See console for info.\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2019-10-30 10:08:53.143716: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LowerBound\r\n2019-10-30 10:08:53.151956: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 3 operators, 6 arrays (0 quantized)\r\n2019-10-30 10:08:53.152029: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 3 operators, 6 arrays (0 quantized)\r\n2019-10-30 10:08:53.152070: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1 operators, 3 arrays (0 quantized)\r\n2019-10-30 10:08:53.152085: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 1 operators, 3 arrays (0 quantized)\r\n2019-10-30 10:08:53.152096: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1 operators, 3 arrays (0 quantized)\r\n2019-10-30 10:08:53.152109: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2019-10-30 10:08:53.152122: I tensorflow/lite/toco/toco_tooling.cc:433] Estimated count of arithmetic ops: 0 billion (note that a multiply-add is counted as 2 ops).\r\n2019-10-30 10:08:53.152826: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: . Here is a list of operators for which you will need custom implementations: LowerBound.\r\nTraceback (most recent call last):\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/bin/toco_from_protos\", line 8, in \r\n    sys.exit(main())\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/francis/dev/projects/cognisance/tensorflow/venv37/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: . Here is a list of operators for which you will need custom implementations: LowerBound.\r\n```", "is it possible to not use searchsorted if you can achieve with basic ops?", "@haozha111 @gadagashwini I switched to TorchScript. (I missed the time to look into a tensorflow workaround).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33827\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33827\">No</a>\n"]}, {"number": 33826, "title": "Calling train in SavedModelEstimator gives ValueError: At least two variables have the same name", "body": "**System information**\r\n- TensorFlow version (use command below): 1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n- Python version: Python 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\n- Defining a model using tensorflow.keras\r\n- Converting the compiled model to estimator\r\n- Training estimator using train_and_evaluate\r\n- Export all saved models for  \"TRAIN\", \"EVAL\" and \"PREDICT\"\r\n\r\nWhen I create a SavedModelEstimator from the previous exported data, I am able to call evaluate and predict successfully. However, if I call the method train I get an error \"ValueError: At least two variables have the same name: dense_1/bias/Adam)\".\r\n\r\n**I can warm-start sucessfully if I change the line in _get_grouped_variables to:** https://github.com/tensorflow/tensorflow/blob/9c52e7ce02532c22a79ae7139f37c663add4c90f/tensorflow/python/training/warm_starting_util.py#L349\r\n\r\n`ops.GraphKeys.TRAINABLE_VARIABLES, scope=v)`\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Dense, Dropout\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.estimator import model_to_estimator\r\nimport keras.backend as K\r\nimport numpy as np\r\nimport os\r\n\r\n\r\ndef model_fn():\r\n    # 10 features\r\n    input_layer = Input(shape=(10,), name=\"inputs\", dtype=tf.float32)\r\n    dense_1 = Dense(units=10, activation=\"relu\", name=\"dense_1\")(input_layer)\r\n    dense_2 = Dense(units=1, activation=\"linear\", name=\"outputs\",\r\n                    dtype=tf.float32)(dense_1)\r\n    model = Model(inputs=input_layer, outputs=dense_2)\r\n    model.compile(optimizer=tf.train.AdamOptimizer(),\r\n                  loss=\"mse\")\r\n    return model\r\n\r\n\r\ndef synthetic_input_fn(num_examples, num_features):\r\n    # dummy data\r\n    return tf.data.Dataset.from_tensor_slices(({\"inputs\": np.random.random((num_examples, num_features))}, np.random.randint(10, size=num_examples))) \\\r\n        .shuffle(512) \\\r\n        .batch(32) \\\r\n        .repeat(10)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    MODEL_DIR = \"./model\"\r\n    SAVED_MODEL_DIR = os.path.join(MODEL_DIR, \"saved\")\r\n\r\n    def cust_train_input_fn():\r\n        return synthetic_input_fn(1000, 10)\r\n\r\n    def cust_eval_input_fn():\r\n        return synthetic_input_fn(100, 10)\r\n\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n    # define model and convert to estimator\r\n    model = model_fn()\r\n    estimator = model_to_estimator(\r\n        keras_model=model,\r\n        model_dir=MODEL_DIR\r\n    )\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn=cust_train_input_fn)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=cust_eval_input_fn)\r\n\r\n    # train and evaluate dummy model\r\n    tf.estimator.train_and_evaluate(\r\n        estimator=estimator,\r\n        train_spec=train_spec,\r\n        eval_spec=eval_spec\r\n    )\r\n\r\n    # export all modes\r\n    feature_spec = {\"inputs\": tf.placeholder(tf.float64, (None, 10))}\r\n\r\n    label_spec = tf.placeholder(dtype=tf.int64)\r\n\r\n    serving_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec)\r\n    training_fn = tf.contrib.estimator.build_raw_supervised_input_receiver_fn(feature_spec, label_spec)\r\n\r\n    rcrv_fn_map = {\r\n        tf.estimator.ModeKeys.TRAIN: training_fn,\r\n        tf.estimator.ModeKeys.EVAL: training_fn,\r\n        tf.estimator.ModeKeys.PREDICT: serving_fn\r\n    }\r\n\r\n    tf.contrib.estimator.export_all_saved_models(\r\n        estimator,\r\n        export_dir_base=SAVED_MODEL_DIR,\r\n        input_receiver_fn_map=rcrv_fn_map\r\n    )\r\n\r\n    tf.keras.backend.clear_session()\r\n\r\n    saved_estimator = tf.contrib.estimator.SavedModelEstimator(os.path.join(SAVED_MODEL_DIR, os.listdir(SAVED_MODEL_DIR)[0]))\r\n    saved_estimator.train(cust_train_input_fn)\r\n```\r\n\r\n\r\n**Other info / logs**\r\nUsing TensorFlow backend.\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using the Keras model provided.\r\n2019-10-29 14:26:59.280066: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nINFO:tensorflow:Using config: {'_model_dir': './model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x62b766be0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Not using Distribute Coordinator.\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./model/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: ('./model/keras/keras_model.ckpt',)\r\nINFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: outputs/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: outputs/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into ./model/model.ckpt.\r\nINFO:tensorflow:loss = 32.57418, step = 1\r\nINFO:tensorflow:global_step/sec: 1201.8\r\nINFO:tensorflow:loss = 24.11268, step = 101 (0.083 sec)\r\nINFO:tensorflow:global_step/sec: 1799.1\r\nINFO:tensorflow:loss = 12.514932, step = 201 (0.056 sec)\r\nINFO:tensorflow:global_step/sec: 1810.48\r\nINFO:tensorflow:loss = 9.915621, step = 301 (0.055 sec)\r\nINFO:tensorflow:Saving checkpoints for 320 into ./model/model.ckpt.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2019-10-29-14:27:00\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from ./model/model.ckpt-320\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [10/100]\r\nINFO:tensorflow:Evaluation [20/100]\r\nINFO:tensorflow:Evaluation [30/100]\r\nINFO:tensorflow:Evaluation [40/100]\r\nINFO:tensorflow:Finished evaluation at 2019-10-29-14:27:00\r\nINFO:tensorflow:Saving dict for global step 320: global_step = 320, loss = 8.093195\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 320: ./model/model.ckpt-320\r\nINFO:tensorflow:Loss for final step: 10.114872.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: ['train']\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\r\nWARNING:tensorflow:Export includes no default signature!\r\nINFO:tensorflow:Restoring parameters from ./model/model.ckpt-320\r\nWARNING:tensorflow:From /anaconda3/envs/tf-abalone/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPass your op to the equivalent parameter main_op instead.\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\r\nWARNING:tensorflow:Export includes no default signature!\r\nINFO:tensorflow:Restoring parameters from ./model/model.ckpt-320\r\nWARNING:tensorflow:From /anaconda3/envs/tf-abalone/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py:1046: calling SavedModelBuilder.add_meta_graph (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPass your op to the equivalent parameter main_op instead.\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\r\nINFO:tensorflow:Restoring parameters from ./model/model.ckpt-320\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:SavedModel written to: ./model/saved/temp-b'1572359223'/saved_model.pb\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/8s/tjdwdq296s5fdljp3p6jd9z40000gn/T/tmp7c0vhh49\r\nINFO:tensorflow:Using config: {'_model_dir': '/var/folders/8s/tjdwdq296s5fdljp3p6jd9z40000gn/T/tmp7c0vhh49', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c316e6208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Checking available modes for SavedModelEstimator.\r\nINFO:tensorflow:Available modes for Estimator: ['train', 'eval', 'infer']\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./model/saved/1572359223/variables/variables', vars_to_warm_start=['dense_1/bias', 'dense_1/bias/Adam', 'dense_1/bias/Adam_1', 'dense_1/kernel', 'dense_1/kernel/Adam', 'dense_1/kernel/Adam_1', 'global_step', 'outputs/bias', 'outputs/bias/Adam', 'outputs/bias/Adam_1', 'outputs/kernel', 'outputs/kernel/Adam', 'outputs/kernel/Adam_1', 'training/TFOptimizer/beta1_power', 'training/TFOptimizer/beta2_power'], var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: ('./model/saved/1572359223/variables/variables',)\r\nINFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/bias/Adam; prev_var_name: Unchanged\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 83, in <module>\r\n    saved_estimator.train(cust_train_input_fn)\r\n  File \"/anaconda3/envs/tf-abalone/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/anaconda3/envs/tf-abalone/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/anaconda3/envs/tf-abalone/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1241, in _train_model_default\r\n    saving_listeners)\r\n  File \"/anaconda3/envs/tf-abalone/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1360, in _train_with_estimator_spec\r\n    warm_starting_util.warm_start(*self._warm_start_settings)\r\n  File \"/anaconda3/envs/tf-abalone/lib/python3.6/site-packages/tensorflow/python/training/warm_starting_util.py\", line 463, in warm_start\r\n    _warm_start_var(variable, ckpt_to_initialize_from, prev_var_name)\r\n  File \"/anaconda3/envs/tf-abalone/lib/python3.6/site-packages/tensorflow/python/training/warm_starting_util.py\", line 170, in _warm_start_var\r\n    current_var_name = _infer_var_name(var)\r\n  File \"/anaconda3/envs/tf-abalone/lib/python3.6/site-packages/tensorflow/python/training/warm_starting_util.py\", line 142, in _infer_var_name\r\n    name_to_var_dict = saver.BaseSaverBuilder.OpListToDict(var)\r\n  File \"/anaconda3/envs/tf-abalone/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 572, in OpListToDict\r\n    name)\r\nValueError: At least two variables have the same name: dense_1/bias/Adam", "comments": ["Issue replicating for TF 1.12,kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/7f3781dbd355dcf9915f96c2c1405944/33826.ipynb) of colab.Thanks!", "Forgot to mention that there is an issue when running the same code in TensorFlow 1.13, 1.14 and 1.15, in the SavedModelEstimator call to train, raising:\r\n\r\n`ValueError: Dimension 0 in both shapes must be equal, but are 0 and 1. Shapes are [0] and [1]. for 'loss/outputs_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation' (op: 'DenseToDenseSetOperation') with input shapes: [0,1], [1,2].`\r\n\r\nShould this be a new issue?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33826\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33826\">No</a>\n"]}, {"number": 33825, "title": "Using metric SparseTopKCategoricalAccuracy on an RNN results in rank mismatch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes. The code is given below to reproduce the issue.**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Linux Ubuntu 18.04**\r\n\r\n- TensorFlow installed from (source or binary):\r\n**Binary**, using pip\r\n\r\n- TensorFlow version:\r\n**v2.0.0-rc2-26-g64c3d38 2.0.0**\r\n\r\n- Python version:\r\n**3.6**\r\n\r\n- CUDA/cuDNN version:\r\n**10.1 / 7.6.2**\r\n\r\n- GPU model and memory:\r\n**GeForce GTX 1070 / 8GB**\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I compile an RNN model with the metric `SparseTopKCategoricalAccuracy()`, the following error results. No error occurs if I use `SparseCategoricalAccuracy()` instead.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1610, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 2 but is rank 3 for 'metrics/sparse_top_k_categorical_accuracy/in_top_k/InTopKV2' (op: 'InTopKV2') with input shapes: [?,?,10], [?,?], [].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"report.py\", line 12, in <module>\r\n    metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 366, in compile\r\n    masks=self._prepare_output_masks())\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2063, in _handle_metrics\r\n    target, output, output_mask))\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2014, in _handle_per_output_metrics\r\n    metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 1067, in call_metric_function\r\n    return metric_fn(y_true, y_pred, sample_weight=weights)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\", line 193, in __call__\r\n    replica_local_fn, *args, **kwargs)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\", line 1135, in call_replica_local_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\", line 176, in replica_local_fn\r\n    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/metrics_utils.py\", line 75, in decorated\r\n    update_op = update_state_fn(*args, **kwargs)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\", line 581, in update_state\r\n    matches = self._fn(y_true, y_pred, **self._fn_kwargs)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\", line 2805, in sparse_top_k_categorical_accuracy\r\n    nn.in_top_k(y_pred, math_ops.cast(y_true, 'int32'), k), K.floatx())\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py\", line 4843, in in_top_k\r\n    return gen_nn_ops.in_top_kv2(predictions, targets, k, name=name)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\", line 5043, in in_top_kv2\r\n    \"InTopKV2\", predictions=predictions, targets=targets, k=k, name=name)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 548, in create_op\r\n    compute_device)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1773, in __init__\r\n    control_input_ops)\r\n  File \"/home/pi/venv/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1613, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Shape must be rank 2 but is rank 3 for 'metrics/sparse_top_k_categorical_accuracy/in_top_k/InTopKV2' (op: 'InTopKV2') with input shapes: [?,?,10], [?,?], [].\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect no error. I suppose `SparseTopKCategoricalAccuracy()` can be used in exactly the same way as `SparseCategoricalAccuracy()`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nmodel = tf.keras.Sequential([\r\n    layers.Embedding(input_dim=1000, output_dim=64),\r\n    layers.LSTM(128, return_sequences=True),\r\n    layers.Dense(10, activation='softmax')])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])\r\n\r\ndata = np.random.randint(0, 1000, (32, 10))  # batch_size=32, seq_length=10\r\nlabels = np.random.randint(0, 10, (32, 10))\r\n\r\nmodel.fit(data, labels, epochs=1, batch_size=32)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI use this custom metric to get around the problem:\r\n\r\n```python\r\nclass InTopK(tf.keras.metrics.Mean):\r\n    def __init__(self, k, name='in_top_k', **kwargs):\r\n        super(InTopK, self).__init__(name=name, **kwargs)\r\n        self._k = k\r\n\r\n    def update_state(self, y_true, y_pred, sample_weight=None):\r\n        matches = tf.nn.in_top_k(\r\n            # flatten tensors\r\n            tf.reshape(tf.cast(y_true, tf.int32), [-1]),\r\n            tf.reshape(y_pred, [-1, y_pred.shape[-1]]),\r\n            k=self._k)\r\n\r\n        return super(InTopK, self).update_state(\r\n            matches, sample_weight=sample_weight)\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.0 ,2.1.0-dev20191029 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/33f418bcd6dfd460f37114fa31dd99ad/untitled315.ipynb). Thanks!", "I have submitted a fix for this now, please try it out and let us know if things look good. Thank you!", "@nickoala Looks like the fix by @pavithrasv resolved the issue. I ran your code with `tf-nightly` and I cannot reproduce the issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/7a0d7006ed71f30a7f65a97e754d5a36/untitled315.ipynb). Thanks!\r\n\r\nI am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33825\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33825\">No</a>\n", "@jvishnuvardhan and @pavithrasv, thank you very much. I am satisfied with the fix. Thanks for the work."]}, {"number": 33824, "title": "code and tutorial description mismatch on CNN", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/images/cnn\r\n## Description of issue (what needs changing):\r\nTwo mismatchs between output shape on model summary and tutorial:\r\n1\"To complete our model, you will feed the last output tensor from the convolutional base (of shape (3, 3, 64))\"\r\nshould be changed to\r\nTo complete our model, you will feed the last output tensor from the convolutional base (of shape (4, 4, 64))\r\n  \r\n2.\"As you can see, our (3, 3, 64) outputs were flattened into vectors of shape (576) before going through two Dense layers.\"\r\nThis should be changed to \r\n\"As you can see, our (4, 4, 64) outputs were flattened into vectors of shape (1024) before going through two Dense layers.\r\n\r\n\r\n\r\n\r\n", "comments": ["@jramirezpr Thanks for finding the typo's. We will update them. Thanks!", "Thanks for the report, can you please send a pull request?\r\nThat file is here in the tensorflow/docs repo: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb\r\nIf you're unfamiliar with editing notebooks, see this section of the TensorFlow docs contributor guide: https://www.tensorflow.org/community/contribute/docs#interactive_notebooks\r\n", "I am working on it right now. I have created a related issue [here](https://github.com/tensorflow/tensorflow/issues/34639). Also relevant may be my observation [here](https://github.com/tensorflow/docs/pull/1238).\r\n\r\n@lamberta Let me know if I am missing something. If not, I'm working on both issues right now.", "This is fixed now. Thanks!"]}, {"number": 33823, "title": "Missing TF 2.0 low-level API guide", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIn the TF1.* docs, a very helpful low-level API guide was provided which helped those of us interested in using TensorFlow for applications other than NN style models.  This appears to be entirely missing from the TF 2.0 documentation.  Is this omission on purpose, and if so how do we teach people how to use the low-level API?\r\n\r\nThanks,\r\n\r\nChris\r\n\r\n\r\n", "comments": ["Stop wasting your time with Tensorflow. \r\nI have too wasted my time. Nobody listens.\r\nMove to PyTorch.", "We have this guide now: https://www.tensorflow.org/tutorials/customization/performance\r\n\r\nBut we are working on more.", "Same question.\r\nWhy are the low-level guides removed? I'm not a Keras idiot...", "if you want to use the low level API of tensorflow for model construction try to see the documentation of `tf.Module` and `@tf.function`, here is an example of how to create one conv layer\r\n```\r\ndef weights(name, shape, mean=0.0, stddev=0.02):\r\n    var = tf.Variable(tf.random_normal_initializer(mean=mean, stddev=stddev)(shape), name=name)\r\n    return var\r\n```\r\n\r\nthen: \r\n\r\n```\r\nclass conv_layer(tf.Module):\r\n    def __init__(self, filters, norm, act_type='ReLu', name=None):\r\n        super(conv_layer, self).__init__(name=name)\r\n        self.filters = filters\r\n        self.norm = norm\r\n        self.act_type = act_type\r\n        with self.name_scope:\r\n            self.normalization = type_norm(norm) # another class using tf.Module\r\n\r\n    @tf.Module.with_name_scope\r\n    def __call__(self, input):\r\n        if not hasattr(self, 'weights'):\r\n            self.weights = weights('weights', (7, 7, input.get_shape()[-1], self.filters))\r\n        padded = tf.pad(input, [[0, 0], [3, 3], [3, 3], [0, 0]], 'REFLECT')\r\n        conv = tf.nn.conv2d(padded, self.weights, strides=[1, 1, 1, 1], padding='VALID')\r\n\r\n        normalized = self.normalization(conv)\r\n\r\n        if self.act_type == 'ReLu':\r\n            output = tf.nn.relu(normalized)\r\n        elif self.act_type == 'tanh':\r\n            output = tf.nn.tanh(normalized)\r\n        return output\r\n``` \r\nwrite the architecture in the same way, then you can write the custom training step decorating the function with `@tf.function` and do your training loop as in tf 1.x", "@chrism0dwk \r\nIs this still an issue.", "@Saduf2019 Yes, I believe it is now covered in the new Tensorflow Basics section of the TF [Guide](https://tensorflow.org/guide).  Very many thanks for the new resource!\r\n", "@chrism0dwk \r\nThanks for your update."]}, {"number": 33822, "title": "tf.contrib.rnn.OutputProjectionWrapper", "body": "hi,\r\nIn tf 1.14.0,there is no new API for the function,\r\ncould you help me ?I want translate the Op in tflite .\r\nthx", "comments": ["Can you please be specific of what your issue is @ucasiggcas ?", "Closing this issue as it has been awaiting response for more than 3 weeks. Please add additional comments and we can open this issue again. Thanks!"]}, {"number": 33821, "title": "0.6.0", "body": "Sde", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33821) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 33820, "title": "normalization in cosine similarity", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/cosine_similarity\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity\r\n\r\n## Description of issue (what needs changing):\r\nThe documentation for the cosine similarity does not state whether `y_true` and `y_pred`\r\nare expected to be normalized vectors. The provided equation `loss = -sum(y_true * y_pred)`\r\nsuggests the need to be, but looking at the source, they are normalized as part of the computation.\r\n```\r\ny_true = nn.l2_normalize(y_true, axis=axis)\r\ny_pred = nn.l2_normalize(y_pred, axis=axis)\r\nreturn -math_ops.reduce_sum(y_true * y_pred, axis=axis)\r\n```\r\nAs a special case, the doc does not state what happens in the case of either being zero.\r\n\r\n(Also, isn't the above implementation suboptimal in terms of speed, as each element is divided by \r\nthe norm, instead of simply dividing the result once?)\r\n", "comments": ["@ngc92 Can you contribute through a PR to update the docs? Thanks!", "Yes, can you send a PR?\r\n\r\nThe code is here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L1097-L1141\r\n\r\nIf you make a PR please use [doctest format](https://docs.python.org/3/library/doctest.html), for example code so that it is testable (prefix code lines with `>>>`).\r\n\r\n", "I think this could be closed since #37090 is merged ?", "@Ir1d Thanks for the note. I am closing this issue as the PR https://github.com/tensorflow/tensorflow/pull/37090 related to this issue already merged. Thanks!", "The documentation is still lacking for the `CosineSimilarity` class, and contains a sign error. \r\nI have submittd #37343 for that."]}, {"number": 33819, "title": "get_default_graph", "body": "", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nCan you please elaborate about the issue & the context.\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 33818, "title": "KMeans module not found in TensorFlow 2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.13.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /\r\n- TensorFlow installed from (source or binary): pip package manager\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): /\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n\r\n**Describe the current behavior**\r\nI am following this TensorFlow documentation https://www.tensorflow.org/api_docs/python/tf/compat/v1/estimator/experimental/KMeans\r\n\r\nWhen running the example I am getting the following error message:\r\n```\r\nAttributeError: module 'tensorflow_estimator.python.estimator.api._v2.estimator.experimental' has no attribute 'KMeans'\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect a example which is possible to execute\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nnum_points = 100\r\ndimensions = 2\r\npoints = np.random.uniform(0, 1000, [num_points, dimensions])\r\n\r\ndef input_fn():\r\n  return tf.train.limit_epochs(\r\n      tf.convert_to_tensor(points, dtype=tf.float32), num_epochs=1)\r\n\r\nnum_clusters = 5\r\nkmeans = tf.estimator.experimental.KMeans(\r\n    num_clusters=num_clusters, use_mini_batch=False)\r\n\r\n# train\r\nnum_iterations = 10\r\nprevious_centers = None\r\nfor _ in xrange(num_iterations):\r\n  kmeans.train(input_fn)\r\n  cluster_centers = kmeans.cluster_centers()\r\n  if previous_centers is not None:\r\n    print 'delta:', cluster_centers - previous_centers\r\n  previous_centers = cluster_centers\r\n  print 'score:', kmeans.score(input_fn)\r\nprint 'cluster centers:', cluster_centers\r\n\r\n# map the input points to their clusters\r\ncluster_indices = list(kmeans.predict_cluster_index(input_fn))\r\nfor i, point in enumerate(points):\r\n  cluster_index = cluster_indices[i]\r\n  center = cluster_centers[cluster_index]\r\n  print 'point:', point, 'is in cluster', cluster_index, 'centered at', center\r\n```\r\n\r\n**Other info / logs**\r\nI also tried to run it with the nightly version of TensorFlow 2, but I get the same results\r\n", "comments": ["@SaschaHeyer As mentioned by the error message and as you can see [here](https://www.tensorflow.org/api_docs/python/tf/estimator/experimental) the `tf.estimator.experimental` module doesn't have `KMeans`. \r\nInstead you can use `tf.compat.v1.estimator.experimental.KMeans` to accomplish the same task. You can find my github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/674ddf0ad388d3e29750e7c3200c67d9/untitled213.ipynb), where I used the same to accomplish the task.", "@gowthamkpr confirmed \ud83d\udc4d\r\nThanks for the fast feedback appreciate it"]}, {"number": 33817, "title": "Failure to install via 'pipenv install' with just \"tensorflow==2.0.0\" in requirements.txt", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.6\r\n- Installed using virtualenv? pip? conda?: 'pipenv install' in a pyenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: not sure \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'm sorry if this is the wrong place to post this but I'm having trouble installing Tensorflow 2.0.0. I posted this problem on Super User [here](https://superuser.com/questions/1496530/difficulty-installing-tensorflow-using-requirements-txt-and-pipenv-install?noredirect=1#) but I'm not sure if that was the best place.\r\n\r\nI have managed to install Tensorflow on my computer but I'm working on a [shared project](https://gitlab.com/neilwarrack/spinformation) and we want it to be installed using the 'pipenv install' command along with a `requirements.txt` file. If I have a one line requirements.txt file that looks like this: `tensorflow==2.0.0`, then I see a lot of python traceback output which I have attached here (and can be found in the [Super User question I posted](https://superuser.com/questions/1496530/difficulty-installing-tensorflow-using-requirements-txt-and-pipenv-install?noredirect=1#))\r\n[tensor_pipenv.log](https://github.com/tensorflow/tensorflow/files/3782673/tensor_pipenv.log)\r\n\r\nI have the following possibly helpful versions/locations of things:\r\n> $ which pip\r\n> /home/nw/.pyenv/shims/pip\r\n> $ which pipenv\r\n> home/nw/.local/bin/pipenv\r\n> $ which python\r\n> /home/nw/.pyenv/shims/python\r\n> $ python --version\r\n> 3.6.6\r\n> $ pip --version\r\n> pip 19.3.1 from /usr/local/lib/python3.5/dist-packages/pip (python 3.5)\r\n\r\nI see the same output if I try this with [our full requirements.txt](https://gitlab.com/neilwarrack/spinformation/blob/master/requirements.txt) file: \r\n[requirements_orig.txt](https://github.com/tensorflow/tensorflow/files/3782698/requirements_orig.txt)\r\n", "comments": ["Someone suggested I upgrade setuptools. But I upgraded to 41.5.1 (latest) and it makes no difference.", "@neilwarrack This is not a tensorflow issue. This issue related to `pipenv` and its been tracked [here](https://github.com/pypa/pipenv/issues/2791). Follow this [comment](https://github.com/pypa/pipenv/issues/2791#issuecomment-446037453) and it should work.", "This is happening in one of my projects, for now you can just skip the pipenv install and use pip directly to install tensorflow, however this isn't an ideal solution.", "> @neilwarrack This is not a tensorflow issue. This issue related to `pipenv` and its been tracked [here](https://github.com/pypa/pipenv/issues/2791). Follow this [comment](https://github.com/pypa/pipenv/issues/2791#issuecomment-446037453) and it should work.\r\n\r\nHi! Thanks for your feedback!\r\n:D ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33817\">No</a>\n"]}, {"number": 33816, "title": "Fixing decode_raw_op for complex numbers on big endian", "body": "The //tensorflow/python/kernel_tests:decode_raw_op_test fails as the current code for handling reversal of byte ordering of elements on big endian does not handle complex numbers correctly.\r\nFixes #33496", "comments": ["@jhseu , Could you please check?", "Thanks @namrata-ibm! Could you please extend DecodeRawOpTest.testEndianness in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/decode_raw_op_test.py to include tests for complex values so that we can ensure your code is tested? Note I have recently cleaned up the file to be more 2.0 friendly.", "@namrata-ibm Could you please check reviewer comments and keep us posted. Thanks!", "Apologies for delay in reply!\r\n@jaingaurav Is it ok to add a change as below:\r\n```diff  \r\n  def testEndianness(self):\r\n    self.assertAllEqual(\r\n        [[0x04030201]],\r\n        parsing_ops.decode_raw(\r\n            [\"\\x01\\x02\\x03\\x04\"], dtypes.int32, little_endian=True))\r\n    self.assertAllEqual(\r\n        [[0x01020304]],\r\n        parsing_ops.decode_raw(\r\n            [\"\\x01\\x02\\x03\\x04\"], dtypes.int32, little_endian=False))\r\n+    self.assertAllEqual(\r\n+        [[1+2j]],\r\n+        parsing_ops.decode_raw(\r\n+            [b'\\x00\\x00\\x80?\\x00\\x00\\x00@'], dtypes.complex64, little_endian=True))\r\n+    self.assertAllEqual(\r\n+        [[1+2j]],\r\n+        parsing_ops.decode_raw(\r\n+            [b'?\\x80\\x00\\x00@\\x00\\x00\\x00'], dtypes.complex64, little_endian=False))\r\n\r\n```\r\nThis verifies that input data representing a complex number \"1+2j\" needs swap midway(4 bytes each) for correctly identifying the real and imaginary data instead of full data reversal(8 bytes for complex64). This test passes on both x86_64 and s390x.", "@namrata-ibm: Yes, that looks perfect.", "@namrata-ibm Could you please address Ubuntu Sanity errors? Thanks!", "Thanks for the fix @namrata-ibm!"]}, {"number": 33815, "title": "tensorflow1.14.0 , cuda 10, cudnn 7.4 , bazel 0.25, shell cmd\uff1abazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so ", "body": "/home/work/.cache/bazel/_bazel_work/4fa4430cecf16c459ae4d856e8ea7fba/external/zlib_archive/BUILD.bazel:5:1: undeclared inclusion(s) in rule '@zlib_archive//:zlib':\r\nthis rule is missing dependency declarations for the following files included by 'external/zlib_archive/compress.c':\r\n\r\n'/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/stdc-predef.h'\r\n  '/opt/compiler/gcc-4.8.2/lib/gcc/x86_64-baidu-linux-gnu/4.8.2/include/stddef.h'\r\n  '/opt/compiler/gcc-4.8.2/lib/gcc/x86_64-baidu-linux-gnu/4.8.2/include-fixed/limits.h'\r\n  '/opt/compiler/gcc-4.8.2/lib/gcc/x86_64-baidu-linux-gnu/4.8.2/include-fixed/syslimits.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/limits.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/features.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/sys/cdefs.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/wordsize.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/gnu/stubs.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/gnu/stubs-64.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/posix1_lim.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/local_lim.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/linux/limits.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/posix2_lim.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/sys/types.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/types.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/typesizes.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/time.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/endian.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/endian.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/byteswap.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/byteswap-16.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/sys/select.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/select.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/sigset.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/time.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/select2.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/sys/sysmacros.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/pthreadtypes.h'\r\n  '/opt/compiler/gcc-4.8.2/lib/gcc/x86_64-baidu-linux-gnu/4.8.2/include/stdarg.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/unistd.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/posix_opt.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/environments.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/confname.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/getopt.h'\r\n  '/opt/compiler/gcc-4.8.2/x86_64-baidu-linux-gnu/sys-root/include/bits/unistd.h'\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.572s, Critical Path: 0.11s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully", "comments": ["bazel tensorflow this rule is missing dependency declarations for the following files included by", "@huake ,\r\nHi please follow this [link](https://www.tensorflow.org/install/source) for installation, looks like some path is incorrect while installing.Also include` ./configure `output.Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@huake ,\r\nany update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33815\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33815\">No</a>\n"]}, {"number": 33814, "title": "Profile contains events with truncated names", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.4\r\n- TensorFlow installed from (source or binary): Docker container\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: K80\r\n\r\n**Describe the current behavior**\r\n\r\nWhen gathering a trace (via the supposedly only supported in TF2 method) the resulting trace file shows incomplete GPU op names. Validated in TensorBoard and deserializing the Protobuf message.\r\nExamples:\r\n```\r\nGpuDevice>, long)\r\nGpuDevice>, int)\r\nDimension<3>, unsigned int*)\r\nreduced_divisor)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nTrace shows complete function names (at least) and signatures (optional) or even better: Layers to which those activities/events belong.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport datetime\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\n\r\ndef preprocess_for_mnist(image, label):\r\n    image = tf.cast(image, tf.float32) / 255.0\r\n    return image, label\r\n\r\n\r\ndatasets, info = tfds.load(name='mnist',\r\n                           with_info=True,\r\n                           as_supervised=True,\r\n                           shuffle_files=False)\r\n\r\nstrategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\r\n\r\ntrain_dataset = datasets['train'].map(preprocess_for_mnist).batch(32)\r\n\r\nlog_dir = os.path.join(os.path.expanduser('~/tf_logs'),\r\n                       datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32,\r\n                               3,\r\n                               activation='relu',\r\n                               input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\n    callbacks = [\r\n        tf.keras.callbacks.TensorBoard(log_dir=log_dir,\r\n                                       histogram_freq=0,\r\n                                       profile_batch=2)\r\n    ]\r\n\r\n    model.compile(loss='sparse_categorical_crossentropy',\r\n                  optimizer=tf.keras.optimizers.Adam(),\r\n                  metrics=['accuracy'])\r\n    model.fit(train_dataset, epochs=2, steps_per_epoch=10, callbacks=callbacks)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nTraces generated: [train.zip](https://github.com/tensorflow/tensorflow/files/3782526/train.zip)\r\n\r\n", "comments": ["I think the issue has already been fixed. The trace was collected on Oct 29th. Can you give us a trace at head, and we can investigate if the issue persists.", "I tried but can't test it currently.\r\n\r\nI just tried with the docker containers but neither tensorflow-devel-gpu-py3 nor tensorflow-devel-py3 seem to actually contain tensorflow. In `/usr/local/lib/python3.6/dist-packages/` I see some Keras_* stuff but nothing with tensorflow. `pip freeze` doesn't list it either.\r\n\r\nUsing pip to install the 2.1.0rc has upgraded the CUDA requirement to 10.1 under which the driver requires admin rights or \"unrestricted mode\" to enable tracing. I passed it on to our HPC admins but it might take a bit.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33814\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33814\">No</a>\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "Still checking if this is an issue, @tensorflowbutler please reopen"]}, {"number": 33813, "title": "the relationship among optimize_for_inference.py quantize_graph.py graph_transforms", "body": "Is graph_transforms  tool the newest tool to optimize and quantize for pb.\r\nwhen I find tf < 1.11,has quantize_graph.py.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 33812, "title": "Add unit tests for keras network get_layer.", "body": "This PR adds unit tests for the method `get_layer` of a Keras `Network`. ", "comments": ["@vcarpani Can you please resolve conflicts? Thanks!", "Can you please resolve conflicts?", "@vcarpani Can you please check @mihaimaruseac's comments and keep us posted. Thanks!", "Yes, thanks for the feedback; will check them, but it make take a little be longer than usual cause COVID", "Sanity build failed due to pylint issue. Please make sure you download log and look for the pylint errors and fix them", "@vcarpani Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 33811, "title": "Got \"Data adapters should be mutually exclusive for handling inputs. Found multiple adapters to handle\" error when calling `model.fit` with ImageDataGenerator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 aarch64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below) 2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.29.0\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen fitting a model with ImageDataGenerator, it raises this error \"Data adapters should be mutually exclusive for handling inputs. Found multiple adapters 'GeneratorDataAdapter', 'KerasSequenceAdapter' to handle\". \r\n\r\n**Describe the expected behavior**\r\n1. Log warning message if multiple data adapters found, instead of raising an error\r\n2. Use the first available data adapter\r\n\r\n**Code to reproduce the issue**\r\nPlease refer to link below:\r\nhttps://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c02_dogs_vs_cats_with_augmentation.ipynb\r\n\r\nI connected to my local jupyter instance with Colab UI.\r\n\r\n```python\r\nBATCH_SIZE = 100\r\nIMG_SHAPE  = 150\r\n\r\nimage_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\r\ntrain_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\r\n                                               directory=train_dir,\r\n                                               shuffle=True,\r\n                                               target_size=(IMG_SHAPE,IMG_SHAPE))\r\nval_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\r\n                                               directory=val_dir,\r\n                                               shuffle=True,\r\n                                               target_size=(IMG_SHAPE,IMG_SHAPE))\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\r\n    tf.keras.layers.MaxPooling2D(2, 2),\r\n\r\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(2,2),\r\n\r\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(2,2),\r\n\r\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(2,2),\r\n\r\n    tf.keras.layers.Dropout(0.5),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(512, activation='relu'),\r\n    tf.keras.layers.Dense(2, activation='softmax')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nepochs = 100\r\nmodel.fit(\r\n    train_data_gen,\r\n    steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))),\r\n    epochs=epochs,\r\n    validation_data=val_data_gen, \r\n    validation_steps=int(np.ceil(total_val / float(BATCH_SIZE)))\r\n)\r\n```\r\n\r\nTo avoid this issue, I'll have to manually exclude \"KerasSequenceAdapter\" before calling `model.fit`\r\n\r\n```python\r\nfrom tensorflow.python.keras.engine import data_adapter\r\nfrom tensorflow.python.keras.engine.data_adapter import ListsOfScalarsDataAdapter\r\nfrom tensorflow.python.keras.engine.data_adapter import TensorLikeDataAdapter\r\nfrom tensorflow.python.keras.engine.data_adapter import GenericArrayLikeDataAdapter\r\nfrom tensorflow.python.keras.engine.data_adapter import DatasetAdapter\r\nfrom tensorflow.python.keras.engine.data_adapter import GeneratorDataAdapter\r\nfrom tensorflow.python.keras.engine.data_adapter import CompositeTensorDataAdapter\r\n\r\ndata_adapter.ALL_ADAPTER_CLS = [\r\n ListsOfScalarsDataAdapter,\r\n TensorLikeDataAdapter,\r\n GenericArrayLikeDataAdapter,\r\n DatasetAdapter,\r\n GeneratorDataAdapter,\r\n#  tensorflow.python.keras.engine.data_adapter.KerasSequenceAdapter,\r\n CompositeTensorDataAdapter      \r\n]\r\n\r\ndata_adapter.ALL_ADAPTER_CLS\r\n```\r\n\r\n**Other info / logs**\r\nN/A.\r\n", "comments": ["Humm, seems that the Image Iterator class implements the interface for both generator and keras sequence object. Let me take a closer look to fix the issue.", "Hey, I think we did some recent update for data_adapter which might fix this issue. Could u have a try with latest nightly?\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/ac20030c96d37e980333b604402ef6dba48ef5e2", "Sure I will, thanks btw~", "@qlzh727 The fix works! Thank you!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33811\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33811\">No</a>\n", "@qlzh727, I'm having this same problem under TF 2.2, TF2.3, and tf-nightly (as of yesterday). The problem seems to be identical in description to that of @gekowa. Any advice? Here is my system info...\r\n\r\n**System information**\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n    TensorFlow installed from (source or binary): binary\r\n    TensorFlow version (use command below): 2.2.0 (also tried 2.3 and tf-nightly)\r\n    Python version: 3.6.9\r\n    Bazel version (if compiling from source):\r\n    GCC/Compiler version (if compiling from source):\r\n    CUDA/cuDNN version: 10.1 / 7.6.5\r\n    GPU model and memory: RTX 2080 Super, 8 GB\r\n"]}, {"number": 33810, "title": "Please add the \"reuse\" parameter to tf.keras.layers.Conv2D()", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\nNo\r\n\r\n**Describe the feature and the current behavior/state.**\r\nUnable to implement complex weight sharing\r\n**Will this change the current api? How?**\r\nwill do\uff01Add the \"reuse\" parameter to tf.keras.layers.Conv2D()\r\n**Who will benefit with this feature?**\r\nEvery user\r\n**Any Other info.**\r\nWhy remove the \u2018reuse\u2019 parameter from tf.keras.layers.Conv2D(), or do you provide other ways to share weights? The algorithm implemented in 1.x is now not implemented in 2.0. If there is no better way to share weights, please let the \u2018reuse\u2019 parameter come back.\r\n", "comments": ["`reuse` is a tf legacy python layer thing, it's never part of keras layers, and especially not part of Tensorflow 2.0 because we removed variable scope and the reusing of variables.\r\nTo achieve weight sharing, either reuse the layer directly, or build customized layer that re-uses the weight "]}, {"number": 33809, "title": "MirroredStrategy compared to OneDeviceStrategy slower and much weaker learning", "body": "**System information**System information\r\n- OS Platform and Distribution: Arch Linux, 5.3.7-arch1-1-ARCH\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.0.0\r\n- Keras version: 2.2.4-tf\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: CUDA 10.1.243 / cuDNN 7.6.2.24\r\n- GPU model and memory: 2x GTX 1080 Ti 11GB\"`\r\n\r\n**Describe the current behavior**\r\nIf the model is trained with OneDeviceStrategy on one GPU an accuracy of 0.9988 is reached after 150 epochs in 5h 24min.\r\nIf the model is trained with MirroredStrategy on two GPUs an accuracy of 0 is reached after 150 epochs in 5h. The loss does not significantly drop.\r\n\r\n**Describe the expected behavior**\r\nWith MirroredStrategy the same accuracy is reached as training on one GPU  in shorter time (ideally in half the time).\r\nMight be related to issue #33767.\r\n\r\n**Code to reproduce the issue**\r\nThe complete code with data is available in a git repo if required.\r\n- Model:\r\n```\r\nclass FeatureExtraction(Layer):\r\n    def __init__(self, conv_filters, pool_size, name='feature-extraction', **kwargs):\r\n        super(FeatureExtraction, self).__init__(name=name, **kwargs)\r\n        self.conv1 = Conv2D(filters=conv_filters, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer='he_normal', name='conv1')\r\n        self.conv2 = Conv2D(filters=conv_filters, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer='he_normal', name='conv2')\r\n        self.max1 = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')\r\n        self.max2 = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')\r\n\r\n    def call(self, inputs):\r\n        x = self.conv1(inputs)\r\n        x = self.max1(x)\r\n        x = self.conv2(x)\r\n        return self.max2(x)\r\n\r\n    def get_config(self):\r\n        return super(FeatureExtraction, self).get_config()\r\n\r\n\r\nclass FeatureReduction(Layer):\r\n    def __init__(self, img_w, img_h, pool_size, conv_filters, name='feature-reduction', **kwargs):\r\n        super(FeatureReduction, self).__init__(name=name, **kwargs)\r\n        target_shape = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters)\r\n        self.reshape = Reshape(target_shape=target_shape, name='reshape')\r\n        self.dense = Dense(32, activation='relu', name='dense')\r\n\r\n    def call(self, inputs):\r\n        x = self.reshape(inputs)\r\n        return self.dense(x)\r\n\r\n    def get_config(self):\r\n        return super(FeatureReduction, self).get_config()\r\n\r\n\r\nclass SequentialLearner(Layer):\r\n    def __init__(self, name='sequential-learner', **kwargs):\r\n        super(SequentialLearner, self).__init__(name=name, **kwargs)\r\n        self.gru_1a = GRU(512, return_sequences=True, kernel_initializer='he_normal', name='gru_1a')\r\n        self.gru_1b = GRU(512, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru_1b')\r\n        self.gru_2a = GRU(512, return_sequences=True, kernel_initializer='he_normal', name='gru_2a')\r\n        self.gru_2b = GRU(512, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru_2b')\r\n\r\n    def call(self, inputs):\r\n        x_1a = self.gru_1a(inputs)\r\n        x_1b = self.gru_1b(inputs)\r\n        x = add([x_1a, x_1b])\r\n        x_2a = self.gru_2a(x)\r\n        x_2b = self.gru_2b(x)\r\n        return concatenate([x_2a, x_2b])\r\n\r\n    def get_config(self):\r\n        return super(SequentialLearner, self).get_config()\r\n\r\n\r\nclass Output(Layer):\r\n    def __init__(self, output_size, name='output', **kwargs):\r\n        super(Output, self).__init__(name=name, **kwargs)\r\n        self.dense = Dense(output_size, kernel_initializer='he_normal', name='dense')\r\n        self.softmax = Activation('softmax', name='softmax')\r\n\r\n    def call(self, inputs):\r\n        x = self.dense(inputs)\r\n        return self.softmax(x)\r\n\r\n    def get_config(self):\r\n        return super(Output, self).get_config()\r\n\r\n\r\nclass OCRNet(Model):\r\n    def __init__(self, output_size, img_w, img_h, max_text_len, name='OCRNet', **kwargs):\r\n        # parameters\r\n        conv_filters = 16\r\n        pool_size = 2\r\n        # define layers\r\n        feature_extraction = FeatureExtraction(conv_filters=conv_filters, pool_size=pool_size)\r\n        sequential_learner = SequentialLearner()\r\n        feature_reduction = FeatureReduction(img_w=img_w, img_h=img_h, pool_size=pool_size, conv_filters=conv_filters)\r\n        output = Output(output_size)\r\n        # NHWC == channels_last NCHW == channels_first\r\n        # initialize input shape\r\n        if 'channels_first' == K.image_data_format():\r\n            input_shape = (1, img_w, img_h)\r\n        else:\r\n            input_shape = (img_w, img_h, 1)\r\n        # input\r\n        inputs = Input(name='the_input', shape=input_shape, dtype='float32')\r\n        labels = Input(name='the_labels', shape=[max_text_len], dtype='float32')\r\n        input_length = Input(name='input_length', shape=[1], dtype='int64')\r\n        label_length = Input(name='label_length', shape=[1], dtype='int64')\r\n        # call layers\r\n        x = feature_extraction(inputs)\r\n        x = feature_reduction(x)\r\n        x = sequential_learner(x)\r\n        predictions = output(x)\r\n        # Keras doesn't currently support loss funcs with extra parameters\r\n        # so CTC loss is implemented in a lambda layer\r\n        loss_out = Lambda(self._ctc_lambda_func, output_shape=(1,), name='ctc')([predictions, labels, input_length, label_length])\r\n        super(OCRNet, self).__init__(\r\n                inputs=[inputs, labels, input_length, label_length], outputs=loss_out,\r\n                name=name, **kwargs)\r\n\r\n        # ctc decoder\r\n        flattened_input_length = K.reshape(input_length, (-1,))\r\n        top_k_decoded, _ = K.ctc_decode(predictions, flattened_input_length)\r\n        self.decoder = K.function([inputs, flattened_input_length], [top_k_decoded[0]])\r\n\r\n    # loss and train functions, network architecture\r\n    def _ctc_lambda_func(self, args):\r\n        predictions, labels, input_length, label_length = args\r\n        # the 2 is critical here since the first couple outputs of the RNN\r\n        # tend to be garbage\r\n        predictions = predictions[:, 2:, :]\r\n        return K.ctc_batch_cost(labels, predictions, input_length, label_length)\r\n```\r\n- training (stripped):\r\n```\r\n    ...\r\n    strategy = tf.distribute.MirroredStrategy() if 1 < ngpus else tf.distribute.OneDeviceStrategy(device=\"/gpu:1\")\r\n    batch_size = batch_size * strategy.num_replicas_in_sync\r\n   ...\r\n    with strategy.scope():\r\n        model = OCRNet(train_gen.output_size, img_w, img_h, max_text_len)\r\n        model.summary()\r\n        adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\r\n        model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam, metrics=['accuracy'])\r\n    callbacks = []\r\n    start = time.perf_counter()\r\n    model.fit(\r\n            train_gen,\r\n            validation_data=val_gen,\r\n            epochs=epochs,\r\n            shuffle=False,\r\n            use_multiprocessing=True,\r\n            workers=6,\r\n            callbacks=callbacks)\r\n    elapsed = time.perf_counter() - start\r\n    logger.info('elapsed: {:0.3f}'.format(elapsed))\r\n```\r\n\r\n\r\n**Other info / logs**\r\n- Output using OneDeviceStrategy:\r\n\r\n> Train for 700 steps, validate for 150 steps                                                                                           \r\n> Epoch 1/150  \r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fb63f70d170> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> 2019-10-29 06:07:28,887 - WARNING - Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fb63f70d170> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> 2019-10-29 06:07:31.225908: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Node 'OCRNet/sequential-learner/gru_1a/StatefulPartitionedCall_StatefulPartitionedCall_2_26': Connecting to invalid output 31 of source node OCRNet/sequential-learner/gru_1a/StatefulPartitionedCall which has 31 outputs.\r\n> 2019-10-29 06:07:31.774256: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2019-10-29 06:07:31.965277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 699/700 [============================>.] - ETA: 0s - loss: 41.7196 - accuracy: 0.0000e+002019-10-29 06:09:19.892252: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_gru_with_fallback_16142_specialized_for_OCRNet_sequential-learner_gru_2b_StatefulPartitionedCall_at___inference_distributed_function_16574' and '__inference_cudnn_gru_with_fallback_16142' both implement 'gru_939794c2-9fc6-48f9-8d2a-e319e349d493' but their signatures do not match.\r\n> 700/700 [==============================] - 134s 191ms/step - loss: 41.6829 - accuracy: 0.0000e+00 - val_loss: 15.7647 - val_accuracy: 0.0000e+00\r\n> Epoch 2/150\r\n> 700/700 [==============================] - 129s 185ms/step - loss: 15.9857 - accuracy: 0.0000e+00 - val_loss: 15.0192 - val_accuracy: 0.0000e+00\r\n> Epoch 3/150\r\n> 700/700 [==============================] - 129s 184ms/step - loss: 14.3529 - accuracy: 0.0000e+00 - val_loss: 13.8274 - val_accuracy: 0.0000e+00\r\n> Epoch 4/150\r\n> 700/700 [==============================] - 129s 185ms/step - loss: 13.4774 - accuracy: 0.0000e+00 - val_loss: 13.1987 - val_accuracy: 0.0000e+00\r\n> Epoch 5/150\r\n> 700/700 [==============================] - 129s 185ms/step - loss: 12.9877 - accuracy: 0.0000e+00 - val_loss: 12.8102 - val_accuracy: 0.0000e+00\r\n> ...\r\n> Epoch 145/150\r\n> 700/700 [==============================] - 130s 185ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0119 - val_accuracy: 0.9952\r\n> Epoch 146/150\r\n> 700/700 [==============================] - 130s 185ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.0118 - val_accuracy: 0.9953\r\n> Epoch 147/150\r\n> 700/700 [==============================] - 130s 185ms/step - loss: 0.0056 - accuracy: 0.9987 - val_loss: 0.0116 - val_accuracy: 0.9953\r\n> Epoch 148/150\r\n> 700/700 [==============================] - 129s 185ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.0114 - val_accuracy: 0.9953\r\n> Epoch 149/150\r\n> 700/700 [==============================] - 129s 185ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.0113 - val_accuracy: 0.9954\r\n> Epoch 150/150\r\n> 700/700 [==============================] - 129s 185ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.0111 - val_accuracy: 0.9954\r\n> 2019-10-28 04:33:15,026 - INFO - elapsed: 19429.327\r\n\r\n\r\n- Output using MirroredStrategy:\r\n\r\n> Train for 350 steps, validate for 75 steps \r\n> Epoch 1/150\r\n> INFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n> 2019-10-28 21:41:23,061 - INFO - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n> INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> 2019-10-28 21:41:23,323 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> 2019-10-28 21:41:23,328 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> 2019-10-28 21:41:24,338 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> 2019-10-28 21:41:24,341 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f787c47b170> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> 2019-10-28 21:41:24,385 - WARNING - Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f787c47b170> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> INFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n> 2019-10-28 21:41:28,827 - INFO - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n> INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> 2019-10-28 21:41:29,117 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> 2019-10-28 21:41:29,120 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> 2019-10-28 21:41:29,128 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> 2019-10-28 21:41:29,130 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n> 2019-10-28 21:41:29.440363: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Node 'replica_1/OCRNet/sequential-learner/gru_1a/StatefulPartitionedCall_replica_1/StatefulPartitionedCall_2_26': Connecting to invalid output 31 of source node replica_1/OCRNet/sequential-learner/gru_1a/StatefulPartitionedCall which has 31 outputs.\r\n> 2019-10-28 21:41:30.730864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2019-10-28 21:41:31.078427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 349/350 [============================>.] - ETA: 0s - loss: 89.5114 - accuracy: 0.0000e+002019-10-28 21:43:16.332927: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_gru_26258' and '__inference_cudnn_gru_with_fallback_26349_specialized_for_OCRNet_sequential-learner_gru_2b_StatefulPartitionedCall_at___inference_distributed_function_28787' both implement 'gru_d38ba96e-e1cb-43bd-a1af-2f107f6aab80' but their signatures do not match.\r\n> 350/350 [==============================] - 138s 395ms/step - loss: 89.5000 - accuracy: 0.0000e+00 - val_loss: 86.4455 - val_accuracy: 0.0000e+00\r\n> Epoch 2/150\r\n> 350/350 [==============================] - 120s 342ms/step - loss: 83.2679 - accuracy: 0.0000e+00 - val_loss: 80.2358 - val_accuracy: 0.0000e+00\r\n> Epoch 3/150\r\n> 350/350 [==============================] - 120s 342ms/step - loss: 76.8871 - accuracy: 0.0000e+00 - val_loss: 73.5664 - val_accuracy: 0.0000e+00\r\n> Epoch 4/150\r\n> 350/350 [==============================] - 120s 342ms/step - loss: 69.5524 - accuracy: 0.0000e+00 - val_loss: 65.3586 - val_accuracy: 0.0000e+00\r\n> Epoch 5/150\r\n> 350/350 [==============================] - 120s 342ms/step - loss: 61.0491 - accuracy: 0.0000e+00 - val_loss: 57.3255 - val_accuracy: 0.0000e+00\r\n> ...\r\n> Epoch 145/150\r\n> 350/350 [==============================] - 120s 343ms/step - loss: 9.9171 - accuracy: 0.0000e+00 - val_loss: 9.8855 - val_accuracy: 0.0000e+00\r\n> Epoch 146/150\r\n> 350/350 [==============================] - 120s 343ms/step - loss: 9.8615 - accuracy: 0.0000e+00 - val_loss: 9.8293 - val_accuracy: 0.0000e+00\r\n> Epoch 147/150\r\n> 350/350 [==============================] - 120s 342ms/step - loss: 9.8055 - accuracy: 0.0000e+00 - val_loss: 9.7728 - val_accuracy: 0.0000e+00\r\n> Epoch 148/150\r\n> 350/350 [==============================] - 120s 343ms/step - loss: 9.7491 - accuracy: 0.0000e+00 - val_loss: 9.7160 - val_accuracy: 0.0000e+00\r\n> Epoch 149/150\r\n> 350/350 [==============================] - 120s 343ms/step - loss: 9.6923 - accuracy: 0.0000e+00 - val_loss: 9.6588 - val_accuracy: 0.0000e+00\r\n> Epoch 150/150\r\n> 350/350 [==============================] - 120s 342ms/step - loss: 9.6351 - accuracy: 0.0000e+00 - val_loss: 9.6010 - val_accuracy: 0.0000e+00\r\n> 2019-10-29 02:41:31,149 - INFO - elapsed: 18013.239", "comments": ["What\u2019s the difference between MirroredStrategy and OneDeviceStrategy, when there is only one GPU available? (TF2.0 stable)", "@olk - can you provide the entire training code? Is it possible to repro this in colab - that will make it easier for us to debug.\r\nAlso, can you try a few things:\r\n- have you tried using a standard loss? I am wondering if we are not handling loss dictionary properly with mirrored strategy\r\n- can you also try mirrored strategy with 1 GPU? I want to see if it's a number of replicas issue, or strategy issue. \r\n- is your training supposed to scale as you double the batch size? training characteristics can change as you increase the global batch size. do you think you may need to tune the hyper params? Alternatively, for now, just test with the same global batch size (ie. half the batch size per replica with 2 replicas). \r\n\r\n@ASLPZHAO - there should not be any user visible difference, but the strategies' implementations are quite different so a bug could lead to some observable difference.\r\n\r\n", "Also, #33767 was marked fixed, so in case that had anything to do with this issue, worth trying with the latest nightly. ", "the problem still remains:\r\n\r\n- using MirroredStrategy with one GPU `strategy = tf.distribute.MirroredStrategy([\"/gpu:1\"]) ` results in an error:\r\n\r\n> 2019-11-13 18:49:44.542233: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Node 'OCRNet/sequential-learner/gru_1a/StatefulPartitionedCall_StatefulPartitionedCall_2_26': Connecting to invalid output 31 of source node OCRNet/sequential-learner/gru_1a/StatefulPartitionedCall which has 31 outputs.\r\n> 2019-11-13 18:49:45.117933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2019-11-13 18:49:45.313954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-11-13 18:49:46.495134: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: var and grad do not have the same shape[2,1536] [512,1536]\r\n> \t [[{{node Adam/Adam/update_14/update_0/ResourceApplyAdam}}]]\r\n> \t [[GroupCrossDeviceControlEdges_0/Identity_3/_185]]\r\n> 2019-11-13 18:49:46.495246: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: var and grad do not have the same shape[2,1536] [512,1536]\r\n> \t [[{{node Adam/Adam/update_14/update_0/ResourceApplyAdam}}]]\r\n> \t [[Identity_4/_190]]\r\n> 2019-11-13 18:49:46.495287: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: var and grad do not have the same shape[2,1536] [512,1536]\r\n> \t [[{{node Adam/Adam/update_14/update_0/ResourceApplyAdam}}]]\r\n>    1/1000 [..............................] - ETA: 2:16:08WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \r\n> 2019-11-13 18:49:46,569 - WARNING - Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \r\n>    1/1000 [..............................] - ETA: 2:18:55Traceback (most recent call last):\r\n>   File \"src/models/train.py\", line 104, in <module>\r\n>     main()\r\n>   File \"src/models/train.py\", line 98, in main\r\n>     callbacks=callbacks)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n>     use_multiprocessing=use_multiprocessing)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n>     total_epochs=epochs)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n>     batch_outs = execution_function(iterator)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n>     distributed_function(input_fn))\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n>     result = self._call(*args, **kwds)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n>     return self._stateless_fn(*args, **kwds)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n>     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n>     self.captured_inputs)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n>     ctx, args, cancellation_manager=cancellation_manager)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n>     ctx=ctx)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n>     six.raise_from(core._status_to_exception(e.code, message), None)\r\n>   File \"<string>\", line 3, in raise_from\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n>   (0) Invalid argument:  var and grad do not have the same shape[2,1536] [512,1536]\r\n> \t [[node Adam/Adam/update_14/update_0/ResourceApplyAdam (defined at /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n> \t [[GroupCrossDeviceControlEdges_0/Identity_3/_185]]\r\n>   (1) Invalid argument:  var and grad do not have the same shape[2,1536] [512,1536]\r\n> \t [[node Adam/Adam/update_14/update_0/ResourceApplyAdam (defined at /usr/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n> 0 successful operations.\r\n> 1 derived errors ignored. [Op:__inference_distributed_function_12688]\r\n> \r\n> Function call stack:\r\n> distributed_function -> distributed_function\r\n> \r\n> 2019-11-13 18:49:46.942138: W tensorflow/core/kernels/data/generator_dataset_op.cc:102] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n> \t [[{{node PyFunc}}]]\r\n> make: *** [Makefile:56: train] Fehler 1\r\n\r\n- the code already doubles the `batch_size` if `MirroredStrategy` is used (two GPUsare available)\r\n- gast was downgraded to 0.2.2 before testing\r\n- I think standard loss is not applicable because CTC is required", "- the entire training code is available at [ki-ocr-spreadsheet](https://github.com/olk/ki-ocr-spreadsheet)\r\n- I don't think that it will run in colab", "@olk thank you for trying out the other things. \r\nRegarding batch size - I meant can you try with 2 GPUs but without doubling the batch size. This would potentially give worse scaling, but should more faithfully match training on 1 GPU in terms of convergence.\r\n\r\n", "@guptapriya:\r\nUsing the same batch-size for MirroredStrategy as for OneDeviceStrategy doesn't change the behaviour.\r\n\r\nWhy does  MirroredStrategy crash if only one GPU has been configured?\r\n\r\nMaybe you already noticed that the loss starts at much higher values for MirroredStrategy as it does for OneDeviceStrategy?!\r\n\r\n>  999/1000 [============================>.] - ETA: 0s - loss: 83.1715 - accuracy: 0.0000e+002019-11-13 21:28:58.423118: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_gru_29792' and '__inference_cudnn_gru_with_fallback_29881_specialized_for_replica_1_OCRNet_sequential-learner_gru_2a_StatefulPartitionedCall_at___inference_distributed_function_30737' both implement 'gru_f0a269b3-1fd4-4b86-a90b-9414b82bd560' but their signatures do not match.\r\n> 1000/1000 [==============================] - 133s 133ms/step - loss: 83.1631 - accuracy: 0.0000e+00 - val_loss: 75.9862 - val_accuracy: 0.0000e+00\r\n> Epoch 2/150\r\n> 1000/1000 [==============================] - 115s 115ms/step - loss: 66.3670 - accuracy: 0.0000e+00 - val_loss: 50.6435 - val_accuracy: 0.0000e+00", "Yeah I noticed the loss starts at higher value for mirrored strategy.\r\n@pavithrasv - do you think there could be some issue with using this custom loss and loss dictionary?\r\n\r\n@olk - would it possible for you to isolate this into a simple test that can be runnable in colab? you can use some fake/synthetic data, and fix the random seed - with that, the model will not converge but we can still check that the loss etc should be same at each step in both cases. \r\nI don't have the bandwidth to take the code from github and try to reproduce it, so if you can provide a runnable sample in colab that reproduces the issue, we can debug much faster. ", "**@guptapriya**, I don't believe that the code can be reduced to run in colab. Could you point me to an example that uses MirroredStrategy and runs in colab? I'd like to test it on my system (2 NUMA nodes, but both GPU are connected tot the same NUMA node + numactl for memory allocation == data do not cross the QPI).", "@guptapriya, I'll test this issue with a modified version (using MirroredStrategy and OneDeviceStrategy) of #34588 , if the issue of #34588 has been solved.", "@olk Here are a couple of resources to help with creating a repro with colab which has 1 GPU:\r\nhttps://www.tensorflow.org/guide/gpu#using_multiple_gpus - you can use virtual device config to create 2 virtual GPUs\r\nhttps://www.tensorflow.org/tutorials/distribute/keras - Colab example with MirorredStrategy - this example doesn't specify any devices but if you setup 2 virtual GPUs as above, it should run with those 2 virtual devices. \r\nI think if it's a quality / correctness issue, we should be able to repro with virtual GPUs and not necessarily need 2 physical GPUs. \r\n\r\n", "I've used a slightly modified version of MNIST  [example available in colab](https://colab.research.google.com/drive/1TXu1dD46r19jhyoD_HOIihxenNaytwQ3#scrollTo=Pasya9wZzJIc).\r\n\r\n1. **Measurements at colab using two logical GPUs**\r\n\r\nOneDeviceStrategy: elapsed time: 880s, accuracy of val-data: 98.75%\r\nMirredStrategy:  elapsed time: 1776s, accuracy of val-data: 98.71%\r\n\r\n2. **Measurements at my NUMA system using two physical GPUs**\r\n\r\n- both GPUs are connected to NUMA node 1\r\n- invoked at command line: _numactl --cpunodebind=1 --membind=1 python mnist.py_\r\n\r\nOneDeviceStrategy: elapsed time: 332, accuracy of val-data: 98.79%\r\nMirredStrategy:  elapsed time: 647s, accuracy of val-data: 98.72%\r\n\r\nThe code block regarding to logic GPU setup has to be commented out because an exception was thrown:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"mnist.py\", line 25, in <module>\r\n>     strategy = tf.distribute.MirroredStrategy()\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 356, in __init__\r\n>     self, devices=devices, cross_device_ops=cross_device_ops)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 396, in __init__\r\n>     self._initialize_strategy(devices)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 410, in _initialize_strategy\r\n>     self._initialize_local(devices)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 420, in _initialize_local\r\n>     cross_device_ops_lib.choose_the_best(devices))\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py\", line 1196, in choose_the_best\r\n>     machine_devices = device_lib.list_local_devices(session_config=session_config)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/client/device_lib.py\", line 41, in list_local_devices\r\n>     for s in pywrap_tensorflow.list_devices(session_config=session_config)\r\n>   File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 2249, in list_devices\r\n>     return ListDevices()\r\n> tensorflow.python.framework.errors_impl.AlreadyExistsError: TensorFlow device (GPU:1) is being mapped to multiple CUDA devices (1 now, and 0 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not  currently supported, see https://github.com/tensorflow/tensorflow/issues/19083\r\n\r\n3. **Observations**\r\n- surprisingly MirroredStrategy is **two times slower** than OneDeviceStrategy\r\n- logical GPU configuration can not be used with two physical GPUs and MirroredStrategy (exception thrown)\r\n\r\n4. **Expectation**\r\n- my target is a NUMA system with two physical GPUs, Tensorflow code should take benefit of it\r\n- with MirroredStrategy the runtime should be almost the half of using OneDeviceStrategy (not two times longer)", "@olk - let's focus on the issue of why the learning is different between the 2 for your use case. We should look at the performance/slowness issue separately. Are you able to change the colab to be more similar to your use case so we can try to repro the convergence issue?", "@guptapriya, I strongly disagree - I'd like to focus at the missing speedup. Please note that I've bought a second GPU in order to speedup the learning. I hope you understand the disappointment that the MirroredStrategy with two GPUs is much slower than using OneDeviceStrategy with one GPU.\r\nMy example can not be ported to colab because of the complex data generation.\r\nLet us stick with MNIST example that is part of tensorflow.", "@olk There are two kinds of scaling when working with more processors: strong-scaling and weak scaling. \r\n\r\nStrong-scaling means that for a fixed **TOTAL** problem size, and in your case it means that **half** the original batch size for each of your two GPUs, and then measure your speedup. Strong-scaling does not concern the convergence rate regarding to the number of steps, because no matter how many GPUs you are using, your original problem, i.e. the model hyper parameters such as batch size, learning rate, etc. are not changed. If you are familiar with MapReduce or any other \"embarrassingly parallel\" kind of problems, they are all strong scaling problems.\r\n\r\nUnfortunately, strong-scaling does not give the best performance speed-up for deep learning models, where we usually employ \"weak-scaling\", or a fixed problem size for **EACH** processor. That, in your case, means doubling the global batch size for 2 GPUs. And since you are not dealing with the exact same problems with more GPUs, your model hyper-parameters need to be updated accordingly to compensate for the convergence rate. If the hyper-parameters are tuned well (in the case of SGD it usually means double the learning rate), you could get close to the ideal speed-up with more GPUs.\r\n\r\nAs @guptapriya said, I will suggest you to take one step at a time, and help us to investigate from the system side first, i.e. why the performance is slow in the strong-scaling case. It cannot give you the best end-to-end performance, but it isolates the root cause so we could focus on the system side and temporarily shift away from investigating how to adjust your model hyper-parameters for weak-scaling. Once we are done in the strong-scaling regime, we can surely move on and try to reach the ideal performance speed-up.", "Are the hyper params found optimal for OneDeviceStrategy optimal for MirroredStrategy too?\r\nIf not, do you have to tune hyper params for each hardware/accelerator configuration (== 2 GPUs vs. 3 GPUs etc.) using MirroredStrategy?", "Using keras-tune and CTC loss (used by my orignal example) doesn't work:\r\n```\r\nW tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: sequence_length(0) <= 12\r\n\t [[{{node OCRNet/ctc/CTCLoss}}]]\r\n   1/1000 [..............................] - ETA: 2:14:41WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: \r\nWARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\r\n...\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  sequence_length(0) <= 12\r\n\t [[node OCRNet/ctc/CTCLoss (defined at usr/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_15256]\r\n```\r\n\r\nSo I've to fallback to MNIST example.", "I've tuned the hyper parameters for Tensorflow's MNIST example with Keras Tuner.\r\nThe network was trained with the tuned hyper parameters.\r\n\r\n[Code](https://colab.research.google.com/drive/19y3BmBymeRKXV6w1BfB3B2_YwPBBnWAs#scrollTo=26yDrlokmr8n) is available in colab.\r\n\r\nMirroredStrategy (2 GPUs): \r\n\r\n> 12000/12000 [==============================] - 135s 11ms/step - loss: 0.0247 - accuracy: 0.9990 - val_loss: 1.6112 - val_accuracy: 0.9859\r\n> elapsed: 664.446s\r\n\r\nOneDeviceStrategy (1 GPU) :\r\n\r\n> 12000/12000 [==============================] - 68s 6ms/step - loss: 0.0341 - accuracy: 0.9971 - val_loss: 0.7370 - val_accuracy: 0.9819\r\n> elapsed: 334.543s\r\n\r\nEven with tuned hyper parameters MirroredStrategy is still 2 times slower than OneDeviceStrategy", "closed - use MNIST example instead", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33809\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33809\">No</a>\n", "I have the same issue! MirroredStrategy is not giving any improvement neither in speed nor accuracy. @olk what was your final conclusion about this?", "@alisaaalehi I switched to MXNet - it utilizes the GPUs much better (Keras can be used too)", "@alisaaalehi is MirroredStrategy leading to worse accuracy? Or same accuracy but with no improvement in speed? Please open an issue with your code to repro. Speed issues can be debugged with profiling tools. ", "@olk I've spent too much time on TF, not happy, but cannot switch either!\r\n", "@guptapriya with the promise of higher speed I have switched to TF2, but it is slower than my TF1 code.\r\nCompared to TF1, with small input images, my TF2 code is two times slower (when using MirroredStrategy with two GPUs). When bigger images are used as input and maximum possible batch size is used, TF2 takes about **700 seconds** while TF1 is **650 seconds** per epoch."]}]