[{"number": 42039, "title": "tf.constant causes CUDA_ERROR_TOO_MANY_PEERS error with more than 10 GPUs", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device**: No\r\n-   **TensorFlow installed from (source or binary)**: Binary\r\n-   **TensorFlow version (use command below)**: 2.2.0\r\n-   **Python version**: 3.8.2\r\n-   **Bazel version (if compiling from source)**: N/A\r\n-   **GCC/Compiler version (if compiling from source)**:  N/A\r\n-   **CUDA/cuDNN version**: 10.2.89\r\n-   **GPU model and memory**: Quadro 8000/48GB\r\n-   **NCCL version:** 2.5.7\r\n\r\n**Describe the current behavior**\r\n\r\nWhen creating a `tf.constant` on a 10x Quadro RTX 8000 server, a `CUDA_ERROR_TOO_MANY_PEERS` error is thrown even after disabling NCCL P2P.\r\n\r\n**Describe the expected behavior**\r\n\r\nCreating a `tf.constant` should not raise a `CUDA_ERROR_TOO_MANY_PEERS` error when the `NCCL_P2P_DISABLE` environment variable is set.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n$ NCCL_P2P_DISABLE=1 python3 -c 'import tensorflow as tf; a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])'\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n$ export NCCL_DEBUG=INFO NCCL_P2P_DISABLE=1\r\n$ python3\r\nPython 3.8.2 (default, Apr 27 2020, 15:53:34)\r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-07-24 06:07:34.108942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\n2020-07-24 06:07:34.110425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\n>>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n2020-07-24 06:07:44.421555: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-24 06:07:44.504448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:1a:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.46GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-07-24 06:07:44.508520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:1b:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.46GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-07-24 06:07:44.512569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties:\r\npciBusID: 0000:1c:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.46GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-07-24 06:07:44.516601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties:\r\npciBusID: 0000:1d:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.46GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-07-24 06:07:44.520621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 4 with properties:\r\npciBusID: 0000:1e:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.46GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-07-24 06:07:44.524650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 5 with properties:\r\npciBusID: 0000:3d:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.46GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-07-24 06:07:44.528674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 6 with properties:\r\npciBusID: 0000:3e:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.46GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-07-24 06:07:44.532699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 7 with properties:\r\npciBusID: 0000:3f:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.46GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-07-24 06:07:44.536720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 8 with properties:\r\npciBusID: 0000:40:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.46GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-07-24 06:07:44.540750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 9 with properties:\r\npciBusID: 0000:41:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.46GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2020-07-24 06:07:44.540781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\n2020-07-24 06:07:44.543092: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-24 06:07:44.545372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-24 06:07:44.545738: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-24 06:07:44.548233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-24 06:07:44.549673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-24 06:07:44.554615: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-24 06:07:44.611146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\r\nNum GPUs Available: 10\r\n```\r\n\r\n```\r\n>>>  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\n2020-07-24 06:07:54.494617: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n2020-07-24 06:07:54.545902: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2100000000 Hz\r\n2020-07-24 06:07:54.552981: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x30a6e40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-24 06:07:54.553039: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n2020-07-24 06:07:59.775126: W tensorflow/compiler/xla/service/platform_util.cc:210] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2020-07-24 06:07:59.778106: W tensorflow/compiler/xla/service/platform_util.cc:210] unable to create StreamExecutor for CUDA:2: failed initializing StreamExecutor for CUDA device ordinal 2: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2020-07-24 06:07:59.778824: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x306aa30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-24 06:07:59.778851: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Quadro RTX 8000, Compute Capability 7.5\r\n2020-07-24 06:07:59.778859: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (1): Quadro RTX 8000, Compute Capability 7.5\r\n2020-07-24 06:07:59.778865: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (2): Quadro RTX 8000, Compute Capability 7.5\r\n2020-07-24 06:07:59.778871: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (3): Quadro RTX 8000, Compute Capability 7.5\r\n2020-07-24 06:07:59.778877: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (4): Quadro RTX 8000, Compute Capability 7.5\r\n2020-07-24 06:07:59.778883: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (5): Quadro RTX 8000, Compute Capability 7.5\r\n2020-07-24 06:07:59.778889: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (6): Quadro RTX 8000, Compute Capability 7.5\r\n2020-07-24 06:07:59.778895: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (7): Quadro RTX 8000, Compute Capability 7.5\r\n2020-07-24 06:07:59.778926: I tensorflow/compiler/jit/xla_gpu_device.cc:161] Ignoring visible XLA_GPU_JIT device. Device number is 0, reason: Invalid argument: device CUDA:0 not supported by XLA service\r\n2020-07-24 06:07:59.779683: I tensorflow/compiler/jit/xla_gpu_device.cc:161] Ignoring visible XLA_GPU_JIT device. Device number is 2, reason: Invalid argument: device CUDA:2 not supported by XLA service\r\n2020-07-24 06:08:00.521575: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n[lambda-server:648257] *** Process received signal ***\r\n[lambda-server:648257] Signal: Aborted (6)\r\n[lambda-server:648257] Signal code: (-6)\r\n[lambda-server:648257] [ 0] /lib/x86_64-linux-gnu/libc.so.6(+0x46210)[0x7f23210b9210]\r\n[lambda-server:648257] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xcb)[0x7f23210b918b]\r\n[lambda-server:648257] [ 2] /lib/x86_64-linux-gnu/libc.so.6(abort+0x12b)[0x7f2321098859]\r\n[lambda-server:648257] [ 3] /usr/lib/python3/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.cpython-38-x86_64-linux-gnu.so(+0xb1da10b)[0x7f21ae9ba10b]\r\n[lambda-server:648257] [ 4] /usr/lib/python3/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.cpython-38-x86_64-linux-gnu.so(_ZN15stream_executor4port17internal_statusor6Helper5CrashERKN10tensorflow6StatusE+0x61)[0x7f21a631090d]\r\n[lambda-server:648257] [ 5] /usr/lib/python3/dist-packages/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow20BaseGPUDeviceFactory16EnablePeerAccessERKSt6vectorINS_3gtl7IntTypeINS_18PlatformGpuId_tag_EiEESaIS5_EE+0x33d)[0x7f219e0b518d]\r\n[lambda-server:648257] [ 6] /usr/lib/python3/dist-packages/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow20BaseGPUDeviceFactory13CreateDevicesERKNS_14SessionOptionsERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPSt6vectorISt10unique_ptrINS_6DeviceESt14default_deleteISE_EESaISH_EE+0x1840)[0x7f219e0bb0d0]\r\n[lambda-server:648257] [ 7] /usr/lib/python3/dist-packages/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPSt6vectorISt10unique_ptrINS_6DeviceESt14default_deleteISE_EESaISH_EE+0x109)[0x7f219e0fcc19]\r\n[lambda-server:648257] [ 8] /usr/lib/python3/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.cpython-38-x86_64-linux-gnu.so(TFE_NewContext+0x8d)[0x7f21a6f3febd]\r\n[lambda-server:648257] [ 9] /usr/lib/python3/dist-packages/tensorflow/python/_pywrap_tfe.cpython-38-x86_64-linux-gnu.so(+0x3c529)[0x7f219c3ff529]\r\n[lambda-server:648257] [10] /usr/lib/python3/dist-packages/tensorflow/python/_pywrap_tfe.cpython-38-x86_64-linux-gnu.so(+0x53d3b)[0x7f219c416d3b]\r\n[lambda-server:648257] [11] python3(PyCFunction_Call+0x55)[0x5f1625]\r\n[lambda-server:648257] [12] python3(_PyObject_MakeTpCall+0x296)[0x5f2246]\r\n[lambda-server:648257] [13] python3(_PyEval_EvalFrameDefault+0x5c0f)[0x56c70f]\r\n[lambda-server:648257] [14] python3(_PyFunction_Vectorcall+0x1ab)[0x5f19cb]\r\n[lambda-server:648257] [15] python3(_PyEval_EvalFrameDefault+0x825)[0x567325]\r\n[lambda-server:648257] [16] python3(_PyEval_EvalCodeWithName+0x262)[0x5654d2]\r\n[lambda-server:648257] [17] python3(_PyFunction_Vectorcall+0x3a5)[0x5f1bc5]\r\n[lambda-server:648257] [18] python3(_PyEval_EvalFrameDefault+0x6fd)[0x5671fd]\r\n[lambda-server:648257] [19] python3(_PyEval_EvalCodeWithName+0x262)[0x5654d2]\r\n[lambda-server:648257] [20] python3(_PyFunction_Vectorcall+0x3a5)[0x5f1bc5]\r\n[lambda-server:648257] [21] python3(_PyEval_EvalFrameDefault+0x1904)[0x568404]\r\n[lambda-server:648257] [22] python3(_PyEval_EvalCodeWithName+0x262)[0x5654d2]\r\n[lambda-server:648257] [23] python3(_PyFunction_Vectorcall+0x3a5)[0x5f1bc5]\r\n[lambda-server:648257] [24] python3(_PyEval_EvalFrameDefault+0x5556)[0x56c056]\r\n[lambda-server:648257] [25] python3(_PyEval_EvalCodeWithName+0x262)[0x5654d2]\r\n[lambda-server:648257] [26] python3(PyEval_EvalCode+0x23)[0x686d53]\r\n[lambda-server:648257] [27] python3[0x676101]\r\n[lambda-server:648257] [28] python3[0x67617f]\r\n[lambda-server:648257] [29] python3[0x4a032d]\r\n[lambda-server:648257] *** End of error message ***\r\nAborted (core dumped)\r\n```", "comments": ["This is originally discussed [here](https://github.com/NVIDIA/nccl/issues/356). But apparently not a NCCL problem.", "@chuanli11 It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42039\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42039\">No</a>\n"]}, {"number": 42038, "title": "ValueError: No gradients provided for any variable (Keras 2.4, Tensorflow 2.3.0)", "body": "So I'm using [this model](https://github.com/ekohendratno/Screenshot-to-code-in-Keras/blob/master/floydhub/Bootstrap/bootstrap_generator.ipynb) to train on Google colab, it was written for Tensorflow 1.9 and Keras 2, but when I train I get the following error, has anyone seen this or how to solve it?\r\n\r\nIt was training fine before but this error started today.\r\n\r\nActual code:\r\n\r\n```\r\nfrom os import listdir\r\nfrom numpy import array\r\nfrom keras.preprocessing.text import Tokenizer, one_hot\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nfrom keras.models import Model, Sequential, model_from_json\r\nfrom keras.utils import to_categorical\r\nfrom keras.layers.core import Dense, Dropout, Flatten\r\nfrom keras.optimizers import RMSprop\r\nfrom keras.layers.convolutional import Conv2D\r\nfrom keras.callbacks import ModelCheckpoint\r\nfrom keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM,     concatenate , Input, Reshape, Dense\r\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img\r\nimport numpy as np\r\n\r\ndir_name = '/data/train/'\r\n\r\n# Read a file and return a string\r\ndef load_doc(filename):\r\nfile = open(filename, 'r')\r\ntext = file.read()\r\nfile.close()\r\nreturn text\r\n\r\ndef load_data(data_dir):\r\ntext = []\r\nimages = []\r\n# Load all the files and order them\r\nall_filenames = listdir(data_dir)\r\nall_filenames.sort()\r\nfor filename in (all_filenames):\r\n    if filename[-3:] == \"npz\":\r\n        # Load the images already prepared in arrays\r\n        image = np.load(data_dir+filename)\r\n        images.append(image['features'])\r\n    else:\r\n        # Load the boostrap tokens and rap them in a start and end tag\r\n        syntax = '<START> ' + load_doc(data_dir+filename) + ' <END>'\r\n        # Seperate all the words with a single space\r\n        syntax = ' '.join(syntax.split())\r\n        # Add a space after each comma\r\n        syntax = syntax.replace(',', ' ,')\r\n        text.append(syntax)\r\nimages = np.array(images, dtype=float)\r\nreturn images, text\r\n\r\ntrain_features, texts = load_data(dir_name)\r\n\r\n# Initialize the function to create the vocabulary \r\ntokenizer = Tokenizer(filters='', split=\" \", lower=False)\r\n# Create the vocabulary \r\ntokenizer.fit_on_texts([load_doc('bootstrap.vocab')])\r\n\r\n# Add one spot for the empty word in the vocabulary \r\nvocab_size = len(tokenizer.word_index) + 1\r\nmax_length = 48\r\n\r\ndef preprocess_data(texts, features, max_sequence):\r\nX, y, image_data = list(), list(), list()\r\nsequences = tokenizer.texts_to_sequences(texts)\r\nfor img_no, seq in enumerate(sequences):\r\n    for i in range(1, len(seq)):\r\n        # Add the sentence until the current count(i) and add the current     count to the output\r\n        in_seq, out_seq = seq[:i], seq[i]\r\n        # Pad all the input token sentences to max_sequence\r\n        in_seq = pad_sequences([in_seq], maxlen=max_sequence)[0]\r\n        # Turn the output into one-hot encoding\r\n        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\r\n        # Add the corresponding image to the boostrap token file\r\n        image_data.append(features[img_no])\r\n        # Cap the input sentence to 48 tokens and add it\r\n        X.append(in_seq[-48:])\r\n        y.append(out_seq)\r\nreturn np.array(image_data), np.array(X), np.array(y)\r\n\r\n\r\n# data generator, intended to be used in a call to model.fit_generator()\r\ndef data_generator(descriptions, features, n_step, max_sequence):\r\n# loop until we finish training\r\nwhile 1:\r\n    # loop over photo identifiers in the dataset\r\n    for i in range(0, len(descriptions), n_step):\r\n        Ximages, XSeq, y = list(), list(),list()\r\n        for j in range(i, min(len(descriptions), i+n_step)):\r\n            image = features[j]\r\n            # retrieve text input\r\n            desc = descriptions[j]\r\n            # generate input-output pairs\r\n            in_img, in_seq, out_word = preprocess_data([desc], [image], max_sequence)\r\n            for k in range(len(in_img)):\r\n                Ximages.append(in_img[k])\r\n                XSeq.append(in_seq[k])\r\n                y.append(out_word[k])\r\n        # yield this batch of samples to the model\r\n        yield [[array(Ximages), array(XSeq)], array(y)]\r\n\r\n#Create the encoder\r\nimage_model = Sequential()\r\nimage_model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3,)))\r\nimage_model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\r\nimage_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\r\nimage_model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\r\nimage_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\r\nimage_model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\r\nimage_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\r\n\r\nimage_model.add(Flatten())\r\nimage_model.add(Dense(1024, activation='relu'))\r\nimage_model.add(Dropout(0.3))\r\nimage_model.add(Dense(1024, activation='relu'))\r\nimage_model.add(Dropout(0.3))\r\n\r\nimage_model.add(RepeatVector(max_length))\r\n\r\nvisual_input = Input(shape=(256, 256, 3,))\r\nencoded_image = image_model(visual_input)\r\n\r\nlanguage_input = Input(shape=(max_length,))\r\nlanguage_model = Embedding(vocab_size, 50, input_length=max_length,    mask_zero=True)(language_input)\r\nlanguage_model = LSTM(128, return_sequences=True)(language_model)\r\nlanguage_model = LSTM(128, return_sequences=True)(language_model)\r\n\r\n#Create the decoder\r\ndecoder = concatenate([encoded_image, language_model])\r\ndecoder = LSTM(512, return_sequences=True)(decoder)\r\ndecoder = LSTM(512, return_sequences=False)(decoder)\r\ndecoder = Dense(vocab_size, activation='softmax')(decoder)\r\n\r\n# Compile the model\r\nmodel = Model(inputs=[visual_input, language_input], outputs=decoder)\r\noptimizer = RMSprop(lr=0.0001, clipvalue=1.0)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\r\n\r\n#Save the model for every 2nd epoch\r\nfilepath=\"org-weights-epoch-{epoch:04d}--loss-{loss:.4f}.hdf5\"\r\ncheckpoint = ModelCheckpoint(filepath, verbose=1, save_weights_only=True,     period=2)\r\ncallbacks_list = [checkpoint]\r\n\r\n# test the data generator\r\ngenerator = data_generator(texts, train_features, 1, 150)\r\nmodel.fit_generator(generator, steps_per_epoch=50, epochs=5, callbacks=callbacks_list, verbose=1)\r\n\r\n```\r\n\r\nError I receive when training:\r\n```\r\nValueError                                \r\nTraceback (most recent call last)\r\n<ipython-input-4-6927891f43ca> in <module>()\r\n  1 # test the data generator\r\n  2 generator = data_generator(texts, train_features, 1, max_sequence)\r\n----> 3 loaded_model.fit_generator(generator, steps_per_epoch=steps, epochs=5, callbacks=callbacks_list, verbose=1)\r\n  4 loaded_model.save(mydrive + '/output/weights.hdf5')\r\n\r\n12 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n971           except Exception as e:  # pylint:disable=broad-except\r\n972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n974             else:\r\n975               raise\r\n\r\nValueError: in user code:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n    return step_function(self, iterator)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\r\n    outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\r\n    outputs = model.train_step(data)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step\r\n    self.trainable_variables)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2737 _minimize\r\n    trainable_variables))\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:562 _aggregate_gradients\r\n    filtered_grads_and_vars = _filter_grads(grads_and_vars)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1271 _filter_grads\r\n    ([v.name for _, v in grads_and_vars],))\r\n\r\nValueError: No gradients provided for any variable: ['embedding_1/embeddings:0', 'lstm_1/lstm_cell/kernel:0', 'lstm_1/lstm_cell/recurrent_kernel:0', 'lstm_1/lstm_cell/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0', 'conv2d_6/kernel:0', 'conv2d_6/bias:0', 'conv2d_7/kernel:0', 'conv2d_7/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'lstm_2/lstm_cell_1/kernel:0', 'lstm_2/lstm_cell_1/recurrent_kernel:0', 'lstm_2/lstm_cell_1/bias:0', 'lstm_3/lstm_cell_2/kernel:0', 'lstm_3/lstm_cell_2/recurrent_kernel:0', 'lstm_3/lstm_cell_2/bias:0', 'lstm_4/lstm_cell_3/kernel:0', 'lstm_4/lstm_cell_3/recurrent_kernel:0', 'lstm_4/lstm_cell_3/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0'].\r\n```\r\nI'm training it on Google Colab using the TPU. If I train it on Tensorflow 1.x it trains fine but takes 8 hours per epoch with my dataset. Tensorflow 2.x was taking 1 hour per epoch but is now giving this error\r\n\r\nEDIT: SOLUTION \r\n\r\nI cannot train on TPU, but I am at least able to train on GPU, I can continue the project !\r\nSolution by @silentkinght25 and @silentkinght25 solves it. \r\n\r\n\"To resolve this u must modify the data generator function as: yield ([array(Ximages), array(XSeq)], array(y)) instead of yield [[array(Ximages), array(XSeq)], array(y)]\"", "comments": ["@ScruffySilky,\r\nOn running the code, I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory: '/data/train/'`. \r\n\r\nCould you please provide all the necessary files required to run the script. Thanks!", "Sure, here are the locations of the two files needed to run it: \r\n1. '/data/train/' = https://github.com/ekohendratno/Screenshot-to-code-in-Keras/tree/master/local/Bootstrap/resources/eval_light\r\n2. 'bootstrap.vocab' = https://github.com/ekohendratno/Screenshot-to-code-in-Keras/blob/master/local/Bootstrap/resources/bootstrap.vocab", "I'm 100% sure it has to do with my data_generator because running the belolw  example works fine It just cant fit the data into memory https://github.com/ekohendratno/Screenshot-to-code-in-Keras/blob/master/floydhub/Bootstrap/bootstrap.ipynb ", "@ScruffySilky,\r\nI was able to run the code without any issues on TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/39db0b14bce096a12d6f4c9961f687de/42038.ipynb). Thanks!", "> @ScruffySilky,\r\n> I was able to run the code without any issues on TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/39db0b14bce096a12d6f4c9961f687de/42038.ipynb). Thanks!\r\n\r\nThe error occurs on the model.fit.generator() function:\r\n```\r\ngenerator = data_generator(texts, train_features, 1, 150)\r\nmodel.fit_generator(generator, steps_per_epoch=50, epochs=5, callbacks=callbacks_list, verbose=1)\r\n```\r\n", "Having the same issue. Using model.fit with just numpy arrays works (with saved data using `numpy.save`), but I was trying to use the new saving/loading dataset features, and I'm getting this error message in my `model.fit_generator(...)`. \r\n\r\nEDIT:\r\nOkay well it was because my generator wasn't yielding a tuple with (x, y), instead just a single dictionary with all my named keys. Now I'm yielding `{ 'input1': ..., 'input2': ...}, label` according to the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit_generator)", "> Having the same issue. Using model.fit with just numpy arrays works (with saved data using `numpy.save`), but I was trying to use the new saving/loading dataset features, and I'm getting this error message in my `model.fit_generator(...)`.\r\n> \r\n> EDIT:\r\n> Okay well it was because my generator wasn't yielding a tuple with (x, y), instead just a single dictionary with all my named keys. Now I'm yielding `{ 'input1': ..., 'input2': ...}, label` according to the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit_generator)\r\n\r\nAny suggestion for my code?", "Try changing the yield part in `data_generator` from:\r\n\r\n```\r\n# yield this batch of samples to the model\r\n        yield [[array(Ximages), array(XSeq)], array(y)]\r\n```\r\n\r\nto just:\r\n\r\n```\r\n# yield this batch of samples to the model\r\n        yield [array(Ximages), array(XSeq)], array(y)\r\n```\r\n\r\nSince the documentation expects a tuple `(x, y)` to be returned from your generator. ", "> Try changing the yield part in `data_generator` from:\r\n> \r\n> ```\r\n> # yield this batch of samples to the model\r\n>         yield [[array(Ximages), array(XSeq)], array(y)]\r\n> ```\r\n> \r\n> to just:\r\n> \r\n> ```\r\n> # yield this batch of samples to the model\r\n>         yield [array(Ximages), array(XSeq)], array(y)\r\n> ```\r\n> \r\n> Since the documentation expects a tuple `(x, y)` to be returned from your generator.\r\n\r\nChanging that gives this error:\r\n\r\n```\r\n`UnavailableError                          Traceback (most recent call last)\r\n<ipython-input-5-38870834c9d8> in <module>()\r\n      1 # test the data generator\r\n      2 generator = data_generator(texts, train_features, 1, max_sequence)\r\n----> 3 model.fit_generator(generator, steps_per_epoch=steps, epochs=5, callbacks=callbacks_list, verbose=1)\r\n      4 model.save(mydrive + '/output/weights.hdf5')\r\n\r\n16 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnavailableError: {{function_node __inference_train_function_28401}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1596748432.082355881\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3948,\"referenced_errors\":[{\"created\":\"@1596748432.082354141\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node IteratorGetNext}}]]`\r\n```", "> The error occurs on the model.fit.generator() function:\r\n> \r\n> generator = data_generator(texts, train_features, 1, 150)\r\n> model.fit_generator(generator, steps_per_epoch=50, epochs=5, callbacks=callbacks_list, verbose=1)\r\n\r\n@ScruffySilky,\r\nCould you please share the code you are running, so that we can look into it. Thanks!", "> > The error occurs on the model.fit.generator() function:\r\n> > generator = data_generator(texts, train_features, 1, 150)\r\n> > model.fit_generator(generator, steps_per_epoch=50, epochs=5, callbacks=callbacks_list, verbose=1)\r\n> \r\n> @ScruffySilky,\r\n> Could you please share the code you are running, so that we can look into it. Thanks!\r\n\r\n```\r\nimport os\r\nfrom os import listdir\r\nfrom numpy import array\r\nfrom keras.preprocessing.text import Tokenizer, one_hot\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nfrom keras.models import Model, load_model, Sequential, model_from_json\r\nfrom keras.utils import to_categorical\r\nfrom keras.layers.core import Dense, Dropout, Flatten\r\nfrom keras.optimizers import RMSprop\r\nfrom keras.layers.convolutional import Conv2D\r\nfrom keras.callbacks import ModelCheckpoint\r\nfrom keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_cluster(resolver)\r\n# This is the TPU initialization code that has to be at the beginning.\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\nfrom google.colab import drive\r\ndrive.mount('/content/gdrive')\r\n\r\n# !ls \"/content/gdrive/My Drive/weights\"\r\nmydrive = \"/content/gdrive/My Drive\"\r\ndir_name = mydrive+\"/data/\" \r\n# ( https://github.com/ekohendratno/Screenshot-to-code-in-Keras/tree/master/local/Bootstrap/resources/eval_light)\r\nbootstrap_vocab = mydrive+\"/weights/bootstrap.vocab\" \r\n# (https://github.com/ekohendratno/Screenshot-to-code-in-Keras/blob/master/local/Bootstrap/resources/bootstrap.vocab)\r\n\r\n# Read a file and return a string\r\ndef load_doc(filename):\r\n    file = open(filename, 'r')\r\n    text = file.read()\r\n    file.close()\r\n    return text\r\n\r\ndef load_data(data_dir):\r\n    text = []\r\n    images = []\r\n    # Load all the files and order them\r\n    all_filenames = listdir(data_dir)\r\n    all_filenames.sort()\r\n    for filename in (all_filenames):\r\n        if filename[-3:] == \"npz\":\r\n            # Load the images already prepared in arrays\r\n            image = np.load(data_dir+filename)\r\n            images.append(image['features'])\r\n        else:\r\n            # Load the boostrap tokens and rap them in a start and end tag\r\n            syntax = '<START> ' + load_doc(data_dir+filename) + ' <END>'\r\n            # Seperate all the words with a single space\r\n            syntax = ' '.join(syntax.split())\r\n            # Add a space after each comma\r\n            syntax = syntax.replace(',', ' ,')\r\n            text.append(syntax)\r\n    images = np.array(images, dtype=float)\r\n    return images, text\r\n\r\ntrain_features, texts = load_data(dir_name)\r\n\r\n# Initialize the function to create the vocabulary \r\ntokenizer = Tokenizer(filters='', split=\" \", lower=False)\r\n# Create the vocabulary \r\ntokenizer.fit_on_texts([load_doc(bootstrap_vocab)])\r\n\r\n# Add one spot for the empty word in the vocabulary \r\nvocab_size = len(tokenizer.word_index) + 1\r\nmax_length = 48\r\nmax_sequence = (max(len(d.split()) for d in texts))\r\n\r\ndef preprocess_data(texts, features, max_sequence):\r\n    X, y, image_data = list(), list(), list()\r\n    sequences = tokenizer.texts_to_sequences(texts)\r\n    for img_no, seq in enumerate(sequences):\r\n        for i in range(1, len(seq)):\r\n            # Add the sentence until the current count(i) and add the current count to the output\r\n            in_seq, out_seq = seq[:i], seq[i]\r\n            # Pad all the input token sentences to max_sequence\r\n            in_seq = pad_sequences([in_seq], maxlen=max_sequence)[0]\r\n            # Turn the output into one-hot encoding\r\n            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\r\n            # Add the corresponding image to the boostrap token file\r\n            image_data.append(features[img_no])\r\n            # Cap the input sentence to 48 tokens and add it\r\n            X.append(in_seq[-48:])\r\n            y.append(out_seq)\r\n    return np.array(image_data), np.array(X), np.array(y)\r\n\r\n# data generator, intended to be used in a call to model.fit_generator()\r\ndef data_generator(descriptions, features, n_step, max_sequence):\r\n    # loop until we finish training\r\n    while 1:\r\n        # loop over photo identifiers in the dataset\r\n        for i in range(0, len(descriptions), n_step):\r\n            Ximages, XSeq, y = list(), list(),list()\r\n            for j in range(i, min(len(descriptions), i+n_step)):\r\n                image = features[j]\r\n                # retrieve text input\r\n                desc = descriptions[j]\r\n                # generate input-output pairs\r\n                in_img, in_seq, out_word = preprocess_data([desc], [image], max_sequence)\r\n                for k in range(len(in_img)):\r\n                    Ximages.append(in_img[k])\r\n                    XSeq.append(in_seq[k])\r\n                    y.append(out_word[k])\r\n            # yield this batch of samples to the model\r\n            yield [[array(Ximages), array(XSeq)], array(y)]\r\n\r\n#Create the encoder\r\nimage_model = Sequential()\r\nimage_model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3,)))\r\nimage_model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\r\nimage_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\r\nimage_model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\r\nimage_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\r\nimage_model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\r\nimage_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\r\n\r\nimage_model.add(Flatten())\r\nimage_model.add(Dense(1024, activation='relu'))\r\nimage_model.add(Dropout(0.3))\r\nimage_model.add(Dense(1024, activation='relu'))\r\nimage_model.add(Dropout(0.3))\r\n\r\nimage_model.add(RepeatVector(max_length))\r\n\r\nvisual_input = Input(shape=(256, 256, 3,))\r\nencoded_image = image_model(visual_input)\r\n\r\nlanguage_input = Input(shape=(max_length,))\r\nlanguage_model = Embedding(vocab_size, 50, input_length=max_length, mask_zero=True)(language_input)\r\nlanguage_model = LSTM(128, return_sequences=True)(language_model)\r\nlanguage_model = LSTM(128, return_sequences=True)(language_model)\r\n\r\n#Create the decoder\r\ndecoder = concatenate([encoded_image, language_model])\r\ndecoder = LSTM(512, return_sequences=True)(decoder)\r\ndecoder = LSTM(512, return_sequences=False)(decoder)\r\ndecoder = Dense(vocab_size, activation='softmax')(decoder)\r\n\r\n# Compile the model\r\nmodel = Model(inputs=[visual_input, language_input], outputs=decoder)\r\noptimizer = RMSprop(lr=0.0001, clipvalue=1.0)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\r\n\r\n#Save the model for every 2nd epoch\r\nfilepath=\"weights.hdf5\"\r\ncheckpoint = ModelCheckpoint(filepath, verbose=1, period=1)\r\ncallbacks_list = [checkpoint]\r\n\r\nsteps = len(texts)\r\nprint('steps: ', steps)\r\nprint('max_sequence: ', max_sequence)\r\n\r\nmodel.fit_generator(data_generator(texts, train_features, 1, max_sequence), steps_per_epoch=steps, epochs=1, callbacks=callbacks_list, verbose=1)\r\nmodel.save(mydrive + '/output/weights.hdf5')\r\n```", "Was able to reproduce the issue with TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/24d62cd4ea682b1d2e87fc1eca0fc266/42038.ipynb). Thanks!", "So you guys upgrade your back-end and I as a paying customer must just forget my project because it doesn't work on your new platform. Is there any resolution to this...", "Is there any planned solution for this bug ? ", "I had same problem  @ScruffySilky  but i resolved it. The problem is in data generator function, because model.fit(generator,....) expect a list of tuple or a dict of tuple. To resolve this u must modify the data generator function as: **yield ([array(Ximages), array(XSeq)], array(y))** instead of **yield [[array(Ximages), array(XSeq)], array(y)]**", "> I had same problem @ScruffySilky but i resolved it. The problem is in data generator function, because model.fit(generator,....) expect a list of tuple or a dict of tuple. To resolve this u must modify the data generator function as: **yield ([array(Ximages), array(XSeq)], array(y))** instead of **yield [[array(Ximages), array(XSeq)], array(y)]**\r\n\r\nI hope this resolves ur issue. ", "Ok I cannot train on TPU, but I am able to train on GPU.\r\nSolution by @silentkinght25 and @silentkinght25 seems to solve it. \r\n\r\n\"To resolve this u must modify the data generator function as: yield ([array(Ximages), array(XSeq)], array(y)) instead of yield [[array(Ximages), array(XSeq)], array(y)]\"", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42038\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42038\">No</a>\n", "> > I had same problem @ScruffySilky but i resolved it. The problem is in data generator function, because model.fit(generator,....) expect a list of tuple or a dict of tuple. To resolve this u must modify the data generator function as: **yield ([array(Ximages), array(XSeq)], array(y))** instead of **yield [[array(Ximages), array(XSeq)], array(y)]**\r\n> \r\n> I hope this resolves ur issue.\r\n\r\nIm still getting the error \r\n here is my code\r\n\r\n```\r\nfrom keras.preprocessing.text import Tokenizer\r\nimport numpy as np\r\n\r\ndef to_lines(descriptions):\r\n\tall_desc = list()\r\n\tfor key in descriptions.keys():\r\n\t\t[all_desc.append(d) for d in descriptions[key]]\r\n     return all_desc\r\ndef create_tokenizer(descriptions):\r\n\tlines = to_lines(descriptions)\r\n\ttokenizer = Tokenizer()\r\n\ttokenizer.fit_on_texts(lines)\r\n\treturn tokenizer\r\n\r\ntokenizer = create_tokenizer(train_descriptions)\r\n\r\nvocab_size = len(tokenizer.word_index) + 1\r\nprint('Vocabulary Size: %d' % vocab_size)\r\n\r\ndef max_length(descriptions):\r\n\tlines = to_lines(descriptions)\r\n\treturn max(len(d.split()) for d in lines)\r\n\r\ndef create_sequences(tokenizer, max_length, desc_list, photo):\r\n\tX1, X2, y = list(), list(), list()\r\n\tfor desc in desc_list:\r\n\t\tseq = tokenizer.texts_to_sequences([desc])[0]\r\n\t\tfor i in range(1, len(seq)):\r\n\t\t\tin_seq, out_seq = seq[:i], seq[i]\r\n\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\r\n\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\r\n\t\t\tX1.append(photo)\r\n\t\t\tX2.append(in_seq)\r\n\t\t\ty.append(out_seq)\r\n\treturn np.array(X1), np.array(X2), np.array(y)\r\ndef data_generator(descriptions, photos, tokenizer, max_length):\r\n    while 1:\r\n      for key, desc_list in descriptions.items():\r\n        # retrieve the photo feature\r\n        photo = photos[key][0]\r\n        in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\r\n        yield ([array(in_img), array(in_seq)], array(out_word))\r\n\r\nmodel = define_model(vocab_size, max_length)\r\nepochs = 20\r\nsteps = len(train_descriptions)\r\nfor i in range(epochs):\r\n\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length)\r\n\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\r\n\tmodel.save('model_' + str(i) + '.h5')\r\n\r\n`\r\n\r\nand the error is\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-267-102bfe3f0b2a> in <module>()\r\n      8         generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\r\n      9        ---> 10         model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\r\n     11         model.save('model_' + str(i) + '.h5')\r\n\r\n```\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Matrix size-incompatible: In[0]: [47,1000], In[1]: [4096,256]\r\n\t [[node functional_67/dense_97/MatMul (defined at <ipython-input-267-102bfe3f0b2a>:10) ]]\r\n\t [[gradient_tape/functional_67/embedding_32/embedding_lookup/Reshape_1/_30]]\r\n  (1) Invalid argument:  Matrix size-incompatible: In[0]: [47,1000], In[1]: [4096,256]\r\n\t [[node functional_67/dense_97/MatMul (defined at <ipython-input-267-102bfe3f0b2a>:10) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_610379]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n\r\n**Please help**\r\nthis is my code plz help\r\n[https://colab.research.google.com/drive/145K_SfItvudtSnTNZKreoKOs8GEYjlsF?usp=sharing](https://colab.research.google.com/drive/145K_SfItvudtSnTNZKreoKOs8GEYjlsF?usp=sharing)", "@kartiksonaghela,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 42037, "title": "Random addition of channel during training when input data is all 1 channel", "body": "**System information**\r\nHave I written custom code [posted below]:\r\nDone on google colab using TF 2.x\r\n\r\n\r\nCurrently during training, somehow my training data gets a surprising second channel. I think this has something to do with datatype changing that happens during the fit function, but is not resolved when forcing dtype prior to entering training. This messes with the concatenation of my Dense-Net like structures. Looking at the warning below, I don't understand how its being called with a shape of  512,512, 1, 1, as the overall shape of image_list is 1540, 512,512, 1. Where each image slice is 512, 512, 1. (HWC)\r\n\r\nTraceback is:\r\n```\r\nWARNING:tensorflow:Model was constructed with shape (None, 512, 512, 1) for input Tensor(\"input_2:0\", shape=(None, 512, 512, 1), dtype=float32), but it was called on an input with incompatible shape (512, 512, 1, 1).\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-17-b77dae276c25> in <module>()\r\n      1 \r\n----> 2 model.fit(dataset, epochs=1 )\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1096                 batch_size=batch_size):\r\n   1097               callbacks.on_train_batch_begin(step)\r\n-> 1098               tmp_logs = train_function(iterator)\r\n   1099               if data_handler.should_sync:\r\n   1100                 context.async_wait()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    821       # This is the first call of __call__, so we have to initialize.\r\n    822       initializers = []\r\n--> 823       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    824     finally:\r\n    825       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    695     self._concrete_stateful_fn = (\r\n    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 697             *args, **kwds))\r\n    698 \r\n    699     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3073             arg_names=arg_names,\r\n   3074             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3075             capture_by_value=self._capture_by_value),\r\n   3076         self._function_attributes,\r\n   3077         function_spec=self.function_spec,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n    972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n    974             else:\r\n    975               raise\r\n\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:747 train_step\r\n        y_pred = self(x, training=True)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:386 call\r\n        inputs, training=training, mask=mask)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\r\n        outputs = node.layer(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/merge.py:183 call\r\n        return self._merge_function(inputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/merge.py:522 _merge_function\r\n        return K.concatenate(inputs, axis=self.axis)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:2881 concatenate\r\n        return array_ops.concat([to_dense(x) for x in tensors], axis)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1654 concat\r\n        return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py:1222 concat_v2\r\n        \"ConcatV2\", values=values, axis=axis, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\r\n        attrs=attr_protos, op_def=op_def)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\r\n        compute_device)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\r\n        op_def=op_def)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1975 __init__\r\n        control_input_ops, op_def)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: Dimension 2 in both shapes must be equal, but are 2 and 1. Shapes are [512,32,2] and [512,32,1]. for '{{node functional_3/concatenate_60/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](functional_3/leaky_re_lu_84/LeakyRelu, functional_3/concatenate_55/concat, functional_3/concatenate_60/concat/axis)' with input shapes: [512,32,2,86], [512,32,1,172], [] and with computed input tensors: input[2] = <3>.\r\n```\r\n\r\nLink to colab: https://colab.research.google.com/drive/1FwuS2Wa589CvbiqOgAznqqnbKruez9Nj?usp=sharing\r\n\r\nlink to sample dataset: <removed>", "comments": ["@zhilothebest \r\nI ran the code shared on colab for 2.2 as shared ans 2.3 and do not face any error, please find [gist here](https://colab.research.google.com/gist/Saduf2019/1d5726ea6a790c644ebeb9cd9feefd5a/untitled328.ipynb).", "With respect to the error shared, please refer to below links:\r\n[link](https://stackoverflow.com/questions/52365277/valueerror-dimension-2-in-both-shapes-must-be-equal-but-are-3-and-32 ) \r\n [link1](https://github.com/titu1994/Image-Super-Resolution/issues/27)", "@Saduf2019 ahh I hadn't responded to this and closed it when I figured out the solution last night. My apologies. I was able to fix my problem by using:`tf.convert_to_tensor()` on my image list and mask list. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42037\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42037\">No</a>\n", "> @Saduf2019 ahh I hadn't responded to this and closed it when I figured out the solution last night. My apologies. I was able to fix my problem by using:`tf.convert_to_tensor()` on my image list and mask list.\r\n\r\n**THIS IS A LIFE SAVER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!** ", "> @Saduf2019 ahh I hadn't responded to this and closed it when I figured out the solution last night. My apologies. I was able to fix my problem by using:`tf.convert_to_tensor()` on my image list and mask list.\r\n\r\ncan u please elaborate by giving the exact statement\r\n", "I was having the same error but  the problem I had was regarding me using a wrong class mode.\r\nIt Worked after I've changed to **binary**:\r\n\r\n`train_it = datagen_train.flow_from_directory(train_directory, class_mode='binary', ..)`", "Sometimes this problem arises in the colab/notebook when you run the tesnordataset.batch() cell twice. as tensors are already batched; running the same cell again batches it again and ends up adding an extra dimension. just clear the variable and you will be fine", "hello sir \r\ni new gys in deep learning ", "i have the same problem \r\n ValueError: Dimension 2 in both shapes must be equal, but are 2 and 1. Shapes are [256,128,2] and [256,128,1]. for '{{node model/concatenate/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](model/conv2d_transpose/BiasAdd, model/conv2d_3/Relu, model/concatenate/concat/axis)' with input shapes: [256,128,2,32], [256,128,1,32], [] and with computed input tensors: input[2] = <3>.\r\n\r\ni tried to solve it with tf.convert_to_tensor() but nothing is changed, what should I do to tackle this problem? please I need help ", "concatenate on axis=2 as the third is unequal. 1 and 2. So the resultant\nshape would be (x,x,3)\n\nOn Sun, 28 Nov 2021, 4:39 pm ayoubk34, ***@***.***> wrote:\n\n> i have the same problem\n> ValueError: Dimension 2 in both shapes must be equal, but are 2 and 1.\n> Shapes are [256,128,2] and [256,128,1]. for '{{node\n> model/concatenate/concat}} = ConcatV2[N=2, T=DT_FLOAT,\n> Tidx=DT_INT32](model/conv2d_transpose/BiasAdd, model/conv2d_3/Relu,\n> model/concatenate/concat/axis)' with input shapes: [256,128,2,32],\n> [256,128,1,32], [] and with computed input tensors: input[2] = <3>.\n>\n> i tried to solve it with tf.convert_to_tensor() but nothing is changed,\n> what should I do to tackle this problem? please I need help\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42037#issuecomment-981070062>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALTHGS63CIQH63JIHYDX25LUOIIIXANCNFSM4PUVA75Q>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "256, 128, 3, 32\nOn Sun, 28 Nov 2021, 5:26 pm Murad Mansoor, ***@***.***>\nwrote:\n\n> concatenate on axis=2 as the third is unequal. 1 and 2. So the resultant\n> shape would be (x,x,3)\n>\n> On Sun, 28 Nov 2021, 4:39 pm ayoubk34, ***@***.***> wrote:\n>\n>> i have the same problem\n>> ValueError: Dimension 2 in both shapes must be equal, but are 2 and 1.\n>> Shapes are [256,128,2] and [256,128,1]. for '{{node\n>> model/concatenate/concat}} = ConcatV2[N=2, T=DT_FLOAT,\n>> Tidx=DT_INT32](model/conv2d_transpose/BiasAdd, model/conv2d_3/Relu,\n>> model/concatenate/concat/axis)' with input shapes: [256,128,2,32],\n>> [256,128,1,32], [] and with computed input tensors: input[2] = <3>.\n>>\n>> i tried to solve it with tf.convert_to_tensor() but nothing is changed,\n>> what should I do to tackle this problem? please I need help\n>>\n>> \u2014\n>> You are receiving this because you commented.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/42037#issuecomment-981070062>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ALTHGS63CIQH63JIHYDX25LUOIIIXANCNFSM4PUVA75Q>\n>> .\n>> Triage notifications on the go with GitHub Mobile for iOS\n>> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n>> or Android\n>> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>>\n>>\n>\n"]}, {"number": 42036, "title": "[TF:MLIR] Legalize tf.diag with tf2xla", "body": "Legalize `tf.diag` with tf2xla as `MlirHloBuilder::Iota` is available.", "comments": ["Can you add an IR test under tensorflow/compiler/mlir/test?\r\nI'm also curious if this pattern is complex or if it can be supported by a native MLIR lowering.", "Test is here FYI: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/xla/tests/legalize-tf-with-tf2xla.mlir", "Hi @joker-eph, I've updated the test. I'm also curious if there is any standard when talking about complicated IR legalization (maybe in terms of number of IRs generated)? Like in this case, the legalized IR is composed of less than 10 mhlo ops, is that complex or not? Thanks!", "There isn't any objective standard to describe complexity of a legalization pattern. It could be the number of ops generated, lines of code for the pattern and test etc.\r\n\r\nIf you could implement rewrite patterns with reasonable effort natively without using the fallback kernel, that should be preferred.", "@WindQAQ  Can you please fix build failures ? Thanks!"]}, {"number": 42034, "title": "Add clustering to TF Model Optimization overview", "body": "Small change to the overview document to reflect clustering as an option for model optimization.", "comments": ["@suharshs Could you review this doc update?", "Contents look good. I'll wait until Monday when the launch occurs prior to LGTM'ing this.", "LGTMd. @MeghnaNatraj could you review as someone from the TFLite side?", "LGTM as well. @gbaned you can go ahead and approve these changes. \r\n\r\n@TamasArm thank you for updating the docs! :)"]}, {"number": 42032, "title": "Fix the usage of uninitialized variable in adaptive_shared_batch_scheduler and windows build error.", "body": "Same as #41745 https://github.com/tensorflow/tensorflow/pull/41745\r\nThe variable best_score is uninitialized, but the old PR was reverted because it broke Windows Build.\r\nThe new PR fixs the build problem.", "comments": ["@kitstar  Can you please check @jaingaurav's comments and keep us posted ? Thanks!\r\n"]}, {"number": 42031, "title": "Enable depthwise convs in auto_mixed_precision", "body": "These are well-supported as of CUDNN v8.\r\nAlso adds a Python test.\r\n\r\ncc @nluehr @reedwm ", "comments": ["Unfortunately this was rolled back in a5a42397ef32593df3050bdf27c1874191fcb81d. The issue is that you call `sysconfig.get_build_info()['cudnn_version']`, but in our internal builds, there is no key `cudnn_version`.\r\n\r\nCan you create a new PR but replace that fragment with `sysconfig.get_build_info().get('cudnn_version', '0.0')`? Sorry for the inconvinience."]}, {"number": 42030, "title": "Tensorflow 2.1 + Docker + CUDA 10.2 Issue with the recognition of the graphics card", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Developer laptop -> Win10, Docker container: ubuntu18.04, Server: Linux\r\n- TensorFlow installed from (source or binary): Source with pip\r\n- TensorFlow version: 2.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: docker (pip)\r\n- CUDA/cuDNN version: Developer laptop: 10.2, Server: 10.2\r\n- GPU model and memory: Developer laptop GTC 1660 Ti, Server=?\r\n\r\n\r\n**Describe the problem**\r\nI am developing a deep learning model with Tensorflow2 and this model should be trained inside a docker container on the server. If I run the code on my laptop without a docker container, the GPU of the laptop will be recognized and can be trained with it. As soon as the code is in the container, no graphics card is recognized anymore, but the output of Tensorflow2 is generated, which is located at the bottom of **Any other info / logs**.\r\n\r\nthe problem seems to be that the containers do not allow access to the graphics cards.\r\n\r\n\r\n**Any other info / logs**\r\n```python\r\n2020-08-04 10:44:10.798030: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-08-04 10:44:10.798086: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-08-04 10:44:10.798117: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (12e682dbeadb): /proc/driver/nvidia/version does not exist\r\n```\r\n", "comments": ["@johnny-mueller \r\nCan you try setting the environment variable CUDA_VISIBLE_DEVICES to either blank or emptystring \"\", or possibly -1.\r\nPlease refer to [this resolved issue](https://github.com/tensorflow/tensorflow/issues/4267#issuecomment-248656096) and let us know.\r\nresolved issues related to the error reported\r\n#4078 [link](https://github.com/tensorflow/tensorflow/issues/255#issuecomment-384846967) [link1](https://stackoverflow.com/questions/60706122/cuda-driver-errors-on-the-machine-without-gpu-while-loading-model)", "@Saduf2019 \r\nPerhaps the following information is already an indication. I am testing directly also the setting of the variable.\r\n\r\nOutput of \"ls -l /usr/local/cuda/lib/libcud*\" from docker container\r\n```python\r\nroot@1dbaa98b68fb:/# ls -l /usr/local/cuda/lib/libcud*\r\nls: cannot access '/usr/local/cuda/lib/libcud*': No such file or directory\r\n```\r\n\r\nOutput of \"export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib\"  from docker container:\r\n```python\r\nroot@1dbaa98b68fb:/# root@1dbaa98b68fb:/# ls -l /usr/local/cuda/lib/libcud*\r\nroot@1dbaa98b68fb:/# ls: cannot access '/usr/local/cuda/lib/libcud*': No such file or directory\r\n```\r\n\r\n\r\n```python\r\nroot@1dbaa98b68fb:/# export NVID_VER=440.33.01\r\nroot@1dbaa98b68fb:/# cd /path/to/libcuda.so.$NVID_VER\r\nbash: cd: /path/to/libcuda.so.440.33.01: No such file or directory\r\nroot@1dbaa98b68fb:/# ln -s libcuda.so.$NVID_VER libcuda.so\r\nroot@1dbaa98b68fb:/# ln -s libcuda.so.$NVID_VER libcuda.so.1\r\n```\r\n\r\nFurthermore, only in the Docker Container the command \"nvidia-smi\" does not work. The command executes on the laptop and on the server. Unfortunately adding the environment variable did not work.", "@johnny-mueller Can you share the output of `docker run --gpus all nvidia/cuda:10.0-base nvidia-smi`?", "@bhack \r\n```python\r\ndocker run --gpus all nvidia/cuda:10.0-base nvidia-smi\r\nUnable to find image 'nvidia/cuda:10.0-base' locally\r\n10.0-base: Pulling from nvidia/cuda\r\n7ddbc47eeb70: Already exists                                                                                            \r\nc1bbdc448b72: Already exists                                                                                            \r\n8c3b70e39044: Already exists                                                                                            \r\n45d437916d57: Already exists                                                                                            \r\nd8f1569ddae6: Already exists                                                                                            \r\nde5a2c57c41d: Already exists                                                                                            \r\nea6f04a00543: Already exists                                                                                            \r\nStatus: Downloaded newer image for nvidia/cuda:10.0-base\r\ndocker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\r\n```", "Can you try to re-run the same command after these https://github.com/NVIDIA/nvidia-docker#ubuntu-160418042004-debian-jessiestretchbuster?", "@bhack that is quite complicated because im working on an Windows laptop and i have not the specific rights to execute these on the linux server ", "Let see if you have an old installation can you test `docker run --rm --runtime=nvidia -ti nvidia/cuda nvidia-smi`", "```python\r\ndocker run --rm --runtime=nvidia -ti nvidia/cuda nvidia-smi\r\ndocker: Error response from daemon: Unknown runtime specified nvidia.\r\nSee 'docker run --help'.\r\n```", "@johnny-mueller So I think that you need to talk with the linux machine sysadmin to ask about enable the docker gpu setup.", "@bhack thanks for the advice. I will call the admin tomorrow and will link this issue. Thanks for your help", "@johnny-mueller Any updates on this issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42030\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42030\">No</a>\n"]}, {"number": 42029, "title": "Bump google-cloud-cpp to 1.16.0", "body": "@mihaimaruseac \r\nAs I said before, This version has some features we need.", "comments": []}, {"number": 42028, "title": "Hadoop filesystem Part 2", "body": "@mihaimaruseac \r\nThis PR adds all missing function from `core/platform/hadoop`. We will need one more PR to tweak the behavior of the filesystem so it matchs with the behavior of `modular_filesystem`.", "comments": []}, {"number": 42027, "title": "Stale(?) references to activation histograms", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nURL above says\r\n\r\n> `histogram_freq`    frequency (in epochs) at which to compute activation and weight histograms ...\r\n\r\nHowever,\r\n- I do not see any activation histograms in my TensorBoard despite using `validation_data` - only biases and kernels.\r\n- I do not see any activation-specific code in https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/callbacks.py#L2227-L2238\r\n\r\nSee also https://stackoverflow.com/questions/60816678/\r\n\r\nTherefore, I suspect that activation histograms are not logged at all.", "comments": ["For more information, the code that produces activation histograms seems to have been removed here:\r\nhttps://github.com/tensorflow/tensorflow/commit/5b7063a3cfdc7710721924c5b4350c8265d5ff2d#diff-7db86ff688aaa0e9c0619dace9b487e7\r\n(look for `layer.output` in `callbacks.py`)", "@bersbersbers Can you please share a simple standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan sure:\r\n\r\n*Update*: Use smaller net, add `validation_data`.\r\n```\r\nimport tensorflow as tf\r\n\r\n(model := tf.keras.applications.mobilenet.MobileNet()).compile(loss=\"mse\")\r\nmodel.fit(\r\n    x=(x := tf.zeros((1, *model.input.shape[1:]))),\r\n    y=(y := tf.zeros((1, *model.output.shape[1:]))),\r\n    validation_data=(x, y),\r\n    callbacks=tf.keras.callbacks.TensorBoard(histogram_freq=1),\r\n)\r\n\r\n# then do\r\n# tensorboard --logdir logs --port 6007\r\n# and open\r\n# http://localhost:6007/#histograms\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/12128514/89504491-623fc980-d7c8-11ea-86c4-d156d8508d38.png)\r\n\r\nNote bias and kernel - no activations.", "@bersbersbers I ran your code and see more histograms. Can you please check [the gist here](https://colab.research.google.com/gist/jvishnuvardhan/b8585ef3e6e0fcfcbc981c24a5e7eb40/untitled973.ipynb) and let me know whether we are missing anything. Thanks!\r\n\r\nIf the above gist is not working, then please look into this [page](https://github.com/jvishnuvardhan/GitHub_Questions/blob/master/42027.ipynb).\r\n\r\nThanks!", "@jvishnuvardhan it seems that my screenshot still shows the histograms of the VGG16 network that I used in my earlier example. I wanted to switch the screenshot as well as the code, but apparently I missed the former. (You can also see that the layer names are different.)\n\nBut still: yes, MobileNet has more types of parameters than VGG16, so it shows more histograms. But all of these shown I understand more as weights, definitely not as activations. So your gist does show the issue: activation histograms are missing.", "I am also having this issue, using completely different code.", "Here's a minimal example:\r\n```python\r\nimport tensorflow as tf\r\n\r\ninp = x = tf.keras.Input(1)\r\nx = tf.keras.layers.Dense(units=10, activation=tf.nn.relu)(x)\r\nx = tf.keras.layers.Dense(units=10)(x)\r\nx = tf.keras.layers.Activation(tf.nn.relu)(x)\r\nmodel = tf.keras.Model(inp, x)\r\n\r\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\r\nmodel.fit(\r\n    tf.ones((32, 1)),\r\n    tf.ones((32, 10)),\r\n    callbacks=[tf.keras.callbacks.TensorBoard(\"logs\", histogram_freq=1)],\r\n    epochs=100,\r\n)\r\n```\r\nWhen viewing the logs with ``tensorboard --logdir logs`` and going to the Histograms tab, histograms for the layer weights show up, but not for any of the activities.", "This may be a duplicate of #39755.", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/a9b892e97465169e3791040285fa874a/untitled973.ipynb#scrollTo=Da1K5IYVzoxs)..Thanks !", "I re-posted this issue in https://github.com/keras-team/keras/issues/15972.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42027\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42027\">No</a>\n"]}, {"number": 42026, "title": "_create_keras_history_helper error ('NoneType' object has no attribute 'op') in 2.3.0 not in 2.2.0", "body": "When using model subclassing, we have been exploiting reinitialization of the subclass as a way to define the inputs for our model at construction, allowing us to call e.g. the `summary` method of our model and have specific input shape be listed as opposed to the 'multiple' input shape present when not constructing with an input. However, moving to tf 2.3.0 from 2.2.0 this now produces an unexpected error on the 2nd time the model is constructed. Here's a minimal reproducer:\r\n\r\n```\r\nclass MyModel(tf.keras.Model):\r\n\r\n  def __init__(self,**kwargs):\r\n    super(MyModel,self).__init__(**kwargs)\r\n    # create model layers ...\r\n    self.layer1 = tf.keras.layers.Dense(10)\r\n    # define input shape, and reinit ...\r\n    inputs = tf.keras.Input(shape=(5,))\r\n    super(MyModel,self).__init__(inputs=inputs, outputs=self.call(inputs),**kwargs)\r\n    # calling 'summary' will show specific input shapes\r\n    self.summary()\r\n  \r\n  def call(self, inputs):\r\n    return self.layer1(inputs)\r\n\r\nm = MyModel()\r\nm = MyModel() ## exception here!\r\n```\r\n\r\nProduces error:\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops, created_layers)\r\n    224                        'op wrapping. Please wrap these ops in a Lambda layer: '\r\n    225                        '\\n\\n```\\n{example}\\n```\\n'.format(example=example))\r\n--> 226     op = tensor.op  # The Op that created this Tensor.\r\n    227     if op not in processed_ops:\r\n    228       # Recursively set `_keras_history`.\r\n\r\nAttributeError: 'NoneType' object has no attribute 'op'\r\n```\r\n\r\nIn honesty we always considered the above 'reinit' a bit of a hack, and would be happy to be told the correct way to achieve what we want (a subclassed model with a defined input shape at construction time). But thought should report this anomalous behaviour none-the-less.\r\n\r\nThanks\r\n\r\n", "comments": ["Was able to reproduce the issue. \r\nCode runs fine on [TF v2.2](https://colab.research.google.com/gist/amahendrakar/64a4bde1feb838966566f5a9ecd24e51/42026-2-2.ipynb), facing an error stating `AttributeError: 'NoneType' object has no attribute 'op'` on running with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/14a9c6e1f715abe2cf223a02da4af973/42026-2-3.ipynb#scrollTo=2pAxuhfXKSvw). Please find the attached gist. Thanks!", "Thanks for confirming reproduction. As I say above, we are happy to be told if there is a better way to achieve the behaviour we are after, or at least be re-assured that 'reinitialization' is the correct/supported way to go?\r\n\r\n", "@will-cern I am not sure that it this is strictly \"a regression\" and I suppose that the error you catched with your class design could be more self-explicative that the current one. \r\nFor what you want to achieve I think that most similar solution it is at https://github.com/tensorflow/tensorflow/issues/25036#issuecomment-667534658 but please read the full thread.", "@will-cern Thanks for the issue!\r\n\r\nUsing subclasses to build Functional API models is actually a common pattern, the catch though is to not to double initialization and to not assign any attrs to the Model before calling the super. This works for example (note that the `call` method and layer tracking is implemented automatically bc the model becomes a Functional API model when initialized with the `inputs` and `outputs` signature):\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n  def __init__(self,**kwargs):\r\n    layer1 = tf.keras.layers.Dense(10)\r\n    # define input shape, and reinit ...\r\n    inputs = tf.keras.Input(shape=(5,))\r\n    super(MyModel,self).__init__(inputs=inputs, outputs=layer1(inputs),**kwargs)\r\n    # calling 'summary' will show specific input shapes\r\n    self.summary()\r\n\r\n\r\nm = MyModel()\r\nm = MyModel()\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42026\">No</a>\n"]}, {"number": 42025, "title": "Confusion Matrix produces different results in Keras model tf==2.3.0", "body": "Using tf==2.3.0, keras 2.4.3\r\nBuilt keras with from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\r\nWith Keras Sequential Model Prediction\r\nTo get Class Labels\r\nwe can do\r\n```\r\nyhat_classes1 = Keras_model.predict_classes(predictors)[:, 0] #this shows deprecated warning in tf==2.3.0\r\n\r\nWARNING:tensorflow:From <ipython-input-54-226ad21ffae4>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\r\nInstructions for updating:\r\nPlease use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\r\n\r\n```\r\nor\r\n```\r\nyhat_classes2 = np.argmax(Keras_model.predict(predictors), axis=1)\r\n```\r\nWith the first class labels if i create confusion matrix, i get\r\n```\r\nmatrix = confusion_matrix(actual_y, yhat_classes1)\r\n [[108579   8674]\r\n [  1205  24086]]\r\n```\r\nBut with the second class labels with the confusion matrix, i get 0 for True Positive and False Positive\r\n```\r\nmatrix = confusion_matrix(actual_y, yhat_classes2)\r\n [[117253      0]\r\n [ 25291      0]]\r\n```\r\nFinal layer has\r\n```\r\nKeras_model.add(Dense(1, activation='sigmoid', kernel_initializer=init_mode)) \r\n```\r\nMay I know whats the issue please, Thanks", "comments": ["@hanzigs \r\nplease provide complete stand alone code [indented] or colab gist with the error, also the tf version on which the issue was faced.\r\nPlease try tf.keras and let us know if it helps.", "Given tf version in Question, don't understand where you are mentioning to use tf.keras,\r\n\r\nto predict classes, below works\r\n```\r\nyhat_classes = (Keras_model.predict(predictors) > 0.5).astype(\"int32\")\r\n```\r\nMy Question was what the warning message suggesting is not working\r\n```\r\nWARNING:tensorflow:From <ipython-input-54-226ad21ffae4>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\r\nInstructions for updating:\r\nPlease use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\r\n```\r\nThis is binary classification problem", "@hanzigs Please provide the complete code for us to reproduce the model. Also, please ignore the error message for now. \r\nThe warning specifically says that you should use this line for binary classification, which is what you're doing:\r\n`(model.predict(x) > 0.5).astype(\"int32\")`\r\nThanks!", "Thanks, thats fine, got the explanation of warning message from stackoverflow."]}, {"number": 42024, "title": "RESIZE_NEAREST_NEIGHBOR Operation version not supported", "body": "def save_tflite():\r\n  converter = tf.lite.TFLiteConverter.from_saved_model(FLAGS.weights)\r\n  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n  converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n  converter.allow_custom_ops = True\r\n  converter.representative_dataset = representative_dataset_gen\r\n  tflite_model = converter.convert()\r\n  open(FLAGS.output, 'wb').write(tflite_model)\r\n\r\n  TF version:2.3.0  \r\n\r\n\r\nEdge TPU Compiler version 14.1.317412892\r\nInput: yolov3-416-int8.tflite\r\nOutput: yolov3-416-int8_edgetpu.tflite\r\n\r\nOperator                       Count      Status\r\n\r\nDEQUANTIZE                     3          Operation is working on an unsupported data type\r\nQUANTIZE                       2          More than one subgraph is not supported\r\nQUANTIZE                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation\r\nCONV_2D                        15         More than one subgraph is not supported\r\nCONV_2D                        60         Mapped to Edge TPU\r\nRESIZE_NEAREST_NEIGHBOR        2          Operation version not supported\r\nPAD                            5          Mapped to Edge TPU\r\nCONCATENATION                  2          More than one subgraph is not supported\r\nADD                            23         Mapped to Edge TPU\r\n\r\nwhen I use Edge TPU Compiler compile tflite model, I got the error about RESIZE_NEAREST_NEIGHBOR,ths.", "comments": ["I see RESIZE_NEAREST_NEIGHBOR Op is [supported](https://coral.ai/docs/edgetpu/models-intro/#supported-operations).\r\nAlso [google-coral/edgetpu](https://github.com/google-coral/edgetpu/issues) repo can be a good platform to raise this issue.\r\nPlease try posting it there.\r\nI am tagging similar [thread ](https://github.com/google-coral/edgetpu/issues/187) for your reference. Thanks!", "@ymodak thanks! This is a compiler issue, not tflite conversion", "@Namburger Thanks for your help.\r\n@ZhouKai90 I will close this issue since you have a more helpful dialogue on another thread. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42024\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42024\">No</a>\n"]}, {"number": 42023, "title": "[Core ML delegate] Forward declaration", "body": "Add a forward declaration to TfLiteDelegate\r\n\r\nIt's faster compile time and reduced binary size besides avoiding namespace pollution without including \"tensorflow/lite/c/common.h\"", "comments": ["@srjoglekar246 could you take a look at this? Thank you!", "Hi idstein,\r\n\r\nCan you add the reason of this change in the commit description?", "@idstein  Can you please check @terryheo's comments and keep us posted ? Thanks!", "@idstein  Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "As only a single symbol is referenced adding the entire common header seems to pollute a lot the namespaces.\r\n\r\nWhen removed it improves compilation time and binary size besides the aforementioned namespace reduction.", "@terryheo  Can you please take a look on the above comment from @idstein. Thanks!", "@idstein , could you share the way you used to measure the improvement?\r\nI'm reluctant to accept this since \"tensorflow/lite/c/common.h\" is basic header so eventually we might need to include this again even though we don't this it with the forward declaration at this time.\r\n\r\nSo I'd like to see how much compile time benefit we can get from this. In my testing, there is no significant difference in building time.", "@idstein  Can you please check @terryheo's comments and keep us posted ? Thanks!", "@idstein Any update on this CL? Please. Thanks!", "PR can be closed as there have been new dependencies introduced in current master. Sorry for the late response."]}, {"number": 42022, "title": "Memory leak in keras.backend preventing graphs from being GC'ed", "body": "In some situations (i.e. [when outside of an eager execution context](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/backend.py#L394)) a Tensor value will be inserted into [_GRAPH_LEARNING_PHASES](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/backend.py#L6389). This is a problem because the Tensor value itself references the graph which is suppose to be weakly held in memory by this dictionary. With the implementation of weakref.WeakKeyDictionary a value that strongly references its key will forever prevent the key from being reclaimed. This means the entire graph is permanently stuck in memory unless the entry is explicitly deleted.\r\n\r\nThis issue is present in tf 2.3.0\r\n\r\nSimple repro code\r\n\r\n```\r\nimport gc\r\n\r\nimport psutil\r\nimport tensorflow as tf\r\n\r\nFIX_LEAK = False\r\n\r\ndef _get_rss():\r\n    return psutil.Process().memory_info().rss\r\n\r\ndef _test_iteration():\r\n    with tf.Graph().as_default() as graph:\r\n        inputs = tf.keras.Input(shape=(8,), dtype=\"int32\")\r\n        embedding_layer = tf.keras.layers.Embedding(\r\n            input_dim=128,\r\n            output_dim=128,\r\n            input_length=8,\r\n        )(inputs)\r\n        tf.keras.layers.LSTM(\r\n            256,\r\n            dropout=0.5,\r\n        )(embedding_layer)\r\n        \r\n    if FIX_LEAK:\r\n        import tensorflow.python.keras.backend as backend\r\n        backend._GRAPH_LEARNING_PHASES.pop(graph, None)\r\n    \r\nfor _ in range(128):\r\n    _test_iteration()\r\n\r\n    gc.collect()\r\n    print(\"RSS\", _get_rss())\r\n```\r\n\r\nWith FIX_LEAK as False the memory usage grows forever (about 8MB or so for me each iteration). If FIX_LEAK is True the memory stays stable. I'm not super familiar with TF/keras, there may be a more bare bones example. In particular it seems that the dropout parameter was needed to trigger the learning_phrase Tensor to be created.", "comments": ["This could be the cause for the leak in this issue as well https://github.com/tensorflow/tensorflow/issues/41718\r\n\r\nI enabled eager execution via `tf.config.experimental_run_functions_eagerly(True)` and the memory usage seems quite stable around 1GB, compared to ~7GB+ that I was getting without eager mode. FWIW, I noticed that CPU usage maxes out at 1 core only, compared to multiple cores when using graph mode.\r\n\r\nThe example code in that issue could be used to reproduce and check this behavior.", "@msg555,\r\nCould you please provide a minimal working example to reproduce the issue reported here. Thanks!", "> @msg555,\r\n> Could you please provide a minimal working example to reproduce the issue reported here. Thanks!\r\n\r\nAdded an example", "@msg555,\r\nThank you for the update. \r\n\r\nI was able to reproduce the issue with TF v2.3, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/6c18a1fd9f1a94c7fe5e942118239bee/42022.ipynb). Thanks!", "> This could be the cause for the leak in this issue as well #41718\r\n> \r\n> I enabled eager execution via `tf.config.experimental_run_functions_eagerly(True)` and the memory usage seems quite stable around 1GB, compared to ~7GB+ that I was getting without eager mode. FWIW, I noticed that CPU usage maxes out at 1 core only, compared to multiple cores when using graph mode.\r\n> \r\n> The example code in that issue could be used to reproduce and check this behavior.\r\n\r\n@jammm You can test if it's the same issue by examining `tensorflow.python.keras.backend._GRAPH_LEARNING_PHASES`. If it continues to grow over time with more elements than it's likely the same issue.", "Sorry for the delayed response here! The backend graph (& backend learning phases) have been historically prone to memory leaks, and we have put a lot of focus into getting rid of the global backend graph entirely in 2.4 to fix these leaks.\r\n\r\nThat said, the specific code snippet in this github issue enters a v1 `tf.Graph.as_default()` explicitly which is a legacy v1 graph compatibility mode. It will not benefit from these improvements, and will not receive performance improvements in the future. If you use eager execution, with tf.function to create/enter graphs as needed, the specific memory leak reported here will not be an issue in 2.4.\r\n\r\nAs such, closing this issue but happy to answer any follow-on questions.", "@tomerk  Thanks a lot for your reply! I'm currently using tf.function in many places. How do I make sure that it's running in eager mode in code outside of tf.function?\r\n", "If you're using tf2 and haven't explicitly used the `tf.compat.v1.disable_eager_execution()` command it should be eager. If you're not sure and you want to check it programmatically, you can use `tf.executing_eagerly()`: https://www.tensorflow.org/api_docs/python/tf/executing_eagerly\r\n\r\nIt will return True when running eagerly, and False when tracing a tf.function into a graph.", "I'm not sure if your issue #41718 is related to this though, that one will necessitate a closer look\r\n\r\nEdit: Looking at issue #41718 your repro colab actually doesn't use Keras at all, so I think there's something lower level going on in the stack that'll require more investigation.", "Hi @saikumarchalla is there a specific reason you re-opened this issue?"]}, {"number": 42021, "title": "Issue Running T5 in colab TPU - Tensorflow", "body": "Hi Team,\r\n\r\nI was trying to do a pre training of T5 from scratch on colab. I could see if i install t5 using (pip install t5[gcp]), and tried to connect to execute tf.tpu.experimental.initialize_tpu_system(tpu), getting below error.\r\n\r\nInvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}\r\n\r\nIf install/ upgrade tensorflow, it gets resolved, however import of t5 does not work as below.\r\nimport t5\r\n\r\nNotFoundError: /usr/local/lib/python3.6/dist-packages/tensorflow_text/python/metrics/_text_similarity_metric_ops.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl11string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS8_EE\r\n\r\nPlease let me know how if there is a way to resolve this.\r\nThanks.", "comments": ["@ashispapu \r\n\r\nThis error has already been answered in another issue. Please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/40622#issuecomment-654488935) from a member of the TensorFlow team. Thanks!\r\n\r\n\r\nIn case you issue is not resolved can you please update update the issue template with the tf version on which this error was faced and simple stand alone code to replicate it or is possible share colab gist with error.\r\n\r\n", "@Saduf2019  I also got the same issues, when i tried to connect the a VM instance with a TPU node(v2-8). ", "@ashispapu\r\nPlease share the colab gist for us to replicate the issue faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42020, "title": "Update ARC back-end to use latest embarc_mli release.", "body": "\r\nUpdate ARC back-end in lite micro to use the latest pre-release (mli 1.1 RC3) of embarc_mli\r\nFix issues caused by upstream changes (some code refactoring was not applied to the arc files)\r\nsmall Fixes for arc emsdp target\r\n", "comments": ["@JaccovG `tensorflow/lite/micro/tools/make/targets/arc/emsdp/uboot.env` is autogenerated , can you please remove it.", "@JaccovG  Can you please check @rthadur's comments and keep us posted ? Thanks!", "Thanks for the comment. we are working on a fix.", "Hi, we removed uboot.env file on your request and updated readme`s accordingly. There are some other minor arc specific changes in the latest commit.  @petewarden , can you please review this PR?"]}, {"number": 42019, "title": "Functional preprocessing stage", "body": "Functional equivalent of (sequential) preprocessing stage.  I imitated the algorithm used in `Functional` to compute output tensor, and adapt each layer before compute the layer.  \r\n\r\nTo be `Dataset` compatible, when dataset object is passed in, I 'unzip' it into a list of dataset objects, each corresponding to one input.  Initially I also considered having everything in one dataset object, and keep a data structure to track which element correspond to each layer in/output. However, such approach would have cost more memory. ", "comments": ["A minor edit dismissed the approval. Can you approve it again?  Thanks! @fchollet ", "Changed according the comment and fixed doctest a bit. Can you re-approve? Thanks! @fchollet ", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42019) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42019) for more info**.\n\n<!-- ok -->", "Why is this failing a lot of tests in windows test?", "What's going on with the internal check? Is there anything I can do about this? @qlzh727 @fchollet ", "Sorry for the long wait. It seems that there were some issue for sync/formatting of the PR. I will fix it and merge it internally. "]}, {"number": 42017, "title": "ValueError: Graph disconnected: cannot obtain value for tensor Tensor\u2026The following previous layers were accessed without issue:", "body": "I want to obtain the output of intermediate sub-model layers with tf2.keras.Here is a model composed of two sub-modules:\r\n```python\r\n    input_shape = (100, 100, 3)\r\n\r\n    def model1():\r\n        input = tf.keras.layers.Input(input_shape)\r\n        cov = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,name='cov1')(input)\r\n        embedding_model = tf.keras.Model(input,cov,name='model1')\r\n        return embedding_model\r\n\r\n    def model2(embedding_model):\r\n\r\n        input_sequence = tf.keras.layers.Input((None,) + input_shape)\r\n\r\n        sequence_embedding = tf.keras.layers.TimeDistributed(embedding_model,name='time_dis1')\r\n\r\n        emb = sequence_embedding(input_sequence)\r\n        att = tf.keras.layers.Attention()([emb,emb])\r\n        dense1 = tf.keras.layers.Dense(64,name='dense1')(att)\r\n        outputs = tf.keras.layers.Softmax()(dense1)\r\n\r\n        final_model = tf.keras.Model(inputs=input_sequence, outputs=outputs,name='model2')\r\n        return final_model\r\n\r\n    embedding_model = model1()\r\n\r\n    model2 = model2(embedding_model)\r\n    print(model2.summary())\r\n```\r\n\r\noutput:\r\n```python\r\nModel: \"model2\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_2 (InputLayer)            [(None, None, 100, 1 0                                            \r\n__________________________________________________________________________________________________\r\ntime_dis1 (TimeDistributed)     (None, None, 98, 98, 896         input_2[0][0]                    \r\n__________________________________________________________________________________________________\r\nattention (Attention)           (None, None, 98, 98, 0           time_dis1[0][0]                  \r\n                                                                 time_dis1[0][0]                  \r\n__________________________________________________________________________________________________\r\ndense1 (Dense)                  (None, None, 98, 98, 2112        attention[0][0]                  \r\n__________________________________________________________________________________________________\r\nsoftmax (Softmax)               (None, None, 98, 98, 0           dense1[0][0]                     \r\n==================================================================================================\r\nTotal params: 3,008\r\nTrainable params: 3,008\r\nNon-trainable params: 0\r\n```\r\nand then,I want to get output intermediate layer of model1 and model2:\r\n\r\n```python\r\n    model1_output_layer = model2.get_layer('time_dis1').layer.get_layer('cov1')\r\n    output1 = model1_output_layer.get_output_at(0)\r\n    output2 = model2.get_layer('dense1').get_output_at(0)\r\n\r\n    output_tensors = [output1,output2]\r\n    model2_input = model2.input\r\n    submodel = tf.keras.Model([model2_input],output_tensors)\r\n    input_data2 = np.zeros((1,10,100,100,3))\r\n\r\n    result = submodel.predict([input_data2])\r\n    print(result)\r\n```\r\nRunning in tf2.3 ,the error I am getting is:\r\n```python\r\n File \"/Users/bouluoyu/anaconda/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py\", line 115, in __init__\r\n    self._init_graph_network(inputs, outputs)\r\n  File \"/Users/bouluoyu/anaconda/envs/tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/bouluoyu/anaconda/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py\", line 191, in _init_graph_network\r\n    self.inputs, self.outputs)\r\n  File \"/Users/bouluoyu/anaconda/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py\", line 931, in _map_graph_network\r\n    str(layers_with_complete_input))\r\nValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"input_1:0\", shape=(None, 100, 100, 3), dtype=float32) at layer \"cov1\". The following previous layers were accessed without issue: ['time_dis1', 'attention', 'dense1']\r\n```\r\nBut the following code works:\r\n\r\n```python\r\n    model1_input = embedding_model.input\r\n    model2_input = model2.input\r\n\r\n    submodel = tf.keras.Model([model1_input,model2_input],output_tensors)\r\n\r\n    input_data1 = np.zeros((1,100,100,3))\r\n    input_data2 = np.zeros((1,10,100,100,3))\r\n\r\n    result = submodel.predict([input_data1,input_data2])\r\n    print(result\r\n```\r\nBut not what I want.This is strange, model1 is part of model2, so why do we need to input an extra tensor ```input_data1``` ? Sometimes,it is hard to get an extra tensor,especially for complex models.What should I do? Do we need a new API to support this functionality?\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/bd181347f5608abf6644aa2bb5a482f3/42017.ipynb). Thanks!", "any updates ?", "@boluoyu Looks like this is an implementation issue. Please take a look at this issue [here](https://stackoverflow.com/questions/56589726/valueerror-graph-disconnected-cannot-obtain-value-for-tensor-tensor-the-foll) and let me know if it helps. Thanks!", "@gowthamkpr No,This issue is very different from that one. These are two completely unrelated issue.\r\n```python\r\n\r\n    embedding_model = model1()\r\n    model2 = model2(embedding_model)\r\n    print(model2.summary())\r\n\r\n    input_data2 = np.zeros((1,10,100,100,3))\r\n\r\n    result = model2.predict(input_data2)\r\n    print(result) #works\r\n```\r\nDo you see it ? when I use the raw model2 to predict ,JUST input one  tensor ,no problem.But I MUST input two tensor when I want to get output intermediate layer of model1 and model2.\r\n```python\r\n    output1 = model1_output_layer.get_output_at(0)\r\n    output2 = model2.get_layer('dense1').get_output_at(0)\r\n\r\n    output_tensors = [output1,output2]\r\n    isubmodel = tf.keras.Model([model1_input,model2_input],output_tensors) #input two tensor\r\n\r\n    input_data1 = np.zeros((1,100,100,3))\r\n    input_data2 = np.zeros((1,10,100,100,3))\r\n\r\n    result = submodel.predict([input_data1,input_data2])\r\n```\r\nModel1 is part of model2, we don't need an extra tensor `input_data1`,I hope the new API changes it.", "I have encountered this same issue as well. In general, it happens any time you use some functional sub-model as a layer in your model, and then you want to use a tensor from that sub-model in the outer model (e.g., exposing it as an extra output to debug or analyze a model).\r\n\r\nExample:\r\n```python\r\nimport tensorflow as tf\r\n\r\ninp1 = tf.keras.Input(1)\r\nh1 = tf.keras.layers.Dense(1)(inp1)\r\nout1 = tf.keras.layers.Dense(1)(h1)\r\nmodel1 = tf.keras.Model(inputs=inp1, outputs=out1)\r\n\r\ninp2 = tf.keras.Input(1)\r\nh2 = model1(inp2)\r\nout2 = tf.keras.layers.Dense(1)(h2)\r\nmodel2 = tf.keras.Model(inputs=inp2, outputs=[out2, model1.layers[1].output])\r\n```\r\n=>\r\n```\r\nValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"input_1:0\", shape=(None, 1), dtype=float32) at layer \"dense\". The following previous layers were accessed without issue: ['functional_1']\r\n```\r\n\r\nThe issue does not occur if the submodel is at the same \"level\" as the outer model, or if you explicitly make the required tensor an output of the sub-model. It's not clear to me if there are any other ways around this, but this seems to be the intent with the functional model API.", "I am also experiencing this issue. I wish to perform feature extraction on  my siamese network model (which contains two sub-models) and I am facing this issue.", "As a workaround, you can add the embedding layers to the model's outputs:\r\n\r\n```\r\nmodel = tf.keras.Model(inputs=[A, B], outputs=[distance, embeddingA, embeddingB])\r\n```\r\n\r\nwhere A and B are the inputs to the sub-networks, and embeddingA and embeddingB are the outputs (the embeddings) of these sub-networks. Not beautiful, but it works in a custom train loop.", "@boluoyu,\r\n\r\nCan you take a look at the above workaround proposed by @Yannik1337 and let us know if it helps? Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42017\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42017\">No</a>\n"]}, {"number": 42016, "title": "Timeseries example is not working with latest 2.4 code", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb\r\n\r\nWhen it comes to Convolution model, this errors out.\r\n===code===\r\nhistory = compile_and_fit(conv_model, conv_window)\r\n\r\nIPython.display.clear_output()\r\nval_performance['Conv'] = conv_model.evaluate(conv_window.val)\r\nperformance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)\r\n==============\r\n===error====\r\nNotFoundError:  No algorithm worked!\r\n\t [[node sequential_3/conv1d/conv1d (defined at <ipython-input-41-716049f06cb3>:12) ]] [Op:__inference_train_function_127129]\r\n\r\nFunction call stack:\r\ntrain_function\r\n", "comments": ["Closing this as duplicate of #41987"]}, {"number": 42015, "title": "How to start recording video when something has been detected!", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes (I took it from someone's github)\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspberry pi, however, this code works with windows too\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**: Using tflite\r\n-   **Python version**:3.7\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThis is the code. how can I start recording video when something gets detected and then store it in a output directory. If possible how would I be able to additionally give a time stamp of when the recording was taken. THIS WOULD BE A HUGE HELP IF SOMEONE CAN HELP ME! I wouldn't mind using a completely different code which is made just for the purpose of storing videos of detection!\r\nimport os\r\nimport argparse\r\nimport cv2\r\nimport numpy as np\r\nimport sys\r\nimport time\r\nfrom threading import Thread\r\nimport importlib.util\r\n\r\n# Define VideoStream class to handle streaming of video from webcam in separate processing thread\r\n# Source - Adrian Rosebrock, PyImageSearch: https://www.pyimagesearch.com/2015/12/28/increasing-raspberry-pi-fps-with-python-and-opencv/\r\nclass VideoStream:\r\n    \"\"\"Camera object that controls video streaming from the Picamera\"\"\"\r\n    def __init__(self,resolution=(640,480),framerate=30):\r\n        # Initialize the PiCamera and the camera image stream\r\n        self.stream = cv2.VideoCapture(0)\r\n        ret = self.stream.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))\r\n        ret = self.stream.set(3,resolution[0])\r\n        ret = self.stream.set(4,resolution[1])\r\n            \r\n        # Read first frame from the stream\r\n        (self.grabbed, self.frame) = self.stream.read()\r\n\r\n\t# Variable to control when the camera is stopped\r\n        self.stopped = False\r\n\r\n    def start(self):\r\n\t# Start the thread that reads frames from the video stream\r\n        Thread(target=self.update,args=()).start()\r\n        return self\r\n\r\n    def update(self):\r\n        # Keep looping indefinitely until the thread is stopped\r\n        while True:\r\n            # If the camera is stopped, stop the thread\r\n            if self.stopped:\r\n                # Close camera resources\r\n                self.stream.release()\r\n                return\r\n\r\n            # Otherwise, grab the next frame from the stream\r\n            (self.grabbed, self.frame) = self.stream.read()\r\n\r\n    def read(self):\r\n\t# Return the most recent frame\r\n        return self.frame\r\n\r\n    def stop(self):\r\n\t# Indicate that the camera and thread should be stopped\r\n        self.stopped = True\r\n\r\n# Define and parse input arguments\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--modeldir', help='Folder the .tflite file is located in',\r\n                    required=True)\r\nparser.add_argument('--graph', help='Name of the .tflite file, if different than detect.tflite',\r\n                    default='detect.tflite')\r\nparser.add_argument('--labels', help='Name of the labelmap file, if different than labelmap.txt',\r\n                    default='labelmap.txt')\r\nparser.add_argument('--threshold', help='Minimum confidence threshold for displaying detected objects',\r\n                    default=0.5)\r\nparser.add_argument('--resolution', help='Desired webcam resolution in WxH. If the webcam does not support the resolution entered, errors may occur.',\r\n                    default='1280x720')\r\nparser.add_argument('--edgetpu', help='Use Coral Edge TPU Accelerator to speed up detection',\r\n                    action='store_true')\r\n\r\nargs = parser.parse_args()\r\n\r\nMODEL_NAME = args.modeldir\r\nGRAPH_NAME = args.graph\r\nLABELMAP_NAME = args.labels\r\nmin_conf_threshold = float(args.threshold)\r\nresW, resH = args.resolution.split('x')\r\nimW, imH = int(resW), int(resH)\r\nuse_TPU = args.edgetpu\r\n\r\n# Import TensorFlow libraries\r\n# If tflite_runtime is installed, import interpreter from tflite_runtime, else import from regular tensorflow\r\n# If using Coral Edge TPU, import the load_delegate library\r\npkg = importlib.util.find_spec('tflite_runtime')\r\nif pkg:\r\n    from tflite_runtime.interpreter import Interpreter\r\n    if use_TPU:\r\n        from tflite_runtime.interpreter import load_delegate\r\nelse:\r\n    from tensorflow.lite.python.interpreter import Interpreter\r\n    if use_TPU:\r\n        from tensorflow.lite.python.interpreter import load_delegate\r\n\r\n# If using Edge TPU, assign filename for Edge TPU model\r\nif use_TPU:\r\n    # If user has specified the name of the .tflite file, use that name, otherwise use default 'edgetpu.tflite'\r\n    if (GRAPH_NAME == 'detect.tflite'):\r\n        GRAPH_NAME = 'edgetpu.tflite'       \r\n\r\n# Get path to current working directory\r\nCWD_PATH = os.getcwd()\r\n\r\n# Path to .tflite file, which contains the model that is used for object detection\r\nPATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,GRAPH_NAME)\r\n\r\n# Path to label map file\r\nPATH_TO_LABELS = os.path.join(CWD_PATH,MODEL_NAME,LABELMAP_NAME)\r\n\r\n# Load the label map\r\nwith open(PATH_TO_LABELS, 'r') as f:\r\n    labels = [line.strip() for line in f.readlines()]\r\n\r\n# Have to do a weird fix for label map if using the COCO \"starter model\" from\r\n# https://www.tensorflow.org/lite/models/object_detection/overview\r\n# First label is '???', which has to be removed.\r\nif labels[0] == '???':\r\n    del(labels[0])\r\n\r\n# Load the Tensorflow Lite model.\r\n# If using Edge TPU, use special load_delegate argument\r\nif use_TPU:\r\n    interpreter = Interpreter(model_path=PATH_TO_CKPT,\r\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n    print(PATH_TO_CKPT)\r\nelse:\r\n    interpreter = Interpreter(model_path=PATH_TO_CKPT)\r\n\r\ninterpreter.allocate_tensors()\r\n\r\n# Get model details\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nheight = input_details[0]['shape'][1]\r\nwidth = input_details[0]['shape'][2]\r\n\r\nfloating_model = (input_details[0]['dtype'] == np.float32)\r\n\r\ninput_mean = 127.5\r\ninput_std = 127.5\r\n\r\n# Initialize frame rate calculation\r\nframe_rate_calc = 1\r\nfreq = cv2.getTickFrequency()\r\n\r\n# Initialize video stream\r\nvideostream = VideoStream(resolution=(imW,imH),framerate=30).start()\r\ntime.sleep(1)\r\n\r\n#for frame1 in camera.capture_continuous(rawCapture, format=\"bgr\",use_video_port=True):\r\nwhile True:\r\n\r\n    # Start timer (for calculating frame rate)\r\n    t1 = cv2.getTickCount()\r\n\r\n    # Grab frame from video stream\r\n    frame1 = videostream.read()\r\n\r\n    # Acquire frame and resize to expected shape [1xHxWx3]\r\n    frame = frame1.copy()\r\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n    frame_resized = cv2.resize(frame_rgb, (width, height))\r\n    input_data = np.expand_dims(frame_resized, axis=0)\r\n\r\n    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\r\n    if floating_model:\r\n        input_data = (np.float32(input_data) - input_mean) / input_std\r\n\r\n    # Perform the actual detection by running the model with the image as input\r\n    interpreter.set_tensor(input_details[0]['index'],input_data)\r\n    interpreter.invoke()\r\n\r\n    # Retrieve detection results\r\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\r\n    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\r\n    scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects\r\n    #num = interpreter.get_tensor(output_details[3]['index'])[0]  # Total number of detected objects (inaccurate and not needed)\r\n\r\n    # Loop over all detections and draw detection box if confidence is above minimum threshold\r\n    for i in range(len(scores)):\r\n        if ((scores[i] > min_conf_threshold) and (scores[i] <= 1.0)):\r\n\r\n            # Get bounding box coordinates and draw box\r\n            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\r\n            ymin = int(max(1,(boxes[i][0] * imH)))\r\n            xmin = int(max(1,(boxes[i][1] * imW)))\r\n            ymax = int(min(imH,(boxes[i][2] * imH)))\r\n            xmax = int(min(imW,(boxes[i][3] * imW)))\r\n            \r\n            cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\r\n\r\n            # Draw label\r\n            object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\r\n            label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\r\n            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\r\n            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\r\n            cv2.rectangle(frame, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\r\n            cv2.putText(frame, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\r\n\r\n    # Draw framerate in corner of frame\r\n    cv2.putText(frame,'FPS: {0:.2f}'.format(frame_rate_calc),(30,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,0),2,cv2.LINE_AA)\r\n\r\n    # All the results have been drawn on the frame, so it's time to display it.\r\n    cv2.imshow('Object detector', frame)\r\n\r\n    # Calculate framerate\r\n    t2 = cv2.getTickCount()\r\n    time1 = (t2-t1)/freq\r\n    frame_rate_calc= 1/time1\r\n\r\n    # Press 'q' to quit\r\n    if cv2.waitKey(1) == ord('q'):\r\n        break\r\n\r\n# Clean up\r\ncv2.destroyAllWindows()\r\nvideostream.stop()\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 42014, "title": "ValueError: Input tensors to a Functional must come from `tf.keras.Input` (TF 2.3)", "body": "Example below works in 2.2; `K.function` [now builds](https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/python/keras/backend.py#L3911) a `Model` in Eager execution, so we're passing `Model(inputs=[learning_phase,...])`.\r\n\r\nI do have a workaround in mind, but it's hackish, and lot more complex than `K.function`; is there an analog to below in 2.3?\r\n\r\n<hr>\r\n\r\n```python\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.python.keras import backend as K\r\nimport numpy as np\r\n\r\nipt = Input((16,))\r\nx   = Dense(16)(ipt)\r\nout = Dense(16)(x)\r\nmodel = Model(ipt, out)\r\nmodel.compile('sgd', 'mse')\r\n\r\nouts_fn = K.function([model.input, K.symbolic_learning_phase()],\r\n                     [model.layers[1].output])  # error\r\nx = np.random.randn(32, 16)\r\nprint(outs_fn([x, True]))\r\n```\r\n```\r\n>>> ValueError: Input tensors to a Functional must come from `tf.keras.Input`. \r\nReceived: Tensor(\"keras_learning_phase:0\", shape=(), dtype=bool) \r\n(missing previous layer metadata).\r\n```", "comments": ["[Resolved](https://stackoverflow.com/q/63238203/10133797).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42014\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42014\">No</a>\n", "I have a problem like you what can I do to solve it", "> I have a problem like you what can I do to solve it\r\n\r\n@mohammadrezaghorvei,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 42013, "title": "Add macro for name hiding in FileSystem derived classes", "body": "This PR adds a macro to expose base class non-transactional api to derived classes so that they are accessible from derived classes and forward to transactional api with nullptr tokens. This is an interim solution until plugin based filesystems migration is complete and a workaround for internal coding standard requirements.", "comments": []}, {"number": 42012, "title": "Added EEMBC benchmark test harness + two targets.", "body": "This is a pull request to enable existing targets in the TFLiteMicro repository to plug into the EEMBC ULPMark-ML low-power machine learning benchmark framework currently under development. Please see the README.md.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42012) for more info**.\n\n<!-- need_sender_cla -->", "Closing due to need for legal review of CLA.", "@petertorelli Thank you for your contribution. Can you please sign CLA? Thanks!", "@gbaned I didn't know there was a CLA, it will need some legal review on my side (that EEMBC comment above was from me, wrong account). I will re-open (or resubmit) after this is cleared up."]}, {"number": 42011, "title": "changed OnesLike to ZerosLike", "body": "@saxenasaurabh @alextp ", "comments": []}, {"number": 42010, "title": "tf.keras.preprocessing.image_dataset_from_directory doesn't work on TPU", "body": "**System information**\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.6\r\n- TPU model and memory: TPU v3-8 (Kaggle default)\r\n\r\n**Describe the current behavior**\r\n`tf.keras.preprocessing.image_dataset_from_directory` doesn't work on TPU.\r\n\r\nError message:\r\n`InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=TensorSliceDataset; signature=components: -> handle:variant; attr=Toutput_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>; NodeDef: {{node TensorSliceDataset}} [Op:MapDataset]`\r\n\r\n", "comments": ["@mjang2000 \r\nI ran the code shared it is working as expected on tf 2.3, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/617a402fc005658237a2ce10695d167d/untitled312.ipynb) for the same.\r\nI see there is a spelling mistake in the code shared, `\"image_dataseet_from_directory\" should be \"set\"`, please make the correction and let us know.", "This is the error message I get while running on a Kaggle kernel. The code can be found [here](https://www.kaggle.com/amyjang/alzheimer-mri-model-tensorflow-2-3-data-loading). Please note that this notebook viewers shows the run on GPU so it does not show this error. When the accelerator is changed to TPU, the following error pops up.\r\n![image](https://user-images.githubusercontent.com/43710235/89347428-99469b80-d65f-11ea-9a0f-72553cdc6fdd.png)\r\n\r\n", "@mjang2000 \r\ni ran the code shared by you on tpu in the issue template as well, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ef719fa6e56583d65fd15c83dd2b4cd6/untitled319.ipynb). please let me know if i am doing something incorrect.", "The gist doesn't seem to run the method or load in images from a directory. I've attached the run on a TPU [here](https://www.kaggle.com/amyjang/bug-testing?scriptVersionId=40276706). Please check cell 5.", "I am using TPU on Google Colab Pro. Just want to add that tf.keras.preprocessing.image_dataset_from_directory produce another kind of error message when using TPU on Google Colab:\r\nNotImplementedError: unable to open file: _gcs_config_ops.so, from paths: ['/usr/local/lib/python3.6/dist-packages/tensorflow_gcs_config/_gcs_config_ops.so']\r\ncaused by: ['/usr/local/lib/python3.6/dist-packages/tensorflow_gcs_config/_gcs_config_ops.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb']", "@haowang-ms89 Can you please provide a standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan Sorry, I just figured out that this is caused by tensorflow 2.4.0-dev. When I switch to 2.3.0, the function works. So I think it is a tensorflow dev version issue. The code:\r\n`data_dir_train = pathlib.Path('./train')`\r\n`train_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n  data_dir_train,\r\n  seed=123,\r\n  image_size=(img_height, img_width),\r\n  batch_size=batch_size,\r\n  label_mode=\"categorical\")`\r\nMay I ask a question here? Because TPU does not read from local directory, I have to put training data on Google Drive or GCS.  (https://github.com/tensorflow/hub/issues/604). The code above works when reading local folder, but will fail at model.fit(). Does the function tf.keras.preprocessing.image_dataset_from_directory accepts a folder in those cloud storage? Thanks!", "Hi @mjang2000, I believe the error message you are seeing has to do with a version mismatch, as noted in [this comment here](https://github.com/tensorflow/tensorflow/issues/40622#issuecomment-654488935).\r\n\r\n@haowang-ms89, Unfortunately I don't think that `image_dataset_from_directory` accepts a GCS bucket. I just tried it out and am unable to get it to work.", "Hi @nikitamaia, I've been running my notebook on Kaggle and the default version for the TPU is v3-8.", "Yes, v3-8 refers to the version of the TPU but I don't know what TF version is there. I think this is a kaggle kernel issue, possibly the default Kaggle TPUs TF version is 2.2, since 2.3 was only recently released. As @haowang-ms89 mentioned, running `image_dataset_from_directory` with a TPU runtime in colab works, so that makes me suspect the issue is on the Kaggle side. A quick search for the error you're seeing on Kaggle seems to return several similar posts that might be helpful, I saw a comment that they were planning to downgrade back to TF 2.2 for the TPU VMs.", "Closing this issue now since it seems to be due to the Kaggle kernel and not `tf.keras`"]}, {"number": 42009, "title": "Tensorflow lite model always gives same output no matter the input", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Using Google Collab platform for model development \r\n- TensorFlow installed from (source or binary): Installed from source \r\n- Tensorflow version (commit SHA if source): v2.1.1\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32 \r\n\r\nMy goal is to run a Keras model I have made in my ESP32 microcontroller. I have the libraries all working correctly.\r\n\r\nI have created a Keras model using google Collab that looks to be working fine when I give it random test data within google Collab. The model has two input features and 4 different outputs.(a multiple-output regression model)\r\n\r\nHowever, when I export and load the model into my c++ application in the ESP32 it does not matter what the inputs are, it always predicts the same output.\r\n\r\nI have based myself in this code in order to load and run the model in c++ : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/magic_wand/main_functions.cc\r\n\r\nAnd this is my version of the code\r\n```\r\nnamespace {\r\n    tflite::ErrorReporter* error_reporter = nullptr;\r\n    const tflite::Model* model = nullptr;\r\n    tflite::MicroInterpreter* interpreter = nullptr;\r\n    TfLiteTensor* input = nullptr;\r\n    TfLiteTensor* output = nullptr;\r\n    int inference_count = 0;\r\n\r\n    // Create an area of memory to use for input, output, and intermediate arrays.\r\n    // Finding the minimum value for your model may require some trial and error.\r\n    constexpr int kTensorArenaSize = 2 * 2048;\r\n    uint8_t tensor_arena[kTensorArenaSize];\r\n}  // namespace \r\n\r\n```\r\n```\r\nstatic void setup(){\r\n    static tflite::MicroErrorReporter micro_error_reporter;\r\n    error_reporter = &micro_error_reporter;\r\n\r\n    model = tflite::GetModel(venti_model);\r\n    if (model->version() != TFLITE_SCHEMA_VERSION) {\r\n        error_reporter->Report(\r\n            \"Model provided is schema version %d not equal \"\r\n            \"to supported version %d.\",\r\n            model->version(), TFLITE_SCHEMA_VERSION);\r\n        return;\r\n    }\r\n\r\n    // This pulls in all the operation implementations we need.\r\n    // NOLINTNEXTLINE(runtime-global-variables)\r\n    static tflite::ops::micro::AllOpsResolver resolver;\r\n\r\n    // Build an interpreter to run the model with.\r\n    static tflite::MicroInterpreter static_interpreter(\r\n            model, resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n    interpreter = &static_interpreter;\r\n\r\n    // Allocate memory from the tensor_arena for the model's tensors.\r\n    TfLiteStatus allocate_status = interpreter->AllocateTensors();\r\n    if (allocate_status != kTfLiteOk) {\r\n        error_reporter->Report(\"AllocateTensors() failed\");\r\n        return;\r\n    }\r\n\r\n    // Obtain pointers to the model's input and output tensors.\r\n    input = interpreter->input(0);\r\n\r\n    ESP_LOGI(\"TENSOR SETUP\", \"input size = %d\", input->dims->size);\r\n    ESP_LOGI(\"TENSOR SETUP\", \"input size in bytes = %d\", input->bytes);\r\n    ESP_LOGI(\"TENSOR SETUP\", \"Is input float32? = %s\", (input->type == kTfLiteFloat32) ? \"true\" : \"false\");\r\n    ESP_LOGI(\"TENSOR SETUP\", \"Input data dimentions = %d\",input->dims->data[1]);\r\n\r\n    output = interpreter->output(0);\r\n\r\n    ESP_LOGI(\"TENSOR SETUP\", \"output size = %d\", output->dims->size);\r\n    ESP_LOGI(\"TENSOR SETUP\", \"output size in bytes = %d\", output->bytes);\r\n    ESP_LOGI(\"TENSOR SETUP\", \"Is input float32? = %s\", (output->type == kTfLiteFloat32) ? \"true\" : \"false\");\r\n    ESP_LOGI(\"TENSOR SETUP\", \"Output data dimentions = %d\",output->dims->data[1]);\r\n\r\n}\r\n```\r\n```\r\nstatic bool setupDone = true;\r\n\r\nstatic void the_ai_algorithm_task(){\r\n\r\n    /* First time task is init setup the ai model */\r\n    if(setupDone == false){\r\n        setup();\r\n        setupDone = true;\r\n    }\r\n\r\n    /* Load the input data i.e deltaT1 and deltaT2 */\r\n    //int i = 0;\r\n    input->data.f[0] = 2.0;   /* Different values dont change the output */\r\n    input->data.f[1] = 3.2;   \r\n\r\n\r\n    // Run inference, and report any error\r\n    TfLiteStatus invoke_status = interpreter->Invoke();\r\n    if (invoke_status != kTfLiteOk) {\r\n        error_reporter->Report(\"Invoke failed\");\r\n        // return;\r\n    }\r\n\r\n    /* Retrieve outputs Fan , AC , Vent 1 , Vent 2 */\r\n    double fan = output->data.f[0];\r\n    double ac = output->data.f[1];\r\n    double vent1 = output->data.f[2];\r\n    double vent2 = output->data.f[3];\r\n\r\n\r\n    ESP_LOGI(\"TENSOR SETUP\", \"fan = %lf\", fan);\r\n    ESP_LOGI(\"TENSOR SETUP\", \"ac = %lf\", ac);\r\n    ESP_LOGI(\"TENSOR SETUP\", \"vent1 = %lf\", vent1);\r\n    ESP_LOGI(\"TENSOR SETUP\", \"vent2 = %lf\", vent2);\r\n    \r\n}\r\n```\r\nThe model seems to load ok as the dimensions and sizes are correct. But the output is always the same 4 values. The input are float32\r\n\r\n```\r\nfan = 0.0087\r\nac = 0.54\r\nvent1 = 0.73\r\nvent2 = 0.32\r\n```\r\nAny idea on what can be going wrong? Is it something about my model or am I just not using the model correctly in my c++ application?", "comments": ["Looking at your wrapper code I don't see anything that stands out immediately. Can you share the Collab notebook where you train the Keras model?\r\n\r\nAlso, how are you running the model? Before running on hardware I typically try to run on my host machine using:\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=posix test_<binary name> -j18\r\nwhere binary name is found in the Makefile.inc for your project, for example \"hello_world\" is declared:\r\n$(eval $(call microlite_test,hello_world,\\\r\n$(HELLO_WORLD_SRCS),$(HELLO_WORLD_HDRS)))\r\n\r\nRunning on the host can help speed up development and isolate platform-specific issues.", "@njeffrie , any update on this issue, @FerenczD did you manage to fix it?", "@njeffrie @eliethesaiyan I apologize forgot this issue was still open. I did solve the problem. It was coming from the creation of my model.\r\n\r\nOriginally I had 4 layers - 1 input, 1 output and 2 hidden layers with 32 and 20 nodes. Values for the hidden layers were picked randomly, to be honest.\r\n\r\nHowever, I changed the nodes of the hidden layer to be 16 and 8 for some reason it started to work on the ESP32 side."]}, {"number": 42008, "title": "assertion error on tf.random.normal() and tf.keras.layers.conv2D()", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: NO\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 64 bit\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: NO\r\n-   **TensorFlow installed from (source or binary)**: Source\r\n-   **TensorFlow version (use command below)**: 2.1.0\r\n-   **Python version**: 3.7.4\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: 10.1\r\n-   **GPU model and memory**: NVIDIA GeForce MX 250, 2GB\r\n-   **Exact command to reproduce**:\r\n** Following is the code I took from the official documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) after getting the exact here at tf.keras.layers.Conv2D traceback. \r\n![Capture](https://user-images.githubusercontent.com/53314831/89201564-50df9d00-d5cf-11ea-8f70-a0dd89fd182d.PNG)\r\n\r\ninput_shape = (4, 28, 28, 3)\r\nx = tf.random.normal(input_shape)\r\ny = tf.keras.layers.Conv2D(2, 3, activation='relu', input_shape=input_shape[1:])(x)\r\nprint(y.shape)\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-403-88a6eef2a73d> in <module>\r\n----> 1 tf.random.normal((2, 2))\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\random_ops.py in random_normal(shape, mean, stddev, dtype, seed, name)\r\n     67   \"\"\"\r\n     68   with ops.name_scope(name, \"random_normal\", [shape, mean, stddev]) as name:\r\n---> 69     shape_tensor = tensor_util.shape_tensor(shape)\r\n     70     mean_tensor = ops.convert_to_tensor(mean, dtype=dtype, name=\"mean\")\r\n     71     stddev_tensor = ops.convert_to_tensor(stddev, dtype=dtype, name=\"stddev\")\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py in shape_tensor(shape)\r\n    992       # not convertible to Tensors becasue of mixed content.\r\n    993       shape = tuple(map(tensor_shape.dimension_value, shape))\r\n--> 994   return ops.convert_to_tensor(shape, dtype=dtype, name=\"shape\")\r\n    995 \r\n    996 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1312 \r\n   1313     if ret is None:\r\n-> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1315 \r\n   1316     if ret is NotImplemented:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    315                                          as_ref=False):\r\n    316   _ = as_ref\r\n--> 317   return constant(v, dtype=dtype, name=name)\r\n    318 \r\n    319 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n    256   \"\"\"\r\n    257   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 258                         allow_broadcast=True)\r\n    259 \r\n    260 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    264   ctx = context.context()\r\n    265   if ctx.executing_eagerly():\r\n--> 266     t = convert_to_eager_tensor(value, ctx, dtype)\r\n    267     if shape is None:\r\n    268       return t\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     93     except AttributeError:\r\n     94       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n---> 95   ctx.ensure_initialized()\r\n     96   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     97 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\context.py in ensure_initialized(self)\r\n    491       if self._initialized:\r\n    492         return\r\n--> 493       assert self._context_devices is None\r\n    494       opts = pywrap_tensorflow.TFE_NewContextOptions()\r\n    495       try:\r\n\r\nAssertionError: \r\n\r\n", "comments": ["Can you test with tensorflow `2.2.0` or `2.3.0`?", "@Mrutyunjay01,\r\nThe code snippet you have provided is different from the one in the screenshot. Could you please provide the complete code you are running.\r\n\r\nAlso, I was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/80686f6f457d043f9b4b870c762a60d9/42008.ipynb#scrollTo=cjXyBOvJHupy). \r\n\r\nAs suggested by @bhack, please upgrade to TF v2.3 and check if you are facing the same error in a virtual environment. Thanks!", "@amahendrakar Yeah, When Conv2D didn't work, I tried running that simple one. But it showed exactly the same error.  However, using tf.compat.v1.disable_eager_execution() surprisingly suppressed the error. But, again I encountered\r\n![error](https://user-images.githubusercontent.com/53314831/89341212-94fd9b00-d6be-11ea-8726-f2e1194e33fe.PNG)\r\n the same here, Here I am attaching the snippet.", "If you have noticed, the error is at 493 line of last block, having this line\r\nassert self._context_devices is None.\r\nYeah, in google colab, it works fine. But what's the problem in my system? And how did it get suppress while using tf.disable_eager_execution(), but again gave error in the same environment?", "What Is this? Are you changing the code example again?\n\nCould you share a single very very minimal but runnable code and tell us  what kind of error you got?\n\nThanks", "Closing the issue as uninstalling and re-installing the updated version solved the issue.\r\n\r\n> @Mrutyunjay01,\r\n> The code snippet you have provided is different from the one in the screenshot. Could you please provide the complete code you are running.\r\n> \r\n> Also, I was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/80686f6f457d043f9b4b870c762a60d9/42008.ipynb#scrollTo=cjXyBOvJHupy).\r\n> \r\n> As suggested by @bhack, please upgrade to TF v2.3 and check if you are facing the same error in a virtual environment. Thanks!\r\n\r\nI am not getting any error now, but Kindly review the 493rd line of my snippet or comment. The code is \r\n- self._context_devices is None.\r\nAnd it has no assertion statement. If you could clarify the cause once, thank you."]}, {"number": 42007, "title": "There are some repeated calls in the data_augmentation.ipynb\uff01", "body": "The location of the problem\uff1a/docs/site/en/tutorials/images/data_augmentation.ipynb\r\nYou can find it according to the mark of the picture\uff1a\r\n![9371596469059_ pic_hd](https://user-images.githubusercontent.com/61530230/89200299-627e7000-d5e2-11ea-910e-50ee98f30cb8.jpg)\r\n---\r\n`image,label = convert(image, label)`  AND `image = tf.image.convert_image_dtype(image, tf.float32)`  are the same.", "comments": ["Can you prepare a small PR?", "OK\r\n\r\n\r\n\r\n\u53d1\u81ea\u6211\u7684iPhone\r\n\r\n\r\n------------------ Original ------------------\r\nFrom: bhack <notifications@github.com&gt;\r\nDate: Mon,Aug 3,2020 11:45 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com&gt;\r\nCc: \u963f\u57fa\u7c73\u5fb7 <os@orangestar.vip&gt;, Author <author@noreply.github.com&gt;\r\nSubject: Re: [tensorflow/tensorflow] There are some repeated calls in the data_augmentation.ipynb\uff01 (#42007)\r\n\r\n\r\n\r\n\r\n\r\n \r\nCan you prepare a small PR?\r\n \r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]