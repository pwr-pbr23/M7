[{"number": 40564, "title": "Fix distributed autocast variable assignment", "body": "This PR fixes assignment and variable updates of `AutoCastVariable` inside distribution strategies as mentioned in https://github.com/tensorflow/tensorflow/pull/40493#discussion_r440508322.\r\nThis makes `AutoCastVariable` work as expected in eager execution and inside TF functions. Distribution strategies in graph mode now remain the only case where `AutoCastVariable` doesn't fully follow the spec due to https://github.com/tensorflow/tensorflow/pull/40493#discussion_r440523571.\r\nThis PR should also remove the overhead of creating new `AutoCastVariable` instances on variable assignment in eager execution, though I haven't benchmarked this since this doesn't seem like a very hot code path during normal usage.\r\n\r\n@reedwm Would be great if you could take a look at this.", "comments": ["@alextp, can you confirm if auto control dependencies are always in effect when eager is enabled? That is, would the following be a valid implementation of a simplified `ResourceVariable.assign_add`?\r\n\r\n```python\r\n  def assign_add(self, delta, read_value=True)\r\n    if ops.executing_eagerly_outside_functions():\r\n      gen_resource_variable_ops.assign_add_variable_op(self.handle, delta)\r\n      return self if read_value else None\r\n    ... # Deal with graph mode case\r\n```\r\n\r\nThis PR effectively has the same logic as the code sample above.", "Yes, Reed, your example is correct.\n\nOn Fri, Jun 19, 2020 at 3:46 AM Lukas Geiger <notifications@github.com>\nwrote:\n\n> *@lgeiger* commented on this pull request.\n>\n> @reedwm <https://github.com/reedwm> Thanks for the review!\n> ------------------------------\n>\n> In\n> tensorflow/python/keras/mixed_precision/experimental/autocast_variable.py\n> <https://github.com/tensorflow/tensorflow/pull/40564#discussion_r442749427>\n> :\n>\n> > @@ -188,61 +188,88 @@ def initial_value(self):\n>\n>    def constraint(self):\n>\n>      return self._variable.constraint\n>\n>\n>\n> +  def _apply_assign_update(\n>\n> +      self, update_fn, value, use_locking=None, name=None, read_value=True):\n>\n> +    if not read_value:\n>\n> +      return update_fn(value, use_locking, name, read_value)\n>\n> +\n>\n> +    if context.executing_eagerly() or ops.inside_function():\n>\n>\n> This test\n> <https://github.com/lgeiger/tensorflow/blob/fix-distributed-autocast-var-assign/tensorflow/python/keras/mixed_precision/experimental/autocast_variable_test.py#L346-L358>\n> runs using tf functions in v1 session mode. If I switch to\n> ops.executing_eagerly_outside_functions() it wouldn't work anymore.\n> Though, I am happy to not support this usage if there are concerns about\n> the correctness there.\n> ------------------------------\n>\n> In\n> tensorflow/python/keras/mixed_precision/experimental/autocast_variable_test.py\n> <https://github.com/tensorflow/tensorflow/pull/40564#discussion_r442749585>\n> :\n>\n> > @@ -357,18 +371,18 @@ def test_assign_stays_in_true_dtype(self, distribution):\n>\n>            dtypes.float16):\n>\n>          # Variable should be increased, despite it appearing to be the same\n>\n>          # float16 value.\n>\n> -        self.assertEqual(1. + small_val,\n>\n> -                         self.evaluate(x.assign(1. + small_tensor)))\n>\n> +        self.evaluate(x.assign(1. + small_tensor))\n>\n> +        self.assertEqual(1. + small_val, self.evaluate(x._variable))\n>\n>\n> \ud83d\udc4d removed\n> ------------------------------\n>\n> In\n> tensorflow/python/keras/mixed_precision/experimental/autocast_variable.py\n> <https://github.com/tensorflow/tensorflow/pull/40564#discussion_r442766571>\n> :\n>\n> > @@ -188,61 +188,88 @@ def initial_value(self):\n>\n>    def constraint(self):\n>\n>      return self._variable.constraint\n>\n>\n>\n> +  def _apply_assign_update(\n>\n> +      self, update_fn, value, use_locking=None, name=None, read_value=True):\n>\n> +    if not read_value:\n>\n> +      return update_fn(value, use_locking, name, read_value)\n>\n> +\n>\n> +    if context.executing_eagerly() or ops.inside_function():\n>\n> +      assign_op = update_fn(value, use_locking, name, False)\n>\n> +      with ops.control_dependencies([assign_op]):\n>\n>\n> Luckily, I don't think it matters since tf.functions automatically create\n> control dependencies to ensure stateful ops are executed.\n>\n> \ud83d\udc4d Good point. I wasn't sure about that since ops.control_dependencies is\n> used quite a lot inside TensorFlow but I guess this is only for\n> compatibility reasons with legacy v1 graph mode then.\n>\n> Also I think we want to return None here if read_value is False, as is\n> done by tf.Variable.\n>\n> Looks like tf.Variable.assign might return an operations inside tf\n> functions:\n>\n> x = tf.Variable(1.0)\n>\n>\n> @tf.function\n> def foo():\n>\n>     print(type(x.assign(2.0, read_value=False)))\n>\n>\n> foo()  # <class 'tensorflow.python.framework.ops.Operation'>\n>\n> But without the need for control dependencies it still can be simplified\n> to return self if read_value else assign_op.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/40564#pullrequestreview-433954087>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPH35ZMCXRXUQPLAF3RXM6WXANCNFSM4OBNCRYQ>\n> .\n>\n\n\n-- \n - Alex\n", "It looks like this PR makes the integration tests fail with `TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.`. @reedwm Do you have an idea what could be the issue here?", "> It looks like this PR makes the integration tests fail with `TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.`. @reedwm Do you have an idea what could be the issue here?\r\n\r\n5b46965a7dbcb7d775d9bca1b6bc4ee4f4652101 fixes the issue for me, though I am not 100% sure if this is the correct approach since it changes the `FuncGraph` code.\r\n\r\n@alextp It looks like you added `control_captures` in 4b8740faf664cb795c89b8943a36dac640897539. Do you think it is fine to change `set` to `ObjectIdentitySet`, or are we missing something on the `AutoCastVariable` side?", "Changing set to ObjectIdentitySet is the right thing here I think\n\nOn Sat, Jun 20, 2020 at 5:47 AM Lukas Geiger <notifications@github.com>\nwrote:\n\n> It looks like this PR makes the integration tests fail with TypeError:\n> Variable is unhashable. Instead, use tensor.ref() as the key.. @reedwm\n> <https://github.com/reedwm> Do you have an idea what could be the issue\n> here?\n>\n> 5b46965\n> <https://github.com/tensorflow/tensorflow/commit/5b46965a7dbcb7d775d9bca1b6bc4ee4f4652101>\n> fixes the issue for me, though I am not 100% sure if this is the correct\n> approach since it changes the FuncGraph code.\n>\n> @alextp <https://github.com/alextp> It looks like you added\n> control_captures in 4b8740f\n> <https://github.com/tensorflow/tensorflow/commit/4b8740faf664cb795c89b8943a36dac640897539>.\n> Do you think it is fine to change set to ObjectIdentitySet, or are we\n> missing something on the AutoCastVariable side.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/40564#issuecomment-646990262>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNJHW6FIX5ANSAUL4DRXSVXLANCNFSM4OBNCRYQ>\n> .\n>\n\n\n-- \n - Alex\n", "This was rolled back in 8535dafb37ec4ce5c7272ffa4b8b4c491d44e999 since an internal test broke. I'm taking a look and will reply with more info soon", "Sorry for the delay, I'm still looking into the internal failure.\r\n\r\nDo you remember what integration tests you failed that was fixed by the ObjectIdentitySet? The error has something to do with the FuncGraph created in a while_loop.", "> Sorry for the delay, I'm still looking into the internal failure.\r\n\r\nNo worries, thanks for taking a look.\r\n\r\n> Do you remember what integration tests you failed that was fixed by the ObjectIdentitySet? The error has something to do with the FuncGraph created in a while_loop.\r\n\r\n[This was the faling test case](https://source.cloud.google.com/results/invocations/62ad6d14-53b0-469a-b8e7-5113f5707ca5/targets)", "> Do you remember what integration tests you failed that was fixed by the ObjectIdentitySet? The error has something to do with the FuncGraph created in a while_loop.\r\n\r\nI'm happy to help with debugging, @reedwm are you able to share more details?", "The main issue is this PR breaks metrics in while loops when the `mixed_bfloat16` policy is used with `TPUStrategy`. Here is an example to reproduce (requires a TPU).\r\n\r\n```python\r\nfrom absl import app\r\nimport tensorflow as tf\r\n\r\n\r\ndef main(_):\r\n  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver('')\r\n  tf.config.experimental_connect_to_cluster(cluster_resolver)\r\n  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\n  strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\r\n\r\n  tf.keras.mixed_precision.experimental.set_policy('mixed_bfloat16')\r\n\r\n  with strategy.scope():\r\n    metric = tf.keras.metrics.CategoricalCrossentropy()\r\n\r\n    @tf.function\r\n    def func():\r\n      def run_fn():\r\n        metric.update_state([[0, 1, 0], [0, 0, 1]],\r\n                            [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\r\n      def cond(i):\r\n        return i < 3\r\n\r\n      def body(i):\r\n        strategy.run(run_fn)\r\n        return i + 1\r\n\r\n      tf.while_loop(cond, body, [0])  # Update metric 3 times\r\n    func()\r\n\r\n\r\nif __name__ == '__main__':\r\n  app.run(main)\r\n```\r\n\r\nThe control dependency that fails is [here](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/ops/while_v2.py;l=261;drc=cfb0250c51dff6380d5e660d5f7fff920d3ffcc7). Unfortunately, I don't think you can easily modify TF code when running on a TPU. Here is a more contrived example to reproduce on a CPU. It's impossible to reproduce using a Metric in a while loop with `strategy.run` due to differences between `TPUStrategy` and `MirroredStrategy`, but it can be reproduced by directly assigning to a variable:\r\n\r\n```python\r\nfrom absl import app\r\nimport tensorflow as tf\r\n\r\n\r\ndef main(_):\r\n  # Split CPU into two logical devices for MirroredStrategy\r\n  physical_devices = tf.config.list_physical_devices('CPU')\r\n  tf.config.set_logical_device_configuration(\r\n      physical_devices[0],\r\n      [tf.config.LogicalDeviceConfiguration(),\r\n       tf.config.LogicalDeviceConfiguration()])\r\n\r\n  strat = tf.distribute.MirroredStrategy(['/cpu:0', '/cpu:1'])\r\n  tf.keras.mixed_precision.experimental.set_policy('mixed_float16')\r\n\r\n  with strat.scope():\r\n    # Get AutoCastDistributedVariable\r\n    layer = tf.keras.layers.Dense(10)\r\n    layer(tf.ones((10, 10)))\r\n    var = layer.kernel\r\n\r\n    @tf.function\r\n    def func():\r\n      update = var.assign_add(tf.ones((10, 10)))\r\n      with tf.control_dependencies([update]):\r\n        var.assign_add(tf.ones((10, 10)))\r\n    func()\r\n\r\n\r\nif __name__ == '__main__':\r\n  app.run(main)\r\n```\r\n\r\nIn both examples, the core issue is that `AutoCastDistributedVariable.assign_add` returns something which cannot to passed to `tf.control_dependencies`.  In the former example, this is done within TensorFlow while in the latter case, this is done explicitly in the example. The return value is the AutoCastDistributedVariable itself, which cannot be used as a control dependency.\r\n\r\nDistributedVariables return a tensor from `assign_add`, which can be used as a control dependency. As we discussed before, `DistributedVariable.assign_add` should return itself, which would also break these cases. I suppose this is one reason they haven't managed to make the change yet.\r\n\r\nMetrics probably shouldn't use AutoCastVariables at all, which can easily be fixed. But it doesn't fix the underlying issue, and users or other parts of TF may still use the return value of `AutoCastVariable.assign_add` as a control dependency.\r\n\r\nThere are three possible fixes:\r\n\r\n1. Try to remove every case of the return value of `Variable.assign_add` from control dependencies. I think this is probably infeasible, especially considering user code may do the same, but I'm not sure.\r\n2. Wait for the Strategy team to fix `DistributedVariable.assign_add` to return the variable, which will require them to allow `DistributedVariable` to be used in control dependencies (or for them to implement (1), which they may choose to do).\r\n3. Have `AutoCastDistributedVariable.assign_add` return something of the same type as `DistributedVariable.assign_add`. I think this is normally a tensor, but maybe it could be a `PerReplica` or some other type (I haven't checked, and it may depend on the strategy, cross-replica vs replica mode, etc). \r\n\r\n@guptapriya @crccw any thoughts? For (2), do you plan on allowing DistributedVariable to be used in control dependencies?\r\n\r\n---\r\n\r\nAnother issue is the output of `AutoCastVariable.assign` does not have an `op` attribute. This is easier to reproduce:\r\n\r\n```python\r\ntf.keras.mixed_precision.experimental.set_policy('mixed_float16')\r\nlayer = tf.keras.layers.Dense(10,)\r\nlayer.build((10, 10))\r\nprint(layer.kernel.assign_add(tf.ones((10, 10))).op)\r\n```\r\n\r\nNormal variables also do not have an `op` attribute, but the return value of `Variable.assign_add` does, and we must emulate that behavior to avoid breakages when `AutoCastVariable is used.\r\n\r\nThis is fixable by returning a subclass of `AutoCastVariable` which simply overrides `op` to return the `op` attribute of the return value of `AutoCastVariable._variable.assign_add`. But we still need to address the first issue.\r\n\r\nI will write tests to catch these issues.", "@reedwm Thank you very much for investigating and thanks for the reproducible example.\r\n\r\nFollowing the code path of `ops.control_dependencies()` it looks like in the end `ResourceVariable._as_graph_element` returns `None` which leads to the failure mentioned above:\r\nhttps://github.com/tensorflow/tensorflow/blob/b671b131f4f7b161d19f084c659b7b2971461806/tensorflow/python/ops/resource_variable_ops.py#L557-L559\r\nI am not so familiar with this code so I am not sure if this is expected or part of the problem.\r\n\r\nHowever, I can make your CPU example pass by overwriting `AutoCastVariable._as_graph_element` with:\r\n```python\r\ndef _as_graph_element(self):\r\n    return self.op\r\n```\r\nThough I have no idea if this retains the correct behaviour (I have to run the unittests over night to know more).", "That's a good workaround but we should only go with it if we do that in `DistributedVariable` as well. @guptapriya @crccw can we implement that workaround for distributed variables?", "> That's a good workaround but we should only go with it if we do that in DistributedVariable as well.\r\n\r\nGood point :+1:\r\n\r\nAnother workaround would be to change `ResourceVariable._as_graph_element()` from\r\nhttps://github.com/tensorflow/tensorflow/blob/35b3d903617ea8993555e59e627501f960698219/tensorflow/python/ops/resource_variable_ops.py#L557-L559\r\nto\r\n```python\r\n  def _as_graph_element(self):\r\n    \"\"\"Conversion function for Graph.as_graph_element().\"\"\"\r\n    if self._graph_element is not None:\r\n      return self._graph_element\r\n    return self._read_variable_op()\r\n```\r\n\r\nThis is just me playing around with the code so I don't know if this is a valid fix or if it has adverse effects that I am missing.", "_as_graph_element() is not allowed to add nodes to the graph, so reading\nthe variable there doesn't work.\n\nI'm surprised we're hitting as_graph_element here though since that only\ngets triggered on session.run (v1-only);.\n\nOn Tue, Jun 30, 2020 at 1:53 AM Lukas Geiger <notifications@github.com>\nwrote:\n\n> That's a good workaround but we should only go with it if we do that in\n> DistributedVariable as well.\n>\n> Good point \ud83d\udc4d\n>\n> Another workaround would be to change ResourceVariable._as_graph_element()\n> from\n>\n> https://github.com/tensorflow/tensorflow/blob/35b3d903617ea8993555e59e627501f960698219/tensorflow/python/ops/resource_variable_ops.py#L557-L559\n> to\n>\n>   def _as_graph_element(self):\n>\n>     \"\"\"Conversion function for Graph.as_graph_element().\"\"\"\n>\n>     if self._graph_element is not None:\n>\n>       return self._graph_element\n>\n>     return self._read_variable_op()\n>\n> This is just me playing around with the code so I don't know if this is a\n> valid fix or if it has adverse effects that I am missing.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/40564#issuecomment-651655630>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRITGFFFYOZTJKD2J6DRZGRXNANCNFSM4OBNCRYQ>\n> .\n>\n\n\n-- \n - Alex\n", "> _as_graph_element() is not allowed to add nodes to the graph, so reading\r\nthe variable there doesn't work.\r\n\r\nMakes sense, forgot about that.\r\n\r\n> I'm surprised we're hitting as_graph_element here though since that only\r\ngets triggered on session.run (v1-only);.\r\n\r\n@alextp I think we are hitting this since `ops.control_dependencies()` checks for `context.executing_eagerly()` which returns `False` inside a `tf.function`, so `Func_Graph.control_dependencies()` ends up calling `_as_graph_element()`:\r\nhttps://github.com/tensorflow/tensorflow/blob/f781cfb7e13a850e99ebf734ce4d4158e5563773/tensorflow/python/framework/ops.py#L5355-L5363\r\n\r\nWe could prevent this by changing the check to `executing_eagerly_outside_functions()` which is a valid workaround.\r\nHowever, would this preserve correct semantics inside tf functions?", "I think it would. Let's switch to executing_eagerly_outside_functions\n\n\nOn Tue, Jun 30, 2020 at 2:39 PM Lukas Geiger <notifications@github.com>\nwrote:\n\n> _as_graph_element() is not allowed to add nodes to the graph, so reading\n> the variable there doesn't work.\n>\n> Makes sense, forgot about that.\n>\n> I'm surprised we're hitting as_graph_element here though since that only\n> gets triggered on session.run (v1-only);.\n>\n> @alextp <https://github.com/alextp> I think we are hitting this since\n> ops.control_dependencies() checks for context.executing_eagerly() which\n> returns False inside a tf.function, so Func_Graph.control_dependencies()\n> ends up calling _as_graph_element():\n>\n> https://github.com/tensorflow/tensorflow/blob/f781cfb7e13a850e99ebf734ce4d4158e5563773/tensorflow/python/framework/ops.py#L5355-L5363\n>\n> We could prevent this by changing the check to\n> executing_eagerly_outside_functions() which is a valid workaround.\n> However, would this preserve correct semantics inside tf functions?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/40564#issuecomment-652060166>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNPORMWJAUV724B4FLRZJLQPANCNFSM4OBNCRYQ>\n> .\n>\n\n\n-- \n - Alex\n", "> I think it would. Let's switch to executing_eagerly_outside_functions\r\n\r\nSounds good. I made a separate PR for this change in https://github.com/tensorflow/tensorflow/pull/40960. If this passes CI I am happy to roll the rest of the changes forward.", "I just looked at the PR and I think it's too aggressive and makes it impossible to build control edges inside tf.function.\r\n\r\nCan we instead either not call _as_graph_element inside tf.function or just have _as_graph_element inside tf.function do a read (since tf.function doesn't have the limitation against making new nodes that v1 session.run has)", "The way normal tf.control_dependencies([v.assgin()]) works for normal variables is because of this line:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L4751\r\n\r\nHow about changing it to apply to all resource variable alike types? ", "> How about changing it to apply to all resource variable alike types?\r\n\r\nThis should work for the AutoCastVariable use case.\r\n\r\nDo you think about doing something like this?\r\n```diff\r\n--- a/tensorflow/python/framework/func_graph.py\r\n+++ b/tensorflow/python/framework/func_graph.py\r\n@@ -349,6 +349,8 @@ class FuncGraph(ops.Graph):\r\n       if (isinstance(c, ops.IndexedSlices) or\r\n           (hasattr(c, \"_handle\") and hasattr(c, \"op\"))):\r\n         c = c.op\r\n+      if resource_variable_ops.is_resource_variable(c):\r\n+        c = c.read_value()  # Alternative: array_ops.identity(c)\r\n       graph_element = ops._as_graph_element(c)  # pylint: disable=protected-access\r\n       if graph_element is None:\r\n         graph_element = c\r\n```", "That sounds good. It checks for a handle and op now to avoid circular\ndependencies.\n\nOn Tue, Jun 30, 2020 at 3:33 PM Ran Chen <notifications@github.com> wrote:\n\n> The way normal tf.control_dependencies([v.assgin()]) works for normal\n> variables is because of this line:\n>\n>\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L4751\n>\n> How about changing it to apply to all resource variable alike types?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/40564#issuecomment-652080941>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLGF2S6BSM2DSWUVVTRZJR2ZANCNFSM4OBNCRYQ>\n> .\n>\n\n\n-- \n - Alex\n", "I'm thinking of:\r\n\r\n       if (isinstance(c, ops.IndexedSlices) or \r\n           resource_variable_ops.is_resource_variable(c)):\r\n         c = c.op\r\n\r\nCurious, what's the circular dependencies issue?\r\n", "For me this makes the above example fail with `AttributeError: Tensor.op is meaningless when eager execution is enabled.`.\r\n\r\nThis is also why `AutoCastVariable` fails the `hasattr(c, \"op\")` check.", "ah I see. In this case we still need hasattr(c, \"op\"). Just replace hasattr(c, \"_handle\")  with resource_variable_ops.is_resource_variable(c)?", "Thanks for the help, I really appreciate it! I made a PR with the required changes to fix the testcase @reedwm mentioned above: #41008", "I resubmitted the changes with additional unittests to cover the failure in #41214"]}, {"number": 40563, "title": "New Model on Android not working", "body": "I have make new model build with:\r\nhttps://teachablemachine.withgoogle.com/\r\n\r\nI download example SDK from:\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android\r\n\r\nI put my model with 2 files on assets directory:\r\n1. model.tflite\r\n2. model_label.txt\r\n\r\nI make new class with extend to \"Classifier\". I replace this two methods:\r\n\r\n```\r\n  @Override\r\n  protected String getModelPath() {\r\n    return \"model.tflite\";\r\n  }\r\n\r\n  @Override\r\n  protected String getLabelPath() {\r\n    return \"model_label.txt\";\r\n  }\r\n```\r\nAnd then run the project. And I dont see any effect. If I use mobilnet model is working.\r\nI was test this model with tensorflow.js and using webcam, and its working, but why on android not working. I have miss it something? Pleaselet me know.\r\n\r\n", "comments": ["@tomeroto \r\nPlease share the tensorflow version, and simple sample code such that we can replicate the issue faced. or if possible share a colab gist withe the issue faced for us to analyse.", "Hello,\r\n\r\nBelow is my model that I use on project:\r\nhttps://drive.google.com/file/d/17mFDcUynbK1ctnRtLSgVfAyE2bkD-gmv/view?usp=sharing\r\n\r\nOn that archive have two tipe model:\r\n\r\n1. Floating Point\r\n-> facemask_unquant.tflite\r\n-> facemask_unquant_labels.txt\r\n\r\n2. Quantized\r\n-> facemask.tflite\r\n-> facemask_labels.txt\r\n\r\nI use this example code of tensorflow for android:\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android\r\n\r\nAnd I was follow the instruction, but nothing happen. Please help. Thank you.\r\n", "@tomeroto \r\nI do not have access to the drive shared, please share a colab gist or the code you ran for which the error was faced.", "Thank you for your response. THis is I make a github link for code:\r\n\r\nhttps://github.com/tomeroto/tensorflow_image_classifications\r\n\r\nPlease help. Thank you.", "@tomeroto Can you please share the files. The drive you shared is private, so I cannot download the files. Thanks!\r\n", "@jvishnuvardhan this for new link:\r\nhttps://drive.google.com/file/d/17mFDcUynbK1ctnRtLSgVfAyE2bkD-gmv/view?usp=sharing", "@tomeroto Can you please try the instructions provided in [this tutorial](https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#1) and let us know how it progresses. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40562, "title": "[AMP] Add `disable_amp_scope` to disable auto mixed precision cast", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\nThis allows disabling AMP autocast in the scope if someone want to force to run in a particular dtype, which has been supported in [PyTorch](https://pytorch.org/docs/master/amp.html).", "comments": ["Because we recommend the `tf.keras.mixed_precision` API over the auto_mixed_precision graph rewrite, and we plan on deprecating the auto_mixed_precision graph rewrite in the future, we are no longer extending the API for AMP. See the last comment of #28609 for more context. Sorry for not accepting this, and let me know if you have any questions."]}, {"number": 40561, "title": "MSVC 2017 NVIDIA CUDA 9.2.1.148 CUDNN 7.5.0.56 target //tensorflow:tensorflow_cc.dll fails to link", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.18362\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:r2.2\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:NA\r\n- Bazel version (if compiling from source): 2.0.0 via bazelisk\r\n- GCC/Compiler version (if compiling from source):MSVC 2017 Microsoft (R) C/C++ Optimizing Compiler Version 19.16.27027.1 for x64\r\n- CUDA/cuDNN version: 9.2.1.148/7.5.0.56\r\n- GPU model and memory:GTX 1060 6Gb\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n`r2.2` wont compile `tensorflow_cc.dll`\r\n\r\nopen  powershell\r\n```\r\ncd D:\\github\r\nmkdir attempt6\r\ncd attempt6\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r2.2\r\ncp .\\..\\..\\*.ps1\r\n. .\\buildenv.ps1\r\n. .\\build-tf.ps1\r\n```\r\n\r\nwhere the contents of `buildenv.ps1` are\r\n\r\n```\r\npushd 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Auxiliary\\Build\\'    \r\ncmd /c \"vcvarsall.bat x64 & set\" |\r\nforeach {\r\n  if ($_ -match \"=\") {\r\n    $v = $_.split(\"=\"); set-item -force -path \"ENV:\\$($v[0])\"  -value \"$($v[1])\"\r\n  }\r\n}\r\npopd\r\nwrite-host \"`nVisual Studio 2017 Command Prompt variables set.\" -ForegroundColor Yellow\r\n```\r\n\r\nand `build-tf.ps1` are\r\n\r\n```\r\n$ENV:USE_BAZEL_VERSION=\"2.0.0\"\r\n$ENV:PYTHON_BIN_PATH=\"C:/Users/user/AppData/Local/Programs/Python/Python36/python.exe\"\r\n$ENV:PYTHON_LIB_PATH=\"C:/Users/user/AppData/Local/Programs/Python/Python36/\"\r\n$ENV:Path += \";C:/msys64/usr/bin\"\r\n$ENV:Path += \";C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/bin\"\r\n$ENV:Path += \";C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/extras/CUPTI/libx64\"\r\n$ENV:Path += \";C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/cudnn-9.2-windows10-x64-v7.5.0.56/cuda/bin\"\r\n$ENV:BAZEL_SH = \"C:/msys64/usr/bin/bash.exe\"\r\n$ENV:CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/\"\r\n$ENV:TF_CUDA_VERSION=\"9.2\"\r\n$ENV:CUDNN_INSTALL_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/cudnn-9.2-windows10-x64-v7.5.0.56/cuda\"\r\n$ENV:TF_CUDA_COMPUTE_CAPABILITIES=\"6.0\"\r\n$ENV:TF_CUDNN_VERSION=\"7\"\r\n$ENV:TF_NCCL_VERSION=\"1\"\r\n$ENV:TF_CUDA_CLANG=\"0\"\r\n$ENV:TF_NEED_CUDA=\"1\"\r\n$ENV:TF_NEED_ROCM=\"0\"\r\n$ENV:TF_NEED_OPENCL_SYCL=\"0\"\r\n\r\n\r\n$params = \"configure.py\",\"\"\r\ncmd /c \"ECHO Y\" | & python.exe @params \r\nbazel.exe build --copt=-nvcc_options=disable-warnings --test_tag_filters=-no_oss,-gpu,-benchmark-test,-nomac,-no_mac --announce_rc --test_timeout 300,450,1200,3600 --test_size_filters=small,medium --define=override_eigen_strong_inline=true  --repository_cache=D:/bazel-cache --jobs=12 //tensorflow:tensorflow_cc.dll\r\n```\r\n**Any other info / logs**\r\n\r\n\r\nWould have expected a good result, only difference from recent build on AWS box with T4 hardware is CUDA 10.2 on AWS box and CUDA 9.2 on local box with GTX 1060 card\r\n\r\nError is\r\n\r\n```\r\nERROR: D:/github/attempt6/tensorflow/tensorflow/BUILD:710:1: Linking of rule '//tensorflow:tensorflow_cc.dll' failed (Exit 1120)\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll.if.lib and object bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll.if.exp\r\nbfc_allocator.lib(bfc_allocator.obj) : warning LNK4049: locally defined symbol ?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level) imported\r\ngraph_mgr.lib(graph_mgr.obj) : warning LNK4049: locally defined symbol ?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level) imported\r\nbatch_kernels.lo.lib(batch_kernels.obj) : warning LNK4049: locally defined symbol ?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level) imported\r\nsnapshot_util.lib(snapshot_util.obj) : warning LNK4049: locally defined symbol ?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level) imported\r\ncaptured_function.lib(captured_function.obj) : warning LNK4049: locally defined symbol ?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level) imported\r\ngrpc_master_service.lo.lib(grpc_master_service.obj) : warning LNK4217: locally defined symbol ?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level) imported in function \"public: __cdecl tensorflow::profiler::TraceMe::TraceMe<class <lambda_ff01698cf6bea26b6df62d500bf7591b> >(class <lambda_ff01698cf6bea26b6df62d500bf7591b>,int)\" (??$?0V<lambda_ff01698cf6bea26b6df62d500bf7591b>@@@TraceMe@profiler@tensorflow@@QEAA@V<lambda_ff01698cf6bea26b6df62d500bf7591b>@@H@Z)\r\neager_service_impl.lib(eager_service_impl.obj) : warning LNK4049: locally defined symbol ?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level) imported\r\nexecute.lib(execute.obj) : warning LNK4049: locally defined symbol ?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level) imported\r\nremote_tensor_handle_data.lib(remote_tensor_handle_data.obj) : warning LNK4049: locally defined symbol ?g_trace_level@internal@profiler@tensorflow@@3U?$atomic@H@std@@A (struct std::atomic<int> tensorflow::profiler::internal::g_trace_level) imported\r\nmemory_optimizer.lib(memory_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\npin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nutils.lib(utils.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\ncollective_param_resolver_distributed.lib(collective_param_resolver_distributed.obj) : warning LNK4217: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported in function \"public: __cdecl tensorflow::CollGroupParams::CollGroupParams(void)\" (??0CollGroupParams@tensorflow@@QEAA@XZ)\r\nbatch_kernels.lo.lib(batch_kernels.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\ncaptured_function.lib(captured_function.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\narithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\ntfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_NewStatus imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4049: locally defined symbol TF_NewStatus imported\r\ntfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_DeleteStatus imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4049: locally defined symbol TF_DeleteStatus imported\r\ntfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_GetCode imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4049: locally defined symbol TF_GetCode imported\r\ntf_tensor.lib(tf_tensor.obj) : warning LNK4049: locally defined symbol TF_GetCode imported\r\ntfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_Message imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4049: locally defined symbol TF_Message imported\r\nutils.lib(utils.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\narithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4217: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported in function \"private: bool __cdecl tensorflow::grappler::`anonymous namespace'::ReorderCastLikeAndValuePreserving::NodeIsOnCpuOrGpu(class tensorflow::NodeDef const *)const \" (?NodeIsOnCpuOrGpu@ReorderCastLikeAndValuePreserving@?A0x41d86e96@grappler@tensorflow@@AEBA_NPEBVNodeDef@4@@Z)\r\nauto_mixed_precision.lib(auto_mixed_precision.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nmemory_optimizer.lib(memory_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\npin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_DataTypeSize imported in function \"void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)\" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)\r\ntf_tensor.lib(tf_tensor.obj) : warning LNK4049: locally defined symbol TF_DataTypeSize imported\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_SetStatus imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\ntf_tensor.lib(tf_tensor.obj) : warning LNK4049: locally defined symbol TF_SetStatus imported\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_NewOpDefinitionBuilder imported in function \"public: bool __cdecl <lambda_e001034d847de908815119ad3d529795>::operator()(void)const \" (??R<lambda_e001034d847de908815119ad3d529795>@@QEBA_NXZ)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_RegisterOpDefinition imported in function \"public: bool __cdecl <lambda_e001034d847de908815119ad3d529795>::operator()(void)const \" (??R<lambda_e001034d847de908815119ad3d529795>@@QEBA_NXZ)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_OpDefinitionBuilderAddAttr imported in function \"public: bool __cdecl <lambda_e001034d847de908815119ad3d529795>::operator()(void)const \" (??R<lambda_e001034d847de908815119ad3d529795>@@QEBA_NXZ)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_OpDefinitionBuilderAddInput imported in function \"public: bool __cdecl <lambda_e001034d847de908815119ad3d529795>::operator()(void)const \" (??R<lambda_e001034d847de908815119ad3d529795>@@QEBA_NXZ)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_OpDefinitionBuilderAddOutput imported in function \"public: bool __cdecl <lambda_e001034d847de908815119ad3d529795>::operator()(void)const \" (??R<lambda_e001034d847de908815119ad3d529795>@@QEBA_NXZ)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_OpDefinitionBuilderSetShapeInferenceFunction imported in function \"public: bool __cdecl <lambda_e001034d847de908815119ad3d529795>::operator()(void)const \" (??R<lambda_e001034d847de908815119ad3d529795>@@QEBA_NXZ)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_NewShapeHandle imported in function \"void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)\" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_ShapeInferenceContextGetInput imported in function \"void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)\" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_ShapeInferenceContextSetOutput imported in function \"void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)\" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_ShapeInferenceContextVectorFromSize imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_NewDimensionHandle imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_ShapeInferenceContext_GetAttrType imported in function \"void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)\" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_ShapeInferenceContextRankKnown imported in function \"void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)\" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_ShapeInferenceContextWithRankAtLeast imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_ShapeInferenceContextDim imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_ShapeInferenceContextSubshape imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_ShapeInferenceContextSetUnknownShape imported in function \"void __cdecl bitcast_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)\" (?bitcast_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_DimensionHandleValueKnown imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_DimensionHandleValue imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_ShapeInferenceContextConcatenateShapes imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_DeleteShapeHandle imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\nbitcast_op_lib.lo.lib(bitcast.obj) : warning LNK4217: locally defined symbol TF_DeleteDimensionHandle imported in function \"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,unsigned __int64,unsigned __int64,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@_K2PEAUTF_Status@@@Z)\r\ngpu_activation.lib(gpu_activation.obj) : error LNK2019: unresolved external symbol \"class stream_executor::gpu::GpuContext * __cdecl stream_executor::gpu::ExtractGpuContext(class stream_executor::gpu::GpuExecutor *)\" (?ExtractGpuContext@gpu@stream_executor@@YAPEAVGpuContext@12@PEAVGpuExecutor@12@@Z) referenced in function \"public: __cdecl stream_executor::gpu::ScopedActivateExecutorContext::ScopedActivateExecutorContext(class stream_executor::gpu::GpuExecutor *)\" (??0ScopedActivateExecutorContext@gpu@stream_executor@@QEAA@PEAVGpuExecutor@12@@Z)\r\ngpu_activation.lib(gpu_activation.obj) : error LNK2019: unresolved external symbol \"class stream_executor::gpu::GpuExecutor * __cdecl stream_executor::gpu::ExtractGpuExecutor(class stream_executor::StreamExecutor *)\" (?ExtractGpuExecutor@gpu@stream_executor@@YAPEAVGpuExecutor@12@PEAVStreamExecutor@2@@Z) referenced in function \"public: __cdecl stream_executor::gpu::ScopedActivateExecutorContext::ScopedActivateExecutorContext(class stream_executor::StreamExecutor *)\" (??0ScopedActivateExecutorContext@gpu@stream_executor@@QEAA@PEAVStreamExecutor@2@@Z)\r\nbazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll : fatal error LNK1120: 2 unresolved externals\r\nTarget //tensorflow:tensorflow_cc.dll failed to build\r\nINFO: Elapsed time: 2607.289s, Critical Path: 349.09s\r\nINFO: 4393 processes: 4393 local.\r\n```\r\n\r\nnote\r\n\r\n```\r\ngpu_activation.lib(gpu_activation.obj) : error LNK2019: unresolved external symbol \"class stream_executor::gpu::GpuContext * __cdecl stream_executor::gpu::ExtractGpuContext(class stream_executor::gpu::GpuExecutor *)\" (?ExtractGpuContext@gpu@stream_executor@@YAPEAVGpuContext@12@PEAVGpuExecutor@12@@Z) referenced in function \"public: __cdecl stream_executor::gpu::ScopedActivateExecutorContext::ScopedActivateExecutorContext(class stream_executor::gpu::GpuExecutor *)\" (??0ScopedActivateExecutorContext@gpu@stream_executor@@QEAA@PEAVGpuExecutor@12@@Z)\r\ngpu_activation.lib(gpu_activation.obj) : error LNK2019: unresolved external symbol \"class stream_executor::gpu::GpuExecutor * __cdecl stream_executor::gpu::ExtractGpuExecutor(class stream_executor::StreamExecutor *)\" (?ExtractGpuExecutor@gpu@stream_executor@@YAPEAVGpuExecutor@12@PEAVStreamExecutor@2@@Z) referenced in function \"public: __cdecl stream_executor::gpu::ScopedActivateExecutorContext::ScopedActivateExecutorContext(class stream_executor::StreamExecutor *)\" (??0ScopedActivateExecutorContext@gpu@stream_executor@@QEAA@PEAVStreamExecutor@2@@Z)\r\nbazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll : fatal error LNK1120: 2 unresolved externals\r\n```\r\n\r\nAdding `*stream_executor*` to the appropriate `.lds` files might fix the problem", "comments": ["trying again with\r\n\r\n```\r\nPS D:\\github\\attempt6\\tensorflow> git diff\r\ndiff --git a/tensorflow/tf_exported_symbols.lds b/tensorflow/tf_exported_symbols.lds\r\nindex 8bbd4199f8..0c391e54dd 100644\r\n--- a/tensorflow/tf_exported_symbols.lds\r\n+++ b/tensorflow/tf_exported_symbols.lds\r\n@@ -7,4 +7,7 @@\r\n *TFE_*\r\n *nsync_*\r\n *stream_executor*\r\n+*GPUContext*\r\n+*GPUExecutor*\r\n+*GPUOptions*\r\n *xla*\r\ndiff --git a/tensorflow/tf_version_script.lds b/tensorflow/tf_version_script.lds\r\nindex 303ba98b9a..f4a3df29b4 100644\r\n--- a/tensorflow/tf_version_script.lds\r\n+++ b/tensorflow/tf_version_script.lds\r\n@@ -10,6 +10,9 @@ tensorflow {\r\n     *nsync_*;\r\n     *stream_executor*;\r\n     *xla*;\r\n+    *GPUContext*;\r\n+    *GPUExecutor*;\r\n+    *GPUOptions*;\r\n   local:\r\n     *;\r\n };\r\n```", "That didnt work, trying again with `r1.14` branch\r\n\r\nI contacted @mihaimaruseac about which bazel version goes with `r1.15` to see if that works.", "`r1.14` is a fail with the recommended bazel `0.24.1` at least on this rig.\r\n\r\nsee\r\n\r\n```\r\nERROR: D:/github/attempt8/tensorflow/tensorflow/core/BUILD:2856:1: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/users/user/_bazel_user/i6smy5mm/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.16299.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET TEMP=C:\\Users\\user\\AppData\\Local\\Temp\r\n    SET TMP=C:\\Users\\user\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/host/genfiles /Ibazel-out/host/bin /Iexternal/com_google_absl /Ibazel-out/host/genfiles/external/com_google_absl /Ibazel-out/host/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/host/genfiles/external/nsync /Ibazel-out/host/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/host/genfiles/external/eigen_archive /Ibazel-out/host/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/host/genfiles/external/local_config_sycl /Ibazel-out/host/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/host/genfiles/external/gif_archive /Ibazel-out/host/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/host/genfiles/external/jpeg /Ibazel-out/host/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/host/genfiles/external/protobuf_archive /Ibazel-out/host/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/host/genfiles/external/com_googlesource_code_re2 /Ibazel-out/host/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/host/genfiles/external/farmhash_archive /Ibazel-out/host/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/host/genfiles/external/fft2d /Ibazel-out/host/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/host/genfiles/external/highwayhash /Ibazel-out/host/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/host/genfiles/external/zlib_archive /Ibazel-out/host/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/host/genfiles/external/double_conversion /Ibazel-out/host/bin/external/double_conversion /Iexternal/local_config_cuda /Ibazel-out/host/genfiles/external/local_config_cuda /Ibazel-out/host/bin/external/local_config_cuda /Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Iexternal/nsync/public /Ibazel-out/host/genfiles/external/nsync/public /Ibazel-out/host/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/host/genfiles/external/eigen_archive /Ibazel-out/host/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/host/genfiles/external/gif_archive/lib /Ibazel-out/host/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/host/genfiles/external/gif_archive/windows /Ibazel-out/host/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/host/genfiles/external/protobuf_archive/src /Ibazel-out/host/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/host/genfiles/external/farmhash_archive/src /Ibazel-out/host/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/host/genfiles/external/zlib_archive /Ibazel-out/host/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/host/genfiles/external/double_conversion /Ibazel-out/host/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/host/genfiles/external/local_config_cuda/cuda /Ibazel-out/host/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/host/bin/external/local_config_cuda/cuda/cuda/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY /Fobazel-out/host/bin/tensorflow/core/_objs/framework_internal_impl/function.obj /c tensorflow/core/framework/function.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(103): warning C4005: 'AF_IPX': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(457): note: see previous definition of 'AF_IPX'C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(144): warning C4005: 'AF_MAX': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(476): note: see previous definition of 'AF_MAX'C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(185): warning C4005: 'SO_DONTLINGER': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(399): note: see previous definition of 'SO_DONTLINGER'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(235): error C2011: 'sockaddr': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(1007): note: see declaration of 'sockaddr'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(437): error C2059: syntax error: 'constant'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(437): error C3805: 'constant': unexpected token, expected either '}' or a ','\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(572): warning C4005: 'IN_CLASSA': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(284): note: see previous definition of 'IN_CLASSA'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(578): warning C4005: 'IN_CLASSB': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(290): note: see previous definition of 'IN_CLASSB'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(584): warning C4005: 'IN_CLASSC': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(296): note: see previous definition of 'IN_CLASSC'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(595): warning C4005: 'INADDR_ANY': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(301): note: see previous definition of 'INADDR_ANY'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(597): warning C4005: 'INADDR_BROADCAST': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(303): note: see previous definition of 'INADDR_BROADCAST'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\shared\\ws2def.h(633): error C2011: 'sockaddr_in': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(1011): note: see declaration of 'sockaddr_in'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(136): error C2011: 'fd_set': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(1019): note: see declaration of 'fd_set'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(156): warning C4005: 'FD_CLR': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(94): note: see previous definition of 'FD_CLR'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(171): warning C4005: 'FD_SET': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(99): note: see previous definition of 'FD_SET'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(180): error C2011: 'timeval': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(1035): note: see declaration of 'timeval'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(236): error C2011: 'hostent': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(1023): note: see declaration of 'hostent'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(249): error C2011: 'netent': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(177): note: see declaration of 'netent'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(256): error C2011: 'servent': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(1027): note: see declaration of 'servent'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(268): error C2011: 'protoent': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(1031): note: see declaration of 'protoent'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(364): error C2011: 'WSAData': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(319): note: see declaration of 'WSAData'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(462): error C2011: 'sockproto': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(491): note: see declaration of 'sockproto'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(504): error C2011: 'linger': 'struct' type redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(1015): note: see declaration of 'linger'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(517): warning C4005: 'SOMAXCONN': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(541): note: see previous definition of 'SOMAXCONN'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(551): warning C4005: 'FD_READ': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(559): note: see previous definition of 'FD_READ'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(554): warning C4005: 'FD_WRITE': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(560): note: see previous definition of 'FD_WRITE'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(557): warning C4005: 'FD_OOB': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(561): note: see previous definition of 'FD_OOB'C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(560): warning C4005: 'FD_ACCEPT': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(562): note: see previous definition of 'FD_ACCEPT'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(563): warning C4005: 'FD_CONNECT': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(563): note: see previous definition of 'FD_CONNECT'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(566): warning C4005: 'FD_CLOSE': macro redefinition\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(564): note: see previous definition of 'FD_CLOSE'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1624): error C2375: 'accept': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(739): note: see declaration of 'accept'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1646): error C2375: 'bind': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(744): note: see declaration of 'bind'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1667): error C2375: 'closesocket': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(749): note: see declaration of 'closesocket'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1684): error C2375: 'connect': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(751): note: see declaration of 'connect'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1705): error C2375: 'ioctlsocket': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(756): note: see declaration of 'ioctlsocket'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1728): error C2375: 'getpeername': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(761): note: see declaration of 'getpeername'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1749): error C2375: 'getsockname': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(766): note: see declaration of 'getsockname'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1770): error C2375: 'getsockopt': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(771): note: see declaration of 'getsockopt'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1795): error C2375: 'htonl': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(778): note: see declaration of 'htonl'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1812): error C2375: 'htons': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(780): note: see declaration of 'htons'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1830): error C2375: 'inet_addr': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(782): note: see declaration of 'inet_addr'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1848): error C2375: 'inet_ntoa': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(784): note: see declaration of 'inet_ntoa'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1948): error C2375: 'listen': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(786): note: see declaration of 'listen'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1967): error C2375: 'ntohl': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(790): note: see declaration of 'ntohl'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(1984): error C2375: 'ntohs': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(792): note: see declaration of 'ntohs'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2001): error C2375: 'recv': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(794): note: see declaration of 'recv'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2024): error C2375: 'recvfrom': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(800): note: see declaration of 'recvfrom'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2051): error C2375: 'select': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(808): note: see declaration of 'select'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2076): error C2375: 'send': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(815): note: see declaration of 'send'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2099): error C2375: 'sendto': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(821): note: see declaration of 'sendto'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2126): error C2375: 'setsockopt': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(829): note: see declaration of 'setsockopt'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2151): error C2375: 'shutdown': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(836): note: see declaration of 'shutdown'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2171): error C2375: 'socket': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(840): note: see declaration of 'socket'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2196): error C2375: 'gethostbyaddr': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(847): note: see declaration of 'gethostbyaddr'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2218): error C2375: 'gethostbyname': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(852): note: see declaration of 'gethostbyname'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2235): error C2375: 'gethostname': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(854): note: see declaration of 'gethostname'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2275): error C2375: 'getservbyport': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(858): note: see declaration of 'getservbyport'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2294): error C2375: 'getservbyname': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(862): note: see declaration of 'getservbyname'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2313): error C2375: 'getprotobynumber': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(866): note: see declaration of 'getprotobynumber'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2330): error C2375: 'getprotobyname': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(868): note: see declaration of 'getprotobyname'C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2350): error C2375: 'WSAStartup': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(872): note: see declaration of 'WSAStartup'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2370): error C2375: 'WSACleanup': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(876): note: see declaration of 'WSACleanup'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2387): error C2375: 'WSASetLastError': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(878): note: see declaration of 'WSASetLastError'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2404): error C2375: 'WSAGetLastError': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(880): note: see declaration of 'WSAGetLastError'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2425): error C2375: 'WSAIsBlocking': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(882): note: see declaration of 'WSAIsBlocking'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2443): error C2375: 'WSAUnhookBlockingHook': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(884): note: see declaration of 'WSAUnhookBlockingHook'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2461): error C2375: 'WSASetBlockingHook': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(886): note: see declaration of 'WSASetBlockingHook'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2479): error C2375: 'WSACancelBlockingCall': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(888): note: see declaration of 'WSACancelBlockingCall'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2497): error C2375: 'WSAAsyncGetServByName': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(890): note: see declaration of 'WSAAsyncGetServByName'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2525): error C2375: 'WSAAsyncGetServByPort': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(898): note: see declaration of 'WSAAsyncGetServByPort'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2553): error C2375: 'WSAAsyncGetProtoByName': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(906): note: see declaration of 'WSAAsyncGetProtoByName'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2579): error C2375: 'WSAAsyncGetProtoByNumber': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(913): note: see declaration of 'WSAAsyncGetProtoByNumber'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2605): error C2375: 'WSAAsyncGetHostByName': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(920): note: see declaration of 'WSAAsyncGetHostByName'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2631): error C2375: 'WSAAsyncGetHostByAddr': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(927): note: see declaration of 'WSAAsyncGetHostByAddr'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2661): error C2375: 'WSACancelAsyncRequest': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(936): note: see declaration of 'WSACancelAsyncRequest'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock2.h(2679): error C2375: 'WSAAsyncSelect': redefinition; different linkage\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\um\\winsock.h(938): note: see declaration of 'WSAAsyncSelect'Target //tensorflow:tensorflow_cc.dll failed to build\r\nINFO: Elapsed time: 65.748s, Critical Path: 28.93s\r\nINFO: 359 processes: 359 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nrelates to \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/5165\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/46110c5aecfafcce220252bb8455b6188389b91d\r\n\r\nI am sort of wondering if I should have spun up a new windows image this morning.", "Unfortunately 1.14 release was made in a bad state (new release engineer, didn't completely follow procedures, we discovered the issue too late after the release was finished). I'd recommend not using 1.14 at all.", "@mihaimaruseac \r\n\r\nThanks for the update, I will either select `r1.12` which I know where it is at\r\n\r\n or `v1.15.3`/`r1.15` with bazel `0.26.1` I know that you gave me the heads up the bazel version is in the `config.py` script.\r\n\r\nBut the info is lacking here:\r\n\r\nhttps://www.tensorflow.org/install/source_windows\r\n\r\nand `r1.14` is listed but `r1.15` is not so thanks for the hot tip that `r1.14` is in a mess", "`v1.15.3` with `MSVC 2017` and `CUDA 9.2.1.148` `CUDNN 7.5.0.56`\r\n\r\nEnds like this\r\n\r\n```\r\nERROR: D:/github/attempt9/tensorflow/tensorflow/stream_executor/BUILD:498:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor_internal' failed (Exit 2)\r\n.\\tensorflow/stream_executor/rng.h(66): error C2589: 'constant': illegal token on right side of '::'\r\n.\\tensorflow/stream_executor/rng.h(66): error C2062: type 'unknown-type' unexpected\r\n.\\tensorflow/stream_executor/rng.h(72): error C2589: 'constant': illegal token on right side of '::'\r\n.\\tensorflow/stream_executor/rng.h(72): error C2062: type 'unknown-type' unexpected\r\nINFO: Elapsed time: 679.350s, Critical Path: 67.34s\r\nINFO: 2268 processes: 2268 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "XLA is no go on Windos\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/30415", "on branch tensorflow/tensorflow `v1.15.3` using bazel `0.26.1` and `MSVC 2017` `CUDA 9.2.1.148` and `CUDNN 7.5.0.56` as follows\r\n\r\n```\r\n$ENV:USE_BAZEL_VERSION=\"0.26.1\"\r\n$ENV:PYTHON_BIN_PATH=\"C:/Users/user/AppData/Local/Programs/Python/Python36/python.exe\"\r\n$ENV:PYTHON_LIB_PATH=\"C:/Users/user/AppData/Local/Programs/Python/Python36/\"\r\n$ENV:Path += \";C:/msys64/usr/bin\"\r\n$ENV:Path += \";C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/bin\"\r\n$ENV:Path += \";C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/extras/CUPTI/libx64\"\r\n$ENV:Path += \";C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/cudnn-9.2-windows10-x64-v7.5.0.56/cuda/bin\"\r\n$ENV:BAZEL_SH = \"C:/msys64/usr/bin/bash.exe\"\r\n$ENV:CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/\"\r\n$ENV:TF_CUDA_VERSION=\"9.2\"\r\n$ENV:CUDNN_INSTALL_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/cudnn-9.2-windows10-x64-v7.5.0.56/cuda\"\r\n$ENV:TF_CUDA_COMPUTE_CAPABILITIES=\"6.0\"\r\n$ENV:TF_CUDNN_VERSION=\"7\"\r\n$ENV:TF_NCCL_VERSION=\"1\"\r\n$ENV:TF_CUDA_CLANG=\"0\"\r\n$ENV:TF_NEED_CUDA=\"1\"\r\n$ENV:TF_NEED_ROCM=\"0\"\r\n$ENV:TF_NEED_OPENCL_SYCL=\"0\"\r\n$ENV:TF_ENABLE_XLA=\"0\"\r\n$ENV:TF_ENABLE_AOT=\"0\"\r\n\r\n\r\n$params = \"configure.py\",\"\"\r\ncmd /c \"ECHO Y\" | & python.exe @params \r\nbazel.exe build --copt=-nvcc_options=disable-warnings --test_tag_filters=-no_oss,-gpu,-benchmark-test,-nomac,-no_mac --announce_rc --test_timeout 300,450,1200,3600 --test_size_filters=small,medium --define=override_eigen_strong_inline=true  --repository_cache=D:/bazel-cache --jobs=12 //tensorflow:tensorflow_cc.dll //tensorflow:tensorflow_cc.lib\r\n```", "That failed also it seems that `MSVC 2015` is needed for < `r2.0` so I am going to try `r2.0` now and see if I get a new set of errors with `MSVC 2017`", "`r2.0` is consistent with `v1.15.3`\r\n\r\n```\r\nERROR: D:/github/attempt11/tensorflow/tensorflow/stream_executor/BUILD:498:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor_internal' failed (Exit 2)\r\ncl : Command line warning D9002 : ignoring unknown option '-nvcc_options=disable-warnings'\r\n.\\tensorflow/stream_executor/rng.h(66): error C2589: 'constant': illegal token on right side of '::'\r\n.\\tensorflow/stream_executor/rng.h(66): error C2062: type 'unknown-type' unexpected\r\n.\\tensorflow/stream_executor/rng.h(72): error C2589: 'constant': illegal token on right side of '::'\r\n.\\tensorflow/stream_executor/rng.h(72): error C2062: type 'unknown-type' unexpected\r\nINFO: Elapsed time: 686.288s, Critical Path: 87.99s\r\nINFO: 1961 processes: 1961 local.\r\nFAILED: Build did NOT complete successfully\r\nPS D:\\github\\attempt11\\tensorflow>                                                                                      PS D:\\github\\attempt11\\tensorflow> git status                                                                           On branch r2.0\r\n```", "`r2.0` is no good for `MSVC 2017` needs `MSVC 2019`\r\n\r\n see:\r\nhttps://github.com/microsoft/vcpkg/issues/7995\r\n\r\nSoftware I am pluggin into needs to be compatible with `CUDA 9.2.1.148` and `CUDNN 7.5.0.56`\r\n\r\nand `nvcc` needs `MSVC 2017` see https://docs.nvidia.com/cuda/archive/9.2/cuda-toolkit-release-notes/index.html#cuda-compiler-new-features\r\n\r\nBut I also know from experience that `MSVC 2015` works fine with `nvcc` from CUDA Toolkit `9.2.1.148`\r\n\r\nI also know that `nvcc` from CUDA `9.2.1.148` doesn't work with `MSVC 2019` ", "I know `r1.12` works with `MSVC 2015` and `CUDA 9.2.1.148` and `CUDNN 7.5.0.56` I might just stick with that.", "> @mihaimaruseac\r\n> \r\n> Thanks for the update, I will either select `r1.12` which I know where it is at\r\n> \r\n> or `v1.15.3`/`r1.15` with bazel `0.26.1` I know that you gave me the heads up the bazel version is in the `config.py` script.\r\n> \r\n> But the info is lacking here:\r\n> \r\n> https://www.tensorflow.org/install/source_windows\r\n> \r\n> and `r1.14` is listed but `r1.15` is not so thanks for the hot tip that `r1.14` is in a mess\r\n\r\nI will add the info there, thank you for pointing the lack of info.", "@av8ramit @bmzhao did our fixes for libtensorflow go into the 2.2 branch? Or do we expect build failures in 2.2 branch.", "They were not backported. The newly cut 2.3 branch should have them though", "If you need the cherrypick them, the list is tagged on the internal bug.", "Heads up, this is libtensorflow_cc, not libtensorflow. libtensorflow_cc corresponds to the TF 1 C++ API, whereas libtensorflow is the C API. av8ramit and I are currently focused on maintaining the C API.", "I think we had similar build issues with libtensorflow_cc as well, right? AFAIK, both libtensorflow and libtensorflow_cc releases stopped once the maintainers left.", "Just to clarify we have only resurrected the libtensorflow (C API) build. I have not looked into libtensorflow_cc. I'm not sure what the build issues there are. ", "The build issues are identified by building the target and getting error messages.\r\n\r\nSome of them are listed on this issue.", "As you mentioned could not reproduce the issue on my box, where I have cuda 10.1.\r\nI will try to see if I can get a machine with msvc 2017 + cuda 9.2, as cuda 9.2 does not support msvc 2019, and thus fails to build.", "From what I can tell, there are compiler bugs in MSVC 2017 that prevent compilation of TF 2.2:\r\n\r\n```\r\nERROR: C:/users/gunan/workspace/tensorflow/tensorflow/compiler/xla/BUILD:431:1: C++ compilation of rule '//tensorflow/compiler/xla:literal' failed (Exit 3)\r\nc:\\users\\gunan\\_bazel_gunan\\uigt6zie\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1136) : fatal error C1001: An internal error has occurred in the compiler.\r\n(compiler file 'd:\\agent\\_work\\3\\s\\src\\vctools\\compiler\\utc\\src\\p2\\main.c', line 187)\r\n To work around this problem, try simplifying or changing the program near the locations listed above.\r\nPlease choose the Technical Support command on the Visual C++\r\n Help menu, or open the Technical Support help file for more information\r\n```\r\n\r\nAre you restricted to MSVC 2017, or can you use MSVC 2019?", "The restriction is CUDA 9.2.1.142 and nvcc supports MSVC 2017\r\n\r\n\r\nBut static linking also needs to be fixed", "@meteorcloudy @gunan \r\nIs this relevant ?\r\n\r\nhttps://stackoverflow.com/questions/35153918/why-cant-the-visual-studio-linker-open-a-very-big-static-library-2-5gb", "We no longer support TF that builds with CUDA 9", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40561\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40561\">No</a>\n"]}, {"number": 40560, "title": "JAVA DataType support float16?", "body": "use python to train a model has float16.but cant reference by java.DataType 19 is not recognized in Java \r\nhttps://www.tensorflow.org/api_docs/java/reference/org/tensorflow/DataType", "comments": ["This DataType is depreciated , please check the latest docs https://www.tensorflow.org/api_docs/java/org/tensorflow/DataType . Thank you "]}, {"number": 40559, "title": "populateTypeInferenceInfo hits assertion when generating type inference interface for tf_ops", "body": "mlir-tblgen: external/llvm-project/mlir/lib/TableGen/Type.cpp:22: mlir::tblgen::TypeConstraint::TypeConstraint(const llvm::Record*): Assertion `def->isSubClassOf(\"TypeConstraint\") && \"must be subclass of TableGen 'TypeConstraint' class\"' failed.\r\n\r\n\r\n**System information**\r\n- Linux Ubuntu 20.04\r\n- TensorFlow installed from source\r\n- TensorFlow version: Latest from source\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? NO\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: GeForce RTX 2070 8G\r\n\r\nbazel build --config=opt --config=noaws --config=nogcp //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\nERROR: /home/xxxxx/Downloads/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:62:1: Generating code from table: ir/tf_ops.td //tensorflow/compiler/mlir/tensorflow:tensorflow_ops_inc_gen__gen_op_defs_genrule failed (Aborted): bash failed: error executing command \r\n  (cd /home/xxxx/.cache/bazel/_bazel_xxxxx/b7319a1803ca3236e14ad4085fc7b534/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \\\r\n    PATH=/home/xxxxx/anaconda3/condabin:/home/xxxxx/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=7.5 \\\r\n    TF_ENABLE_XLA=1 \\\r\n    TF_NEED_CUDA=1 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen -gen-op-defs tensorflow/compiler/mlir/tensorflow/ir/tf_ops.td -Ibazel-out/k8-opt/bin -I external/llvm-project/mlir/include -I external/org_tensorflow -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include -I $(dirname tensorflow/compiler/mlir/tensorflow/ir/tf_ops.td) -o bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc.inc')\r\nExecution platform: @local_execution_config_platform//:platform\r\nmlir-tblgen: external/llvm-project/mlir/lib/TableGen/Type.cpp:22: mlir::tblgen::TypeConstraint::TypeConstraint(const llvm::Record*): Assertion `def->isSubClassOf(\"TypeConstraint\") && \"must be subclass of TableGen 'TypeConstraint' class\"' failed.\r\nPLEASE submit a bug report to  and include the crash backtrace.\r\nStack dump:\r\n0.\tProgram arguments: bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen -gen-op-defs tensorflow/compiler/mlir/tensorflow/ir/tf_ops.td -Ibazel-out/k8-opt/bin -I external/llvm-project/mlir/include -I external/org_tensorflow -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include -I tensorflow/compiler/mlir/tensorflow/ir -o bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc.inc \r\n #0 0x000055b79f119f53 llvm::sys::PrintStackTrace(llvm::raw_ostream&) (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0xe9f53)\r\n #1 0x000055b79f117f45 llvm::sys::RunSignalHandlers() (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0xe7f45)\r\n #2 0x000055b79f1184e5 SignalHandler(int) (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0xe84e5)\r\n #3 0x00007fdfd66ed3c0 __restore_rt (/usr/lib/x86_64-linux-gnu/libpthread.so.0+0x153c0)\r\n #4 0x00007fdfd633018b raise (/usr/lib/x86_64-linux-gnu/libc.so.6+0x4618b)\r\n #5 0x00007fdfd630f859 abort (/usr/lib/x86_64-linux-gnu/libc.so.6+0x25859)\r\n #6 0x00007fdfd630f729 (/usr/lib/x86_64-linux-gnu/libc.so.6+0x25729)\r\n #7 0x00007fdfd6320f36 (/usr/lib/x86_64-linux-gnu/libc.so.6+0x36f36)\r\n #8 0x000055b79f0c26bf mlir::tblgen::TypeConstraint::TypeConstraint(llvm::Record const*) (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x926bf)\r\n #9 0x000055b79f0b2fda mlir::tblgen::Operator::getResultTypeConstraint(int) const (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x82fda)\r\n#10 0x000055b79f0b3f9c mlir::tblgen::Operator::populateTypeInferenceInfo(llvm::StringMap<int, llvm::MallocAllocator> const&)::'lambda'(int)::operator()(int) const (.isra.0) (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x83f9c)\r\n#11 0x000055b79f0b5693 mlir::tblgen::Operator::populateTypeInferenceInfo(llvm::StringMap<int, llvm::MallocAllocator> const&) (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x85693)\r\n#12 0x000055b79f0b6cd9 mlir::tblgen::Operator::populateOpStructure() (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x86cd9)\r\n#13 0x000055b79f0b75b1 mlir::tblgen::Operator::Operator(llvm::Record const&) (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x875b1)\r\n#14 0x000055b79f05ec60 emitOpList(std::vector<llvm::Record*, std::allocator<llvm::Record*> > const&, llvm::raw_ostream&)::'lambda'(llvm::Record*)::operator()(llvm::Record*) const (.isra.0) (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x2ec60)\r\n#15 0x000055b79f06f356 _ZNSt17_Function_handlerIFbRKN4llvm12RecordKeeperERNS0_11raw_ostreamEEUlS3_S5_E2_E9_M_invokeERKSt9_Any_dataS3_S5_ (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x3f356)\r\n#16 0x000055b79f0c5042 llvm::TableGenMain(char const*, bool (*)(llvm::raw_ostream&, llvm::RecordKeeper&)) (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x95042)\r\n#17 0x000055b79f04c6df main (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x1c6df)\r\n#18 0x00007fdfd63110b3 __libc_start_main (/usr/lib/x86_64-linux-gnu/libc.so.6+0x270b3)\r\n#19 0x000055b79f04ef7e _start (bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen+0x1ef7e)\r\n/bin/bash: line 1: 494760 Aborted                 (core dumped) bazel-out/host/bin/external/llvm-project/mlir/mlir-tblgen -gen-op-defs tensorflow/compiler/mlir/tensorflow/ir/tf_ops.td -Ibazel-out/k8-opt/bin -I external/llvm-project/mlir/include -I external/org_tensorflow -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include -I $(dirname tensorflow/compiler/mlir/tensorflow/ir/tf_ops.td) -o bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc.inc\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 23.476s, Critical Path: 16.84s\r\nINFO: 414 processes: 414 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n", "comments": ["Thanks for reporting this, at what rev is this?", "The change that caused this failure has been rolled back and the root cause of this failure has been address upstream.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40559\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40559\">No</a>\n", "Compiled successfully now."]}, {"number": 40558, "title": "Segmentation fault (core dumped)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip install --upgrade tensorflow\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: GeForce RTX 2080, 12GB\r\n\r\n**Describe the current behavior**\r\nHi, \r\nI installed tensorflow from source in python3 using cuda 10.2. But when I run my script, I got this error without any further details:\r\n```\r\nSegmentation fault (core dumped)\r\n```\r\nWhat is the cause of this issue?\r\n\r\nThank you in advance.\r\n", "comments": ["@pvnieo \r\n\r\nCan you try with cuda 10.0 and see if the problem still persists.Please, see tested build configuration from [here](https://www.tensorflow.org/install/source#gpu).Also refer few related issues #39939,#36841 #37689 #35968 #36201 and see if it helps you. Request you to share the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "I built tensorflow from source for cuda 10.2.\r\nclosing issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40558\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40558\">No</a>\n"]}, {"number": 40557, "title": "Generate ops with type annotations", "body": "Tensorflow Python APIs are generated in C++ and have Python wrappers as the public API. This PR will add type annotations to the generated python bindings of TensorFlow op kernels.\r\n\r\nThe changes in this PR do the following:\r\n- Generate Type variables above each op with the accepted dtypes\r\n- Generate type annotations for function arguments\r\n- Generate type annotations for the tensor returned by the op\r\n\r\n### Example\r\nAn op `Foo` is registed as:\r\n```C++\r\nREGISTER_OP(\"Foo\u201d)\r\n    .Input(\"x: T\")\r\n    .Output(\"y: Tout\")\r\n    .Attr(\"T: {int8, uint8}\")\r\n    .Attr(\"Tout: {complex64, complex128}\")\r\n```\r\nThe tensorflow python op generated with type annotations:\r\n```python\r\nTV_Foo_T = TypeVar(\"TV_Foo_T\", dtypes.Int8, dtypes.UInt8)\r\nTV_Foo_Tout = TypeVar(\"TV_Foo_Tout\", dtypes.Complex64, dtypes.Complex128)\r\n\r\ndef foo(x: ops.Tensor[TV_Foo_T]) -> ops.Tensor[TV_Foo_Tout]:\r\n   ...\r\n```\r\n\r\nWithout these changes, the function definition would not have type annotations and will be generated as:\r\n```python\r\ndef foo(x):\r\n   ...\r\n```", "comments": []}, {"number": 40556, "title": "Tensorflow Installation Issue tf_logging", "body": "**System information**\r\n- OS Platform: Windows 10\r\n- TensorFlow installed from : conda install with Anaconda distribution\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the problem**\r\nAt first I couldn't install tensorflow due to the folder google_pasta-0.2.0.dist-info not having a METADATA in it. I found somewhere online that by deleting the folder and using pip to reinstall it works. After that it had a METADATA and other files in it and I was able to install tensorflow and went to check if it worked in a Jupyter notebook however after running the program below I got the error described further below\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-7-7380a45e29ab>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 75, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\framework_lib.py\", line 25, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 42, in <module>\r\n    from tensorflow.python.eager import core\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\core.py\", line 22, in <module>\r\n    from tensorflow.python.framework import errors\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\errors.py\", line 22, in <module>\r\n    from tensorflow.python.framework import errors_impl as _impl\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\errors_impl.py\", line 30, in <module>\r\n    from tensorflow.python.util import deprecation\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 25, in <module>\r\n    from tensorflow.python.platform import tf_logging as logging\r\nImportError: cannot import name 'tf_logging' from 'tensorflow.python.platform' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\__init__.py)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 10, in <module>\r\n    from tensorflow.python.eager import core as _core\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\core.py\", line 22, in <module>\r\n    from tensorflow.python.framework import errors\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\errors.py\", line 22, in <module>\r\n    from tensorflow.python.framework import errors_impl as _impl\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\errors_impl.py\", line 30, in <module>\r\n    from tensorflow.python.util import deprecation\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 25, in <module>\r\n    from tensorflow.python.platform import tf_logging as logging\r\nImportError: cannot import name 'tf_logging' from 'tensorflow.python.platform' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\__init__.py)", "comments": ["@BOSSrobot,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "@amahendrakar \r\nI am new to all of this so sorry if I am misunderstanding you however although I did install it with Anaconda wouldn't it still be a tensorflow issue since the installation was successful but upon trying to test tensorflow the module gave the error above? Thanks!", "@BOSSrobot,\r\nFrom the error log you have given, it seems like TensorFlow was not installed successfully. Hence, I had asked you to submit an issue in the Anaconda repo. \r\n\r\nPlease check if the installation was successful using the below code. \r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n```\r\nThanks!", "I see what you are saying. In the end you were right and the installation was not successful, however I got around this issue be using conda remove tensorflow and then reinstalling it. Sorry and thanks for your help!", "Since the issue is solved, can we close it?", "Yes. Thanks for your help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40556\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40556\">No</a>\n"]}, {"number": 40554, "title": "Add benchmark for Bidirectional LSTM on IMDB", "body": "Adding benchmark for Bidirectional LSTM on IMDB at keras.io example and already passed the tests on CPU and GPU. \r\n\r\n@yhliang2018 Thanks for reviewing! ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40554) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40554) for more info**.\n\n<!-- ok -->", "You may want to make sure all checks get passed on this PR.", "> Thanks for the changes!\r\n> \r\n> With the new change, you may also need to update the [cpu benchmark](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/benchmark/keras_cpu_benchmark_test.py) to reduce the redundant code.\r\n\r\nYeah, I will do it in this PR.", "Already Update.", "> Thanks. The code part looks great! Most comments are on docstrings.\r\n> \r\n> BTW, do you have the plan to simplify the file \"keras_cpu_benchmarks_test.py\" this PR? It can also be in next PR if you prefer.\r\n\r\nYeah, I didn't simplify it in this PR because I want to make sure we can define the template first. And I will submit another PR to fix it."]}, {"number": 40553, "title": "tf.io.matching_files hangs given a certain pattern", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): No \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n`tf.io.matching_files` hangs and does not terminate when passing `/*` or `/?`  in the beginning of `pattern` argument.\r\n\r\n**Describe the expected behavior**\r\nAs far as `/*` or `/?` is not placed in the beginning, the function terminates with a proper error handling. I would expect a similar behavior for my input below. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.io.matching_files('/*name',name=None)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.2, nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1145726bc641ac37372231cf6d7866d3/untitled42.ipynb).Thanks!", "The reason for this problem is the algorithm in `tf.io.matching_files` will search for all files and nested directories under `/` when `/*name` is used. I'd love to help to fix this problem.", "This is fixed in latest nightly (probably even from before)\r\n\r\n```\r\n...$ python -c \"import tensorflow as tf; print(tf.io.gfile.glob('/*p'))\"\r\n['/tmp']\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40553\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40553\">No</a>\n"]}, {"number": 40552, "title": "Wider vector for FP16 RELU Grad on GPUs", "body": "To solve a Build issue brought by https://github.com/tensorflow/tensorflow/pull/39767\r\n@sanjoy ", "comments": ["@sanjoy I fixed one conflict. PTAL."]}, {"number": 40551, "title": "Tensorflow Roadmap README link broken", "body": "Link to 'Tensorflow Roadmap' in README is broken: https://www.tensorflow.org/community/roadmap\r\n\r\nThis template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@4d55397500,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/37509#issuecomment-598049023) from a similar issue and let us know if it helps. Thanks!", "Ah I see @amahendrakar ", "@4d55397500,\r\nHope the above comment answers your query. Please feel free to close the issue if resolved. Thanks!"]}, {"number": 40550, "title": "AttributeError: type object 'TFLiteConverterV2' has no attribute 'from_keras_model_file'", "body": "I am having one TensorFlow Keras model \"model.h5\". I want to generate tflite from it. I am using the below-mentioned code for that. I am using tensorflow version '2.0.0' on Anaconda Spyder 3.7, 64 bit, windows10. Later on I want to convert this model for the google coral edge tpu. \r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    from tensorflow import lite\r\n\r\n    dataset_dir = \"C:\\\\Users\\\\Ravi\\\\dataset\"\r\n    IMAGE_SIZE = 224\r\n    saved_keras_model = \"C:\\\\Users\\\\Ravi\\\\model.h5\"\r\n\r\n    def representative_data_gen():\r\n      dataset_list = tf.data.Dataset.list_files(dataset_dir + '/*/*')\r\n      for i in range(100):\r\n        image = next(iter(dataset_list))\r\n        image = tf.io.read_file(image)\r\n        image = tf.io.decode_jpeg(image, channels=3)\r\n        image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\r\n        image = tf.cast(image / 255., tf.float32)\r\n        image = tf.expand_dims(image, 0)\r\n        yield [image]\r\n\r\n    converter =  lite.TFLiteConverter.from_keras_model_file(saved_keras_model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    # This ensures that if any ops can't be quantized, the converter throws an error\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    # These set the input and output tensors to uint8\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    # And this sets the representative dataset so we can quantize the activations\r\n    converter.representative_dataset = representative_data_gen\r\n    tflite_model = converter.convert()\r\n\r\n    with open('mobilenet_v2_1.0_224_quant.tflite', 'wb') as f:\r\n          f.write(tflite_model)\r\n\r\nI am getting this kind of error.\r\n\r\n    Traceback (most recent call last):\r\n\r\n     File \"C:\\Users\\Ravi\\tflite_model.py\", line 28, in <module>\r\n        converter =  lite.TFLiteConverter.from_keras_model_file(saved_keras_model)\r\n\r\n    AttributeError: type object 'TFLiteConverterV2' has no attribute 'from_keras_model_file'\r\n\r\nWhat can be the problem? How can I solve this issue?", "comments": ["You can either depend on the following choices:\r\n\r\n1) After loading a keras model instance, you can use `tf.lite.TFLiteConverter.from_keras_model`.\r\n\r\n```\r\nfrom tensorflow import keras\r\nmodel = keras.models.load_model('path/to/location')\r\n```\r\n\r\n2) Or you can use the v1 API, `tf.compat.v1.lite.TFLiteConverter.from_keras_model_file`.", "I have tried with the \"tf.compat.v1.lite.TFLiteConverter.from_keras_model_file\". Now I am getting value error. \r\n\r\n    ValueError: Cannot set tensor: Got tensor of type STRING but expected type FLOAT32 for input 183, name: input_1 \r\n\r\n    INFO:tensorflow:Froze 264 variables.\r\n    INFO:tensorflow:Converted 264 variables to const ops.\r\n    Traceback (most recent call last):\r\n\r\n      File \"C:\\Users\\Ravi\\tflite_model.py\", line 47, in <module>\r\n    tflite_model = converter.convert()\r\n\r\n      File \"C:\\Users\\Ravi\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 908, in convert\r\n    inference_output_type)\r\n\r\n      File \"C:\\Users\\Ravi\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 200, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n\r\n      File \"C:\\Users\\Ravi\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py\", line 75, in calibrate_and_quantize\r\n    self._calibrator.FeedTensor(calibration_sample)\r\n\r\n      File \"C:\\Users\\Ravi\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\tensorflow_lite_wrap_calibration_wrapper.py\", line 112, in FeedTensor\r\n        return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)\r\n\r\n    ValueError: Cannot set tensor: Got tensor of type STRING but expected type FLOAT32 for input 183, name: input_1 \r\n\r\n\r\n", "I have tried on google colab and it worked! "]}, {"number": 40549, "title": "Installed Tensorflow-directml show no module named tensorflow", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n import tensorflow as tf\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1903\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  pip install tensorflow-directml==0.0.1.dev0\r\n- TensorFlow version (use command below):\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nSo the long waited Directml was released today, but it seems that the package wasn't compiled yet\r\n, I installed through pypi, obviously that size of wheel is 6.4 KB, which isn't right\r\n, should be about 200 MB or so.\r\n\r\n**Describe the expected behavior**\r\n\r\nAny schedule on when the proper wheels will be released? \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi limapedro,\r\n\r\nThe tensorflow-directml PyPI package is a placeholder until the project's [size limit is increased](https://github.com/pypa/pypi-support/issues/457). For now, you'll need to get the actual packages from our [GitHub ](https://github.com/microsoft/DirectML/releases/tag/tensorflow-directml-1.15.3.dev200615)(notes for [Windows](https://docs.microsoft.com/en-us/windows/win32/direct3d12/gpu-tensorflow-windows); notes for [WSL](https://docs.microsoft.com/en-us/windows/win32/direct3d12/gpu-tensorflow-wsl)).\r\n\r\nThanks for testing this out!", "@jstoecker Thank you.", "@limapedro \r\n\r\nwas this resolved by following @jstoecker  suggestion? Please close the issue if it was resolved already. Thanks!", "@ravikyram It's all working now, the pypi page is working, you can see the files for the project here: https://pypi.org/project/tensorflow-directml/#files", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40549\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40549\">No</a>\n"]}, {"number": 40548, "title": "Make keras.callbacks.Callback and tensorflow.keras.callbacks.Callback mutually compatible", "body": "`tensorflow.keras.callbacks.Callback` has private methods [`_implements_train_batch_hooks`](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/callbacks.py#L739) etc which are [called](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/callbacks.py#L328) e.g. when the callback is used with `tensorflow.keras.models.Model.fit`.\r\n\r\n`keras.callbacks.Callback` provides no such methods and so using it with e.g. `tensorflow.keras.models.Model.fit` results in `AttributeError: 'SomeClassExtendingCallback' object has no attribute '_implements_train_batch_hooks'`.\r\n\r\nIf those two versions of Keras aim to be compatible, then this should be fixed.", "comments": ["@PiotrJander,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "Versions:\r\n\r\n```\r\ntensorflow==2.2.0\r\nKeras==2.3.1\r\n```\r\n\r\nMinimal example:\r\n\r\n```python\r\nimport neptune\r\nfrom tensorflow import keras\r\n\r\n# this is an import from standalone Keras, as opposed to the Keras shipped with TF\r\nfrom keras.callbacks import BaseLogger \r\n\r\nmnist = keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = keras.models.Sequential([\r\n  keras.layers.Flatten(),\r\n  keras.layers.Dense(100, activation=keras.activations.softmax)\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          epochs=5,\r\n          batch_size=256,\r\n          callbacks=[BaseLogger()])\r\n```\r\n\r\nOutput and error:\r\n\r\n```\r\nUsing TensorFlow backend.\r\n2020-06-19 10:32:15.050829: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-19 10:32:15.069239: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff6bf6ae880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-19 10:32:15.069257: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"tf-issue.py\", line 18, in <module>\r\n    model.fit(x_train, y_train,\r\n  File \"/Users/piotr/projects/neptune-sandbox/.env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/Users/piotr/projects/neptune-sandbox/.env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 819, in fit\r\n    callbacks = callbacks_module.CallbackList(\r\n  File \"/Users/piotr/projects/neptune-sandbox/.env/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\", line 230, in __init__\r\n    self._should_call_train_batch_hooks = any(\r\n  File \"/Users/piotr/projects/neptune-sandbox/.env/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\", line 231, in <genexpr>\r\n    cb._implements_train_batch_hooks() for cb in self.callbacks)\r\nAttributeError: 'BaseLogger' object has no attribute '_implements_train_batch_hooks\r\n```\r\n\r\nI hope this helps!", "> Versions:\r\n> \r\n> ```\r\n> tensorflow==2.2.0\r\n> Keras==2.3.1\r\n> ```\r\n> \r\n> Minimal example:\r\n> \r\n> ```python\r\n> import neptune\r\n> from tensorflow import keras\r\n> \r\n> # this is an import from standalone Keras, as opposed to the Keras shipped with TF\r\n> from keras.callbacks import BaseLogger \r\n> \r\n> mnist = keras.datasets.mnist\r\n> (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n> x_train, x_test = x_train / 255.0, x_test / 255.0\r\n> \r\n> model = keras.models.Sequential([\r\n>   keras.layers.Flatten(),\r\n>   keras.layers.Dense(100, activation=keras.activations.softmax)\r\n> ])\r\n> \r\n> model.compile(optimizer='adam',\r\n>               loss='sparse_categorical_crossentropy',\r\n>               metrics=['accuracy'])\r\n> \r\n> model.fit(x_train, y_train,\r\n>           epochs=5,\r\n>           batch_size=256,\r\n>           callbacks=[BaseLogger()])\r\n> ```\r\n> \r\n> Output and error:\r\n> \r\n> ```\r\n> Using TensorFlow backend.\r\n> 2020-06-19 10:32:15.050829: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2020-06-19 10:32:15.069239: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff6bf6ae880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-06-19 10:32:15.069257: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> Traceback (most recent call last):\r\n>   File \"tf-issue.py\", line 18, in <module>\r\n>     model.fit(x_train, y_train,\r\n>   File \"/Users/piotr/projects/neptune-sandbox/.env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n>     return method(self, *args, **kwargs)\r\n>   File \"/Users/piotr/projects/neptune-sandbox/.env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 819, in fit\r\n>     callbacks = callbacks_module.CallbackList(\r\n>   File \"/Users/piotr/projects/neptune-sandbox/.env/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\", line 230, in __init__\r\n>     self._should_call_train_batch_hooks = any(\r\n>   File \"/Users/piotr/projects/neptune-sandbox/.env/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\", line 231, in <genexpr>\r\n>     cb._implements_train_batch_hooks() for cb in self.callbacks)\r\n> AttributeError: 'BaseLogger' object has no attribute '_implements_train_batch_hooks\r\n> ```\r\n> \r\n> I hope this helps!\r\n\r\nsame issue", "> `keras.callbacks.Callback` provides no such methods and so using it with e.g. `tensorflow.keras.models.Model.fit` results in `AttributeError: 'SomeClassExtendingCallback' object has no attribute '_implements_train_batch_hooks'`.\r\n\r\n@PiotrJander,\r\nYou are facing this error because of the mixed up imports of `keras` and `tf.keras`. \r\n\r\nI was able to run the code without any issues using [keras](https://colab.research.google.com/gist/amahendrakar/9bb1bc3830a1b184c0d21ca6f4bbf9b3/40548-keras.ipynb), whereas I'm facing an error stating `KeyError: 'metrics'` on running the code with [tf.keras](https://colab.research.google.com/gist/amahendrakar/4c583e22e490ace733b043e48e8c135e/40548-tf-keras.ipynb#scrollTo=tmmFSCO_mhdV). Please check the linked gist. \r\n\r\nFor more information, please take a look at [this](https://stackoverflow.com/a/57123200) StackOverflow comment. Thanks! ", "Thank you. I knew this is due to mixing up `keras` and `tf.keras`, I just wanted to raise this in case mutual compatibility were a goal.\r\n\r\nPerhaps the docs should state visibly that `keras` and `tf.keras` are not mutually compatible?", "I solved this problem by using tensorflow==1.14.0 and keras==2.3.1 ", "@PiotrJander This is a well known fact in the tensorflow community.\r\n\r\ntf.keras is the Tensorflow specific implementation of the Keras API specification. It adds the framework the support for many Tensorflow specific features like: perfect support for tf.data.Dataset as input objects, support for eager execution, ...\r\n\r\nIn Tensorflow 2.0 tf.keras will be the default and I highly recommend to start working using tf.keras\r\n\r\nLet me know if we can close this issue as it has been resolved", "Sure, we can close this", "Using tensorflow.keras instead of keras worked for me. Thanks!"]}, {"number": 40547, "title": "TF 2.2 GPU memory usage regression on model.predict() with big inputs", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: PyPI\r\n- TensorFlow version : v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.1\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA Tesla M60, 8GB\r\n\r\n**Describe the current behavior**\r\n\r\nI created a 3D Unet which takes 192x192x192 inputs and outputs a segmentation of the same size. I did not train the network but tried passing dummy inputs to `model.predict()` instead. I'm getting an OOM error when I limit the available GPU memory to 5GB on an input of size (70, 192, 192, 192, 1) (`batch_size=1`). I don't get the same error on TF 2.1 with the same input size. In order for the script to work on 2.2, I need to increase available memory to 5.7GB.\r\nThe exception is raised repeatably at batch 47 of 70.\r\nI can't see anything related in v2.2 changelog, which leads me to believe that this is a regression.\r\n\r\n**Describe the expected behavior**\r\nFor `batch_size=1`, I expect the inputs to be transferred to GPU memory one by one, and the outputs to be transferred back to RAM sequentially. There should not be significant GPU memory usage differences between predicting on 1000 inputs and 100 inputs. There probably should be a buffer such that the next batch of prediction is not held up by GPU-RAM I/O, but a difference of 700MB is suspicious.\r\nI could work around this by splitting the prediction and then concatenating it like so:\r\n```\r\nMAX_INPUT_SIZE = 47\r\npred1 = model.predict(test_input[:MAX_INPUT_SIZE], batch_size=1)\r\npred2 = model.predict(test_input[MAX_INPUT_SIZE:], batch_size=1)\r\n```\r\nbut this should probably be done by TF, not the user. Am I seeing a feature or a bug? \r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nfrom tensorflow.keras import Input\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Conv3D, BatchNormalization, Activation, Layer, MaxPool3D, Conv3DTranspose, Concatenate\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef limit_gpu_memory_use(limit=None):\r\n    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\n    for gpu in gpus:\r\n        if limit is None:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n        else:\r\n            tf.config.experimental.set_virtual_device_configuration(\r\n                gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=limit)]\r\n                )\r\n\r\n\r\ndef test_big_model():\r\n    # TF 2.1: 5000, TF 2.2: 5700\r\n    limit_gpu_memory_use(5000)\r\n    model = build_model()\r\n    test_input = np.ones(shape=(70, 192, 192, 192, 1), dtype=np.int16)\r\n    model.predict(test_input, verbose=1, batch_size=1)\r\n\r\n\r\ndef build_model():\r\n    depth = 4\r\n    n_base_filters = 8\r\n    actfun = \"relu\"\r\n\r\n    skip_layers = []\r\n    inputs = Input(shape=(192, 192, 192, 1))\r\n    x = Conv3D(\r\n        filters=n_base_filters,\r\n        kernel_size=(3, 3, 3),\r\n        activation=actfun,\r\n        padding=\"same\",\r\n    )(inputs)\r\n\r\n    x = BatchNormalization()(x)\r\n    x = Conv3D(\r\n        filters=n_base_filters * 2,\r\n        kernel_size=(3, 3, 3),\r\n        activation=actfun,\r\n        padding=\"same\",\r\n    )(x)\r\n\r\n    skip_layers.append(x)\r\n\r\n    for i in range(1, depth):\r\n        x = create_downsampling_block(input_layer=x, num_filters=n_base_filters * 2 ** i, actfun=actfun)\r\n        skip_layers.append(x)\r\n\r\n    for i in range(depth - 1, 0, -1):\r\n        x = create_upsampling_block(\r\n            input_layer=x,\r\n            skip_input_layer=skip_layers[i - 1],\r\n            num_filters=n_base_filters * 2 ** (i + 1),\r\n            actfun=actfun,\r\n        )\r\n\r\n    x = Conv3D(filters=1, kernel_size=(1, 1, 1))(x)\r\n    out = Activation(\"sigmoid\", dtype=\"float32\")(x)\r\n\r\n    return Model(inputs=inputs, outputs=out)\r\n\r\n\r\ndef create_downsampling_block(input_layer: Layer, num_filters: int, actfun: str) -> Layer:\r\n    x = MaxPool3D(pool_size=(2, 2, 2))(input_layer)\r\n    for i in [1, 2]:\r\n        x = Conv3D(\r\n            filters=num_filters * i,\r\n            kernel_size=(3, 3, 3),\r\n            activation=actfun,\r\n            padding=\"same\",\r\n        )(x)\r\n    return x\r\n\r\n\r\ndef create_upsampling_block(input_layer: Layer, skip_input_layer: Layer, num_filters: int, actfun: str) -> Layer:\r\n    x = Conv3DTranspose(filters=num_filters, kernel_size=(2, 2, 2), strides=(2, 2, 2))(input_layer)\r\n    x = Concatenate()([skip_input_layer, x])\r\n    for _ in range(2):\r\n        x = Conv3D(\r\n            filters=num_filters // 2,\r\n            kernel_size=(3, 3, 3),\r\n            activation=actfun,\r\n            padding=\"same\",\r\n        )(x)\r\n\r\n    return x\r\n\r\ntest_big_model()\r\n```\r\n\r\n**Other info / logs** \r\n[traceback.log](https://github.com/tensorflow/tensorflow/files/4793605/traceback.log)\r\n", "comments": ["@alkamid \r\nPlease let us know if [this gist](https://colab.research.google.com/gist/Saduf2019/cf62fcb02d8a75e68c9c3fb5a28f6736/untitled237.ipynb) confirms your issue.", "@Saduf2019 yes, it does. The point at which the gist fails (30/70) is different, but I suppose it might be environment-dependent.", "I can reproduce the error on TF 2.3 and CUDA 10.2.", "@Saduf2019 I can reproduce the issue with the provided example and want to try to resolve it. I want to start contributing to the repo. Can you please guide me on that?", "Could reproduce the issue with **`Tensorflow Version, 2.5`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/d255a4f5b7106f424675e0f126df2a95/untitled237.ipynb#scrollTo=hAsAgD-9PHSi). Thanks!", "In [this gist](https://colab.research.google.com/drive/17GvpewWCpnrMq6ozrKUYphirpA1SuWxr?usp=sharing) I have reproduced the issue with tensorflow 2.6, using IODataset.from_numpy() as an input source, as this was suggested (e.g. https://github.com/tensorflow/tensorflow/issues/44711#issuecomment-736186819) as a way to handle large inputs in a more memory-optimized way.\r\n", "This is still there in tf 2.7 - see [Colab](https://colab.research.google.com/drive/1PqNtXN1ojugT9yt1Prs49GcwhMIShXJv?usp=sharing)", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! "]}, {"number": 40546, "title": "`tf.saved_model.save` attempts to compile __call__ even if `signatures` is not `None`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Microsoft Windows 10 Enterprise\r\n- TensorFlow installed from (source or binary): official pipy wheel\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n\r\n**Describe the current behavior**\r\nConsider a model whose `call` method can not be decorated by `tf.function`. We want to save some methods of the model (but not the call) using `tf.saved_model.save`. So we pass `signatures` argument to the  `tf.saved_model.save` where we list the methods we want to save. In TF2.0 and TF2.1 this worked. \r\nIn TF2.2 it fails as tensorflow seems to try to decorate `call` by `tf.function` anyway.\r\n\r\n**Describe the expected behavior**\r\nSame behaviour as in TF2.0 and TF2.1. Passing `signatures` argument to `tf.saved_model.save` should prevent compilation of other methods.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass SomeClass:\r\n    def __init__(self, x):\r\n        self.x = x\r\n\r\n    def __repr__(self):\r\n        return f\"SomeClass({self.x})\"\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def build(self, input_shape):\r\n        self.a = tf.Variable(1.0, name=\"a\")\r\n\r\n    def serve(self, x):\r\n        return x + self.a\r\n\r\n    def call(self, x):\r\n        \"\"\"This method can't be decorated by `tf.function` because it returns an object that is not a Tensor.\r\n        It should not matter, because we do not want to save it.\r\n        \"\"\"\r\n        x = tf.cast(x, tf.float32)\r\n        return SomeClass(x + self.a)\r\n\r\n\r\nmodel = MyModel()\r\nprint(\"Call the model in order to build it: \", model(4))\r\n\r\nsignatures = {\"my_stuff\": tf.function(model.serve, input_signature=[tf.TensorSpec([None], tf.float32)])}\r\ntf.saved_model.save(model, export_dir=\"\", signatures=signatures)\r\n```\r\nThis code works in TF2.0 and TF2.1 but not in TF2.2.\r\n\r\n**Other info / logs** \r\nHere is the traceback in TF2.2\r\n```\r\nCall the model in order to build it:  SomeClass(5.0)\r\nTraceback (most recent call last):\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 543, in make_tensor_proto\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 543, in <listcomp>\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 87, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got SomeClass(Tensor(\"my_model/add:0\", shape=(), dtype=float32))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 937, in convert\r\n    x = ops.convert_to_tensor_or_composite(x)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1464, in convert_to_tensor_or_composite\r\n    value=value, dtype=dtype, name=name, as_ref=False)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1503, in internal_convert_to_tensor_or_composite\r\n    accepted_result_types=(Tensor, composite_tensor.CompositeTensor))\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1341, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 321, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 262, in constant\r\n    allow_broadcast=True)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 300, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 547, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class '__main__.SomeClass'> to Tensor. Contents: SomeClass(Tensor(\"my_model/add:0\", shape=(), dtype=float32)). Consider casting elements to a supported type.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bordel.py\", line 31, in <module>\r\n    tf.saved_model.save(model, export_dir=\"\", signatures=signatures)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 951, in save\r\n    obj, export_dir, signatures, options, meta_graph_def)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1012, in _build_meta_graph\r\n    signature_serialization.validate_saveable_view(checkpoint_graph_view)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_serialization.py\", line 268, in validate_saveable_view\r\n    saveable_view.root):\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 108, in list_dependencies\r\n    extra_dependencies = self.list_extra_dependencies(obj)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 137, in list_extra_dependencies\r\n    self._serialization_cache)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 2746, in _list_extra_dependencies_for_serialization\r\n    .list_extra_dependencies_for_serialization(serialization_cache))\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\base_serialization.py\", line 74, in list_extra_dependencies_for_serialization\r\n    return self.objects_to_serialize(serialization_cache)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\layer_serialization.py\", line 73, in objects_to_serialize\r\n    serialization_cache).objects_to_serialize)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\layer_serialization.py\", line 92, in _get_serialized_attributes\r\n    serialization_cache)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\model_serialization.py\", line 47, in _get_serialized_attributes_internal\r\n    default_signature = save_impl.default_save_signature(self.obj)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\save_impl.py\", line 203, in default_save_signature\r\n    fn.get_concrete_function()\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 959, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 865, in _get_concrete_function_garbage_collected\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 506, in _initialize\r\n    *args, **kwds))\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 986, in func_graph_from_py_func\r\n    expand_composites=True)\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 617, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 617, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"D:\\git\\workon_hrab2\\venv_tf_22\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 943, in convert\r\n    (str(python_func), type(x)))\r\nTypeError: To be compatible with tf.contrib.eager.defun, Python functions must return zero or more Tensors; in compilation of <function trace_model_call.<locals>._wrapped_model at 0x00000278D27BE828>, found return\r\n value of type <class '__main__.SomeClass'>, which is not a Tensor.\r\n```", "comments": ["I have tried in colab with TF version 2.2, nightly versions and was able to reproduce the issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/527d7062d60904f309f4b45714834440/untitled41.ipynb)However i am not seeing any issues with TF versions 2.0, 2.2.Thanks!", "You probably meant: \"i am not seeing any issues with TF versions 2.0, 2.1.\", didn't you?\r\nJust to avoid confusion.\r\n\r\n", "@PistaSaki \r\n\r\nSorry my bad. Typo mistake. I am not seeing issue with TF versions 2.0,2.1.Thanks!", "If somebody needs it, I found a way to circumvent the problem which works for me.\r\n1. Define your own `__call__` method and don't rely on tensorflow to create `__call__` from `call`.\r\n2. Before saving, set `model.built = False`\r\n\r\nHere is the code:\r\n```python\r\nfrom functools import wraps\r\nfrom typing import Callable\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nclass SomeClass:\r\n    def __init__(self, x):\r\n        self.x = x\r\n\r\n    def __repr__(self):\r\n        return f\"SomeClass({self.x})\"\r\n\r\n\r\ndef main_call(call: Callable) -> Callable:\r\n    @wraps(call)\r\n    def decorated_call(self, *args, **kwargs):\r\n        with tf.name_scope(self.name_scope()):\r\n            if not self.built:\r\n                self.build([])\r\n                self.built = True\r\n\r\n            return call(self, *args, **kwargs)\r\n\r\n    return decorated_call\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def build(self, input_shape):\r\n        self.a = tf.Variable(1.0, name=\"a\")\r\n\r\n    def serve(self, x):\r\n        return x + self.a\r\n\r\n    @main_call\r\n    def __call__(self, x):\r\n        \"\"\"This method can't be decorated by `tf.function` because it returns an object that is not a Tensor.\r\n        It should not matter, because we do not want to save it.\r\n        \"\"\"\r\n        x = tf.cast(x, tf.float32)\r\n        return SomeClass(x + self.a)\r\n\r\n\r\nmodel = MyModel()\r\nprint(\"Call the model in order to build it: \", model(4))\r\n\r\nsignatures = {\"my_stuff\": tf.function(model.serve, input_signature=[tf.TensorSpec([None], tf.float32)])}\r\nmodel.built = False\r\ntf.saved_model.save(model, export_dir=\"d:/tmp_model\", signatures=signatures)\r\n```", "If you install tf-nightly, you can use the newly added `save_traces` argument to prevent the model from being traced. \r\n\r\n`model.save(export_dir=\"\", signatures=signatures, save_traces=False)`\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40546\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40546\">No</a>\n"]}, {"number": 40545, "title": "TF-TRT Improve reshape op converter", "body": "This PR improves the Reshape op converter to handle explicit batch and dynamic shape input data.\r\n\r\nSince TRT6, IShuffleLayer has an option to accept the shape parameter as an [input tensor](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_shuffle_layer.html#a0ab2ab37e4377c9c317c1d869908efbc). If the shape input tensor corresponds to an input tensor of  the TRTEngineOp, then we need to [specify the input shape values](https://github.com/tfeher/tensorflow/blob/318b5c416db756432125af9d25446bdc3db5ee1f/tensorflow/compiler/tf2tensorrt/utils/trt_engine_utils.cc#L127-L134) for the engine. Additionally we need to specify optimization profile parameters for the shape values. The necessary changes were implemented in TrtShapeOptimizationProfile, TRTEngineOp, trt_engine_utils.\r\n\r\nThe unit tests have only a single op, and therefore handling shape input values are required to test\r\nreshape with tensor input shape. In normal network it is less likely to have shape input tensors. \r\n\r\nAdditionally the handling of scalar input/output shapes are improved.\r\n\r\nC++ test for serialization / deserialization of input shapes and shape values added.\r\n\r\nNote: TensorRT has a function  [INetworkDefinition::markOutputForShapes](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_network_definition.html#a1f0fff6e9c2469e31ac3be82dd3f0f28) which could enable the output tensor's value to be computed by IExecutionContext::getShapeBinding. This is not used in this PR, but it might be useful for shape inference.\r\n\r\n", "comments": ["@tfeher This PR is in draft, any update on this? Please. Thanks!\r\n", "I am working on an update that would enable the missing python test for this feature. It shall be ready by next week. ", "@tfeher This PR is in draft, any update on this? Please. Thanks!", "I have updated the PR, it is ready for review.", "@tfeher  Can you please check @bixia1's comments and keep us posted ? Thanks!", "@tfeher  Any update on this PR? Please. Thanks!", "There is an issue with the converter using TRT 7.1.3. I am working on a fix.", "@tfeher  Any update on this PR? and please resolve conflicts Thanks!", "The unit test of the converter fails in explicit batch mode with TRT 7.1.3 due to a bug in TRT. Waiting for feedback from the TRT team.", "@tfeher  Any update on this PR? and please resolve conflicts Thanks!", "@tfeher  Can you please check @bixia1's comments and keep us posted ? Thanks!", "@tfeher Any update on this PR? Please. Thanks!", "@tfeher  Can you please check @DEKHTIARJonathan's comments and keep us posted ? Thanks!", "@tfeher Any update on this PR? Please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "This PR depends on #45588, I will update here once #45588 is finalized.", "@bixia1 this is basically ready for review. But this PR contains the changeset from #45588 which makes it harder to read until #45588 is merged. I will rebase the PR once #45588 is merged.\r\n\r\nI have improved the shape value handling as [described here](https://github.com/tensorflow/tensorflow/pull/40545#discussion_r572285311). \r\n\r\n", "@bixia1 I have rebased the PR, it is ready for review.", "@tfeher Can you please check @bixia1's comments and keep us posted ? Thanks!", "I have rebased the PR to resolve the merge conflicts.", "I have noticed a problem with checking the shape value profiles during engine lookup. I am working on a fix.", "Fixed issue with shape value handling during profile selection, and added the missing datavec.h file. In a subsequent commit I will add one more python test to check shape value handling.", "I got three tests failed:\r\n(1) tensorflow/python/compiler/tensorrt:gpu_combined_nms_test. message is like\r\n  File \"/build/work/fe0e3a7166e31378da05c1404a4fcd4fedb1/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 1107, in _Test\r\n    self.RunTest(run_params)\r\n  File \"/build/work/fe0e3a7166e31378da05c1404a4fcd4fedb1/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 1030, in RunTest\r\n    GraphState.INFERENCE)\r\n  File \"/build/work/fe0e3a7166e31378da05c1404a4fcd4fedb1/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 907, in _VerifyGraphDef\r\n    graph_state)\r\n  File \"/build/work/fe0e3a7166e31378da05c1404a4fcd4fedb1/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 893, in _VerifyGraphDefV2\r\n    [name for name in unexpected_names if name in all_op_names])\r\nAssertionError: ['score_threshold', 'max_output_size_per_class', 'iou_threshold', 'max_total_size'] has length of 4.\r\n(2) tensorflow/python/compiler/tensorrt:gpu_neighboring_engine_test\r\nand tensorflow/python/compiler/tensorrt:gpu_multi_connection_neighbor_engine_test\r\nthey both failed because of native segment execution happened,\r\n  (0) Aborted: User disallowed engine native segment execution\r\n\t [[{{node TRTEngineOp_0_1}}]]", "tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu also failed\r\n[test.log](https://github.com/tensorflow/tensorflow/files/6098834/test.log)\r\n", "- I have disabled ConvertDynamicReshape before TRT 7.1.3, this way the reshape unit test works in all TRT versions that I have tested (5.1.5, 6.0.1, 7.1.3 and 7.2.1). \r\n- I could not reproduce the Conv3D test with TRT 7.1.3.4.\r\n- I confirm the problems with the two neighbor engine tests. I will continue to investigate those.", "I found the reason for the Conv3D failure. [This line of code ](https://github.com/tensorflow/tensorflow/blob/8cb7f3a462db810d70521fab26eac8e2fed9e2b4/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L3428)is accessing garbage value when c_index == 4, and for implicit batch mode.\r\nIn case you need a test to verify it, [here is one](https://github.com/tensorflow/tensorflow/blob/8cb7f3a462db810d70521fab26eac8e2fed9e2b4/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc#L5018-L5032).\r\nThe problem is actually not introduced by this PR, but [this one](https://github.com/tensorflow/tensorflow/pull/46940).\r\n", "Thanks @bixia1 for looking into the Conv3D failure, I will add a fix in this PR.\r\n\r\nThe last commit seems to solve the problem with the with the two neighbor engine tests. There is still an issue with the combined_nms_test.py, I am looking into that."]}, {"number": 40544, "title": "Keras does not wait for multiprocessing pool to finish", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.X\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 1060ti , 6 gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nKeras does not wait until the multiprocessing pool is done. I am using the pool function from the python built in multiprocessing library.\r\n\r\n**Describe the expected behavior**\r\n\r\nI want to do my multiprocessing pool, close it, and then use keras. \r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40544\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40544\">No</a>\n"]}, {"number": 40543, "title": "[ROCm] Fix for XLA \"scatter\" op related unit test failures.", "body": "After the upstream commit 4de4c60972da38d09662842614ad4dcfd019a6be, the following unit-tests started failing on the ROCm platform\r\n\r\n```\r\n//tensorflow/python/keras/optimizer_v2:adam_test_gpu\r\n//tensorflow/compiler/xla/tests:scatter_test_gpu\r\n//tensorflow/compiler/tests:scatter_nd_op_test_gpu\r\n```\r\n\r\nThe cause seems to be a change in the commit above that updates the LLVM version in use.\r\n\r\nThe LLVM version change (more specifically some AMDGPU backend change contained within the LLVM version change) either introduces an issue or lets manifest an existing issue, w.r.t alloca instructions outside of the entry basic block of a function. The AMDGPU backend seems to expect all alloca instructions to be inside the entry basic block. Having this assumption broken, leads to the regression failures we see above.\r\n\r\nThis PR/commit changes IR generation for \"scatter\" op to ensure that the alloca instruction gets emitted in the entry basic block of the function. This changesmakes the above unit tests pass again. This commit also updates other instances in XLA code where alloca instructions were getting added outside of the entry basic block of a function.\r\n\r\n-----------------------------\r\n\r\n/cc @whchung @ekuznetsov139 @cheshire @chsigg @nvining-work ", "comments": ["George, you know much more about this code than I do. Would you mind to review this PR? Thanks!", "> The AMDGPU backend seems to expect all alloca instructions to be inside the entry basic block\r\n\r\nMay I ask why? Is it a fundamental limitation or a bug or a missing feature?\r\n@sanjoy ", "> May I ask why? Is it a fundamental limitation or a bug or a missing feature?\r\n\r\nI believe LLVM is just better at eliminating `alloca` instructions in the entry block.", "@gbaned : gentle ping", "@gbaned : gentle ping", "@gbaned Any idea why the changes aren't getting auto-merged?  I see @joker-eph as \"1 pending reviewer\", is that why?", "@sanjoy if you click on \"Show all checks\" and then on \"Details\" next to \"import/copybara\" you'll get to the internal version of the change and see that it does not pass all the internal tests and requires some manual intervention apparently.", "> @sanjoy if you click on \"Show all checks\" and then on \"Details\" next to \"import/copybara\" you'll get to the internal version of the change and see that it does not pass all the internal tests and requires some manual intervention apparently.\r\n\r\nWow, looks like the UI is pretty misleading then.  Without going through the steps you suggested, here is what I see:\r\n\r\n![Screen Shot 2020-07-05 at 1 15 49 PM](https://user-images.githubusercontent.com/136291/86541448-d9c8c300-bec1-11ea-88cd-dc8c7fdea2a5.png)\r\n\r\nI will file a bug internally.", "The discrepancy is because the branch PR is made from does not have scatter.hlo.test, which is the failing test:\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream/tree/google_upstream_rocm_platform_alloca_to_entry_block/tensorflow/compiler/xla/service/gpu/tests\r\nRebasing the PR on top of the main branch, and rerunning github checks will show the failure."]}, {"number": 40542, "title": "[Grappler] improve HoistCWiseUnaryChainsStage to support Reshape", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\n@ezhulenev \r\nThis allows the reshape nodes in the unary op chain to be processed by the optimizer HoistCWiseUnaryChainsStage.", "comments": ["@xinan-jiang Can you please check @rthadur's comments and keep us posted. Thanks!", "@xinan-jiang  Any update on this PR? Please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 40541, "title": "how to get image's shape using tf.data map", "body": "my dataset is large and the image is irregular. I need to pad the image according to different width and length, but I can't get the image's shape using image.shape. The shape is [None, None, 3].", "comments": ["Have you applied any image resizing/altering functions using `tf.image` operations? Namely, from here: https://www.tensorflow.org/api_docs/python/tf/image\r\n\r\nYou can chain together such operations in a separate function then pass it to a `tf.data.Dataset.map` function. For instance,\r\n\r\n```python\r\ndef preprocess_image(image):\r\n    # .. define your desired operations here\r\n    return preprocessed_image\r\n\r\nds = tf.data.Dataset.from_tensor_slices(...)\r\nds = ds.map(preprocess_image)\r\n```\r\n\r\n", "@colorful-ocean \r\nPlease update as per above comment.", "@iobtl\r\n```python\r\ndef process_image(image, hw_dst=416):\r\n\r\n    h_src, w_src = image.shape[:2]\r\n\r\n    if h_src>w_src:\r\n        scales = hw_dst/h_src\r\n        h_resize = hw_dst\r\n        w_resize = int(w_src*scales)\r\n    else:\r\n        scales = hw_dst/w_src\r\n        h_resize = int(h_src*scales)\r\n        w_resize = hw_dst\r\n    \r\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)#/255.\r\n    image = tf.image.resize(image, [h_resize, w_resize])\r\n    #image = tf.math.divide(image, 255.)\r\n    image = tf.image.pad_to_bounding_box(image, 0, 0, hw_dst, hw_dst)\r\n    return image, scales\r\n```\r\nI need to pad the image according to different width and length, like the function above. use map method can't work, because the **image.shape** is (None,None,3)", "@colorful-ocean How are you forming your `tf.data.Dataset` object? Could you try generating the output from the following code:\r\n```python\r\nds = tf.data.Dataset.from_tensor_slices(image_paths)\r\nds = ds.map(tf.io.read_file)\r\nds = ds.map(tf.image.decode_jpeg)\r\n\r\nfor image in ds.take(5):\r\n    print(image.shape)\r\n```\r\n\r\nTo verify your image shapes. Using `tf.data.Dataset.map` is not an issue even if you have different image shapes. The image.shape is (None, None, 3) because you have different image shapes for every image. ", "@iobtl \r\nyes\uff0cbut I need divide the img's h and w. This will get error, because it is None.\r\n```python\r\n    hr_image = tf.io.read_file(image_path)\r\n    hr_image = tf.io.decode_png(hr_image, channels=3)\r\n    \r\n    h_src, w_src = hr_image.shape[:2]\r\n    inp_h, inp_w = input_shape\r\n    \r\n    lr_h, lr_w = h_src//downscale+1, w_src//downscale+1\r\n    hr_h, hr_w = lr_h*downscale, lr_w*downscale\r\n```\r\n\r\n```\r\n    <ipython-input-2-1c8a4b52a1ec>:21 make_dataset  *\r\n        lr_h, lr_w = h_src//downscale+1, w_src//downscale+1\r\n\r\n    TypeError: unsupported operand type(s) for //: 'NoneType' and 'int'\r\n```\r\n**This problem has been bothering me**", "@iobtl\r\nI found the solution! \r\n**fail**\r\n```python\r\n    h_src, w_src = hr_image.shape[:2]\r\n```\r\n**fail**\r\n```python\r\n    h_src, w_src = tf.shape(hr_image)[:2]\r\n```\r\n**success**\r\n```python\r\n    h_src = tf.shape(hr_image)[0]\r\n    w_src = tf.shape(hr_image)[1] \r\n```\r\nfinally!", "h_src = tf.shape(hr_image)[0]; w_src = tf.shape(hr_image)[1]\n\n\nLe mar. 23 juin 2020 \u00e0 15:01, colorful-ocean <notifications@github.com> a\n\u00e9crit :\n\n> @iobtl <https://github.com/iobtl>\n> I found the solution!\n> *fail*\n>\n>     h_src, w_src = hr_image.shape[:2]\n>\n> *fail*\n>\n>     h_src, w_src = tf.shape(hr_image)[:2]\n>\n> *success*\n>\n>     h_src = tf.shape(hr_image)[0]\n>     w_src = tf.shape(hr_image)[1]\n>\n> finally!\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40541#issuecomment-648164466>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AHD3NAE4CGBWIAHVJM5H3NTRYCYTNANCNFSM4OAQAOMA>\n> .\n>\n", "using tf.shape does not seems to work depending on which method will use the image shape. For instance: \r\n\r\n**working**\r\n```python\r\ndef rnd_crop(img):\r\n    w = tf.shape(img)[0]\r\n    h = tf.shape(img)[1]\r\n    c = tf.shape(img)[2]\r\n    crop_size = (w,h,c)\r\n    img_crop = tf.reshape(img, crop_size)\r\n    return img_crop\r\n\r\ndataset = dataset.map(rnd_crop)\r\n```\r\n\r\n**not working** \r\n```python\r\ndef rnd_crop(img):\r\n    w = tf.shape(img)[0]\r\n    h = tf.shape(img)[1]\r\n    c = tf.shape(img)[2]\r\n    crop_size = (w,h,c)\r\n    img_crop = tf.image.random_crop(img,  crop_size)\r\n    return img_crop\r\n\r\ndataset = dataset.map(rnd_crop)\r\n```\r\n\r\nI am getting the folloring error \r\n\r\n>  ValueError: Dimensions must be equal, but are 4 and 3 for '{{node random_crop/GreaterEqual}} = GreaterEqual[T=DT_INT32](random_crop/Shape, random_crop/size)' with input shapes: [4], [3].\r\n\r\nit seems that the shape of the image in tf.image.random.crop does not has the same rank as the one returned by tf.shape... "]}, {"number": 40540, "title": "Modify LogOp to gry in auto_mixed_precision_test", "body": "`Log` had been removed to GrayList in commit bf8190aa.", "comments": ["@reedwm, can you please take a look"]}, {"number": 40537, "title": "Tensorflow tf function with numpy hyper-parameters and numpy calls", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: happens on CPU as well\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to build a `tf.function` which has numpy hyper-parameters and performs some numpy operations.\r\nI currently am getting the following error:\r\n\r\n```\r\nNotImplementedError: in user code:\r\n\r\n    <ipython-input-2-7da02a23bf62>:7 my_function  *\r\n        range_array = np.arange(range_lim)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:749 __array__  **\r\n        \" array.\".format(self.name))\r\n\r\n    NotImplementedError: Cannot convert a symbolic Tensor (strided_slice:0) to a numpy array.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI would like `tf.function` to be able to handle functions with numpy code in them.\r\n\r\n**Standalone code to reproduce the issue**\r\nMy current minimal reproducible example is the following ([colab link](https://colab.research.google.com/drive/1VWkX7sQP4QKd52qb4S1pCZXAJPjuWYpk?usp=sharing)):\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef my_function(params):\r\n  np_array = params['np_array']\r\n  my_list = []\r\n  for i in range(2):\r\n    range_lim = np_array[i]\r\n    range_array = np.arange(range_lim)\r\n    my_list.append(range_array)\r\n  # then do other things\r\n\r\nparams = {'np_array': np.array([4, 5])}\r\n\r\nmy_function(params)\r\n```\r\n\r\n**Other info / logs**\r\nWhen I look at the autograph generated code, using `print(tf.autograph.to_code(my_function.python_function))`, I see that the pure-numpy call `range_array = np.arange(range_lim)` is converted:\r\n\r\n```python\r\nrange_array = ag__.converted_call(np.arange, (range_lim,), None, fscope)\r\n```\r\nI think this is where the bug is coming from: autograph is called on a line which shouldn't be transformed.", "comments": ["@zaccharieramzi \r\nPlease refer to issues with similar error and let us know if it helps to resolve your issue:\r\n[link](https://stackoverflow.com/questions/58479556/notimplementederror-cannot-convert-a-symbolic-tensor-2nd-target0-to-a-numpy) [link1](https://github.com/krasserm/super-resolution/issues/35) [link2](https://cnasolution.com/questions/223061/notimplementederror-cannot-convert-a-symbolic-tensor-up-sampling2d-4-target-0-to-a-numpy-array).", "The [first one](https://stackoverflow.com/questions/58479556/notimplementederror-cannot-convert-a-symbolic-tensor-2nd-target0-to-a-numpy) is actually just a confusion in writing between tensors and numpy arrays.\r\nThe [second one](https://github.com/krasserm/super-resolution/issues/35) is seemingly a version problem.\r\nThe [third one](https://cnasolution.com/questions/223061/notimplementederror-cannot-convert-a-symbolic-tensor-up-sampling2d-4-target-0-to-a-numpy-array) from what I gather although not clear is also about the confusion between numpy arrays and tensors.", "@zaccharieramzi tf.function works best with TensorFlow ops; NumPy and Python calls are converted to constants", "Oh are you saying that `np_array` is converted to a constant Tensor? that's why `range_lim` would be a Tensor (`(strided_slice:0)` more specifically) that can't be used to do a numpy call.\r\n\r\nI think maybe then it should be made clearer in the documentation that all python and numpy objects are converted to constants (I don't think it's the call that is converted to a constant but rather the object itself). Wdyt?", "@zaccharieramzi Its mentioned clearly in the documentation [here](https://www.tensorflow.org/guide/function). Please take a look at it and let me know if it helps. Thanks!", "Hahaha ok my bad indeed !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40537\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40537\">No</a>\n"]}, {"number": 40536, "title": "Combining multiple savedmodels into one SavedModel", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**Describe the expected behavior**\r\nI was able to combine multiple tf estimator savedmodels (linear classifiers) into one and then serve that final model instead of serving multiple models. I assumed the same process would work for boosted trees saved models.\r\n\r\n**Describe the current behavior**\r\n\r\nBut when it comes to boosted trees, i am experiencing the following error :\r\n\r\nNotFoundError:  Resource localhost/boosted_trees/QuantileAccumulator/_load_2153_load_11463/N10tensorflow34BoostedTreesQuantileStreamResourceE does not exist.\r\n\t [[node import/boosted_trees/BoostedTreesQuantileStreamResourceGetBucketBoundaries_1 (defined at <ipython-input-136-92a81f76cf4e>:2) ]] [Op:__inference_pruned_12525]\r\n\r\nFunction call stack:\r\npruned\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nModel1, Model2 are my two loaded linear classifer estimators on the same feature set but for different cities; replacing them with boosted trees estimators yields the error \r\n\r\n# \r\nclass CombinedModel(tf.Module):\r\n    def __init__(self):\r\n        self.model1 = model1\r\n        self.model2 = model2\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32),\r\n                                 tf.TensorSpec(shape=[None], dtype=tf.float32),\r\n                                 tf.TensorSpec(shape=[None], dtype=tf.string)])\r\n    def __call__(self, feature1, feature2, city):\r\n        input1 = {\r\n            \"feature1\" : feature1,\r\n            \"feature2\": feature2\r\n        }\r\n        print(type(allotment_eta))\r\n        city_chennai = tf.constant([\"chennai\"])\r\n        city_mumbai = tf.constant([\"mumbai\"])\r\n\r\n        # IF CITY IS CHENNAI\r\n        if tf.equal(city[0],city_chennai):\r\n            inference_func = self.model1.signatures[\"predict\"]\r\n            predictions = inference_func(**({\"feature1\": feature1, \"feature2\": feature2}))        \r\n            return predictions['probabilities']\r\n        # ELSE CITY IS MUMBAI\r\n        else :\r\n            inference_func = self.model2.signatures[\"predict\"]\r\n            predictions = inference_func(**({\"feature1\": feature1, \"feature2\": feature2}))        \r\n            return predictions['probabilities']\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@yaswitag1 \r\nPlease, let us which Tensorflow version you are using?. Request you to share colab link or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Hey,\r\nThe notebook can be found in the link attached below.\r\nhttps://drive.google.com/file/d/1LaLmnzHAeRdqw-2LDv8Ew_TD-CGB7jvI/view?usp=sharing\r\n", "I have tried in colab with TF version 2.2 ,nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d8b40c810496be63402ca0427ff45f4d/untitled39.ipynb).Thanks!", "Hey,\r\n  Are there any updates on this ?", "@yaswitag1 I tried to execute the code in TF 2.5 and facing different error. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/d4362e264996d6c54a5bbb2f6b87938a/untitled39.ipynb#scrollTo=x2I_IrKzp5Zb&uniqifier=3).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40536\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40536\">No</a>\n"]}, {"number": 40535, "title": "Combining multiple SavedModels into one SavedModel", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI was able to combine multiple tf estimator savedmodels (linear classifiers) into one and then serve that final model instead of serving multiple models. But when it comes to boosted trees, i am experiencing an the following error :\r\n\r\nNotFoundError:  Resource localhost/boosted_trees/QuantileAccumulator/_load_2153_load_11463/N10tensorflow34BoostedTreesQuantileStreamResourceE does not exist.\r\n\t [[node import/boosted_trees/BoostedTreesQuantileStreamResourceGetBucketBoundaries_1 (defined at <ipython-input-136-92a81f76cf4e>:2) ]] [Op:__inference_pruned_12525]\r\n\r\nFunction call stack:\r\npruned\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nServing on port gives enables easier maintenance \r\n**Any Other info.**\r\n", "comments": ["@yaswitag1,\r\nLooks like this is a duplicate of [#40536](https://github.com/tensorflow/tensorflow/issues/40536). Can you please close this issue since it is already being tracked there. Thanks!"]}, {"number": 40534, "title": "Need: Tensorflow for Unity plugin", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.1\r\n\r\n**Feature requested**\r\n-Tensorflow for Unity plugin needed. \r\n\r\nFor three reasons:\r\n\r\n1.Although tensorflow lite can be used to build Android or IOS apps, developer prefer to use cross-platform IDEs so they only need to adapt and develop once.\r\n\r\n2. Native Andoird (Android studio) or IOS (Xcode) are not really designed for 3D apps. For 3D developers, it is hard to create 3D scene and manipulate them in Android studio or Xcode. On the other hand, more and more AI  module in tensorflow can now show us 3d informaitons from 2D image (facemesh, hand pose, etc.), a 3d based IDE + tensorflow would be a better solution to use these 3d information in real productive apps.\r\n\r\n3. There is no official release of Unity Tensorflow plugin, I tried some no-official ones, some of them can not use GPUs, some of them are full of bugs. Developers are strugglling finding a right version. If we have an official version, a large number of user would be increased.\r\n\r\n", "comments": ["Here's an [example TFLite Unity plugin](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/examples/unity/TensorFlowLitePlugin). There are some community examples using this plugin, e.g., https://github.com/asus4/tf-lite-unity-sample. We don't have any plans to release a full TF Unity plugin."]}, {"number": 40533, "title": "how to build no stripped tensorflow lite so", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?:no\r\n- Bazel version (if compiling from source): 0.25.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build  --strip=never -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n\r\ni want build an no stripped so so that i can investigate the crash from user, even i add the '-strip=never', the so is also stripped\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I found the strip flag is in tensorflow/lite/build_def.bzl and define in this method`tflite_symbol_opts`, after comment the '-s ', the so built will no stripped :\r\n    \r\n    def tflite_symbol_opts():\r\n        \"\"\"Defines linker flags whether to include symbols or not.\"\"\"\r\n        return select({\r\n            \"//tensorflow:android\": [\r\n                \"-latomic\",  # Required for some uses of ISO C++11 <atomic> in x86.\r\n            ],\r\n            \"//conditions:default\": [],\r\n        }) + select({\r\n            \"//tensorflow:debug\": [],\r\n            \"//conditions:default\": [],\r\n            #     \"-s\",  # Omit symbol table, for all non debug builds\r\n            # ],\r\n        })", "By reading tflite_symbol_opts definition, it sounds doable if you do a debug build \r\n e.g. include \"//tensorflow:debug\" as a dependency.", "You'd better use debug build.\r\n\r\n```\r\nbazel build -c dbg --fat_apk_cpu=arm64-v8a,armeabi-v7a\r\n--host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n//tensorflow/lite/java:tensorflow-lite\r\n```", "Actually, the origin problem is the app crashed and i had collected the crashed address, i need translate the address to the code line number.", "Once you have stack traces with debug build aar, you can use ndk-stack.\r\nhttps://developer.android.com/ndk/guides/ndk-stack", "> Once you have stack traces with debug build aar, you can use ndk-stack.\r\n> https://developer.android.com/ndk/guides/ndk-stack\r\n\r\n@terryheo  Thanks for your reply , i can't reproduce the crash with my own app, so i checkout the code to the version used in the app, and build a so with symbols, then translate the user crash address to the code line number.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40533\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40533\">No</a>\n"]}, {"number": 40532, "title": "segfault during tf.image.non_max_suppression_padded", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nSegmentation fault occurs in c++ when passing a large value for `max_output_size`  and `True` for `pad_to_max_output_size`.\r\n\r\n**Describe the expected behavior**\r\nNo segmentation fault. I would expect a proper error handling exception if necessary rather than a segfault.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nboxes = [[4.0, 6.0, 3.0, 6.0],\r\n       [2.0, 1.0, 5.0, 4.0],\r\n       [9.0, 0.0, 9.0, 9.0]]\r\nscores = [5.0, 6.0, 5.0]\r\nmax_output_size = 1000000000000\r\n\r\ntf.image.non_max_suppression_padded(\r\n   boxes, scores, max_output_size, iou_threshold=0.5,\r\n    score_threshold=float('-inf'), pad_to_max_output_size=True, name=None)\r\n)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@mjkim720,\r\nI was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/8fa34dfa5f2649c6717708afa5ffba9d/40532.ipynb). However, the issue seems to be fixed with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/327bcc67c1e0f310feb39ca2b12b0642/40532-tf-nightly.ipynb#scrollTo=ZvE2D_WQv2Mk). \r\n\r\nOn passing a large value, the code throws an `InvalidArgumentError` stating `InvalidArgumentError: Need k >= 0, got -727379968 [Op:TopKV2]`. Please check the linked gist. Thanks!", "@amahendrakar @mihaimaruseac Thank you for your feedback. It seems the segfault is gone in the nightly version, but the exception message is a bit misleading because it does not reflect the actual input. I passed a large positive value, but the exception message complains about a negative value, which is never given.  I suspect there is an overflow going on internally. ", "@mjkim720,\r\nUpon running your code in the latest version of Tensorflow (2.4.1), I think proper error is being raised. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/e800797cb02ffb7b88988ec9ac5188ac/40532-tf-nightly.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40532\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40532\">No</a>\n"]}]