[{"number": 39082, "title": "google.protobuf.message.DecodeError With Multi-GPU and Mirrored Strategy on 2.2rc3 (and 2.2rc4)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): `python3 -m pip install tensorflow-gpu==2.2rc3` (and 2.2rc4)\r\n- TensorFlow version (use command below): `v2.2.0-rc2-77-gaad398b5e9 2.2.0-rc3`\r\n- Python version: `3.6.9`\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **CUDA - 10.1, cuDNN 7.6.4**\r\n- GPU model and memory: **eight Tesla V100-SXM2-16GB**\r\nouput of `nvidia-smi topo -m`\r\n\r\n```\r\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tmlx5_0\tmlx5_1\tmlx5_2\tCPU Affinity\r\nGPU0\t X \tNV1\tNV1\tNV2\tNV2\tSYS\tSYS\tSYS\tPIX\tPHB\tSYS\t0-19,40-59\r\nGPU1\tNV1\t X \tNV2\tNV1\tSYS\tNV2\tSYS\tSYS\tPIX\tPHB\tSYS\t0-19,40-59\r\nGPU2\tNV1\tNV2\t X \tNV2\tSYS\tSYS\tNV1\tSYS\tPHB\tPIX\tSYS\t0-19,40-59\r\nGPU3\tNV2\tNV1\tNV2\t X \tSYS\tSYS\tSYS\tNV1\tPHB\tPIX\tSYS\t0-19,40-59\r\nGPU4\tNV2\tSYS\tSYS\tSYS\t X \tNV1\tNV1\tNV2\tSYS\tSYS\tPIX\t20-39,60-79\r\nGPU5\tSYS\tNV2\tSYS\tSYS\tNV1\t X \tNV2\tNV1\tSYS\tSYS\tPIX\t20-39,60-79\r\nGPU6\tSYS\tSYS\tNV1\tSYS\tNV1\tNV2\t X \tNV2\tSYS\tSYS\tPHB\t20-39,60-79\r\nGPU7\tSYS\tSYS\tSYS\tNV1\tNV2\tNV1\tNV2\t X \tSYS\tSYS\tPHB\t20-39,60-79\r\nmlx5_0\tPIX\tPIX\tPHB\tPHB\tSYS\tSYS\tSYS\tSYS\t X \tPHB\tSYS\t\r\nmlx5_1\tPHB\tPHB\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tPHB\t X \tSYS\t\r\nmlx5_2\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\tPHB\tPHB\tSYS\tSYS\t X \t\r\n\r\n```\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running the following:\r\n\r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\nrun_config = tf.estimator.tpu.RunConfig(\r\n      ...\r\n      train_distribute=strategy\r\n  )\r\ntrain_estimator = tf.estimator.tpu.TPUEstimator(\r\n        ...\r\n        config=run_config)\r\ntrain_estimator.train(...)\r\n```\r\n\r\nThe following exception occurs, when at least one visible GPU is not connected via NVLink. \r\n\r\n```\r\nINFO:tensorflow:training_loop marked as finished\r\nI0501 17:10:16.417816 139720608089920 error_handling.py:115] training_loop marked as finished\r\nWARNING:tensorflow:Reraising captured error\r\nW0501 17:10:16.418111 139720608089920 error_handling.py:149] Reraising captured error\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nParsing Inputs...\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 427, in <module>\r\n    tf.app.run(main)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"main.py\", line 283, in main\r\n    FLAGS.train_batch_size))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3083, in train\r\n    rendezvous.raise_errors()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py\", line 150, in raise_errors\r\n    six.reraise(typ, value, traceback)\r\n  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3078, in train\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 349, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1180, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1242, in _train_model_distributed\r\n    self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1356, in _actual_train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1514, in _train_with_estimator_spec\r\n    log_step_count_steps=log_step_count_steps) as mon_sess:\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 737, in __init__\r\n    h.begin()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 563, in begin\r\n    self._summary_writer = SummaryWriterCache.get(self._checkpoint_dir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer_cache.py\", line 63, in get\r\n    logdir, graph=ops.get_default_graph())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py\", line 372, in __init__\r\n    super(FileWriter, self).__init__(event_writer, graph, graph_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py\", line 84, in __init__\r\n    self.add_graph(graph=graph, graph_def=graph_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py\", line 194, in add_graph\r\n    true_graph_def = graph.as_graph_def(add_shapes=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3136, in as_graph_def\r\n    result, _ = self._as_graph_def(from_version, add_shapes)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3053, in _as_graph_def\r\n    graph.ParseFromString(compat.as_bytes(data))\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\n\r\nHowever when limiting this to GPUs connected via NVLink (i.e. `CUDA_VISIBLE_DEVICES=\"0,1,2,3\"` or `CUDA_VISIBLE_DEVICES=\"4,5,6,7\"`) this error does not occur. It does occur even when 2 GPUs are given that aren't NVLink connected (i.e. `CUDA_VISIBLE_DEVICES=\"0,5\"`).\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nI was unable to reproduce using a minimal example, however the code is a [distributed fork of google brain's efficientdet](https://github.com/fsx950223/automl).\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@mattangus Using MirroredStrategy with TPUEstimator may result in unexpected errors. Can you try using [TPUStrategy or MirroredStrategy](https://www.tensorflow.org/guide/distributed_training)? Thanks!", "I am currently using `MirroredStrategy`.  I don't have access to TPUs so I don't think I can use `TPUStrategy`.\r\n\r\nI have also tried this with just creating a `MonitoredTrainingSession`, no `Estimator`s and I got the same error.", "The skeleton of that version of my code is:\r\n\r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    dataset = input_fn(...)\r\n    ...\r\n    loss, train_op = strategy.experimental_run_v2(model_fn,\r\n                          args=(...))\r\n    train_group = tf.group(train_op.values)\r\n\r\nwith tf.train.MonitoredTrainingSession(...) as sess:\r\n    for i in range(max_steps):\r\n          sess.run(train_op)\r\n          if sess.should_stop():\r\n               break\r\n          sess.run(train_group)\r\n```\r\n\r\nBut the same error happened.", "@mattangus Just to confirm, does this happen without MirroredStrategy? I need a standalone reproducible example to debug this issue. Can you use the skeleton code you pasted above to make a more complete example?\r\nWe haven't tested MirroredStrategy without Estimator so it would be preferable to use Estimators in the example above. We are also focusing on Keras which is the high level API of TensorFlow. Would you be able to use Keras instead?", "This doesn't happen without MirroredStrategy (for example on a single GPU). Only with MirroredStrategy *and* a sufficient number of GPUs. The error doesn't happen with a very small example (adding two variables). I have yet to find a sufficiently self contained example where this occurs.\r\n", "@anj-s Could the graph def be too large? In the function `tensorflow/python/framework/ops.py(3053)_as_graph_def()` there is a bit in the docstring saying:\r\n\r\n> Raises:\r\n      ValueError: If the `graph_def` would be too large.\r\n\r\nUsing `pdb.post_mortem` I get that `len(data)` is `5167271945` which is about `4.8` GB. ", "I'm not sure if this is helpful at all but the result of `print(graph)` after the error occurs is:\r\n\r\n```\r\ngraph\r\nnode {\r\n  name: \"Const\"\r\n  op: \"Const\"\r\n  device: \"/replica:0/task:0/device:CPU:0\"\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_FLOAT\r\n        tensor_shape {\r\n          dim {\r\n            size: 196416\r\n            name: \"\\010\\004\"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```", "@mattangus It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.4.1 or 2.5.0 and let us know if the issue still persists? Thanks!", "I think this issue can be closed. I no longer have access to the machine that was causing problems so I cannot test if an upgrade would work.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39082\">No</a>\n"]}, {"number": 39081, "title": "Broken URL in Fairness Indicators documentation", "body": "On this documentation [page](https://www.tensorflow.org/tfx/guide/fairness_indicators), the URL linked at the end of the paragraph under \"Data\" is broken.\r\n\r\n<img width=\"719\" alt=\"Screen Shot 2020-05-01 at 10 22 37\" src=\"https://user-images.githubusercontent.com/20053362/80825676-b6b62600-8b95-11ea-90bd-84d7e64f0b2f.png\">\r\n\r\nIt directs to https://www.tensorflow.org/tfx/guide/bit.ly/fairness-indicators-guidance, but that leads to an Error 404 page.", "comments": ["This URL ``` https://www.tensorflow.org/tfx/guide/bit.ly/fairness-indicators-guidance ```\r\n\r\nNeeds to be changed to [this](https://www.tensorflow.org/tfx/fairness_indicators/guidance) url.\r\n", "@catyeo18,\r\nSince this issue is regarding TFX, could you please create a new issue in the TFX repo from [this link](https://github.com/tensorflow/tfx/issues/new), so that we can track it there. Thanks!", "New issue in TFX repo has been created [here](https://github.com/tensorflow/tfx/issues/1769). Closing this issue now."]}, {"number": 39080, "title": "About website link 404 not found in README", "body": "Could you add the website link in the following url\uff1a\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.13/tensorflow/contrib/quantize\r\n\r\n![image](https://user-images.githubusercontent.com/35229624/80811968-97e97b00-8bf9-11ea-9752-8b65e886e8d5.png)\r\nthe link marked in blue is 404 not found.\r\n\r\nThank you!\r\n\r\n", "comments": ["The `contrib/quantize` is deprecated. We recommend using [post-training quantization](https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html) or [quantization aware training](https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html) approach with TF 2.X \r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39079, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows Version\t10.0.18362 Build 18362\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  NA\r\n- TensorFlow installed from (source or binary):  pip3 install keras and tensorflow, confirmed using Anaconda Navigator 1.9.12\r\n- TensorFlow version (use command below):  2.1.0 based on Anaconda, commands below for TF 1.0 and 2.0 both failed with same error: ImportError: DLL load failed: The specified module could not be found.\r\n- Python version:   Python 3.7.6\r\n- Bazel version (if compiling from source):  NA\r\n- GCC/Compiler version (if compiling from source):  NA\r\n- CUDA/cuDNN version:  CUDAtoolkit 10.0.130 and cuDNN 7.6.5 were installed later, but did not resolve the issue/bug\r\n- GPU model and memory:  Intel(R) HD Graphics 620 with 1 GB RAM\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nfrom keras.models import Sequential\r\n\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nOR\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\ncaused the same problems, but starting with:\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  **File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>**\r\n    from tensorflow_core import *\r\n\r\nLine 101 shows up below as well, and the rest of the error continues similarly: lines 40, 50, 44, 127, 49, 74, 58, 28, 24, 242, 342\r\n\r\n\r\n\r\nUsing TensorFlow backend.\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-9c5e0a19b646>\", line 1, in <module>\r\n    from keras.models import Sequential\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  **File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>**\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Max\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI'm not sure.  I'm new to Python using Keras and Tensorflow.  \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nfrom keras.models import Sequential\r\nOR\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nOR\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n[Error messages - LSTM RNN, Keras Tensorflow.txt](https://github.com/tensorflow/tensorflow/files/4564109/Error.messages.-.LSTM.RNN.Keras.Tensorflow.txt)\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "This error occurs to many Users \r\n\r\nThis occurs because you may not have Visual C++ Redistributable for Windows. Please download it.\r\n\r\nRefer [here](https://www.tensorflow.org/install/pip#windows) to check if you have installed properly\r\n\r\nAlso, refer similar issues.\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!\r\n\r\nPlease, check Your CPU/Python is on 32 bits?\r\n\r\nPlease, refer #36167 #38615 and see if it helps you. \r\n\r\nThanks!", "@maxlooo \r\n\r\nSolution should be downloading and installing visual studio 2015-2019 x86 and x64 from here:https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads.Thanks!", "Closing as low effort duplicate", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39079\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39079\">No</a>\n"]}, {"number": 39078, "title": "TPU usage while running a model using Transfer Learning ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Google colab version\r\n- TensorFlow version (use command below):2.2.0rc3\r\n- Python version:3.6.9\r\n- Bazel version (if compiling from source):No\r\n- GCC/Compiler version (if compiling from source):No\r\n- CUDA/cuDNN version:No\r\n- GPU model and memory: TPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Showing this error while usingTransfer learning and TPU together **\r\n `\r\nFailedPreconditionError: Error while reading resource variable efficientnet-b0/stem/conv2d/kernel_31710 from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/efficientnet-b0/stem/conv2d/kernel_31710/N10tensorflow3VarE does not exist.`\r\n\r\n**The model should start fitting the data**\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/14Gx2l195vQaCC_GWoyhgW_Qnwsfk5vGN#scrollTo=2CVARbH2K01F&line=1&uniqifier=1\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hey, @prudhvirajboddu kindly set the share option your colab to everyone.", "https://www.github.com/prudhvirajboddu/TensorFlowML/tree/master/Classification_of_Flowers_using_a_TPU.ipynb\n\nHere it is not able to run an transfer learning models and also Tensoboard callbacks aren't working when using TPU in colab", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/4d06a579310ffead5fae8438f493e861/39078-2-1.ipynb), and [TF v2.2.0rc4](https://colab.research.google.com/gist/amahendrakar/b4044a2dfd7d20d80933ec65f1725303/39078.ipynb#scrollTo=nEip_PfXU5FB). Please find the attached gist. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39078\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39078\">No</a>\n"]}, {"number": 39077, "title": "Add extra platform support for ARC processors", "body": "In this pull request support for the synopsys EMSDP platform is added, as well as extra slicing support for reading tensors from external memory.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39077) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39077) for more info**.\n\n<!-- ok -->", "Thanks for your feedback Pete.\r\nI will discuss internally if and how we can download the .tcf files.\r\nWith respect to the scratch buffers, we have implemented this in such a way that everything mostly works out of the box with reasonable performance, and that if users want to improve on performance they can play with the sizes of the scratch buffers (as described in the readme) and with the placement of the buffers in the linker command file. The arc platform has very powerful local memories. For graphs that completely fit into local memory, the scratch buffers are not needed. The implementation of the slicing logic and scratch buffer usage is such that it supports the case where both  tensor arena and flat-buffer are in local memory, and the case where both are in system memory, and any combination of this.\r\nI hope this clarifies things a bit.\r\n", "@JaccovG Can you please resolve conflicts? Thanks!", "@JaccovG sorry for the delay, can you please correct below errors \r\n\r\n`Error running (PERMANENT_ERROR):\r\n  - File 'tensorflow/lite/micro/tools/make/targets/arc/emsdp/emsdp.lcf' failed validation 'Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)''.\r\n  - File 'tensorflow/lite/micro/tools/make/targets/arc/emsdp/uboot.env' failed validation 'Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)''.\r\n  - File 'tensorflow/lite/micro/examples/person_detection_experimental/arc_emsdp/emsdp.lcf' failed validation 'Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)''.\r\n  - File 'tensorflow/lite/micro/tools/make/targets/arc/emsdp/emsdp_v2.lcf' failed validation 'Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)''.\r\n  - 4 file(s) failed the validation of Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)'.`", "License details was added to all mentioned LCF files. Please check it. \r\n\r\nJust to make it clear, *tensorflow/lite/micro/tools/make/targets/arc/emsdp/uboot.env* from the list above is auto generated file and can\u2019t be complemented with license information.", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 39076, "title": "LSTM/GRU performance discrepancy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora32/Colab/Ubuntu 19.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0rc4/2.2.0rc3/2.1.0\r\n- Python version: 3.7.6/3.7.4/3.6.9\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 970 4GB/Colab GPU/NVIDIA Tesla P100 16GB\r\n\r\n**Describe the current behavior**\r\nI'm experimenting with models that require the state of LSTMs to be processed after after each time step. So I tried unrolling the LSTM layer in a loop, iterating over the time steps. That's were I noticed a huge increase in time per training step. Uppon further investigation I noticed that this is also the case in some other configuration featuring LSTMs/GRUs. Different versions of TensorFlow are effected differently. See below:\r\n\r\nSome results:\r\n|    | Colab | Compute Cloud | Local |\r\n| ------------- | ------------- | ------------- | ------------- |\r\n| Keras Sequential with LSTMs  | 17ms/step  | 15ms/step | 17ms/step |\r\n| Keras Sequential with GRUs  | 17ms/step  | 14ms/step | 16ms/step |\r\n| [Custom Model](https://www.tensorflow.org/guide/keras/custom_layers_and_models) with LSTMs  | 17ms/step  | 14ms/step | 17ms/step |\r\n| Custom Model with GRUs  | 88ms/step  | 14ms/step | 40ms/step |\r\n| Custom Model with LSTMs in loop  | 120ms/step  | 173ms/step | 81ms/step |\r\n| Custom Model with GRUs in loop  | 107ms/step  | 153ms/step | 66ms/step \r\n| Custom Model with compat LSTMs  | 19ms/step  | 15ms/step | 17ms/step |\r\n| Custom Model with compat GRUs  | 20ms/step  | 15ms/step | 17ms/step |\r\n| Custom Model with compat LSTMs in loop  | 87ms/step  | 108ms/step | 62ms/step |\r\n| Custom Model with compat GRUs in loop  | 78ms/step  | 86ms/step | 49ms/step |\r\n\r\nColab: TF 2.2.0rc3, Python 3.6.9, with GPU\r\nCompute Cloud: TF 2.1.0, Python 3.7.4, Cuda 10.1, Nvidia Tesla P100, Ubuntu 19.04\r\nLocal: TF 2.2.0rc4, Python 3.7.6, Cuda 10.1, Nvidia GTX 970, Fedora 32\r\n\r\n![index](https://user-images.githubusercontent.com/64591330/80797451-28946c80-8ba2-11ea-9196-b526c4767bd6.png)\r\n\r\n**Describe the expected behavior**\r\nTime per step should be more or less similar.\r\n\r\n**Standalone code to reproduce the issue** \r\n[Colab Notebook](https://colab.research.google.com/drive/19-JZo_iJwsoR2gqpXtuN_sbBkEyQtxx4)\r\n", "comments": ["@AwekasGerald \r\nI ran the code shared and face an error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/bd5bc7699b1084438ef8bfa778ef7d90/39076.ipynb)", "Please make sure to use a **colab runtime with GPU**!\r\n(runtime -> change runtime type -> select \"GPU\" as hardware accelerator)", "@AwekasGerald\r\nI ran the code on gpu, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/2e3fc015fd92aca06e4d8218e5a00501/39076.ipynb) and let us know if this confirms your issue.", "Yes, it does.", "Thanks for reporting the issue. I think the performance discrepancy is expected between a python while loop vs a fused LSTM/GRU layer.\r\n\r\nIn TF2, we use cudnn kernels in tf.keras.layers.LSTM/GRU, which will execute the whole timesteps in one op. This is usually much more fast than an unrolled for loop, which will execute the individual timesteps one by one (while will be slower than tf.while)."]}, {"number": 39075, "title": "ForwardAccumulator fails with `experimental_run_functions_eagerly(True)`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos Catalina\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  `2.2.0rc4`\r\n- Python version: `3.7.5`\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n**Describe the current behavior**\r\nRunning the examples in [tf.ForwardAccumulator docs](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) fail with `RecursionError: maximum recursion depth exceeded` when running with `tf.config.experimental_run_functions_eagerly(True)`.\r\n\r\n**Describe the expected behavior**\r\nRunning the examples in [tf.ForwardAccumulator docs](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) with `tf.config.experimental_run_functions_eagerly(True)` work the same way as when running with `tf.config.experimental_run_functions_eagerly(False)`.\r\n\r\n**Standalone code to reproduce the issue**\r\nThis is the standard example from https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator, with just the `experimental_run_functions_eagerly(True)` call added.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.config.experimental_run_functions_eagerly(True)\r\n\r\n\r\nv = tf.Variable([1., 2.])\r\nwith tf.autodiff.ForwardAccumulator(\r\n    v,\r\n    # The \"vector\" in Hessian-vector product.\r\n    tf.constant([1., 0.])) as acc:\r\n  with tf.GradientTape() as tape:\r\n    y = tf.reduce_sum(v ** 3.)\r\n  backward = tape.gradient(y, v)\r\nbackward  # gradient from backprop\r\n\r\nacc.jvp(backward)  # forward-over-backward Hessian-vector product\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n...\r\n    self._push_tape()\r\n  File \"/Users/hartikainen/conda/envs/policy-evaluation/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\", line 849, in _push_tape\r\n    watch_accessed_variables=self._watch_accessed_variables)\r\n  File \"/Users/hartikainen/conda/envs/policy-evaluation/lib/python3.7/site-packages/tensorflow/python/eager/tape.py\", line 48, in push_new_tape\r\n    return Tape(tape)\r\nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\n", "comments": ["@hartikainen \r\n\r\nI have tried in colab with TF 2.1.0, 2.2-rc4 and i am able to reproduce the issue.With `tf.config.experimental_run_functions_eagerly(True)` i am able to reproduce the issue.However with `tf.config.experimental_run_functions_eagerly(False) `i am not seeing any issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/6a9e8b37f8f59badd116f25c9de947f9/untitled847.ipynb).Thanks!", "Yep, that's what I see too: fails with `tf.config.experimental_run_functions_eagerly(True)` and works with `tf.config.experimental_run_functions_eagerly(False)`. Sorry if that was not clear from the title and description.", "Thank you for the report. I'll opt that forwardprop utility function out of run_functions_eagerly. The change should land in a few hours.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39075\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39075\">No</a>\n"]}, {"number": 39074, "title": "Loading model with add_loss fails", "body": "**System information**\r\n- Have I written custom code: Yes. Minimal Failing Example given below.\r\n- OS Platform and Distribution: MacOs Mojave Version 10.14.6. Also tested on Linux Ubuntu 18.04.\r\n- TensorFlow installed from: binary\r\n- TensorFlow version (use command below): 2.1.0 (v2.1.0-rc2-17-ge5bf8de410 2.1.0)\r\n- Python version: 3.7.7\r\n- CPU execution only.\r\n\r\n**Describe the current behavior**\r\n\r\nModel loading using `tf.keras.models.load_model` does not work for models with custom layers that add losses with `self.add_loss`. \r\n\r\nIn the example below, I create a one-layer model with a custom layer. The layer adds a dummy loss using `self.add_loss`. This is the only loss of the model and there is no other loss passed to `model.compile('adam')`, which is intentional. The model compiles correctly and can successfully be stored to disk in `SavedModel` format with `model.save('my_model')`.\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    \"\"\"Imaginary Layer that adds a custom loss in the call\"\"\"\r\n\r\n    def __init__(self, a):\r\n        super().__init__()\r\n        self.var = tf.Variable(a, name='var_a')\r\n\r\n    def call(self, inputs, training=False):\r\n        output = tf.reduce_sum(inputs * self.var, axis=-1)\r\n        self.add_loss(tf.reduce_mean(output))\r\n        return output\r\n\r\n\r\ndef get_model(input_dim: int) -> tf.keras.Model:\r\n    layer = CustomLayer(0.1)\r\n    inputs = tf.keras.Input((input_dim,), name=\"inputs\")\r\n    outputs = layer(inputs)\r\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n    model.compile(optimizer='adam')\r\n    return model\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    num_data = 100\r\n    X, Y = np.random.randn(num_data, 1), np.random.randn(num_data, 1)\r\n\r\n    model = get_model(input_dim=X.shape[-1])\r\n    model.summary()\r\n    print(\"model.losses\", model.losses)\r\n\r\n    model.save('my_model')\r\n    reconstructed_model = tf.keras.models.load_model('my_model')  # ~~ breaks \r\n    reconstructed_model.summary()\r\n\r\n    # Let's check:\r\n    np.testing.assert_allclose(\r\n        model.predict(X),\r\n        reconstructed_model.predict(X)\r\n    )\r\n```\r\n***PROBLEM:*** The issue arises when loading the model. When the load method tries to compile the model the program terminates with the following stack trace:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"save_model.py\", line 36, in <module>\r\n    reconstructed_model = tf.keras.models.load_model('my_model')\r\n  File \"/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 99, in load\r\n    training_config))\r\n  File \"/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 446, in compile\r\n    self._compile_weights_loss_and_weighted_metrics()\r\n  File \"/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1592, in _compile_weights_loss_and_weighted_metrics\r\n    self.total_loss = self._prepare_total_loss(masks)\r\n  File \"/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1691, in _prepare_total_loss\r\n    raise ValueError('The model cannot be compiled '\r\nValueError: The model cannot be compiled because it has no loss to optimize.\r\n```\r\nSo it looks like the model can not be reconstructed/compiled because there is no loss specified. The original model, however, did have a loss:\r\n```python\r\nprint(\"model.losses\", model.losses)\r\n>>> model.losses [<tf.Tensor 'custom_layer/Mean:0' shape=() dtype=float32>]\r\n```\r\nInterestingly, if we load the model without compiling we get\r\n```python\r\nreconstructed_model = tf.keras.models.load_model('my_model', compile=False)\r\nprint(\"reconstructed_model.losses\", reconstructed_model.losses)\r\n>>> reconstructed_model.losses []\r\n```\r\nwhich indicates that the losses indeed aren't correctly loaded into the reconstructed model. Not compiling the model, however, is not an option as the `model.predict` doesn't work as long as the model is not compiled.\r\n\r\n***Additional info:***\r\n1) Passing the source code of the layer in the load method results in the same crash:\r\n```python\r\nreconstructed_model = tf.keras.models.load_model('my_model', custom_objects={\"CustomLayer\": CustomLayer})\r\n```\r\nHowever, in my use-case I don't have access to the source code at load time. So this solution would not fit my needs (but is also doesn't work).\r\n\r\n2) Interestingly, the program terminates correctly when the original model does not get compiled. Again, this doesn't fit my use-case.\r\n\r\n**Describe the expected behavior**\r\n\r\nAccording to the docs, the Keras API `save_model()` and the `SavedModel` format supports the saving and loading of models that add losses using `add_loss` in their `call` method. \r\n\r\nMany thanks for the support.\r\n \r\n", "comments": ["@vdutor Thanks for the report. This was resolved recent TF version released after `TF2.1`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/38ae04d64871be44312671101a93e5cb/untitled142.ipynb) is a gist with `TF2.2.0rc4` for your reference. If you like using stable version, there will be `TF2.2` releasing soon in near future. Thanks\r\n\r\nI am closing this issue as it was resolved. Please feel free to reopen if the issue persists in `TF2.2`. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39074\">No</a>\n", "Hi @jvishnuvardhan \r\nThanks for your swift response. The snippets executes successfully in `2.2.0rc4` as per your suggestion. However, there is still a difference when calling `model.losses`:\r\n```python\r\nprint(\"model.losses:\", model.losses)\r\nprint(\"reconstructed_model.losses:\", reconstructed_model.losses)\r\n```\r\ngives \r\n```bash\r\nmodel.losses: [<tf.Tensor 'custom_layer/Mean:0' shape=() dtype=float32>]\r\nreconstructed_model.losses: []  # <-- empty\r\n``` \r\nThis problem is immediately noticeable when running `reconstructed_model.fit(X)`, which now crashes. Happy to create a new issue for this if you agree this is a bug.\r\nThanks", "@vdutor saving model with a custom layer is little different when compared to other models that don't use `custom_objects`. When we use `custom_objects`, we need to add implement `get_config` method in the custom_layer and mention custom layer as `custom_objects` while loading.\r\n\r\nI added these lines\r\n```\r\n    def get_config(self):\r\n      return {'a': self.var.numpy()}\r\n\r\n    reconstructed_model = tf.keras.models.load_model('my_model',custom_objects={'CustomLayer':CustomLayer})  # ~~ breaks \r\n```\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/e6dc7d848a80df4cf039159dbb54d5ac/39074.ipynb). Hope this helps. Please let us know if you have any questions. Thanks!"]}, {"number": 39073, "title": "Improve android sample", "body": "It doesn't build out of the box\r\n\r\n* enable build on command line\r\n* Remove an unused and error causing class\r\n* Update to forced necessary Gradle version\r\n* Give project a meaningful name\r\n\r\n![image](https://user-images.githubusercontent.com/3314607/80793653-ff6ede80-8b97-11ea-9fd6-50c4b66aaa10.png)\r\n\r\n", "comments": ["@caisq, Can you please take a look this PR ? Thanks!", "@gbaned I don't think I have the necessary background and expertise to review this PR. Can you check with experts such as @petewarden or @miaout17 ?", "The breaking change was to delete this file \r\n`tensorflow/java/src/main/java/org/tensorflow/op/core/Zeros.java` \r\nIt was preventing form successful build", "` ./gradlew clean build`\r\n\r\nshow now\r\n\r\n<img width=\"413\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3314607/83641484-e77ee600-a5ad-11ea-9517-af0e13fa79b8.png\">\r\n", "@hannesa2 What is the build error when `Zeros.java` is left alone? We do have users who are using code in `Zeros.java` so we can't delete it.", "> @hannesa2 What is the build error when `Zeros.java` is left alone? We do have users who are using code in `Zeros.java` so we can't delete it.\r\n\r\n@frankchn \r\n## A: This is the first error\r\n\r\n<img width=\"937\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3314607/88209154-922c9000-cc52-11ea-8ed6-63b5b8d885f0.png\">\r\n\r\n## B: After solving A\r\n\r\n<img width=\"1331\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3314607/88209385-ed5e8280-cc52-11ea-900d-f19209781d3d.png\">\r\n\r\nBecause I don't know your internal structure, I can't solve it to fulfill internal and external build\r\n", "@hannesa2 Have you tried importing `org.tensorflow.op.core.Fill` to `Zeros.java` to resolve that error instead?", "Note that `Fill` is a generated class, so you might have to run some generator in your environment in order for that symbol to resolve. I suspect you have to change/add gradle settings for that to work.", "> Have you tried importing org.tensorflow.op.core.Fill to Zeros.java to resolve that error instead?\r\n\r\nStill the same\r\n<img width=\"431\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3314607/88214373-60b7c280-cc5a-11ea-8e72-ac45635db47e.png\">\r\n", "There is a test for `Zeros.java` at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/test/java/org/tensorflow/op/core/ZerosTest.java so you might want to make sure the test can pass too.", "> There is a test for Zeros.java at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/test/java/org/tensorflow/op/core/ZerosTest.java so you might want to make sure the test can pass too.\r\n\r\nSorry, it's out of the Android project \r\n\r\n<img width=\"506\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3314607/88214825-1125c680-cc5b-11ea-94d3-38b14612db0a.png\">\r\n", "I think you have to somehow get gradle/your environment to run the code at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/gen/cc/op_generator.cc during builds. That code generates all the relevant op files that are used. The standard TensorFlow Java Bazel build environment is set up to do this, but I am not sure how to do it on gradle and android.\r\n\r\n> Sorry, it's out of the Android project\r\n\r\nYeah it is, but it shows that the `Zeros.java` code can be built and runs properly and this seems to be more of a build environment setup issue on the Android example.", "> I think you have to somehow get gradle/your environment to run the code at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/gen/cc/op_generator.cc during builds.\r\n\r\nI've the feeling that it's a way in the wrong direction.\r\nI should run a generator, to fix a class, which Android don't need. This sounds not logic", "And the CI is successful for Android ?\r\n\r\nI guess at least for Android the command `./gradlew build` would be the right command. This one will fail\r\n\r\n<img width=\"1118\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3314607/88215847-8cd44300-cc5c-11ea-8e74-7a7debc70e3e.png\">", "> I've the feeling that it's a way in the wrong direction. I should run a generator, to fix a class, which Android don't need. This sounds not logic\r\n\r\nDeleting a file that builds and tests in the normal TensorFlow environment with the standard TensorFlow build tooling is not correct either.\r\n\r\n> I guess at least for Android the command ./gradlew build would be the right command. This one will fail\r\n\r\nZerosTest.java will fail if you delete Zeros.java in the main TensorFlow build, and this will break the main TensorFlow Java build.", "Ok, I keep the `Zeros.java` untouched. \r\nAs long, as I don't really understand why it's included, I'm unable to fix this.\r\n\r\nThe Android CI should build properly (but on local machine it fails because of untouched `Zeros.java`)\r\n\r\nNow the pull request only change for Android \r\n* fix the Gradle setup \r\n* update to recent Android Studio build tools\r\n* update libraries\r\n* targetSdk 29\r\n* improve .gitignore", "Thanks @hannesa2! Just one comment: for the `tensorflow/examples/android/gradlew` file, can we not have the chmod set to 0755 and leave it as 0644?", "> Thanks @hannesa2! Just one comment: for the `tensorflow/examples/android/gradlew` file, can we not have the chmod set to 0755 and leave it as 0644?\r\n\r\nReally ? If I don't do it, I can't run it locally \r\n\r\n<img width=\"409\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3314607/88266191-0c025f00-cccf-11ea-9dcd-18a8562469be.png\">\r\n\r\nThat's why I did ` chmod +x gradlew`\r\n\r\nShort: At least for me it's necessary ! And when  you ever build it on CI, you will need it too \r\n\r\n"]}, {"number": 39072, "title": "Support Ragged inputs in a Dense layer.", "body": "**System information**\r\n- TensorFlow version (you are using): **TF 2.1, 2.2.0-rc4, TF nightly**\r\n- Are you willing to contribute it (Yes/No): **Yes**\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, `tf.keras.layers.Dense` does not support RaggedTensor inputs.\r\nhttps://github.com/tensorflow/tensorflow/blob/203aa8b6341b30abef0e24edb549c11c7966229e/tensorflow/python/keras/layers/core.py#L1180\r\nHowever, if the ragged dimension is not the last, it would be extremely useful if it did.\r\n\r\nFor example, if you want to do sequence labeling, you pass a ragged tensor with differently length sequences through an Embedding layer, then through a RNN, and then you would need to add the final classification Dense layer, which is currently not straightforward.\r\n\r\nTherefore, I propose to use something like `RaggedTensor.with_flat_values` on the output of the Dense layer applied to `RaggedTensor.flat_values`.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo new methods; the Dense layer would only add support for ragged tensors.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nI think anybody using ragged tensors.", "comments": ["Hi @foxik -\r\n\r\nCan you try wrapping your Dense layer in a TimeDistributed? As long as your innermost dimension is not ragged, this should work at head. If not, please let me know!", "@markomernick Oh, I did not realize it could be used that way -- yes, it works (and it does exactly what I was hoping for). I am closing the issue, sorry for taking your time. Cheers!", "> Hi @foxik -\r\n> \r\n> Can you try wrapping your Dense layer in a TimeDistributed? As long as your innermost dimension is not ragged, this should work at head. If not, please let me know!\r\n\r\nHi, I just had the same issue and this is indeed a great solution.\r\nI think it would make sense to mention this in the error message, as currently it only reads \r\n\r\n>ValueError: Layer dense does not support RaggedTensors as input. Inputs received: [...],[...]. You can try converting your input to an uniform tensor.\r\n\r\nNot sure where else to bring this up, I hope it is alright to comment here."]}, {"number": 39071, "title": "ImportError: DLL load failed: The specified module could not be found", "body": "Hello. My OS is windows10, Python version is 3.7 (Anaconda Python). I install tensorflow using `pip install tensorflow`. After installing when i use `import tensorflow`, i am getting following too long error:\r\n\r\nimport tensorflow\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-d6579f534729>\", line 1, in <module>\r\n    import tensorflow\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nError in callback <bound method AutoreloadMagics.post_execute_hook of <autoreload.AutoreloadMagics object at 0x000001AE0977D088>> (for post_execute):\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 538, in post_execute_hook\r\n    _, pymtime = self._reloader.filename_and_mtime(sys.modules[modname])\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 184, in filename_and_mtime\r\n    if not hasattr(module, '__file__') or module.__file__ is None:\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\dell\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@hiteshnitetc \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download [the latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "I have core i3 system, 64 bit, 3 gb ram. Purchased in 2012. I have installed 64 bit version of python. I also have 2015 microsoft visual c++. ", "You need the 2019 redistributable and you CPU probably does not support AVX.\r\n\r\nClosing as duplicate, see all the duplicate issues with the same error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39071\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39071\">No</a>\n", "I have core i3 system, 64 bit, 3 gb ram. Purchased in 2012. I have installed 64 bit version of python. I also have 2015 microsoft visual c++. ", "You need the 2019 one"]}, {"number": 39070, "title": "Passing call arguments to individual layers of a model", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThere does not seem to be a way to pass call argument to individual layers of a model.\r\nFor example I have been unable to find a way to pass initial_state call argument to the lstm layers inside a model. I explored multiple pathways and all failed.\r\n          \r\n\r\n**Will this change the current api? How?** \r\n\r\n**Who will benefit with this feature?** everyone\r\n\r\n**Any Other info.** Seems to be a simple fix\r\n", "comments": ["I think you can build your model by subclassing tf.keras.Model like below:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass Model(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.lstm = tf.keras.layers.LSTM(10)\r\n\r\n  def call(self, inputs, initial_state):\r\n    output = self.lstm(inputs, initial_state=initial_state)\r\n    return output\r\n\r\nmodel = Model()\r\nmodel(tf.zeros((8, 2, 5)), initial_state=[tf.zeros((8, 10)), tf.zeros((8, 10))])\r\n```"]}, {"number": 39069, "title": "Ability to exclude columns from tf.data.experimental.CsvDataset", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.0\r\n- Are you willing to contribute it (Yes/No): No (beginner level python skills)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThere is an existing \"select_cols\" parameter; it would be useful to provide an \"exclude_cols\" parameter (possibly mutually exclusive) so that when you have 200+ columns in a CSV, but a small number of them are useful only to a human reader, you can exclude them from the dataset.\r\n\r\n**Will this change the current api? How?**\r\nIt would only add an additional optional parameter\r\n\r\n**Who will benefit with this feature?**\r\nPeople wanting to train neural nets on CSVs full of multi-column data, e.g. to identify musical instruments from frequency spectrum\r\n\r\n**Any Other info.**\r\n", "comments": ["This sounds useful. Marking as \"contributions welcome\".", "Hey @omatai - thanks for filing this issue. @stjohnso98 has started working on it in #39799 and we wanted to clarify what the expectations for this feature are.\r\n\r\nOn a CSV file with many columns, even if we had an `exclude_cols` parameter, a user would still have to know and specify the types of the included columns. For example, suppose we had a CSV file with 200 int32 columns and we knew we wanted to exclude columns 3 and 5, we could call this API as follows:\r\n\r\n```\r\nrecord_defaults = [tf.int32] * 198\r\ndataset = tf.data.experimental.CsvDataset(files, record_default=record_defaults, exclude_cols=[3,5])\r\n```\r\n\r\nbut this to me doesn't seem that useful on top of what already exists (`select_cols`), since a user still has to specify all the included columns in `record_defaults`, and one can easily derive `select_cols` as the complement of `exclude_cols` as follows:\r\n\r\n```\r\nexclude_cols_set = set(exclude_cols)\r\ntotal_cols = len(record_defaults) + len(exclude_cols)\r\nselect_cols = [i for i in range(total_cols) if i not in exclude_cols_set]\r\n```\r\n\r\nMy take is that this would make more sense in a context where the user doesn't have to specify the columns included at all (for example, `tf.data.experimental.make_csv_dataset` tries to infer the types and number of columns if it's not provided).\r\n\r\nJust wanted to clarify what you think this feature should look like -- can you give some code examples of how you expect to call `exclude_cols`? Thanks!", "Hi Rachel,\r\n\r\nI am comparatively new to Tensorflow, and not a native speaker of python. But I have 30+ years experience in programming, mostly in C++.\r\n\r\nWhat you have written \u2013 two lines to specify directly what I would have wanted \u2013 is perfect. It makes Tensorflow more accessible to those strong on data science, but weak on python skills.\r\n\r\nIf you had asked me how to produce a list of included columns from a list of excluded columns in python, I would have wasted at least 15-30 mins. In my experience, this happens far too regularly with Tensorflow \u2013 I have taken 8 weeks on and off trying to get a specific adaptation of a VGG network translated from TF1.4 to TF2.1, but what used to work fine in TF1.4 still fails to converge as a ground-up rewrite in TF2.1. Both have fewer than 150 lines of code in them. The TF2.1 code is simpler and clearer, but it just doesn\u2019t work. In TF1.4 it works, but it\u2019s completely unintelligible. It\u2019s exasperating. The hard work of getting network training to converge should not be held back by interfaces that make it hard to fit the pieces together. IMHO, Tensorflow could be and should be no harder than Lego.\r\n\r\nSo please consider providing this feature EXACTLY as you have imagined it so that Tensorflow does not become the equivalent of an \u201cEnglish speakers only\u201d exclusive club. Accessibility matters.\r\n\r\nThanks,\r\nPaul.\r\n\r\nFrom: Rachel Lim <notifications@github.com>\r\nSent: Tuesday, 2 June 2020 7:56 am\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Paul Qualtrough <paulq@alchemysort.com>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Ability to exclude columns from tf.data.experimental.CsvDataset (#39069)\r\n\r\n\r\nHey @omatai<https://github.com/omatai> - thanks for filing this issue. @stjohnso98<https://github.com/stjohnso98> has started working on it in #39799<https://github.com/tensorflow/tensorflow/pull/39799> and we wanted to clarify what the expectations for this feature are.\r\n\r\nOn a CSV file with many columns, even if we had an exclude_cols parameter, a user would still have to know and specify the types of the included columns. For example, suppose we had a CSV file with 200 int32 columns and we knew we wanted to exclude columns 3 and 5, we could call this API as follows:\r\n\r\nrecord_defaults = [tf.int32] * 198\r\n\r\ndataset = tf.data.experimental.CsvDataset(files, record_default=record_defaults, exclude_cols=[3,5])\r\n\r\nbut this to me doesn't seem that useful on top of what already exists (select_cols), since a user still has to specify all the included columns in record_defaults, and one can easily derive select_cols as the complement of exclude_cols as follows:\r\n\r\nexclude_cols_set = set(exclude_cols)\r\n\r\ntotal_cols = len(record_defaults) + len(exclude_cols)\r\n\r\nselect_cols = [i for i in range(total_cols) if i not in exclude_cols_set]\r\n\r\nMy take is that this would make more sense in a context where the user doesn't have to specify the columns included at all (for example, tf.data.experimental.make_csv_dataset tries to infer the types and number of columns if it's not provided).\r\n\r\nJust wanted to clarify what you think this feature should look like -- can you give some code examples of how you expect to call exclude_cols? Thanks!\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/39069#issuecomment-637069974>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AIQOMWY6QE42LFSNJ3DICP3RUQBVXANCNFSM4MW3MWSQ>.\r\n", "Thanks for the feedback - we'll go ahead and implement that. "]}, {"number": 39068, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "From CMD Run as Administrator.\r\n\r\n1) C:\\windows\\system32>pip install --upgrade pip\r\n2) C:\\windows\\system32>**pip install tensorflow**\r\n3) C:\\windows\\system32>python\r\n>>> import tensorflow as ts\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\sprod\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n**ImportError: DLL load failed: The specified module could not be found.**", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39068\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39068\">No</a>\n"]}, {"number": 39067, "title": "Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:1.14\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.2/7.6\r\n- GPU model and memory:4G\r\n\r\n\r\n\r\n**Describe the problem**\r\nOn windows10 systems, the call to the tensorflow-cpu version using Java runs successfully, but replacing libtensorflow_jni with libtensorflow_jni_gpu will report an error in Can't find dependent libraries?\r\ncpu:successful\r\n     maven:dependency\r\n\t\tlibtensorflow and libtensorflow_jni\r\ngpu:error\r\n     maven:dependency\r\n\t\tlibtensorflow and libtensorflow_jni-gpu\r\n     Cannot find TensorFlow native library for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.         at org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)         at org.tensorflow.TensorFlow.init(TensorFlow.java:66)         at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)         at org.tensorflow.Graph.<clinit>(Graph.java:479)         at HelloTensorFlow.main(HelloTensorFlow.java:8)\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@swzaaaaaaa Looking at your system specification, I notice there is an incompatibility issue. `pip` binary that you used to install `TF1.14` was built using `CUDA 10` but you have `CUDA10.2`. When you run TF-cpu, it doesn't look for CUDA but when you run `TF-gpu` then it will looks for some CUDA libraries under `CUDA10.0` path but you have all the libraries under `cuda10.2` which is why it is throwing error.\r\n\r\nSolution: Uninstall TF-->uninstall CUDA -->RESTART --> install CUDA 10 --> Install TF\r\n\r\nPlease check the [`Tested build configurations`](https://www.tensorflow.org/install/source_windows#gpu)\r\n\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-1.14.0 | 3.5-3.7 | MSVC 2017 | Bazel 0.24.1-0.25.2 | 7.4 | 10\r\n\r\n\r\nPlease feel free to close this issue if this was resolved for you. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39067\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39067\">No</a>\n"]}, {"number": 39066, "title": "Float 16 quantization error", "body": "Hi \r\n\r\nI get the following error - \r\n\r\n\r\n`\"tflite.py\", line 5, in <module>\r\n    converter.target_spec.supported_types = [tf.lite.constants.FLOAT16]\r\nAttributeError: module 'tensorflow_core._api.v2.lite' has no attribute 'constants'`\r\n\r\nwhile trying the following code to quantize my .pb file to float 16 \r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('out/')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.lite.constants.FLOAT16]\r\ntflite_quant_model = converter.convert()\r\n```\r\n\r\nTensorflow version 2.1.0, OS - Mac ", "comments": ["There are details on compatibility between `tf.lite.constants` in 1.X to 2.0 available [here](https://www.tensorflow.org/lite/convert/1x_compatibility#liteconstants).\r\n\r\nSomething like the following should work:\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('out/')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\ntflite_quant_model = converter.convert()\r\n```", "Thank you for your response @gargn . I tried the following code above and get the following error - \r\n\r\n```\r\n File \"tflite.py\", line 13, in <module>\r\n    tflite_quanit_model = converter.convert()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 418, in convert\r\n    raise ValueError(\"This converter can only convert a single \"\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n$ python3 tflite.py \r\n2020-05-01 14:30:08.503886: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-01 14:30:08.522133: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa891dc1140 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-01 14:30:08.522153: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"tflite.py\", line 5, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 418, in convert\r\n    raise ValueError(\"This converter can only convert a single \"\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n```\r\n\r\nCould you please help with this ? \r\n\r\nP.S. I tried the steps at #34350\r\n\r\nbut when I try  `print(saved_model_obj.signatures.keys())` I get `KeysView(_SignatureMap({}))` which suggests I cannot use `concrete_func = saved_model_obj.signatures['serving_default']`. \r\n\r\nCan you suggest how do I go about this ? \r\n\r\nThe model that I am using can be found here - https://github.com/argman/EAST/issues/296", "@gargn  any update ? ", "same bug to me\r\n`\r\nTraceback (most recent call last):\r\n  File \"int_optimize_convert.py\", line 7, in <module>\r\n    converter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\nAttributeError: module 'tensorflow._api.v2.lite' has no attribute 'constants'\r\n`", "@raghavgurbaxani \r\nIs this still an issue", "Hi,\r\n\r\nat first I had the same issue than I tried \r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/path/to/saved/model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\ntflite_model = converter.convert()\r\n\r\nand it worked perfectly. TF version: 2.3.0-dev20200608\r\n\r\n", "@raghavgurbaxani\r\nPlease update if this is still an issue.", "Moving this to closed status as it works in 2.3-dev as informed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39066\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39066\">No</a>\n", "@Saduf2019 @stefano555  it does not work for me . I tried with tf-nightly==2.3.0-dev20200608\r\n\r\nBut I still get the error - \r\n```\r\nFile \"try_fp16.py\", line 2, in <module>\r\n    converter = tf.lite.TFLiteConverter.from_saved_model('out/')\r\n  File \"/home/ahi/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 980, in from_saved_model\r\n    raise ValueError(\"Only support a single signature key.\")\r\nValueError: Only support a single signature key.\r\n```\r\n\r\n@Saduf2019  can you help ? ", "@Saduf2019  could you reopen the issue , I am still unable to get it to work. \r\n\r\nI also tried replacing 'from_saved_model' to  'from_keras_model' with the keras model here \r\n\r\nhttps://drive.google.com/file/d/1hfIzGuQn-xApDYiucMDZvOCosyAVwvku/view\r\n\r\nand I get the issue \r\n\r\n```\r\n self._keras_model.save(temp_dir, save_format=\"tf\")\r\nAttributeError: 'str' object has no attribute 'save'\r\n```\r\n\r\nBoth keras and tensorflow options do not work. :( ", "@raghavgurbaxani May be something not right with the shared model. When I tried to load the model, code is throwing the following error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/ab88f7074ef586b0b496f9b0f4e0f4a9/untitled953.ipynb). \r\n\r\nCan you please share model building code? Thanks!\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-ee9580594b88> in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 model = tf.keras.models.load_model('/content/EAST_IC15_13_model.h5')\r\n      3 converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n      4 converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n      5 converter.target_spec.supported_types = [tf.float16]\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)\r\n    173     model_config = f.attrs.get('model_config')\r\n    174     if model_config is None:\r\n--> 175       raise ValueError('No model found in config file.')\r\n    176     model_config = json.loads(model_config.decode('utf-8'))\r\n    177     model = model_config_lib.model_from_config(model_config,\r\n\r\nValueError: No model found in config file.\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39066\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39066\">No</a>\n"]}, {"number": 39065, "title": "ModuleNotFoundError: No module named 'tensorflow.contrib'", "body": "Tryint to iter through the dataset \r\nusing tensorflow 2.0 \r\n\r\n```\r\ndef make_dataset(X_data,y_data,n_splits):\r\n    \r\n    def gen():\r\n        for train_index, test_index in KFold(n_splits).split(X_data):\r\n            X_train, X_test = X_data[train_index], X_data[test_index]\r\n            y_train, y_test = y_data[train_index], y_data[tests_index]\r\n            yield X_train, y_train,X_test,y_test\r\n    return tf.data.Dataset.from_generator(gen,(tf.float32,tf.float32),(tf.TensorShape([40,40,9]), tf.TensorShape([40,40,1])))\r\ndataset= make_dataset(X,y,5)\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nfor X_train, y_train,X_test,y_test in tfe.Iterator(dataset):\r\n    print(X_train.shape)\r\n\r\n```\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-21-950ccb8e9a23> in <module>\r\n----> 1 for X_train, y_train,X_test,y_test in trt.Iterator(dataset):\r\n      2     print(X_train.shape)\r\n\r\nAttributeError: module 'tensorflow.python.compiler.tensorrt.trt_convert' has no attribute 'Iterator'\r\ntime: 17.4 ms", "comments": ["`tf.contrib` does not exist in TensorFlow 2.0.\r\nIs there a particular reason you wish to use `tfe.Iterator(dataset)`?  \r\n\r\nOtherwise, this may work as well:\r\n```python\r\nimport tensorflow as tf\r\nfor X_train, y_train, X_test, y_test in dataset.take(3).as_numpy_iterator():\r\n    print(X_train.shape)\r\n```", "@SlowMonk \r\ntensorflow.contrib is being removed in version TF 2.x and it works in only TF 1.x. In Tensorflow 2.0, eager execution is enabled by default. Can you try with @iobtl suggestion and see if it helps you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39065\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39065\">No</a>\n"]}, {"number": 39064, "title": "Error in Keras Save with TF Lookup", "body": "When using the save method from a tf.keras model (or tf.saved_model.save), an error appears that exporting the model fails due an untracked `tf.Variable`. [Here is a Colab replicating the issue](https://colab.research.google.com/drive/1PCbiubCnzUszW6mB8f8wtft7YFVZlkaB).\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Create dummy model\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.Input(shape=(1,), dtype=tf.float32),\r\n    tf.keras.layers.Dense(1)\r\n])\r\n\r\n# Create Simple Signature Function\r\ndef outer_fn(filepath):\r\n    @tf.function\r\n    def inner_fn(b):\r\n        text_init = tf.lookup.TextFileInitializer(filename=filepath, \r\n                            key_dtype=tf.string, key_index=0,\r\n                            value_dtype=tf.int64, value_index=tf.lookup.TextFileIndex.LINE_NUMBER\r\n                            )\r\n        table = tf.lookup.StaticHashTable(initializer=text_init, default_value=0)\r\n        return table.lookup(b)\r\n    return inner_fn\r\n\r\n# Signature\r\nsignatures = {\r\n    'example_signature_1':\r\n        outer_fn(filepath='./test_file').get_concrete_function(\r\n            tf.TensorSpec(shape=[None], dtype=tf.string, name='ex_sig_1')\r\n            ),\r\n}\r\n\r\n# Model Save [ERROR - Untracked Tensor]\r\nmodel.save('.', signatures=signatures)\r\n\r\n>>> AssertionError: Tried to export a function which references untracked object Tensor(\"164:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n```\r\nThis is assuming a simple text file (here named 'test_file') like this:\r\n```\r\nhere_is_one_row\r\nhere_is_another_row\r\n```\r\n\r\nTo attempt to fix this, I (1) tried putting the tf.lookup outside the function, (2) created a tf.keras subclassed layer to use the `config` method - neither of which succeeded. Is there any way to save a `tf.lookup` object that does not appear in the documentation?\r\n", "comments": ["Running the code with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/4280b8dd9d3424d138fa08da085d55cf/39064-2-1.ipynb) throws an error stating `InvalidArgumentError: In[0] is not a matrix. Instead it has shape [1] [Op:MatMul]`\r\n\r\nWas able to reproduce the issue with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/74eb3960f6bca83268e4d9bfa34e7b6e/39064-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/7ec20802efde45edfc669f0c3e11fed2/39064-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Thanks for the response! \r\n\r\nThe matrix error has been removed now. I should note the focus was not on this but rather on the tf.lookup working on a function wrapped with 'tf.function', but being able to save using `model.save()`.", "This works for me on tf2.2. Notice you need to attach table to model so that trace does not complain.\r\n\r\nimport tensorflow as tf\r\n\r\n# Create dummy model\r\n\r\n\r\n\"\"\"\r\n# Create Simple Signature Function\r\ndef outer_fn(model, vocab_file, num_oov=1):\r\n    model.init = tf.lookup.TextFileInitializer(\r\n        filename=vocab_file,\r\n        key_dtype=tf.string,\r\n        key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\r\n        value_dtype=tf.int64,\r\n        value_index=tf.lookup.TextFileIndex.LINE_NUMBER)\r\n    model.table = tf.lookup.StaticVocabularyTable(\r\n        model.init, num_oov, lookup_key_dtype=tf.string)\r\n\r\n    @tf.function\r\n    def inner_fn(b):\r\n        return model.table.lookup(b)\r\n    return inner_fn\r\n\"\"\"\r\n\r\n\r\ndef out_intent_fn(model, vocab_file, num_oov=1):\r\n    init = tf.lookup.TextFileInitializer(\r\n        filename=vocab_file,\r\n        key_dtype=tf.string,\r\n        key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\r\n        value_dtype=tf.int64,\r\n        value_index=tf.lookup.TextFileIndex.LINE_NUMBER)\r\n\r\n    model.table = tf.lookup.StaticVocabularyTable(\r\n        init, num_oov, lookup_key_dtype=tf.string)\r\n\r\n    @tf.function\r\n    def predict_fn(probes):\r\n        ptokens = model.table.lookup(probes)\r\n        return ptokens\r\n\r\n    return predict_fn\r\n\r\n\r\ndef outmost():\r\n    export_path = \"./\"\r\n    vocab_file = \"./test.file\"\r\n\r\n    model = tf.keras.Sequential([\r\n        tf.keras.Input(shape=(1,), dtype=tf.float32),\r\n        tf.keras.layers.Dense(1),\r\n        tf.keras.layers.Dropout(rate=0.2)\r\n    ])\r\n\r\n    # Signature\r\n    signatures = {\r\n        'service_default':\r\n            out_intent_fn(model, vocab_file=vocab_file).get_concrete_function(\r\n                tf.TensorSpec(shape=[None], dtype=tf.string)\r\n                ),\r\n    }\r\n\r\n    # Model Save [ERROR - Untracked Tensor]\r\n    model.save(export_path, signatures=signatures)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    outmost()", "Hi @xiaoyunwu \r\n    I have some question about it. If your serving function like above, how to construct the serving client's input data", "@xiaoyunwu You have some interesting answer! Can you please format the code? We can't read it properly due to bad formatting. Thanks!", "Alrighty, since @xiaoyunwu's response was not properly formatted; putting it here _again_ in more readable format. Feel free to comment on if any line is not properly indented. \r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Create Simple Signature Function\r\ndef outer_fn(model, vocab_file, num_oov=1):\r\n\r\n    model.init = tf.lookup.TextFileInitializer(\r\n            filename=vocab_file,\r\n            key_dtype=tf.string,\r\n            key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\r\n            value_dtype=tf.int64,\r\n            value_index=tf.lookup.TextFileIndex.LINE_NUMBER\r\n        )\r\n    model.table = tf.lookup.StaticVocabularyTable(model.init, num_oov, lookup_key_dtype=tf.string)\r\n\r\n    @tf.function\r\n    def inner_fn(b):\r\n        return model.table.lookup(b)\r\n\r\n    return inner_fn\r\n\r\n\r\ndef out_intent_fn(model, vocab_file, num_oov=1):\r\n\r\n    init = tf.lookup.TextFileInitializer(\r\n                filename=vocab_file,\r\n                key_dtype=tf.string,\r\n                key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\r\n                value_dtype=tf.int64,\r\n                value_index=tf.lookup.TextFileIndex.LINE_NUMBER)\r\n    model.table = tf.lookup.StaticVocabularyTable(init, num_oov, lookup_key_dtype=tf.string)\r\n\r\n    @tf.function\r\n    def predict_fn(probes):\r\n        ptokens = model.table.lookup(probes)\r\n        return ptokens\r\n\r\n    return predict_fn\r\n\r\ndef outmost():\r\n    export_path = \"./\"\r\n    vocab_file = \"./test.file\"\r\n\r\n    model = tf.keras.Sequential([\r\n        tf.keras.Input(shape=(1,), dtype=tf.float32),\r\n        tf.keras.layers.Dense(1),\r\n        tf.keras.layers.Dropout(rate=0.2)\r\n    ])\r\n\r\n    # Signature\r\n    signatures = {\r\n        'service_default':\r\n            out_intent_fn(\r\n                model, vocab_file=vocab_file\r\n                ).get_concrete_function(tf.TensorSpec(shape=[None], dtype=tf.string))\r\n    }\r\n\r\n    # Model Save [ERROR - Untracked Tensor]\r\n    model.save(export_path, signatures=signatures)\r\n\r\nif name == \"main\":\r\n    outmost()\r\n``` ", "Was able to replicate the issue with TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/9bd12e8535271cd3f78db86f7099b553/untitled302.ipynb) ..Thanks!", "This issue in replicating in TF 2.7 and nightly version . Attaching [Gist](https://colab.research.google.com/gist/mohantym/98e7a70c2071aa6b7d00e1c4bf4ecd31/untitled302.ipynb#scrollTo=GOXO1knUzouk) for reference. Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39064\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39064\">No</a>\n"]}, {"number": 39063, "title": "how to perform cross validation with this type of dataset?", "body": "```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\ninp = Input(shape=(3,))\r\noutput = Dense(1, activation='sigmoid')(inp)\r\nmodel = Model(inp, output)\r\nmodel.compile(optimizer=Adam(1e-2), loss='binary_crossentropy')\r\n\r\ndef simple_generator():\r\n    while True:\r\n        yield [0.5, 0.2, -0.3], 0.0\r\n        yield [-0.5, 0.3, -0.1], 1.0\r\n        \r\ndataset = tf.data.Dataset.from_generator(simple_generator,\r\n                                         output_types=(tf.float32,\r\n                                                       tf.float32))\r\ndataset = dataset.batch(4).prefetch(1)\r\n\r\nmodel.fit(dataset)\r\n```\r\n\r\nHow to perform cross validation with tf.data.Dataset.From_generator ? ", "comments": ["@SlowMonk \r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39062, "title": "[INTEL MKL] Fix for build error.", "body": "Fix for build errors caused by the refactoring commit https://github.com/tensorflow/tensorflow/commit/8ba31084528f63e623fb5f58d9ee09b148088c6d", "comments": ["@gbaned  The import/copybara test is failing, can you provide more on the failure?", "@gbaned  @alextp  the previous commit was causing the Windows build test  to fail due to circular dependencies.  I updated PR with a new commit that will resolve the issue.  Please review.   ", "Hi @gbaned . It looks like the Windows Bazel GPU failure is not related to the changes made in the PR.   Can you restart the tests to see if it fails again?", "Hi @gbaned and @alextp this PR seems to be stuck at internal review, can you take a look at it? Thanks."]}, {"number": 39061, "title": "Windows build error: 'mlir::FoldingHook': 'value' is not a valid template type argument for parameter 'ConcreteType'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 10.0.18363\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.1 (SHA-1 is c878390581fc817564f8ebe1f4237d0cbd225f14)\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: N/A?\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): (BAZEL_VS: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools)\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5\r\n- GPU model and memory: RTX 2080 (8GB)\r\n\r\nI'm following this guide https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows and advice here https://github.com/tensorflow/tensorflow/issues/23542 with some adjustments to build Tensorflow 2.1 for Windows.\r\n\r\nI ran `python configure.py` and this ended up being my `.tf_configure.bazelrc`:\r\n\r\n    build --action_env PYTHON_BIN_PATH=\"C:/Python37/python.exe\"\r\n    build --action_env PYTHON_LIB_PATH=\"C:/Python37/lib/site-packages\"\r\n    build --python_path=\"C:/Python37/python.exe\"\r\n    build --config=xla\r\n    build --action_env CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\"\r\n    build --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.5\"\r\n    build --config=cuda\r\n    build:opt --copt=/arch:AVX2\r\n    build:opt --define with_default_optimizations=true\r\n    build --define=override_eigen_strong_inline=true\r\n    test --flaky_test_attempts=3\r\n    test --test_size_filters=small,medium\r\n    test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial\r\n    test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu\r\n    test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only\r\n    test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only\r\n    build --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\nThen I did\r\n\r\n`bazel build --config=cuda --copt=-nvcc_options=disable-warnings tensorflow:tensorflow.dll`\r\n\r\nI've done this twice and got the same error during the build. Pardon the mess...\r\n\r\nERROR: C:/users/SOMEUSER/_bazel_SOMEUSER/dktb5wq4/external/llvm-project/mlir/BUILD:75:1: C++ compilation of rule '@llvm-project//mlir:IR' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/SOMEUSER/_bazel_SOMEUSER/dktb5wq4/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python37/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python37/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\SOMEU~1\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\SOMEU~1\\AppData\\Local\\Temp\r\n  C:/Python37/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/llvm-project /Ibazel-out/x64_windows-opt/bin/external/llvm-project /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/DialectSymbolRegistry /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen /Iexternal/llvm-project/mlir/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include /Iexternal/llvm-project/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLLVM_ENABLE_STATS /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DLLVM_BUILD_GLOBAL_ISEL /showIncludes /MD /O2 /DNDEBUG /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /std:c++14 /Fobazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_objs/IR/Module.o /c external/llvm-project/mlir/lib/IR/Module.cpp\r\nExecution platform: @local_execution_config_platform//:platform\r\nexternal/llvm-project/mlir/include\\mlir/IR/OpDefinition.h(1185): error C2923: 'mlir::FoldingHook': 'value' is not a valid template type argument for parameter 'ConcreteType'\r\nexternal/llvm-project/llvm/include\\llvm/ADT/STLExtras.h(1281): note: see declaration of 'value'\r\nexternal/llvm-project/mlir/include\\mlir/IR/Module.h(36): note: see reference to class template instantiation 'mlir::Op<mlir::ModuleOp,mlir::OpTrait::ZeroOperands,mlir::OpTrait::ZeroResult,mlir::OpTrait::IsIsolatedFromAbove,mlir::OpTrait::PolyhedralScope,mlir::OpTrait::SymbolTable,mlir::OpTrait::SingleBlockImplicitTerminator<mlir::ModuleTerminatorOp>::Impl,mlir::SymbolOpInterface::Trait>' being compiled\r\nexternal/llvm-project/mlir/include\\mlir/IR/OpDefinition.h(1187): error C2955: 'mlir::FoldingHook': use of class template requires template argument list\r\nexternal/llvm-project/mlir/include\\mlir/IR/OpDefinition.h(273): note: see declaration of 'mlir::FoldingHook'\r\nTarget //tensorflow:tensorflow.dll failed to build\r\nINFO: Elapsed time: 1466.809s, Critical Path: 96.50s\r\nINFO: 3656 processes: 3656 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nThe first time the build stopped entirely. The second time, which is right now, the same error reported, but it's still running. I see\r\n`[10,544 / 12,151] Compiling tensorflow/core/kernels/scatter_op.cc; 5820s local`\r\n\r\nIs there any hope that this will finish? What can be done to fix the error? Thanks for reading.\r\nUpdate: I noticed my CPU wasn't working hard at all while waiting for the second round, so I decided to terminate it.\r\n", "comments": ["I pulled 580b03a12c50df594d693bda032fdfb40b8a186b and tried to build again. No error this time but it's stalling on \r\n`[12,645 / 12,648] Compiling tensorflow/core/kernels/tile_ops.cc; 36514s local`\r\n\r\nTask manager doesn't show the CPU working hard, so I think it's not going anywhere. What can I do? Problems of this sort have been my experience for several months. It's difficult to get a ballpark of how long any specific compilation task should take.", "The Windows build breakage was fixed. For the slow compilation time I'd recommend filing a separate issue as it is unrelated to this issue and folks might not notice it due to issue title.\r\n\r\nIf you add -s to the bazel command it will show all the instructions being ran and you could check rerunning that command directly (I'd suggest only adding that option post it getting stuck to avoid it reporting all the successful commands too). I have not seen compile times near that length even on Windows so that's weird. Potentially also try using clang to build it on Windows to narrow down causes.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39061\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39061\">No</a>\n"]}, {"number": 39060, "title": "XLA function crashed when called under GradientTape", "body": "Hello,\r\n\r\n**Describe the current behavior**\r\n\r\nI have a function compiled with XLA (tf.function + experimental_compile=True) which writes the processed output to the TensorArray. When called inside GradientTape it crashes. Without experimental_compile=True everything works fine.\r\n\r\n**Describe the expected behavior**\r\nfunction does not crash\r\n\r\n**Standalone code to reproduce the issue**\r\nColab link to reproduce the crash: https://colab.research.google.com/drive/1Hh7fFkWVC8YCPqTtkQXDNVB8elQS8rq0#scrollTo=MpeMlEwGje0n\r\n", "comments": ["It seems like the TensorArray is causing this crash.", "Was able to reproduce the error with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/8c6095ec7494526e323fd789253b753b/39060-2-2.ipynb#scrollTo=yHUVor_qkn7h) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/d113930f63ca0bb2c640ada2efaee9b4/39060-tf-nightly.ipynb). \r\nHowever, code works without any issues with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/32dfbddc0ddfb20bacb17eef47761b0a/39060-2-1.ipynb). Please find the attached gist. Thanks!", "@EgorLakomkin As a workaround, you can remove `@tf.function(experimental_compile=True)` around the `call` method inside the layer. Here it does not do anything (since the whole computation is compiled by the outer function), and is causing the crash.\r\n\r\nIn general, `TensorArray` handling in XLA does have some sharp edges...", "@cheshire Thank you for the suggestion! I am looking how to use XLA for implementing a RNN, maybe there is an alternative to TensorArray to do it?", "@EgorLakomkin There is one bug and one missing feature here: \r\n\r\n1) Nested `tf.function(experimental_compile=True)` should not affect the result at all, but here it does (the error you are seeing)\r\n2) If you remove the outer `tf.function` you'll get the missing feature: `TensorArray` transition across TF/XLA boundary is not implemented.\r\n\r\nIf you are fine with keeping the outer `tf.function(experimental_compile=True)` you should be able to do it as you are doing now and keep good performance, right?\r\nOr do you want to ship it as a library where you have no control over the user code which might take a derivative?", "@cheshire  Thank you! If I keep only the outer experimental_compile I also have the code crash (repro in colab https://colab.research.google.com/drive/1ehh6kklHmXSmcDs6mCdM-6y59v01dXKF?usp=sharing), though its completely different one:\r\n\r\n    XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time.", "@EgorLakomkin I have a fix for the issue with both inner and outer function compiled, which should be released shortly.\r\n\r\nCould you file a separate bug on `XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time.` and assign it to me? Also your colab link is requiring some permissions I don't have.", "0e4e0c593bf7957aefd29818e2d24caee00c841a fixes your original problem. Could you file another bug for a different problem you are seeing?", "@cheshire Thank you! Filed a ticket with another issue: https://github.com/tensorflow/tensorflow/issues/39904 Cannot assign you directly on GitHub (at least did not find out how).", "@EgorLakomkin can we close this issue, since the original issue is solved. ", "Sure, thx for the fix!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39060\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39060\">No</a>\n"]}, {"number": 39059, "title": "Using autograph when calculating gradient over tf.case", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): TF 2.1.0\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce GTX 1080 Ti 11G\r\n\r\n**Describe the current behavior**\r\nI am trying to compute a gradient over tf.case using an autograph.\r\n\r\nFor example, let's say I have a case function where it takes a batch of input and computes output based on the sign of the input:\r\n```python\r\ndef case_fn(x):                                                                                                                                                                                                                                                                                                          \r\n    N = tf.shape(x)[0]                                                                                                                                                                                                                                                                                                   \r\n    positive_idx = tf.cast(tf.squeeze(tf.where(tf.squeeze(tf.math.greater(x, 0.)))),tf.int32)                                                                                                                                                                                                                            \r\n    negative_idx = tf.cast(tf.squeeze(tf.where(tf.squeeze(tf.math.less_equal(x, 0.)))),tf.int32)                                                                                                                                                                                                                         \r\n    def all_positive_case():                                                                                                                                                                                                                                                                                             \r\n        y_positive = x*2.                                                                                                                                                                                                                                                                                                \r\n                                                                                                                                                                                                                                                                                                                         \r\n        return y_positive                                                                                                                                                                                                                                                                                                \r\n                                                                                                                                                                                                                                                                                                                         \r\n    def all_negative_case():                                                                                                                                                                                                                                                                                             \r\n        y_negative = x-2.                                                                                                                                                                                                                                                                                                \r\n                                                                                                                                                                                                                                                                                                                         \r\n        return y_negative                                                                                                                                                                                                                                                                                                \r\n                                                                                                                                                                                                                                                                                                                         \r\n    def some_positive_some_negative_case():                                                                                                                                                                                                                                                                              \r\n        x_positive = tf.gather(x, positive_idx)                                                                                                                                                                                                                                                                          \r\n        x_negative = tf.gather(x, negative_idx)                                                                                                                                                                                                                                                                          \r\n                                                                                                                                                                                                                                                                                                                         \r\n        y_positive = x_positive*2.                                                                                                                                                                                                                                                                                       \r\n        y_negative = x_negative-2.                                                                                                                                                                                                                                                                                       \r\n                                                                                                                                                                                                                                                                                                                         \r\n        y_positive = tf.scatter_nd(tf.expand_dims(positive_idx,1),y_positive,tf.stack([N,1]))                                                                                                                                                                                                                            \r\n        y_negative = tf.scatter_nd(tf.expand_dims(negative_idx,1),y_negative,tf.stack([N,1]))                                                                                                                                                                                                                            \r\n                                                                                                                                                                                                                                                                                                                         \r\n        return y_positive + y_negative                                                                                                                                                                                                                                                                                   \r\n                                                                                                                                                                                                                                                                                                                         \r\n    all_positive = tf.math.equal(tf.shape(negative_idx)[0], 0)                                                                                                                                                                                                                                                           \r\n    all_negative = tf.math.equal(tf.shape(positive_idx)[0], 0)                                                                                                                                                                                                                                                           \r\n    return tf.case([(all_positive, all_positive_case), (all_negative, all_negative_case)], default=some_positive_some_negative_case)\r\n```\r\nThen, I calculate a gradient with the following code:\r\n```python\r\ntrainable_variable = tf.Variable([[1.], [-1.], [2.], [-2.]])                                                                                                                                                                                                                                                             \r\n@tf.function                                                                                                                                                                                                                                                                                                             \r\ndef compute_grad():                                                                                                                                                                                                                                                                                                      \r\n    with tf.GradientTape() as tape:                                                                                                                                                                                                                                                                                      \r\n        y = case_fn(trainable_variable)                                                                                                                                                                                                                                                                                  \r\n    grad = tape.gradient(y, trainable_variable)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \r\n    return grad                                                                                                                                                                                                                                                                                                          \r\n                                                                                                                                                                                                                                                                                                                         \r\nprint(compute_grad())   \r\n```\r\nIf I do not use ```@tf.function``` decorator, it returns a correct value which is ```IndexedSlices(indices=tf.Tensor([0, 2, 1, 3], shape=(4,), dtype=int32), values=tf.Tensor([[2.],[2.],[1.],[1.]], shape=(4, 1), dtype=float32), dense_shape=tf.Tensor([4 1], shape=(2,), dtype=int32))```.\r\nHowever, if I use ```@tf.function``` decorator, it returns a value error saying\r\n```\r\nTraceback (most recent call last):\r\n  File \"examples/case_gradient.py\", line 102, in <module>\r\n    print(compute_grad())\r\n  File \"/home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    examples/case_gradient.py:99 compute_grad  *\r\n        grad = tape.gradient(y, trainable_variable)\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:1029 gradient\r\n        unconnected_gradients=unconnected_gradients)\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py:77 imperative_grad\r\n        compat.as_str(unconnected_gradients.value))\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:141 _gradient_function\r\n        return grad_fn(mock_op, *out_grads)\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/ops/cond_v2.py:121 _IfGrad\r\n        false_graph, grads, util.unique_grad_fn_name(false_graph.name))\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/ops/cond_v2.py:381 _create_grad_func\r\n        func_graph=_CondGradFuncGraph(name, func_graph))\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:978 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/ops/cond_v2.py:380 <lambda>\r\n        lambda: _grad_fn(func_graph, grads), [], {},\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/ops/cond_v2.py:371 _grad_fn\r\n        src_graph=func_graph)\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py:669 _GradientsHelper\r\n        lambda: grad_fn(op, *out_grads))\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py:336 _MaybeCompile\r\n        return grad_fn()  # Exit early\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py:669 <lambda>\r\n        lambda: grad_fn(op, *out_grads))\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/ops/cond_v2.py:183 _IfGrad\r\n        building_gradient=True,\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/ops/cond_v2.py:219 _build_cond\r\n        _make_indexed_slices_indices_types_match(_COND, [true_graph, false_graph])\r\n    /home/junhyeok/.venv/ccmbrl/lib/python3.6/site-packages/tensorflow_core/python/ops/cond_v2.py:652 _make_indexed_slices_indices_types_match\r\n        (current_index, len(branch_graphs[0].outputs)))\r\n\r\n    ValueError: Insufficient elements in branch_graphs[0].outputs.\r\n    Expected: 6\r\n    Actual: 3\r\n```\r\n\r\n**Describe the expected behavior**\r\nBehave equivalently even I use ```@tf.function``` decorator.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nOne wierd thing is when I run this in colab, it doesn't return the value error.\r\nBut here is the colab link: https://colab.research.google.com/drive/1DmyMNffPOloFt66_FoqHxxHVMfYInCIf#scrollTo=cEqKALBoHzmB\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have checked this with version ```2.2.0-rc3``` and didn't see any issue.\r\nIt might be resolved in the new version."]}, {"number": 39058, "title": "TF 2.2.0rc4 does not have python 3.8 macOS wheel", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: TF 2.2.0 rc4\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the problem**\r\n\r\nWhile looking into the available pip wheel packages on pypi.org for 2.2.0rc4:\r\n\r\nhttps://pypi.org/project/tensorflow/2.2.0rc4/#files\r\n\r\nI noticed that pip wheel for macOS python 3.8 is not available (only two cp38 files on pypi.org, one is windows and another is linux).\r\n\r\nHowever, pip wheel package for python 3.8 on macOS is available for 2.2.0rc3:\r\n\r\nhttps://pypi.org/project/tensorflow/2.2.0rc4/#files\r\n\r\nWondering if 3.8 for macOS will be available when 2.2.0 final is released?", "comments": ["@yongtang thanks for filing this issue, macos py3.8 binaries for 2.2.0-rc4 are now available both on [tensorflow](https://pypi.org/project/tensorflow/2.2.0rc4/#files) and [tensorflow-cpu](https://pypi.org/project/tensorflow-cpu/2.2.0rc4/#files) projects.\r\n\r\nI am closing this issue, please do reopen if you run into further issues with these binaries.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39058\">No</a>\n"]}, {"number": 39057, "title": "XLA gives error about dynamic shapes in TF 2.1.0, but not in TF 1.15.2", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Binary I believe, it is part of a docker image\r\n- TensorFlow version (use command below): 1.15.2 / 2.1.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.2.89 / 7.6.5\r\n- GPU model and memory: Nvidia Tesla P100 (16GB)\r\n\r\n**Describe the current behavior**\r\n\r\nFor the same code ([see gist here](https://gist.github.com/MatthiasKohl/3100f81e71309fb02e3b69773352e2c0)) I'm getting an error in TF2 but not in TF1 on XLA compile.\r\n\r\nI need to migrate some code from TF1 to TF2 and want to find out what goes wrong here.\r\nSomehow, I believe this has to do with a difference in `tf.compat.v1.data.make_initializable_iterator(tf_data_batched)` between TF1 and TF2 as it might give unknown shapes in TF2 but not TF1. But I'm not really sure and it won't always fail, such as for example when removing the following loss from the graph (lines 77/78 of the gist):\r\n```python\r\ngaussian = tfd.MultivariateNormalDiag(tf.zeros_like(samples), tf.ones_like(samples))\r\nloss_samples = gaussian.log_prob(samples)\r\n```\r\n\r\nCan you help me figure out what's the issue here and how to migrate this to TF2 ? Thank you!\r\nBelow is the exact error I'm getting on TF2.\r\n\r\n```\r\nXLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluate\r\nd at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.\r\n         [[{{node gradients/gradients/MultivariateNormalDiag_1/log_prob/MultivariateNormalDiag_chain_of_MultivariateNormalDiag_shift_of_MultivariateNormalDiag_scale_matvec_linear_operator/inverse/Multivariat\r\neNormalDiag_scale_matvec_linear_operator/inverse/LinearOperatorDiag/solve/LinearOperatorDiag/solve/mul_grad/Sum_grad/DynamicStitch}}]]\r\n         [[gradients/while_loop_grad/while_loop_grad]]\r\n        This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 o\r\nr the environment variable TF_XLA_FLAGS=\"tf_xla_auto_jit=2\" which will attempt to use xla to compile as much of the graph as the compiler is able to.\r\n         [[cluster]]\r\n         [[output_0/_3]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo error in TF2 just like in TF1.\r\n\r\n**Standalone code to reproduce the issue**\r\nSee gist: https://gist.github.com/MatthiasKohl/3100f81e71309fb02e3b69773352e2c0\r\nNote that this requires the tensorflow_probability package! I wasn't able to make a smaller example reproducing this bug, unfortunately.\r\n", "comments": ["@MatthiasKohl,\r\nWhile running the code with TF v2.1, I'm facing an error stating `ImportError: This version of TensorFlow Probability requires TensorFlow version >= 2.2; Detected an installation of version 2.1.0. Please upgrade TensorFlow to proceed.`\r\n\r\nHowever, I did not face any errors while running the code with TF v2.2.0-rc4. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b96b90a3e897b0396cccfadeca77f6d0/39057-2-2.ipynb). Thanks!", "Thank you for the pointer, I guess you used TF probability version 0.10.0-rc0/1 which is made for TF 2.2+.\r\nInstead, I used TF probability 0.9.0 which is for TF 2.1.0.\r\nSorry about not stating this more clearly in the description.\r\nI'll try with TFP 0.10 and TF 2.2 as you suggested, hopefully it'll work !", "So I was able to run this using TF v2.2.0-rc3 and TFP 0.10.0-rc0 as I didn't find any container for TF v2.2.0-rc4 GPU right now.\r\nIt does work with the TF v2.2.0-rc3, too, and this has solved the issue in my larger application as well.\r\nThank you very much!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39057\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39057\">No</a>\n"]}, {"number": 39055, "title": "Add TANH Op to Micro", "body": "Added TANH operator and test to Tensorflow Lite Micro\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39055) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@yair-ehrenwald Thank you for your contribution. Can you please sign CLA? Thanks!", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39055) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it."]}, {"number": 39054, "title": "Conv2d with large filters fails on V100 GPU", "body": "**System information**\r\n- Have I written custom code: **yes**\r\n- OS Platform and Distribution: **Ubuntu 16.04**\r\n- TensorFlow installed from: **binary**\r\n- TensorFlow version: **2.1.0**\r\n- Python version: **3.7.3**\r\n- CUDA/cuDNN version: **10.1 and 7.6.4**\r\n- GPU model and memory: **Tesla V100 with 16 or 32GB**\r\n\r\n**Describe the current behavior**\r\n\r\nCode segfaults when using a large 2D convolution kernel on a V100 GPU, but not on a GeForce GPU, e.g. GeForce Titan X. Also works fine without a GPU. On a V100, this code segfaults:\r\n```\r\nsome_input = tf.random.normal((1, 512, 512, 1))\r\nkernel = tf.random.normal((363, 363, 1, 1))\r\ntf.nn.conv2d(some_input, kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\r\n```\r\nIf we set the kernel size to 362 then it works again, but 363x363 and beyond segfaults on the V100 GPU.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should not segfault on the V100.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSimple code snippet to reproduce is available here as a gist:\r\nhttps://gist.github.com/CNugteren/138bc1ecbfbfa42e7b1be85923f48f62\r\n\r\n**Other info / logs**\r\n\r\nExample log of running the code in the above gist:\r\n```\r\n$ python tf_conv_bug.py\r\n(... some TF GPU information)\r\n>> kernel_size =  362\r\n>> done for kernel_size =  362\r\n>> kernel_size =  363\r\nSegmentation fault (core dumped)\r\n```\r\n", "comments": ["@CNugteren \r\n\r\nI have tried in colab with TF 2.1.0, 2.2-rc3 and i am not seeing any issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/76475d07e72d8daa6d7bc8d6ce28de49/untitled846.ipynb).Thanks!", "I don't see any GPU info in your link. Are you sure you are on a V100?", "@CNugteren \r\n\r\nSorry my bad i have executed in cpu but not GPU.I have tried in colab with TF 2.10-GPU and i am seeing a message (`Your session crashed for an unknown reason`. ) with kernel size 363.Thanks!", "This looks like a CUDA issue, since I can also reproduce in PyTorch:\r\n\r\n```python\r\nimport torch\r\ntorch.backends.cudnn.benchmark = True  # Enable autotune, like in TF\r\nfor kernel_size in (362, 363):\r\n  input_image = torch.ones((1, 1, 512, 512)).cuda()\r\n  print('>> kernel_size = ', kernel_size)\r\n  kernel = torch.ones((1, 1, kernel_size, kernel_size)).cuda()\r\n  torch.nn.functional.conv2d(input_image, kernel, stride=1, padding=181)\r\n  print('>> done for kernel_size = ', kernel_size)\r\n```\r\n\r\nRunning either the TF or PyTorch version with the enviornmental variables `CUDNN_LOGINFO_DBG=1 CUDNN_LOGDEST_DBG=stderr` to enable logging, I see it crashes right after the log that shows conv2d is running with the algorithm `CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM`. I can also reproduce with CUDA 11.\r\n\r\n@nluehr, can you address this CUDA issue?\r\n", "I can reproduce with CUDNN 7.x. The issue will be fixed in CUDNN 8.", "@CNugteren now that TF nightlies are built against cudnn 8, can you verify that the issue is resolved?", "I can confirm indeed that the above code now passes successfully on a Tesla V100 GPU using `pip install tf-nightly`, CUDA 11.0 and cuDNN 8.0.3.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39054\">No</a>\n"]}, {"number": 39053, "title": "Failure to convert model to tflite on local laptop", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): tf-nightly\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n**The output from the converter invocation**\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-7-d6af5d9fe17b> in <module>\r\n     14 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n     15 converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n---> 16 tflite_model = converter.convert()\r\n     17 open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n     18 e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    709         input_tensors=input_tensors,\r\n    710         output_tensors=output_tensors,\r\n--> 711         **converter_kwargs)\r\n    712 \r\n    713     if quant_mode.post_training_int8_no_float():e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    546       input_data.SerializeToString(),\r\n    547       debug_info_str=debug_info_str,\r\n--> 548       enable_mlir_converter=enable_mlir_converter)\r\n    549   return data\r\n    550 e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    251       stdout = _try_convert_to_unicode(stdout)\r\n    252       stderr = _try_convert_to_unicode(stderr)\r\n--> 253       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    254   finally:\r\n    255     # Must manually cleanup files.ConverterError: See console for info.\r\n2020-04-30 17:01:13.684285: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:307] Ignored output_format.\r\n2020-04-30 17:01:13.684836: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:310] Ignored drop_control_dependency.\r\n2020-04-30 17:01:13.802287: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-04-30 17:01:13.813650: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27f74d24df0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-30 17:01:13.814159: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nloc(callsite(\"model/time_distributed_1/lstm/while\"(\"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\":988:0) at callsite(\"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\":1082:0 at callsite(\"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\":578:0 at callsite(\"<ipython-input-7-d6af5d9fe17b>\":12:0 at callsite(\"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\":3331:0 at callsite(\"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\":3254:0 at callsite(\"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\":3063:0 at callsite(\"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\async_helpers.py\":68:0 at callsite(\"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\":2886:0 at \"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\":2858:0)))))))))): error: body function result type tensor<?x75x20xf32> is incompatible with result type tensor<15x1x20xf32> at index 3\r\nTraceback (most recent call last):\r\n  File \"E:\\ProjectTools\\Python3.6.8\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"E:\\ProjectTools\\Python3.6.8\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"E:\\ProjectTools\\venv_tf2.2\\Scripts\\toco_from_protos.exe\\__main__.py\", line 9, in <module>\r\n  File \"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: e:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:988:1: error: body function result type tensor<?x75x20xf32> is incompatible with result type tensor<15x1x20xf32> at index 3\r\n        self._initialize(args, kwargs, add_initializers_to=initializers)\r\n^\r\ne:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:1082:1: note: called from\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n^\r\ne:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:578:1: note: called from\r\n    concrete_func = func.get_concrete_function()\r\n^\r\n<ipython-input-7-d6af5d9fe17b>:12:0: note: called from\r\ne:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331:1: note: called from\r\n                    exec(code_obj, self.user_global_ns, self.user_ns)\r\n^\r\ne:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3254:1: note: called from\r\n                    if (await self.run_code(code, result,  async_=asy)):\r\n^\r\ne:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063:1: note: called from\r\n                       interactivity=interactivity, compiler=compiler, result=result)\r\n^\r\ne:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\async_helpers.py:68:1: note: called from\r\n        coro.send(None)\r\n^\r\ne:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2886:1: note: called from\r\n            return runner(coro)\r\n^\r\ne:\\projecttools\\venv_tf2.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2858:1: note: called from\r\n                raw_cell, store_history, silent, shell_futures)\r\n^\r\ne:\\projecttools\\venv_tf2.2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:988:1: note: see current operation: %146:13 = \"tf.While\"(%16, %7, %16, %3, %145, %50, %50, %49, %117, %127, %42, %43, %41) {_lower_using_switch_merge = false, _num_original_outputs = 13 : i64, _read_only_resource_inputs = [10, 11, 12], body = @model_time_distributed_1_lstm_while_body_5735_frozen0, cond = @model_time_distributed_1_lstm_while_cond_5734_frozen0, device = \"\", is_stateless = false, output_shapes = [#tf.shape<>, #tf.shape<>, #tf.shape<>, #tf.shape<>, #tf.shape<?x20>, #tf.shape<?x20>, #tf.shape<?x20>, #tf.shape<>, #tf.shape<>, #tf.shape<>, #tf.shape<10x80>, #tf.shape<80>, #tf.shape<20x80>], parallel_iterations = 32 : i64} : (tensor<i32>, tensor<i32>, tensor<i32>, tensor<15x1x20xf32>, tensor<75x20xf32>, tensor<75x20xf32>, tensor<75x20xf32>, tensor<i32>, tensor<15x75x10xf32>, tensor<15x75x1xi1>, tensor<10x80xf32>, tensor<80xf32>, tensor<20x80xf32>) -> (tensor<i32>, tensor<i32>, tensor<i32>, tensor<15x1x20xf32>, tensor<75x20xf32>, tensor<75x20xf32>, tensor<75x20xf32>, tensor<i32>, tensor<15x75x10xf32>, tensor<15x75x1xi1>, tensor<10x80xf32>, tensor<80xf32>, tensor<20x80xf32>)\r\n        self._initialize(args, kwargs, add_initializers_to=initializers)\r\n```\r\n\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\nNER implementation from the link https://www.depends-on-the-definition.com/lstm-with-char-embeddings-for-ner/\r\n```\r\n\r\n# Put link here or attach to the issue.\r\n```\r\nI am trying to implement NER from the link https://www.depends-on-the-definition.com/lstm-with-char-embeddings-for-ner/\r\nI am able to generate model file(h5), but not able to convert the model to tflite with the command mentioned above\r\n\r\nIf I remove the line \"converter.optimizations = [tf.lite.Optimize.DEFAULT]\", then tflite model gets created. But can't infer from the model on mobile\r\nThe error i get while inferencing the model is\r\nFailed to run on the given Interpreter: tensorflow/lite/kernels/concatenation.cc:74 t->dims->data != t0->dims->data (75 != 1)\r\n\r\nNote: With the same command mentioned in \"Copy and paste here the exact command\" tflite conversion works fine with Colab. The same model works fine in mobile\r\nThe problem is happening in Local windows 10 PC", "comments": ["Was able to reproduce the issue with TF-nightly. Whereas, running the code with TF v2.2.0-rc4 throws an error stating `error: 'tfl.concatenation' op operand #0 must be tensor of 32-bit float or 64-bit integer or 32-bit integer or 16-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<1x75x1xi1>'`\r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f9eed538a8f24266272852d051781a3e/39053.ipynb). Thanks!", "Are there any ways to fix this problem?", "also facing same issue !! ", "Having the same issue", "Any solutions found?", "Hi @harsha113! \r\nWe are checking to see whether you still need help in this issue .Have you checked the same in Latest version TF 2.6 yet? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39053\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39053\">No</a>\n"]}, {"number": 39052, "title": "Inference blocks indefinitely on GPU when Eager mode is enabled", "body": "<!--\r\nPlease make sure that this is a bug. \r\n\r\nAs per our GitHub Policy (https://github.com/tensorflow/models/blob/master/ISSUES.md), we only address code bugs, documentation issues, and feature requests on GitHub.\r\n\r\nPlease go to Stack Overflow (http://stackoverflow.com/questions/tagged/tensorflow-model-garden) for help and support.\r\n\r\nThe research models (https://github.com/tensorflow/models/tree/master/research) are a large collection of models implemented in TensorFlow by researchers. They are not officially supported. It is up to the individual researchers to maintain the models and/or provide support on issues and pull requests.\r\n-->\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI made small changes (use of opencv to capture images) to the object_detection_tutorial file.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): python -m pip install tensorflow\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1 / 7.6.5.32\r\n- GPU model and memory: GTX 960M 2GB or RTX2070 Super 8 GB\r\n\r\n<!-- \r\nYou can collect some of this information using our environment capture (https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: \r\n\r\n1. TensorFlow 1.0\r\n`python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` \r\n\r\n2. TensorFlow 2.0\r\n`python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n-->\r\n\r\n**Describe the current behavior**\r\nWith GPU and Eager mode enabled, running inference blocks indefinitely after processing a few frames (<15).  \r\nIf I then disable Eager mode, it runs fine.\r\n\r\nWhen eager is disabled and I use a session to process the data, it also blocks indefinitely on GPU. \r\n\r\nEverything works fine when only using the CPU\r\n\r\n**Describe the expected behavior**\r\nBeing able to run inference on GPU without indefinite blocks with Eager mode enabled. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport os\r\nimport pathlib\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n#from object_detection.utils import ops as utils_ops\r\n\r\ndef load_model(model_name):\r\n    base_url = 'http://download.tensorflow.org/models/object_detection/'\r\n    model_file = model_name + '.tar.gz'\r\n    model_dir = tf.keras.utils.get_file(\r\n        fname=model_name,\r\n        origin=base_url + model_file,\r\n        untar=True)\r\n    model_dir = pathlib.Path(model_dir)/\"saved_model\"\r\n    model = tf.saved_model.load(str(model_dir))\r\n    model = model.signatures['serving_default']\r\n    return model\r\n\r\ndef run_inference_for_single_image(model, image):\r\n    image = np.asarray(image)\r\n    input_tensor = tf.convert_to_tensor(image)\r\n    input_tensor = input_tensor[tf.newaxis, ...]\r\n    # Run inference\r\n    print(\"Inference start\")\r\n    model(input_tensor)\r\n    print(\"Inference end\")\r\n\r\nif \"models\" in pathlib.Path.cwd().parts:\r\n    while \"models\" in pathlib.Path.cwd().parts:\r\n        os.chdir('..')\r\n\r\n#disable eager mode\r\n#tf.compat.v1.disable_eager_execution()\r\n\r\nMODELNAME = 'ssd_mobilenet_v1_coco_2017_11_17'\r\nDETECTION_MODEL = load_model(MODELNAME)\r\n\r\n#utils_ops.tf = tf.compat.v1\r\ntf.gfile = tf.io.gfile\r\n\r\n#IMGPATH = PATH_TO_IMAGE\r\nIMAGE = np.zeros((640, 480, 3), np.uint8)\r\n\r\nwhile True:\r\n    run_inference_for_single_image(DETECTION_MODEL, IMAGE)\r\n```\r\nI ran this from the research\\object_detection folder\r\n\r\n**Other info / logs**\r\nI am not sure how to support the claim of it being a bug. I tried it on different machines and the code is based on an example. I thought it was because there are no error or warning messages before hanging, it works fine when just using the CPU (with or without Eager mode), it works on GPU without Eager mode and it hangs in a library function.\r\n\r\nI never did anything like this before. If i did something wrong or more information is required, please let me know. \r\n", "comments": ["@McSlay \r\nI ran the code shared by you and face an error, please share all dependencies and complete stand alone code for us to help you.\r\nPlease find the [gist](https://colab.sandbox.google.com/gist/Saduf2019/694c1acdc4a31910add3630b3477b0db/39052.ipynb) for the same.", "I updated the code. After updating it worked in the gist you linked. Is the gist using the GPU?\r\nI am not sure why it works there but not on the 3 different machines I have tried. What should I do?", "@McSlay \r\nyes we can use GPU to do so, please select change runtime and select gpu then save, and run as normal.", "It works with the GPU enabled in the gist. \r\nHow can I get it to work on my own machine aswell?", "@McSlay \r\nCould you please run the code in a virtual environment and check if you are facing the same issue", "Thank you for the suggestion. \r\nI installed tensorflow 2.1.0 (pip install tensorflow==2.1.0) and opencv (pip install opencv-contrib-python) in the virtual environment. \r\n\r\nI put the cuDNN dll in the root of the env folder. \r\nI made a python file with the code above, and also put it in the env folder. \r\n\r\nIt loads the required dll's for the GPU successfully but it still hangs afer a few frames (12 this time).\r\nI am also unable to stop the execution with CTRL+C in the windows terminal. ", "I am not sure what else to try. \r\n\r\nI tried a different python version 3.6.8 and a different cuDNN version 7.6.2.24. \r\nBoth are within the accepted range for Tensorflow 2.0 as I understand. \r\nSince Cuda is fixed at 10.1, I dont see any other tools I can try different versions of. \r\n\r\nIn earlier tests I also tried Tensorflow 1.15. This also blocks, but then when running the session. \r\n\r\nI also tried the python virtual environment on a different windows machine, one with an RTX 2070 super. Here I only installed tensorflow 2.1.0. And this also blocks after a few frames. \r\n\r\nThe code itself works, and I am bound to windows. So there is no point in trying Linux. \r\n\r\n\r\n", "i am able to replicate this, please find the gist here with [disable eager mode](https://colab.sandbox.google.com/gist/Saduf2019/1018b3abb9759f8b3db7e7c9810db9f0/2.ipynb) and [without](https://colab.sandbox.google.com/gist/Saduf2019/76a81a891a558d7c1a7b14359632c94a/untitled169.ipynb)", "@McSlay I ran your code in Eager model with GPU. It was inferencing without any blocks. I added a counter to see how many inferences (with `TF2.2` see below ), later I stopped execution. I tried both `TF2.1` and `TF2.2` and didn't face the issue. [gist](https://colab.research.google.com/gist/jvishnuvardhan/c949d112acc3625973f4bbdaf887f995/2.ipynb) is same as @Saduf2019 shared above. Did you try similar images (640,480,3) or bigger images? Not sure what is the root-cause. \r\n\r\n```\r\n\r\nInference start\r\nInference end\r\n61256\r\nInference start\r\nInference end\r\n61257\r\nInference start\r\nInference end\r\n61258\r\nInference start\r\nInference end\r\n61259\r\nInference start\r\n---------------------------------------------------------------------------\r\nKeyboardInterrupt                         Traceback (most recent call last)\r\n<ipython-input-3-a5d4c7888742> in <module>()\r\n```", "@jvishnuvardhan I also tried larger images (1280 x 720 and 1920 x 1080). These resolutions might be too big (I dont know much about TensorFlow and AI in general). So for the test I tried a small resolution. \r\n\r\nI have googled this problem allot, and it seems that I am the only one having this problem. I have tried this on 4 different windows machines, so I am probably installing it wrong. \r\nWhat I do is:\r\n\r\n- Install python 3.7\r\n- Install tensorflow \"pip install tensorflow==2.1\"\r\n- install cuda toolkit version 10.1 update 2\r\n- download cuDNN version 7.6.5.32, extract it and add it to the path variable\r\n- run the test code\r\n\r\nI also tried \r\n- lower the tensorflow versions  (1.15 [changed the code to get it to work on CPU], 2.0)\r\n- CUDA toolkit update 1\r\n- older cuDNN version: 7.6.2.24\r\n\r\nI am not sure if I tried a different model source. \r\n\r\nI also tried asking this question on the git page of the object detection. Today I got a reply that stated that the model I am using was not suitable for Tensorflow 2 (https://github.com/tensorflow/models/issues/8449). But the first line in the tutorial file that is included in the object_detection API is '!pip install -U --pre tensorflow==\"2.*\"' when I run it in jupyter notebook. \r\nThe code also did not work when I installed TensorFlow 1.15. And it also still hangs on GPU when I convert the code to work with session.run. ", "> it hangs in a library function\r\n\r\nWhich library function?  A Python stack trace from a hung process will be very useful.\r\n\r\nAlso, can you attach the logs from a run that hung?\r\n\r\nFinally, does this reproduce on TF 2.2?", "@sanjoy it hangs in the function \"model(input_tensor)\". \r\n\r\nI made a stack dump for the code that is supplied with the question\r\n```\r\nFile: \"test.py\", line 77, in <module>  run_inference_for_single_image(DETECTION_MODEL, IMAGE)\r\nFile: \"test.py\", line 32, in run_inference_for_single_image  model(input_tensor)\r\nFile: \\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1605, in __call__  return self._call_impl(args, kwargs)\r\nFile: \\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1645, in _call_impl  return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\nFile: \\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1746, in _call_flat  ctx, args, cancellation_manager=cancellation_manager))\r\nFile: \\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 598, in call  ctx=ctx)\r\nFile: \\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute  inputs, attrs, num_outputs)\r\n```\r\n\r\nThis is the output of when it hangs. It shows the \"inference start\" print (is printed just before calling the model function), and then it stops. After the model function it should print \"inference end\"\r\n```\r\n2020-04-24 11:30:16.579805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-24 11:30:18.916146: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-04-24 11:30:18.941805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 SUPER computeCapability: 7.5\r\ncoreClock: 1.785GHz coreCount: 40 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-04-24 11:30:18.946134: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-24 11:30:18.951172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-24 11:30:18.954809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-24 11:30:18.957258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-24 11:30:18.961662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-24 11:30:18.965553: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-24 11:30:18.978671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-24 11:30:18.980998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-24 11:30:18.982226: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-04-24 11:30:18.984167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 SUPER computeCapability: 7.5\r\ncoreClock: 1.785GHz coreCount: 40 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-04-24 11:30:18.987291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-24 11:30:18.988809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-24 11:30:18.990303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-24 11:30:18.991792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-24 11:30:18.993320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-24 11:30:18.996960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-24 11:30:18.998497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-24 11:30:19.000191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-24 11:30:19.430864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-24 11:30:19.433076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-04-24 11:30:19.434566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-04-24 11:30:19.436400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6281 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n[<tf.Tensor 'image_tensor:0' shape=(None, None, None, 3) dtype=uint8>]\r\ninference start\r\n2020-04-24 11:30:24.728554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-24 11:30:25.608426: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n2020-04-24 11:30:25.625904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n```\r\n\r\nIt also hangs in TF 2.2. This was used to create the stackdump in a virtual environment", "Hi @McSlay \r\n\r\nI believe this is the key line:\r\n\r\n```\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n```\r\n\r\nIt means that TF is using the CUDA driver to JIT compiler PTX to SASS.  This takes a long time.\r\n\r\nThere is no easy way to out of this unfortunately.  TF ships with recompiled SASS for a specific set of compute capabilities, and if your GPU is not compatible with any of those we have to JIT compile.  The not-easy way out is to build TF from source and ask it to compile SASS for the compute capability for your GPU.", "Hi @sanjoy \r\n\r\nThat's a shame, I will have to look into other solutions then. \r\n\r\nThank you for your help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39052\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39052\">No</a>\n"]}]