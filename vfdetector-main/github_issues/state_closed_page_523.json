[{"number": 38045, "title": "Unable to set top_k", "body": "I downloaded this example file in order to execute detection using tensorflow lite as described in the [Coral edgetpu example](https://coral.ai/docs/edgetpu/tflite-python/#load-tensorflow-lite-and-run-an-inference).\r\nI executed on a lots of image and i get **ALWAYS 10 detections** even if i lower the threshold to 0.\r\nI want to upper this limit (during my research i found something like a top_k variable set somewhere in the modules).\r\nHow do i change it from python?\r\n\r\nThe example file is [here](https://github.com/tensorflow/tensorflow/blob/82ff6702ab409a88bfb070e5f3e8749e5865c303/tensorflow/lite/examples/python/label_image.py#L62)", "comments": ["@fabian57fabian,\r\nI was able to run the code from the example file without any issues. As per the given example, I got 5 outputs. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b19ff297ca29cb334ed61d914af7cb3d/38045.ipynb). Thanks!", "@amahendrakar i know that the example prints out 5 items.\r\nLet's think that my problem is to detect the number of faces in a photo. I train my model, i run it with this script.\r\n\r\nThe pipeline in the script is:\r\n- initialize the interpreter with model\r\n- set tensor\r\n- invoke\r\n- get tensor\r\n- filter 5 best results\r\n- print results\r\n\r\nSo my problem is that the variable **result** after squeezing the output of the tensor contains only 10 values. Even when i have 50 people, the result is always 10.\r\nHow do i remove this limit of 10 results?\r\nWould you like a modified script and the output to make my point?\r\nP.S. i'm using the USB accelerator, not the coral dev board", "SOLVED.\r\nI retrained a model based on ssd_mobilenet_v1_quantized_300x300_coco14_sync.\r\nExecuting detection with ssd_mobilenet_v2_coco_quant_postprocess_edgetpu.tflite gives more than 10 results.\r\nSo the problem was the way i trained my model. It had 10 placeholders for 10 bboxes."]}, {"number": 38044, "title": "Distributed tensorflow\uff1aDoes every worker also have all tf.Variable parameters?", "body": "Execute the following code on the two workers, and the printed values are different, indicating that each worker has parameters, so is ps sever only used to update parameters?\r\nHow should the parameters of the PS server be stored in the specified GPU?\r\n# task0: 140340378338864 140340378339144 140340378449344\r\n# task1: 140099479135792 140099479136072 140099479246272\r\n\r\nworker_device = \"/job:worker/task:{i}\".format(i=FLAGS.task_index)\r\n with tf.device(tf.train.replica_device_setter(\r\n            worker_device=worker_device,\r\n            cluster=cluster\r\n        )):\r\n            x = tf.Variable([[1, 2, 3, 4]])\r\n            w = tf.Variable([[2], [2], [2], [2]])\r\n            mat = tf.matmul(x, w)\r\n            print(id(x), id(w), id(mat))\r\n", "comments": ["@Ljl-Jdsk, Please share the complete standalone to replicate the reported issue. Thanks!", "@Ljl-Jdsk, Please update for the above comment.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 38043, "title": "TextVectorizationLayer supporting custom padding token", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): v2.2.0-rc2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen using the `tf.keras.layers.experimental.preprocessing.TextVectorization` we are not able to specify a padding token which should map to the value 0. When we have a sequence such as:\r\n\r\n```\r\ntf.constant(['<PAD> word1 word2 OOV <EOS>'], dtype=tf.string)\r\n```\r\nin which the `<PAD>` is the padding token. If the TextVectorization  `output_sequence_length` is bigger than the size of the previous sequence (5), for example 10, we would get:\r\n\r\n```\r\nvectorize_layer(tf.expand_dims(tf.constant(['<PAD> word1 word2 OOV <EOS>'], dtype=tf.string), axis=0))\r\n\r\n`<tf.Tensor: shape=(1, 10), dtype=int64, numpy=array([[5,   2,   3, 1,  6,   0,   0,   0,   0,   0]])>`\r\n```\r\n(the oov token maps to 1, while the `<PAD>` token has a value different than 0 which is the default padding value by the layer). In my preprocessing I am generating sequences of the same size in each batch and the `<PAD>` token only appears at the beginning, so the model is oblivious to the 0 values and only sees the 5's as the padding token ID. However, it would be nice if we only had one token ID for the padding.\r\n\r\nI overrided this behavior in tf v2.1.0 by doing the following:\r\n\r\n```\r\nvectorize_layer.adapt(tf.data.TextLineDataset('train.txt').batch(32))\r\npad_token_vocab = vectorize_layer._convert_to_ndarray(['<PAD>'])\r\npad_token_value = vectorize_layer._convert_to_ndarray([0])\r\nvectorize_layer._insert_table_data(pad_token_vocab, pad_token_value)\r\nvectorize_layer.set_vocabulary[('<EOS>'], append=True)\r\n```\r\nHowever, there were major changes from v2.1.0 to v.2.2-rc2 so this is no longer possible. \r\n\r\n**Will this change the current api? How?** Adding an optional argument: padding_token.\r\n\r\n**Who will benefit with this feature?** I believe it's a nice to have for everyone training language models.\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @EduardoDixo! \r\nWe are checking to see whether you still need help in this issue . Have you tried latest version (TF 2.7) yet?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38042, "title": "saved_model_cli broken on 2.2rc", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): all platforms (Linux, Win, MacOS)\r\n- TensorFlow installed from binary\r\n- TensorFlow version == 2.2rc[012]\r\n- Python version: 3.6\r\n\r\nHi,\r\n\r\n`saved_model_cli` appears to be broken on all platforms and all versions, except for rc0, linux.\r\n\r\nSimply trying to use `saved_model_cli` from the command line yields on all versions and all platforms:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\marco\\repos\\tfutils\\venv\\Scripts\\saved_model_cli.exe\\__main__.py\", line 5, in <module>\r\n  File \"c:\\users\\marco\\repos\\tfutils\\venv\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 51, in <module>\r\n    from tensorflow.python.tools import saved_model_aot_compile\r\nImportError: cannot import name 'saved_model_aot_compile' from 'tensorflow.python.tools' (c:\\users\\marco\\repos\\tfutils\\venv\\lib\\site-packages\\tensorflow\\python\\tools\\__init__.py)\r\n```\r\nLinux 2.2rc0 appears to be the only exception:\r\n```\r\nusage: saved_model_cli [-h] [-v] {show,run,scan,convert,aot_compile_cpu} ...\r\nsaved_model_cli: error: too few arguments\r\n```\r\n\r\nBest Regards,\r\n\r\nMarco\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide colab link or minimal standalone code to reproduce the issue reported here. Thanks!", "Hi @ravikyram,\r\n\r\nYou can reproduce the issue by simply installing tensorflow and calling `saved_model_cli -h` to print its usage.\r\n\r\nUsing 2.2rc0 on Linux:\r\n```console\r\nvirtualenv ./venv_rc0\r\nsource ./venv_rc0/bin/activate\r\npip3 install tensorflow==2.2rc0\r\nsaved_model_cli -h\r\n```\r\nyields:\r\n```\r\nusage: saved_model_cli [-h] [-v] {show,run,scan,convert,aot_compile_cpu} ...\r\n\r\nsaved_model_cli: Command-line interface for SavedModel\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  -v, --version         show program's version number and exit\r\n\r\ncommands:\r\n  valid commands\r\n\r\n  {show,run,scan,convert,aot_compile_cpu}\r\n                        additional help\r\n```\r\n\r\nUsing 2.2rc1 or rc2 on any platform, or rc0 on MacOS or Windows:\r\n```console\r\ndeactivate\r\nvirtualenv ./venv_rc1\r\nsource ./venv_rc1/bin/activate\r\npip3 install tensorflow==2.2rc1\r\nsaved_model_cli -h\r\n```\r\nyields:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/venv_rc1/bin/saved_model_cli\", line 5, in <module>\r\n    from tensorflow.python.tools.saved_model_cli import main\r\n  File \"/home/ubuntu/venv_rc1/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 51, in <module>\r\n    from tensorflow.python.tools import saved_model_aot_compile\r\nImportError: cannot import name 'saved_model_aot_compile'\r\n```", "@marcoadurno thanks for reporting, we are looking into this, and will address it for the next release candidate.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38042\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38042\">No</a>\n", "(thanks so much for the clean repro!)", "Could someone please verify.\r\n\r\nThe file `saved_model_aot_compile.py` is missing in my `/Python/3.7/lib/python/site-packages/tensorflow/python/tools/` folder. \r\n\r\nIt works after I copied `saved_model_aot_compile.py` from the repo directly to `/tools/saved_model_aot_compile.py`.  Somehow it was not there.\r\n\r\n```\r\n\u279c  ~ python3\r\nPython 3.7.7 (default, Mar 10 2020, 15:43:33) \r\n[Clang 11.0.0 (clang-1100.0.33.17)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.2.0-rc2'\r\n```", "We think this will be solved in rc3.\n\nOn Mon, Apr 13, 2020 at 1:52 PM Rob <notifications@github.com> wrote:\n\n> Could someone please verify.\n>\n> The file saved_model_aot_compile.py is missing in my\n> /Python/3.7/lib/python/site-packages/tensorflow/python/tools/ folder.\n>\n> It works after I copied saved_model_aot_compile.py from the repo directly\n> to /tools/saved_model_aot_compile.py. Somehow it was not there.\n>\n> \u279c  ~ python3\n>\n> Python 3.7.7 (default, Mar 10 2020, 15:43:33)\n>\n> [Clang 11.0.0 (clang-1100.0.33.17)] on darwin\n>\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>\n> >>> import tensorflow as tf\n>\n> >>> tf.__version__\n>\n> '2.2.0-rc2'\n>\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38042#issuecomment-613092284>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG5CR4233DIRI7JQAZDRMN3RJANCNFSM4LWRPNYA>\n> .\n>\n", "for me this is now working with 2.2.0-rc3. Thanks"]}, {"number": 38041, "title": "Using SavedModels with low-level API in TF 2.x", "body": "I am not exactly sure whether this is a bug but it seems so\r\n\r\n**System information** \r\n* I have custom code\r\n* OS: Windows\r\n* Tensorflow 2.1.0, Conda MKL version\r\n* Python 3.7\r\n\r\n**Describe the current behavior**\r\nI am developing a system that uses Tensorflow SavedModel format to serve models in a custom embedded environment. So I want to create a basic computational graph and move on from there to test my toolchain and find bugs easily. For that I tried to create a simple graph as below:\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```python3\r\nimport tensorflow as tf\r\nimport tensorflow_core as tfcore\r\nimport pathlib\r\n\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n    A = tf.raw_ops.Placeholder(dtype=tf.dtypes.float32, shape=None, name=\"A\")\r\n    B = tf.raw_ops.Placeholder(dtype=tf.dtypes.float32, shape=None, name=\"B\")\r\n    Result = tf.raw_ops.Add(x=A, y=B, name=\"Result\")\r\n\r\n\r\nwith tfcore.python.Session(graph=graph) as sess:\r\n    script_dir = pathlib.Path(__file__).resolve().parent\r\n    builder = tfcore.python.saved_model.builder.SavedModelBuilder(str(script_dir / \"saved_model\"))\r\n    save_signature = tfcore.python.saved_model.signature_def_utils.predict_signature_def(\r\n        inputs={\"A\": A, \"B\": B},\r\n        outputs={\"Result\": Result})\r\n    builder.add_meta_graph_and_variables(sess=sess,\r\n                                         signature_def_map={\"predict\": save_signature},\r\n                                         tags=[\"test_tag\"],\r\n                                         main_op=Result)\r\n    builder.save(as_text=True) \r\n```\r\n\r\nHowever this throws an exception when executed:\r\n\r\n```\r\n2020-03-30 13:10:56.894798: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-03-30 13:10:56.897115: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nWARNING:tensorflow:From C:\\Users\\ongun\\miniconda3\\envs\\npu_tf\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\r\nTraceback (most recent call last):\r\n  File \"C:/Users/ongun/code/02_npu/simple_tf_model/sum_graph.py\", line 25, in <module>\r\n    main_op=Result)\r\n  File \"C:\\Users\\ongun\\miniconda3\\envs\\npu_tf\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\ongun\\miniconda3\\envs\\npu_tf\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\builder_impl.py\", line 582, in add_meta_graph_and_variables\r\n    main_op = main_op or legacy_init_op\r\n  File \"C:\\Users\\ongun\\miniconda3\\envs\\npu_tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 757, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"C:\\Users\\ongun\\miniconda3\\envs\\npu_tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 526, in _disallow_bool_casting\r\n    self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n  File \"C:\\Users\\ongun\\miniconda3\\envs\\npu_tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 515, in _disallow_in_graph_mode\r\n    \" this function with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```\r\n\r\nI traced the bug a little bit and the line https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder_impl.py#L537 seems to cause the error by trying to treat  `main_op` as a boolean.\r\n\r\nA simple fix seems to be viable by changing the line to:\r\n\r\n```python3\r\nmain_op = main_op if main_op is not None else legacy_init_op\r\n```\r\n\r\nI can create a PR if that's an appropriate fix.", "comments": ["@ongun-kanat,\r\nI tried to reproduce the error, but I am facing an error stating `NameError: name '__file__' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/fc4c5821238be70114a666c8c4ead564/38041-2-1.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "It is the complete script file that I am trying to run. Just a PoC code.\r\n\r\nThe script is not designed to be run in a notebook. `__file__` is obviously undefined since you're not running it in a script called by python executable. \r\n\r\nI don't know if users have write permission for files in collab notebooks but you can try it with a hardcoded file name.\r\n\r\n```python3\r\nimport tensorflow as tf\r\nimport tensorflow_core as tfcore\r\n\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n    A = tf.raw_ops.Placeholder(dtype=tf.dtypes.float32, shape=None, name=\"A\")\r\n    B = tf.raw_ops.Placeholder(dtype=tf.dtypes.float32, shape=None, name=\"B\")\r\n    Result = tf.raw_ops.Add(x=A, y=B, name=\"Result\")\r\n\r\n\r\nwith tfcore.python.Session(graph=graph) as sess:\r\n    builder = tfcore.python.saved_model.builder.SavedModelBuilder(\"saved_model\")\r\n    save_signature = tfcore.python.saved_model.signature_def_utils.predict_signature_def(\r\n        inputs={\"A\": A, \"B\": B},\r\n        outputs={\"Result\": Result})\r\n    builder.add_meta_graph_and_variables(sess=sess,\r\n                                         signature_def_map={\"predict\": save_signature},\r\n                                         tags=[\"test_tag\"],\r\n                                         main_op=Result)\r\n    builder.save(as_text=True) \r\n```\r\n\r\nOtherwise run it in a proper computer, container or VM.", "Was able to reproduce the issue. Please find the Gist [here](https://colab.research.google.com/gist/amahendrakar/e391040cc4d3fe2064732d0741f18d55/38041-2-1.ipynb). Thanks!", "The suggested fix should work -- can you submit a PR (with a test)? Thank you!", "@ongun-kanat \r\nCan you please update on the above comment.\r\nIs this still an issue?", "@Saduf2019 I submitted a PR for the issue. Sorry, I had forgotten it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38041\">No</a>\n"]}, {"number": 38040, "title": "Fix for issue #35318: zero offset for weights should be zero for int8\u2026", "body": "\u2026 fully connect tests\r\n\r\nFor asymmetric data types, the fully connect kernel expects the zero offset of the weights tensor to be zero. this PR updates the test parameters to fix this.", "comments": []}, {"number": 38038, "title": "Custom Metrics and Losses: AttributeError: 'Tensor' object has no attribute 'numpy' raised during training", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes \r\n- OS Platform and Distribution: Linux Ubuntu 18.04 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device:  No\r\n- TensorFlow installed from (source or\r\nbinary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: - Bazel\r\nversion (if compiling from source): 3.6.9 64-bit\r\n- GCC/Compiler version (if compiling from\r\nsource): Ubuntu 7.5.0-3ubuntu1~18.04\r\n- CUDA/cuDNN version: - GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to implement a custom metric function as well as a custom loss function. Both implementations are face the same issue, so I am going to focus this post in just one of them.\r\n\r\nAs an example, we have the dummy code below. The current behaviour is \r\n`AttributeError: 'Tensor' object has no attribute 'numpy'`.\r\nThe full log is also shown below. \r\n\r\n**Describe the expected behavior**\r\n\r\nMy goal is to access the value of a tensor during the fit method in order to make calculations based on said values stored in both `y_true `and `y_pred`. **These calculations cannot be done using built-in Keras backend functions.**\r\n\r\nI wrote this dummy function test just to illustrate the issue. If only `tf.print` is used, the code runs and the values in the tensors are printed on `stdout` after the fit is done. However, if I try something like `y_true.numpy()` or `print(y_true.numpy())` the code returns\r\n\r\n`AttributeError: 'Tensor' object has no attribute 'numpy'`\r\n\r\nI have tried several methods from several StackOverflow and Github threads (e.g. #27519, #36979), including combinations of `sess = tf.Session()` with `.eval()`, `tf.GradientTape`, but somehow failed to implement any of them successfully.\r\n\r\nDoes anyone know how to solve this problem?\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    from tensorflow.keras.models import Sequential, Model\r\n    from tensorflow.keras.layers import Input, LSTM, Dense\r\n    from tensorflow.keras.metrics import Metric\r\n\r\n    x, y = list(), list()\r\n    for _ in range(10):\r\n        x.append(np.arange(10))\r\n        y.append(np.random.randint(0, 2))\r\n\r\n    x = np.reshape(x, (len(x), 1, len(x[0])))\r\n    y = np.asarray(y)\r\n\r\n    print(tf.convert_to_tensor(x).numpy())\r\n\r\n    class custom_metric(Metric):\r\n        def __init__(self, name = 'custom_metrics', **kwargs):\r\n            super(custom_metric, self).__init__(name = name, **kwargs)\r\n            self.true_positives = self.add_weight(name = 'tp', initializer = 'zeros')\r\n\r\n        def update_state(self, y_true, y_pred, sample_weight = None):\r\n            self.test(y_true, y_pred)\r\n            # In a real application, new_metric would be a function that depends on\r\n            # the values stored in both y_true and y_pred \r\n            new_metric = 0.1 \r\n            self.true_positives.assign_add(tf.reduce_sum(new_metric))\r\n\r\n        def result(self):\r\n            return self.true_positives\r\n\r\n        def reset_states(self):\r\n            self.true_positives.assign(0.)\r\n\r\n        def test(self, y_true, y_pred):\r\n            tf.print(y_true)\r\n            print(y_true.numpy())\r\n\r\n    model = Sequential([\r\n        LSTM(5,\r\n             input_shape = (np.asarray(x).shape[1], np.asarray(x).shape[2]),\r\n             return_sequences = True,\r\n             recurrent_initializer = 'glorot_uniform',\r\n             activation = 'tanh',\r\n             recurrent_dropout = 0.2,\r\n             dropout = 0.2\r\n        ),\r\n        Dense(2, activation = 'softmax')\r\n    ])\r\n\r\n    model.compile(\r\n        optimizer = 'adam',\r\n        loss = 'sparse_categorical_crossentropy',\r\n        metrics = ['sparse_categorical_accuracy', custom_metric()]\r\n    )\r\n\r\n    model.run_eagerly = True\r\n\r\n    model.fit(\r\n        x, y,\r\n        epochs = 1,\r\n        batch_size = 1\r\n    )`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n    array([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\r\n\r\n           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\r\n\r\n           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\r\n\r\n           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\r\n\r\n           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\r\n\r\n           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\r\n\r\n           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\r\n\r\n           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\r\n\r\n           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\r\n\r\n           [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])\r\n    ---------------------------------------------------------------------------\r\n    AttributeError                            Traceback (most recent call last)\r\n    ~/path/to/file.py in \r\n         54     optimizer = 'adam',\r\n         55     loss = 'sparse_categorical_crossentropy',\r\n    ---> 56     metrics = ['sparse_categorical_accuracy', custom_metric()]\r\n         57 )\r\n         58 model.run_eagerly = True\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n        455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n        456     try:\r\n    --> 457       result = method(self, *args, **kwargs)\r\n        458     finally:\r\n        459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n        437           targets=self._targets,\r\n        438           skip_target_masks=self._prepare_skip_target_masks(),\r\n    --> 439           masks=self._prepare_output_masks())\r\n        440 \r\n        441       # Prepare sample weight modes. List with the same length as model outputs.\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _handle_metrics(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)\r\n       2002           metric_results.extend(\r\n       2003               self._handle_per_output_metrics(self._per_output_metrics[i],\r\n    -> 2004                                               target, output, output_mask))\r\n       2005         if return_weighted_and_unweighted_metrics or return_weighted_metrics:\r\n       2006           metric_results.extend(\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights)\r\n       1953       with K.name_scope(metric_name):\r\n       1954         metric_result = training_utils.call_metric_function(\r\n    -> 1955             metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n       1956         metric_results.append(metric_result)\r\n       1957     return metric_results\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in call_metric_function(metric_fn, y_true, y_pred, weights, mask)\r\n       1153 \r\n       1154   if y_pred is not None:\r\n    -> 1155     return metric_fn(y_true, y_pred, sample_weight=weights)\r\n       1156   # `Mean` metric only takes a single value.\r\n       1157   return metric_fn(y_true, sample_weight=weights)\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/metrics.py in __call__(self, *args, **kwargs)\r\n        194     from tensorflow.python.keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top\r\n        195     return distributed_training_utils.call_replica_local_fn(\r\n    --> 196         replica_local_fn, *args, **kwargs)\r\n        197 \r\n        198   @property\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)\r\n       1133     with strategy.scope():\r\n       1134       return strategy.extended.call_for_each_replica(fn, args, kwargs)\r\n    -> 1135   return fn(*args, **kwargs)\r\n       1136 \r\n       1137 \r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/metrics.py in replica_local_fn(*args, **kwargs)\r\n        177     def replica_local_fn(*args, **kwargs):\r\n        178       \"\"\"Updates the state of the metric in a replica-local context.\"\"\"\r\n    --> 179       update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n        180       with ops.control_dependencies([update_op]):\r\n        181         result_t = self.result()  # pylint: disable=not-callable\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)\r\n         74 \r\n         75     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\r\n    ---> 76       update_op = update_state_fn(*args, **kwargs)\r\n         77     if update_op is not None:  # update_op will be None in eager execution.\r\n         78       metric_obj.add_update(update_op)\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n        566         xla_context.Exit()\r\n        567     else:\r\n    --> 568       result = self._call(*args, **kwds)\r\n        569 \r\n        570     if tracing_count == self._get_tracing_count():\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n        613       # This is the first call of __call__, so we have to initialize.\r\n        614       initializers = []\r\n    --> 615       self._initialize(args, kwds, add_initializers_to=initializers)\r\n        616     finally:\r\n        617       # At this point we know that the initialization is complete (or less\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n        495     self._concrete_stateful_fn = (\r\n        496         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    --> 497             *args, **kwds))\r\n        498 \r\n        499     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n       2387       args, kwargs = None, None\r\n       2388     with self._lock:\r\n    -> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n       2390     return graph_function\r\n       2391 \r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n       2701 \r\n       2702       self._function_cache.missed.add(call_context_key)\r\n    -> 2703       graph_function = self._create_graph_function(args, kwargs)\r\n       2704       self._function_cache.primary[cache_key] = graph_function\r\n       2705       return graph_function, args, kwargs\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n       2591             arg_names=arg_names,\r\n       2592             override_flat_arg_shapes=override_flat_arg_shapes,\r\n    -> 2593             capture_by_value=self._capture_by_value),\r\n       2594         self._function_attributes,\r\n       2595         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n        976                                           converted_func)\r\n        977 \r\n    --> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n        979 \r\n        980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n        437         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n        438         # the function a weak reference to itself to avoid a reference cycle.\r\n    --> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n        440     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n        441 \r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n        966           except Exception as e:  # pylint:disable=broad-except\r\n        967             if hasattr(e, \"ag_error_metadata\"):\r\n    --> 968               raise e.ag_error_metadata.to_exception(e)\r\n        969             else:\r\n        970               raise\r\n\r\n    AttributeError: in converted code:\r\n\r\n    ~/path/to/file.py:7 update_state  *\r\n        self.test(y_true, y_pred)\r\n    ~/path/to/file.py:20 test  *\r\n        y_true.numpy()\r\n\r\n    AttributeError: 'Tensor' object has no attribute 'numpy'\r\n", "comments": ["convert x,y to tensors before training\r\nuse `x,y=tf.convert_to_tensor(x),tf.convert_to_tensor(y)`", "> convert x,y to tensors before training\r\n> use `x,y=tf.convert_to_tensor(x),tf.convert_to_tensor(y)`\r\n\r\nSame error occurs in `model.compile`", "@renatomello \r\nplease refer to these [link](https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy) and let us know if it helps\r\n", "> @renatomello\r\n> please refer to these [link](https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy) and let us know if it helps\r\n\r\nThe most voted answer talks about enabling eager execution. I'm using TF 2.1.0, so Eager Execution is enabled by default.\r\n`tf.executing_eagerly()`\r\n`>>> True`", "@renatomello\r\ni tried executing your code on nightly and there are no errors there, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/17aa5392b3fd5c396b72ebce42000f66/38038.ipynb) please let us know if that helps.\r\n\r\n", "> @renatomello\r\n> i tried executing your code on nightly and there are no errors there, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/17aa5392b3fd5c396b72ebce42000f66/38038.ipynb) please let us know if that helps.\r\n\r\nFinally. This one works. You almost literally saved my life. \r\n\r\nJust to be sure, what's exactly the difference from the nightly version to the 2.1.0 stable version that does the trick?", "@renatomello After releasing TF2.1 stable version, there were lots of updates. It is possible but difficult to trace which update resolved your issue. tf-nightly versions are released everyday. Thanks!", "@renatomello \r\nAs the issue is resolved in tf_nightly, please confirm if we may move this issue to closed status.", "> @renatomello\r\n> As the issue is resolved in tf_nightly, please confirm if we may move this issue to closed status.\r\n\r\nJust want to point out that, regardless of using tf_nightly, the conde only runs when `model.run_eagerly = True` passed.\r\n\r\nAll in all, the issue can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38038\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38038\">No</a>\n", "> > @renatomello\r\n> > As the issue is resolved in tf_nightly, please confirm if we may move this issue to closed status.\r\n> \r\n> Just want to point out that, regardless of using tf_nightly, the conde only runs when `model.run_eagerly = True` passed.\r\n> \r\n> All in all, the issue can be closed.\r\n\r\n@renatomello : The code does work when `model.run_eagerly=True` is added before the `fit()`. However, it is slowing the entire training down for me (eg. 50 secs vs. 5 mins per epoch). Did you observe a similar behaviour?", "Hi, @renatomello \r\n\r\nI see you everywhere about this issue and thanks so much for your contribution. However, I still meet the same error even if `model.run_eagerly = True`. Here is the simple code and I have no idea how to do.\r\n[model_print_data.txt](https://github.com/tensorflow/tensorflow/files/5263106/model_print_data.txt) \r\n\r\n> **The error I got:**\r\n> Traceback (most recent call last):\r\n>   File \"/home/training/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n>     \"__main__\", mod_spec)\r\n>   File \"/home/training/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"/home/training/r08943133/kws_test/train/model_print_data.py\", line 93, in <module>\r\n>     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n>   File \"/home/training/r08943133/venv/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n>     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n>   File \"/home/training/r08943133/venv/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n>     _run_main(main, args)\r\n>   File \"/home/training/r08943133/venv/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n>     sys.exit(main(argv))\r\n>   File \"/home/training/r08943133/kws_test/train/model_print_data.py\", line 38, in main\r\n>     print_test_data(flags)\r\n>   File \"/home/training/r08943133/kws_test/train/model_print_data.py\", line 77, in print_test_data\r\n>     print(model.get_layer(index=i).output.numpy) \r\n> AttributeError: 'Tensor' object has no attribute 'numpy'\r\n> tf.executing_eagerly: True\r\n> model.run_eagerly: True\r\n> \r\n\r\nWould you mind giving me a hand?\r\n\r\n"]}, {"number": 38037, "title": "Unable to cross compile TFLite for Raspberry Pi Zero  using Docker image nightly-develop", "body": "\r\n**System information**\r\n- OS Platform and Distribution:  Linux Mint 19:03 (based on Ubuntu 18:04)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A ?  Attempting to crosscompile tensorflowlite for Raspbery Pi Zero (Armv6) \r\n- TensorFlow installed from (source or binary): nightly-develop  docker image\r\n- TensorFlow version: 2.2\r\n- Python version: 2.7 \r\n- Installed using virtualenv? pip? conda?: docker file from latest nightly build\r\n- Bazel version (if compiling from source): bazel 0:15:0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: none\r\n\r\nWhen cross compiling for raspberry pi zero (ARMv6), as per instructions on following page [https://www.tensorflow.org/lite/guide/build_rpi](https://www.tensorflow.org/lite/guide/build_rpi)\r\nand running the command indicated to build for the ARMv6 architecture, \r\n`./tensorflow/lite/tools/make/build_rpi_lib.sh TARGET_ARCH=armv6`\r\n\r\n  there are errors indicating that the compiler options set are incorrect\r\n\r\n/usr/arm-linux-gnueabihf/include/bits/stdio.h: In function 'int getchar()':\r\n/usr/arm-linux-gnueabihf/include/bits/stdio.h:44:14: sorry, unimplemented: Thumb-1 hard-float VFP ABI\r\n getchar (void)\r\n\r\n**Any other info / logs**\r\n\r\nif I examine the file \r\n\r\n`./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n`\r\nI found that the TARGET_ARCH parameter was hard coded for armv7 in the final line of the file\r\n\r\nIf I edit the file so that this reads\r\n\r\n`CC_PREFIX=arm-linux-gnueabihf- make -j 3 -f tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv6\r\n`\r\n(the hardcoding is not ideal for obvious reasons )\r\n and then change the armv6 compiler options in\r\n\r\n`tensorflow/lite/tools/make/targets/rpi_makefile.inc\r\n`\r\nto the following\r\n\r\n```\r\nifeq ($(TARGET_ARCH), armv6)\r\nCXXFLAGS +=\r\n-marm\r\n-mfpu=vfp\r\n-mlong-calls\r\n-mthumb-interwork\r\n-mfloat-abi=hard\r\n\r\nCCFLAGS += \\\r\n  -marm \\\r\n  -mfpu=vfp \\\r\n  -mlong-calls \\\r\n  -mthumb-interwork \\\r\n  -mfloat-abi=hard\r\n\r\nLDFLAGS := \\\r\n  -Wl,--no-export-dynamic \\\r\n  -Wl,--exclude-libs,ALL \\\r\n  -Wl,--gc-sections \\\r\n  -Wl,--as-needed \\\r\n  -mfpu=vfp \\\r\n  -mlong-calls \\\r\n  -mfloat-abi=hard \\\r\n  -mthumb-interwork \r\nendif\r\n\r\n```\r\nI can compile without errors (and just a couple of warnings).\r\n\r\nGiven that it seems to be difficult (or just very slow) to compile bazel directly on a raspberry pi zero due to the lack of memory and that increasing the memory available (even via machine virtualisation such as QEMU appears impossible), I think that for deployment on a PI Zero, cross compiling is the only way forward. At the moment I can inference, but only by using a full tensorflow implementation, which is really too slow for my needs) \r\n\r\nAdditionally it would be nice to have a way to generate a python wheel as well. The main ci_build process does this for the full fat tensorflow package, (and i can run one of those on the pi zero), but I'd like to just use tensorflow-lite for my inferencing, as i think any performance improvements that I can get would be beneficial. I'm using a Pi Zero for it's low power consumption for energy efficiency reasons, so swapping out to a PI 3 or Pi 4 would not really be viable for my project. \r\n\r\nthanks for your help\r\n\r\n\r\n", "comments": ["Failed to run on Pi Zero W with the recipe. \"Illegal instruction\" in the end", "I'm facing the same issue, tried with different flags, but still \"Illegal instruction\" when importing the library.\r\n\r\n```\r\n    -march=armv6zk \\\r\n    -marm \\\r\n    -mfpu=vfp \\\r\n    -mlong-calls \\\r\n    -mthumb-interwork \\\r\n    -mfloat-abi=hard\r\n```\r\n\r\nI also tried compiling with [this](https://github.com/rvagg/rpi-newer-crosstools/tree/master/x64-gcc-6.5.0/arm-rpi-linux-gnueabihf) gcc made for rpi, but it lacks the c++ python3.7m headers and won't compile. \r\n\r\nNote that I'm compiling the tflite wheel 2.2.0 for python3.7 with the debian buster base image (using the provided Docker script).\r\n\r\nIf it's any help, [here](https://github.com/askemottelson/ada-alarm/raw/master/ML/lite/dist/tflite_runtime-2.2.0-cp37-cp37m-linux_armv6l.whl) is the python3 wheel generated, which results in \"Illegal instruction\" when imported.", "I've left to compile on Raspberry. At the end got:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter_wrapper.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /usr/local/lib/python3.7/dist-packages/tflite_runtime/_interpreter_wrapper.so: undefined symbol: __atomic_compare_exchange_8\r\n```\r\nI've added -latomic in make files. But since bazel was in use, interpreter_wrapper.so was linked without it:\r\n\r\n> arm-linux-gnueabihf-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-armv6l-3.7/interpreter_wrapper/interpreter_wrapper_wrap.o build/temp.linux-armv6l-3.7/interpreter_wrapper/interpreter_wrapper.o build/temp.linux-armv6l-3.7/interpreter_wrapper/numpy.o build/temp.linux-armv6l-3.7/interpreter_wrapper/python_error_reporter.o build/temp.linux-armv6l-3.7/interpreter_wrapper/python_utils.o -L/home/pi/Soft/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/lib/ -ltensorflow-lite -o build/lib.linux-armv6l-3.7/tflite_runtime/_interpreter_wrapper.so", "@mikhailkin Could you try:\r\n```\r\nLD_PRELOAD=/usr/lib/arm-linux-gnueabihf/libatomic.so.1.2.0 python3\r\n>>> from tflite_runtime.interpreter import Interpreter\r\n```", "> ```\r\n> LD_PRELOAD=/usr/lib/arm-linux-gnueabihf/libatomic.so.1.2.0 python3\r\n> ```\r\nWorked for me! Thank you a lot!\r\n\r\n", "> > ```\r\n> > LD_PRELOAD=/usr/lib/arm-linux-gnueabihf/libatomic.so.1.2.0 python3\r\n> > ```\r\n> \r\n> Worked for me! Thank you a lot!\r\n\r\n@mikhailkin would you mind sharing your .whl? I've been trying to cross compile for days!", "> \r\n> @mikhailkin would you mind sharing your .whl? I've been trying to cross compile for days!\r\n\r\n@askemottelson it was the first thing I intended to do... but I've called \"make clean\" in order to try other options.\r\nAnyway, I copied binaries from /usr/local/lib/python3.7/dist-packages\r\nhttps://drive.google.com/drive/folders/1hm4ShvPCqXYDuh7ijA6tepycFuP-II4K\r\nI'll put wheel as soon as it is rebuilt.\r\nCommand to launch\r\n`LD_PRELOAD=/usr/lib/arm-linux-gnueabihf/libatomic.so.1.2.0 python3 <your_args>`\r\n\r\n**UPDATE.** I've uploaded wheel", "> Anyway, I copied binaries from /usr/local/lib/python3.7/dist-packages\r\n> https://drive.google.com/drive/folders/1hm4ShvPCqXYDuh7ijA6tepycFuP-II4K\r\n\r\nThanks. Works here!", "Mikhail - this works compared to the official TF Lite instructions. With the official, I am still getting ModuleNotFoundError: No module named 'tflite_runtime' even after a very long installation on rpi zero.\r\n\r\nHowever when I run your compiled whl, I am getting ModuleNotFoundError: No module named '_interpreter_wrapper'.\r\n\r\nI am on RPI Zero and trying to install TF Lite Standalone. Above error comes when I use the following;\r\nimport tflite_runtime.interpreter as tflite\r\n\r\nI am using pip3 install for your wheel.", "Can anyone help with this? Trying to install TFlite on Rpi zero (without TF). Any pointers / guidance will help. Thanks\r\n", "> > Anyway, I copied binaries from /usr/local/lib/python3.7/dist-packages\r\n> > https://drive.google.com/drive/folders/1hm4ShvPCqXYDuh7ijA6tepycFuP-II4K\r\n> \r\n> Thanks. Works here!\r\n\r\nWhen I run your compiled whl, I am getting ModuleNotFoundError: No module named '_interpreter_wrapper'.\r\n\r\nI am on RPI Zero and trying to install TF Lite Standalone. Above error comes when I use the following;\r\nimport tflite.runtime.interpreter as tflite\r\n\r\nI am using pip3 install for your wheel.", "@rajurimu could you try the below?\r\n\r\n```\r\nwget https://github.com/askemottelson/ada-alarm/blob/master/ML/lite/dist/tflite_runtime-2.2.0rc3-cp37-cp37m-linux_armv6l.whl?raw=true\r\npip3 install tflite_runtime-2.2.0rc3-cp37-cp37m-linux_armv6l.whl\r\nLD_PRELOAD=/usr/lib/arm-linux-gnueabihf/libatomic.so.1.2.0 python3\r\n```\r\n```\r\n>>> from tflite_runtime.interpreter import Interpreter\r\n```", "> @rajurimu could you try the below?\r\n> \r\n> ```\r\n> wget https://github.com/askemottelson/ada-alarm/blob/master/ML/lite/dist/tflite_runtime-2.2.0rc3-cp37-cp37m-linux_armv6l.whl?raw=true\r\n> pip3 install tflite_runtime-2.2.0rc3-cp37-cp37m-linux_armv6l.whl\r\n> LD_PRELOAD=/usr/lib/arm-linux-gnueabihf/libatomic.so.1.2.0 python3\r\n> ```\r\n> \r\n> ```\r\n> >>> from tflite_runtime.interpreter import Interpreter\r\n> ```\r\n\r\naskemottelson, this worked and i can load tflite runtime as you suggested. Many many many thanks for that. However when I run in Thonny, it gives me the following errors - summarized below (not full traceback).\r\n\r\n***\r\nImportError: /home/pi/.local/lib/python3.7/site-packages/tflite_runtime/_interpreter_wrapper.cpython-37m-arm-linux-gnueabihf.so: undefined symbol: __atomic_compare_exchange_8\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nModuleNotFoundError: No module named '_interpreter_wrapper'\r\n***\r\nHowever these are resolved once I LD_PRELOAD Thonny with libatomic.so.\r\n\r\nMany many many thanks to you. I have been trying to find solution for quite some time and was not successful. Thanks a lot.\r\n\r\nA few followup questions though\r\n1. Is there a way to build my own whl? Instruction from earlier thread did not work. If you have something which i can follow and learn will be very useful. I am at an entry level and trying to learn\r\n2. Is there a way to 'avoid' LD_PRELOAD and have this always included instead of running LD_PRELOAD? Seems libatomic is referenced in many places as 'missing library' making it a part of PRELOAD\r\n\r\nThanks", "hi @rajurimu \r\n\r\nAs you can see from the thread I myself struggled with creating my own wheel for armv6 using the cross compilation scripts provided by tensorflow. It's definitely possible, it's about setting the right compiler flags (the missing atomic lib is related), and using the correct cross compiler. \r\n\r\ncheck https://www.tensorflow.org/lite/guide/build_rpi, but prepare to change the build scripts and CC/CX flags \r\n\r\nI think you can also just compile on the board, but it will take a long time", "https://github.com/tensorflow/tensorflow.git, commit 5c1c1085fe331de3dde07e6b3f40f69de4cb5a19.\r\nRaspberry Pi Zero W (model name\t: ARMv6-compatible processor rev 7 (v6l), any other relevant info?)\r\n\r\nSo far I haven't been able to get tflite to compile on the RPi itself.\r\nFirst I had to increase swap to 1024MB to get very far at all.\r\nNow it's failing on a link step, -o .../tensorflow/lite/tools/make/gen/linux_armv6l/bin/minimal, with errors about missing symbol __atomic_load_8 and others. I copied that particular call to g++, added -latomic and it linked.\r\n\r\nHowever I'm not sure where to add that extra link. I'll keep poking around, but any suggestions would be great.\r\n", "FYI\r\n\r\nI was able to find a working solution for running tensorflow(lite as well) on the ARM6 based raspberry pi (like the zero or the Pi1):\r\n\r\n- get a pre compilied python wheel from https://github.com/lhelontra/tensorflow-on-arm/releases or compile it on your own. \r\n- Install it on the device using pip3.\r\n\r\nin python:\r\n\r\n```\r\nimport tensorflow as  tf\r\n\r\ninterpreter = tf.lite.Interpreter(model_path)\r\n```\r\n\r\nAfter a long time looking for a good solution I found this working pretty well and easy!\r\n\r\n", "> FYI\r\n> \r\n> I was able to find a working solution for running tensorflow(lite as well) on the ARM6 based raspberry pi (like the zero or the Pi1):\r\n> \r\n> * get a pre compilied python wheel from https://github.com/lhelontra/tensorflow-on-arm/releases or compile it on your own.\r\n> * Install it on the device using pip3.\r\n> \r\n> in python:\r\n> \r\n> ```\r\n> import tensorflow as  tf\r\n> \r\n> interpreter = tf.lite.Interpreter(model_path)\r\n> ```\r\n> \r\n> After a long time looking for a good solution I found this working pretty well and easy!\r\n\r\nIt worked for me, happily running TF 2.3 and TFlite in my Pi Zero :)  ", "Sure, but I'd prefer not to have the entire TensorFlow installed, and just use the tflite_runtime lib for performance reasons", "Hi @garethjscoleman !\r\nWe are checking to see whether you still need help in this issue .Could you try with Python 3.6-3.8 with latest version (TF 2.7) and other tested configurations from [here](https://www.tensorflow.org/install/source#tested_build_configurations). Attaching relevant threads for reference. [link1](https://qengineering.eu/install-tensorflow-2-lite-on-raspberry-pi-4.html),[link2](https://www.tensorflow.org/lite/guide/build_cmake_arm),[link3](https://www.tensorflow.org/lite/guide/build_cmake_pip).Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38037\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38037\">No</a>\n"]}, {"number": 38036, "title": "java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: Nokia 7.1\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): tf-nightly 2.2-dev\r\n- Python version: - Bazel 3.7\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory: Mobile gpu Adreno 506\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to use the following [model code here](https://stackoverflow.com/a/60869999/8030107) and have made a tensorflow lite model, when we run it on android we get the following error using the GPU delegate:\r\n\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n```\r\n\r\nAnd when we switch to the CPU version we get the following error:\r\n\r\n```\r\n\r\n Cannot convert between a TensorFlowLite tensor with type FLOAT32 and a Java object of type [I (which is compatible with the TensorFlowLite type INT32).\r\n------------------------------------------------\r\nwhole error -> 2020-03-30 12:26:15.613 3471-3471/com.valuepitch.intruderdetector W/System.err: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite tensor with type FLOAT32 and a Java object of type [I (which is compatible with the TensorFlowLite type INT32).\r\n2020-03-30 12:26:15.614 3471-3471/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.Tensor.throwIfTypeIsIncompatible(Tensor.java:316)\r\n2020-03-30 12:26:15.614 3471-3471/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.Tensor.getInputShapeIfDifferent(Tensor.java:218)\r\n2020-03-30 12:26:15.614 3471-3471/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:137)\r\n2020-03-30 12:26:15.614 3471-3471/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:311)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect the model to take 3 arguments and ouput the model outputs. I've seen similar issue [here](https://github.com/tensorflow/tensorflow/issues/25131) , but couldn't comment there for unknown reasons or might be because the issue was closed? No clue as to that.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nPls find colab notebook [here](https://drive.google.com/file/d/1MN4-FX_-hz3y-UAuf7OTj_XYuVTlsSTP/view?usp=sharing)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sirius0503 Can you please share a colab? Current link not pointing to colab. Thanks!\r\nYour issue looks more like [This](https://github.com/tensorflow/tensorflow/issues/37706#issuecomment-601358807) issue. Please take a look at the responses. Thanks!", "> @sirius0503 Can you please share a colab? Current link not pointing to colab. Thanks!\r\n> Your issue looks more like [This](https://github.com/tensorflow/tensorflow/issues/37706#issuecomment-601358807) issue. Please take a look at the responses. Thanks!\r\n\r\nHere's the [colab link](https://colab.research.google.com/drive/1sPfULw3TDjWUvHvAaMXweBxwWeGfj25j) @jvishnuvardhan , have updated it in the question too!\r\n", "As the error message shows, your model has dynamic shapes and the GPU delegate doesn't support this.\r\nYou will need to fully specify the input shape (Static sizes) during conversion, and then the model can be used with gpu delegate.\r\n\r\n", "> As the error message shows, your model has dynamic shapes and the GPU delegate doesn't support this.\r\n> You will need to fully specify the input shape (Static sizes) during conversion, and then the model can be used with gpu delegate.\r\n\r\n@karimnosseir When I use `batch_size=None` and pass a `[1000, 4]` shape, then it works with the **cpu**, but produces this error for the `gpu` and `nnapi` delegate, any reason why?\r\n\r\nI have asked the same thing [here](https://github.com/tensorflow/tensorflow/issues/37706#issuecomment-606507426), @jdduke had asked me for a tflite model there.\r\n\r\nAlso, I'm assuming the problem is with the `tf.gather` ?", "We're working in improving support for this. Note that, even if we do get this working correctly in the short term (resizing graphs when using GPU/NNAPI acceleration), it's unlikely that you'll want to be resizing with any frequency, as there is a fixed cost for recreating the graph and regenerating associated shaders.", "> We're working in improving support for this. Note that, even if we do get this working correctly in the short term (resizing graphs when using GPU/NNAPI acceleration), it's unlikely that you'll want to be resizing with any frequency, as there is a fixed cost for recreating the graph and regenerating associated shaders.\r\n\r\n@jdduke Can you explain the resizing graphs with any frequency and the fixed cost and the associated shaders part part in a more simple manner and/or provide some links where I can read up more on them. Thanks a lot!\r\n\r\nAlso, where can I follow the development/support for this for the GPU/NNAPI acceleration?", "Hey @sirius0503 , apologies for the delay.\r\n\r\n> Can you explain the resizing graphs with any frequency and the fixed cost and the associated shaders part part in a more simple manner and/or provide some links where I can read up more on them. \r\n\r\nSure! The short answer is that, when you use GPU delegation (or NNAPI delegation), there is a preparation stage where the graph is effectively recompiled into a different format, based on the current tensor shapes in the graph. For the GPU, this means creating and compiling a sequence of GPU shaders. For NNAPi, this means creating an NNAPI model, underneath which there can be additional preparation/processing done by various NNAPi drivers. All of this preparation can be expensive in terms of latency, often more expensive than a single inference call.\r\n\r\nCurrently, when an input is resized, all of that preparation work has to be executed *again* based on the new graph shapes. In an ideal world, the hardware backends could accommodate dynamic shapes, avoiding the need to \"recompile\" the model after a resize. However, implementing this behavior is complicated.\r\n\r\nI can't give a concrete timeline as to when this will be possible. Some workarounds that clients have used:\r\n * Create several different interpreters with various batch sizes, and use the right one depending on the current batch input.\r\n * Use a larger batch size than may be needed, always running with that batch size, but only feed in a smaller number of batches if necessary during inference.\r\n", "I'm having a similar problem. \r\n```\r\nInternal error: Failed to run on the given Interpreter: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n```\r\n\r\nThough I've used static shapes like input1: (1, 256, 256, 3) and input2: (1, 256, 256, 3) while converting the model (using concrete function) to TFLite using both [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] ops. \r\n\r\nSo, **Question:** How do you check whether the TFlite file has static sizes or dynamic sizes and which of the nodes/tensors have such dynamic sizes?\r\n\r\nMisc Info:\r\nUsing Tensorflow version 2.2 binary from Anaconda\r\n\r\nPS: Can't post the TFlite model due to company restrictions.\r\n\r\nThanks in advance! ", "@tastelessjolt Does your model have an op that produce dynamic shape ?\r\nTry visualizing it and inspect which op causes this.\r\nIf you have a reproduce example, that will be good to check what is your problem.\r\n\r\nThanks", "@karimnosseir can you explain what do mean by a reproduce example? I don't understand.  \n\nI'll try visualizing it and check for ops producing dynamic shape. But how is it different? Since you know the shape of input of the network, doesn't it mean you know the shape of all the tensors in the graph? \n\nBtw, can you also suggest some visualizing tool for tflite? I use Netron, it seems fine but if there's a better one out there I'd like that. \n\nThanks\n", "@tastelessjolt a model that trigger this case when specifying static shape in conversion.\r\n\r\nYou can use Netron", "Does `TransposeConv` output a dynamic shape? Also in Netron, as far as I can see (The model is quite big) all the shapes are defined. Also, all the outputs of the model have outputs defined. \r\n\r\nAlso, the static shape error shows up on an Android device when I use GPU or NNAPI. And I can't use the CPU because apparently there's no implementation of `dilation` > 1 in `conv2d_transpose` for CPU in both TF and TFLite. And I can't run it on TF Interpreter in Python on PC because there's no Flex Ops support yet in Python. ", "Also, does reduce_mean and reduce_sum and similar aggregating operators produce dynamic shapes?\r\nBecause I do have a few of them.  ", "@karimnosseir Can you please reply to some of my doubts above? I'm tagging you as I forgot to tag you in the previous comments. \r\n\r\nThank you", "@tastelessjolt It depends on your model. Please share reproduce example, so we can have a look.\r\n\r\nThanks", "Hi! I encountered same error. Model runs on Android-Cpu fine, but on Gpu gives the same error.\r\nI converted model using [1,512,512,3] shapes, so inputs are static, but I think problem is in the outputs because Neutron does not shows their shapes:\r\n\r\n![image](https://user-images.githubusercontent.com/7304884/85278019-04138d00-b48d-11ea-9fa9-62ecf62c74c0.png)\r\n\r\nThe model is efficientdet-d0 from https://github.com/google/automl, I think I will be able to upload model a little bit later. \r\n", "I am having the same issue and I am pretty sure that my graph does not have any dynamic shapes because I looked that up and fixed the batch size while training the data. But adding tflite nnapi delegates still say that \"Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors\" \r\n", "Variable batch size is just one way that a graph can have dynamic-sized tensors. If you don't mind sharing your .tflite model, we can help diagnose where the dynamic node is to see if there's a potential workaround.", "yeah alright, here is my tflite model\r\n\r\nhttps://drive.google.com/file/d/10COhJsGLiu15y51IyVAd2eoz8me8iixn/view?usp=sharing \r\nadding the drive link as the github is not allowing files larger than 10MB", "Having the same problem, here is my model\r\n\r\n[model_new.zip](https://github.com/tensorflow/tensorflow/files/5122767/model_new.zip)\r\n", "Any news? @jdduke @karimnosseir ", "@srjoglekar246 can help diagnose the issue in the model from [your earlier comment](https://github.com/tensorflow/tensorflow/issues/38036#issuecomment-678924619), and why it has dynamic shapes. ", "Sorry for the late reply.\r\n\r\n@pranshugo Your model has TF ops (converted with SELECT_TF) currently this introduces dynamic shape in the runtime. It's on our radar to fix SELECT option to avoid dynamic shape during inference.\r\n", "@ryanaleksander Your model has dynamic shape, but i think we can get the information during conversion. Do you mind sharing your original TF graph, i want to check why the shape is missing during conversion.\r\n\r\nThanks", "[lstm_tflite_model.zip](https://github.com/tensorflow/tensorflow/files/5166095/lstm_tflite_model.zip)\r\nHaving the same problem and here is my model\r\nThanks a lot! \r\n@karimnosseir ", "@karimnosseir Thanks for looking into my problem, here is the original TF model\r\n[frozen_inference_graph.zip](https://github.com/tensorflow/tensorflow/files/5166146/frozen_inference_graph.zip)\r\n\r\n", "Hey @karimnosseir, can you suggest some method on how can I convert my graph to tflite without using SELECT_TF_OPS tflite supported ops flag. Because without it, the converter raises an exception. Or what could be in my graph that needs SELECT_TF_OPS? ", "@pranshugo Can you post your model with SELECT_TF_OPS? We can identify which TF ops are required to be made into TFLite custom ops (so that we dont need SELECT_TF_OPS)", "hey @srjoglekar246, \r\nHere is my model converted with SELECT_TF_OPS flag: https://drive.google.com/file/d/10COhJsGLiu15y51IyVAd2eoz8me8iixn/view?usp=sharing\r\n\r\nAlso, I am attaching my frozen graph along, so if you can suggest me a method to convert it with static shapes.\r\nI am using tensorflow2.2.0\r\nhttps://drive.google.com/file/d/16j7w8KwUnaxHvygsAygk5DM1qK61x4cs/view?usp=sharing", "I got the same **\"Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors\"** error running CREPE model on TF Lite (https://github.com/marl/crepe).\r\n\r\nHere is the converted model (from the original Keras model):\r\nhttps://drive.google.com/file/d/16IlycajzJqo5NtXv8N_7PqE3QfDUUDRd/view?usp=sharing\r\n\r\nKeras model summary\r\n```\r\nModel: \"functional_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput (InputLayer)           [(None, 1024)]            0         \r\n_________________________________________________________________\r\ninput-reshape (Reshape)      (None, 1024, 1, 1)        0         \r\n_________________________________________________________________\r\nconv1 (Conv2D)               (None, 256, 1, 128)       65664     \r\n_________________________________________________________________\r\nconv1-BN (BatchNormalization (None, 256, 1, 128)       512       \r\n_________________________________________________________________\r\nconv1-maxpool (MaxPooling2D) (None, 128, 1, 128)       0         \r\n_________________________________________________________________\r\nconv1-dropout (Dropout)      (None, 128, 1, 128)       0         \r\n_________________________________________________________________\r\nconv2 (Conv2D)               (None, 128, 1, 16)        131088    \r\n_________________________________________________________________\r\nconv2-BN (BatchNormalization (None, 128, 1, 16)        64        \r\n_________________________________________________________________\r\nconv2-maxpool (MaxPooling2D) (None, 64, 1, 16)         0         \r\n_________________________________________________________________\r\nconv2-dropout (Dropout)      (None, 64, 1, 16)         0         \r\n_________________________________________________________________\r\nconv3 (Conv2D)               (None, 64, 1, 16)         16400     \r\n_________________________________________________________________\r\nconv3-BN (BatchNormalization (None, 64, 1, 16)         64        \r\n_________________________________________________________________\r\nconv3-maxpool (MaxPooling2D) (None, 32, 1, 16)         0         \r\n_________________________________________________________________\r\nconv3-dropout (Dropout)      (None, 32, 1, 16)         0         \r\n_________________________________________________________________\r\nconv4 (Conv2D)               (None, 32, 1, 16)         16400     \r\n_________________________________________________________________\r\nconv4-BN (BatchNormalization (None, 32, 1, 16)         64        \r\n_________________________________________________________________\r\nconv4-maxpool (MaxPooling2D) (None, 16, 1, 16)         0         \r\n_________________________________________________________________\r\nconv4-dropout (Dropout)      (None, 16, 1, 16)         0         \r\n_________________________________________________________________\r\nconv5 (Conv2D)               (None, 16, 1, 32)         32800     \r\n_________________________________________________________________\r\nconv5-BN (BatchNormalization (None, 16, 1, 32)         128       \r\n_________________________________________________________________\r\nconv5-maxpool (MaxPooling2D) (None, 8, 1, 32)          0         \r\n_________________________________________________________________\r\nconv5-dropout (Dropout)      (None, 8, 1, 32)          0         \r\n_________________________________________________________________\r\nconv6 (Conv2D)               (None, 8, 1, 64)          131136    \r\n_________________________________________________________________\r\nconv6-BN (BatchNormalization (None, 8, 1, 64)          256       \r\n_________________________________________________________________\r\nconv6-maxpool (MaxPooling2D) (None, 4, 1, 64)          0         \r\n_________________________________________________________________\r\nconv6-dropout (Dropout)      (None, 4, 1, 64)          0         \r\n_________________________________________________________________\r\ntranspose (Permute)          (None, 1, 4, 64)          0         \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 256)               0         \r\n_________________________________________________________________\r\nclassifier (Dense)           (None, 360)               92520     \r\n=================================================================\r\nTotal params: 487,096\r\nTrainable params: 486,552\r\nNon-trainable params: 544\r\n_________________________________________________________________\r\n```\r\n\r\nI am using TF 2.3.0 and here is the code for the model conversion\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```\r\n\r\nFrom https://github.com/tensorflow/tensorflow/issues/24607, it seems that the dynamic size issue has been fixed with `converter.experimental_new_converter = True` (I guess the shape with \"None\" will be converted to 1?). So is there any other \"dynamic-sized tensors / operators\" included in this model causing the issue and anything I could do with it?", "@monadiconion Your model's input shape has an 'unknown' batch dim (The `None` above). If you can build the Keras model such that it has a fixed input shape (maybe by setting batch=1), the model will get acceleration support. Delegate backends work with fixed-shape graphs *only*, to avoid constant re-compilation of internal data structures.\r\n\r\n@pranshugo Which version of TF/tflite_convert are you using for your conversion? Your model contains a ResizeNearestNeighbor op, which is causing the issue. I think its being used in a way not supported by TFLite. Can you post the snippet of code where its coming from?", "I am using TF 2.2.0: \r\n$pip3 show tensorflow-gpu\r\n```\r\nName: tensorflow-gpu\r\nVersion: 2.2.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/nymble/.local/lib/python3.6/site-packages\r\nRequires: absl-py, grpcio, h5py, astunparse, keras-preprocessing, gast, numpy, protobuf, six, termcolor, tensorboard, google-pasta, wheel, tensorflow-estimator, opt-einsum, scipy, wrapt\r\nRequired-by: \r\n```\r\n\r\nI use this code in converting my model\r\n`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \r\n                                            tf.lite.OpsSet.SELECT_TF_OPS]`\r\n\r\nIn my model, I use keras.layers.Upsampling2D layer in Keras, which in turn gets converted to ResizeNearestNeighbour operation in tflite.\r\nConverting my model without above converter supported ops gives error as follows:\r\n`<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor,ResizeNearestNeighbor,ResizeNearestNeighbor.`\r\n\r\nIs upsampling2D layer not supported by tflite? or is there anything I can use in place of upsampling2D?", "@pranshugo maybe you could try to use `tf.image.resize` instead of `keras.layers.Upsampling2D`.\r\nIn TF2.3, I encountered a same problem which I solved by using the above method.", "That worked out, \r\nThanks @zldrobit \r\nAlso I found out that tf.keras.Layers.Upsampling2D with interpolation argument as 'nearest' isn't supported by tflite, whereas 'bilnear' works.", "> Also I found out that tf.keras.Layers.Upsampling2D with interpolation argument as 'nearest' isn't supported by tflite, whereas 'bilnear' works.\r\n\r\nWhich version of TensorFlow are you using? Can you file a separate bug? We do support ResizeNearestNeighbor in TFLite, but more recently added support for some attributes (e.g., half_pixel_centers).", "#43481 \r\n@jdduke Here, I filed a separate bug regarding this.\r\n", "@pranshugo @jdduke \r\n Official Release version of tflite is 2.1.0 right now and it doesn't support ResizeNearestNeighbor with unsupported problem('RESIZE_NEAREST_NEIGHBOR' version '3' for custom YOLO)\r\n\r\nI found [ https://github.com/PINTO0309/TensorflowLite-bin ] which supports 2.3.0 tflite-runtime and it fix the prev error.\r\n\r\nbut it doens't solved Attempting to use a delegate that only supports static-sized.\r\neven if i turned 'nearest' to 'bilinear'", "I was getting same Problem and i solved !!\r\n\r\nTry This code Below\r\nenv :\r\nTF :2.3\r\nTF-Lite-Runtime : 2.3\r\n\r\nThe Major point was Matching OPs betweene. TF vs TFLite\r\n```python\r\n\r\n\r\nc = tf.lite.TFLiteConverter.from_keras_model(yolo)\r\nc.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = c.convert()\r\nopen(\"./v3_tiny_noop.tflite\", 'wb').write(tflite_model)\r\n\r\n```\r\n\r\nI saw some opinion that \"experimental_new_converter\" attr sometime ocurring problems.\r\nBut in my case  ( TF 2.3, TF-L-Runtime 2.3 ) none-toco-ver fixesd problem.\r\n\r\nCustum op problem, static-sized tensors Problem\r\n\r\nI hope anyone suffering from this prob getting relieved", "Hi @Lizhi-Liao ,\r\n\r\nI met the same problem with me. In my case there's a BiLSTM layer in my model and I implement it by Keras.\r\nHave you fix it\uff1f\r\n\r\nThank you.\r\n \r\n", "Hi @srjoglekar246 @jdduke @karimnosseir ,\r\n\r\nI came with the same problem\uff1a\r\n'RuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors. Failed to apply the default TensorFlow Lite delegate indexed at 0.'\r\n I'm trying to run a tflite model on x86 CPU with XNNPack complied from TF2.4 source code. Here is my model:\r\nhttps://drive.google.com/file/d/1QZHUAao4rMo64Oy9KhdD_X1-0aAy32u2/view?usp=sharing\r\nThere is a BiLSTM layer declared by Keras otherwise I cannot convert from .pb to .tflite. Other layers are declared by TF.\r\nI use 'converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]' for converting, otherwise there would be a bug.\r\nBy the way when I run inference on this model with tf-nightly 2.4 built by pip, no error occurs. And I can run ResNet50 tflite model with XNNPack complied. Any help or suggestions are appreciated.\r\n\r\nThank you \r\n", "We only recently added support for SELECT_TF_OPS to work with XNNPack. So this should only work with the latest nightly, and may not work with the last stable TF release.", "@srjoglekar246 So if I install the latest nightly via pip, will XNNPack be complied by default?\r\nThank you", "XNNPack will work with SELECT_TF_OPS without any error at runtime.", "Hi @srjoglekar246, \r\nI installed the latest tf-nightly(2.4.0dev0929) via pip, but XNNPack is not invoked during runtime. So how to enable XNNPack in the latest version of tf-nightly?\r\nThank you.", "You cannot always know beforhand the output shape of a tensor. What about the unique op? Both in TF and TFLite the shape of the first output tensor is calculated after everything is evaluated. \r\n\r\nSo in OpenCL how do you implement this? When you only know it's a 1D tensor?", "> @monadiconion Your model's input shape has an 'unknown' batch dim (The `None` above). If you can build the Keras model such that it has a fixed input shape (maybe by setting batch=1), the model will get acceleration support. Delegate backends work with fixed-shape graphs _only_, to avoid constant re-compilation of internal data structures.\r\n\r\n@srjoglekar246 \r\nAlmost all the pre-trained models have such kind of placeholder for batch dim, and some may even have other dynamic dims like width and height. In fact, most of the immediate layer's output shape are dependent on the input size. Is there any way to set all the layers with a corresponding fixed value other than rebuild the model from scratch? It seems no API in TensorFlow can do it directly. How did you solve it when generated hosted models from pre-trained models in [TensorFlow Hub](https://tfhub.dev/s?deployment-format=lite)? Thanks.\r\n\r\n", "@hamlatzis @objectc  You do not need to specify the shape of every intermediate tensor, since TFLite propagate shapes from the input dimensions. You only need to set the *input* shapes. Also note that you can resize the batch dims later, but you need something other than 'unknown' for on-device accelerators to fuse operations together and build their internal data structures. See [this](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_lib_tf2.py#L245) as an example of how to set input shape while generating a SavedModel for TFLite conversion.", "@srjoglekar246 \r\n\r\nregarding my comment (https://github.com/tensorflow/tensorflow/issues/38036#issuecomment-702746904) I was referring to OpenCL code that runs on the GPU. \r\n\r\nFor the CPU both tensorflow and tflite for the unique op (https://www.tensorflow.org/api_docs/python/tf/unique) the first tensor is 1D but shape is calculated dynamically when the unique values are found. Let's say now that you want to make an implementation of your own to run on the GPU. Your options are OpenCL, OpenGL and Metal delegates (depending on the OS). I am interested in OpenCL. As far as I know you cannot have dynamic arrays in OpenCL, so if I were to implement this op in OpenCL I can think of only two ways.\r\n\r\n1. Allocate a static buffer with the maximum possible size, as if all values are unique. But it's a waste of memory in case I have all values the same. In this case TF and tflite just allocate a tensor of size 1\r\n\r\n2. Run the code on CPU and calculate the unique values, pass the size of the tensor to OpenCL and then run the op there. But this is a waste of processing power, I would have run the op twice, once on the CPU and once on the GPU\r\n\r\nI assume similar problems exist for other ops that the shape is calculated at runtime.\r\n\r\n", "Which is why our GPU backend doesn't support the case where shapes are generated at runtime :-)", "> Which is why our GPU backend doesn't support the case where shapes are generated at runtime :-)\r\n\r\nSo is it never going to be addressed? ", "> So is it never going to be addressed?\r\n\r\nThat is not the case. It hasn't been prioritized yet, but we plan to look at it in the future.", "> @hamlatzis @objectc You do not need to specify the shape of every intermediate tensor, since TFLite propagate shapes from the input dimensions. You only need to set the _input_ shapes. Also note that you can resize the batch dims later, but you need something other than 'unknown' for on-device accelerators to fuse operations together and build their internal data structures. See [this](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_lib_tf2.py#L245) as an example of how to set input shape while generating a SavedModel for TFLite conversion.\r\n\r\n@srjoglekar246 \r\n\r\nThis works for part of the models. Thanks.\r\n\r\nWhat if the model contains a [custom layer](https://github.com/fizyr/keras-retinanet/blob/01737e6523c09df922d36edac332a68bcda4af90/keras_retinanet/layers/filter_detections.py#L54) with operations like `greater`, `pad`, and `gather` that would generate results with the unknown shape? According to my test, even all the sizes of input and output(fixed by `pad`) from each layer are fixed, the same error still there. \r\n\r\nI have attached a converted demo model that can reproduce the issue here. Any help would be grateful. Thanks! \r\n[test_inference_model.tflite.zip](https://github.com/tensorflow/tensorflow/files/5351262/test_inference_model.tflite.zip)", "@objectc Did you author the model? Or have you exported/converted it from our model garden? If possible, it could be more useful to explore changes to the source model while we investigate delegate support for graphs with dynamic tensors.", "> @objectc Did you author the model? Or have you exported/converted it from our model garden? If possible, it could be more useful to explore changes to the source model while we investigate delegate support for graphs with dynamic tensors.\r\n\r\n@srjoglekar246 [Here](https://colab.research.google.com/drive/13D5QfXitMeNS8CN3bb5O3SG5mbbnynI8?usp=sharing) is a naive version of it build from scratch. As I mentioned, the operation `tf.greater` would generate output with unknown size. Is it possiable that TF Lite's GPU delegate would support this? Thanks.", "It is unlikely, atleast in the short term. We are still exploring use-cases for GPU dynamic-batch support, since it will take quite a bit of investment.\r\n\r\nHowever, for your use-case, may I suggest splitting the model into two TFLite models? This has worked for some clients in the past, where the \"heavy\" part of the model (Conv, Dense, etc) is accelerated by the GPU, and some post-processing (that typically involves dynamic tensors) is performed in another model. There may be some tradeoff for copying over the first model's outputs to the second's inputs, but the GPU speedup could still be more.", "Got it, That's exactly what I'm doing right now. Thank you so much for your support!", "> @monadiconion Your model's input shape has an 'unknown' batch dim (The `None` above). If you can build the Keras model such that it has a fixed input shape (maybe by setting batch=1), the model will get acceleration support. Delegate backends work with fixed-shape graphs _only_, to avoid constant re-compilation of internal data structures.\r\n> \r\n> @pranshugo Which version of TF/tflite_convert are you using for your conversion? Your model contains a ResizeNearestNeighbor op, which is causing the issue. I think its being used in a way not supported by TFLite. Can you post the snippet of code where its coming from?\r\n\r\nTo add on, this works. if you have keras `Input` layer in the model, make sure you set the batch_size of the Input layer to a fixed value before you convert it to tflite. Otherwise the batch_size dim will remain -1 as dynamic. Something like this:\r\n\r\n```\r\nimage_input = Input(shape=(input_size, input_size, 3), name='image', batch_size=1)\r\n```", "Hi @sirius0503 !\r\nWe are checking to see if you still need help in this issue .Attaching similar issues  for answers [link1](https://github.com/tensorflow/tensorflow/issues/46388#issuecomment-871074251),[link2](https://github.com/tensorflow/tensorflow/issues/49224),[link3](https://github.com/tensorflow/tensorflow/issues/46388#issuecomment-871074251),[link4](https://stackoverflow.com/questions/60826779/valueerror-none-is-only-supported-in-the-1st-dimension-tensor-flatbuffer-data/60869999#60869999) ? Please create a new issue if the issue is replicating in latest versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38036\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38036\">No</a>\n", "Hi @karimnosseir, I am having the same problem and here is my model: https://github.com/Gaozhongpai/mediapipe/blob/master/mediapipe/modules/hand_landmark/hand_mesh.tflite Could you help me check it? \r\n"]}, {"number": 38035, "title": "Couldn't match files for checkpoint", "body": "Tensorflow can't load weight from a folder with a `+` symbol such as `a+b`, but if I simply change the folder name to `a`, it works well. See the [notebook](https://colab.research.google.com/drive/1CrWIySL6kofS7CEJgN72l8yrpgohIavJ) to reproduce this issue yourself.\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): no\r\n- OS Platform and Distribution:  MacOS 10.15.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: no\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  v2.2.0-rc0-43-gacf4951a2f 2.2.0-rc1\r\n- Python version: Python 3.7.7 (default, Mar 10 2020, 15:43:33)\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\n>>> model.load_weights(\"data/dev/checkpoint/a+b/cp1.ckpt\")\r\nTraceback (most recent call last):\r\n  File \"/Users/izhangzhihao/Library/Caches/pypoetry/virtualenvs/ve-0MKN22N3-py3.7/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 95, in NewCheckpointReader\r\n    return CheckpointReader(compat.as_bytes(filepattern))\r\nRuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for data/dev/checkpoint/a+b/cp1.ckpt\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/izhangzhihao/Library/Caches/pypoetry/virtualenvs/ve-0MKN22N3-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 249, in load_weights\r\n    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)\r\n  File \"/Users/izhangzhihao/Library/Caches/pypoetry/virtualenvs/ve-0MKN22N3-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1226, in load_weights\r\n    py_checkpoint_reader.NewCheckpointReader(filepath)\r\n  File \"/Users/izhangzhihao/Library/Caches/pypoetry/virtualenvs/ve-0MKN22N3-py3.7/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 99, in NewCheckpointReader\r\n    error_translator(e)\r\n  File \"/Users/izhangzhihao/Library/Caches/pypoetry/virtualenvs/ve-0MKN22N3-py3.7/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 35, in error_translator\r\n    raise errors_impl.NotFoundError(None, None, error_message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for data/dev/checkpoint/a+b/cp1.ckpt\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n```\r\n>>> model.load_weights(\"data/dev/checkpoint/a/cp1.ckpt\")\r\n<tensorflow.python.training.tracking.util.CheckpointLoadStatus object at 0x148b5c250>\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nhttps://colab.research.google.com/drive/1CrWIySL6kofS7CEJgN72l8yrpgohIavJ\r\n", "comments": ["I can work on this as well with @shahshriya ", "We tried to replicate this issue using the same code that the author indicated that created this issue, but the program was able to execute successfully.  The load weights function was able to find the file in the same manner it finds files without \"+\" signs.  We tested additional file names (\"c+d\", \"e+f+\") and these cases also ran successfully.  Please let us know if this still appears to be an issue for you @izhangzhihao ", "Yup, it seems works as expected now with version `2.2.0-rc2`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38035\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38035\">No</a>\n"]}, {"number": 38034, "title": "Build failed on Windows with -c dbg", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: branch master 2.2\r\n- Python version: 3.7.7 Debug\r\n- Bazel version (if compiling from source): 2.0\r\n- GCC/Compiler version (if compiling from source): MSVC 19 ( 14.25.28610 )\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n\r\n\r\n**Describe the problem**\r\n```\r\ngrpc_python_plugin.exe has stopped can not seek string iterator after end \r\nprotoc-gen-grpc: Plugin failed with status code 3221226505.\r\n```\r\nit failed when try to build\r\n```\r\nProtoCompile tensorflow/core/profiler/profiler_analysis_pb2.py; 15s local\r\nProtoCompile tensorflow/core/debug/debug_service_pb2.py; 15s local\r\nProtoCompile tensorflow/core/profiler/profiler_service_pb2.py; 15s local\r\n```\r\n<img width=\"478\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17511625/77883331-1ebed700-728d-11ea-8e77-8c5f7e7dcfb2.png\">\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build --config=opt -c dbg --jobs=8 --cxxopt=\"/wd4716\" //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["https://github.com/grpc/grpc/blob/e1ff06310ce8a2ec3c90b9d71e12f3818566b18f/src/compiler/python_generator_helpers.h#L145\r\n\r\nI fixed it. The problem comes from the grpc code it self. because when `current == s.end()`. the line `current = next + 1` throw a error in debug level. The temporary way to fix it is to change it to\r\n```\r\n#if _ITERATOR_DEBUG_LEVEL >= 1\r\n    if (next == s.end()) {\r\n      break;\r\n    }\r\n    current = next + 1;\r\n#else\r\n    current = next + 1;\r\n#endif\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38034\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38034\">No</a>\n"]}, {"number": 38033, "title": "tflite_runtime pip package compiled from source does not run", "body": "**System information** \r\n- OS Platform and Distribution: \r\nUbuntu 18.04 running in Windows 10 WSL\r\n- TensorFlow installed from: \r\nBinary, however the tflite_runtime installed from source.\r\n- TensorFlow version:  2.1.0\r\n- Python version: 3.6\r\n - GCC/Compiler version: 7.4.0\r\n\r\n**Describe the current behavior**\r\nCompiled the tflite_runtime from master using:\r\n`./tensorflow/lite/tools/pip_package/build_pip_package.sh`\r\nand installed:\r\n`pip install --upgrade tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.1.0-cp36-cp36m-linux_x86_64.whl`\r\nTrying to run load an tflite  file:\r\n`python -c \"from tflite_runtime.interpreter import Interpreter; Interpreter('foo.tflite')\"`\r\ngives the following error:\r\n`Traceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/fresve01/.local/lib/python3.6/site-packages/tflite_runtime/interpreter.py\", line 203, in __init__\r\n    _interpreter_wrapper.CreateWrapperFromFile(\r\nAttributeError: module 'tflite_runtime.interpreter_wrapper' has no attribute 'CreateWrapperFromFile'`\r\n\r\n**Describe the expected behavior**\r\nThe tflite file should load without error.\r\nAlso the README.md file with build instructions should be updated since there is no swig dependency any more.\r\n\r\n**Standalone code to reproduce the issue** \r\n`./tensorflow/lite/tools/pip_package/build_pip_package.sh`\r\n`pip install --upgrade tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.1.0-cp36-cp36m-linux_x86_64.whl`\r\n`python -c \"from tflite_runtime.interpreter import Interpreter; Interpreter('foo.tflite')\"`", "comments": ["Hi nemo42,\r\n\r\nI'm working on this -- meanwhile, would you try changing following code pieces?\r\n\r\non `tensorflow/lite/tools/pip_package/setup.py`\r\n```\r\ndiff --git a/tensorflow/lite/python/interpreter.py b/tensorflow/lite/python/interpreter.py\r\nindex e3d7d04be1..7015ffd670 100644\r\n--- a/tensorflow/lite/python/interpreter.py\r\n+++ b/tensorflow/lite/python/interpreter.py\r\n@@ -35,15 +35,15 @@ if not __file__.endswith('tflite_runtime/interpreter.py'):\r\n   # rule.\r\n   # pylint: disable=g-inconsistent-quotes\r\n   _interpreter_wrapper = LazyLoader(\r\n      \"_interpreter_wrapper\", globals(),\r\n      \"tensorflow.lite.python.interpreter_wrapper.\"\r\n      '_pywrap_tensorflow_interpreter_wrapper')\r\n   # pylint: enable=g-inconsistent-quotes\r\n\r\n   del LazyLoader\r\n else:\r\n   # This file is part of tflite_runtime package.\r\n-  from tflite_runtime import interpreter_wrapper as _interpreter_wrapper\r\n+  from tflite_runtime import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper\r\n```\r\n\r\non `tensorflow/lite/tools/pip_package/setup.py`\r\n```\r\ndiff --git a/tensorflow/lite/tools/pip_package/setup.py b/tensorflow/lite/tools/pip_package/setup.py\r\nindex 0895e30255..5a7fa0bed8 100644\r\n--- a/tensorflow/lite/tools/pip_package/setup.py\r\n+++ b/tensorflow/lite/tools/pip_package/setup.py\r\n@@ -160,7 +160,7 @@ LIB_TFLITE = 'tensorflow-lite'\r\n LIB_TFLITE_DIR = make_output('libdir')\r\n\r\n ext = Extension(\r\n-    name='%s._interpreter_wrapper' % PACKAGE_NAME,\r\n+    name='%s._pywrap_tensorflow_interpreter_wrapper' % PACKAGE_NAME,\r\n```\r\n", "> Hi nemo42,\r\n> \r\n> I'm working on this -- meanwhile, would you try changing following code pieces?\r\n\r\nHi teijeong,\r\n\r\nApplying the patches above seems to have solved the issue, thanks!", "@nemo42 \r\n\r\n Please close this thread if it solves your question. Thanks!", "Good to hear that! Closing this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38033\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38033\">No</a>\n", "Actually this change is not applied to the repo yet, please wait to close this issue until the change makes its way to the code.", "Fixed by https://github.com/tensorflow/tensorflow/commit/6a701314351845ecbc6641ecd2f68e21b582c3f8", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38033\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38033\">No</a>\n"]}, {"number": 38032, "title": "When I replace fit_generator with fit, behavior is inconsistent.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Windows 10\r\n- TensorFlow installed from (source or\r\nbinary): conda - TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.6\r\n\r\nYou can collect some of this information using our environment capture\r\nunknown 2.0.0\r\n\r\n**Describe the current behavior**\r\nI train a multi-outputs model with a custom data generator. It runs successfully with the api(model.fit_generator). But when I swap to model.fit, it is broken. \r\n\r\nI found that 'fit' can not handle 'multi outputs in list' (such as `yield x, [y1, y2, ...]`) correctly, but tuple(such as `yield x, (y1, y2, ...)`) is ok.\r\n\r\n**Describe the expected behavior**\r\nI think both fit_generator and fit should have a consistent behavior to a same generator.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow.keras import layers, optimizers, losses, Model, Input\r\n\r\ninputs = Input(shape=(10,), name='img_input')\r\n\r\nx1 = layers.Dense(5)(inputs)\r\nx2 = layers.Dense(2)(inputs)\r\n\r\nmodel = Model(inputs=inputs,\r\n                    outputs=[x1, x2])\r\n\r\nmodel.compile(\r\n    optimizer=optimizers.Adam(),\r\n    loss=losses.categorical_crossentropy)\r\n\r\n\r\nimg_data = np.random.random_sample(size=(1, 10))\r\ntargets_0 = np.random.random_sample(size=(1, 5))\r\ntargets_1 = np.random.random_sample(size=(1, 2))\r\n\r\ndef generator_tuple():\r\n\r\n    while True:\r\n        yield img_data, (targets_0, targets_1)\r\n\r\n\r\ndef generator_list():\r\n    while True:\r\n        yield img_data, [targets_0, targets_1]\r\n\r\nmodel.fit_generator(generator_tuple(), steps_per_epoch=1, epochs=3) # ok\r\nmodel.fit(generator_tuple(), steps_per_epoch=1, epochs=3) # ok\r\nmodel.fit_generator(generator_list(), steps_per_epoch=1, epochs=3) # ok\r\nmodel.fit(generator_list(), steps_per_epoch=1, epochs=3) # raise error\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Users/yuyang/Documents/codehub/yolov3/bug.py\", line 40, in <module>\r\n    model.fit(generator_list(), steps_per_epoch=1, epochs=3) # ok\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 224, in fit\r\n    distribution_strategy=strategy)\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 547, in _process_training_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 606, in _process_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\", line 566, in __init__\r\n    reassemble, nested_dtypes, output_shapes=nested_shape)\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 540, in from_generator\r\n    output_types, tensor_shape.as_shape, output_shapes)\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\nest.py\", line 471, in map_structure_up_to\r\n    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\nest.py\", line 471, in <listcomp>\r\n    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 1216, in as_shape\r\n    return TensorShape(shape)\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 776, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 776, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 718, in as_dimension\r\n    return Dimension(value)\r\n  File \"C:\\Users\\yuyang\\Miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 193, in __init__\r\n    self._value = int(value)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'\r\n```\r\n", "comments": ["@yuto3o,\r\nI was able to reproduce the error with [TF v2.0](https://colab.research.google.com/gist/amahendrakar/6845d9d75de52e4bcfd057cdbb930cf3/38032.ipynb). However, the issue seems to be fixed in the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/7c1cbc3d1cc01db03b77e2712e77d6dc/38032-tf-nightly.ipynb) version, I was able to run the code without any issues. Please find the attached gist. Thanks!", "Thank you. However, the issue seems to be fixed in TF v2.1.0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38032\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38032\">No</a>\n"]}, {"number": 38031, "title": "fix wrong usage of _maybe_build function in compute_output_shape", "body": "Related to the issue https://github.com/tensorflow/tensorflow/issues/38030#issue-589977024\r\n\r\nThis commit aims to fix the wrong usage of _maybe_build function from cimpute_output_shape in order to provide more accurate and robust automatic output shape inference.", "comments": ["@omalleyt12 Would you be able to review this commit?", "@yoshihikoueno Can you please resolve conflicts? Thanks!", "@gbaned I've resolved conflicts.\r\nI noticed that [a commit](https://github.com/tensorflow/tensorflow/commit/a807a425a296af73fd0e74e488d21dde116a4c7f) had been made to fix the same issue as this pull request aimed to fix. If that commit fixes all the bugs, then I think I would just close this PR, but I'm a bit skeptical if that commit really solves the issue without causing any other bugs.\r\n\r\nMy approach was to let ```_maybe_build``` take  ```input_shaps``` if the caller prefers that way, and the ```_maybe_build``` will just stop trying to guess ```input_shapes``` from ```inputs``` as it's already provided in that case.\r\n\r\nThe approach taken by the commit was to implicitly pass ```input_shapes``` as ```inputs```. And I'm not sure if this is a better approach.\r\nThe biggest problem is that ```inputs``` may be either really  ``` inputs``` or ```input_shapes``` inside ```_maybe_build```. Although ```inputs```, which represents literally \"inputs\" to the model/layer, and ```input_shapes```, which represents the \"shapes\" of the inputs to model/layer, are two totally different things, this approach will mix them up in a single variable ```inputs``` and make ```_maybe_build``` function guess what the content of the variable really is.\r\n\r\nNot only does this kind of variable usage affect code readability, but it might also cause other bugs inside ```_maybe_build```. ```_maybe_build``` will use  ```inputs``` to make sure inputs are compatible and guess input type if necessary. One obvious bug that could happen is ```_maybe_build``` function guesses a wrong type because of  ```inputs``` being ```input_shapes```.", "@gbaned @omalleyt12 I would like to discuss and clarify which way of fixing this issue works best and I'm happy to modify my PR to reflect the conclusion.\r\n\r\nAnd I just want to kindly ask @gbaned and @omalleyt12 if it's possible to shorten the time waiting for the review especially for this kind of very very tiny PR where just a couple of lines being changed. Ignoring PRs will cause multiple people to work on the same issue like this one, and in the end, it just makes me feel like the time I spent working on the issue was totally a waste."]}, {"number": 38030, "title": "Wrong usage of ```tf.keras.layers.Layer._maybe_build```", "body": "The function [```tf.keras.layers.Layer._maybe_build```](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L2357-L2394) is used inappropriately in some functions inside tensorflow implementation.\r\n\r\nFor example, [```tf.keras.layers.Layer.compute_output_shape```](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L663-L705) calls [```tf.keras.layers.Layer._maybe_build```](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L2357-L2394) with ```input_shapes``` argument([*1](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L687)), where it's supposed to be ```inputs```.\r\n\r\n[```tf.keras.layers.Layer._maybe_build```](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L2357-L2394) tries to detect input shape by accessing ```inputs.shape``` attribute([*2](https://github.com/tensorflow/tensorflow/blob/1f98a556eb4a8981241a9ebd4257d8e95060462c/tensorflow/python/keras/engine/base_layer.py#L2371)), which doesn't exist because ```inputs``` is not a tensor but ```input_shape``` already, hence this function doesn't work as expected. This incident is eventually reported to a user as layer output shape inference failure and suggests overriding ```compute_output_shape``` function.\r\n\r\nAlthough this error can be solved by overriding ```compute_output_shape``` as suggested by the error message, there're many cases where users don't need to do so if the functions are working properly.\r\n\r\nPossible ways to solve this circumstance I can think of are\r\n\r\n1. Add keyword argument ```input_shape``` to ```_maybe_build``` function so that they can receive either ```inputs``` or ```input_shapes```.\r\n\r\n2. Create a dummy tensor inside ```compute_output_shape``` which can be used as an input argument to ```_maybe_build``` function.\r\n\r\nI think either way won't take that much time to implement, so I hope this can be fixed soon.\r\nIf developers in the TensorFlow team are busy and can't spare time for this issue, I can work on this and issue a pull request. In that case, please give me some suggestions or opinions regarding the implementation or the modification, so that it can be approved and merged smoothly.\r\n\r\nThank you.", "comments": ["Here is the sample code to demonstrate this issue.\r\n\r\n```python\r\nfrom tensorflow.keras.layers import Layer\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import Model\r\nimport traceback\r\n\r\n\r\nclass CustomLayer(Layer):\r\n    def __init__(self, units_base, *args, **kargs):\r\n        super().__init__(self, *args, **kargs)\r\n        self.units_base = units_base\r\n        return\r\n\r\n    def build(self, input_shapes):\r\n        self.layer = layers.Dense(self.units_base + input_shapes[-1])\r\n        self.built = True\r\n        return\r\n\r\n    def call(self, inputs, *args, **kargs):\r\n        x = self.layer(inputs)\r\n        return x\r\n\r\n\r\nclass CustomModel(Model):\r\n    def __init__(self, *args, **kargs):\r\n        super().__init__(self, *args, **kargs)\r\n        return\r\n\r\n    def build(self, input_shapes):\r\n        self.l0 = CustomLayer(input_shapes[-1])\r\n        self.l1 = CustomLayer(input_shapes[-1])\r\n        self.built = True\r\n        return\r\n\r\n    def call(self, inputs, *args, **kargs):\r\n        x = self.l0(inputs)\r\n        x = self.l1(x)\r\n        return x\r\n\r\n\r\nif __name__ == '__main__':\r\n    print('Case1: the model is not built in prior to compute_output_shape')\r\n    try:\r\n        model = CustomModel()\r\n        print('DEBUG: init bult:', model.built)\r\n        outputshape = model.compute_output_shape([100, 50])\r\n        print(outputshape)\r\n    except Exception as e:\r\n        print('Failed')\r\n        traceback.print_exc()\r\n\r\n    print('\\nCase2: the model is built in prior to compute_output_shape')\r\n    try:\r\n        model = CustomModel()\r\n        model.build([100, 50])\r\n        print('DEBUG: init bult:', model.built)\r\n        outputshape = model.compute_output_shape([100, 50])\r\n        print(outputshape)\r\n    except Exception as e:\r\n        print('Failed')\r\n        traceback.print_exc()\r\n```", "@yoshihikoueno, I tried with Tf2.2 rc1, received `TypeError: 'NoneType' object is not subscriptable` . Please take a look at [gist](https://colab.sandbox.google.com/gist/gadagashwini/0ee73dfa03dce12f0bcebc2496bdf7ae/untitled486.ipynb) and confirm the expected behavior. Thanks", "@gadagashwini Thank you very much for sparing time to try out my sample code.\r\nYes, that behavior is not expected and it's caused by the bug I'm pointing out here. The root problem of this bug is that compute_output_shape function calls _maybe_build function with the wrong argument, which makes input_shapes to be None.\r\n\r\nWith my [pull request](https://github.com/tensorflow/tensorflow/pull/38031), it works as expected.", "@gadagashwini Here's the sample result.\r\n\r\nWith the current master branch:\r\n```\r\nCase1: the model is not built in prior to compute_output_shape\r\nDEBUG: init bult: False\r\nFailed\r\n\r\nCase2: the model is built in prior to compute_output_shape\r\nDEBUG: init bult: True\r\nTraceback (most recent call last):\r\n  File \"<ipython-input-2-041b5e76fb83>\", line 45, in <module>\r\n    outputshape = model.compute_output_shape([100, 50])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\", line 718, in compute_output_shape\r\n    return super(Network, self).compute_output_shape(input_shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 681, in compute_output_shape\r\n    self._maybe_build(input_shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 2416, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File \"<ipython-input-2-041b5e76fb83>\", line 29, in build\r\n    self.l0 = CustomLayer(input_shapes[-1])\r\nTypeError: 'NoneType' object is not subscriptable\r\n(100, 150)\r\n```\r\n\r\nWith [the pull request](https://github.com/tensorflow/tensorflow/pull/38031) applied:\r\n```\r\nCase1: the model is not built in prior to compute_output_shape\r\nDEBUG: init bult: False\r\n(100, 150)\r\n\r\nCase2: the model is built in prior to compute_output_shape\r\nDEBUG: init bult: True\r\n(100, 150)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38030\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38030\">No</a>\n"]}, {"number": 38029, "title": "Update the tflite Makefile", "body": "Update the tflite Makefile, as the `ruy` subfolder has been moved into `lite/experimental/ruy`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38029) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38029) for more info**.\n\n<!-- ok -->"]}, {"number": 38028, "title": "[tflite] Build tflite using ./build_lib.sh failed!", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux16.04 \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2\r\n- GCC/Compiler version (if compiling from source):  g++ 5.4\r\n\r\n**Describe the problem**\r\n\r\nBuild tflite using ./build_lib.sh failed!\r\n\r\n```sh\r\ntensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a(cpu_backend_context.o): In function `tflite::CpuBackendContext::~CpuBackendContext()':\r\ncpu_backend_context.cc:(.text+0x3e8): undefined reference to `ruy::detail::SystemAlignedFree(void*)'\r\ncpu_backend_context.cc:(.text+0x43d): undefined reference to `ruy::detail::SystemAlignedFree(void*)'\r\ncpu_backend_context.cc:(.text+0x445): undefined reference to `ruy::detail::SystemAlignedAlloc(long)'\r\ncpu_backend_context.cc:(.text+0x470): undefined reference to `ruy::detail::SystemAlignedFree(void*)'\r\ncpu_backend_context.cc:(.text+0x48e): undefined reference to `ruy::detail::SystemAlignedFree(void*)'\r\ncpu_backend_context.cc:(.text+0x4f0): undefined reference to `ruy::detail::SystemAlignedFree(void*)'\r\ncpu_backend_context.cc:(.text+0x4f8): undefined reference to `ruy::detail::SystemAlignedAlloc(long)'\r\ncpu_backend_context.cc:(.text+0x520): undefined reference to `ruy::detail::SystemAlignedFree(void*)'\r\ncpu_backend_context.cc:(.text+0x53e): undefined reference to `ruy::detail::SystemAlignedFree(void*)'\r\ncpu_backend_context.cc:(.text+0x57e): undefined reference to `ruy::ThreadPool::~ThreadPool()'\r\n...\r\n```\r\n\r\nAs the `ruy` subfolder has been moved into `lite/experimental/ruy`, we should update the `Makefile` to correct the depends.\r\n\r\n```diff\r\ntensorflow/lite/tools/make$ git diff ./Makefile\r\ndiff --git a/tensorflow/lite/tools/make/Makefile b/tensorflow/lite/tools/make/Makefile\r\nindex ef265cc..fa2a3f1 100644\r\n--- a/tensorflow/lite/tools/make/Makefile\r\n+++ b/tensorflow/lite/tools/make/Makefile\r\n@@ -119,7 +119,8 @@ $(wildcard tensorflow/lite/c/*.c) \\\r\n $(wildcard tensorflow/lite/core/*.cc) \\\r\n $(wildcard tensorflow/lite/core/api/*.cc) \\\r\n $(wildcard tensorflow/lite/experimental/resource/*.cc) \\\r\n-$(wildcard tensorflow/lite/experimental/ruy/*.cc)\r\n+$(wildcard tensorflow/lite/experimental/ruy/*.cc)  \\\r\n+$(wildcard tensorflow/lite/experimental/ruy/ruy/*.cc)\r\n ifneq ($(BUILD_TYPE),micro)\r\n CORE_CC_ALL_SRCS += \\\r\n $(wildcard tensorflow/lite/kernels/*.cc) \\\r\n@@ -142,14 +143,18 @@ CORE_CC_EXCLUDE_SRCS := \\\r\n $(wildcard tensorflow/lite/*test.cc) \\\r\n $(wildcard tensorflow/lite/*/*test.cc) \\\r\n $(wildcard tensorflow/lite/*/*/benchmark.cc) \\\r\n+$(wildcard tensorflow/lite/*/*/*/benchmark.cc) \\\r\n $(wildcard tensorflow/lite/*/*/example*.cc) \\\r\n+$(wildcard tensorflow/lite/*/*/*/example*.cc) \\\r\n $(wildcard tensorflow/lite/*/*/test*.cc) \\\r\n $(wildcard tensorflow/lite/*/*/*test.cc) \\\r\n $(wildcard tensorflow/lite/*/*/*tool.cc) \\\r\n $(wildcard tensorflow/lite/*/*/*/*test.cc) \\\r\n+$(wildcard tensorflow/lite/experimental/ruy/ruy/test*.cc) \\\r\n+$(wildcard tensorflow/lite/experimental/ruy/ruy/profiler/test*.cc) \\\r\n $(wildcard tensorflow/lite/kernels/*test_main.cc) \\\r\n $(wildcard tensorflow/lite/kernels/*test_util*.cc) \\\r\n-tensorflow/lite/experimental/ruy/tune_tool.cc \\\r\n+tensorflow/lite/experimental/ruy/ruy/tune_tool.cc \\\r\n tensorflow/lite/tflite_with_xnnpack.cc \\\r\n $(MINIMAL_SRCS)\r\n\r\n@@ -351,3 +356,4 @@ $(DEPDIR)/%.d: ;\r\n .PRECIOUS: $(DEPDIR)/%.d\r\n\r\n -include $(patsubst %,$(DEPDIR)/%.d,$(basename $(ALL_SRCS)))\r\n```\r\n\r\nAnd I make a pull request : https://github.com/tensorflow/tensorflow/pull/38029\r\n", "comments": ["I make a pull request: https://github.com/tensorflow/tensorflow/pull/38029", "@ausk \r\nAs there is already a PR can we please move this issue to closed as #38029 this would be sufficient to monitor the issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38028\">No</a>\n"]}, {"number": 38027, "title": "TensorFlow for .NET / C#", "body": "Provides a .NET Standard binding for TensorFlow. It aims to implement the complete Tensorflow API in C# which allows .NET developers to develop, train and deploy Machine Learning models with the cross-platform .NET Standard framework.", "comments": ["> Provides a .NET Standard binding for TensorFlow. It aims to implement the complete Tensorflow API in C# which allows .NET developers to develop, train and deploy Machine Learning models with the cross-platform .NET Standard framework.\r\n\r\nyes\uff0c", "Great!", "yes\uff0csupport", "TensorFlow has not forgotten that there are 6 million .NET developers in the world.", "I am a machine vision developer in the industrial field. At present, it is very convenient to use tensorflow for development, but there are many difficulties in the production line deployment, mainly the compatibility between Python and the  existing .Net platform. Generally, the system environment of the industrial production site is difficult to change, so if there is tensorflow under the .Net platform\uff08especially C#\uff09, it's very convenient!", "Hi @Oceania2018,\r\n\r\nThis doesn't really belong in the tensorflow/tensorflow repository: docs are in the [tensorflow/docs](https://github.comn/tensorflow/docs)\r\n\r\nWe have a link to this on the tensorflow.org/api_docs page, from here: https://github.com/tensorflow/docs/tree/master/site/en/api_docs\r\n\r\nWe'll see if we can do more than that, but can we continue this conversation over on tensorflow/docs?", "@MarkDaoust Appricate for you quick response. Maybe I could tell more specific reason why we need this kind of folder.\r\n* .NET binding will complete most of the key functions act like python binding.\r\n* .NET binding will add more custimized APIs in the `cswrap_tfe.h`, making changes in the seperate folder will reduce the main repo's risk.\r\n* .NET binding will contains two parts, one is C++ exported functions in `tensorflow` repo, another one is C# code in `tensorflow.net` repo.\r\n\r\n![image](https://user-images.githubusercontent.com/1705364/77929673-8d8d3780-726f-11ea-8c4e-e1f558a4cdb3.png)\r\n\r\n\r\n", "@Oceania2018\r\n\r\nThanks for the context, but I think I'm still missing some. \r\n\r\nThis sounds like a big plan, are you discussing this plan with the TensorFlow team? Is there an RFC for this or something? Or is this PR the start of that conversation?", "@MarkDaoust Right, I consider this PR as an initial converstaion. Because I've no idea how to communicate with the TensorFlow team. No clue how to make this plan happen. Is there any docs related to add new language binding in the repo? I'm the key contributor for [TensorFlow.NET](https://github.com/SciSharp/TensorFlow.NET). Our goal is making tensorflow more friendly to .NET developers.\r\n\r\nBTW, we really need mentor to guide how to collaborate with tensorflow team in terms of code standards and so on.", "Then I think the place you want to start is with the [tensorflow/community](https://github.com/tensorflow/community/) repository.\r\n\r\nFor significant changes to core tensorflow we have the conversations as RFCs in that repository.\r\nBut you may want to look into creating a SIG also in [tensorflow/community](https://github.com/tensorflow/community/).\r\n\r\n@joanafilipa @ewilderj: what would be the next step here?", "Before diving in and writing a full proposal or anything big, maybe open an issue there describing your plans, and they may be able to advise.", "@MarkDaoust Do you think https://github.com/SciSharp/TensorFlow.NET/issues/529 is making sense?", "If there's interest (there seems to be) I would look into making a SIG.\r\n\r\nI'm expecting it to be hard, or impossible to get anything significant into the core TensorFlow repo.\r\nTensorFlow's been trying to split up into smaller parts wherever possible.\r\n\r\n`java`, for example is in the process of moving out into a [repo owned by sig-jvm](https://github.com/tensorflow/java).\r\n\r\nSo I expect keeping the code out of the main repository, if at all possible, to meet less resistance.", "That would be great if there is a repo https://github.com/tensorflow/csharp", "@Oceania2018  he is one of best developer, one of best programming language, so make him help @google-admin ", "@MarkDaoust => @Oceania2018  has demonstrated on his twitter site that using Csharp binding on Windows, tensorflow 2.2 is faster than the version running through python. \r\n\r\nWe need tensorflow.dll both CPU and GPU for windows to stay commitment. ", "@Oceania2018 \r\n\r\nSupport  [https://github.com/tensorflow/csharp](https://github.com/tensorflow/tensorflow/issues/35405#issuecomment-653629266)\r\nLatest [tensorflow.dll ](https://github.com/tensorflow/tensorflow/issues/35405#issuecomment-641619156)for Tensorflow.NET"]}, {"number": 38026, "title": "How to run .pb model with tensorflow( or tensorflowLite)  using DSP in android", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@suyali \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nMake sure you also include the code snippet to reproduce the issue.Please, fill the issue template.\r\n\r\nAlso, refer [link1](https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#1) and [link2](https://www.tensorflow.org/lite) and see if it helps you. Thanks!", "@suyali \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38026\">No</a>\n"]}, {"number": 38025, "title": "segfault in FlexDelegate on android", "body": "I'm hoping to run a custom tensorflow/tflite model (one that uses tflite's select ops) on-device in an android app.  My understanding is that I need to configure the tflite interpreter with a FlexDelegate, but when I try to do this (on the android-studio emulator), the app segfaults, apparently in the FlexDelegate constructor. I've managed to reproduce the crash in a minimal code, which I link to and describe below. \r\n\r\nThanks in advance for any help on this, and thanks also to all the devs for creating tensorflow!\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Only a little.  I've added a call to the FlexDelegate constructor in the MainActivity of the default flutter app that android studio generates when you tell it to start a new project.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): the crash I'm seeing happens in an android phone emulator, but the box the emulator is running on is running gentoo linux.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: the android emulator that android-studio provides (I've tested a few configurations including api 27, 29, and R as well as x86 and x86_64 abis)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): \r\nin app/build.gradle, \r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n\r\n**Describe the current behavior**\r\nthe app crashes while attempting to construct a FlexDelegate instance while running in the emulator.  (I actually don't have a physical device handy, so I can't test to see if it happens on real hardware right now.)\r\n\r\n**Describe the expected behavior**\r\nFlexDelegate should be created with no segfault\r\n\r\n**Standalone code to reproduce the issue** \r\nThe line that crashes is \r\n\r\nFlexDelegate delegate = new FlexDelegate();\r\n\r\nwhich I've added to the configureFlutterEngine method of the app's MainActivity.  I've put the code for the full example app in this repository:\r\n\r\nhttps://github.com/particlebbq/tflite_bug_report\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe error message in the logcat is:\r\n2020-03-29 16:22:01.392 11042-11042/com.example.tflitebugreport A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xfffffff4 in tid 11042 (tflitebugreport), pid 11042 (tflitebugreport)\r\n", "comments": ["The first thing to check is that you're using the latest nightly builds. Can you try manually [clearing your gradle cache](https://stackoverflow.com/questions/23025433/how-to-clear-gradle-cache) or change\r\n\r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\nimplementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n```\r\nto\r\n```\r\nimplementation('org.tensorflow:tensorflow-lite:0.0.0-nightly') { changing = true }\r\nimplementation('org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly') { changing = true }\r\n```\r\n? ", "Ok, I've tried switching to the \"changing=true\" syntax and rerunning, and after that I tried clearing the gradle cache (by removing ~/.gradle/caches) and I reran again.  In both cases, the segfault was still there.\r\n", "I see, thanks for checking. It's possible this is an emulator-specific issue. We'll take a look.", "Ok, thanks!  If there are any other checks I can do that would be helpful, I'm happy to give them a shot; just let me know.", "If you can attach the logcat preceding the seg fault, that might be useful.", "Ok, the logcat from the last run I did follows.\r\n\r\n2020-03-31 14:31:28.646 18654-18654/? I/tflitebugrepor: Not late-enabling -Xcheck:jni (already on)\r\n2020-03-31 14:31:28.659 18654-18654/? I/tflitebugrepor: Unquickening 13 vdex files!\r\n2020-03-31 14:31:28.660 18654-18654/? W/tflitebugrepor: Unexpected CPU variant for X86 using defaults: x86\r\n2020-03-31 14:31:28.894 18654-18654/com.example.tflitebugreport I/tflitebugrepor: The ClassLoaderContext is a special shared library.\r\n2020-03-31 14:31:28.959 18654-18675/com.example.tflitebugreport I/ResourceExtractor: Found extracted resources res_timestamp-1-1585678399005\r\n2020-03-31 14:31:29.007 18654-18678/com.example.tflitebugreport D/libEGL: loaded /vendor/lib/egl/libEGL_emulation.so\r\n2020-03-31 14:31:29.008 18654-18678/com.example.tflitebugreport D/libEGL: loaded /vendor/lib/egl/libGLESv1_CM_emulation.so\r\n2020-03-31 14:31:29.009 18654-18678/com.example.tflitebugreport D/libEGL: loaded /vendor/lib/egl/libGLESv2_emulation.so\r\n2020-03-31 14:31:29.035 18654-18654/com.example.tflitebugreport D/HostConnection: HostConnection::get() New Host Connection established 0xe9c48910, tid 18654\r\n2020-03-31 14:31:29.046 18654-18654/com.example.tflitebugreport D/HostConnection: HostComposition ext ANDROID_EMU_CHECKSUM_HELPER_v1 ANDROID_EMU_native_sync_v2 ANDROID_EMU_native_sync_v3 ANDROID_EMU_native_sync_v4 ANDROID_EMU_dma_v1 ANDROID_EMU_direct_mem ANDROID_EMU_host_composition_v1 ANDROID_EMU_host_composition_v2 ANDROID_EMU_vulkan ANDROID_EMU_deferred_vulkan_commands ANDROID_EMU_vulkan_null_optional_strings ANDROID_EMU_vulkan_create_resources_with_requirements ANDROID_EMU_YUV_Cache ANDROID_EMU_async_unmap_buffer ANDROID_EMU_vulkan_ignored_handles GL_OES_vertex_array_object GL_KHR_texture_compression_astc_ldr ANDROID_EMU_gles_max_version_2 \r\n2020-03-31 14:31:29.061 18654-18654/com.example.tflitebugreport D/EGL_emulation: eglCreateContext: 0xe9bd3280: maj 2 min 0 rcv 2\r\n2020-03-31 14:31:29.143 18654-18681/com.example.tflitebugreport D/HostConnection: HostConnection::get() New Host Connection established 0xe9c48c80, tid 18681\r\n2020-03-31 14:31:29.145 18654-18681/com.example.tflitebugreport D/HostConnection: HostComposition ext ANDROID_EMU_CHECKSUM_HELPER_v1 ANDROID_EMU_native_sync_v2 ANDROID_EMU_native_sync_v3 ANDROID_EMU_native_sync_v4 ANDROID_EMU_dma_v1 ANDROID_EMU_direct_mem ANDROID_EMU_host_composition_v1 ANDROID_EMU_host_composition_v2 ANDROID_EMU_vulkan ANDROID_EMU_deferred_vulkan_commands ANDROID_EMU_vulkan_null_optional_strings ANDROID_EMU_vulkan_create_resources_with_requirements ANDROID_EMU_YUV_Cache ANDROID_EMU_async_unmap_buffer ANDROID_EMU_vulkan_ignored_handles GL_OES_vertex_array_object GL_KHR_texture_compression_astc_ldr ANDROID_EMU_gles_max_version_2 \r\n2020-03-31 14:31:29.226 18654-18681/com.example.tflitebugreport D/EGL_emulation: eglMakeCurrent: 0xe9bd3280: ver 2 0 (tinfo 0xe7a8ea00)\r\n2020-03-31 14:31:29.328 18654-18687/com.example.tflitebugreport I/flutter: Observatory listening on http://127.0.0.1:34829/x57mhTHSbR4=/\r\n2020-03-31 14:31:29.448 18654-18654/com.example.tflitebugreport I/MainActivity: Made it here!\r\n2020-03-31 14:31:29.498 18654-18654/com.example.tflitebugreport W/native: cpu_feature_guard.cc:36 The TensorFlow library was compiled to use SSE instructions, but these aren't available on your machine.\r\n2020-03-31 14:31:29.498 18654-18654/com.example.tflitebugreport A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xfffffff4 in tid 18654 (tflitebugreport), pid 18654 (tflitebugreport)", "> 2020-03-31 14:31:29.498 18654-18654/com.example.tflitebugreport W/native: cpu_feature_guard.cc:36 The TensorFlow library was compiled to use SSE instructions, but these aren't available on your machine.\r\n\r\nThat looks potentially suspicious, though it doesn't immediately address the seg fault. It's very helpful though, we'll try to get back to you soon.", "Thanks!  I agree that the \"SSE instructions\" message does look suspicious.  \r\n\r\nOne more observation, in case it helps:  I notice that if I change the FlexDelegate to an NnApiDelegate, then I don't see a segfault, and I don't get the 'SSE instructions' message either.\r\n", "I can reproduce the problem in my side as well. Will take a look for finding a root cause.", "FYI, you can disable using SSE instructions by providing  --copt=\"-mno-sse4\" to the bazel build command.\r\n", "I met the same problem:\r\n\r\n```cpu_feature_guard.cc:36 The TensorFlow library was compiled to use SSE instructions, but these aren't available on your machine. \r\nA/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xfffffff4 in tid xxx (xxx), pid xxx (xxx)  \r\n```\r\n\r\n\r\nAfter build tensorflow without sse4.1 and sse4.2, it works!\r\nMy compiled tesroflow is available here https://github.com/fuzhenxin/Tensorflow-Lite-Select-TF-Ops-AAR", "Is there any possibility this issue could be solved natively in tensorflow-lite? I'm also experiencing it when running an Android emulator.", "Internal build script is updated.\r\nI've verified with today's org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly release.\r\nPlease let me know if you still have the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38025\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38025\">No</a>\n", "Looks like this works on my setup -- many thanks to all who helped!", "This also worked for me.\r\n\r\nIf you're not familiar with gradle like myself, because the dependency version numbers are the same for every nightly build, you need to refresh your dependencies to get the latest nightly release.\r\n\r\nClick Gradle (right-hand side of Android Studio), then Execute Gradle Task (the elephant icon), and enter `gradle build --refresh-dependencies`. This will re-download all your dependencies.", "Has this issue returned for anyone else? I've done the above process to re-download dependencies, but my app has started crashing with the same segfault errors again.\r\n\r\nbuild.gradle:\r\n```\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'\r\n```\r\n\r\nCrash:\r\n\r\n```\r\nW/native: cpu_feature_guard.cc:36 The TensorFlow library was compiled to use SSE instructions, but these aren't available on your machine.\r\nA/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xfffffff4 in tid 32685 (init-21), pid 32643 (gdp19.truevoice)\r\n```", "@terryheo could you take a look at https://github.com/tensorflow/tensorflow/issues/38025#issuecomment-740262718 ?", "I can verified the issue. Let me dig and prepare a fix.", "The change was merged a week ago. But using nightly is still crashing since there is an issue of updating JCenter.\r\nBut you can use nightly directly from the Cloud Storage.\r\nhttps://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/google3/ubuntu_16/lite/nightly/511/20201214-223707/tensorflow-lite-select-tf-ops.aar", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38025\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38025\">No</a>\n"]}, {"number": 38024, "title": "Unable to build micro_speech for Sparkfun Edge", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 44400dfcde6e39aca68c4bc103c2e4e15b5379c5\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to build the micro_speech example for Sparkfun Edge from the `master` branch, but getting a few compile errors.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nI am following the steps at:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech\r\n\r\nThe very first step,\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge TAGS=\"cmsis-nn\" micro_speech_bin`\r\nresults in the following compile errors and warnings:\r\n\r\n```\r\ntensorflow/lite/micro/kernels/cmsis-nn/softmax.cc: In function 'void tflite::ops::micro::activations::SoftmaxQuantized(const TfLiteTensor*, TfLiteTensor*, const tflite::SoftmaxParams&)':\r\ntensorflow/lite/micro/kernels/cmsis-nn/softmax.cc:97:30: error: 'input_shape' was not declared in this scope\r\n     const int trailing_dim = input_shape.DimensionsCount() - 1;\r\n                              ^~~~~~~~~~~\r\ntensorflow/lite/micro/kernels/cmsis-nn/softmax.cc:97:30: note: suggested alternative: 'initstate'\r\n     const int trailing_dim = input_shape.DimensionsCount() - 1;\r\n                              ^~~~~~~~~~~\r\n                              initstate\r\ntensorflow/lite/micro/kernels/cmsis-nn/softmax.cc:99:60: error: 'output_shape' was not declared in this scope\r\n         MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\r\n                                                            ^~~~~~~~~~~~\r\ntensorflow/lite/micro/kernels/cmsis-nn/softmax.cc:99:60: note: suggested alternative: 'outer_size'\r\n         MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\r\n                                                            ^~~~~~~~~~~~\r\n                                                            outer_size\r\n```\r\n\r\nAny help is super appreciated. Cheers!", "comments": ["@dmanning23 sorry for the late reply on this. Is this still an issue? I tried it on the latest baseline and it seem to work.\r\n\r\n1. `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge TAGS=\"cmsis-nn\" clean clean_downloads`\r\n2. `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge TAGS=\"cmsis-nn\" micro_speech_bin`", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38024\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38024\">No</a>\n"]}, {"number": 38023, "title": "tf.sparse.reorder() produces fully-unknown shaped outputs from partially-unknown shaped placeholder inputs", "body": "When reordering placeholder `SparseTensor`s with partially unknown shapes, the output shape is fully unknown. This prevents usage of the `tf.sparse.reorder()` function in keras models because an unknown batch size at model compile time prevents downstream layers from knowing their expected input shape.\r\n\r\n**System information** \r\n\r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): **Yes**\r\n\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): **Linux CentOS 7**\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: **N/A**\r\n\r\n- TensorFlow installed from (source or\r\nbinary): **Binary**\r\n\r\n- TensorFlow version (use command below):  **2.2.0-dev20200327**\r\n\r\n- Python version: **Python 3.7.6**\r\n\r\n- Bazel version (if compiling from source): **N/A**\r\n\r\n- GCC/Compiler version (if compiling from source):  **N/A**\r\n\r\n- CUDA/cuDNN version: - GPU model and memory: **Unused**\r\n\r\n**Describe the current behavior**\r\nWhen calling `tf.sparse.reorder()` on a placeholder SparseTensor with a partially\r\nunknown shape, the returned SparseTensor has a fully unknown shape\r\n\r\n**Describe the expected behavior**\r\nWhen calling `tf.sparse.reorder()` on a placeholder SparseTensor with a partially\r\nunknown shape, the returned SparseTensor has the same partially-known shape\r\nas the input tensor\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nplaceholder = tf.keras.backend.placeholder(shape=(None, 32), dtype=tf.float32, sparse=True)\r\nreordered = tf.sparse.reorder(sparse_placeholder)\r\n\r\nprint(placeholder.shape)\r\nprint(reordered.shape)\r\n\r\n> (None, 32)\r\n> (None, None)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.", "comments": ["@reesehyde-dst \r\nplease share a simple standalone code for us to replicate the issue.", "Hi @Saduf2019 is the code sample in the original post sufficient? Copied below:\r\n\r\n```python\r\nplaceholder = tf.keras.backend.placeholder(shape=(None, 32), dtype=tf.float32, sparse=True)\r\nreordered = tf.sparse.reorder(sparse_placeholder)\r\n\r\nprint(placeholder.shape)\r\nprint(reordered.shape)\r\n\r\n> (None, 32)\r\n> (None, None)\r\n```", "@reesehyde-dst \r\nReplicated code shared by you for analysis please find [gist](https://colab.sandbox.google.com/gist/Saduf2019/686b4428fd4f8d821768b2b5c60d9a60/38023.ipynb) of error faced, please share simple standalone code that can be used to replicate the issue faced.", "Sorry about that, the code sample was copied from a notebook and using the variable by a different name. Here it is fixed ([gist here](https://colab.research.google.com/gist/reesehyde-dst/3487cd382a7a4b199d117b38c74a8302/38023.ipynb)):\r\n\r\n```python\r\nplaceholder = tf.keras.backend.placeholder(shape=(None, 32), dtype=tf.float32, sparse=True)\r\nreordered = tf.sparse.reorder(placeholder)\r\n\r\nprint(placeholder.shape)\r\nprint(reordered.shape)\r\n\r\n> (None, 32)\r\n> (None, None)\r\n```", "Hi @Saduf2019 , does the sample above and gist demonstrate the issue?", "i have replicated the issue with shared gist, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/f68bd79eef90e8085b76c365b53fc766/untitled129.ipynb)", "@ghost, Sorry for late response. \r\n\r\nTo use ` tf.sparse.reorder()`  input must be `SparseTensor`. Please check the [gist here](https://colab.sandbox.google.com/gist/chunduriv/34a5fe5af0167dc552cbd193c06ad27e/untitled66.ipynb) has a example to reorders a `SparseTensor` into the canonical, row-major ordering. Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38023\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38023\">No</a>\n"]}, {"number": 38022, "title": "Failed to get convolution algorithm. (Checked existing solution, but not working)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No, using this example. \r\n- OS Platform and Distribution: Ubuntu 19.10\r\n- TensorFlow installed from (source or\r\nbinary): pip \r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n- GCC/Compiler version (if compiling from\r\nsource): 7.3.0\r\n- CUDA/cuDNN version: 10.1.243 and 7.6.4\r\n- GPU model and memory: RTX2060S 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTrainning failed\r\n\r\n**Describe the expected behavior**\r\nTrainning is processing. \r\n\r\n**Standalone code to reproduce the issue** \r\nhttps://www.tensorflow.org/tutorials/images/cnn\r\njupyte notebook download locally\r\n\r\n\r\n**Other info / logs** \r\nWARNING:tensorflow:From <ipython-input-19-01c6f78f4d4f>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use Model.fit, which supports generators.\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nTrain for 500 steps, validate for 250 steps\r\nEpoch 1/15\r\n  1/500 [..............................] - ETA: 9:01\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-19-01c6f78f4d4f> in <module>\r\n      4     epochs=epochs,\r\n      5     validation_data=val_data_gen,\r\n----> 6     validation_steps=total_val // batch_size\r\n      7 )\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1304         use_multiprocessing=use_multiprocessing,\r\n   1305         shuffle=shuffle,\r\n-> 1306         initial_epoch=initial_epoch)\r\n   1307 \r\n   1308   @deprecation.deprecated(\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    630         # Lifting succeeded, so variables are initialized and we can run the\r\n    631         # stateless function.\r\n--> 632         return self._stateless_fn(*args, **kwds)\r\n    633     else:\r\n    634       canon_args, canon_kwds = \\\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n   2365   @property\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~/.conda/envs/fcgf/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-19-01c6f78f4d4f>:6) ]] [Op:__inference_distributed_function_1027]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n", "comments": ["@SwagJ \r\n\r\nI have tried in colab with TF 2.1.0 and i am not seeing any issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/aaef581f4b793f373160dfe3f8446930/untitled751.ipynb) .Thanks!", "Colab Works fine. It's not working locally with my PC setup. I setup locally with the correct version match of cuda and cudnn. Why it is not working? ", "@SwagJ \r\n\r\nYou may try limiting your gpu memory usage with set_memory_growth option.\r\nSee https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\r\nPut the following snippet on top of your code and execute again\r\n\r\n```\r\n import tensorflow as tf\r\n gpus= tf.config.experimental.list_physical_devices('GPU')\r\n tf.config.experimental.set_memory_growth(gpus[0], True)\r\n```\r\nThanks!", "Thank you very much. Problem Solved. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38022\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38022\">No</a>\n"]}, {"number": 38021, "title": "Problem with can bus interface on TM4C123GH6PM", "body": "I tried to make an interface for can bus on TM4C123GH6PM, i used loop back test, i sent data and make a test on these data using loop back test mode to make sure that data is sent successfully by turning on a led connected on PORT F but nothing happened, I enabled interrupts and created a function called ( CAN0_Handler ) this is my ISR, when i received data an interrupt occurred and the program go to ISR and turn on a led depending on the receiving data but nothing happened and i do not know where the problem is, This is my code.\r\n\r\n\r\n#include <stdbool.h>\r\n#include <stdint.h>\r\n\r\n#define RCGC2_R *((volatile unsigned int*) 0x400FE108)\r\n#define GPIODEN_R *((volatile unsigned int*) 0x4002451C)\r\n#define GPIODIR_R *((volatile unsigned int*) 0x40024400)\r\n#define GPIOAFSEL_R *((volatile unsigned int*) 0x40024420)\r\n#define GPIOPCTL_R *((volatile unsigned int*) 0x4002452C)\r\n#define RCGC0_R *((volatile unsigned int*) 0x400FE100)\r\n#define CANCTL_R *((volatile unsigned int*) 0x40040000)\r\n#define CANIF1CRQ_R *((volatile unsigned int*) 0x40040020)\r\n#define CANIF1CMSK_R *((volatile unsigned int*) 0x40040024)\r\n#define CANIF1ARB1_R *((volatile unsigned int*) 0x40040030)\r\n#define CANIF1ARB2_R *((volatile unsigned int*) 0x40040034)\r\n#define CANIF1MCTL_R *((volatile unsigned int*) 0x40040038)\r\n#define CANBIT_R *((volatile unsigned int*) 0x4004000C)\r\n#define CANBRPE_R *((volatile unsigned int*) 0x40040018)\r\n#define CANSTS_R *((volatile unsigned int*) 0x40040004)\r\n#define CANIF1DA1_R *((volatile unsigned int*) 0x4004003C)\r\n#define CANIF1DA2_R *((volatile unsigned int*) 0x40040040)\r\n#define CANIF1DB1_R *((volatile unsigned int*) 0x40040044)\r\n#define CANIF1DB2_R *((volatile unsigned int*) 0x40040048)\r\n#define CANTST_R *((volatile unsigned int*) 0x40040014)\r\n#define CANIF1MSK1_R *((volatile unsigned int*) 0x40040028)\r\n#define CANIF1MSK2_R *((volatile unsigned int*) 0x4004002C)\r\n#define NVIC_EN1_R *((volatile unsigned int*) 0xE000E104)\r\n#define GPIODEN_PORTF_R *((volatile unsigned int*) 0x4002551C)\r\n#define GPIODIR_PORTF_R *((volatile unsigned int*) 0x40025400)\r\n#define GPIODR2R_PORTF_R *((volatile unsigned int*) 0x40025500)\r\n#define GPIODR4R_PORTF_R *((volatile unsigned int*) 0x40025504)\r\n#define GPIODR8R_PORTF_R *((volatile unsigned int*) 0x40025508)\r\n#define GPIODATA_PORTF_R *((volatile unsigned int*) 0x40025008)\r\n\r\nchar Can_data_buffer[8];\r\n\r\n\r\ntypedef struct\r\n{\r\n    unsigned long MsgID ;       // CAN message ID , 11 or 29 bit\r\n    unsigned long MsgIDMask ;   // CAN message ID mask\r\n    unsigned long MsgLen ;      // CAN message data length field\r\n    unsigned long MsgFlags ;    // CAN message control and status\r\n    const char *MsgData ;    // CAN message data\r\n\r\n} CAN_Msg_Object ;\r\n\r\n\r\n\r\nvoid CAN0_init_mode();\r\nvoid CAN_Config_Tx_Message ( CAN_Msg_Object can_msg );\r\nvoid CAN_Config_Tx_Message_test_mode ( CAN_Msg_Object can_msg );\r\nvoid Delay ( unsigned long counter );\r\nvoid CAN0_Handler(void);\r\n\r\n\r\nint main(void)\r\n{\r\n    RCGC2_R |= 0x30;                                                    //Enable clock to port E & F\r\n    GPIODEN_R |= 0x30;                                                  //enable digital function to PE4 , PE5\r\n    GPIODIR_R |= 0x20;                                                  //PE4 as input RX & PE5 as output TX\r\n    GPIOAFSEL_R |= 0x30;                                                //\r\n    GPIOPCTL_R |= 0x880000;                                             //enable CAN0 TX , RX on PE5 , PE4\r\n    RCGC0_R |= 0x1000000;                                               //enable clock to CAN0\r\n    Delay ( 3 );\r\n\r\n    GPIODEN_PORTF_R |= 0x02;                                            //\r\n    GPIODIR_PORTF_R |=0xFF;                                             //\r\n    GPIODR2R_PORTF_R = 0;\r\n    GPIODR4R_PORTF_R = 0;\r\n    GPIODR8R_PORTF_R = 0xFF;\r\n\r\n\r\n    CAN0_init_mode();\r\n\r\n    CAN_Msg_Object id;\r\n\r\n    id.MsgID = 0x011;\r\n    id.MsgLen = 0x8;\r\n    id.MsgData = \"abcdefgh\";\r\n\r\n    CAN_Config_Tx_Message_test_mode ( id );\r\n\r\n    /*if(Can_data_buffer[0]=='a')\r\n    {\r\n        GPIODATA_PORTF_R = 2;\r\n    }*/\r\n\r\n\r\n    return 0;\r\n}\r\n\r\n\r\nvoid CAN0_init_mode()\r\n{\r\n    unsigned char msg_num;\r\n    CANCTL_R |= 0x1;                                                     //enable initialization mode on CAN0\r\n    while( CANIF1CRQ_R & 0x8000 );                                       //wait for busy bit to clear\r\n    CANIF1CMSK_R |= 0xB0;                                                //\r\n    CANIF1ARB1_R = 0;                                                    //\r\n    CANIF1ARB2_R = 0;                                                    //\r\n    CANIF1MCTL_R = 0;                                                    //\r\n\r\n    // Configure all messages objects in message RAM as invalid\r\n    for(msg_num=1; msg_num<=32; msg_num++)\r\n    {\r\n        while( CANIF1CRQ_R & 0x8000 );                                   //wait for busy bit to clear\r\n        CANIF1CRQ_R = msg_num;\r\n\r\n    }\r\n\r\n    CANIF1CMSK_R |= 0x0C;                                                // Make sure that the interrupt and new data flags are updated\r\n\r\n    for(msg_num=1; msg_num<=32; msg_num++)\r\n    {\r\n        while( CANIF1CRQ_R & 0x8000 );                                   //wait for busy bit to clear\r\n        CANIF1CRQ_R = msg_num;\r\n\r\n    }\r\n\r\n    CANCTL_R |= 0x40;                                                   //Write accesses to the CANBIT register are allowed\r\n\r\n    CANBIT_R = (0x0002<<12) | (0x0003<<8) | (0x0001<<6) | 0x3;          //setting bit timing make baud rate 500kbps and sampling point at 60%\r\n    CANBRPE_R = 0;                                                      //\r\n    CANCTL_R |= 0x0E;                                                   //Enable CAN 0 interrupts for error , status and CAN controller\r\n    NVIC_EN1_R |= 0x80;                                                 //enable global interrupt for CAN0\r\n\r\n    CANCTL_R &= ~(0x41);                                                //disable initialization mode\r\n\r\n}\r\n\r\n\r\n//Normal mode\r\nvoid CAN_Config_Tx_Message ( CAN_Msg_Object can_msg )\r\n{\r\n    CANIF1CMSK_R |= 0xF3;                                               //\r\n    CANIF1ARB1_R = 0;                                                   //\r\n    CANIF1ARB2_R = 0xA000 | (can_msg.MsgID << 2);                       //\r\n    CANIF1MCTL_R = (0x180 | can_msg.MsgLen);                            //\r\n\r\n    CANIF1DA1_R = can_msg.MsgData[0] + (can_msg.MsgData[1] << 8);       //\r\n    CANIF1DA2_R = can_msg.MsgData[2] + (can_msg.MsgData[3] << 8);       //\r\n    CANIF1DB1_R = can_msg.MsgData[4] + (can_msg.MsgData[5] << 8);       //\r\n    CANIF1DB2_R = can_msg.MsgData[6] + (can_msg.MsgData[7] << 8);       //\r\n\r\n    CANIF1CRQ_R = 2;                                                    //Initiate programming message object to second message buffer\r\n    while( CANIF1CRQ_R & 0x8000 );                                      //wait for busy bit to clear\r\n\r\n}\r\n\r\n\r\n//loop back test mode\r\nvoid CAN_Config_Tx_Message_test_mode ( CAN_Msg_Object can_msg )\r\n{\r\n    CANCTL_R |= 0x80;                                                   //test mode\r\n    CANTST_R |= 0x10;                                                   //enable loop back test\r\n    CANIF1CMSK_R |= 0xF3;                                               //\r\n    CANIF1MSK1_R = 0;                                                   //\r\n    CANIF1MSK2_R = 0;                                                   //\r\n    CANIF1ARB1_R = 0;                                                   //\r\n    CANIF1ARB2_R = 0xA000 | (can_msg.MsgID << 2);                       //\r\n    CANIF1MCTL_R = (0x180 | can_msg.MsgLen);                            //\r\n\r\n    CANIF1DA1_R = can_msg.MsgData[0] + (can_msg.MsgData[1] << 8);       //\r\n    CANIF1DA2_R = can_msg.MsgData[2] + (can_msg.MsgData[3] << 8);       //\r\n    CANIF1DB1_R = can_msg.MsgData[4] + (can_msg.MsgData[5] << 8);       //\r\n    CANIF1DB2_R = can_msg.MsgData[6] + (can_msg.MsgData[7] << 8);       //\r\n\r\n    CANIF1CRQ_R = 2;                                                    //Initiate programming message object to second message buffer\r\n    while( CANIF1CRQ_R & 0x8000 );                                      //wait for busy bit to clear\r\n\r\n}\r\n\r\n\r\nvoid Delay ( unsigned long counter )\r\n{\r\n    unsigned long i = 0;\r\n    for (i =0; i< counter ; i ++) ;\r\n}\r\n\r\n\r\n\r\n//CAN interrupt handler\r\nvoid CAN0_Handler(void)\r\n{\r\n    unsigned long status;\r\n    status = CANSTS_R;                                                  //Clear interrupt register\r\n\r\n    if(status & 0x10)\r\n    {\r\n        CANIF1MCTL_R |= 0x088;                                          //\r\n        CANIF1CRQ_R = 1;                                                //\r\n        while( CANIF1CRQ_R & 0x8000 );                                  //wait for busy bit to clear\r\n\r\n        Can_data_buffer[0] = (char)CANIF1DA1_R;                         //\r\n        Can_data_buffer[1] = (char)(CANIF1DA1_R >> 8);                  //\r\n        Can_data_buffer[2] = (char)CANIF1DA2_R;                         //\r\n        Can_data_buffer[3] = (char)(CANIF1DA2_R >> 8);                  //\r\n        Can_data_buffer[4] = (char)CANIF1DB1_R;                         //\r\n        Can_data_buffer[5] = (char)(CANIF1DB1_R >> 8);                  //\r\n        Can_data_buffer[6] = (char)CANIF1DB2_R;                         //\r\n        Can_data_buffer[7] = (char)(CANIF1DB2_R >> 8);                  //\r\n\r\n    }\r\n\r\n    if(Can_data_buffer[0]=='a')\r\n    {\r\n        GPIODATA_PORTF_R = 2;\r\n    }\r\n}\r\n", "comments": ["Not a TF issue"]}, {"number": 38020, "title": "Fix document error for ApplyFtrl, ApplyFtrlV2 and SparseApplyFtrlV2", "body": "The implementation of Ftrl optimizer has been updated, but the internal doc is incorrect.", "comments": ["@blueyi, @mihaimaruseac Any update on this PR? Please. Thanks!"]}, {"number": 38019, "title": "Correct formula of ApplyFtrlV2", "body": "Fix documentation of SparseApplyFtrlV2", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38019) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38019) for more info**.\n\n<!-- ok -->"]}, {"number": 38018, "title": "tf.estimator.add_metrics ends in Shapes (None, 12) and (None,) are incompatible", "body": "**System information** \r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit\r\nTensorFlow installed from (source or binary): PyCharm\r\nTensorFlow version (use command below): 2.0.0\r\nPython version: 3.7.6\r\n\r\n**Describe the current behavior**\r\nI am using a DNNClassifier as my estimator and wanted to add some additional metrics to the estimator. the code I am using is basically the one from the tf.estimator.add_metrics documentation \r\n[https://www.tensorflow.org/api_docs/python/tf/estimator/add_metrics](url).\r\nThe model works fine without the add_metrics statement. But runs into an ValueError: \"Shapes (None, 12) and (None,) are incompatible\" when including it. The error occures in the line:\r\n\r\n`auc_metric.update_state(y_true=labels, y_pred=predictions['logits'])`\r\n\r\nThe line is called by `est.evaluate(validation_data)`.\r\n\r\nIt is not clear to me why this happens, but it seems like the y_true parameter is not filled correctly. Hence, the label column is not processed correctly to the function. This seems strange since the model works correctly without the additional metric. The training and validation data is created by the following function:\r\n\r\n```\r\ndef get_dataset_from_tensor_slices(data_input, label_column, n_epochs=None, shuffle=True):\r\n    def get_dataset():\r\n        dataset = tf.data.Dataset.from_tensor_slices((dict(data_input), label_column))\r\n        if shuffle:\r\n            dataset = dataset.shuffle(len(label_column))\r\n\r\n        # For training, cycle through dataset as many times as need (n_epochs=None).\r\n        dataset = dataset.repeat(n_epochs)\r\n        # In memory training doesn't use batching.\r\n        dataset = dataset.batch(len(label_column))\r\n        return dataset\r\n    return get_dataset\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should be able to add an additional metric to the estimator.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\ndef my_auc(labels, predictions):\r\n    auc_metric = tf.keras.metrics.AUC(name=\"my_auc\")\r\n    auc_metric.update_state(y_true=labels, y_pred=predictions['logits'])\r\n    return {'auc': auc_metric}\r\n\r\n\r\ndef model_evaluation(features, training_data, validation_data, labels, validation_label_column):\r\n    hidden_layers = len(training_data.__call__().element_spec[0])\r\n    final_layer = len(labels)\r\n    est = tf.estimator.DNNClassifier(feature_columns=features,\r\n                                     hidden_units=[hidden_layers, (hidden_layers / 2), (hidden_layers / 4),\r\n                                                   final_layer],\r\n                                     n_classes=final_layer, label_vocabulary=labels)\r\n    est = tf.estimator.add_metrics(est, my_auc)\r\n\r\n    est.train(training_data, max_steps=100)\r\n\r\n    result = est.evaluate(validation_data)\r\n\r\n```\r\n**Other info / logs** \r\nAs far as I debugged it, the problem goes back to the fact that the labels created from the get_dataset_from_tensor_slices method have the shape (None,). --> Thats maybe the problem...how can I fix this?\r\nWhereas the predictions are generated in shape (None, 12) (where 12 is the number of possible labels).\r\n\r\n\r\nDoes anybody know why this happens? Any help is appreciated!\r\n", "comments": ["@MaUt89, Thanks for reporting this issue, Can you provide the complete code to replicate the reported issue Thanks!", "Please find in the following the complete code. The data I'm using consist exclusively of String features so I have only categorical columns. Thank you for your help.\r\n\r\n**main():**\r\n```\r\nfrom data_preprocessing import data_preprocessing\r\nfrom model_evaluation import model_evaluation\r\n\r\nlabel_column = '123'\r\nlabels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l']\r\n\r\nfeatures, training_data, validation_data, validation_label_column = data_preprocessing(label_column)\r\nmodel_evaluation(features, training_data, validation_data, labels, validation_label_column)\r\n```\r\n\r\n**data_preprocessing():**\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nfrom operator import is_not\r\nfrom functools import partial\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport os.path\r\n\r\n\r\ndef data_preprocessing(label_column):\r\n    print(tf.__version__)\r\n\r\n    train_file_path = 'Learning Data Input/TrainingDataFromPandas.csv'\r\n    validation_file_path = 'Learning Data Input/ValidationDataFromPandas.csv'\r\n\r\n    pandas_data = pd.read_csv(train_file_path, delimiter=';', dtype='string')\r\n    train_label_column = pandas_data.pop(label_column)\r\n    training_data = get_dataset_from_tensor_slices(pandas_data, train_label_column)\r\n\r\n    pandas_data = pd.read_csv(validation_file_path, delimiter=';', dtype='string')\r\n    validation_label_column = pandas_data.pop(label_column)\r\n    validation_data = get_dataset_from_tensor_slices(pandas_data, validation_label_column, shuffle=False, n_epochs=1)\r\n\r\n    show_batch(training_data.__call__())\r\n    show_batch(validation_data.__call__())\r\n\r\n    categorical_columns = get_categorical_feature_columns(training_data.__call__())\r\n\r\n    example_batch, labels_batch = next(iter(training_data.__call__()))\r\n\r\n    categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\r\n    print(categorical_layer(example_batch).numpy()[0])\r\n\r\n    return categorical_columns, training_data, validation_data, validation_label_column\r\n\r\ndef get_dataset_from_tensor_slices(data_input, label_column, n_epochs=None, shuffle=True):\r\n    def get_dataset():\r\n        dataset = tf.data.Dataset.from_tensor_slices((dict(data_input), label_column))\r\n        if shuffle:\r\n            dataset = dataset.shuffle(len(label_column))\r\n\r\n        # For training, cycle through dataset as many times as need (n_epochs=None).\r\n        dataset = dataset.repeat(n_epochs)\r\n        # In memory training doesn't use batching.\r\n        dataset = dataset.batch(round(len(label_column)/10))\r\n        return dataset\r\n    return get_dataset\r\n\r\n\r\ndef show_batch(dataset):\r\n    for batch, label in dataset.take(1):\r\n        for key, value in batch.items():\r\n            print(\"{:20s}: {}\".format(key, value.numpy()))\r\n    return\r\n\r\n\r\ndef get_categorical_feature_columns(data):\r\n    categorical_columns = []\r\n    for batch, label in data.take(1):\r\n        for key, value in batch.items():\r\n            # create list of only unique values per key\r\n            unique_values = []\r\n            for unique_value in value:\r\n                # remove eager tensor added characters\r\n                if value.dtype == 'string':\r\n                    unique_value = format(unique_value)[2:-1]\r\n                else:\r\n                    unique_value = format(unique_value)\r\n\r\n                if len(unique_values) == 0:\r\n                    unique_values.append(unique_value)\r\n                elif unique_value not in unique_values:\r\n                    unique_values.append(unique_value)\r\n                else:\r\n                    continue\r\n\r\n            unique_values = list(filter(partial(is_not, None), unique_values))\r\n\r\n            cat_col = tf.feature_column.categorical_column_with_vocabulary_list(key=key,\r\n                                                                                vocabulary_list=unique_values)\r\n            categorical_columns.append(tf.feature_column.indicator_column(cat_col))\r\n    return categorical_columns\r\n```\r\n\r\n**model_evaluation():**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport pandas as pd\r\n\r\n\r\ndef my_auc(labels, predictions):\r\n    auc_metric = tf.keras.metrics.AUC(name=\"my_auc\")\r\n    auc_metric.update_state(y_true=labels, y_pred=predictions['logits'])\r\n    return {'auc': auc_metric}\r\n\r\n\r\ndef model_evaluation(features, training_data, validation_data, labels, validation_label_column):\r\n    hidden_layers = len(training_data.__call__().element_spec[0])\r\n    final_layer = len(labels)\r\n    est = tf.estimator.DNNClassifier(feature_columns=features, hidden_units=[hidden_layers, hidden_layers, hidden_layers], n_classes=final_layer, label_vocabulary=labels)\r\n    est = tf.estimator.add_metrics(est, my_auc)\r\n\r\n    # Training\r\n    est.train(training_data, max_steps=1000)\r\n\r\n    # Validation\r\n    result = est.evaluate(validation_data)\r\n\r\n    # Predicition\r\n    prediction = est.predict(validation_data)\r\n\r\n    # Results\r\n    print(pd.Series(result))\r\n\r\n    for pred_dict, expec in zip(prediction, validation_label_column):\r\n        class_id = pred_dict['class_ids'][0]\r\n        probability = pred_dict['probabilities'][class_id]\r\n\r\n        print('Prediction is \"{}\" ({:.1f}%), expected \"{}\"'.format(\r\n            labels[class_id], 100 * probability, expec))\r\n\r\n    pred_list = list(est.predict(validation_data))\r\n    probs = pd.Series([max(pred['probabilities']) for pred in pred_list])\r\n    probs.plot(kind='hist', bins=100, title='predicted probabilities')\r\n\r\n    return\r\n\r\n```", "@MaUt89, Can you share the input data csv files. Thanks! ", "Attached you can find test and validation files. They are really small and only show the structure of the learning input I am using. But the program behaves exactly as described above with these two files.\r\n\r\n[InputData.zip](https://github.com/tensorflow/tensorflow/files/4427439/InputData.zip)\r\n", "Was able to reproduce the issue with tf2.0.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/767cb65f1b8267b8c5023e393763da5f/untitled492.ipynb). Thanks!", "Hey,\r\nI found the issue...obviousley it is not possible to create a AUC curve for DNNClassifiers with more than two possible outputs. Usually you have:\r\n\r\nPrediction -> Expected -> Result\r\n0 -> 0 -> TN\r\n0 -> 1 -> FN\r\n1 -> 1 -> TP\r\n1 -> 0 -> FP\r\n\r\nIf you have more than two possible outcomes the predicted result is either positive or negative...which means that the AUC curve, which relies on the TN, FN, TP, FP input, cannot be calucluated. Please close this issue and thanks for your help up to that point.", "as user confirms the issue is resolved moving this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38018\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38018\">No</a>\n"]}, {"number": 38017, "title": "Broken colab button in nn_from_scratch.ipnyb", "body": "# issue\r\nThe colab button in https://github.com/tensorflow/examples/blob/master/community/en/nn_from_scratch.ipynb is Broken.\r\n# Pull Request\r\nI've sent the PR regarding this issue.", "comments": []}, {"number": 38016, "title": "[INTEL MKL] Fusing BN and Relu in mkl path", "body": "This PR supports BN and relu fusion for MKL in grappler and then rewrites it into mkl op.", "comments": ["@penpornk Any update on this PR?", "@ShengYang1 Can you please resolve conflicts? Thanks!", "@penpornk Thanks for your review. Already made some changes."]}, {"number": 38015, "title": "Adding correct version of tensorflow in udacity deep learning course", "body": "# issue\r\nThis https://github.com/tensorflow/examples/tree/master/courses/udacity_deep_learning repo\r\ncontains Udacity deep learning course having codes of tensorflow 1.x  . But now colab is having tensorflow 2.x by default.\r\n# Pull Request\r\nHey, @MarkDaoust will you please tell me the right tensorflow version so that I'll update all the notebooks. ", "comments": ["All of these should work in TensorFlow 2. If anything specific doesn't let us know and we'll help get it working."]}]