[{"number": 27835, "title": "[TF 2.0 docs] Add description for activation functions", "body": "Add descriptions to softplus, softsign and tanh. I am not a native English speaker, so maybe there are some spelling errors.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27835) for more info**.\n\n<!-- need_author_consent -->", "We've signed the CLA's. @rthadur, could you set CLA's to yes and review please\r\n\r\nThanks..", "Thanks, @kyscg for the improvements! ;)", "@rthadur, gentle ping to review and set CLA's to yes", "@aweers thank you for your contribution , please make sure all the contributors have signed CLA", "@rthadur @kyscg and I signed the cla and we are the only contributors ;)", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27835) for more info**.\n\n<!-- cla_yes -->", "@rthadur @akshaym Gentle ping to review", "Hi @fchollet, thanks for your suggestions! I hope now everything matches the style conventions. ;)", "Can one of the admins verify this patch?", "@aweers can you please resolve conflicts ?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 27834, "title": "Add support for line breaks in jupyter", "body": "jupyter cells could have a single line magic, but that splits a line with `\\`. \r\nThis PR adds support for such cases.\r\n\r\nCloses https://github.com/lc0/tf2up/issues/31", "comments": ["cc @martinwicke ", "I still don't understand the logic behind commenting out the continued lines.\r\n\r\nIs that encoding ever removed?", "@martinwicke right, it's removed once I update from updated code back to notebook. Just some lines below at https://github.com/tensorflow/tensorflow/blob/850d3855648f279570667b2cf55e30da4be7caae/tensorflow/tools/compatibility/ipynb.py#L167", "Got it. Test cases would be nice, maybe just a test notebook that has cells containing stuff like this that you can run this on to make sure it works.", "@lc0 can you please check build failures.", "@rthadur I am not sure it's related to my changes\r\n\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/compiler/xla/BUILD:887:1: name 'cc_header_only_library' is not defined\r\nERROR: in target '//tensorflow/compiler/xla/service:friends', no such label '//tensorflow/compiler/xla:friends': Target '//tensorflow/compiler/xla:friends' contains an error and its package is in error\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/compiler/xla/service/BUILD:30:1: Target '//tensorflow/compiler/xla:xla_data_proto_py_genproto' contains an error and its package is in error and referenced by '//tensorflow/compiler/xla/service:hlo_proto_py_genproto'\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/core/protobuf/tpu/BUILD:61:1: Target '//tensorflow/compiler/xla:xla_data_proto_py_genproto' contains an error and its package is in error and referenced by '//tensorflow/core/protobuf/tpu:compilation_result_proto_py_genproto'\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/core/protobuf/tpu/BUILD:61:1: Target '//tensorflow/compiler/xla:xla_data_proto_py' contains an error and its package is in error and referenced by '//tensorflow/core/protobuf/tpu:compilation_result_proto_py'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\n```", "seems like there are some issues with pilling xla as dependencies, must be coming from a different place cc @rthadur ", "Would you mind to try to re-run the tests? XLA fails have nothing to do with my changes."]}, {"number": 27833, "title": "Problem building libtensorflow.so on Debian system with Bazel", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Stretch (9.8, latest)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.13.1\r\n- Python version: Python 2.7.13\r\n- Installed using virtualenv? pip? conda?: Official Debian repository\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\n- GPU model and memory: Intel HD 4000\r\n\r\nI am trying to compile Tensorflow release 1.13 from source in order to obtain the shared library libtensorflow.so using bazel. I do this by invoking\r\n`bazel build //tensorflow:libtensorflow.so`\r\n\r\nAfter some time the build stops with the following error message\r\n`ERROR: ~/tensorflow/tensorflow_r1.13.1/tensorflow/core/kernels/BUILD:4312:1: C++ compilation of rule '//tensorflow/core/kernels:sparse_reduce_op' failed (Exit 1)\r\nIn file included from tensorflow/core/kernels/sparse_reduce_op.cc:25:0:\r\n./tensorflow/core/util/sparse/sparse_tensor.h: In static member function 'static tensorflow::Status tensorflow::sparse::SparseTensor::Create(tensorflow::Tensor, tensorflow::Tensor, tensorflow::sparse::SparseTensor::VarDimArray, tensorflow::sparse::SparseTensor::VarDimArray, tensorflow::sparse::SparseTensor*)':\r\n./tensorflow/core/util/sparse/sparse_tensor.h:68:22: warning: 'dims' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if (order.size() != dims) {\r\n         ~~~~~~~~~~~~~^~~~~~~\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:124:0,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/numeric_types.h:20,\r\n                 from ./tensorflow/core/framework/allocator.h:23,\r\n                 from ./tensorflow/core/framework/op_kernel.h:23,\r\n                 from tensorflow/core/kernels/sparse_reduce_op.cc:20:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In static member function 'static void std::_Function_handler<void(_ArgTypes ...), _Functor>::_M_invoke(const std::_Any_data&, _ArgTypes&& ...) [with _Functor = Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable, Tileable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<std::complex<float>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer> >; bool Vectorizable = true; bool Tileable = false]::<lambda(Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<std::complex<float>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex, Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<std::complex<float>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex)>; _ArgTypes = {long int, long int}]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801:9: internal compiler error: in emit_move_insn, at expr.c:3547\r\n         values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,\r\n         ^~~~~~\r\n`\r\n", "comments": ["Can you please follow these steps to [install TF from sources](https://www.tensorflow.org/install/source#setup_for_linux_and_macos). Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Exact same problem here. Also on debian same error. Tried different versions of bazel, different versions of tensorflow (2.0, 1.14) but always getting compile error.\r\n\r\nUsing the following command for building:\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" -k //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\nI followed closely the build instructions.\r\n", "I have the same error\r\nDebian stretch\r\nGCC 6\r\npython3.6\r\n\r\n![Imgur](https://i.imgur.com/054jt6o.png)", "Ubuntu 18.04.2 ,TF 1.13\r\nbazel 0.19.2\r\ngcc 7.4 & gcc 8.3 meet same this error \r\n\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralBlockPanelKernel.h:1879:3:  internal compiler error: in emit_move_insn, at expr.c:3723 } ^\r\n\r\n\r\n\r\n", "Custom Linux, TF 1.13\r\nbazel 0.19.2\r\ngcc 6.2.1\r\naarch64\r\n\r\nshows the exact same error. ", "Same story here. \r\nGCC 8.2.0 \r\nTF 2.2.0 \r\nbazel 3.0.0\r\n64bit cpu"]}, {"number": 27832, "title": "Converting unsupported operations -  Enter, Exit, Merge, Switch", "body": "**System information**\r\n- OS Platform and Distribution : Ubuntu 16.04.3\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.0 alpha\r\n\r\nI am using bazel to convert .pb file to .tflite. This is the command that I used to convert the model. \r\n\r\nabdullah@abdullah-OptiPlex-7060:~/tensorflow/tensorflow/lite/toco$ bazel run . --define=with_select_tf_ops=true -- --output_file=/home/abdullah/Documents/CRNN_MODEL/bazelled.tflite   --input_file=/home/abdullah/Documents/CRNN_MODEL/my_model__.pb   --input_arrays=the_input   --output_arrays=softmax/truediv \r\n\r\nI receive the following output: \r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FULLY_CONNECTED, LESS, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, RANGE, REDUCE_MAX, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, SUB, SUM, TANH, TILE, TRANSPOSE, UNPACK, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3\r\n```\r\nWhen I added the  --allow_custom_ops flag to the same command above, I received the following output:\r\n``` \r\nE tensorflow/lite/toco/toco_tooling.cc:456] TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch.\r\n```\r\n\r\nIt is possible to avoid using those control flow ops (Enter, Exit, Merge, Switch) in my original .pb model? \r\nThanks for your help.\r\n", "comments": ["Duplicate #27241"]}, {"number": 27830, "title": "tf-nightly-gpu 1.14.1 failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error", "body": "My Python version is 3.7.1, the operating system is Windows 10, the CUDA version is cuda_10.0.130_411.31_win10, the cudnn version is cudnn-10.0-windows 10-x64-v7.4.2.24, the GPU is Intel (R) UHD Graphics 620, and the tf-nightly-gpu-1.14.1.20190323 is installed.\r\nWhen I run `import tensorflow as tf`, there is no problem.\r\nWhen I run:\r\n`m1 = tf.constant([[3, 3]])`\r\n`m2 = tf.constant([[2], [2]])`\r\n`p = tf.matmul(m1, m2)`\r\n`sess =  tf.Session()`\r\n`r = sess.run(p)`\r\nthere is no variable \"r\" in my variable explorer.\r\nWhen I run:\r\n`print(p)`\r\nI get this:\r\n![QQ\u56fe\u724720190414154532 (2)](https://user-images.githubusercontent.com/38285222/56089888-83422400-5ecc-11e9-9d7a-5ff8bee54cf4.jpg)\r\nWhat should I do? Thanks!", "comments": ["I know the answer.", "@wmylxmj Please share the answer so that others can benefit from it. I'm running into the same error.", "@wmylxmj I have the same problem, and I am not smart enough to solve it. Can you please share what you did to make it work? ", "@bine-86-04 This usually happens if you are trying to run tensorflow-gpu with an nvidia GPU. It is possible that you don't have CUDA installed on your computer. It is also possible that the tensorflow binary you downloaded was compiled with a different version of CUDA than you have installed."]}, {"number": 27829, "title": "Cannot create a stateful RNN with recurrent dropout", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION=2.0.0-dev20190413\r\ntf.version.GIT_VERSION=v1.12.0-12481-gc7ce6f4cd9\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nI get an exception when trying to use `recurrent_dropout` in a stateful RNN:\r\n\r\n```\r\n.../tensorflow/python/ops/resource_variable_ops.py in __imul__(self, unused_other)\r\n   1449\r\n   1450   def __imul__(self, unused_other):\r\n-> 1451     raise RuntimeError(\"Variable *= value not supported. Use \"\r\n   1452                        \"`var.assign(var * value)` to modify the variable or \"\r\n   1453                        \"`var = var * value` to get a new Tensor object.\")\r\n\r\nRuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object.\r\n```\r\n\r\nThe full stacktrace is below.\r\n\r\n**Describe the expected behavior**\r\nNo exception.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nfrom tensorflow import keras\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.GRU(128, return_sequences=True, stateful=True,\r\n                     batch_input_shape=[32, None, 5],\r\n                     recurrent_dropout=0.2)\r\n])\r\n```\r\n\r\n**Other info / logs**\r\nComplete stacktrace:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-1-3e98e7412ec2> in <module>\r\n      4     keras.layers.GRU(128, return_sequences=True, stateful=True,\r\n      5                      batch_input_shape=[32, None, 5],\r\n----> 6                      recurrent_dropout=0.2)\r\n      7 ])\r\n\r\n.../tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    456     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    457     try:\r\n--> 458       result = method(self, *args, **kwargs)\r\n    459     finally:\r\n    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n.../tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)\r\n    106     if layers:\r\n    107       for layer in layers:\r\n--> 108         self.add(layer)\r\n    109\r\n    110   @property\r\n\r\n.../tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    456     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    457     try:\r\n--> 458       result = method(self, *args, **kwargs)\r\n    459     finally:\r\n    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n.../tensorflow/python/keras/engine/sequential.py in add(self, layer)\r\n    167           # and create the node connecting the current layer\r\n    168           # to the input layer we just created.\r\n--> 169           layer(x)\r\n    170           set_inputs = True\r\n    171\r\n\r\n.../tensorflow/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)\r\n    620\r\n    621     if initial_state is None and constants is None:\r\n--> 622       return super(RNN, self).__call__(inputs, **kwargs)\r\n    623\r\n    624     # If any of `initial_state` or `constants` are specified and are Keras\r\n\r\n.../tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    631                       base_layer_utils.AutoAddUpdates(self,\r\n    632                                                       inputs)) as auto_updater:\r\n--> 633                 outputs = call_fn(inputs, *args, **kwargs)\r\n    634                 auto_updater.set_outputs(outputs)\r\n    635\r\n\r\n.../tensorflow/python/keras/layers/recurrent_v2.py in call(self, inputs, mask, training, initial_state)\r\n    328           input_length=timesteps,\r\n    329           time_major=self.time_major,\r\n--> 330           zero_output_for_mask=self.zero_output_for_mask)\r\n    331       # This is a dummy tensor for testing purpose.\r\n    332       runtime = _runtime('unknown')\r\n\r\n.../tensorflow/python/keras/backend.py in rnn(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\r\n   3558     # the value is discarded.\r\n   3559     output_time_zero, _ = step_function(input_time_zero,\r\n-> 3560                                         initial_states + constants)\r\n   3561     output_ta = tuple(\r\n   3562         tensor_array_ops.TensorArray(\r\n\r\n.../tensorflow/python/keras/layers/recurrent_v2.py in step(cell_inputs, cell_states)\r\n    316\r\n    317       def step(cell_inputs, cell_states):\r\n--> 318         return self.cell.call(cell_inputs, cell_states, **kwargs)\r\n    319\r\n    320       last_output, outputs, states = K.rnn(\r\n\r\n.../tensorflow/python/keras/layers/recurrent.py in call(self, inputs, states, training)\r\n   1706\r\n   1707       if 0. < self.recurrent_dropout < 1.:\r\n-> 1708         h_tm1 *= rec_dp_mask[0]\r\n   1709\r\n   1710       if self.reset_after:\r\n\r\n.../tensorflow/python/ops/resource_variable_ops.py in __imul__(self, unused_other)\r\n   1449\r\n   1450   def __imul__(self, unused_other):\r\n-> 1451     raise RuntimeError(\"Variable *= value not supported. Use \"\r\n   1452                        \"`var.assign(var * value)` to modify the variable or \"\r\n   1453                        \"`var = var * value` to get a new Tensor object.\")\r\n\r\nRuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object.\r\n```", "comments": ["@ageron In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Hi @muddham ,\r\n\r\nI did! It's in the section \"Code to reproduce the issue\". :)\r\n", "@ageron I tried to reproduce the bug in TF2.0.0-alpha0 but I don't get the runtime error. I see a warning and a deprecation message as follows. Just for your info, I ran your code in Google colab\r\n\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0418 17:16:01.808634 139806816728960 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n\r\nPlease let me know what you think. Thanks! ", "Apparently the problem is now fixed, I don't get the error anymore.  Thanks @jvishnuvardhan .", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27829\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27829\">No</a>\n", "@ageron does it still work for you? I tried using recurrent_dropout with a GRU (as you are) and it seems to break for me. The problem seems to be with recurrent_dropout, cos if you switch it out everything seems to work. This problem also exists with LSTMs, and not just GRUs.", "Apparently the bug is back. Using VERSION='2.0.0-dev20190524' and GIT_VERSION='v1.12.1-2720-geafe861c2b'.\r\n", "I am also having a similar issue. Was wondering if there was an update or an older nightly where this is stable?", "@jlanday , it worked when I posted my comment on April 19th, so perhaps try a nightly from April 18th or 19th?", "@ageron I tried to use the nightly version from both April 18th and 19th and it looks like it still doesn't work. Does version `pip install tf-nightly-gpu-2.0-preview==2.0.0-dev20190518` work for you?", "@ageron I don't see any error with `!pip install tf-nightly`. Gist is [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/4b32a507c8209a95d38335e8efc3e231/untitled203.ipynb). \r\n\r\nBut I notice error is back with    `pip install tf-nightly-gpu-2.0-preview==2.0.0-dev20190518` Thanks!", "Thanks for reporting the issue, will send a fix very soon.", "Should now be fixed by https://github.com/tensorflow/tensorflow/commit/6a6e8c2586dfd2aeeebe0d94d60dcca4604ab481.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27829\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27829\">No</a>\n", "`def build_cell(self):`\r\n\r\n        char_input = self.tf.keras.layers.Input(shape=(1), batch_size=1, name=\"char_input\", dtype=self.tf.int32)\r\n        char_input_one_hot = self.tf.keras.backend.one_hot(char_input, self.vocab_size)\r\n        previous_hidden_state_input = self.tf.keras.layers.Input(shape=(self.num_units), batch_size=1, name=\"previous_hidden_state_input\")\r\n        previous_cell_state_input = self.tf.keras.layers.Input(shape=(self.num_units), batch_size=1, name=\"previous_cell_state_input\")`\r\n        \r\n        \r\n        hidden_input_stacked = self.tf.keras.layers.concatenate([self.tf.keras.backend.squeeze(char_input_one_hot, axis=1), previous_hidden_state_input], axis=1)\r\n        #Forget gate\r\n        forget_gate_f = self.tf.keras.layers.Dense(units=self.num_units, activation=\"sigmoid\")(hidden_input_stacked) #Helps us to take decisions about what must be removed from previous hidden state\r\n        #Input gate\r\n        input_gate_i = self.tf.keras.layers.Dense(units=self.num_units, activation=\"sigmoid\")(hidden_input_stacked) #Decides which values to update\r\n        input_gate_g = self.tf.keras.layers.Dense(units=self.num_units, activation=\"tanh\")(hidden_input_stacked)  #Creates a vector for new candidates to added to present cell state.\r\n        #Output_gate\r\n        output_state_o = self.tf.keras.layers.Dense(units=self.num_units, activation=\"sigmoid\")(hidden_input_stacked)\r\n        #Current cell state\r\n        current_cell_state = self.tf.keras.layers.add([self.tf.keras.layers.multiply([input_gate_g,input_gate_i]),self.tf.keras.layers.multiply([previous_cell_state_input,forget_gate_f])], name=\"current_cell_state\")\r\n        #output_hidden_state\r\n        output_hidden_state = self.tf.keras.layers.multiply([self.tf.keras.layers.Activation(\"tanh\")(current_cell_state), output_state_o], name=\"output_hidden_state\")\r\n        #output_char_probs\r\n        output_char_probs = self.tf.keras.layers.Dense(units=self.vocab_size, activation=\"softmax\", name=\"output_char_probs\")(output_hidden_state)\r\n        \r\n        cell = self.tf.keras.Model(inputs=[char_input, previous_hidden_state_input, previous_cell_state_input], outputs=[output_char_probs, output_hidden_state, current_cell_state])\r\n        return cell`\r\n\r\nThis code will break tensorflow 2.0, I found the problem to be in tensorflow/tensorflow/python/keras/layers/merge.py  / with the add and multiply functions. Please fix\r\n\r\nThe error I received trying to feed input in:\r\n\r\n>     raise RuntimeError(\"Variable *= value not supported. Use \"\r\n>     RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable >     or `var = var * value` to get a new Tensor object.", "Can the line:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3a094e6d1c927967438c0b263db0bf42d16813ae/tensorflow/python/keras/layers/merge.py#L245\r\n\r\nbe changed to \r\n\r\n`output = output + inputs[i]`\r\n\r\nto fix this?\r\n(and similar within `Subtract` etc.)\r\n\r\nIt doesn't like the `+=` notation when applied to a `tf.Variable`"]}, {"number": 27828, "title": "Build tensorflow C++ failed", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):nightly\r\n- Python version:\r\n- Bazel version (if compiling from source):0.24.1\r\n- GCC/Compiler version (if compiling from source):6.3.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nERROR: /home/fsx950223/tensorflow/tensorflow/core/kernels/BUILD:3370:1: C++ compilation of rule '//tensorflow/core/kernels:batch_matmul_op' failed (Exit 1)\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:296:0,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/kernels/batch_matmul_op_impl.h:25,\r\n                 from tensorflow/core/kernels/batch_matmul_op_real.cc:16:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralBlockPanelKernel.h: In member function 'void Eigen::internal::lhs_process_one_packet<nr, LhsProgress, RhsProgress, LhsScalar, RhsScalar, ResScalar, AccPacket, LhsPacket, RhsPacket, ResPacket, GEBPTraits, LinearMapper, DataMapper>::peeled_kc_onestep(Eigen::Index, const LhsScalar*, const RhsScalar*, GEBPTraits, LhsPacket*, Eigen::internal::lhs_process_one_packet<nr, LhsProgress, RhsProgress, LhsScalar, RhsScalar, ResScalar, AccPacket, LhsPacket, RhsPacket, ResPacket, GEBPTraits, LinearMapper, DataMapper>::RhsPacketx4*, RhsPacket*, AccPacket*, AccPacket*, AccPacket*, AccPacket*) [with int nr = 4; long int LhsProgress = 1l; long int RhsProgress = 1l; LhsScalar = Eigen::half; RhsScalar = Eigen::half; ResScalar = Eigen::half; AccPacket = Eigen::half; LhsPacket = Eigen::half; RhsPacket = Eigen::half; ResPacket = Eigen::half; GEBPTraits = Eigen::internal::gebp_traits<Eigen::half, Eigen::half, false, false, 1, 0>; LinearMapper = Eigen::internal::BlasLinearMapper<Eigen::half, long int, 0>; DataMapper = Eigen::internal::blas_data_mapper<Eigen::half, long int, 0, 0>]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralBlockPanelKernel.h:1351:33: internal compiler error: in assign_temp, at function.c:961\r\n     __asm__  (\"\" : \"+x,m\" (*A0));\r\n```\r\n\r\n**Describe the expected behavior**\r\nCompile success\r\n\r\n**Code to reproduce the issue**\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n``` sh\r\nbazel build -c dbg --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/cc/example:example --local_resources 2048,.5,1.0\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Same here. I'm having the same issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27828\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27828\">No</a>\n", "@fsx950223 Same issue, did you resolve this problem finally? If so, could you share a workaround, thanks !", "> @fsx950223 Same issue, did you resolve this problem finally? If so, could you share a workaround, thanks !\r\n\r\nadd ```-c opt``` to the ```bazel``` build command."]}, {"number": 27827, "title": "Eager: Network with eager execution is not learning and low performance", "body": "**System information**\r\n- OS Platform: Win 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: CUDA V10.0.130, cuDNN 7.5\r\n- GPU model and memory: RTX 2070 8GB\r\n\r\n\r\n**Describe the current behavior**\r\nI've convert my keras network to tensorflow eager and this network dosn't learn anything. The network predicts only the same values. The performance is 2-3 times slower as the keras model.\r\nWith tensorflow graph without eager execution everything works fine.\r\n\r\n**Describe the expected behavior**\r\nI want to predict actions for various input images in reinforcement learning \r\n\r\n**Code to reproduce the issue**\r\n**Keras model**\r\n\r\n```python\r\nfrom keras.layers import *\r\nfrom keras.models import Model\r\nfrom keras.optimizers import Adam\r\nimport tensorflow as tf\r\n\r\n\r\nclass KerasTest:\r\n    def __init__(self, state_space, action_space, lr):\r\n        self.state_space = state_space\r\n        self.action_space = action_space\r\n        self.lr = lr\r\n\r\n        inputs = Input(shape=(84, 84, 4))\r\n        rewards = Input(shape=(1,))\r\n\r\n        x = Conv2D(32, kernel_size=[8, 8], padding='valid', strides=[4, 4], activation=None)(inputs)\r\n        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)\r\n        x = Activation('relu')(x)\r\n        x = Conv2D(64, kernel_size=[4, 4], padding='valid', strides=[2, 2], activation=None)(x)\r\n        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)\r\n        x = Activation('relu')(x)\r\n        x = Conv2D(64, kernel_size=(4, 4), padding='valid', strides=(2, 2), activation=None)(x)\r\n        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)\r\n        x = Activation('relu')(x)\r\n        x = Flatten()(x)\r\n        x = Dense(512, activation='relu')(x)\r\n        x = Dropout(0.5)(x)\r\n        x = Dense(256, activation='relu')(x)\r\n        x = Dropout(0.3)(x)\r\n        x = Dense(128, activation='relu')(x)\r\n        x = Dropout(0.05)(x)\r\n        logits = Dense(self.action_space, activation=None)(x)\r\n\r\n        self.model = Model(inputs=[inputs, rewards], outputs=logits)\r\n\r\n        def policy_loss(r):\r\n            def loss(labels, logits):\r\n                policy = tf.nn.softmax(logits)\r\n                entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=policy, logits=logits)\r\n                log = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\r\n                p_loss = log * tf.stop_gradient(r)\r\n                p_loss = p_loss - 0.01 * entropy\r\n                total_loss = tf.reduce_mean(p_loss)\r\n                return total_loss\r\n\r\n            return loss\r\n\r\n        self.model.compile(optimizer=Adam(lr=lr), loss=policy_loss(rewards))\r\n        self.model.summary()\r\n\r\n    def get_probs(self, s):\r\n        s = s[np.newaxis, :]\r\n        probs = self.model.predict([s, np.array([1])])\r\n        probs = probs.squeeze()\r\n        probs = self.softmax(probs)\r\n        return probs\r\n\r\n    def softmax(self, x):\r\n        e_x = np.exp(x - np.max(x))\r\n        return e_x / e_x.sum(axis=0)\r\n\r\n    def update_policy(self, s, r, a):\r\n        self.model.train_on_batch([s, r], a)\r\n```\r\n\r\n**Eager**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import *\r\nimport numpy as np\r\n\r\ntf.enable_eager_execution()\r\nprint(tf.executing_eagerly())\r\n\r\n\r\nclass EagerTest:\r\n    def __init__(self, state_space, action_space, lr):\r\n        self.action_space = action_space\r\n        self.lr = lr\r\n\r\n        inputs = Input(shape=(84, 84, 4))\r\n        rewards = Input(shape=(1,))\r\n\r\n        x = Conv2D(32, kernel_size=[8, 8], padding='valid', strides=[4, 4], activation=None)(inputs)\r\n        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)\r\n        x = Activation('relu')(x)\r\n        x = Conv2D(64, kernel_size=[4, 4], padding='valid', strides=[2, 2], activation=None)(x)\r\n        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)\r\n        x = Activation('relu')(x)\r\n        x = Conv2D(64, kernel_size=(4, 4), padding='valid', strides=(2, 2), activation=None)(x)\r\n        x = BatchNormalization(trainable=True, epsilon=1e-5)(x)\r\n        x = Activation('relu')(x)\r\n        x = Flatten()(x)\r\n        x = Dense(512, activation='relu')(x)\r\n        x = Dropout(0.5)(x)\r\n        x = Dense(256, activation='relu')(x)\r\n        x = Dropout(0.3)(x)\r\n        x = Dense(128, activation='relu')(x)\r\n        x = Dropout(0.05)(x)\r\n        logits = Dense(self.action_space, activation=None)(x)\r\n\r\n        self.model = tf.keras.Model(inputs=[inputs, rewards], outputs=logits)\r\n\r\n        def policy_loss(r):\r\n            def loss(labels, logits):\r\n                policy = tf.nn.softmax(logits)\r\n                entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=policy, logits=logits)\r\n                log = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\r\n                p_loss = log * tf.stop_gradient(r)\r\n                p_loss = p_loss - 0.01 * entropy\r\n                total_loss = tf.reduce_mean(p_loss)\r\n                return total_loss\r\n\r\n            return loss\r\n\r\n        self.model.compile(optimizer=tf.train.AdamOptimizer(lr), loss=policy_loss(rewards))\r\n        self.model.summary()\r\n\r\n    def get_probs(self, s):\r\n        s = s[np.newaxis, :]\r\n        probs = self.model([s, np.array([1])]).numpy()\r\n        probs = probs.squeeze()\r\n        probs = self.softmax(probs)\r\n        return probs\r\n\r\n    def softmax(self, x):\r\n        e_x = np.exp(x - np.max(x))\r\n        return e_x / e_x.sum(axis=0)\r\n\r\n    def update_policy(self, s, r, a):\r\n        self.model.train_on_batch([s, r], a)\r\n\r\n```\r\n**And same issue if I try to optimize manually**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import *\r\n\r\nimport numpy as np\r\n\r\ntf.enable_eager_execution()\r\nprint(tf.executing_eagerly())\r\n\r\n\r\nclass EagerSeqTest:\r\n    def __init__(self, state_space, action_space, lr):\r\n        self.state_space = state_space\r\n        self.action_space = action_space\r\n\r\n        self.model = tf.keras.Sequential()\r\n\r\n        self.model.add(InputLayer(input_shape=(84, 84, 4)))\r\n\r\n        # Conv\r\n        self.model.add(Conv2D(filters=32, kernel_size=(8, 8), strides=(4, 4), name='conv1'))\r\n        self.model.add(BatchNormalization(trainable=True, epsilon=1e-5, name='batch_norm1'))\r\n        self.model.add(ReLU(name='conv_1_out'))\r\n\r\n        self.model.add(Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), name='conv2'))\r\n        self.model.add(BatchNormalization(trainable=True, epsilon=1e-5, name='batch_norm2'))\r\n        self.model.add(ReLU(name='conv_2_out'))\r\n\r\n        self.model.add(Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), name='conv3'))\r\n        self.model.add(BatchNormalization(trainable=True, epsilon=1e-5, name='batch_norm3'))\r\n        self.model.add(ReLU(name='conv_3_out'))\r\n\r\n        self.model.add(Flatten(name='flatten'))\r\n\r\n        # Fully connected\r\n        self.model.add(Dense(units=512, activation='relu', name='fc1'))\r\n        self.model.add(Dropout(rate=0.4, name='dr1'))\r\n        self.model.add(Dense(units=256, activation='relu', name='fc2'))\r\n        self.model.add(Dropout(rate=0.3, name='dr2'))\r\n        self.model.add(Dense(units=64, activation='relu', name='fc3'))\r\n        self.model.add(Dropout(rate=0.03, name='dr3'))\r\n\r\n        # Logits\r\n        self.model.add(Dense(units=self.action_space, activation='linear', name='logits'))\r\n\r\n        self.model.summary()\r\n\r\n        # Optimizer\r\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\r\n\r\n    def get_probs(self, s):\r\n        s = s[np.newaxis, :]\r\n        logits = self.model(s)\r\n        probs = tf.nn.softmax(logits).numpy().squeeze()\r\n        return probs\r\n\r\n    def update_policy(self, s, r, a):\r\n        with tf.GradientTape() as tape:\r\n            loss = self.calc_loss(s, r, a)\r\n        grads = tape.gradient(loss, self.model.trainable_variables)\r\n        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n\r\n    def calc_loss(self, s, r, a):\r\n        logits = self.model(s)\r\n        policy_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=a, logits=logits)\r\n        policy_loss = tf.reduce_mean(policy_loss * tf.stop_gradient(r))\r\n        return policy_loss\r\n```\r\n\r\n\r\n**Other info / logs**\r\n**Keras outputs first episode**\r\n```\r\n[0.33398542 0.32970214 0.33631247]\r\n[0.33452868 0.32947394 0.33599734]\r\n[0.33389196 0.32968676 0.3364213 ]\r\n[0.33383334 0.32978088 0.33638576]\r\n[0.33387497 0.3296482  0.33647686]\r\n[0.33390424 0.32952887 0.33656695]\r\n[0.33359385 0.32956737 0.33683875]\r\n[0.33371714 0.32963908 0.3366438 ]\r\n[0.33354157 0.32960638 0.33685204]\r\n[0.33366755 0.32962722 0.33670527]\r\n            ...\r\n```\r\n**Keras outputs after 10 episodes**\r\n```\r\n[0.2601601  0.31925505 0.42058486]\r\n[0.3081141  0.35050547 0.34138042]\r\n[0.30224514 0.41226208 0.28549278]\r\n[0.2596298  0.4011653  0.33920488]\r\n[0.24976756 0.44619036 0.30404213]\r\n[0.22203408 0.50170064 0.2762653 ]\r\n[0.25094017 0.460491   0.28856885]\r\n[0.25819647 0.44147474 0.30032873]\r\n[0.26891825 0.42985788 0.30122384]\r\n[0.24224553 0.46432737 0.2934271 ]\r\n              ...\r\n```\r\n**Eager outputs first episode**\r\n```\r\n[0.32854515 0.33827797 0.33317688]\r\n[0.3286925  0.33824813 0.3330594 ]\r\n[0.3283261  0.33858737 0.33308655]\r\n[0.328541   0.3381491  0.33330992]\r\n[0.328702   0.33778957 0.3335084 ]\r\n[0.32902354 0.3377183  0.33325812]\r\n[0.32862762 0.33784387 0.3335285 ]\r\n[0.3286057  0.3380287  0.33336568]\r\n[0.32879072 0.33825696 0.33295232]\r\n[0.3287569  0.33825108 0.33299205]\r\n              ...\r\n```\r\n**Eager outputs after 10, 20, 100, .... episodes**\r\n```\r\n[0.3276771  0.33870533 0.33361754]\r\n[0.3276953  0.33861196 0.33369276]\r\n[0.3273576  0.33890665 0.33373576]\r\n[0.32751966 0.3387702  0.33371013]\r\n[0.3276457  0.33878204 0.33357224]\r\n[0.32758534 0.3388276  0.33358708]\r\n[0.3277567  0.3381556  0.33408773]\r\n[0.32765684 0.33842626 0.33391687]\r\n[0.32840943 0.3379219  0.33366865]\r\n[0.32794353 0.33860624 0.33345023]\r\n              ...\r\n```\r\n\r\n\r\n", "comments": ["@karmel can you triage?", "This may be a duplicate of https://github.com/tensorflow/tensorflow/issues/27472 -- @robieta , assigning this to you as well assuming they have a similar action item.", "Hi!@tk2232! \r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Also attempted to replicate the issue with TF v2.5 ,please find the[ gist](https://colab.research.google.com/gist/mohantym/2191d26cc6539b7458f838efb6b24fee/27827.ipynb#scrollTo=J6MwNgwybTEH) here .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27827\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27827\">No</a>\n"]}, {"number": 27826, "title": "C++ compilation of rule '//tensorflow/core/grappler/optimizers:memory_optimizer' failed (Exit 2)", "body": "Hi,\r\n\r\nWhen i run: bazel build tensorflow/tools/graph_transforms:transform_graph i get the following issue:\r\ntensorflow/core/grappler/optimizers/BUILD:428:1: C++ compilation of rule '//tensorflow/core/grappler/optimizers:memory_optimizer' failed (Exit 2)\r\n\r\nI am running this on Windows 10, has anyone come accross this isse?\r\n\r\nThanks in advance.", "comments": ["win10, bazel0.24.1\r\nrun: bazel build tensorflow/tools/graph_transforms:transform_graph\r\nget\r\nERROR: C:/users/user/tensorflow_test2/tensorflow/tensorflow/core/kernels/data/BUILD:78:1: C++ compilation of rule '//tensorflow/core/kernels/data:captured_function' failed (Exit 2)\r\nany fix method?", "@Clamps251 Is this still an issue? Can you please provide following information? Thanks!\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "question fix, environment install error, C++ visual studio install fail", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n@LiuPangYao Please post a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose) and provide all the information asked by the template.  We ask this because it's more efficient to have one thread dedicated to one issue. Thanks!"]}, {"number": 27825, "title": "TFLite: Div op Neon optimization", "body": "Added float32 division optimized with Neon SIMD instructions.", "comments": ["@gbaned, do you know when this PR will be processed? Are there any problems with it?", "@mwtarnowski Thanks for your contribution, waiting for a googler to approve the changes.", "@aselle, @jianlijianli, do you know (more or less) when this code could be reviewed?", "Can one of the admins verify this patch?", "Can you provide some additional details on the associated speedup? and how you benchmarked/verified the speedup? Thanks!", "@mwtarnowski Can you please resolve conflicts? Thanks!", "@gbaned Done.\r\n\r\n@jdduke Below are results of running [benchmark tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) on Google Pixel 3 XL\r\n\r\n`./benchmark_model --graph=div.tflite --num_threads=1`\r\n\r\nfor a simple model `div.tflite` consisting only of elementwise division of two tensors of shape (1,512,512,128).\r\n![visualize_div](https://user-images.githubusercontent.com/16261078/67624291-17ebee00-f82f-11e9-888a-6b76384ac934.png)\r\n\r\ncurrent implementation:\r\n```\r\nInitialized session in 0.497ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=3 first=155086 curr=121309 min=121309 max=155086 avg=132658 std=15859\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=120700 curr=121057 min=120227 max=122100 avg=120982 std=415\r\n\r\nMemory usage: max resident set size = 640.383 MB\r\nMemory usage: total malloc-ed memory = 704.17 MB\r\nAverage inference timings in us: Warmup: 132658, Init: 497, no stats: 120982\r\n```\r\nproposed optimization:\r\n```\r\nInitialized session in 0.447ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=5 first=89585 curr=40311 min=40149 max=89585 avg=50116.8 std=19734\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=40277 curr=41256 min=40021 max=41256 avg=40326 std=181\r\n\r\nMemory usage: max resident set size = 640.387 MB\r\nMemory usage: total malloc-ed memory = 704.17 MB\r\nAverage inference timings in us: Warmup: 50116.8, Init: 447, no stats: 40326\r\n```\r\nPlease let me know if you need to see results for other devices.", "@mwtarnowski Still, conflicts appearing. Could you please resolve those? Thanks!", "@gbaned Resolved.", "@mwtarnowski Could you please check reviewer comments and keep us posted. Thanks!", "@gbaned Updated."]}, {"number": 27824, "title": "Tensorflow gpu import error", "body": "\r\n\r\n**System information**\r\n- Windows 10\r\n- Python version: conda python 3.6\r\n- Installed using pip\r\n- CUDA 10.1 /cuDNN 7.5.0 version:\r\n- GPU 1050 Ti\r\n\r\n\r\n**Describe the problem**\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\clint\\Anaconda3\\envs\\myenv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n", "comments": ["You should use CUDA10.0,not 10.1.", "> You should use CUDA10.0,not 10.1.\r\n\r\nYou should use CUDA10.0,not 10.1.\r\n---------------------------------------------------\r\nhi, I use\r\ncuda_10.0.130_411.31_win10,\r\ncudnn-10.0-windows10-x64-v7.5.0.56,\r\ntensorflow-gpu-1.13.1\r\npython3.6,\r\nI activate conda env for python3.6, and then installed via pip,\r\nit also throw the same error when I import tensorflow, could you give some suggestions ?\r\nI use visual studio 2019, does it work ?\r\n", "I used visual studio 2017 community,\r\nI also use tensorflow 1.13"]}, {"number": 27823, "title": "tf.keras.layers.BatchNormalization() causes KeyError when converting model with TFLiteConverter ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **colab**\r\n- TensorFlow installed from (source or binary): **binary** \r\n- TensorFlow version (or github SHA if from source): **2.0.0-alpha0**\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-58-c06b2de5a868> in <module>()\r\n      6 \r\n      7 converter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)\r\n----> 8 tflite_model = converter.convert()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    244     \"\"\"\r\n    245     frozen_func = _convert_to_constants.convert_variables_to_constants_v2(\r\n--> 246         self._func)\r\n    247     input_tensors = [\r\n    248         tensor for tensor in frozen_func.inputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2(func)\r\n    171       resource_placeholders[input_name] = {\r\n    172           \"dtype\": node.attr[\"dtype\"],\r\n--> 173           \"data\": tensor_data[input_name],\r\n    174       }\r\n    175 \r\n\r\nKeyError: 'sequential_21/batch_normalization_v2_20/batchnorm/ReadVariableOp/resource'\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\nColab containing following code:\r\nhttps://colab.research.google.com/drive/1lT_Hg4sstFCBi_3ksVQYYhhfxLKhDtdt\r\n\r\n**Any other info / logs**\r\n\r\nThe model is a simple MNIST example, with an added Batch Normalization.\r\n```python3\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n!pip install tensorflow-gpu==2.0.0-alpha0\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train, x_test\r\n\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n  tf.keras.layers.BatchNormalization(),  \r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax),\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test, y_test)\r\n\r\nrun_model = tf.function(lambda x: model(x, training = False))\r\nconcrete_func = run_model.get_concrete_function(\r\n  tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)\r\n)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)\r\ntflite_model = converter.convert()\r\n```", "comments": ["The issue is resolved with the latest nightly version of Tensor Flow.\r\nFor more details see:\r\nhttps://github.com/tensorflow/tensorflow/issues/26672"]}, {"number": 27822, "title": "tflite support for Softplus (exp and log are already supported)", "body": "**System information**\r\n- gLinux\r\n- TensorFlow installed from source\r\n- TensorFlow version head\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, MAX_POOL_2D, MEAN, MIRROR_PAD, MUL, RSQRT, SQUARED_DIFFERENCE, SUB, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: Softplus.\r\n```\r\n\r\nWe already support log and exp. Softplus seems like a simple addition.", "comments": ["@jdduke , what is your take on this, should we have this operator support in the tflite kernel or should we transform the operator into log and exp using transform graph ?\r\n\r\nRegards\r\nAmit", "Beware. It will be problematic if SoftPlus is simply transformed to `log(exp(x) + 1)`.  There will be numerical stability issues when input is too large or too small. \r\n\r\nIf going the transformation route, we need to do something like this:\r\nhttps://github.com/tensorflow/tensorflow/blob/ee82131dbccd4e99decb8c05c43bc2bb387ad6ac/tensorflow/core/kernels/softplus_op.h#L50", "@miaout17,thanks for the heads up, highly appreciated your valuable input, will study this deeply and raise a PR in coming days\r\n\r\nRegards \r\nAmit ", "@miaout17 , i have raised the PR, kindly have a look.\r\n\r\nRegards\r\nAmit", "@jackshi0912,\r\nSorry for the delayed response. With respect to [jdduke's comment in this PR](https://github.com/tensorflow/tensorflow/pull/28042#issuecomment-510988913), \r\n\r\n> Until we find another model which requires this op, I'd rather we rely on using the select TF ops, or we add this as a custom op that users can optionally link into their app.\r\n\r\nIt's more than 2 years since this feature has been requested and there has been no **`Model`** which requires this Op. Do you still think this feature is still relevant? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 27821, "title": "Update tf_upgrade_v2.py", "body": "The script prints \"forsafety\" because of the missing blank space here", "comments": ["I am not sure if the error stems from my commit:\r\n\r\n`Test tensorflow/contrib/learn/grid_search_test exited with error code 139`\r\n\r\n`external/bazel_tools/tools/test/test-setup.sh: line 310:    15 Segmentation fault      (core dumped)`"]}, {"number": 27820, "title": "Eager execution Blas SGEMM launch failed and GPU huge consumption.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. Code based on [https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb).\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): <s>Ubuntu 16.04.5 LTS</s> Red Hat Enterprise Linux 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Tensorflow docker image(tensorflow/tensorflow:latest-gpu-py3)\r\n- TensorFlow version (use command below): 1.14.1-dev20190413\r\n- Python version: 3.5.2(in docker image)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:  4 GeForce RTX 2080\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen eager execution is enabled, GPU resources are consumed significantly with only single calculation. Following are two simplified versions of the problem, the first one comes from [https://github.com/tensorflow/tensorflow/issues/25403](https://github.com/tensorflow/tensorflow/issues/25403) and the second one is my problem.\r\n\r\n1. Huge GPU consumption when running a simple matrix multiplication.\r\n\r\n2. Blas SGEMM launch failed Error when running loading the InceptionV3 model and feed with a zero vector(same error when feeding with dataset samples).\r\nThere is **NO** Blas SGEMM launch failed Error when Eager Execution is NOT enabled.\r\n\r\n**Describe the expected behavior**\r\nBe able to run the scripts.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nFirst run the docker image then run the code in python:\r\n\r\ndocker run -it --runtime=nvidia --rm -e NVIDIA_VISIBLE_DEVICE=\"0,1,2,3\" -v /usr/scratch:/usr/scratch tensorflow/tensorflow:latest-gpu-py3 python\r\n\r\n1. import tensorflow as tf\r\ntf.enable_eager_execution()\r\nprint(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))\r\nnvidia-smi\r\n\r\n2. import tensorflow as tf\r\ntf.enable_eager_execution()\r\nimage_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\r\nnew_input = image_model.input\r\nhidden_layer = image_model.layers[-1].output\r\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\r\nx = tf.zeros([1, 299, 299, 3])  # just use a zero tensor instead of dataset tensor\r\nimage_features_extract_model(x)  # this will raise the error: \"InternalError: Blas SGEMM launch failed : m=5329, n=80, k=64 [Op:Conv2D]\"\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n1. **vnivia-smi output before running code:**\r\n+---------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+--------------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|   Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+================|\r\n|   0  GeForce RTX 2080    Off  | 00000000:19:00.0 Off |                  N/A |\r\n| 24%   39C    P0    41W / 215W |      0MiB /  7952MiB |      0%      Default |\r\n+-------------------------------+----------------------+------------------------+\r\n|   1  GeForce RTX 2080    Off  | 00000000:1A:00.0 Off |                  N/A |\r\n| 24%   40C    P0    41W / 215W |      0MiB /  7952MiB |      1%      Default |\r\n+-------------------------------+----------------------+------------------------+\r\n|   2  GeForce RTX 2080    Off  | 00000000:67:00.0 Off |                  N/A |\r\n| 22%   43C    P0    51W / 215W |      0MiB /  7952MiB |      0%      Default |\r\n+-------------------------------+----------------------+------------------------+\r\n|   3  GeForce RTX 2080    Off  | 00000000:68:00.0 Off |                  N/A |\r\n| 14%   45C    P0     1W / 215W |      0MiB /  7951MiB |      0%      Default |\r\n+-------------------------------+----------------------+------------------------+\r\n+---------------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n**nvidia-smi after running the code:**\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|   Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+=================|\r\n|   0  GeForce RTX 2080    Off  | 00000000:19:00.0 Off |                  N/A |\r\n| 24%   41C    P2    42W / 215W |   7633MiB /  7952MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 2080    Off  | 00000000:1A:00.0 Off |                  N/A |\r\n| 23%   43C    P2    40W / 215W |    121MiB /  7952MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce RTX 2080    Off  | 00000000:67:00.0 Off |                  N/A |\r\n| 24%   46C    P2    51W / 215W |    121MiB /  7952MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce RTX 2080    Off  | 00000000:68:00.0 Off |                  N/A |\r\n| 22%   48C    P2    51W / 215W |    121MiB /  7951MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=================================================|\r\n|    0     96311      C   python                                      7623MiB |\r\n|    1     96311      C   python                                       111MiB |\r\n|    2     96311      C   python                                       111MiB |\r\n|    3     96311      C   python                                       111MiB |\r\n+-----------------------------------------------------------------------------+\r\n**A simple matrix multiplcation consumes an entire GPU andsome in other three.**\r\n\r\n2. **nvidia-smi before running the code:**\r\nsame as 1, nothing running GPU.\r\n**nvidia-smi after running the code except for the last line of code of \"image_features_extract_model(x)\":**\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|   Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+================|\r\n|   0  GeForce RTX 2080    Off  | 00000000:19:00.0 Off |                  N/A |\r\n| 24%   40C    P2    42W / 215W |   7667MiB /  7952MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 2080    Off  | 00000000:1A:00.0 Off |                  N/A |\r\n| 23%   41C    P8    13W / 215W |    121MiB /  7952MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce RTX 2080    Off  | 00000000:67:00.0 Off |                  N/A |\r\n| 24%   44C    P8    24W / 215W |    121MiB /  7952MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce RTX 2080    Off  | 00000000:68:00.0 Off |                  N/A |\r\n| 22%   46C    P8    20W / 215W |    121MiB /  7951MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|==============================================|\r\n|    0     96729      C   python                                      7657MiB |\r\n|    1     96729      C   python                                       111MiB |\r\n|    2     96729      C   python                                       111MiB |\r\n|    3     96729      C   python                                       111MiB |\r\n+-----------------------------------------------------------------------------+\r\n**Error when running the last line of code:**\r\n2019-04-13 15:52:15.698521: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n2019-04-13 15:52:15.724436: E tensorflow/stream_executor/cuda/cuda_blas.cc:510] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-04-13 15:52:15.724477: W tensorflow/stream_executor/stream.cc:2130] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 592, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\", line 815, in call\r\n    mask=masks)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\", line 1000, in _run_internal_graph\r\n    output_tensors = layer(computed_tensor, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 592, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 194, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 966, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 591, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 208, in __call__\r\n    name=self.name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 994, in conv2d\r\n    name=name, ctx=_ctx)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1072, in conv2d_eager_fallback\r\n    ctx=_ctx, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=5329, n=80, k=64 [Op:Conv2D]", "comments": ["Just to provide an easier-to-read version of my problem.\r\nI am using tensorflow gpu python3 docker image on Red Hat. I have 4 GeForce RTX 2080 GPUs.\r\nWhen I enable eager execution and run the code:\r\n\r\n1. import tensorflow as tf\r\ntf.enable_eager_execution()\r\nprint(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))\r\n\r\nthere will be a huge consumption of GPU, which consumes an entire GPU and some of the other three. I am not sure if this is normal.\r\n\r\n2. import tensorflow as tf\r\ntf.enable_eager_execution()\r\nimage_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\r\nnew_input = image_model.input\r\nhidden_layer = image_model.layers[-1].output\r\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\r\nx = tf.zeros([1, 299, 299, 3]) # just use a zero tensor instead of dataset tensor\r\nimage_features_extract_model(x)\r\n\r\nbefore running the last line of code, the GPU consumption is like 1, it consumes an entire GPU and some of the other three. And after running the last line of code, it raises error of Blas SGEMM launch failed. But if I do NOT enable eager execution, it works fine with **NO** Blas SGEMM launch failed error.", "Can someone help? :/", "> Apologies for the delay in response. By default, TensorFlow maps nearly all of the GPU memory of all GPUs visible to the process. Have you considered [Allowing GPU memory growth](https://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth) option to limit gpu memory usage?.\r\n\r\nThank you for the response. Basically, my purpose is to run the code so I don't care if TensorFlow uses all GPUs. As in the problem description, I set \"NVIDIA_VISIBLE_DEVICE=\"0,1,2,3\"\" when creating the docker image, and I attempted to let the script to use all GPUs if needed. What confused me was:\r\n1. TensorFlow does not use all GPUs but fully consumes one GPU and a little of the other three. I am not sure if that is normal.\r\n2. It raised \"Blas SGEMM launch failed\" and with eager execution mode only.", "@smit-hinsu Can you PTAL? Thanks!", "TensorFlow will by default only use a single GPU. To use multiple GPUs you need to either do data or model parallelism. The easiest way to do data parallelism is using distribution strategies; take a look at mirrored strategy or our [tf2 guide on distributed training](https://www.tensorflow.org/alpha/guide/distribute_strategy).\r\n\r\ncc @guptapriya ", "I also met the same problem, where my tensorflow version is 1.12.0-gpu, have you solved this? @zhangjh915 ", "> I also met the same problem, where my tensorflow version is 1.12.0-gpu, have you solved this? @zhangjh915\r\n\r\nCan you try TensorFlow 2 for your code? I did not solve this problem at that time. But I am now using TF2 a lot, and I have not encountered such issue anymore."]}, {"number": 27819, "title": "Tensorflow 2.0.0 gpu Performance Issue on RTX 2060, win 10 ", "body": "Hi I'm not used to write English so Please Understand my situation.\r\nActually My proplem is not quite sure about performance problem or just myself problem.\r\nBut No Solution appear in my circumstance include searching stackover-flow or googling.\r\nAnd also in my country quite many of them is saying same issue about it.\r\nAnd Yeah i do almost everything concern this problem searching googling or blogs or sites\r\nSo I decide to ask u to help me out.\r\n\r\nMy problem is I Change my GPU device NVIDIA GTX1050 TI to RTX 2060.\r\nAnd i ran 1050 ti in tensorflow gpu well enough but when i change my GPU and unistall CUDA 9 before version and reinstall CUDA 10 and reinstall matching cudnn also\r\nCause As u know CUDA 10 is the only choice of RTX Series.\r\nBut After Changing my GPU and CUDA, cudnn, My GPU performance is too low to confuse to using it. Yeah It's working But TOO LOW PERFORMANCE!\r\nActually I usually jupyter notebook in anaconda env system and my sub env 1 is tensorflow 2.0.0 alpha and sub env 2 is tensorflow 1.13. \r\n\r\nBut When i build conv model and train images,  Both tensorflow version of gpu is too low under 15% And CPU usage is almost 100%, RAM 60~70%\r\nUsually GPU use 5 ~ 10%. \r\nWhen i use 1050 ti, GPU performance is almost 80 to 90% high\r\n\r\nAnd I know tensorflow recommend docker Linux only but Even though I install All of these gpu concern program and other needed program.\r\n\r\nSo i sincerely ask tensorflow Is this Right Performance or My OS problem or some other probelm\r\nI want to be clear Answer about it.\r\nAnd Even if tensorflow 2.0 is not fit on windows 10, Why my another env tensorflow 1.13 version of GPU is also low and CPU is almost 100% right now\r\n\r\nand Here is the image that Run tensorflow 2.0 tutorial about GPU \r\nAnd My GPU is Fine Device proof png file is upload\r\nThank you for reading my nonsence skill of eng writing.\r\n\r\n![gpu1](https://user-images.githubusercontent.com/43261434/56080846-32391e00-5e41-11e9-95cb-476a0404cf2d.png)\r\n\r\n![gpu colab\uacfc \ube44\uad50 2](https://user-images.githubusercontent.com/43261434/56080864-572d9100-5e41-11e9-9e8c-271faf53645f.png)\r\n\r\n![3d mark2](https://user-images.githubusercontent.com/43261434/56080973-a0321500-5e42-11e9-9176-ae339b5caf26.png)\r\n![3d mark](https://user-images.githubusercontent.com/43261434/56080974-a0321500-5e42-11e9-836c-6b42dfd85cc3.png)\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Anaconda\r\n- TensorFlow version (use command below): Tensorflow 2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10, cuDNN 7.5(2.21.2019 for CUDA 10)\r\n- GPU model and memory: RTX 2060 Emtec & 6G RAM(Edited)\r\n- CPU : intel i5-8400\r\n- Computer RAM - 8G\r\n\r\n", "comments": ["The CUDA10 is not just for NVIDIA Turing/Volta GPUs.it's work well on NVIDIA Geforce GTX1080TI.\r\nIf you plan to evaluate your GPU DL performance, use tensorflow/benchmark.In addition, why does your RTX2060 have 8GB of video memory, which seems wrong?", "> The CUDA10 is not just for NVIDIA Turing/Volta GPUs.it's work well on NVIDIA Geforce GTX1080TI.\r\n> If you plan to evaluate your GPU DL performance, use tensorflow/benchmark.In addition, why does your RTX2060 have 8GB of video memory, which seems wrong?\r\n\r\nSorry about Wrongly Writing GPU RAM Memory. I Miss Understand Which is my computer ram.\r\nAnd Thank you apply my Message.\r\nHowever I searched internet and i can't find tf 2.0-alpha version of tensorflow / benchmark In windows. SO if you recommend any other method please leave a message", "@stellaluminary I can see benchmark tests for master branch [here](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) and the results are [here](https://www.tensorflow.org/guide/performance/benchmarks). Thanks!", "i have a same issue did you solve it ?", "Same here.\r\n\r\nEvery possible check says GPU is working, but calculations are very slow (much slower than on Google Colab) and Task Manager shows CPU working on 30% and GPU almost 0%.", "Sorry for the long delay. @stellaluminary, my guess this is caused by #18652. In summary, the program will take a long time to start computing as it has to JIT compile PTX code. Once it successfully runs the first GPU op, it means the compilation is done and everything should run quickly. We already include the compiled code for GTX 1050 Ti's (by compiling for compute capability 6.1), but not for RTX 2060 (which has compute capability 7.5) which is why the program must JIT compile it at the start.\r\n\r\nIt is highly recommended to run the ops/model once before starting the timer, which is called a warmup run. The first run of the model will take longer, both due to the PTX compilation and for several other reasons. We are working on reducing this startup time, but the first run will likely always be a little bit slower than the rest.\r\n\r\nClosing this issue as it seems to be a dupe of #18652. @auliarananana, @Anamitr, if you have performance issues, I recommend opening a new issue with code to reproduce the issue. You can also comment here if you have any questions.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27819\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27819\">No</a>\n", "Reproduced on Ubuntu 18.04, no such an issue."]}, {"number": 27818, "title": " Fixing small typo in comment", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27818) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27818) for more info**.\n\n<!-- ok -->"]}, {"number": 27817, "title": "Input shape of initial_state of keras.layers.RNN", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): r1.13.1\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=[InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2)]; however `cell.state_size` is [[8, 8], [8, 8], [8, 8]]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo errors during `model.compile()`, just like when using `keras` instead of `tf.keras`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n#import keras\r\nimport tensorflow.keras as keras\r\n\r\ntimesteps = 10\r\ninput_dim = 8\r\noutput_dim = 8\r\n\r\ncells = [\r\n    keras.layers.LSTMCell(output_dim),\r\n    keras.layers.LSTMCell(output_dim),\r\n    keras.layers.LSTMCell(output_dim),\r\n]\r\n\r\ninputs = keras.Input((timesteps, input_dim))\r\nencoder_output = keras.layers.RNN(cells, return_state=True)(inputs)\r\n\r\nstates = encoder_output[1:]\r\n\r\ndecoder_output = keras.layers.RNN(cells)(inputs, initial_state=states)\r\n\r\nmodel = keras.models.Model(inputs, decoder_output)\r\nmodel.compile(optimizer='rmsprop', loss='mse')\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"rnn_test.py\", line 19, in <module>\r\n    decoder_output = keras.layers.RNN(cells)(inputs, initial_state=states)\r\n  File \"/Users/username/python-env/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 742, in __call__\r\n    output = super(RNN, self).__call__(full_input, **kwargs)\r\n  File \"/Users/username/python-env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 538, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/Users/username/python-env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1603, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/Users/username/python-env/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 629, in build\r\n    self._validate_state_spec(state_size, self.state_spec)\r\n  File \"/Users/username/python-env/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 664, in _validate_state_spec\r\n    raise validation_error\r\nValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=[InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2), InputSpec(shape=(None, 8), ndim=2)]; however `cell.state_size` is [[8, 8], [8, 8], [8, 8]]\r\n```\r\n", "comments": ["Thanks for reporting the issue, it should now be fixed by https://github.com/testkevinbonz/tensorflow/commit/a15ab60741dd5792baff01a366e2d1d89edb8481", "I am still facing the same issue after upgrading to the newest version. \r\n\r\n` An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=[InputSpec(shape=(2, 20), ndim=2), InputSpec(shape=(2, 20), ndim=2), InputSpec(shape=(2, 20), ndim=2)]; however `cell.state_size` is [[20, 20], [20, 20], [20, 20]]`\r\n\r\nI think the issue is not closed by now. ", "@alexv1247, can u provide the code snippet to reproduce your issue?", "Hey, thanks for your reply. \r\n\r\nthis is the important part of the code:\r\n\r\n` import tensorflow as tf\r\nimport numpy as np\r\nimport keras\r\n\r\n# define arguments for a RNN/ LSTM\r\n\r\n\r\n    class RNNArg():\r\n       def __init__(self):\r\n           self.state_size = 20\r\n           self.hidden_layer_size = [self.state_size, self.state_size, self.state_size]\r\n           self.dropout = 0\r\n           self.dropoutKeepProb = 0.8\r\n           self.num_epochs = 100\r\n           self.MCSamples = 10\r\n           self.ensemble_size = self.MCSamples\r\n           self.output_mean = 1\r\n           self.outputvar = 1\r\n           self.extimate_output_noise_var = False\r\n           self.batch_size = 40\r\n           self.noutput = 2\r\n           self.return_sequences = False\r\n           self.return_state = False\r\n          self.lr = 1e-3\r\n          self.timesteps = 10\r\n          self.features = 6\r\n\r\n\r\n    class RNNBasicCell:\r\n        def __init__(self, RNNArg, model_scope):\r\n            with tf.variable_scope(model_scope + 'Model'):\r\n                self.mode_id = model_scope\r\n                self.batchX_placeholder = tf.placeholder(tf.float32, shape=(None, RNNArg.timesteps, RNNArg.features))\r\n                self.batchY_placeholder = tf.placeholder(tf.int32, shape=(None, RNNArg.noutput))\r\n                self.num_layers = len(RNNArg.hidden_layer_size)\r\n                with tf.variable_scope(model_scope + 'init_state'):\r\n                    self.init_state = tf.placeholder(dtype=tf.float32, shape=[self.num_layers, 2, RNNArg.state_size])\r\n                    self.rnn_state = tf.unstack(self.init_state, axis=0)\r\n\r\n                self.labels = tf.reshape(self.batchY_placeholder, [-1])\r\n\r\n                self.weight = tf.Variable(np.random.rand(RNNArg.state_size, RNNArg.noutput), dtype=tf.float32)\r\n                self.bias = tf.Variable(np.zeros((1, RNNArg.noutput)), dtype=tf.float32)\r\n\r\n        def num_of_batches(self, total_length, RNNArg):\r\n            return total_length // RNNArg.batch_size\r\n\r\n\r\n    class MultiLayerLSTM_Classificator(RNNBasicCell):\r\n       def __init__(self, RNNARg, model_scope):\r\n           RNNBasicCell.__init__(self, RNNARg, model_scope)\r\n           with tf.variable_scope(model_scope + 'Model'):\r\n               inputs = tf.keras.layers.Input(shape=(RNNARg.timesteps, RNNARg.features), tensor=self.batchX_placeholder)\r\n               # create a lstm layer list\r\n               self.rnn_layers = []\r\n               self.batch_size = tf.Variable(RNNARg.batch_size)\r\n               self.keepProb = tf.Variable(RNNARg.dropoutKeepProb)\r\n               self.noutput = tf.Variable(RNNARg.noutput)\r\n               self.dropout = tf.Variable(RNNARg.dropout)\r\n               for _ in RNNARg.hidden_layer_size:\r\n                   self.cell = tf.keras.layers.LSTMCell(RNNARg.state_size)\r\n                   if (RNNARg.dropout == 1):\r\n                       self.cell = tf.nn.rnn_cell.DropoutWrapper(self.cell, output_keep_prob=self.keepProb, state_keep_prob=self.keepProb, input_keep_prob=self.keepProb, variational_recurrent=True, dtype=tf.float32)\r\n                   self.rnn_layers.append(self.cell)\r\n               self.cell = tf.keras.layers.StackedRNNCells(self.rnn_layers)\r\n               # create Model\r\n               self.rnn = tf.keras.layers.RNN(self.cell, return_state=True)(inputs, initial_state=self.rnn_state)\r\n               self.state_series = self.rnn[0]\r\n               self.current_state = self.rnn[1:]\r\n               self.current_state = tf.reshape(tf.concat([states for states in self.current_state], 0), [self.num_layers, 2, self.batch_size, RNNARg.state_size])\r\n               self.state_series = tf.reshape(self.state_series, [-1, RNNARg.state_size])\r\n               # create ouput layer with softmax\r\n               self.output = tf.matmul(self.state_series, self.weight) + self.bias\r\n               self.labels = tf.reshape(tf.argmax(self.batchY_placeholder, axis=1), [RNNARg.batch_size])\r\n               # calculate loss\r\n               self.residuals = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=self.labels)\r\n               self.loss = tf.reduce_mean(self.residuals)\r\n               self.train_steps = tf.train.AdamOptimizer(learning_rate=RNNARg.lr).minimize(self.loss)\r\n\r\n\r\n\r\n`\r\nI hoe that helps.\r\n", "Hi! This issue is persistent in Keras 2.2.4\r\n\r\nI built a Seq2Seq network in Keras where I stacked 2 LSTM layers on either side and passing the initial state from the encoder to the decoder.\r\n\r\n`ValueError: An initial_state was passed that is not compatible with cell.state_size. Received state_spec=[InputSpec(shape=(None, 256), ndim=2\r\n)]; however cell.state_size is (256, 256)`", "hi @akashchintha , have you solved this?", "https://github.com/tensorflow/tensorflow/issues/33886#issuecomment-554491736 get_initial_state gives a hint on the right shape, solved the problem for me."]}, {"number": 27816, "title": "How to fix 'ImportError: DLL load failed: The specified procedure could not be found.' while importing tensorflow-gpu?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.0\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10.1/7.5.0\r\n- GPU model and memory: GEFORCE 840M cuda enabled compute capability 5.0\r\n\r\n\r\n\r\nI cannot import tensorflow-gpu==1.13.1 on python 3.6.0.\r\n\r\nWhat i have already done:\r\n1. Nvidia's drivers.\r\n2. I installed Visual Studio 2017 Professional (did not select any specifications in workload).\r\n3. I installed CUDA 10.1 (was having issues with Visual studio integration so i disabled it during the installation).\r\n4. I pasted Cudnn 7.5.0 onto cuda's folders.\r\n5. Restarted.\r\n6. Pip installed tensorflow-gpu==1.13.1 on python 3.6.0.\r\n7. but then i cannot import tensorflow at all it shows the error below.\r\n\r\n\r\nERROR:\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n\r\n```", "comments": ["replaced cuda 10.1 with cuda 9.0\r\nreplaced cudnn 7.5.0 with cudnn 7.4.1.5\r\ninstalled tensorflow-gpu==1.12.0 instead\r\nnow getting the following error\r\n\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 59, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n```", "> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n> * TensorFlow installed from (source or binary): binary\r\n> * TensorFlow version: 1.13.1\r\n> * Python version: 3.6.0\r\n> * Installed using virtualenv? pip? conda?: pip\r\n> * CUDA/cuDNN version: 10.1/7.5.0\r\n> * GPU model and memory: GEFORCE 840M cuda enabled compute capability 5.0\r\n> \r\n> I cannot import tensorflow-gpu==1.13.1 on python 3.6.0.\r\n> \r\n> What i have already done:\r\n> \r\n> 1. Nvidia's drivers.\r\n> 2. I installed Visual Studio 2017 Professional (did not select any specifications in workload).\r\n> 3. I installed CUDA 10.1 (was having issues with Visual studio integration so i disabled it during the installation).\r\n> 4. I pasted Cudnn 7.5.0 onto cuda's folders.\r\n> 5. Restarted.\r\n> 6. Pip installed tensorflow-gpu==1.13.1 on python 3.6.0.\r\n> 7. but then i cannot import tensorflow at all it shows the error below.\r\n> \r\n> ERROR:\r\n> \r\n> ```\r\n> >>> import tensorflow as tf\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\BenBouali\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> >>>\r\n> ```\r\n\r\nI have the same problem, the only difference is I use cuda 10.0, the others are the same. it throw the same error when I import tensorflow, I dont know why..... Im going to be mad....", "> I have the same problem, the only difference is I use cuda 10.0, the others are the same. it throw the same error when I import tensorflow, I dont know why..... Im going to be mad....\r\n\r\nI removed all of python and installed anaconda, within anaconda i loaded in tensorflow-gpu and it works like a charm, just make sure you have a higher or equal version of cuda on your computer as anaconda has in its installation (it will tell you)", "I have a CPU , Windows 10, Python 3.6, tensor-flow 1.5.0, install msvcp140.dll vis redistributable VS C++ package, also have set the environment variables, but still has the same problem.\r\nError.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\zaheer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 47, in preload_check\r\n    ctypes.WinDLL(build_info.msvcp_dll_name)\r\n  File \"C:\\Users\\zaheer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\ctypes\\__init__.py\", line 344, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\zaheer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\zaheer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\zaheer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Users\\zaheer\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 55, in preload_check\r\n    % build_info.msvcp_dll_name)\r\nImportError: Could not find 'msvcp140.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. You may install this DLL by downloading Visual C++ 2015 Redistributable Update 3 from this URL: https://www.microsoft.com/en-us/download/details.aspx?id=53587\r\n\r\nPlease request for help\r\n", "```\r\nImportError: Could not find 'msvcp140.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. You may install this DLL by downloading Visual C++ 2015 Redistributable Update 3 from this URL: https://www.microsoft.com/en-us/download/details.aspx?id=53587\r\n```\r\n\r\nYou have to place `msvcp140.dll` in one directory found on `%PATH%`."]}, {"number": 27815, "title": "Differential (Delta) Model Update", "body": "I'm new to tensorflow. My area of usage would be on device execution for item recognition.\r\nAs my use case requires frequent updates on the items to recognize, I was wondering if it is possible to update models in a delta way, e.g. to only transfer the updated data and not the whole model each time.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!", "Well, it would be a feature request if this functionality isn\u2019t available yet. :P\n\n> On 15 Apr 2019, at 11:33, gadagashwini <notifications@github.com> wrote:\n> \n> This question is better asked on StackOverflow <http://stackoverflow.com/questions/tagged//tensorflow> since it is not a bug or feature request. There is also a larger community that reads questions there.\n> Thanks!\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/27815#issuecomment-483178484>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ACtKm4K0P3MgxQABEB4pIqpuZnDim9Z4ks5vhEeFgaJpZM4cuCnn>.\n> \n\n", "@MentalGear, Tensorflow supports save and restore model. Please find tutorial in this [link](https://www.tensorflow.org/tutorials/keras/save_and_restore_models) which help you to save model and restore. Thanks!", "Well, my feature request was about a more efficient transfer of a model for offline use, not save and restore operations.\n\n> On 16 Apr 2019, at 13:15, gadagashwini <notifications@github.com> wrote:\n> \n> @MentalGear, Tensorflow supports save and restore model. Please find tutorial in this link which help you to save model and restore. Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "@MentalGear Can you please fill the [feature request](https://github.com/tensorflow/tensorflow/issues/new?template=30-feature-request.md) issue template and post issue. Please try to elaborate as much as you can. Thanks!"]}, {"number": 27814, "title": "[TF 2.0 API Docs] Added Documentation to tf.keras.activations.elu", "body": "Fix #27652 by adding required information to tf.keras.activations.elu [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py) including an image for visuals and example usage from the Intro to CNNs tutorial as suggested.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27814) for more info**.\n\n<!-- need_author_consent -->", "We've signed them. @rthadur, please set CLA's to yes and review\r\n\r\nThanks!!", "Sorry @kyscg, I realized I made a typo while rendering the image, which has now been corrected. Please re-review, apologies!!", "I was wondering about that, I should have mentioned it earlier\r\n\r\nThanks a lot for the change @Bharat123rox ", "@rthadur, gentle ping to review and set CLA's to yes", "@Bharat123rox thank you for your contribution , please make sure all the contributors have signed CLA", "Yes, all contributors have signed the CLA @Bharat123rox and @kyscg and are\nwilling to merge contributions in source code\n\nOn Mon, 15 Apr 2019, 10:41 pm rthadur, <notifications@github.com> wrote:\n\n> @Bharat123rox <https://github.com/Bharat123rox> thank you for your\n> contribution , please make sure all the contributors have signed CLA\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27814#issuecomment-483337436>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AMwu8RSxYYHuypi3p-ijEDOLOBYNTYPOks5vhLLQgaJpZM4ct-zh>\n> .\n>\n", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27814) for more info**.\n\n<!-- cla_yes -->", "@rthadur @akshaym Gentle ping to review", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27814) for more info**.\n\n<!-- need_author_consent -->", "@rthadur or @gbaned please add the `cla:yes` label, @kyscg and/or @fchollet please re-review", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27814) for more info**.\n\n<!-- cla_yes -->", "Can one of the admins verify this patch?", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27814) for more info**.\n\n<!-- ok -->", "@Bharat123rox Can you please resolve conflicts? Thanks!", "@gbaned Conflict is resolved, thanks!", "@Bharat123rox Can you please address Ubuntu Sanity errors? Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27814) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27814) for more info**.\n\n<!-- ok -->", "@gbaned The Ubuntu Sanity errors should be fixed, thanks!", "@lamberta Can you please review this PR ? Thanks!", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 27813, "title": "after quantization aware training and convert to tflite, when running the converted tflite model, ERROR: tensorflow/lite/kernels/pooling.cc:104 input->params.scale != output->params.scale (0 != 1067861562)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android 8.1\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.14\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nwhen run tflite model using tflite benchmark tool, following error happened:\r\n\r\nERROR: tensorflow/lite/kernels/pooling.cc:104 input->params.scale != output->params.scale (0 != 1067861562)\r\nERROR: Node number 92 (AVERAGE_POOL_2D) failed to prepare.\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["any ideas to solve this problem?", "In order to expedite the trouble-shooting process, Please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27812, "title": "Update links to correct example projects", "body": "Currently, the links are opening the image classification examples, not object detection.", "comments": []}, {"number": 27811, "title": "Can't enable eager execution mode in map function", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0.dev20190412\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI write some preprocess code which can only be executed eagerly in map function for dataset formating, but I find that the code can't be executed properly. Even if the main function is executed in eager mode, the map function is still executed in graph mode.\r\n\r\n**Describe the expected behavior**\r\n\r\nI hope the map function can also work in eager mode to ease the coding difficulty.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nassert tf.executing_eagerly can trigger an assertion failure.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@breadbread1984  Can you provide a small code snippet which can just be copy-pasted in the interpreter to reproduce the issue?", "@captain-pool this code can reproduce the problem.\r\n```python\r\n#!/usr/bin/python3\r\n\r\nimport tensorflow as tf;\r\nimport tensorflow_datasets as tfds;\r\n\r\ndef map_func(feature):\r\n\r\n    # assertion fails\r\n    assert tf.executing_eagerly();\r\n    return feature;\r\n\r\ndef main():\r\n\r\n    trainset = tfds.load(name = \"mnist\", split = tfds.Split.TRAIN);\r\n    trainset = trainset.map(map_func);\r\n    for feature in trainset:\r\n        pass;\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # assertion passes\r\n    assert tf.executing_eagerly();\r\n    main();\r\n\r\n```", "In 2.0, code inside Datasets maps is turned into a subgraph for speed, just as it was in 1.x eager execution. You generally want to avoid Python inside your data pipeline.  Otherwise, when you start using accelerators you could easily get I/O bound.\r\n\r\nIn the future for 2.0, tf.function (and thus Autograph) will automatically be applied to those functions, but it doesn't happen yet.\r\n \r\nOne easy way to run Python eagerly on your Dataset is to do it after it comes out of the Dataset within an eager loop (just call your transformation on each batch). If being inside the map function is key, you can use py_function to force Python execution.  See:\r\n\r\nhttps://www.tensorflow.org/alpha/tutorials/load_data/tf_records#writing_a_tfrecord_file\r\n\r\nVarious footnotes from 1.x Eager discussions:\r\nhttps://github.com/tensorflow/tensorflow/issues/14732\r\nhttps://stackoverflow.com/questions/50538038/tf-data-dataset-mapmap-func-with-eager-mode\r\nhttps://stackoverflow.com/questions/49270477/tensorflow-dataset-map-api\r\n"]}, {"number": 27810, "title": "Porting of Relu variant to TFLite Micro.", "body": "This is for HiKey Borads.", "comments": ["I am planning to put padding, round, prelu  along with lstm support for tflite Macro, let me know if this work is conflicting with anyone.\r\n\r\nRegrds\r\nAmit", "@aselle , I have resolved the merge conflicts, can you please take a look.\r\n\r\nRegards\r\nAmit", "@petewarden , great fan of your blogs , i have re-based the code again ,kindly have a look, also let me know if you want me to split the activation kernels into different files ?\r\n\r\nRegards\r\nAmit", "@petewarden  can you please respond to the above query.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "> @amitsrivastava78 Could you please resolve the conflicts? Thanks!\r\n\r\n@gbaned , conflict resolved.\r\n\r\nRegards\r\nAmit", "@petewarden , thanks for your comments, i have updated the code as per your suggestion, \r\nResolved the merge conflicts and tested the code again. kindly have a look.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "Can one of the admins verify this patch?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27809, "title": "Can't import tensorflow 2.0-alpha on win10 python=3.6.8", "body": "**System information**\r\n- OS Platform and Distribution: ```Win10```\r\n- TensorFlow installed from (source or binary): ```pip install tensorflow==2.0.0-alpha0```\r\n- TensorFlow version: ```tensorflow 2.0.0a0```\r\n- Python version: ```Python 3.6.8 :: Anaconda, Inc.```\r\n- Installed using virtualenv? pip? conda?: ```Anaconda pip```\r\n\r\n**Describe the problem**\r\n1. Run ```python``` at anaconda prompt, start python runtime.\r\n2. Run ```import tensorflow```, get error as below:\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n```", "comments": ["@StudyExchange, Please do confirm, whether the tensorflow is installed at Anaconda prompt or outside the anaconda. Thanks!", "Tensorflow is installed at Anaconda Prompt!\r\n```\r\n(base) C:\\Users\\username>activate tf2\r\n(tf2) C:\\Users\\username>python\r\nPython 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n>>>\r\n```", "@StudyExchange You need to add sub folders: bin and lib to the path. Thanks!", "I created bin and lib at project folder, but don't work!\r\n```\r\n(base) D:\\>mkdir tf2_demo\r\n\r\n(base) D:\\>cd tf2_demo\r\n\r\n(base) D:\\tf2_demo>activate tf2\r\n\r\n(tf2) D:\\tf2_demo>mkdir bin lib\r\n\r\n(tf2) D:\\tf2_demo>dir\r\n \u9a71\u52a8\u5668 D \u4e2d\u7684\u5377\u662f \u65b0\u52a0\u5377\r\n \u5377\u7684\u5e8f\u5217\u53f7\u662f 169B-ACE3\r\n\r\n D:\\tf2_demo \u7684\u76ee\u5f55\r\n\r\n2019/04/16  20:27    <DIR>          .\r\n2019/04/16  20:27    <DIR>          ..\r\n2019/04/16  20:27    <DIR>          bin\r\n2019/04/16  20:27    <DIR>          lib\r\n               0 \u4e2a\u6587\u4ef6              0 \u5b57\u8282\r\n               4 \u4e2a\u76ee\u5f55 53,466,030,080 \u53ef\u7528\u5b57\u8282\r\n\r\n(tf2) D:\\tf2_demo>python\r\nPython 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n>>>\r\n```", "You need to install the Microsoft Visual C++ 2015 Redistributable Update 3.\r\nhttps://www.tensorflow.org/install/pip#1.-install-the-python-development-environment-on-your-system", "It work by installing the Microsoft Visual C++ 2015 Redistributable Update 3, thank you!"]}, {"number": 27808, "title": "cannot import _mklinit", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Windows 10\r\n- TensorFlow installed from pip install tensorflow-gpu\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: GTX 970M- 3GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI accidentally deleted Anaconda3 in my home directory so I installed Anaconda again and things started to fall apart. Before that, everything worked perfectly with tensorflow 1.9, pthon 3.6.4, and CUDA 9.0\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Hiep Nguyen\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <mo\r\ndule>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Hiep Nguyen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 47,\r\n in <module>\r\n    import numpy as np\r\n  File \"C:\\Users\\Hiep Nguyen\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py\", line 140, in <module\r\n>\r\n    from . import _distributor_init\r\n  File \"C:\\Users\\Hiep Nguyen\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py\", line 34, in\r\n <module>\r\n    from . import _mklinit\r\n  ImportError: DLL load failed: The specified module could not be found.\r\n```", "comments": ["It is actually a conda-related problem. I did not activate the environment before importing tensorflow.", "I am using Anaconda3, Python 3.6 and was facing the same issue. I fixed it doing \r\npip install numpy==1.17.5\r\n"]}, {"number": 27807, "title": "NNAPI doesn't support tensors with rank 0 (index 3 name PadV2/constant_values)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Windows 10 and Android 9.0\r\n- Mobile device : Xiaomi 8\r\n- TensorFlow version : org.tensorflow:tensorflow-lite:0.0.0-nightly (using 1.11.0 has the same problem)\r\n- Model: resnet18 convert from Pytorch \r\n\r\n**logs from Android 9.0**\r\nE/tflite: NNAPI doesn't support tensors with rank 0 (index 3 name PadV2/constant_values)\r\nE/tflite: Returning error since TFLite returned failure nnapi_delegate.cc:736.\r\n    Failed to build graph for NNAPI\r\n\r\n**other**\r\nif I use resnet50 from tensorflow.contrib.slim.python.slim.nets, it also has similar problem like:\r\nE/tflite: NNAPI doesn't support tensors with rank 0 (index 7 name inference_model/Mean/reduction_indices)\r\n    Returning error since TFLite returned failure nnapi_delegate.cc:736.\r\n    Failed to build graph for NNAPI\r\n", "comments": ["@RoyIronGrey In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "> @RoyIronGrey In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n```java\r\nprotected Interpreter tflite;\r\nprivate Interpreter.Options tfliteOptions = new Interpreter.Options();\r\n\r\ntfliteModel = loadModelFile(activity);\r\n\r\n/** This is for tensorflow-lite:0.0.0-nightly **/\r\n/** If I activate this option, the error like the title will occur**/\r\ntfliteOptions.setUseNNAPI(true);\r\n\r\ntflite = new Interpreter(tfliteModel, tfliteOptions);\r\n\r\nprotected String getModelPath() {\r\n      return \"resnet18_mv.tflite\";\r\n} \r\n```\r\n\r\nBelow is Python Code and Script to describe how to get the model\r\n```python\r\n'''\r\n  Package Version\r\n    torch:       0.4.0\r\n    torchvision: 0.2.1\r\n    tensorflow:  1.12.0\r\n    mmdnn:       0.2.4\r\n'''\r\n\r\n'''\r\n  Part 1: Pytorch resnet18 saving\r\n'''\r\n\r\n# Resnet18 model from torchvision\r\nbase_model = getattr(torchvision.models, 'resnet18')(pretrained=True)\r\n\r\n# Load weights\r\nweights    = 'resnet18_model_checkpoint.pth.tar'\r\ncheckpoint = torch.load(weights)\r\nbase_dict  = {'.'.join(k.split('.')[1:]): v for k,v in list(checkpoint['state_dict'].items())}\r\nbase_model.load_state_dict(base_dict)\r\nbase_model.eval()\r\n\r\n# Save model\r\nmodel_path = 'resnet18_model.pth'\r\ntorch.save(base_model, model_path)\r\n```\r\n\r\n```script\r\n'''\r\n  Part 2: Resnet18 from Pytorch (.pth) to Tensorflow (.pb) with MMdnn\r\n          See https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/pytorch/README.md#convert-pytorch-pre-trained-models-to-ir\r\n'''\r\n\r\nmmtoir -f pytorch -d resnet18 --inputShape 3,224,224 -n resnet18_model.pth\r\nmmtocode -f tensorflow --IRModelPath resnet18.pb --IRWeightPath resnet18.npy --dstModelPath tf_resnet18.py\r\nmmtomodel -f tensorflow -in tf_resnet18.py -iw resnet18.npy -o tf_resnet18 --dump_tag SERVING\r\n```\r\n\r\n```python\r\n'''\r\n  Part 3: Tensorflow (.pb) to Tensorflow Lite (.lite) with TFLiteConverter.from_session\r\n'''\r\n\r\nwith tf.Session() as sess:\r\n\r\n    mmdnn_pb_path = 'tf_resnet18/'\r\n\r\n    # Load pb model file (MMdnn, see https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/tensorflow/README.md#reuse-the-converted-tensorflow-model)\r\n    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], mmdnn_pb_path)\r\n\r\n    input_node_name  = 'input'\r\n    output_node_name = 'dense/BiasAdd'\r\n    input_tensor     = sess.graph.get_tensor_by_name(input_node_name  + ':0')   # Input tensor\r\n    output_tensor    = sess.graph.get_tensor_by_name(output_node_name + ':0')   # Logits tensor\r\n\r\n    # Convert\r\n    converter        = tf.contrib.lite.TFLiteConverter.from_session(sess, [input_tensor], [output_tensor])\r\n    tflite_model     = converter.convert()\r\n\r\n    open('tf_resnet18.tflite', \"wb\").write(tflite_model)\r\n```\r\n", "> @RoyIronGrey In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\nAnd Thanks for your attention!", "@miaowang14 can you provide an update on scalar support?", "I have a pending fix internally, and will further test / improve it. Looking forward to get it ready for the next couple of days.", "@RoyIronGrey is the issue still reproducible on top of tree?", "> @RoyIronGrey is the issue still reproducible on top of tree?\r\n\r\nThanks for your reply. Yes, If I use the same edition of TF lite API. Is there anything new or any new edition for this problem? ", "> @RoyIronGrey is the issue still reproducible on top of tree?\r\n\r\nOr I may misunderstand what you mean", "@RoyIronGrey We just submitted a couple of fixes related to this issue. I don't think tensorflow-lite:0.0.0-nightly or 1.11.0 build include them.\r\n\r\nYou might be able to build TFLite binaries from source and try it out: https://www.tensorflow.org/lite/guide/build_arm64", "The nightly should have the fixes, but you might need to [clear your gradle cache](https://stackoverflow.com/questions/23025433/how-to-clear-gradle-cache).", "> The nightly should have the fixes, but you might need to [clear your gradle cache](https://stackoverflow.com/questions/23025433/how-to-clear-gradle-cache).\r\nYes, It works. But why does it cost more time and unsteady?(Maybe I made some mistakes)\r\nBelow it's the difference\r\n```java\r\n     //If set useNNAPI true\r\n     tfliteOptions.setUseNNAPI(true);\r\n     //time from Log\r\n     infer mv/frame: 375 ms\r\n     infer mv/frame: 175 ms\r\n     infer mv/frame: 516 ms\r\n```\r\n```java\r\n     //If set useNNAPI false\r\n     tfliteOptions.setUseNNAPI(false);\r\n     //time from Log\r\n     infer mv/frame: 100 ms\r\n     infer mv/frame: 87 ms\r\n```\r\n", "I guess the performance issues could be related two possible causes:\r\n1. There are still operations in the Model not supported by NNAPI, so they fallback to TFLite CPU implementation. The overhead and sync cost between multiple subgraphs  could be high.\r\n2. The model itself is a floating point model, while Mi8 may just have an hardware accelerator (Hexagon DSP) that can only accelerate quantized 8bit models.", "I will mark this specific issue as fixed. But feel free to keep comment on it or create new ones if you have questions about the performance and NNAPI delegate.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27807\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27807\">No</a>\n", "> I will mark this specific issue as fixed. But feel free to keep comment on it or create new ones if you have questions about the performance and NNAPI delegate.\r\n\r\nThanks a lot! I'll stay focus on the processing of this project as it really supports my research "]}, {"number": 27806, "title": "Does the latest version 1.13 support CUDA10.1? I need to install tensorflow with gpu.", "body": "\r\n", "comments": ["No.You need to use CUDA 10.0 ", "@berylyellow TF 1.1.3 was compiled with CUDA 10.0 as listed in the release [notes](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md). Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 27805, "title": "Fix the flaky issue in ParallelInterleaveDatasetOpTest", "body": "This PR fixes the flaky issue in ParallelInterleaveDatasetOpTest. We reset the iterator before restoring it so that all the resources/tasks the old iterator holds can be released. \r\n\r\ncc:@jsimsa", "comments": ["> Should all roundtrip tests include this fix?\r\n\r\nYes, the roundtrip test for `ParallelMapDataset` has a similar flaky issue. Will add this fix to all other roundtrip tests soon.", "This fix is added to all roundtrip tests via https://github.com/tensorflow/tensorflow/pull/27805/commits/edb84f20f105df979a372784e152de728b905a9d.", "Good suggestion! I change it via this commit (https://github.com/tensorflow/tensorflow/pull/27805/commits/9a86868aa720c0e4e19f3fddf06772193d2505bd). Could you please have a look at the change?", "Sorry that there is a typo issue. I just fix it (https://github.com/tensorflow/tensorflow/pull/27805/commits/bbed106374e5db95057f19fd17113811bd85c4b3). @rthadur Could you please re-trigger the test?", "The test failure in `Linux GPU` is unrelated. "]}]