[{"number": 11817, "title": "Looks like a bug tensorflow 1.2.1 with gpu error during basic_gpu_test", "body": "### System information\r\n- **No custom code\r\n- **OS Windows 10\r\n- **TensorFlow installed from:https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.2.1-cp35-cp35m-win_amd64.whl\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.5 (Anaconda environment)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: python basic_gpu_test.py\r\n\r\nThe trace:\r\n\r\n.....C:\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py:591: RuntimeWarning: invalid value encountered in greater\r\n  np.abs(a - b) > atol + rtol * np.abs(b), np.isnan(a) != np.isnan(b))\r\nnot close where =  (array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\r\nnot close lhs =  []\r\nnot close rhs =  []\r\nnot close dif =  []\r\nnot close tol =  []\r\ndtype = float32, shape = (1, 3, 5)\r\n......E.\r\n======================================================================\r\nERROR: testTypes (__main__.MathBuiltinUnaryTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"basic_gpu_test.py\", line 136, in testTypes\r\n    self._testDtype(dtype, use_gpu=True)\r\n  File \"basic_gpu_test.py\", line 114, in _testDtype\r\n    self._compare(data, np.arcsinh, math_ops.asinh, use_gpu)\r\nAttributeError: module 'tensorflow.python.ops.math_ops' has no attribute 'asinh'\r\n\r\n----------------------------------------------------------------------\r\nRan 13 tests in 12.158s\r\n\r\nFAILED (errors=1)\r\n\r\n", "comments": ["Same error is associated with\r\nacosh, cosh, sinh, atanh\r\n\r\nas well.", "You should run tests with bazel instead of directly running the .py files.\r\n\r\nFor example, to run `tensorflow/python/kernel_tests/basic_gpu_test.py`:\r\n```\r\nbazel test --config=opt --config=cuda  tensorflow/python/kernel_tests:basic_gpu_test\r\n```", "I am on windows platform...I did install chocolatey (which installed python 2.7 outside of conda) and then installed bazel which failed because in windows I have msys2_shell.bat (and not msys2.exe)\r\nSo a generally futile exercise. I commented out the 5 lines associated with asinh, acosh, cosh, sinh, atanh from basic_gpu_test to make sure that my gpu is playing nicely with tensorflow_gpu.\r\nThank you for your help though.\r\nLooks like none of these tools (chocolately, bazel and for that matter even CUDA are not meant for Windows. Looks like my next machine must be Ubuntu with CUDA.\r\nThanks for your help (I think) though.", "@gunan, is there anyway to run the tests using the easier to use cmake build?", "While the recommended way is to use bazel, most of our tests \"should\" run without bazel, just by running `python <filename>`.\r\nFor the equivalent of what we are running on our CI for windows, you may check this script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/windows/gpu/cmake", "Thank you gunan,\n\nThat is exactly how I ran the test:\n\n\n>>>python basic_gpu_test.py\n\n\nand that is how I got the error message described in the issue.\n\nNo big deal..I commented the line out. and..commented out every line that lead to similar error.\n\n\nI was only interested in checking if my gpu is working at all. The test (with few lines commented out) did tell me it was working alright.\n\n\nI felt obliged to let wonderful people at google that asinh (and 4 other functions) is not being supported by tensor 1.2.1\n\n\nFeel free to ignore it\n\n\nBye for now,\nShirish\n\n\n________________________________\nFrom: gunan <notifications@github.com>\nSent: Friday, July 28, 2017 7:33 PM\nTo: tensorflow/tensorflow\nCc: Shirish (Sam) Ranade; Author\nSubject: Re: [tensorflow/tensorflow] Looks like a bug tensorflow 1.2.1 with gpu error during basic_gpu_test (#11817)\n\n\nWhile the recommended way is to use bazel, most of our tests \"should\" run without bazel, just by running python <filename>.\nFor the equivalent of what we are running on our CI for windows, you may check this script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/windows/gpu/cmake\n\n[https://avatars0.githubusercontent.com/u/15658638?v=3&s=400]<https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/windows/gpu/cmake>\n\ntensorflow/tensorflow/tools/ci_build/windows/gpu/cmake at ...<https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/windows/gpu/cmake>\ngithub.com\ntensorflow - Computation using data flow graphs for scalable machine learning\n\n\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/11817#issuecomment-318742597>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AHaPnUSLDI96KzzZW9K--3-Vv2Wck0xMks5sSjeJgaJpZM4OlR1p>.\n", "To check if my GPU is working or not, I like to use `tf.test.is_gpu_available()` method.\r\nThis only checks if the GPU is accessible. The computations are still best checked by unit tests.", "Thank you sir !\n\nWill try that out !!\n\nBye for now,\nShirish\n\n\n________________________________\nFrom: gunan <notifications@github.com>\nSent: Friday, July 28, 2017 8:56 PM\nTo: tensorflow/tensorflow\nCc: Shirish (Sam) Ranade; Author\nSubject: Re: [tensorflow/tensorflow] Looks like a bug tensorflow 1.2.1 with gpu error during basic_gpu_test (#11817)\n\n\nTo check if my GPU is working or not, I like to use tf.test.is_gpu_available() method.\nThis only checks if the GPU is accessible. The computations are still best checked by unit tests.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/11817#issuecomment-318759625>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AHaPnSxfTYSlWW5DNFCju17ere3AcoQTks5sSksVgaJpZM4OlR1p>.\n", "Thanks gunan"]}, {"number": 11816, "title": "Snappy related tests are failing", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 'v1.2.1-0-gb4957ff', '1.2.1'\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: No GPU\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: bazel test //tensorflow/core:lib_io_snappy_snappy_buffers_test\r\n\r\n### The problem:\r\nWhile executing test `TEST(SnappyBuffers, MultipleWritesWithoutFlush)`. It fails when `Snappy_Uncompress()` method is called which internally calls `snappy::RawUncompress()`.\r\nCompared the same on intel x86 where it works fine; However the data somehow gets lost on s390x.\r\n\r\nI am aware that Snappy behaves differently on s390x as compared to others.\r\n\r\nThere is another test `//tensorflow/core:lib_io_table_test` which fails when snappy compress/uncompress is used.\r\nWould like to know if the mentioned test-cases are used to test some complex functionality of TensorFlow? Can they be ignored?\r\n\r\n### Source code / logs\r\nRunning main() from test_main.cc\r\n[==========] Running 5 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 5 tests from SnappyBuffers\r\n[ RUN      ] SnappyBuffers.MultipleWritesWithoutFlush\r\n2017-07-27 12:25:34.898300: F tensorflow/core/lib/io/snappy/snappy_buffers_test.cc:148] Non-OK-status: TestMultipleWrites(10000, 10000, 10000, 10000, 2) status: Data loss: Snappy_Uncompress failed\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 159: 17664 Aborted                 (core dumped) \"${TEST_PATH}\" \"$@\"\r\n", "comments": ["Most likely it is a big-endian vs little endian issue in snappy buffers.  As usual, we don't really have a great way to debug this, but you should be able to look at the code and find raw buffer interpretations that need endian swaps included for the big-endian case.\r\n", "This problem no longer appears in TF 2.3.1.", "@duane-ibm Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks !", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/11816\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/11816\">No</a>\n"]}, {"number": 11815, "title": "tf.contrib.data.Iterator - Continue from dataset with reinitializable iterator.", "body": "I'd like to request the following feature. In TF 1.3.0rc0 and probably before the reinitializable iterator can be used to switch between dataset sources as shown [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md). This is great for switching between train and test data, but has a nasty side-effect: when reinitializing the iterator the iteration starts from the beginning of the dataset. Therefore, switching the dataset between epoch (i.e do a validation run every 100 iterations or so) causes the epoch to never finish. Moreover, training will only ever use the first 100 batches (except you have a huge shuffle buffer size) of the dataset. \r\n\r\nThe following illustrates the problem\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.contrib.data.Dataset.range(50)\r\nb = tf.contrib.data.Dataset.range(50)\r\n\r\niterator = tf.contrib.data.Iterator.from_structure(a.output_types, a.output_shapes)\r\nnext_element = iterator.get_next()\r\n\r\na_init_op = iterator.make_initializer(a)\r\nb_init_op = iterator.make_initializer(b)\r\n\r\nsess = tf.Session()\r\nsess.run(a_init_op)\r\nsess.run(next_element) \r\n# 0\r\nsess.run(next_element) \r\n# 1\r\n\r\nsess.run(b_init_op)\r\nsess.run(next_element) \r\nsess.run(next_element) \r\n\r\nsess.run(a_init_op)\r\nsess.run(next_element) \r\n# 0  <- rather expect 2\r\nsess.run(next_element) \r\n# 1 <- rather expect 3\r\n```\r\n\r\nI'd like to request a reinitializable iterator that maintains the state of the dataset and continues from where it left off. Something along the lines\r\n\r\n```python\r\n# ... Same as above\r\n\r\na_init_op, a_continue_op = iterator.make_initializer(a)\r\n\r\nsess = tf.Session()\r\nsess.run(a_init_op)\r\nsess.run(next_element) # 0\r\n\r\nsess.run(b_init_op)\r\nsess.run(next_element) \r\n\r\nsess.run(a_continue_op)\r\nsess.run(next_element) # 1\r\n```", "comments": ["Closing this, just found that a feedable iterator covers my use case. Thanks!"]}, {"number": 11814, "title": "Fix error with default python path selection", "body": "When the env var USE_DEFAULT_PYTHON_LIB_PATH is set to 1, and there isn't PYTHON_LIB_PATH set, then the configure used to select the first entry that python itself returned.\r\n\r\nthis was broken recently. I suspect that it was a mistake rather than a deliberate choice.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "I made one small change to move the setting of the env var outside of the if/else so the contract of the if/else is more clear. \r\n\r\n@tensorflow-jenkins test this please", "the MacOS CPU test is an out of disk space problem on the build machine by the looks of things.\r\n"]}, {"number": 11813, "title": "DOC: Fix typo.", "body": "you could could be I/O bottlenecked.\r\nTO:\r\nyou could be I/O bottlenecked.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I was just signing the CLA while I was pushing the merge request. Anyway I have signed the CLA with my Google account and provided my Github account in the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 11812, "title": "InternalError: Blas GEMM launch failed ", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 16.04.2 LTS\r\nRelease:        16.04\r\n- **TensorFlow installed from (source or binary)**:\r\npip3 install tensorflow-gpu\r\n\r\n- **TensorFlow version (use command below)**:\r\nv1.2.0-5-g435cdfc 1.2.1\r\n\r\n- **Python version**: \r\n3.5\r\n\r\n- **CUDA/cuDNN version**:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Tue_Jan_10_13:22:03_CST_2017\r\nCuda compilation tools, release 8.0, V8.0.61\r\n\r\n- **GPU model and memory**:\r\n description: 3D controller\r\n          product: GK210GL [Tesla K80]\r\n          vendor: NVIDIA Corporation\r\n          physical id: 0\r\n          bus info: pci@99ba:00:00.0\r\n          version: a1\r\n          width: 64 bits\r\n          clock: 33MHz\r\n          capabilities: bus_master cap_list\r\n          configuration: driver=nvidia latency=0\r\n          resources: iomemory:100-ff iomemory:140-13f irq:24 memory:21000000-21ffffff \r\n          memory:1000000000-13ffffffff memory:1400000000-1401ffffff\r\n\r\n- **Code example**:\r\nestimator = KerasRegressor(build_fn=self.create_model_function,\r\n                                   input_dim=self.input_dim, output_dim=self.output_dim,\r\n                                   **self.model_parameters)\r\n\r\nparam_grid = {'epochs': [5]\r\n              ,'batch_size': [256]\r\n              ,'neurons': [[10, 10, 10]]\r\n              ,'dropout': [[0.0, 0.0]]}\r\n\r\ngrid = GridSearchCV(estimator=estimator, param_grid=param_grid, n_jobs=-1)\r\n\r\n\r\n### Describe the problem\r\nThe InternalError occurred when I fit a sklearn.GridSearchCV object.\r\nThe error occurred only if I use GPU and I I use GridSearch object. It works fine on CPU and on single model fitting (using Keras wrapper).\r\n\r\n\r\n### Error log\r\nFile \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/model_selection/_search.py\", line 945, in fit\r\n    return self._fit(X, y, groups, ParameterGrid(self.param_grid))\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/model_selection/_search.py\", line 564, in _fit\r\n    for parameters in parameter_iterable\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 728, in __call__\r\n    n_jobs = self._initialize_backend()\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 540, in _initialize_backend\r\n    **self._backend_args)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 311, in configure\r\n    self._pool = MemmapingPool(n_jobs, **backend_args)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 600, in __init__\r\n    super(MemmapingPool, self).__init__(**poolargs)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 420, in __init__\r\n    super(PicklingPool, self).__init__(**poolargs)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/pool.py\", line 168, in __init__\r\n    self._repopulate_pool()\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/pool.py\", line 233, in _repopulate_pool\r\n    w.start()\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/context.py\", line 267, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/popen_fork.py\", line 74, in _launch\r\n    code = process_obj._bootstrap()\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\r\n    self.run()\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 344, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\r\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\r\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\", line 238, in _fit_and_score\r\n    estimator.fit(X_train, y_train, **fit_params)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/keras/wrappers/scikit_learn.py\", line 136, in fit\r\n    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\r\n  File \"/home/aateam/Amplifon/amplifon-adv-planning/src/libs/amplifon_objects.py\", line 176, in create_test_model\r\n    model.add(Dense(neurons[0], input_dim=input_dim, activation=last_activation))\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/keras/models.py\", line 436, in add\r\n    layer(x)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/keras/engine/topology.py\", line 596, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/keras/layers/core.py\", line 838, in call\r\n    output = K.dot(inputs, self.kernel)\r\n  File \"/home/aateam/.conda/envs/amplifon-dev3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 978, in dot\r\n    out = tf.matmul(x, y)\r\n  File \"/home/aateam/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1816, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/home/aateam/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1217, in _mat_mul\r\n    transpose_b=transpose_b, name=name)\r\n  File \"/home/aateam/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/aateam/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/aateam/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(256, 32), b.shape=(32, 10), m=256, n=10, k=32\r\n         [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_dense_1_input_0_0/_15, dense_1/kernel/read)]]\r\n         [[Node: mul_1/_43 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_795_mul_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n", "comments": ["Make sure you have no other processes using the GPU running. Run `nvidia-smi` to check this.", "I runned the `nvidia-smi` command while just before the crash. Here the output:\r\n\r\n`Fri Jul 28 14:01:37 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | BD8C:00:00.0     Off |                    0 |\r\n| N/A   46C    P0    54W / 149W |  11439MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      6216    C   python                                       10888MiB |\r\n|    0      6217    C   python                                         383MiB |\r\n|    0      6218    C   python                                         163MiB |\r\n+-----------------------------------------------------------------------------+`\r\n\r\nThe GridSeachCV function creates 3 parallel jobs, is it correct?", "If multiple TensorFlow processes are used, either [`per_process_gpu_memory_fraction`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L21) or [`allow_growth`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L40) should be passed to the TensorFlow Session in a `ConfigProto` to prevent one process from using all the GPU memory. Preferably, multiple TensorFlow processes wouldn't be used at the same time.\r\n\r\n@fchollet I don't know Keras or sklearn, so I'm not sure how to do this with KerasRegressor. Do you know how to resolve this problem?", "I have the same problem when I use KerasClassifier and GridSearchCV for tuning the parameters.\r\n\r\n**System information**\r\n\r\n- OS: Linux 4.13.3-1-ARCH\r\n- TensorFlow: 1.3.0\r\n- Scikit-learn: 0.19.0\r\n- Keras: 2.0.8\r\n- Python: 3.6.2\r\n- GPU: GeForce GTX 1080 Ti\r\n- CUDA/cuDNN: release 8.0, V8.0.61\r\n\r\n**Code example**\r\n\r\n`model = KerasClassifier(build_fn=create_model, verbose=0, epochs=20, batch_size=1)`\r\n`optimizer = ['SGD', 'Adagrad', 'Adam']`\r\n`param_grid = dict(optimizer=optimizer)`\r\n`grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)`", "It looks like `GridSearchCV` uses `joblib`. If it attempts to pickle a Keras model (and thus a TF graph) for use in different processes, then that is potentially not going to be possible safely (@reedwm may be able to say more).\r\n\r\n> If multiple TensorFlow processes are used, either per_process_gpu_memory_fraction or allow_growth should be passed to the TensorFlow Session in a ConfigProto to prevent one process from using all the GPU memory. Preferably, multiple TensorFlow processes wouldn't be used at the same time.\r\n\r\nIt is possible to do this in Keras by creating your own session (with `per_process_gpu_memory_fraction` configured), then setting this session as the Keras session via `keras.backend.set_session(sess)`. Do this before creating your Keras model.", "> If it attempts to pickle a Keras model (and thus a TF graph) for use in different processes, then that is potentially not going to be possible safely (@reedwm may be able to say more).\r\n\r\nI'm not sure what you mean here. According to [this page](https://pythonhosted.org/joblib/parallel.html), `joblib` uses `fork()`, which, according to #5448, does not work well with TensorFlow.", "Did anyone fix this? \r\n\r\nWe are getting this error (Blas SGEMM launch failed) while trying to learn word2vec embeddings. The code is pretty much the one in the TF tutorial (https://www.tensorflow.org/tutorials/word2vec). It runs without any problem on a regular GPU machine. However, we are trying to run it in a CodaLab setup (http://codalab.org/). The TF docker image is the official one from https://hub.docker.com/r/tensorflow/tensorflow/ specified as: tensorflow/tensorflow:1.0.0-gpu\r\n\r\nSince we may be potentially sharing the GPUs, we have: \r\ncfg = tf.ConfigProto(allow_soft_placement=True )\r\ncfg.gpu_options.allow_growth = True\r\nfor the config used for the session.\r\n\r\nThe full error can be seen here: \r\nhttps://codalab.rollme.ml.cmu.edu/bundles/0x3646a0f2dc614a6a971cbeadf753cf15/", "@rosecatherinek I am not sure what CodaLab is or how it works, but I doubt it's `fork()`ing the TensorFlow so this seems like a different issue. \r\n\r\nFrom the logs, I see the line\r\n```\r\nfailed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n```\r\n\r\nThis seems to indicate TensorFlow cannot initialize cuBLAS within the codalab docker container. Maybe try asking for support from the codalab authors? This is difficult to debug for us since we do not use codalab.\r\n\r\n@zheng-xq, @yzhwang, any ideas what the issue could be?", "Thanks for pointing that out, @reedwm ! We hadn't noticed that at first. We are trying to fix that now. Thanks again!", "has anyone had any luck with this? I also get the error.\r\nSame platform as paolof89. \r\nCPU works fine.\r\n\r\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(1, 135), b.shape=(135, 512), m=1, n=512, k=135\r\n\t [[Node: stacked_cell_1/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/lstm_cell/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](stacked_cell_1/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/lstm_cell/concat, stacked_cell_1/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/lstm_cell/MatMul/Enter)]]\r\n\t [[Node: Mean/_119 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3495_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nmore information:\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:551] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_EXECUTION_FAILED\r\n2017-12-20 14:47:27.533452: I tensorflow/stream_executor/stream.cc:4662] stream 0x5637f2c5d720 did not memzero GPU location; source: 0x7fc40affbf90", "Try 'Shutdown' the running notebooks which uses your GPU. Restart the kernel. \r\nRun the code again.. This time it should work.", "@jidhu-mohan I only have a single python process running when I get this error", "Hi, \r\nI experienced the same error when executing the code in tutorial [tf.estimator](https://www.tensorflow.org/get_started/get_started)\r\n\r\nHere's the error message\r\n\r\n```\r\n$ python tfestimator.py\r\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\<username>\\AppData\\Local\\Temp\\tmpdlwf3cjk\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1306, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Anaconda3\\lib\\contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMV launch failed:  m=1, n=4\r\n         [[Node: linear/linear_model/x/weighted_sum = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](linear/linear_model/x/Reshape, linear/linear_model/x/weights)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tfestimator.py\", line 29, in <module>\r\n    estimator.train(input_fn=input_fn, steps=1000)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 241, in train\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 686, in _train_model\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 518, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 862, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 818, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 972, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 818, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMV launch failed:  m=1, n=4\r\n         [[Node: linear/linear_model/x/weighted_sum = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](linear/linear_model/x/Reshape, linear/linear_model/x/weights)]]\r\n\r\nCaused by op 'linear/linear_model/x/weighted_sum', defined at:\r\n  File \"tfestimator.py\", line 29, in <module>\r\n    estimator.train(input_fn=input_fn, steps=1000)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 241, in train\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 630, in _train_model\r\n    model_fn_lib.ModeKeys.TRAIN)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 615, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\linear.py\", line 316, in _model_fn\r\n    config=config)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\linear.py\", line 89, in _linear_model_fn\r\n    units=head.logits_dimension)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py\", line 321, in linear_model\r\n    column, builder, units, weight_collections, trainable))\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py\", line 1376, in _create_dense_column_weighted_sum\r\n    return math_ops.matmul(tensor, weight, name='weighted_sum')\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1844, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1289, in _mat_mul\r\n    transpose_b=transpose_b, name=name)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): Blas GEMV launch failed:  m=1, n=4\r\n         [[Node: linear/linear_model/x/weighted_sum = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](linear/linear_model/x/Reshape, linear/linear_model/x/weights)]]\r\n\r\n2018-01-12 11:12:13.512132: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-12 11:12:13.512150: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-12 11:12:13.993976: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce 940MX\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.2415\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.66GiB\r\n2018-01-12 11:12:13.993992: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0\r\n2018-01-12 11:12:13.993996: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y\r\n2018-01-12 11:12:13.994018: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0)\r\n2018-01-12 11:12:17.243570: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018-01-12 11:12:17.243585: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\stream.cc:1756] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n```\r\n\r\nMy apology, should I write a new issue?\r\nThank you\r\n\r\n", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "use nvidia-smi to see if gpu is overload, and use 'kill -9 pid' to kill the process.", "To be clear, tensorflow will try (by default) to consume all available GPUs. It cannot be run with other programs also active. Closing. Feel free to reopen if this is actually another problem.", "I ported from pytorch to tensorflow and had an \"import torch\" . No idea how that caused the crash, but it did.", "It's possible that torch is reserving memory up front. Good to know.", "In my case, removing `n_jobs=-1` worked.", "> Try 'Shutdown' the running notebooks which uses your GPU. Restart the kernel.\r\n> Run the code again.. This time it should work.\r\n\r\nThis worked for me. If you're using Jupyter, try shutting the kernel (server) down and restarting it. ", "> Try 'Shutdown' the running notebooks which uses your GPU. Restart the kernel.\r\n> Run the code again.. This time it should work.\r\n\r\nThis worked for me. I was running multiple python consoles in PyCharm and making sure there is just one seems to help.", "> In my case, removing `n_jobs=-1` worked.\r\n\r\nIt worked for me. Thank you!", "I got \"Blas GEMM launch failed\" while the virtualbox(genymotion) vm is running.", "> \r\n> \r\n> Try 'Shutdown' the running notebooks which uses your GPU. Restart the kernel.\r\n> Run the code again.. This time it should work.\r\n\r\nClosing (shutting down) all other notebooks and restarting the current notebook resolved the issue. Thanks for the suggestion.", "Since the GPU memory is not enough. So here is one of the solution.\r\n```\r\nimport keras.backend.tensorflow_backend as KTF\r\nimport tensorflow as tf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True   \r\nsess = tf.Session(config=config)\r\n\r\nKTF.set_session(sess)\r\n```", "This is happening to me on my Windows machine using Keras.\r\nIf I use my Keras model as inference, I don't have a problem.\r\nIf I retrain the model and then try and use it as inference, I get this error.\r\nI think there's some dangling process hanging around on the GPU after the training phase.  Unfortunately, my Nvidia driver is not compatible with `nvidia-smi` so I'm unsure if that's true.\r\n\r\nWhen I restart my Windows machine and it fixes the problem but it's an annoying workaround. \r\nI've tried deleting the Nvidia cache folder in %Temp% but that does not fix it.", "this is happening to me on my windovs while i'm usuing tf\r\nso problem is really multi kernels with Jupyter\r\nshutdown all of your JupyterNotebooks and restart the kernell", "> Try 'Shutdown' the running notebooks which uses your GPU. Restart the kernel.\r\n> Run the code again.. This time it should work.\r\n\r\nThat's it! \r\nWhen I used jupyter notebook, meanwhile I used terminal to run the same code, the error above occured.", "Adding this on top of your code works for me in tensoflow version 2.0.0\r\n\r\nimport tensorflow as tf\r\nimport keras.backend.tensorflow_backend as KTF\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth=True   \r\nsession = tf.compat.v1.Session(config=config)\r\n\r\nKTF.set_session(session)", "Working for me on tf 2.1.0 (ref: https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth)\r\n\r\nimport tensorflow as tf\r\nphysical_devices = tf.config.list_physical_devices('GPU') \r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)", "This working for me:\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Set to -1 if CPU should be used CPU = -1 , GPU = 0\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ncpus = tf.config.experimental.list_physical_devices('CPU')\r\n\r\nif gpus:\r\n    try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n    except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\nelif cpus:\r\n    try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        logical_cpus= tf.config.experimental.list_logical_devices('CPU')\r\n        print(len(cpus), \"Physical CPU,\", len(logical_cpus), \"Logical CPU\")\r\n    except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n```", "Restarting my docker container did the trick for me:\r\n```\r\ndocker run -d --gpus all -p 8888:8888 tensorflow/tensorflow:latest-gpu-py3-jupyter\r\nf4f5fac66b05\r\ndocker restart f4f\r\n```", "Reinstall the compatible **NVIDIA** driver.\r\n\r\n> In the ubuntu _Software centre_ change the driver in **Additional Drivers** panel."]}, {"number": 11811, "title": "fix memory leak, and remove duplicated implements", "body": "fix memory leak, and remove duplicated implements.", "comments": ["Can one of the admins verify this patch?", "The previous code was added by https://github.com/tensorflow/tensorflow/commit/00b79b47939beda85c22d5f603b2904596c624d2 with the explicit purpose of reducing code size, so you would have to verify that the benchmark and the code size are not affected by this change.  Can you verify this?  Thanks!", "How should I run `status_test.cc` alone and execute benchmark? Please give me some advise. \r\n\r\nIn addition, I want to run all test cases or dependent test cases every time before submit the code, but the compile time is too long, Do you have any good suggestions?\r\n", "I believe if you do\r\n\r\n```\r\nbazel build -c opt tensorflow/core:status_test  (or maybe it's core:lib_core_status_test)\r\n```\r\n\r\nthen you can run:\r\n\r\n```\r\nbazel-bin/tensorflow/core/status_test --benchmarks=all \r\n```\r\n\r\nto run the benchmark, and you can compare before and after your change to see the difference in performance.\r\n\r\nMy belief is that it is very intentional to avoid constructing empty string objects, and to instead leak memory, since the program is about to crash anyway.  So I'm inclined to want to leave the code as is, unless we can prove that constructing the empty strings is not harmful.\r\n\r\nAs for all test cases: it's hard to test all dependencies without recompiling everything :(.  That's why our test infrastructure can be useful.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "OK, I give up this commit, and thanks for you help. @vrv\r\n"]}, {"number": 11810, "title": "Momentum, Adam, and other optimizers don't work for variable input/output sizes", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04.2\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.2.1, 1.3.0-rc0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.0\r\n- **CUDA/cuDNN version**: 8.0/5.1.10\r\n- **GPU model and memory**: GeForce GTX Titan X, 12GB\r\n- **Exact command to reproduce**: N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nNone of the optimizers other than GradientDescentOptimizer seem to work when the network is working with inputs and outputs of variable sizes (in FCN, input size === output size, defined by [BATCH_SIZE, None, None, CHANNELS]). Below are the error I get when using various optimizers: \r\nMomentum Optimizer: `AttributeError: 'Tensor' object has no attribute 'is_fully_defined'`\r\n\r\nRMSPropOptimizer: `ValueError: Shape of a new variable (expanding/step4/deconv/bias/RMSProp/) must be fully defined, but instead was <unknown>.`\r\n\r\nAdamOptimizer: `AttributeError: 'Tensor' object has no attribute 'is_fully_defined'`\r\n\r\nGradientDescentOptimizer: Works!\r\n\r\n**Note that these optimizers had worked for the same code when I was using TF 1.0.1**\r\n\r\n### Source code / logs\r\nThis is the stack trace for MomentumOptimizer\r\n\r\n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    323 \r\n    324     return self.apply_gradients(grads_and_vars, global_step=global_step,\r\n--> 325                                 name=name)\r\n    326 \r\n    327   def compute_gradients(self, loss, var_list=None,/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)\r\n    444                        ([str(v) for _, _, v in converted_grads_and_vars],))\r\n    445     with ops.control_dependencies(None):\r\n--> 446       self._create_slots([_get_variable_for(v) for v in var_list])\r\n    447     update_ops = []\r\n    448     with ops.name_scope(name, self._name) as name:/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/momentum.pyc in _create_slots(self, var_list)\r\n     64   def _create_slots(self, var_list):\r\n     65     for v in var_list:\r\n---> 66       self._zeros_slot(v, \"momentum\", self._name)\r\n     67 \r\n     68   def _prepare(self):\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in _zeros_slot(self, var, slot_name, op_name)\r\n    764     named_slots = self._slot_dict(slot_name)\r\n    765     if _var_key(var) not in named_slots:\r\n--> 766       named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)\r\n    767     return named_slots[_var_key(var)]\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.pyc in create_zeros_slot(primary, name, dtype, colocate_with_primary)\r\n    168   slot_shape = (slot_shape if slot_shape.is_fully_defined()\r\n    169                 else array_ops.shape(primary.initialized_value()))\r\n--> 170   if slot_shape.is_fully_defined():\r\n    171     initializer = init_ops.zeros_initializer(dtype)\r\n    172     return create_slot_with_initializer(AttributeError: 'Tensor' object has no attribute 'is_fully_defined'", "comments": ["The optimizers all work for network whose inputs/outputs are partially defined. I can train FCN with unknown input shapes with adam without any problems.\r\n\r\nYour error log indicates that you have some trainable variables that are partially defined. This is unsupported and you probably have done something wrong in your code.", "@ppwwyyxx , thank you for responding. As far as I know, all my trainable parameters are shape-defined. My only 'partially' defined objects are the placeholders for input images and true labels. But, I'll go through it again as you suggested.\r\n\r\nHowever, at this moment, the same code (with Adam) runs on a machine with TF1.0, while fails on TF1.2. It also worked before I upgraded:\r\n> Note that these optimizers had worked for the same code when I was using TF 1.0.1\r\n\r\nIf you know anything that might have changed concerning optimizers or shape definitions, it would be very helpful to me in confirming or invalidating this behaviour. ", "[This line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L169) in tensorflow seems to be causing the problem:\r\n```python\r\n slot_shape = (slot_shape if slot_shape.is_fully_defined()\r\n                else array_ops.shape(primary.initialized_value()))\r\n  if slot_shape.is_fully_defined():\r\n```\r\nThe first line return either a TensorShape or a Tensor, depending on the condition. This is probably a bug anyway. In the above case it somehow returns a Tensor, which causes AttributeError in the second line.\r\n\r\nSince the first line gives a tensor, it means this function gets some tensor or variable with undefined shape. It will be helpful to know what that is. Probably related to `expanding/step4/deconv/bias/RMSProp` in your log.\r\n\r\nAnyway it will be easier if you could post some reproducible code. As I said I can train with unknown shapes without any problems, so the problem might lie somewhere more subtle.\r\n", "Yes, that line indeed looks fishy. There were a few [changes](https://github.com/tensorflow/tensorflow/commit/8f75f24a52a7645a4ef93cbe244d45d30d5e4a3f) made to the slot_creator.py in March '17. I got as far as this, and then lost the trail. \r\n\r\nUnfortunately, I cannot post my raw code here. I'll try to build a simpler block where this error is reproducible. Again, thank you!", "@ppwwyyxx : Here you go! Find the partially shape-defined placeholders (input_pl and mask_pl) toward the end of the code block. In my environment, this block does give me the above mentioned errors. \r\n\r\nNote that the shape of all trainable variables (`w` and `b` here) is absolutely defined.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n### Definition routines ###\r\n\r\nconv_filter_size = 3\r\ndeconv_filter_size = 2\r\n\r\ndef _conv_block(x, channels_in, channels_out):\r\n    stddev = tf.cast(tf.sqrt(tf.divide(2,((conv_filter_size**2 * channels_in)))), tf.float32)\r\n    with tf.variable_scope('conv'):\r\n        shape = [conv_filter_size, conv_filter_size, channels_in, channels_out]\r\n        w = tf.get_variable('weights', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\r\n        b = tf.Variable(tf.zeros([channels_out]), name = 'bias')\r\n        conv = tf.nn.relu(tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='VALID', name='conv') + b)\r\n    return conv\r\n\r\n\r\ndef _upconv_block(x, channels_in, channels_out):\r\n    output_shape = [tf.shape(x)[0], 2*tf.shape(x)[1], 2*tf.shape(x)[2], channels_in]\r\n    stddev = tf.cast(tf.sqrt(tf.divide(2,((deconv_filter_size**2 * channels_in)))), tf.float32)\r\n    with tf.variable_scope('deconv'):\r\n        shape = [deconv_filter_size, deconv_filter_size, channels_in, channels_out]\r\n        w = tf.get_variable('weights_up', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\r\n        b = tf.Variable(tf.zeros([channels_out]), name='bias', validate_shape=False)\r\n        upconv = tf.nn.relu(tf.nn.conv2d_transpose(x, w, output_shape, strides=[1,2,2,1], padding='VALID', name='upconv') + b)\r\n    return upconv\r\n\r\ndef inference(image_batch):\r\n    with tf.variable_scope('contracting'):\r\n        with tf.variable_scope('step1'):\r\n            conv_block_1 = _conv_block(image_batch, 1, 32)\r\n            conv_pool_1 = tf.nn.max_pool(conv_block_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\r\n        with tf.variable_scope('step2'):\r\n            conv_block_2 = _conv_block(conv_pool_1, 32, 64)\r\n            conv_pool_2 = tf.nn.max_pool(conv_block_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\r\n        with tf.variable_scope('step3'):\r\n            conv_block_3 = _conv_block(conv_pool_2, 64, 128)\r\n    with tf.variable_scope('expanding'):\r\n        with tf.variable_scope('step2_up'):\r\n            upconv_block_1 = _upconv_block(conv_block_3, 128, 128)\r\n        with tf.variable_scope('step1_up'):\r\n            upconv_block_2 = _upconv_block(upconv_block_1, 128, 128)\r\n    with tf.variable_scope('scoring'):\r\n        with tf.variable_scope('conv'):\r\n            channels_in = 128\r\n            channels_out = 2 # Binary-class segmentation\r\n            shape = [1, 1, channels_in, channels_out]\r\n            stddev = tf.sqrt(2.0 / (conv_filter_size**2 * channels_in))\r\n            w = tf.get_variable('weights_scoring', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\r\n            b = tf.Variable(tf.zeros([channels_out]), name='bias')\r\n            score = tf.nn.conv2d(upconv_block_2, w, strides=[1, 1, 1, 1], padding='VALID', name='conv') + b\r\n            prediction = score\r\n    return prediction\r\n\r\ndef loss(mask, prediction):\r\n    mask = tf.cast(mask, tf.int32) \r\n    loss_im = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=mask, logits=prediction, name='cross_entropy_softmax')\r\n    loss = tf.reduce_mean(loss_im)\r\n    return loss\r\n\r\ndef training(loss, learning_rate):\r\n    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.99) ### Fails\r\n    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate) ### Fails\r\n    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) ### Fails\r\n    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) ### Succeeds!\r\n    train_op = optimizer.minimize(loss)\r\n    return train_op\r\n\r\n\r\n### Graph contruction ###\r\n\r\nlearning_rate = 1e-6\r\nwith tf.Graph().as_default():\r\n    # PLACEHOLDERS\r\n    input_pl = tf.placeholder(tf.float32, shape=(1, None, None, 1))\r\n    mask_pl = tf.placeholder(tf.float32, shape=(1, None, None))\r\n    # OPs\r\n    prediction_op = inference(input_pl)\r\n    loss_op = loss(mask_pl, prediction_op)\r\n    train_op = training(loss_op, learning_rate)\r\n```", "Why do you use `validate_shape=False`? This gives you a variable of unknown shape.", "Wow! I did not know that. That is probably a remnant of my earlier attempts to make it shape-independent. It now works. Thank you! I'll close this now unless you think we should report this:\r\n```\r\nslot_shape = (slot_shape if slot_shape.is_fully_defined()\r\n                else array_ops.shape(primary.initialized_value()))\r\n if slot_shape.is_fully_defined():\r\n```", "Those two lines still look suspicious. Any comments? @aselle ", "This is related to (or the same as) #5972."]}, {"number": 11809, "title": "Setting weights for different layers in a CNN", "body": "I have a number of models which I have trained in `lasagne`. I have weights saved for all those models. As of now I want to switch to `tensorflow`. I can build the whole architecture in `tf` but I want to load the weights from my hard disk. How can I set the pre-trained weights for the network?", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\n\r\nDoes this help? \r\nhttps://github.com/fchollet/keras/wiki/Converting-convolution-kernels-from-Theano-to-TensorFlow-and-vice-versa\r\nThis really is a S/O question however and you should post there instead."]}, {"number": 11808, "title": "tf.contrib.streaming_mean_squared_error returns incorrect result", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18\r\n- **TensorFlow installed from (source or binary)**: Binary (pip)\r\n- **TensorFlow version (use command below)**: v1.3.0.0rc0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n`tf.contrib.streaming_mean_squared_error` returns wonky results. It returns the current mean square error tensor and an update op. According to the documentation their evaluated values should match. They don't, and the values of the latter tend to get weird. A minimal example should demonstrate.\r\n\r\n### Source code / logs\r\nExample: mean squared error between two arrays whose difference is an array of ones. The MSE should consistently be 1. It isn't though.\r\n\r\n```python\r\nsess = tf.InteractiveSession()\r\na = tf.constant(np.arange(3,7))\r\nb = tf.constant(np.arange(2,6))\r\ne = tf.contrib.metrics.streaming_mean_squared_error(a,b)\r\ninit_op = tf.local_variables_initializer(); sess.run(init_op)\r\nsess.run(e) # (1.0, 1.0)\r\nsess.run(e) # (1.0, 1.0)\r\nsess.run(e) # (1.5, 1.0) !!!\r\nsess.run(e) # (1.0, 1.0)\r\n```\r\nAfter having a brief look at the code, I don't see why the returned values don't match. The [docs](https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/docs_src/api_guides/python/contrib.metrics.md) speak of _finalizing_ the value but it just looks like `total/count` is returned.\r\n", "comments": ["@alextp Does this look like a bug, or unclear documentation?", "The metrics functions return two operations: one which computes the metrics based on the current value of a variable (with no dependency on its updating) and another which updates the variables and returns the computed value based on the new values. Your session.run call is running both of those, and the racy one can return different values.\r\n\r\nDo `_, e = tf.contrib.metrics...` instead to always use the safe version.", "A race condition! Good stuff, makes sense. Might want to update the docs somehow though because I remember feeling like I should use the first one to get the updated value. "]}, {"number": 11807, "title": "tensorflow build error in Illegal ambiguous match on configurable attribute \"copts\" in //tensorflow/python:gen_math_ops_py_wrappers_cc:", "body": "$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\nERROR: /root/tools/tensorflow-master/tensorflow/python/BUILD:1166:1: Illegal ambiguous match on configurable attribute \"copts\" in //tensorflow/python:gen_math_ops_py_wrappers_cc:\r\n@local_config_cuda//cuda:using_clang\r\n@local_config_cuda//cuda:using_nvcc\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\nINFO: Elapsed time: 0.190s", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Hi, I have a similar error\r\n\r\n```bash\r\n/home/nvidia/tensorflow/tensorflow/python/BUILD:1131:1: Illegal ambiguous match on configurable attribute \"copts\" in //tensorflow/python:gen_io_ops_py_wrappers_cc:\r\n@local_config_cuda//cuda:using_clang\r\n@local_config_cuda//cuda:using_nvcc\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\nINFO: Elapsed time: 1.099s\r\n```\r\nMy environment is:\r\nHardware: Jetson TX1\r\nTensorflow: v1.3.0-rc2\r\n```bash\r\nLinux tegra-ubuntu 4.4.38 #1 SMP PREEMPT Sun Aug 6 15:54:28 UTC 2017 aarch64 aarch64 aarch64 GNU/Linux\r\n```\r\n\r\nThanks for your help\r\n", "i have Solve it   by use python3.6  \uff0cbut python2.7  is error   \u3002centos7   tensorflow is newest", "@martinwicke can you take a look?", "@av8ramit This appears to be on 1.3RC2. Is it one of the know bazel issues with 0.5.3?\r\n\r\n@eddytrex what version of bazel is this?", "It is not one of the known issues, but I'm curious to see if this is a regression in rc2 or a new issue with 0.5.3\r\n\r\n@eddytrex or @linyia01 please reply with the bazel versions.", "bazel 0.5.2 @av8ramit ", "Build label: 0.5.0- (@non-git)\r\n", "I am seeing the same behavior with bazel 0.5.3\r\n\r\nUbuntu 16.04\r\nPython 3.5.2", "I'm unfortunately unable to reproduce this with python 2.7, rc2, and 0.5.2 Still investigating.", "python3.6      bazel 0.5.2    centos7   tf:1.3.0-rc0    @av8ramit       ", "The guilty lines are in tensorflow.bzl:\r\n\r\n```\r\ndef _cuda_copts():\r\n  \"\"\"Gets the appropriate set of copts for (maybe) CUDA compilation.\r\n\r\n    If we're doing CUDA compilation, returns copts for our particular CUDA\r\n    compiler.  If we're not doing CUDA compilation, returns an empty list.\r\n\r\n    \"\"\"\r\n  return cuda_default_copts() + select({\r\n      \"//conditions:default\": [],\r\n      \"@local_config_cuda//cuda:using_nvcc\": ([\r\n          \"-nvcc_options=relaxed-constexpr\",\r\n          \"-nvcc_options=ftz=true\",\r\n      ]),\r\n      \"@local_config_cuda//cuda:using_clang\": ([\r\n          \"-fcuda-flush-denormals-to-zero\",\r\n      ]),\r\n  })\r\n```\r\n\r\nThe reproduce the behavior it seems like you'd need to build CUDA support using Clang.", "[This answer](https://stackoverflow.com/a/45644766/825785) at StackOverflow would seem to support that this interference in the configuration between using CUDA and using Clang is the problem.\r\n\r\nBazel fails with an error in exactly the way described by this issue when multiple configurations match in a select with no obvious tie-breakers.\r\n\r\nhttps://docs.bazel.build/versions/master/be/functions.html#select\r\n\r\nThis would suggest that the problem may lie in configure.py allowing configuration choices which trigger this confusion.", "The issue here, which remains a problem on `master` as of commit 89f15928293f62b63404ffbdabbb6a3b07274ff8, is that when you:\r\n\r\n```\r\nexport TF_CUDA_CLANG=1\r\n./configure\r\n```\r\n\r\n...then we create broken bazel `select` blocks like this:\r\n\r\n```\r\ndef _cuda_copts():\r\n  \"\"\"Gets the appropriate set of copts for (maybe) CUDA compilation.\r\n\r\n    If we're doing CUDA compilation, returns copts for our particular CUDA\r\n    compiler.  If we're not doing CUDA compilation, returns an empty list.\r\n\r\n    \"\"\"\r\n  return cuda_default_copts() + select({\r\n      \"//conditions:default\": [],\r\n      \"@local_config_cuda//cuda:using_nvcc\": ([\r\n          \"-nvcc_options=relaxed-constexpr\",\r\n          \"-nvcc_options=ftz=true\",\r\n      ]),\r\n      \"@local_config_cuda//cuda:using_clang\": ([\r\n          \"-fcuda-flush-denormals-to-zero\",\r\n      ]),\r\n  })\r\n```\r\n\r\nA build with this configuration reliably fails as documented above.  We can prove to ourselves this must be the issue by \"correcting\" the configuration and hard-coding the result we want for each `select`:\r\n\r\n```\r\ndiff --git a/tensorflow/core/platform/default/build_config/BUILD b/tensorflow/core/platform/default/build_config/BUILD\r\nindex f746b15fe..6e9da1179 100644\r\n--- a/tensorflow/core/platform/default/build_config/BUILD\r\n+++ b/tensorflow/core/platform/default/build_config/BUILD\r\n@@ -34,11 +34,7 @@ tf_cuda_library(\r\n     deps = [\r\n         \"//tensorflow/stream_executor\",\r\n     ] + select({\r\n-        \"//tensorflow:using_cuda_clang\": [\"//tensorflow/stream_executor:cuda_platform\"],\r\n-        \"//tensorflow:using_cuda_nvcc\": [\"//tensorflow/stream_executor:cuda_platform\"],\r\n-        \"//tensorflow:using_cuda_clang_with_dynamic_build\": [],\r\n-        \"//tensorflow:using_cuda_nvcc_with_dynamic_build\": [],\r\n-        \"//conditions:default\": [],\r\n+        \"//conditions:default\": [\"//tensorflow/stream_executor:cuda_platform\"],\r\n     }) + select({\r\n         \"@local_config_cuda//cuda:darwin\": [\"IOKit\"],\r\n         \"//conditions:default\": [],\r\ndiff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl\r\nindex 3001a3747..2398abd34 100644\r\n--- a/tensorflow/tensorflow.bzl\r\n+++ b/tensorflow/tensorflow.bzl\r\n@@ -725,12 +725,7 @@ def _cuda_copts():\r\n \r\n     \"\"\"\r\n   return cuda_default_copts() + select({\r\n-      \"//conditions:default\": [],\r\n-      \"@local_config_cuda//cuda:using_nvcc\": ([\r\n-          \"-nvcc_options=relaxed-constexpr\",\r\n-          \"-nvcc_options=ftz=true\",\r\n-      ]),\r\n-      \"@local_config_cuda//cuda:using_clang\": ([\r\n+      \"//conditions:default\": ([\r\n           \"-fcuda-flush-denormals-to-zero\",\r\n       ]),\r\n   })\r\ndiff --git a/third_party/gpus/cuda/build_defs.bzl.tpl b/third_party/gpus/cuda/build_defs.bzl.tpl\r\nindex ca8bbc1ee..6d9ad452d 100644\r\n--- a/third_party/gpus/cuda/build_defs.bzl.tpl\r\n+++ b/third_party/gpus/cuda/build_defs.bzl.tpl\r\n@@ -7,9 +7,7 @@ def if_cuda(if_true, if_false = []):\r\n \r\n     \"\"\"\r\n     return select({\r\n-        \"@local_config_cuda//cuda:using_nvcc\": if_true,\r\n-        \"@local_config_cuda//cuda:using_clang\": if_true,\r\n-        \"//conditions:default\": if_false\r\n+        \"//conditions:default\": if_true\r\n     })\r\n \r\n \r\ndiff --git a/third_party/toolchains/gpus/cuda/build_defs.bzl b/third_party/toolchains/gpus/cuda/build_defs.bzl\r\nindex badaf4301..1b7aac066 100644\r\n--- a/third_party/toolchains/gpus/cuda/build_defs.bzl\r\n+++ b/third_party/toolchains/gpus/cuda/build_defs.bzl\r\n@@ -10,9 +10,7 @@ def if_cuda(if_true, if_false = []):\r\n \r\n     \"\"\"\r\n     return select({\r\n-        \"@local_config_cuda//cuda:using_nvcc\": if_true,\r\n-        \"@local_config_cuda//cuda:using_clang\": if_true,\r\n-        \"//conditions:default\": if_false\r\n+        \"//conditions:default\": if_true\r\n     })\r\n```\r\n\r\nThat's a workaround, and clearly not the right way to fix it, but it does allow the build to get past this issue (and then, in my testing with `TF_CUDA_CLANG=1`, to fail in myriad other places for unrelated reasons).", "I tried to build with opencl support and got this error \r\n\r\n` Illegal ambiguous match on configurable attribute \"deps\" in //tensorflow/core/kernels:compare_and_bitpack_op:\r\n@local_config_sycl//sycl:using_sycl_ccpp\r\n@local_config_sycl//sycl:using_sycl_trisycl\r\nMultiple matches are not allowed unless one is unambiguously more specialized.INFO: Elapsed time: 1.099s, Critical Path: 0.00s\r\nINFO: Elapsed time: 1.099s, Critical Path: 0.00s\r\n`\r\nwhat is needed to fix this ", "@av8ramit, could you take a look again. The @traviscross  comment seems to indicate that we need to handle the USE_CLANG case differently to make things work.", "I am seeing the same error with bazel 0.5.4\r\npython 2.7    centos7.3     tensorflow:v1.3    cuda8.0     cudnn6.0    GTX1080\r\n", "When you do config, just say NO for using Clang as CUDA compiler. That solved my problem. ", "So I just figured out that doing `--config=cuda_clang` works, but setting environment `TF_CUDA_CLANG=1` with `--config=cuda` does not. Seems like the solution is to make the latter case work and then the 0/1 for `TF_CUDA_CLANG` could toggle using nvcc/clang.", "I resolved this by not selecting clang in the nofigure step.\r\nJust setup using default when asked for, \r\n\r\n> Do you want to use clang as CUDA compiler? [y/N]:\r\n\r\nthe configure script should automatically pick nvcc\r\n", "@linyia01 Hi, does this issue still exist ? If so, can you please try the workaround suggested by @anilmaddala ", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue due to staleness. Please use the latest version of TensorFlow and build again. Feel free to reopen if the issue still persists. Thanks!", "What worked for me is specifying `--config=cuda_clang` (instead of `--config=cuda`).\r\n\r\nCompiling GPU code with clang can succeed in some instances where using nvcc does not.", "--config=cuda_clang ,sure,I have to check it out"]}, {"number": 11806, "title": "sparse ClusterSpec fails when using tf.cond", "body": "Here's the minimal code to reproduce.\r\non machine 1 and machine 2\r\n```python\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ncluster_spec = tf.train.ClusterSpec({\r\n  \"a\": { 0: \"machine1:8000\" },\r\n  \"b\": { 0: \"machine2:8001\" },\r\n})\r\njobname = sys.argv[1]\r\ntaskid = int(sys.argv[2])\r\nserver = tf.train.Server(cluster_spec, jobname, taskid)\r\n\r\nwith tf.device(\"/job:a/task:0/cpu:0\"):\r\n  queue = tf.FIFOQueue(\r\n    capacity=100, dtypes=[tf.int64],\r\n    shapes=[[]], shared_name=\"a_queue\", name=\"a_queue\")\r\n\r\nif jobname == \"a\" and taskid == 0:\r\n  enqueue_op = queue.enqueue(10)\r\n  sess = tf.Session(server.target)\r\n  while True:\r\n    sess.run(enqueue_op)\r\nelse:\r\n  dequeue_op = queue.dequeue()\r\n  sess = tf.Session(server.target)\r\n  while True:\r\n    print(sess.run(dequeue_op))\r\n```\r\n\r\non machine 3:\r\n\r\n```python\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ncluster_spec = tf.train.ClusterSpec({\r\n  \"a\": { 0: \"machine1:8000\" },\r\n  \"b\": { 1: \"machine3:8001\" },\r\n})\r\njobname = sys.argv[1]\r\ntaskid = int(sys.argv[2])\r\nserver = tf.train.Server(cluster_spec, jobname, taskid)\r\n\r\nwith tf.device(\"/job:a/task:0/cpu:0\"):\r\n  queue = tf.FIFOQueue(\r\n    capacity=100, dtypes=[tf.int64],\r\n    shapes=[[]], shared_name=\"a_queue\", name=\"a_queue\")\r\n\r\nif jobname == \"a\" and taskid == 0:\r\n  enqueue_op = queue.enqueue(10)\r\n  sess = tf.Session(server.target)\r\n  while True:\r\n    sess.run(enqueue_op)\r\nelse:\r\n  with tf.device(\"/job:b/task:1\"):\r\n    out = queue.dequeue()\r\n    queue_b = tf.FIFOQueue(capacity=100, dtypes=[tf.int64], shapes=[[]], name=\"b_queue\")\r\n    # 1.\r\n    # enq = queue_b.enqueue(out)\r\n    # no_op = tf.no_op()\r\n    # out = tf.cond(tf.equal(out, 10), lambda: enq, lambda: no_op)\r\n    # 2.\r\n    out = tf.cond(tf.equal(out, 10), lambda: queue_b.enqueue(out), lambda: tf.no_op())\r\n\r\n  sess = tf.Session(server.target)\r\n  while True:\r\n    print(sess.run(out))\r\n```\r\n\r\nOn machine3, it crashes complaining\r\n\r\n```shell\r\ntensorflow.python.framework.errors_impl.InternalError: No worker known as /job:b/replica:0/task:1\r\n\t [[Node: cond/pred_id_S5 = _HostRecv[client_terminated=false, recv_device=\"/job:a/replica:0/task:0/cpu:0\", send_device=\"/job:b/replica:0/task:1/gpu:0\", send_device_incarnation=720279685140440577, tensor_name=\"edge_8_cond/pred_id\", tensor_type=DT_BOOL, _device=\"/job:a/replica:0/task:0/cpu:0\"]()]]\r\n```\r\nThere is no problem if the true_fn and false_fn just returns a already constructed op, like in the commented code.", "comments": ["Can [reproduce](https://github.com/yaroslavvb/stuff/tree/master/mavelin) on my laptop, maybe @saeta has an idea? Essentially a worker named `/job:b/replica:0/task:1` is complaining that it doesn't know about worker named `/job:b/replica:0/task:1`\r\n\r\n(ps: generated graph also has unsatisfiable colocation constraints, but that issue is unrelated to the error here)", "Brennan @saeta , could you please take a look?", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=11806\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=11806\">No</a>\n"]}, {"number": 11805, "title": "Cannot build Tensorflow: 'GLIBCXX_3.4.21' not found", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**: 8.0/5.0\r\n- **GPU model and memory**: GTX1080, 8G\r\n- **Exact command to reproduce**: bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n### Describe the problem\r\n\r\nI cannot build Tensorflow from source. I have gcc libraries installed in \"/opt/ohpc/pub/compiler/gcc/5.4.0/lib64\". But the installer was not able to pick it up. It always tries to use the one in \"/usr/lib64\". I have set \"LDFLAGS\" \"LD_LIBRARY_PATH\", etc. Nothing works.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n$ bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nWARNING: ignoring http_proxy in environment.\r\nWARNING: Output base '/home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nWARNING: /home/kai/test/tensorflow-1.2.1/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /home/kai/test/tensorflow-1.2.1/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nERROR: /home/kai/test/tensorflow-1.2.1/tensorflow/contrib/training/BUILD:339:1: null failed: protoc failed: error executing command\r\n  (cd /home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/protobuf/protoc '--python_out=bazel-out/local_linux-py3-opt/genfiles/' -I. -Iexternal/protobuf/python -Ibazel-out/local_linux-py3-opt/genfiles/external/protobuf/python tensorflow/contrib/training/python/training/hparam.proto): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n/home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3/execroot/org_tensorflow/_bin/process-wrapper: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3/execroot/org_tensorflow/_bin/process-wrapper)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3.922s, Critical Path: 0.03s", "comments": ["This seems similar to #3261. Can you try the workarounds described [here](https://github.com/tensorflow/tensorflow/issues/3261#issuecomment-234147379) or [here](https://github.com/bazelbuild/bazel/issues/1358#issuecomment-258501870)?\r\n\r\n@martinwicke any thoughts? Similar issues (#3261, bazelbuild/bazel#1358, bazelbuild/bazel#898) have been posted before.\r\n", "I had read this before posting the issue. But no, it did not work. Also the file `third_party/gpus/crosstool/CROSSTOOL` no longer exists. I tried both `third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl` and `third_party/gpus/crosstool/CROSSTOOL_clang.tpl`, but none of them worked.", "Right now I can use the pip wheel. So I don't need workarounds here, at least now. If this is an intended behavior or you know how to fix this in future release, feel free to close it. Otherwise, let me know what I can help.", "This is an issue with bazel not picking up the right environment -- I don't believe we can fix this on the TF side. I'll close this issue."]}, {"number": 11804, "title": " No OpKernel was registered to support Op 'RFFT'  for CPU (running on android)", "body": "I'm running tensorflow on android. and got this error:\r\n`Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'RFFT' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                       <no registered kernels>`\r\n\r\nI'm using the master branch, where the RFFT CPU verison is already supported. So I'm wondering why this problem show up.\r\n\r\nThe TF version I used to build the graph(*.pb file) is also the latest master branch.\r\nHere is to code I wrtie .pb graph:\r\n\r\n        with tf.Graph().as_default(), tf.Session(config=tf.ConfigProto(\r\n                allow_soft_placement=True)) as session:\r\n            with tf.variable_scope(\"model\"):\r\n                model = DeployModel(config=config)\r\n\r\n            print('Graph build finished')\r\n            # variable_names = [n.name for n in\r\n            #                   tf.get_default_graph().as_graph_def().node]\r\n            # for n in variable_names:\r\n            #     print(n)\r\n\r\n            saver = tf.train.Saver()\r\n            saver.restore(session, save_path=path_join(self.config.model_path,\r\n                                                       'latest.ckpt'))\r\n            print(\"model restored from %s\" % config.model_path)\r\n\r\n            frozen_graph_def = graph_util.convert_variables_to_constants(\r\n                session, session.graph.as_graph_def(),\r\n                ['model/inputX', 'model/softmax', 'model/nn_outputs'])\r\n            tf.train.write_graph(\r\n                frozen_graph_def,\r\n                os.path.dirname(graph_path),\r\n                os.path.basename(graph_path),\r\n                as_text=False,\r\n            )\r\n\r\nSo maybe RFFT for CPU is still not supported on android in the latest branch?\r\n", "comments": ["Android only builds a subset of the ops, You need to add the ops that you need that are not in the \"commonly used set\" by hacking the build files for your needs.", "@liuzqt You can follow @aselle's suggestion by adding fft_ops.cc to the //tensorflow/core/kernels:android_extended_ops_group1 filegroup and see if that solves the issue.", "@andrewharp @aselle adding fft_ops.cc to build file solve the problem, many thanks!  But it come up with a new problem, when running fft, raise this exception:\r\n`                                                                      Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'ComplexAbs' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                       <no registered kernels>\r\n                                                                     \r\n                                                                     \t [[Node: model/fft = ComplexAbs[T=DT_COMPLEX64, Tout=DT_FLOAT, _device=\"/device:CPU:0\"](model/rfft)]]\r\n                                                                         at org.tensorflow.Session.run(Native Method)`\r\nand then I refer to \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_abs.cc\r\nfound that \r\n`#if !defined(IS_MOBILE_PLATFORM)\r\nREGISTER2(UnaryOp, CPU, \"ComplexAbs\", functor::abs, complex64, complex128);\r\n#endif`\r\nso it means Complex2real is not supported on android?\r\nIf I want to implement this, how should I do? Can I just remove the \"if !defined\" to make this REGISTER compiled to android lib?", "It seems like complexabs is not implemented. You could just try removing the #if !defined and see if it works. I'm not sure why that check was put in, but it is possible that it was for an old version that didn't support the complex type. @petewarden , any ideas?", "@aselle I remove that #if !defined and it works! many thank!"]}, {"number": 11803, "title": "/gpu:0/stream doesn't appear on Timeline", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n('v1.2.0-2479-g88abddb', '1.3.0-rc0')\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\nbazel release 0.4.5\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0\r\ncuDNN 5.1.5\r\n- **GPU driver version**\r\n375.66 \r\n- **GPU model and memory**:\r\nGeForce GTX 1080 \r\n8GB\r\n- **Exact command to reproduce**:\r\npython convolution.py (attached)\r\n\r\n### The Problem\r\nMaking a timeline.Timeline object in my environment, \"/gpu:0/stream:xx\" rows don't appear. \r\n![timeline_no_gpu](https://user-images.githubusercontent.com/1290076/28655300-d8748114-72d5-11e7-80a9-fd1dd8198baa.png)\r\n\r\nI found the event collector function [BufferCompleted](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/gpu_tracer.cc#L149) were not called at all during measuring performance. \r\n\r\nThough BufferCompleted is registered as a callback function to CUPTI for processing GPU events, CUPTI doesn't call it in my environment. This comes from failing to flush GPU events.\r\nGPU events are flushed by [ActivityFlushAll](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/gpu_tracer.cc#L206) with flag [CUPTI_ACTIVITY_FLAG_NONE](http://docs.nvidia.com/cuda/cupti/group__CUPTI__ACTIVITY__API.html#group__CUPTI__ACTIVITY__API) in tensorflow. But this flag doesn't seem to cover necessary GPU events, so GPU events were not flushed and collected.\r\n\r\nI modified gpu_tracer.cc to call ActivityFlushAll with flag CUPTI_ACTIVITY_FLAG_FORCE_INT, then I got \"/gpu:0/stream:xx\" rows in Timeline. \r\n![timeline_with_gpu](https://user-images.githubusercontent.com/1290076/28655697-86a8cd7e-72d8-11e7-8d1f-03374c4ad788.png)\r\n\r\nIs there any reason to use CUPTI_ACTIVITY_FLAG_NONE?\r\n\r\n\r\n### Source code / logs\r\nAttached source code comes from [tensorflow/models](https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py) .\r\nThe following modification was added to measure performance.\r\n\r\n        from tensorflow.python.client import timeline\r\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n        run_metadata = tf.RunMetadata()\r\n        l, lr, predictions = sess.run([loss, learning_rate, train_prediction],\r\n                                       feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\r\n\r\n        tl = timeline.Timeline(run_metadata.step_stats)\r\n        ctf = tl.generate_chrome_trace_format(show_memory=True,\r\n                                            show_dataflow=True)\r\n        with open(\"timeline.json\", \"w\") as f:\r\n          f.write(ctf)\r\n\r\n\r\n[convolutional.py.zip](https://github.com/tensorflow/tensorflow/files/1178943/convolutional.py.zip)\r\n", "comments": ["I'm having the exact same problem, but I didn't compile from code\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')\r\n- **Python version**: Python 2.7.6\r\n- **Bazel version (if compiling from source)**: not applicable\r\n- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 5.1.10\r\n- **GPU model and memory**: TITAN X 12GB\r\n- **Exact command to reproduce**:", "@laket Thanks for the issue report and the clear investigation.  I agree that it looks like the tracing buffers aren't getting flushed.\r\n\r\nThe docs on `cuptiActivtyFlushAll` are a bit vague about what those flags mean and we have been running without `CUPTI_ACTIVITY_FLAG_FLUSH_FORCED` for a long time without seeing this issue.  I guess something has changed in recent driver versions, \r\n\r\nI don't see a description of what `CUPTI_ACTIVITY_FLAG_FORCE_INT` does, only a definition as `0x7fffffff`.   Internal?  Interrupt? \r\n\r\nDoes this work for you with the \"documented\" flag?  ", "@prb12 yes. I got  /gpu:0/stream:xx lines in Timeline with  `CUPTI_CALL(ActivityFlushAll(CUPTI_ACTIVITY_FLAG_FLUSH_FORCED));`\r\n", "Seems reasonable.  I dug around into the history of this flag.  It seems that in CUDA6 and earlier this flag was *required* to be zero, and there have been a few bugs related to buffer flushing (e.g. losing the last trace record in the buffer).\r\n\r\nSince (I think) we no longer need to support cuda versions < 7 this seems like a sensible change to me.\r\n\r\nCould you please submit a PR? \r\n", "@prb12 thanks for the detail background. I submit the PR. "]}, {"number": 11802, "title": "3D Convolutions not being forwarded to MKL", "body": "This is a placeholder reminder for the Tensorflow/Intel team.  I'm in touch with Toby Boyd and Intel on this issue, I just want it to be in the databasase.\r\n\r\nWhen compiling for MKL (--config=mkl) 3D convolutions remain in native Eigan. This strongly impacts a 3D medical application that needs to run inferencing on an edge device.\r\n\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04 and CentOS 7\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.2.0rc0\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.5.2\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\nFollow standard bazel build, say \"yes\" for MKL, build  wheel with --config=mkl. and install with pip\r\n\r\n", "comments": ["This is true.  We are working with Intel to add this feature.  It may take some time as this feature does not seem to be implemented in the underlying library.  Intel has indicated that they have customers also very interested in conv3d.  I will keep it assigned to me and this may take time.    ", "Word is there is still no body working on this at Intel.\n\nOn Fri, Jul 28, 2017 at 8:14 PM, Toby Boyd <notifications@github.com> wrote:\n\n> This is true. We are working with Intel to add this feature. It may take\n> some time as this feature does not seem to be implemented in the underlying\n> library. Intel has indicated that they have customers also very interested\n> in conv3d. I will keep it assigned to me and this may take time.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11802#issuecomment-318710809>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AG0nM50yj9i9_XaQ27S7uCoImHWBuWCTks5sShb8gaJpZM4Ok0ty>\n> .\n>\n", "@dahvid do you have a shareable benchmark that could be used to estimate performance on your workload?", "Hi Ng,\nWe're working directly with the Intel Inferencing Engine development group\nin Moscow.  We're under NDA with them so they're allowed to have the data.\nBut if you take any standard 3d convolution benchmark.  Compile TF with MKL\nenabled and then check against w/o MKL, or better yet, look in VTune and\nsee what's going on.  You'll see that the 3d convolutions are not being\nforwarded to MKL.  I've confirmed this with the Intel developers.\n(disclosure, I work for GE Research)\nThanks,\nDavid\n\nOn Sun, Feb 11, 2018 at 6:50 PM, Choong Ng <notifications@github.com> wrote:\n\n> @dahvid <https://github.com/dahvid> do you have a shareable benchmark\n> that could be used to estimate performance on your workload?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11802#issuecomment-364766095>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AG0nM5s6WDN-ETW1vyxKfSzBzUwcuwCYks5tTxpqgaJpZM4Ok0ty>\n> .\n>\n", "JFYI, Intel MKL-DNN team is working on adding 3D support.", "MKL-DNN added 3D convolutions starting from https://github.com/intel/mkl-dnn/commit/da98a46bfe77d8f70c06f78f209abd0245b4bc74. Support for other primitives in the works.", "You can close the issue.  I won't have time to confirm this for a couple of\nmonths.\n\nOn Tue, Apr 24, 2018 at 2:18 AM, Roma Dubtsov <notifications@github.com>\nwrote:\n\n> MKL-DNN added 3D convolutions starting from intel/mkl-dnn@da98a46\n> <https://github.com/intel/mkl-dnn/commit/da98a46bfe77d8f70c06f78f209abd0245b4bc74>.\n> Support for other primitives in the works.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11802#issuecomment-383751803>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AG0nM5rmYWDdPNsLa9Xd9xs9Ty1kacLSks5trmE-gaJpZM4Ok0ty>\n> .\n>\n", "Nagging Assignee @tfboyd: It has been 137 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 11801, "title": "rename target protobuf, closes #11599", "body": "", "comments": ["Can one of the admins verify this patch?", "This looks like it's already done in the tested and pending merge of https://github.com/tensorflow/tensorflow/pull/11768 during the upgrade of gRPC -- would you mind if we take that one instead?  It also requires bazel cleaning for our test infra, so that one is probably more complete.  Sorry we did not know of the work that was in parallel :("]}, {"number": 11800, "title": "Refine docstrings", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11799, "title": "Support with statement for summary.FileWriter", "body": "This fix adds support of `with` for `summary.FileWriter`\r\n\r\nThis fix fixes #11750.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Thanks!  Probably would be good to check in a test of this extra behavior too to verify that it works.\r\n\r\nThough again, this probably requires API review since it's adding a feature to the public API, so marking it as such.", "@vrv Thanks. I added a test case to cover the changes.", "@tensorflow-jenkins test this please\r\n\r\nThanks, this is nice, so I'll wait for API review and then hopefully we can merge, if they like this.", "Approval for API review, please update the goldens (which will require running using Python 2).", "Thanks @josh11b for the review. I run through\r\n```\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test           --update_goldens True\r\n```\r\n\r\nand there are no API changes.\r\n", "@tensorflow-jenkins test this please", "The macos test failure is unrelated.\r\n\r\n@josh11b did you expect changes to the goldens?", "I doubt __enter__ and __exit__ are documented by default, so I don't expect this to be an API-visible change from the goldens perspective, even though this is an API addition.\r\n\r\nThis is probably ready to merge.", "@vrv thanks for the clarification. Merging.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 11798, "title": "bitwise_ops and gen_bitwise_ops cannot be imported", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Python version**:  3.5.3\r\n- **CUDA/cuDNN version**: 8.0.60\r\n- **GPU model and memory**:  NVIDIA GeForce GT 730\r\n- **Exact command to reproduce**:\r\n\r\nIn the bitwise_ops_test.py file, located in the ops folder in python, this code is there:\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport six\r\n\r\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import test_util\r\nfrom tensorflow.python.ops import bitwise_ops\r\nfrom tensorflow.python.ops import gen_bitwise_ops\r\nfrom tensorflow.python.platform import googletest\r\n```\r\n\r\nThese two lines are giving an error:\r\n\r\n```\r\nfrom tensorflow.python.ops import bitwise_ops\r\nfrom tensorflow.python.ops import gen_bitwise_ops\r\n```\r\n\r\n\r\nI get an import error saying:\r\n\r\n\r\n```\r\nfrom tensorflow.python.ops import bitwise_ops\r\nImportError: cannot import name 'bitwise_ops'\r\n```\r\n\r\n\r\nAlso theres no gen_bitwise_ops file in the ops folder, and the method isn't available in the init file.\r\n\r\n", "comments": ["What is the exact command you are running to produce this error? Is this just doing import tensorflow as tf from a python shell?}", "I wanted to run bitwise_ops_test.py, which runs this code:\r\n`if __name__ == \"__main__\":\r\n  googletest.main()`\r\n\r\nAnd no, I'm running it on an IDE, on a anaconda environment.", "It appears that you are using TensorFlow 1.1.0, but the `tensorflow.python.ops.bitwise_ops` module was added in TensorFlow 1.3. You'll need to upgrade to the latest version of TensorFlow (currently in the release candidate stage) or a nightly build to import that module."]}, {"number": 11797, "title": "Refactoring device name utils", "body": "remove duplicated code for full_name and legacy_name for DeviceNameUtils.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11796, "title": "Branch 163282839", "body": "", "comments": []}, {"number": 11795, "title": "GPU Tracer: Add peer-to-peer memcpy annotations", "body": "In order to trace memory copies between devices with more recent versions of\r\nCUDA, the GPU tracer must capture peer-to-peer (i.e. device-to-device) memory\r\ntransfers (e.g. cuMemcpyDtoD, cudaMemcpy(., ., ., cudaMemcpyDeviceToDevice)).\r\nAdd handling to capture these.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "This compiles on enough tests that I think this is fine."]}, {"number": 11794, "title": "Update version to rc1.", "body": "", "comments": []}, {"number": 11793, "title": "Update README.md", "body": "Remove bad practices of sudo pip and install use safer pip install commands", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please  (for sanity check)"]}, {"number": 11792, "title": "Mac build problem with local_config_python", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nNo, this repros on master, r1.3, and r1.2\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMacOS Sierra, 10.12.5\r\n- **TensorFlow installed from (source or binary)**:\r\nSource, repros on master, r1.3, r1.2\r\n- **TensorFlow version (use command below)**:\r\nSee above.\r\n- **Python version**: 2.7.10. Clean virtualenv\r\n- **Bazel version (if compiling from source)**: 0.5.1-homebrew\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**:\r\n`bazel build //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI'm trying to build the pip package generator so I can test changes to the pip package. The same command works on Linux.\r\n\r\n### Source code / logs\r\n\u276f bazel build //tensorflow/tools/pip_package:build_pip_package\r\nERROR: /private/var/tmp/_bazel_dandelion/bd129d2cd1c27b48980a1c74fec9319a/external/local_config_python/BUILD:136:12: in outs attribute of genrule rule @local_config_python//:numpy_include: Genrules without outputs don't make sense.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.", "comments": ["Apparently we're still using 0.4.5 to build. @av8ramit FYI\r\n\r\n@damienmg Do we know of something in 0.5.1 that could be doing this (on Mac only)? ", "/cc @meteorcloudy @nlopezgi\r\n\r\nI don't think there was any change since 0.4.5 that would trigger the issue but Nicolas made some change since then on the python configuration in TF which might generate an genrule with empty outs in some cornercases", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "https://www.tensorflow.org/install/install_sources#tested_source_configurations\r\n\r\nWe now build with Bazel 0.5.4 on mac with TensorFlow 1.4. Closing the issue."]}, {"number": 11791, "title": "Feature request: equivalent of tf.nn.maxpooling_with_argmax for 3D maxpooling", "body": "It would be great to have the equivalent of tf.nn.maxpooling_with_argmax in 3D, in order to allow for 3D unpooling layers.\r\n\r\nI am implementing a 3Dversion of the originally 2D Deconvolution network [Noh et al. 2015]. It has been implemented in 2D in Tensorflow [here](https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation/blob/master/DeconvNet.py) and in 3D in caffe [here](https://github.com/Microsoft/O-CNN/blob/master/caffe/examples/o-cnn/segmentation_5.prototxt). \r\n\r\nI need to use unpooling, and for that I need the indexes of the elements selected during pooling. This feature is implemented for 2D (tf.nn.maxpooling_with_argmax), but not for 3D.\r\n", "comments": ["No one is currently working on this, but contributions are welcome!\r\n\r\n@zheng-xq any thoughts on whether someone should work on this?", "@reedwm @zheng-xq I'd like to work on this, if it's still open/worthwhile", "This issue is still open. I am not sure how worthwhile it is. This op should probably be added to contrib first, then if it is shown to be worthwhile it can be moved to core.\r\n\r\n/CC @mjanusz @fchollet, do you know how worthwhile this is?", "Hi @agataf !\r\nWe are checking to see if you still need help in this issue , Have you checked this [thread ](https://pretagteam.com/question/using-ktfnnmaxpoolwithargmax-with-an-3d-input-tensor) yet? Please create a new  issue  if it is still replicating in Latest version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 11790, "title": "Removing Mac GPU links out of r1.3.", "body": "", "comments": ["Sorry, can you resolve the conflicts? :(", "I merged this in with https://github.com/tensorflow/tensorflow/pull/11794 which caused the conflict. "]}, {"number": 11789, "title": "Branch 163213141", "body": "(repushing, will not squash commits)", "comments": []}, {"number": 11788, "title": "Add support for XLA cross products", "body": "XLA currently supports basic trigonometric operations but does not natively support cross products. Cross products are fundamental to many geometric operations, and TF has a `tf.cross` function. Note relevant discussion in #8315 ([comment](https://github.com/tensorflow/tensorflow/issues/8315#issuecomment-317947166)), and preliminary work [here](https://gist.github.com/learyg/4019bae1cfc5bacbcdb318fd882b4d5d).", "comments": ["So it seems like there's not a mechanism for desugaring Python tf.cross directly to the Python you provided, but we *can* go implement it in the tf2xla bridge along these lines. I'll take a shot at that.", "Submitting an implementation and tests downstream, should be available in the next merge.", "Fantastic. I look forward to it!", "Landed in 32e198f2d5787ca81aba89bf073e4eb380769253\r\n\r\nThanks for filing! Let us know if you have other issues."]}]