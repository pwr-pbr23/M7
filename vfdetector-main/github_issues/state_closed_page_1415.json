[{"number": 10530, "title": "Protobuf serialization slows down RDMA", "body": "According to a test similar to https://github.com/tensorflow/tensorflow/issues/6116, the current RDMA implementation has performance issues.\r\n\r\nThis tensor copy throughput according to [this test](https://gist.github.com/yaroslavvb/1124bb02a9fd4abce3d86caf2f950cb2) (change `assign_add` to `assign` in L53, and test with tensor size 100MB):\r\n```\r\nDistributed rate: 536.72 MB per second\r\n```\r\nWhich is just slightly higher than the gRPC implementation (the gRPC fixed version: https://github.com/tensorflow/tensorflow/pull/7466)\r\n\r\nHere is the profiling result:\r\n[device1 worker profiling report](https://github.com/tensorflow/tensorflow/files/1060989/dev1.pdf)\r\n[device2 worker profiling report](https://github.com/tensorflow/tensorflow/files/1060990/dev2.pdf)\r\n\r\n\r\nThere are some obvious issue, for example, the [string resize issue](https://github.com/google/protobuf/blob/master/src/google/protobuf/stubs/stl_util.h#L67), the unnecessary memory copy and the single threaded serialization/deserialization.\r\n\r\n", "comments": []}, {"number": 10529, "title": "source build fails: cannot find python bin due to empty environment", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Gentoo Linux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master (commit 02dbe153afe2c7f418fcf9044e2e3a8795a21d3a)\r\n- **Bazel version (if compiling from source)**:  0.4.5\r\n- **CUDA/cuDNN version**: No CUDA\r\n- **GPU model and memory**: No CUDA\r\n- **Exact command to reproduce**: `./configure && bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Environment\r\n```== cat /etc/issue ===============================================\r\nLinux mace 4.8.17-hardened-r2 #2 SMP Thu Jun 1 15:39:15 CEST 2017 x86_64 Intel(R) Xeon(R) CPU E5-2407 v2 @ 2.40GHz GenuineIntel GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Gentoo Hardened 4.9.4 p1.0, pie-0.6.4) 4.9.4\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux mace 4.8.17-hardened-r2 #2 SMP Thu Jun 1 15:39:15 CEST 2017 x86_64 Intel(R) Xeon(R) CPU E5-2407 v2 @ 2.40GHz GenuineIntel GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"/root/src/tensorflow_src/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named 'tensorflow.python.pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/root/src/tensorflow_src/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/root/src/tensorflow_src/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/root/src/tensorflow_src/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/root/src/tensorflow_src/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named 'tensorflow.python.pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n\r\n== cuda libs  ===================================================\r\n```\r\n\r\n### Describe the problem\r\n\r\nI configured the source code (with MKL, no CUDA, nothing else enabled) and the bazel build fails 120s with\r\n```ERROR: /root/src/tensorflow_src/tensorflow/tensorboard/components/vz_sorting/BUILD:8:1: Compiling 4 TypeScript files //tensorflow/tensorboard/components/vz_sorting:vz_sorting failed: execrooter failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/07ee3a30c05d85d959138fae6c764ebf/execroot/tensorflow_src && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter bazel-out/local-py3-opt/bin/tensorflow/tensorboard/components/vz_sorting/vz_sorting-tsc-execroot.json): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\r\npython: unable to find executable in PATH.\r\n```\r\n\r\nWhile the command runs ok, when I omit running inside a clean environment `env -`. I am running inside a virtualenv. But also without a virtual env I am running into this problem:\r\n\r\n```$ which python\r\n/usr/bin/python\r\n$ env - which python \r\nwhich: no python in ((null))\r\n```", "comments": ["From the `env` man page: \r\n\r\n    The env utility uses the PATH environment variable to locate the\r\n    requested utility if the name contains no `/' characters, unless the -P\r\n    option has been specified.\r\n\r\nSo it appears your PATH is empty, or python is not in the path, but is instead an alias set in your shell.\r\n\r\nEither way, this doesn't appear to be a problem with TensorFlow itself. I'll close this issue. Please comment to reopen if you have new information that suggests there is a bug in TensorFlow.", "Yes, my path is empty (I thought that is the purpose of \"env -\"). Python is installed into `/usr/bin/python`. Never the less, I don't want to build with my systems default python binary, but with the one inside my virtualenv. This is not possible, if the environment is cleared before (with `env -`).\r\n\r\nWhy is it even cleared?", "If you are in a `virtualenv` when building, then it's tied to that `virtualenv`. I don't think there's any need to do `env -` either way.", "To the real question: which component creates this command and why? It looks like some javascript/typescript stuff. I cannot find it in the code, so it may be some third party problem.", "The command is generated by bazel. It tries to make the build hermetic, and it should use the python binary that you gave it when you configured TensorFlow. ", "~~my workaround is to replace the env binary with~~\r\nmy workaround is to wrap the env binary and remove the dash:\r\n```bash\r\n#!/bin/bash\r\n\r\n[[ \"$1\" == \"-\" ]] && shift\r\n\r\nexec /bin/real_env $@\r\n```\r\n\r\nquite a bit ugly, but works for now. I still think, this should be reopened and investigated. Btw I have this problem on all my Gentoo systems.", "@damienmg I don't know why the right python isn't used in @ribx's case, but the env workaround is clearly not in the spirit of bazel. Is this an error in how TF does python config? Or is it a bug in bazel's treatment of python?", "It looks like the whole generated command is wrong:\r\n\r\nthe last line:\r\n```\r\nbazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter bazel-out/local-py3-opt/bin/tensorflow/tensorboard/components/vz_sorting/vz_sorting-tsc-execroot.json\r\n```\r\ntries to execute the json file. The \"execrooter\" script is a python script that tries to rebuild the environment and then execute the first commandline argument, which is in this case a json file.\r\n\r\nI forgot to mention that everything worked fine with the version I compiled 2 weeks ago.\r\n\r\nI will clean my env reclone the git repo and try again.", "Just as a note: everything works fine when I compile outside my virtualenv."]}, {"number": 10528, "title": "Make stage tests less sensitive to timeouts", "body": "Previously, staging tests used the number of failed token dequeues (with 50 ms timeouts) on\r\na signalling queue to indicate blocked puts. This was probably too sensitive on highly contended testing systems. Basically, there's no good way to distinguish between a put that took too long and a blocking put.\r\n\r\nStaging tests now simply count the number of iterations before a timeout of 1s occurs on the signalling queue. This should mean that:\r\n\r\n1. The test case is less brittle around what is considered a block vs  the putting thread not being able to keep up.\r\n2. Both stage_op_test and map_stage_op_test take slightly longer, ~2.5s vs ~1s, to run on an uncontended system..\r\n\r\nAlso simplified the tests cases in general.\r\n\r\n/cc @ekelsen @jhseu I saw #10516 and the duration of test runs (~30s on your test systems vs ~1s on my laptop) made me reconsider the duration on the blocking timeout. I've upped it 20X from 50ms to 1s.\r\n\r\nIn an ideal world, the test cases wouldn't depend on a timeout when running -- if you can suggest a different method of testing blocking puts and gets, please let me know.\r\n\r\nI also relooked at the multi-threaded code in the StagingArea, but I think the standard producer/consumer queue has been implemented correctly.\r\n\r\nI hope this helps.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please", "Simon - thanks for looking into this.  I know it's annoying to deal with test failures that only happen on the test machines and not locally.\r\n\r\nWe had disabled the tests, so to be able to test them on the test server you'll need to re-enable them.", "> Simon - thanks for looking into this. I know it's annoying to deal with test failures that only happen on the test machines and not locally.\r\n\r\nLikewise, thanks for your patience with this. I've re-enabled the tests.\r\n\r\n", "Jenkins, test this please.", "Test failures look unrelated. Note that stage_op_test.py never managed to run on the `Linux CPU Test`, while both map_stage_op_test.py and stage_op_test.py completed successfully on the `Windows Cmake Tests`", "I agree, this looks good.  It would be nice to get that api compatibility test fixed so we can get a run on linux.", "I misread the logs -- `stage_op_test.py` passed successfully in both of the failed test suites.", "Could you fix the conflicts?", "@jhseu does this change address http://b/62429636? If so, we can re-enable the tests (they don't run right now, so this is an untested change, and I don't want to merge it like this).", "Rebased and removed all tags in `stage_op_test.py` and `map_stage_op_test.py`. The `noguitar` and `notap` tags aren't documented so I'm not sure what their removal implies...", "@martinwicke I'm not certain, but it seems likely to fix that bug. ", "Jenkins, test this please", "Staging Tests look good. The Windows Cmake Tests failed on an unrelated test case.", "Confirming that windows cmake issues are unrelated. Should be fixed at master in the last hour."]}, {"number": 10527, "title": "tensor_array.h not part of pip include header files", "body": "I'm not sure if it is supposed to be part of it. If so, then this is a bug, because it is missing currently.\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "pip install tensorflow-gpu, version TF 1.1, Ubuntu 16.04 Linux.", "@skye, do you know if pip is supposed to allow writing c++ code directly. I always thought you needed to build from source in order to do that.", "Many header files are part of pip.\n\n\nAm 13.06.2017 11:33 schrieb \"Andrew Selle\" <notifications@github.com>:\n\n> @skye <https://github.com/skye>, do you know if pip is supposed to allow\n> writing c++ code directly. I always thought you needed to build from source\n> in order to do that.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10527#issuecomment-308045908>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AADm_OOidkvpTseobCXgmbGbn7reiKuLks5sDkllgaJpZM4Nz_-N>\n> .\n>\n", "As @albertz says, our pip install does include many header files so it is possible to do some C++ development using it, but AFAIK no one maintains it as an \"official\" C++ installation method (unfortunately there is no official C++ dev package yet, but we're working on it). Maybe mark community support?", "Maybe to add some context: I wanted that `tf.TensorArray` uses a global resource manager and not the per-run/per-step resource manager (described [here](https://stackoverflow.com/questions/44418036/tf-tensorarray-across-multiple-computation-steps-supposed-to-remember)). To accomplish this, I derived my own `GlobalTensorArrayCreationOp` which basically does the same as `TensorArrayCreationOp` except that it uses the global session resource manager (code is [here](https://github.com/rwth-i6/returnn/blob/master/TFUtil.py)). Unfortunately, I cannot compile this as `tensor_array.h` is not part of the pip include header files.\r\n", "Hmm according to our docs at https://www.tensorflow.org/extend/adding_an_op, installing from pip should be OK: (under \"Prerequisites\") \"Must have installed the TensorFlow binary, or must have downloaded TensorFlow source, and be able to build it.\"\r\n\r\n@albertz if you're still working on this, it looks like you'll need to build from source for now, but we should either make the pip package include headers needed for custom op development or change our docs.", "@skye Many header files already are, and you can do custom op development, so the documentation is not wrong. This is a special case here because I want to derive from the existing `TensorArrayCreationOp`.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Sorry for dropping this. Looking at this again, I don't think we'll support deriving from internal classes from the pip packages. I'm gonna close. cc @allenlavoie"]}, {"number": 10526, "title": "Re-enable some python tests in Windows Bazel build", "body": "```\r\n//py_test_dir/tensorflow/python:framework_meta_graph_test                PASSED in 7.2s\r\n//py_test_dir/tensorflow/python:gradients_test                           PASSED in 14.6s\r\n//py_test_dir/tensorflow/python/kernel_tests:depthwise_conv_op_test      PASSED in 32.1s\r\n//py_test_dir/tensorflow/python/kernel_tests:sets_test                   PASSED in 21.1s\r\n//py_test_dir/tensorflow/python/kernel_tests:summary_ops_test            PASSED in 5.5s\r\n//py_test_dir/tensorflow/python:localhost_cluster_performance_test       PASSED in 6.4s\r\n//py_test_dir/tensorflow/python:saver_test                               PASSED in 40.1s\r\n//py_test_dir/tensorflow/python:sync_replicas_optimizer_test             PASSED in 12.8s\r\n```\r\n\r\nThese tests are passing on Windows and not flaky.", "comments": ["http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/25/console"]}, {"number": 10525, "title": "Try both python and python3", "body": "Used `configure` script for reference", "comments": ["Can one of the admins verify this patch?", "Did you mean to close this?", "@aselle, This was intended be to taken together with another PR, but I seam to have missed this one."]}, {"number": 10524, "title": "[Bash] Group commands to redirect", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2129", "comments": ["Can one of the admins verify this patch?", "Closing out the rest of the static analysis changes. Please convert them to one pull request. It's easier to review and test that way :)"]}, {"number": 10523, "title": "I update libprotobuf but tendoflow still use the old version", "body": "i update libprotobuf \r\n\r\n>    ldconfig -p\r\n>         libprotobuf.so.13 (libc6,x86-64) => /usr/local/lib/libprotobuf.so.13\r\n>         libprotobuf.so.10 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libprotobuf.so.10\r\n>         libprotobuf.so (libc6,x86-64) => /usr/local/lib/libprotobuf.so\r\n>         libprotobuf-lite.so.13 (libc6,x86-64) => /usr/local/lib/libprotobuf-lite.so.13\r\n>         libprotobuf-lite.so.10 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libprotobuf-lite.so.10\r\n>         libprotobuf-lite.so (libc6,x86-64) => /usr/local/lib/libprotobuf-lite.so\r\n\r\n\r\nand it worked \r\n     \r\n\r\n>   protoc --version\r\n>           libprotoc 3.3.0\r\n\r\nbut tensorflow still use uses the 3.0.0. version\r\n\r\n> \r\n>   [libprotobuf FATAL google/protobuf/stubs/common.cc:67] This program requires version 3.3.0 of the   Protocol Buffer runtime library, but the installed version is 3.0.0.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"google/protobuf/descriptor.pb.cc\".)\r\n terminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  This program requires version 3.3.0 of the Protocol Buffer runtime library, but the installed version is 3.0.0.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"google/protobuf/descriptor.pb.cc\".)\r\n\r\ncan someone help me\r\n ", "comments": ["Please fill out the template -- with only the information given it is very hard to help you.\r\n\r\nIt may be that your version of TF is linked against 3.0.0 and remembers this. "]}, {"number": 10522, "title": "np.float64(\"np.inf\").astype(np.int32) is negative on x86 but positive on ppc64le", "body": "- np.float64(\"np.inf\").astype(np.int32) is negative on x86 but positive on ppc64le, that is the reason we \r\n  added this special case for ppc64le\r\n\r\n- Discussed with numpy community and they mentioned as follows -\r\n1) Doing this cast incurs undefined behaviour, so it's hard to say which platform is incorrect here.\r\n    I'd argue that we should just issue a warning and leave the platform-specific behaviour.\r\n2) This is also true of any C program that tries to cast float to int. If the author of a python package \r\n    included a C extension that does this cast, they still have the problem. \r\n   The same argument you use for \"we should fix this is numpy, not in the user package\" can be \r\n    applied to \"we should fix this in C, not in numpy\". Presumably for speed reasons, that argument was \r\n    rejected during the  design of C.\r\n    Having said that, we already diverge from C behaviour in some place (eg, integer promotion for \r\n    arithmetic and comparison), so it wouldn't be unreasonable  to add special casing here.\r\n    \r\nNumpy link to relevant discussion - https://github.com/numpy/numpy/issues/9040\r\nAlso discussed this on Tensorflow , link to relevant discussion - https://github.com/tensorflow/tensorflow/issues/9360\r\n", "comments": ["Can one of the admins verify this patch?", "Hi  @martinwicke, as discussed on thread https://github.com/tensorflow/tensorflow/issues/9360, I have created this PR. \r\n\r\nCould you please have a look?", "For second (https://github.com/tensorflow/tensorflow/pull/10522/commits/c63cebb80a26cb7eeaf0afe095886488f1a8ac8a) commit details are as follows :\r\n\r\nThis is regarding failure of test case //tensorflow/core:platform_profile_utils_cpu_utils_test in tensorflow/core/platform/profile_utils/cpu_utils_test.cc file.\r\nOn ppc64le this test was failing for following two checks i.e. CHECK_GT(cpu_frequency, 0) and CHECK_NE(cpu_frequency, CpuUtils::INVALID_FREQUENCY) :\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/core/platform/profile_utils/cpu_utils_test.cc#L56-L57\r\n\r\n```\r\nTEST_F(CpuUtilsTest, CheckCycleCounterFrequency) {\r\n  const int64 cpu_frequency = CpuUtils::GetCycleCounterFrequency();\r\n  CHECK_GT(cpu_frequency, 0);\r\n  CHECK_NE(cpu_frequency, CpuUtils::INVALID_FREQUENCY);\r\n  if (DBG) {\r\n    LOG(INFO) << \"Cpu frequency = \" << cpu_frequency;\r\n  }\r\n}\r\n```\r\n\r\nAfter investigation found that, \"int64 cpu_frequency\" value on x86 is \"unsigned\" by default whereas on ppc64le it is \"signed\" by default, that is why this test was failing on ppc64le, PFA for log details \r\n[platform_profile_utils_cpu_utils_test failure_log.txt](https://github.com/tensorflow/tensorflow/files/1071075/platform_profile_utils_cpu_utils_test.failure_log.txt)\r\n\r\n\r\nTo make the results compatible with x86, some code changes were required on ppc64le. \r\nAfter code changes, I ran this test and now it is passing on ppc64le as well as on X86.\r\n\r\nCould someone please review this changes as well (along with first commit)?\r\nThanks!\r\n", "@prb12 friendly nudge?", "**Details for the change to GetCycleCounterFrequency :** \r\n\r\nThis is regarding test case` //tensorflow/core:platform_profile_utils_cpu_utils_test`. While running this test on `ppc64le` platform `int64 GetCycleCounterFrequency` function returning  unexpected  value (i.e. `-1` vs expected `greater than 1`)  ,which is ` \"int64 cpu_frequency\"` value\r\nAs per analysis here we found that  :  \"`int64`\"  value on `x86` is \"unsigned\" by default whereas on `ppc64le` it is \"signed\" by default. Because of that we were getting incorrect value on ppc64le.\r\n\r\nTo make the result compatible with `x86`, I have done this changes on `ppc64le`.\r\nAfter code changes, I ran this test and now it is passing on` ppc64le` as well as on `X86`.\r\n\r\nIn first commit (https://github.com/tensorflow/tensorflow/commit/c63cebb80a26cb7eeaf0afe095886488f1a8ac8a) I had made generic changes . After that I realized that this test is passing successfully on` X86` without any change. Hence, now I have made only ppc64le specific changes in latest commit (i.e.  https://github.com/tensorflow/tensorflow/pull/10522/commits/c42d2f52a8237e7e73e29a5658bfe0c83baee414)\r\n\r\nPlease provide your comments on this and  let me know if you need any additional details. Thanks!", "@drpngx fixed comment style and made required changes. Please review and let me know if you need any additional details. Thanks!", "Jenkins, test this please.\n\nOn Jun 27, 2017 3:10 AM, \"sandipmgiri\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> fixed comment style and made required\n> changes. Please review and let me know if you need any additional details.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10522#issuecomment-311314125>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbSPiDcmgpdaxYMokPzo5EfOqYJMEks5sINUTgaJpZM4NzwVu>\n> .\n>\n", "Thanks!\r\n\r\n@prb12 good to go?", "> \"int64\" value on x86 is \"unsigned\" by default\r\n\r\nWhat exactly do you mean by this? `int64_t` is defined as a _\"signed integer type with width of exactly 8, 16, 32 and 64 bits respectively\"_. So I still have no understanding of why that change to `GetCycleCounterFrequency` is necessary"]}, {"number": 10521, "title": "Windows: conv_grad_ops_3d.cc and conv_ops_3d.cc take a long time to compile", "body": "From http://ci.tensorflow.org/job/tf-master-win-bzl/1073/console, I see:\r\n```\r\n00:54:33 [3,344 / 3,367] Compiling tensorflow/core/kernels/conv_grad_ops_3d.cc; 906s standalone ... (2 actions running)\r\n00:58:07 [3,344 / 3,367] Compiling tensorflow/core/kernels/conv_grad_ops_3d.cc; 1121s standalone ... (2 actions running)\r\n01:01:55 [3,344 / 3,367] Compiling tensorflow/core/kernels/conv_grad_ops_3d.cc; 1348s standalone ... (2 actions running)\r\n01:06:33 [3,344 / 3,367] Compiling tensorflow/core/kernels/conv_grad_ops_3d.cc; 1626s standalone ... (2 actions running)\r\n01:12:26 [3,345 / 3,367] Compiling tensorflow/core/kernels/conv_ops_3d.cc; 1978s standalone\r\n```\r\nAs you can see, it takes about half an hour to compile these two files.\r\nThis has been a bottleneck of the TF Windows Bazel build time for a while.\r\nI guess it's because we are compiling with `/O2` option, so compiler was spending too much time on optimizing. But is there anything we can do to optimize the build time?\r\n", "comments": ["@mrry @guschmue @vit-stepanovs \r\nAny ideas?", "No idea... perhaps @mjanusz (the original author) has some sense of why this code might take a long time to compile, but I don't see any extreme number of template instantiations that might contribute....", "just tried compiling for cpu - true, it takes long.\r\nTrying to isolate this a little: if I take out the call to functor::CuboidConvolution<CPUDevice, T>()(...) its fine. Need to look a little what is behind that.", "I tried vs2017 - same thing. There are a bunch of references that excessive use of templates result in slowness and the code behind functor::CuboidConvolution uses a good amount of templates. Let me see if I can find help.", "found somebody to look at it; we update the issue once we know more.", "@guschmue Any updates?", "I know the compiler team looked at it and there was a bug filed but it has been quite since. Let me ping them. ", "Any improvement on this is welcomed. Currently `conv_grad_ops_3d.cc` and `conv_ops_3d.cc` took 2 hours to compile each on my machine (2 Core + 4GB RAM + HDD). I have to try to avoid making changes to common header files to avoid recompiling these two files...", "@rongjiecomputer The slow compiling is caused by `__force_inline` used in eigen, we have a change to remove `__force_inline`, but it's not merged yet.\r\n\r\nIf you don't mind you can use @gunan's temporary repository. Follow the change in https://github.com/tensorflow/tensorflow/pull/15265.\r\n\r\n", "The build time improvement is huge! Less than 1 hour now.\r\n\r\nI am a bit worried about the impact of runtime performance after the removal of `__force_inline` though.", "Yes, that's why we haven't merged this, we want to make sure it doesn't hurt the performance too much by running some benchmark first.", "In V8 (Chrome), there are 3 build configurations: Debug, Release and Official build mode. Release mode turns on most of the optimizations so that final binary runs fast enough for most testing/benchmarking purpose, but the time-consuming build optimizations like LTO are only turned on in Official build.\r\n\r\nMaybe we can have three modes as well, enable __force_inline in Official Build mode and not in Release mode. This will be very useful for external contributors.", "Some perf numbers for stock builds and and builds without __force_inline. \r\nThere is some impact, maybe 10% on average - details below.\r\nThis is from a little benchmark tool that runs some simple fc, lstm, conv nets and takes numbers for training and eval. The eval numbers (ms units, the tool runs 1000 and takes the average) are worse than training.\r\nThe diff column shows the change compared to a stock build.\r\n\r\nnet | name | env | time(sec) | diff% | evaltime(ms) | diff%\r\n-- | -- | -- | -- | -- | -- | --\r\nconv | stock | vs2015.3 | 809.27 | \u00a0 | 1.4375 | \u00a0\r\nconv | nofi | vs2015.3 | 844.64 | 4.37060561 | 1.5937 | 10.86609\r\nconv | stock | vs2017.15.4 | 783.12 | \u00a0 | 1.4063 | \u00a0\r\nconv | nofi | vs2017.15.4 | 824.65 | 5.30314639 | 1.5469 | 9.997867\r\nfc | stock | vs2015.3 | 178.28 | \u00a0 | 0.3281 | \u00a0\r\nfc | nofi | vs2015.3 | 194.4 | 9.04195647 | 0.3906 | 19.04907\r\nfc | stock | vs2017.15.4 | 180.66 | \u00a0 | 0.3437 | \u00a0\r\nfc | nofi | vs2017.15.4 | 185.48 | 2.66799513 | 0.3437 | 0\r\nlstm | stock | vs2015.3 | 176.34 | \u00a0 | 1.8906 | \u00a0\r\nlstm | nofi | vs2015.3 | 202.7 | 14.9483951 | 2.2728 | 20.2158\r\nlstm | stock | vs2017.15.4 | 182.29 | \u00a0 | 1.9471 | \u00a0\r\nlstm | nofi | vs2017.15.4 | 197.25 | 8.2067036 | 1.9063 | -2.09542\r\n\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "From https://blogs.msdn.microsoft.com/vcblog/2018/01/04/visual-studio-2017-throughput-improvements-and-advice/\r\n\r\n> Beyond just turning off the optimizer for functions with pathological compile times, I also need to warn against too liberal a use of __forceinline when possible. ... [SNIP] ... This is the root cause of long compile times in places like Tensorflow/libsodium. This is something we are looking to address in a future release.\r\n\r\nGood to know that at least Visual C++ team is aware of this issue.", "Thanks to a lot of help from @guschmue we were able to debug this to use of `__forceinline` in eigen.\r\nUnfortunately, removing `__forceinline` brings performance hits in the built binary, which is unacceptable for TF. Therefore, in our presubmits we remove forceinline, and our releases we do use forceinline.\r\n\r\nSince this is a VS-eigen issue that unfortunately hits TF in a bad way, I will close this issue.\r\nThere is not much else we can do given all the information.\r\n"]}, {"number": 10520, "title": "tf.layers.conv3d_transpose() gives error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04.2 LTS (Xenial Xerus)\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.2.0-rc2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8/5.1.10\r\n- **GPU model and memory**:\r\nGeForce GTX 1080\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nx_3d = tf.placeholder(tf.float32,shape=[None,5,5,5,1])\r\nconv_t = tf.layers.conv3d_transpose(x_3d,20,[3,3,3])\r\n```\r\n\r\n### Describe the problem\r\nI am getting a TypeError when I use a placeholder with batch size as None as inputs to the conv3d_transpose. This problem does not happen with the tf.layers.conv2d_transpose()\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nx_2d = tf.placeholder(tf.float32,shape=[None,5,5,1])\r\nconv2d_t = tf.layers.conv2d_transpose(x_2d,20,[3,3])\r\nx_3d = tf.placeholder(tf.float32,shape=[None,5,5,5,1])\r\nconv_t = tf.layers.conv3d_transpose(x_3d,20,[3,3,3])\r\n```\r\n\r\n> ---------------------------------------------------------------------------\r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-3-94e71945bcae> in <module>()\r\n>       4 \r\n>       5 x_3d = tf.placeholder(tf.float32,shape=[None,5,5,5,1])\r\n> ----> 6 conv_t = tf.layers.conv3d_transpose(x_3d,20,[3,3,3])\r\n> \r\n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.pyc in conv3d_transpose(inputs, filters, kernel_size, strides, padding, data_format, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, trainable, name, reuse)\r\n>    1538       _reuse=reuse,\r\n>    1539       _scope=name)\r\n> -> 1540   return layer.apply(inputs)\r\n>    1541 \r\n>    1542 \r\n> \r\n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc in apply(self, inputs, *args, **kwargs)\r\n>     490       Output tensor(s).\r\n>     491     \"\"\"\r\n> --> 492     return self.__call__(inputs, *args, **kwargs)\r\n>     493 \r\n>     494   def _assert_input_compatibility(self, inputs):\r\n> \r\n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\r\n>     439         # Check input assumptions set after layer building, e.g. input shape.\r\n>     440         self._assert_input_compatibility(inputs)\r\n> --> 441         outputs = self.call(inputs, *args, **kwargs)\r\n>     442 \r\n>     443         # Apply activity regularization.\r\n> \r\n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.pyc in call(self, inputs)\r\n>    1456         outputs_4d = array_ops.reshape(outputs, [\r\n>    1457             outputs_shape[0], outputs_shape[1] * outputs_shape[2],\r\n> -> 1458             outputs_shape[3], outputs_shape[4]\r\n>    1459         ])\r\n>    1460       outputs_4d = nn.bias_add(\r\n> \r\n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc in reshape(tensor, shape, name)\r\n>    2449   \"\"\"\r\n>    2450   result = _op_def_lib.apply_op(\"Reshape\", tensor=tensor, shape=shape,\r\n> -> 2451                                 name=name)\r\n>    2452   return result\r\n>    2453 \r\n> \r\n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\r\n>     491           except TypeError as err:\r\n>     492             if dtype is None:\r\n> --> 493               raise err\r\n>     494             else:\r\n>     495               raise TypeError(\r\n> \r\n> TypeError: Failed to convert object of type <type 'list'> to Tensor. Contents: [None, 49, 7, 20]. Consider casting elements to a supported type.\r\n> \r\n> ", "comments": ["I'm running into the same issue on 1.2 (not the rc), but I discovered that turning off the bias gets it to work:\r\n\r\n    xinput = tensorflow.placeholder(tensorflow.float32,shape=[None,16,8,4,128])\r\n    tensorflow.layers.conv3d_transpose(xinput, kernel_size=3, padding='SAME', strides=(2,2,2),   filters=32,activation=tensorflow.nn.relu, use_bias=False)\r\n    # produces : \r\n    # <tf.Tensor 'conv3d_transpose_10/Relu:0' shape=(?, 32, 16, 8, 32) dtype=float32>\r\n    tensorflow.layers.conv3d_transpose(xinput, kernel_size=3, padding='SAME', strides=(2,2,2), filters=32,activation=tensorflow.nn.relu, use_bias=True)\r\n    # produces:\r\n    # TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 512, 8, 32]. Consider casting elements to a supported type.\r\n", "I also got the same error on 1.2rc before turning off the bias. ", "A similar error can be triggered by using conv3d and setting data_format to \"channel_first\". And here is the reason for them. For better efficiency, an input/output 5D tensor is reshaped into a 4D tensor (by chunking the third and the fourth dimension), and added to the bias. However, when the 5D tensor (or the output for deconv) has a non-determined batch size (None). The reshape operation will refuse to take the list as the shape parameter (since it can not be converted to a typed tensor). My workaround is to always set the first element to -1 when calling reshape (and when reshaping the tensor back to 5D). See the lines below.\r\n\r\n[Conv3D with channel first data format](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L174\r\n)\r\n[Conv3D Transpose with channel first](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1453)\r\n[Conv3D Transpose with channel last](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1458\r\n)\r\n\r\nI'd highly appreciate someone on the dev team can take a carefule look at Conv3D/Conv3D transpose layers. Right now, they are a bit problematic to use. For example, fused batch norm is not supported for 3D convolution (see issue [#5694](https://github.com/tensorflow/tensorflow/issues/5694)).\r\n\r\n", "The problem still occurs in version 1.4. As @happyharrycn explained right above, you can solve the problem by turning off the biases add operation, or defining a static batch size.", "This is still a problem.", "@Warvito  you can close this as it is not likely viewed as a bug at this point, but rather a peculiarity. Best advice, if batch is None then set `use_bias=False`. If you want a bias then also set `activation=None` and then create your own bias variable, add it, then apply your own activation.", "Nagging Assignee @fchollet: It has been 80 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 10519, "title": "tf.contrib.data: tf-slim training pipeline gets stuck", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux leto28 3.16.0-4-amd64 1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux\r\nVERSION_ID=\"8\"\r\nVERSION=\"8 (jessie)\"\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.2.0-rc2\r\ntf.GIT_VERSION = v1.2.0-rc1-24-gce1d6ec\r\ntf.COMPILER_VERSION = v1.2.0-rc1-24-gce1d6ec\r\n\r\n- **Bazel version (if compiling from source)**:\r\nNone\r\n\r\n- **CUDA/cuDNN version**:\r\n8.0/5.1\r\n\r\n- **GPU model and memory**:\r\nTITAN X (Pascal), 12189MiB\r\n\r\n- **Exact command to reproduce**:\r\n`python ./mwe.py`\r\n\r\n### Describe the problem\r\nI recently ported my dataset handling to the new dataset API from `tf.contrib.data`. Now it seems that the `tf-slim` training pipeline stalls if I request just 1 or 2 CPUs for my job (it used to work just fine with the dataset API provided by `tf-slim`). I does work if I grab 4 CPUs. I tried to come up with a MWE (see below). The interesting thing is that it is not getting stuck if I remove one of the `tf.summary.scalar`s or `.map()` at line 39. I suspect this issue is related to #10369.\r\n\r\n### Source code / logs\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.data as tcd\r\nimport tensorflow.contrib.slim as slim\r\n\r\nfrom tensorflow.contrib.data.python.ops.dataset_ops import _get_file_names\r\n\r\nDATASET_DIR = '/path_to_the_dataset'\r\nFILE_PATTERN = 'shapes_{}_*.tfrecord'\r\nIMAGE_SHAPE = [48, 48, 3]\r\n\r\n\r\ndef _parse_function(example_proto):\r\n    features = {\r\n        \"image/encoded\": tf.FixedLenFeature(\r\n            (), tf.string, default_value=\"\"),\r\n        'image/annotation/color': tf.FixedLenFeature(\r\n            (), tf.int64, default_value=0),\r\n        'image/annotation/shape': tf.FixedLenFeature(\r\n            (), tf.int64, default_value=0),\r\n    }\r\n    parsed_features = tf.parse_single_example(example_proto, features)\r\n    image_decoded = tf.image.decode_image(parsed_features[\"image/encoded\"])\r\n    color = parsed_features['image/annotation/color']\r\n    shape = parsed_features['image/annotation/shape']\r\n\r\n    return image_decoded, color, shape\r\n\r\n\r\ndef get_batch(batch_size=32, group_size=3, split_name='train'):\r\n    file_pattern = os.path.join(\r\n        DATASET_DIR, FILE_PATTERN.format(split_name))\r\n\r\n    file_names = _get_file_names(file_pattern, randomize_input=True)\r\n\r\n    dataset = tcd.TFRecordDataset(file_names)\r\n    dataset = dataset.map(_parse_function)\r\n\r\n    dataset = dataset.map(lambda image, color, shape: image)\r\n    dataset = dataset.shuffle(buffer_size=10000)\r\n    dataset = dataset.repeat().batch(group_size * batch_size)\r\n\r\n    iterator = dataset.make_one_shot_iterator()\r\n    images = iterator.get_next()\r\n\r\n    images = tf.split(images, group_size, axis=0)\r\n    images = [tf.reshape(x, [batch_size] + IMAGE_SHAPE) for x in images]\r\n\r\n    return images\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    with tf.Graph().as_default():\r\n        x_1, x_2, x_3 = get_batch(batch_size=32,\r\n                                  group_size=3)\r\n\r\n        val = tf.reduce_sum(tf.add_n([x_1, x_2, x_3]))\r\n        val = tf.Print(val, [tf.constant(0)], \"I'm alive! \")\r\n\r\n        global_step = slim.get_or_create_global_step()\r\n        with tf.control_dependencies([val]):\r\n            update_global_step_op = tf.assign_add(global_step, 1)\r\n\r\n        train_op = update_global_step_op\r\n\r\n        tf.summary.scalar('Summary 1', val)\r\n        tf.summary.scalar('Summary 2', train_op)\r\n\r\n        logdir = 'mwe_logdir'\r\n        slim.learning.train(\r\n            train_op=train_op,\r\n            logdir=logdir,\r\n            number_of_steps=1000000)\r\n```\r\n", "comments": ["Thanks for reporting this... it definitely looks like a bug. I think I've tracked it down to the `\"OneShotIterator\"` op, which internally blocks on this line while a function executes to build the dataset:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/91cb809bd6bc885458c583d46f4a322c30fa12cf/tensorflow/core/kernels/iterator_ops.cc#L248\r\n\r\nThat will block one of the inter-op thread pool threads for the (typically short) execution time of the dataset construction function. The number of CPUs determines the default number of threads in that thread pool: when you have only 1 CPU, the system will deadlock as soon as you hit that line (because no more thread pool threads are available to run the function that will unblock it). When you have 2 CPUs, it can work, *but* `slim.learning.train()` uses `tf.train.Supervisor`, which asynchronously runs a background thread... that runs the same `\"OneShotIterator\"` op. To ensure that the op only initializes once, the initialization runs under a lock, acquired here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/91cb809bd6bc885458c583d46f4a322c30fa12cf/tensorflow/core/kernels/iterator_ops.cc#L194\r\n\r\nThe problem is probably starting to become clear: two concurrent executions of the same `\"OneShotIterator\"` kernel will potentially block two inter-op thread pool threads, leading to deadlock in a 2-CPU system, because there are no more threads available to run the function that will unblock them.\r\n\r\nAnyway, mea culpa, and thanks again for finding the bug. I'll be working on a fix, although it might not make it into the final 1.2 release. In the mean time, there are a couple of workarounds:\r\n\r\n1. Increase the number of threads to more than 2 in the inter-op thread pool. You can do this by passing `session_config=tf.ConfigProto(inter_op_parallelism_threads=3)` to `slim.learning.train()`.\r\n\r\n2. Use `dataset.make_initializable_iterator()` instead of `dataset.make_one_shot_iterator()`. This comes with the additional requirement that you have to run `iterator.initializer`, which is not completely trivial with `slim.learning.train()` because you don't have access to the `tf.Session`. One possibility is to pass `local_init_op=tf.group(tf.local_variables_initializer(), tf.tables_initializer(), iterator.initializer)` to `slim.learning.train()`. ", "Wow, thanks for the swift response! The first workaround seems to be working perfectly.\r\n\r\nOn a side note, I'm quite happy with the `tf.contrib.data`. My code has become way cleaner."]}, {"number": 10518, "title": "some op frequently disappears in the log of tfprof", "body": "Please go to Stack Overflow for help and support:\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat 4.8.3-9\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.0\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: N.A   the code is runned with CPU\r\n- **GPU model and memory**: N.A\r\n- **Exact command to reproduce**: python the_source_code.py\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI tried to use the scripts below to profile the running time of tf.matmul(). But, with each different runs, some ops frequently disappear from the log of tfprof. I tried to add time stamp in the Compute() method of the underlying op, and it shows that the running time is quite stable for different runs. The same problem also happens with larger networks. I'm using CPU other than GPU in this case.\r\n\r\nsometimes the log is like this, MatMul reports 0us, which is definitely not true:\r\n\r\n==================Model Analysis Report======================\r\n_TFProfRoot (0B/47.32MB, 0us/104.00ms)\r\n  **MatMul (6.76MB/6.76MB, 0us/0us)**\r\n  random_normal (6.76MB/20.28MB, 1.20ms/52.71ms)\r\n    random_normal/RandomStandardNormal (6.76MB/6.76MB, 50.20ms/50.20ms)\r\n    random_normal/mean (4B/4B, 0us/0us)\r\n    random_normal/mul (6.76MB/6.76MB, 1.30ms/1.30ms)\r\n    random_normal/shape (8B/8B, 2us/2us)\r\n    random_normal/stddev (4B/4B, 0us/0us)\r\n  random_normal_1 (6.76MB/20.28MB, 0us/51.29ms)\r\n    random_normal_1/RandomStandardNormal (6.76MB/6.76MB, 51.29ms/51.29ms)\r\n    random_normal_1/mean (0B/0B, 0us/0us)\r\n    random_normal_1/mul (6.76MB/6.76MB, 0us/0us)\r\n    random_normal_1/shape (0B/0B, 0us/0us)\r\n    random_normal_1/stddev (0B/0B, 0us/0us)\r\n\r\n\r\n\r\nsome times like this, which is expected:\r\n\r\n==================Model Analysis Report======================\r\n_TFProfRoot (0B/47.32MB, 0us/82.61ms)\r\n  **MatMul (6.76MB/6.76MB, 36.83ms/36.83ms)**\r\n  random_normal (6.76MB/20.28MB, 2.21ms/41.42ms)\r\n    random_normal/RandomStandardNormal (6.76MB/6.76MB, 37.09ms/37.09ms)\r\n    random_normal/mean (4B/4B, 0us/0us)\r\n    random_normal/mul (6.76MB/6.76MB, 2.13ms/2.13ms)\r\n    random_normal/shape (8B/8B, 0us/0us)\r\n    random_normal/stddev (4B/4B, 0us/0us)\r\n  random_normal_1 (6.76MB/20.28MB, 2.17ms/4.36ms)\r\n    random_normal_1/RandomStandardNormal (6.76MB/6.76MB, 0us/0us)\r\n    random_normal_1/mean (0B/0B, 0us/0us)\r\n    random_normal_1/mul (6.76MB/6.76MB, 2.19ms/2.19ms)\r\n    random_normal_1/shape (0B/0B, 0us/0us)\r\n    random_normal_1/stddev (0B/0B, 0us/0us)\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\nsize = 1300\r\n\r\ndef main():\r\n  x = tf.random_normal(shape = [1, size])\r\n  w = tf.random_normal(shape = [size, 2*size])\r\n  y = tf.matmul(x,w)\r\n\r\n  with tf.Session(config=tf.ConfigProto(device_count={'GPU': 0, 'CPU': 1}, \\\r\n    inter_op_parallelism_threads = 1, intra_op_parallelism_threads = 1, \\\r\n    log_device_placement=True)) as sess:\r\n\r\n    run_metadata = tf.RunMetadata()\r\n    opts = tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY\r\n    opts['min_micros'] = 0\r\n    opts['min_bytes'] = 0\r\n    predictions = sess.run(y,\r\n                         options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\r\n                         run_metadata=run_metadata)\r\n    tf.contrib.tfprof.model_analyzer.print_model_analysis(\r\n                        tf.get_default_graph(),\r\n                        run_meta=run_metadata,\r\n                        tfprof_options=opts)\r\n \r\nif __name__ == '__main__':\r\n  main()\r\n\r\n\r\n```", "comments": ["I also met for this problem. Sometimes the most time-consuming operations in theory are always 0us in tfprof. ", "@linearhit thanks for reporting the issue.\r\nDo you mean tfprof shows non-deterministic behaior? The code you provided sometimes shows 0 MatMul time, while other times shows >0 MatMul time?", "@wangsiyu It would be great if you could give me a sample.", "@linearhit \r\n\r\nLooks like you are running in CPU?\r\nI run the following test for 100 times without failure. Could you give me more information?\r\n```python\r\n  def testGithubIssue10518(self):\r\n    x = tf.random_normal(shape = [1, 1300])\r\n    w = tf.random_normal(shape = [1300, 2*1300])\r\n    y = tf.matmul(x,w)\r\n\r\n    with tf.Session(config=tf.ConfigProto(device_count={'GPU': 0, 'CPU': 1}, \\\r\n      inter_op_parallelism_threads = 1, intra_op_parallelism_threads = 1, \\\r\n      log_device_placement=True)) as sess:\r\n\r\n      run_metadata = tf.RunMetadata()\r\n      opts = tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY\r\n      opts['min_micros'] = 0\r\n      opts['min_bytes'] = 0\r\n      _ = sess.run(y,\r\n                   options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\r\n                   run_metadata=run_metadata)\r\n      tfprof_node = model_analyzer.print_model_analysis(\r\n            tf.get_default_graph(),\r\n            run_meta=run_metadata,\r\n            tfprof_options=opts)\r\n\r\n      self.assertEqual(tfprof_node.children[0].name, 'MatMul')\r\n      self.assertGreater(tfprof_node.children[0].exec_micros, 100)\r\n```", "Also tried GPU\r\n```python\r\n  def testGithubIssue10518(self):\r\n    size = 1300\r\n\r\n    with tf.device('/gpu:0'):\r\n      x = tf.random_normal(shape = [1, size])\r\n      w = tf.random_normal(shape = [size, 2 * size])\r\n      y = tf.matmul(x,w)\r\n\r\n    with tf.Session(config=tf.ConfigProto(\r\n        inter_op_parallelism_threads = 1,\r\n        intra_op_parallelism_threads = 1,\r\n        log_device_placement=True)) as sess:\r\n\r\n      run_metadata = tf.RunMetadata()\r\n      opts = tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY\r\n      opts['min_micros'] = 0\r\n      opts['min_bytes'] = 0\r\n      _ = sess.run(y,\r\n                   options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\r\n                   run_metadata=run_metadata)\r\n      tfprof_node = model_analyzer.print_model_analysis(\r\n            tf.get_default_graph(),\r\n            run_meta=run_metadata,\r\n            tfprof_options=opts)\r\n\r\n      self.assertEqual(tfprof_node.children[0].name, 'MatMul')\r\n      self.assertGreater(tfprof_node.children[0].exec_micros, 10)\r\n```", "Do you mean tfprof shows non-deterministic behaior? The code you provided sometimes shows 0 MatMul time, while other times shows >0 MatMul time?\r\n\r\n@panyx0718 , yes, this is what i observed. \r\nWith half the probability it reports 0us.  I'm running it on CPU.\r\nPlease let me know what else information do you need, thanks.", "@linearhit \r\n\r\nCould use share the RunMetadata file and the GraphDef with me?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 10517, "title": "TensorFlow Convolution Code Optmization", "body": "I am using C++ version of TensorFLow and have built 'TensorFlow for Android' successfully using below command\r\n'bazel build -c opt //tensorflow/examples/android:tensorflow_demo'\r\n as described in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#bazel\r\n\r\nI am trying to optimize the convolution code. Below are the issues faced\r\n\r\n1. Unable to find the exact location of convolution code. \r\n    I am able to debug the code till below function in  \r\n\r\n   'return choose(\r\n      Cond<internal::traits<Input>::Layout == ColMajor>(),\r\n      kernel.reshape(kernel_dims)\r\n          .contract(input\r\n                        .extract_image_patches(\r\n                            kernelRows, kernelCols, row_stride, col_stride,\r\n                            row_in_stride, col_in_stride, padding_type)\r\n                        .reshape(pre_contract_dims),\r\n                    contract_dims)\r\n          .reshape(post_contract_dims),\r\n      input\r\n          .extract_image_patches(kernelRows, kernelCols, row_stride, col_stride,\r\n                                 row_in_stride, col_in_stride, padding_type)\r\n          .reshape(pre_contract_dims)\r\n          .contract(kernel.reshape(kernel_dims), contract_dims)\r\n          .reshape(post_contract_dims));'\r\n\r\nas present in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions.h\r\n\r\nI have few questions related to the above function.\r\n\r\n1.1 Is the above function really performing convolution ? If so where is the code ?\r\n1.2 Is contraction (contract function ) same as convolution ? If both convolution and contraction are same, why is the contract operation being performed to both input and kernel matrix ?\r\n1.3 Where are the definitions of functions - choose, reshape, contract ,extract image patches etc ?\r\n\r\n2. Unable to extract data (matrices ) from input and kernel matrix .This is in reference to the same page\r\n https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions.h \r\n\r\n2.1 I have found a line of code 'kern(kernel);' at line no 946 in the above page. Can I know the location definition of the above function ?\r\n\r\n2.2 I am unable to extract input and kernel matrices from the corresponding 4d tensors(input and kernel) as a float array, as i would like to try optimizing the convolution code using parallel processing.I have looked at https://www.tensorflow.org/api_docs/cc/class/tensorflow/tensor  but I coudn'yt find any method to convert Tensor Matrices from Tensor 4D to an array. \r\n\r\nPlease help me in answering the above questions\r\n\r\n  \r\n\r\n\r\n", "comments": ["Its picking from the eigen TensorFlow files from cache"]}, {"number": 10516, "title": "Disable stage_op_test and map_stage_op_test due to timeouts", "body": "For example:\r\nhttp://ci.tensorflow.org/job/tensorflow-master-linux-gpu/2381/console\r\n\r\nAlso reverted the size to medium, since switching to large didn't fix the issue.", "comments": []}, {"number": 10515, "title": "Tensorboard 'Site cannot be reached' error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Installed with Docker as per https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#1\r\n- **TensorFlow version (use command below)**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: not being used\r\n- **GPU model and memory**: GPU not being used\r\n- **Exact command to reproduce**: \r\n\r\n\t\t:/tf_files# tensorboard --logdir training_summaries --debug\r\n\t\tStarting TensorBoard 47 at http://0.0.0.0:6006\r\n\t\t(Press CTRL+C to quit)\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nHi. I'm following this tutorial for image classification: https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#4 but with my own image dataset. I let the retraining run overnight, and it successfully completed. Now I am trying to visualize the results with Tensorboard. \r\n\r\nI ran tensorboard and this is the only terminal output:\r\n\r\n    \t:/tf_files# tensorboard --logdir training_summaries --debug\r\n\t\tStarting TensorBoard 47 at http://0.0.0.0:6006\r\n\t\t(Press CTRL+C to quit)\r\n\r\nWhen I visit http://0.0.0.0:6006 the page does not load and it says 'Site cannot be reached.'\r\n\r\nMy retraining command was: \r\n\r\n    python retrain.py \\\r\n\t  --bottleneck_dir=bottlenecks \\\r\n\t  --model_dir=inception \\\r\n\t  --summaries_dir=training_summaries/long \\\r\n\t  --output_graph=retrained_graph.pb \\\r\n\t  --output_labels=retrained_labels.txt \\\r\n\t  --image_dir=data\r\n\t  \r\nCan anyone help? I'm new to TF so I appreciate the advice. \r\n", "comments": ["I don't think this is a Tensorflow/Tensorboard bug but I think it is the way you have docker set up. It's been some time since I played with docker but I think that you might have to forward the docker port to one on your local host. The tutorial you followed did include this but it might be worth making sure that your ports are forwarding correctly. The key line in this code is the publish argument\r\n```\r\ndocker run -it \\\r\n  --publish 6006:6006 \\\r\n  --volume ${HOME}/tf_files:/tf_files \\\r\n  --workdir /tf_files \\\r\n  tensorflow/tensorflow:1.1.0 bash\r\n```\r\nThat should be enough but if you're ports aren't forwarding properly may I suggest that you try some of these commands to expose the Docker ports: \r\n[Docker - Exposing Ports](https://github.com/wsargent/docker-cheat-sheet#exposing-ports)\r\n\r\nStack Overflow has some good questions about it but some of them are over complicated for your needs.\r\n\r\nIf you have tried that and are convinced it's a problem with Tensorflow then keep this open and hopefully someone else can help troubleshoot! Hope this helps in the mean time.", "Closing for now, but please reopen if you can't get help on stackoverflow, or this seems to be a real TensorFlow bug.", "```tensorboard --logdir=<$MODEL_DIR> --host localhost ```\r\nthis would solve it. ", "> `tensorboard --logdir=<$MODEL_DIR> --host localhost `\r\n> this would solve it.\r\n\r\nWhen I try this, I get \"Access is denied\". Do you happen to know why?", "I am getting a similar error.I am running my jupyter notebook that trains a tensorflow model(ResNet) and when I call tensorboard in command line using \r\n`tensorboard` --logdir=\"path/to/logs/\"`\r\n It direct to a port and when I access the link it says\r\n\r\n> This site can\u2019t be reached... took too long to respond.\r\nThanks !", "> `tensorboard --logdir=<$MODEL_DIR> --host localhost `\r\n> this would solve it.\r\n\r\nThanks this worked for me \ud83d\udc4d \u263a\ufe0f \r\nI have one question can't we download the structure graph and view on the system instead of viewing it on tensorboard? Thanks in advance ", "> I am getting a similar error.I am running my jupyter notebook that trains a tensorflow model(ResNet) and when I call tensorboard in command line using\r\n> `tensorboard` --logdir=\"path/to/logs/\"`\r\n> It direct to a port and when I access the link it says\r\n> \r\n> > This site can\u2019t be reached... took too long to respond.\r\n> > Thanks !\r\n\r\nwere you able to solve it?"]}, {"number": 10512, "title": "Delete non-deterministic testEmpty() test", "body": "Causing flaky Windows cmake tests because `np.empty()` may return a singular matrix.\r\n```\r\n15:26:41 ======================================================================\r\n15:26:41 ERROR: testEmpty (__main__.MatrixSolveOpTest)\r\n15:26:41 ----------------------------------------------------------------------\r\n15:26:41 Traceback (most recent call last):\r\n15:26:41   File \"C:/tf_jenkins/home/workspace/tensorflow-master-win-cmake-py/tensorflow/python/kernel_tests/matrix_solve_op_test.py\", line 102, in testEmpty\r\n15:26:41     self._verifySolve(np.empty([2, 2]), np.empty([2, 0]))\r\n15:26:41   File \"C:/tf_jenkins/home/workspace/tensorflow-master-win-cmake-py/tensorflow/python/kernel_tests/matrix_solve_op_test.py\", line 48, in _verifySolve\r\n15:26:41     np_ans = np.linalg.solve(a_np, b)\r\n15:26:41   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line 375, in solve\r\n15:26:41     r = gufunc(a, b, signature=signature, extobj=extobj)\r\n15:26:41   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line 90, in _raise_linalgerror_singular\r\n15:26:41     raise LinAlgError(\"Singular matrix\")\r\n15:26:41 numpy.linalg.linalg.LinAlgError: Singular matrix\r\n```", "comments": ["Jenkins, test this please", "Jenkins, test this please.", "Approving and merging this to turn CI green.\r\nWe can discuss what we can do to replace the tests tomorrow."]}, {"number": 10511, "title": "Printdhruv deep cnn.md", "body": "Typo", "comments": ["Can one of the admins verify this patch?", "Thanks for the pull request! Both versions are correct, but since the change is unnecessary, closing."]}, {"number": 10510, "title": "Remove RewriterConfig from the Python API", "body": "RewriterConfig has not made it into a release, and so is not subject to semantic versioning. The API needs a bit of work, so it's not going into 1.2.\r\n\r\nIn order to minimize disruption to non-Google users (some using tf.RewriterConfig from master) and hold up the 1.2 release as little as possible, the plan is to *only* remove RewriterConfig from the 1.2 release, but leave it in master. We will then develop a more permanent way to configure Grappler graph rewriting.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please", "Jenkins, test this please."]}, {"number": 10509, "title": "[Bash] Declare and assign separately", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2155", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please.\r\n", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 10508, "title": "[Bash] Prefer read -a to split path", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2207", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Windows failure known issue, merging."]}, {"number": 10507, "title": "[Bash] Prefer [ p ] && [ q ] over [ p -a q ]", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2166", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please.", "Windows failure known issue. merging."]}, {"number": 10506, "title": "[Bash] Remove unquoting quotes", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2027", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 10505, "title": "[Bash] Use $(...) instead of legacy `...`", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2006", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please.", "Closing out the rest of the static analysis changes. Please convert them to one pull request. It's easier to review and test that way :)"]}, {"number": 10504, "title": "[Bash] read with -r to not mangle backslashes", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2162", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please.\r\n"]}, {"number": 10503, "title": "[Bash] Simplify Conditional", "body": "", "comments": ["Jenkins, test this please", "Windows failures known issues, merging."]}, {"number": 10502, "title": "A few changes to kernel_tests.", "body": "@meteorcloudy FYI", "comments": ["Jenkins, test this please.", "stage_op_test is still timing out too. Set that to large?", "I think it just gets stuck.\r\nI can set it to large, but I think we will need to disable it."]}, {"number": 10501, "title": "Fix numpy 1.13 incompatibilities", "body": "Fixes the CPU build", "comments": []}, {"number": 10500, "title": "Open up visibility of tf_imports", "body": "This is a very simple change that unblocks our friend @jameswex. Hasty merge appreciated.\r\n\r\nThis change was previously done in https://github.com/tensorflow/tensorflow/commit/9dc38431dffbccca1ae817f460fe4b1bfcb3ceb0 but got undone by mistake.", "comments": ["Jenkins, test this please", "Ignoring unrelated failures (looking at them right now)."]}, {"number": 10499, "title": "reader_ops_test failing on windows for Bazel CI", "body": "On Bazel's CI system, TensorFlow's reader_ops_test is failing on all Windows builds, and has been since between June 1 and June 5:\r\nhttp://ci.bazel.io/view/Dashboard/job/TensorFlow/863/\r\n\r\nThe logs don't show anything useful, unfortunately.:\r\nhttp://ci.bazel.io/view/Dashboard/job/TensorFlow/862/BAZEL_VERSION=HEAD,PLATFORM_NAME=windows-msvc-x86_64/consoleFull\r\n```\r\nFAIL: //py_test_dir/tensorflow/python/kernel_tests:reader_ops_test (see C:/tmp/_bazel_system/bcthfi-n/execroot/org_tensorflow/bazel-out/msvc_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/kernel_tests/reader_ops_test/test.log)\r\nINFO: From Testing //py_test_dir/tensorflow/python/kernel_tests:reader_ops_test:\r\n==================== Test output for //py_test_dir/tensorflow/python/kernel_tests:reader_ops_test:\r\n..........================================================================================\r\n```\r\n\r\n", "comments": ["@meteorcloudy told us that this is actually a bug in bazel:\r\nhttps://github.com/bazelbuild/bazel/issues/3134\r\n\r\nClosing this issue as this is not a bug in TF.\r\nPlease reopen if investigation in bazel proves otherwise."]}]