[{"number": 1119, "title": "Normalization of image", "body": "Generally pixel intensity or features are normalised to [-1,1] range. But, here the range was [-0.5,0.5]. \nSorry for inconvenience, if it was intentional.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "The range does not matter, as long as it's O(1.0).\n"]}, {"number": 1118, "title": "use string::append to catenate strings", "body": "With VC++, when std::string is empty, there will be an error -- \"string iterator not dereferencable\" -- to use string::begin.\n\nAnd using raw pointer to manipulate std::string doesn't look like a good idea.\n", "comments": ["something is wrong, closing it.\n"]}, {"number": 1117, "title": "embedding_lookup on multiple dimensions with AdagradOptimizer throwing exception", "body": "I am completing the Udacity course on Tensorflow, and noticed that when embedding_lookup is used in 3 dimensions with AdagradOptimizer, the optimizer throws an error:\n\nCODE (with error):\n\n```\nimport math\nimport tensorflow as tf\n\nbatch_size = 128\nembedding_size = 128 # Dimension of the embedding vector.\nskip_window = 1 # How many words to consider left and right.\nnum_sampled = 64 # Number of negative examples to sample.\nvocabulary_size = 50000\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, 2 * skip_window])\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n\n  # Variables.\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n  softmax_weights = tf.Variable(\n    tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size)))\n  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n  # Model.\n  # Look up embeddings for inputs.\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n  embed2 = tf.Variable(tf.zeros([batch_size, embedding_size]))\n  for i in xrange(2*skip_window):\n    embed2 += embed[:, i, :]\n  # Compute the softmax loss, using a sample of the negative labels each time.\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed2,\n                               train_labels, num_sampled, vocabulary_size))\n  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n```\n\nError message:\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-32-104452f9cf81> in <module>()\n     39     tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed2,\n     40                                train_labels, num_sampled, vocabulary_size))\n---> 41   optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, name)\n    186         aggregation_method=aggregation_method)\n    187     return self.apply_gradients(grads_and_vars, global_step=global_step,\n--> 188                                 name=name)\n    189 \n    190   def compute_gradients(self, loss, var_list=None, gate_gradients=GATE_OP,\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)\n    287             update_ops.append(self._apply_dense(grad, var))\n    288           else:\n--> 289             update_ops.append(self._apply_sparse(grad, var))\n    290       if global_step is None:\n    291         return self._finish(update_ops, name)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.pyc in _apply_sparse(self, grad, var)\n     75     return training_ops.sparse_apply_adagrad(\n     76         var, acc, self._learning_rate_tensor, grad.values, grad.indices,\n---> 77         use_locking=self._use_locking)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.pyc in sparse_apply_adagrad(var, accum, lr, grad, indices, use_locking, name)\n    200   return _op_def_lib.apply_op(\"SparseApplyAdagrad\", var=var, accum=accum,\n    201                               lr=lr, grad=grad, indices=indices,\n--> 202                               use_locking=use_locking, name=name)\n    203 \n    204 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self, op_type_name, g, name, **keywords)\n    662         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    663                          input_types=input_types, attrs=attr_protos,\n--> 664                          op_def=op_def)\n    665         outputs = op.outputs\n    666         return _Restructure(ops.convert_n_to_tensor_or_indexed_slices(outputs),\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes)\n   1834                     original_op=self._default_original_op, op_def=op_def)\n   1835     if compute_shapes:\n-> 1836       set_shapes_for_outputs(ret)\n   1837     self._add_op(ret)\n   1838     self._record_op_seen_by_control_dependencies(ret)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1474       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1475                          % op.type)\n-> 1476   shapes = shape_func(op)\n   1477   if len(op.outputs) != len(shapes):\n   1478     raise RuntimeError(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_ops.pyc in _SparseApplyAdagradShape(op)\n    115   _AssertInputIsScalar(op, 2)  # lr\n    116   grad_shape = op.inputs[3].get_shape().merge_with(\n--> 117       tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n    118   unused_indices_shape = op.inputs[4].get_shape().merge_with(\n    119       tensor_shape.vector(grad_shape[0]))\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in merge_with(self, other)\n    525       return other\n    526     else:\n--> 527       self.assert_same_rank(other)\n    528       new_dims = []\n    529       for i, dim in enumerate(self._dims):\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in assert_same_rank(self, other)\n    568       if self.ndims != other.ndims:\n    569         raise ValueError(\n--> 570             \"Shapes %s and %s must have the same rank\" % (self, other))\n    571 \n    572   def assert_has_rank(self, rank):\n\nValueError: Shapes TensorShape([Dimension(None), Dimension(None), Dimension(None)]) and TensorShape([Dimension(None), Dimension(128)]) must have the same rank\n```\n\nJust change it to get embedding look up once for each time for the third dimension:\n\n```\n  # Look up embeddings for inputs.\n  # embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n  # print embed.get_shape()\n  embed2 = tf.Variable(tf.zeros([batch_size, embedding_size]))\n  for i in xrange(2*skip_window):\n    embed2 += tf.nn.embedding_lookup(embeddings, train_dataset[:, i])\n```\n\nThe code runs smoothly!\n", "comments": ["You can still do a single lookup, and `tf.reduce_sum(embed, [1])` instead of += to aggregate.\n", "@vincentvanhoucke @vrv This is the first thing that I tried. This does not work. It still throws the same exception.\nIn fact, if the optimizer is changed to GradientDescent, the error goes away.\n\nAlso I think that this should fail gracefully with a message saying that \"XXX is not supported with AdagradOptimizer\" instead of a cryptic message that makes the user believe that it's his fault.\n\nI am not the only one with this issue, check out the discussion on udacity deep learning forums: \n1. https://discussions.udacity.com/t/assignment-5-value-error-tensors-must-have-the-same-rank/46235\n2. https://discussions.udacity.com/t/assignment-5-general-understanding-of-cbow-vs-skip-gram-model/46393/8\n3. https://discussions.udacity.com/t/assignment-5-different-tensor-ranks/157296\n", "Thanks for the pointers. Let me take a look. Having different behaviors for different optimizers seems very fishy.\n", "Just to note, I ran into this as well. From memory both `GradientDescent` and `AdamOptimizer` worked happily but `AdagradOptimizer` threw errors. Thinking it was related to the single lookup issue @vincentvanhoucke mentioned and assumed it was expected behaviour.\n\n@shaileshahuja - whilst not equivalent, a temporary helper would be to use `AdamOptimizer` as it will give you many of the adaptive learning rate advantages of `AdagradOptimizer`.\n", "@shaileshahuja @Smerity I can run the code fine at the head of the TensorFlow tree. Are you using a specific version of TensorFlow? The Docker container?\n", "@Smerity Thank you!\n@vincentvanhoucke I am using the docker container with tensorflow==0.6.0.\n", "@vincentvanhoucke I updated the version of tensorflow to 0.7.0 in the docker instance, which solved this issue. Must have been addressed as a part of another bug fix I guess :smile: \n", "Ok, thanks. I'll close this now. Let me know if it affects the Docker container that was built for the assignments. I'm pretty sure it does not, but I can rebuild it if it does.\n", "I'm getting a similar optimizer dependent error with the following code on 0.7.0 (hash 875a67f3d298ab7f7438a3004e4597c71d92b529)\n\n```\nassert tf.__version__ == '0.7.0'\ntf.reset_default_graph()\n\ninitializer = tf.random_uniform_initializer(-0.1, 0.1)\nx = tf.placeholder(tf.int32)\np = tf.get_variable('p', shape=[10, 2], initializer=initializer)\nx_ = tf.nn.embedding_lookup(p, x)\n\nloss = tf.reduce_sum(tf.abs(x_))\nopt = tf.train.RMSPropOptimizer(0.01, decay=0.9, epsilon=0.00001)\ntrain_op = opt.minimize(loss)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    feed = {x: 3}\n    print(sess.run([x_], feed_dict=feed))\n```\n\nThe code works with Adam and SGD, but not RMSProp.\n"]}, {"number": 1116, "title": "keep numpy version in pip.sh", "body": "We should keep building against numpy 1.8.2 - the version in ubuntu 14.04.\n", "comments": ["Note: user home directory is cached between the builds - bazel is picking up the newer version installed after first run of pip.sh\n\nAlso this should be able to automatically downgrade all the installations in all the jobs on ci.tensorflow.org. But it will need two builds. First one will still pick the new version ob numpy.\n", "Ok, it works. But the first build fails. Just like on this PR. The first bazel build took 1.10.4 cached from previous run and then failed while running test with downgraded 1.8.2. Next runs should work.\n\n@tensorflow-jenkins: test this please\n", "Also the docker container build with this works :-)\n", "So we need to re-run the release twice to propagate this to the binaries?\n"]}, {"number": 1115, "title": "Update os_setup for 0.7", "body": "", "comments": ["one missing 0.6.0 ref but otherwise LGTM\n", "LGTM\n", "merged.\n"]}, {"number": 1114, "title": "update docker for 0.7.0", "body": "", "comments": ["Why change the URLs to `http` from `https`?\n", "Odd problem with urllib.\n\nOn Wed, Feb 17, 2016 at 8:02 AM Alex Rothberg notifications@github.com\nwrote:\n\n> Why change the URLs to http from https?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1114#issuecomment-185271733\n> .\n", "Care to elaborate?\n", "@jendap, what was the problem there?\nOn Sat, Feb 20, 2016 at 11:49 Alex Rothberg notifications@github.com\nwrote:\n\n> Care to elaborate?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1114#issuecomment-186670176\n> .\n"]}, {"number": 1113, "title": "Adding google/protobuf back to pip", "body": "", "comments": ["Can one of the admins verify this patch?\n", "LGTM\n", "merged.\n"]}, {"number": 1112, "title": "TensorFlow: add thread annotations to mutex classes to prevent", "body": "thread annotation warnings from popping up (mostly on Mac with\nclang).\n\nTests passed with this change, but this seems a bit ugly, and I wonder\nif there's a better way to do the annotation.  Suggestions welcome.\n", "comments": ["Perfect! I was looking at this a while back but never finished it. Merge it! It will kill gadzillion of warnings in mac build :-)\n", "merged.\n"]}, {"number": 1111, "title": "Fixing exit code in ci_parameterized_build.sh", "body": "Commit\nhttps://github.com/tensorflow/tensorflow/commit/00b5ea730ddf63e0cb49a9200f3b610cdbbdc13c#diff-6ea208942bad0a8c2194f5540b5dd618\n\nbroke the exit code. This changeset fixes it.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1110, "title": "Fix bitcast_op_test.py on Python 3", "body": "This test is failing for me on Python 3 because the `getbuffer` interface only exists in Python 2.\nReplacing it with `.data` should work for both versions.\n\nHere is the log output of the failing test:\n\n```\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n-----------------------------------------------------------------------------\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"DrawBoundingBoxes\" device_type: \"CPU\"') for unknown op: DrawBoundingBoxes\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SparseTensorDenseMatMul\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_COMPLEX64 } } } host_memory_arg: \"a_shape\"') for unknown op: SparseTensorDenseMatMul\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SparseTensorDenseMatMul\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"a_shape\"') for unknown op: SparseTensorDenseMatMul\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SparseTensorDenseMatMul\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_DOUBLE } } } host_memory_arg: \"a_shape\"') for unknown op: SparseTensorDenseMatMul\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SparseTensorDenseMatMul\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"a_shape\"') for unknown op: SparseTensorDenseMatMul\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SampleDistortedBoundingBox\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: SampleDistortedBoundingBox\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SampleDistortedBoundingBox\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: SampleDistortedBoundingBox\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SampleDistortedBoundingBox\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: SampleDistortedBoundingBox\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SampleDistortedBoundingBox\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: SampleDistortedBoundingBox\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SampleDistortedBoundingBox\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: SampleDistortedBoundingBox\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SampleDistortedBoundingBox\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_DOUBLE } } }') for unknown op: SampleDistortedBoundingBox\nE tensorflow/core/framework/op_kernel.cc:662] OpKernel ('op: \"SampleDistortedBoundingBox\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }') for unknown op: SampleDistortedBoundingBox\nE.EEEE.\n======================================================================\nERROR: testEmpty (__main__.BitcastTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py\", line 68, in testEmpty\n    self._testBitcast(x, datatype, shape)\n  File \"/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py\", line 31, in _testBitcast\n    buff_after = np.getbuffer(out)\nAttributeError: 'module' object has no attribute 'getbuffer'\n\n======================================================================\nERROR: testLarger (__main__.BitcastTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py\", line 46, in testLarger\n    self._testBitcast(x, datatype, shape)\n  File \"/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py\", line 31, in _testBitcast\n    buff_after = np.getbuffer(out)\nAttributeError: 'module' object has no attribute 'getbuffer'\n\n======================================================================\nERROR: testSameDtype (__main__.BitcastTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py\", line 51, in testSameDtype\n    self._testBitcast(x, x.dtype, shape)\n  File \"/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py\", line 31, in _testBitcast\n    buff_after = np.getbuffer(out)\nAttributeError: 'module' object has no attribute 'getbuffer'\n\n======================================================================\nERROR: testSameSize (__main__.BitcastTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py\", line 56, in testSameSize\n    self._testBitcast(x, tf.double, shape)\n  File \"/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py\", line 31, in _testBitcast\n    buff_after = np.getbuffer(out)\nAttributeError: 'module' object has no attribute 'getbuffer'\n\n======================================================================\nERROR: testSmaller (__main__.BitcastTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py\", line 40, in testSmaller\n    self._testBitcast(x, datatype, shape)\n  File \"/hepgpu1-data3/ibabusch/.cache/bazel/_bazel_ibabusch/f5ec547a72254d5c5a13ec3d76a853de/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/python/bitcast_op_test.runfiles/tensorflow/python/kernel_tests/bitcast_op_test.py\", line 31, in _testBitcast\n    buff_after = np.getbuffer(out)\nAttributeError: 'module' object has no attribute 'getbuffer'\n\n----------------------------------------------------------------------\nRan 7 tests in 0.026s\n\nFAILED (errors=5)\n```\n", "comments": ["Jenkins, test this please.\n", "You're, right! I managed to write `x.data` twice, which makes the test pass every time.\nChanging the first x \u2192 out shows that my original fix was wrong (as `x.data != out.data`).\nI've changed it to `memoryview(x).tobytes()`, which should work with both Python 2 and 3.\n", "Thank you!\n"]}, {"number": 1109, "title": "shrink pip package content", "body": "@martinwicke @caisq we need this and the questions is if we want to keep the \"external\" there.\n", "comments": ["The external contains a lot of things - all the javascript files for tensorboard (we probably want those), but also all the eigen header files (we don't need those), and perhaps many other things. I'm going to look at those directories and probably remove a few of them now. But that's hacky, not long term solution. \n", "It is updated to remove the eigen headers. All javascript stay.\n", "@benoitsteiner @danmane This is in, but I'd like your opinions on what can/should stay and what can be removed. \n", "We can absolutely shrink the pip package further. Right now we don't minify TensorBoard's HTML and JS, and I'm not sure whether we are taking the minified js or full js (or both) into the pip package. So there are some very low hanging fruit here.\n"]}, {"number": 1108, "title": "Basic support for trigonometric operations", "body": "I'd like to request support for basic trigonometric ops. Nothing fancy, but just simple stuff like tan and the arc complement (arcsin, arctan, etc). All that's there right now is sin and cos.\n", "comments": ["And, of course, tanh. :)\n\nThat's a good idea.\n", "Also all the hyperbolic trig functions.\n", "I gave a shot at implementing the first ones. \n", "#483 is related as well.\n", "#483 is probably separate since it involves a dtype change, and can't participate in the same pile of macros. \n", "I guess atan2 would get us part of the way there. Perhaps it can be included with the other elementary trig functions?\n", "Can this issue be closed? It looks like changes (in PR #1573) were merged into master back in May.\n", "Sure!\n", "Hope these functions can support batch input"]}, {"number": 1107, "title": "model_with_buckets in seq2seq.py", "body": "Hi all,\n\nThe online version of https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L873\n\n```\ndef model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,\n                       buckets, seq2seq, softmax_loss_function=None,\n                       per_example_loss=False, name=None):\n```\n\nis different with the one installed from tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n\n```\ndef model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,\n                       buckets, num_decoder_symbols, seq2seq,\n                       softmax_loss_function=None, name=None):\n```\n\n(extra **num_decoder_symbols** parameter) which caused \n\n```\nTypeError: model_with_buckets() takes at least 7 arguments (7 given) \n```\n\nwhen running https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py#L133.\n\nWhich one should I use?\n\nThanks,\n", "comments": ["If you have installed 0.6.0 from pip, always check out the code repository at the same branch/tag -- they should be compatible.\n\nIf you want the latest changes from master, you'll have to build the pip package from sources.\n"]}, {"number": 1106, "title": "Python API docs fix array syntax", "body": "Fix array syntax in comments for examples `tf.pow` and `tf.accumulate_n`\n", "comments": ["Can one of the admins verify this patch?\n", "I've also updated the docstrings in `tensorflow/python/ops/math_ops.py`.\n", "Thanks!\n"]}, {"number": 1105, "title": "cifar10 not running", "body": "Execution tracebacks of cyfar10.py and cifar10_train.py:\n\npython cifar10.py:\n\n```\nTraceback (most recent call last):\n  File \"/media/konet/01D15611945D72C0/Pycharm_ubuntu/tf_models/image/cifar10/cifar10.py\", line 53, in <module>\n    \"\"\"Number of images to process in a batch.\"\"\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_flags.py\", line 86, in DEFINE_integer\n    _define_helper(flag_name, default_value, docstring, int)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_flags.py\", line 60, in _define_helper\n    type=flagtype)\n  File \"/usr/lib/python2.7/argparse.py\", line 1297, in add_argument\n    return self._add_action(action)\n  File \"/usr/lib/python2.7/argparse.py\", line 1671, in _add_action\n    self._optionals._add_action(action)\n  File \"/usr/lib/python2.7/argparse.py\", line 1498, in _add_action\n    action = super(_ArgumentGroup, self)._add_action(action)\n  File \"/usr/lib/python2.7/argparse.py\", line 1311, in _add_action\n    self._check_conflict(action)\n  File \"/usr/lib/python2.7/argparse.py\", line 1449, in _check_conflict\n    conflict_handler(action, confl_optionals)\n  File \"/usr/lib/python2.7/argparse.py\", line 1456, in _handle_conflict_error\n    raise ArgumentError(action, message % conflict_string)\nargparse.ArgumentError: argument --batch_size: conflicting option string(s): --batch_size\n```\n\npython cifar10.py:\n\n```\nTraceback (most recent call last):\n  File \"/media/konet/01D15611945D72C0/Pycharm_ubuntu/tf_models/image/cifar10/cifar10_train.py\", line 135, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/media/konet/01D15611945D72C0/Pycharm_ubuntu/tf_models/image/cifar10/cifar10_train.py\", line 128, in main\n    if tf.gfile.Exists(FLAGS.train_dir):\nAttributeError: 'module' object has no attribute 'gfile'\n```\n\nUsing:\ntensorflow v 0.6 + Titan GPU configured for GPU usage as per instruction in setup.\n", "comments": ["Something similar with MNIST: (TF built from source from scratch 10 minutes ago, master branch)\n\n> Traceback (most recent call last):\n>   File \"/home/****_/**_**/****/expr.py\", line 133, in <module>\n>     mnist = input_data.read_data_sets('MNIST_data')\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 199, in read_data_sets\n>     train_images = extract_images(local_file)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 57, in extract_images\n>     with tf.gfile.Open(filename) as f, gzip.GzipFile(fileobj=f) as bytestream:\n> AttributeError: 'module' object has no attribute 'Open'\n> Extracting MNIST_data/train-images-idx3-ubyte.gz\n> \n> Process finished with exit code 1\n", "I fixed the latter one, I'm not sure why the former is happening.  You shouldn't be running cifar10.py -- it's a library called by the other executables.\n", "(Note that you have to build from sources to get the fix.  Our first post-0.7 binary release will likely get this fix.)\n", "I'm confused by the post-0.7 part, as far as I can see the 0.7 branch is still behind in some aspects to master, you have already built the 0.7 release binaries?\n", "Yeah, the binaries are being built right now, we can maybe make a patch-release with these fixes (I expect we're going to want a protobuf fix in the patch release too).\n", "In the instructions says (https://www.tensorflow.org/versions/v0.6.0/tutorials/deep_cnn/index.html) \n\nThat the following needs to be executed to train the network:\npython cifar10_train.py\n\nIs this fixed now?\n", "If you build from sources at HEAD, yes.\n", "updating tensorflow install with `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl` fixed the cifar10 issue. \n(PS: for my case I am using the CPU version.) \n"]}, {"number": 1104, "title": "Failure to build, gcc 5.2.1, cuda 7.0", "body": "Hi,\n\nI followed the instructions at https://www.tensorflow.org/versions/v0.6.0/get_started/os_setup.html and tried building from source. I installed bazel 0.1.5 and I have gcc 5.2.1, cuda 7.0 and a GTX 960.\n\n`bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`\nfails with \n\n> INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_sub.cu.cc:\n> /usr/include/c++/5/bits/stl_iterator_base_types.h(154): error: class \"std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>\" has no member \"iterator_category\"\n>           detected during:\n>             instantiation of class \"std::__iterator_traits<_Iterator, void> [with _Iterator=std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>]\" \n> (163): here\n>             instantiation of class \"std::iterator_traits<_Iterator> [with _Iterator=std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>]\" \n> /usr/include/c++/5/sstream(348): here\n>             instantiation of class \"std::__cxx11::basic_stringbuf<_CharT, _Traits, _Alloc> [with _CharT=char, _Traits=std::char_traits<char>, _Alloc=std::allocator<char>]\" \n> /usr/include/c++/5/bits/sstream.tcc(272): here\n> \n> /usr/include/c++/5/type_traits(1492): error: class \"std::__is_convertible_helper<<error-type>, std::input_iterator_tag, false>\" has no member class \"type\"\n>           detected during:\n>             instantiation of class \"std::is_convertible<_From, _To> [with _From=<error-type>, _To=std::input_iterator_tag]\" \n> /usr/include/c++/5/sstream(348): here\n>             instantiation of class \"std::__cxx11::basic_stringbuf<_CharT, _Traits, _Alloc> [with _CharT=char, _Traits=std::char_traits<char>, _Alloc=std::allocator<char>]\" \n> /usr/include/c++/5/bits/sstream.tcc(272): here\n> \n> /usr/include/c++/5/type_traits(1492): error: not a class or struct name\n>           detected during:\n>             instantiation of class \"std::is_convertible<_From, _To> [with _From=<error-type>, _To=std::input_iterator_tag]\" \n> /usr/include/c++/5/sstream(348): here\n>             instantiation of class \"std::__cxx11::basic_stringbuf<_CharT, _Traits, _Alloc> [with _CharT=char, _Traits=std::char_traits<char>, _Alloc=std::allocator<char>]\" \n> /usr/include/c++/5/bits/sstream.tcc(272): here\n> \n> /usr/include/c++/5/bits/stl_iterator_base_types.h(154): error: class \"std::__cxx11::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t>>\" has no member \"iterator_category\"\n>           detected during:\n>             instantiation of class \"std::__iterator_traits<_Iterator, void> [with _Iterator=std::__cxx11::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t>>]\" \n> (163): here\n>             instantiation of class \"std::iterator_traits<_Iterator> [with _Iterator=std::__cxx11::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t>>]\" \n> /usr/include/c++/5/sstream(348): here\n>             instantiation of class \"std::__cxx11::basic_stringbuf<_CharT, _Traits, _Alloc> [with _CharT=wchar_t, _Traits=std::char_traits<wchar_t>, _Alloc=std::allocator<wchar_t>]\" \n> /usr/include/c++/5/bits/sstream.tcc(278): here\n> \n> /usr/include/c++/5/bits/stl_iterator_base_types.h(154): error: class \"std::vector<std::seed_seq::result_type, std::allocator<std::seed_seq::result_type>>\" has no member \"iterator_category\"\n>           detected during:\n>             instantiation of class \"std::__iterator_traits<_Iterator, void> [with _Iterator=std::vector<std::seed_seq::result_type, std::allocator<std::seed_seq::result_type>>]\" \n> (163): here\n>             instantiation of class \"std::iterator_traits<_Iterator> [with _Iterator=std::vector<std::seed_seq::result_type, std::allocator<std::seed_seq::result_type>>]\" \n> /usr/include/c++/5/bits/random.h(6059): here\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1294): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1300): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1306): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1312): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1318): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1324): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1330): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1336): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/emmintrin.h(1342): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/smmintrin.h(270): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/smmintrin.h(798): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(233): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(240): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(247): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(254): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(261): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(268): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(275): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx2intrin.h(282): error: expression must have arithmetic, unscoped enum, or pointer type\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(3606): error: identifier \"__builtin_ia32_pbroadcastq512_gpr_mask\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(3616): error: identifier \"__builtin_ia32_pbroadcastq512_gpr_mask\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(3625): error: identifier \"__builtin_ia32_pbroadcastq512_gpr_mask\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13031): error: identifier \"__builtin_ia32_pd512_pd\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13038): error: identifier \"__builtin_ia32_ps512_ps\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13045): error: identifier \"__builtin_ia32_si512_si\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13052): error: identifier \"__builtin_ia32_pd512_256pd\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13059): error: identifier \"__builtin_ia32_ps512_256ps\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(13066): error: identifier \"__builtin_ia32_si512_256si\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(52): error: identifier \"__builtin_ia32_movapd256_mask\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(61): error: identifier \"__builtin_ia32_movapd256_mask\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(71): error: identifier \"__builtin_ia32_movapd128_mask\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(80): error: identifier \"__builtin_ia32_movapd128_mask\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(90): error: identifier \"__builtin_ia32_loadapd256_mask\" is undefined\n> \n> /usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(99): error: identifier \"__builtin_ia32_loadapd256_mask\" is undefined\n> \n> lots of errors like that\n> \n> Error limit reached.\n> 100 errors detected in the compilation of \"/tmp/tmpxft_00004c6c_00000000-7_cwise_op_gpu_sub.cu.cpp1.ii\".\n> Compilation terminated.\n> ERROR: /home/bernardo/programs/tensorflow/tensorflow/core/BUILD:334:1: output 'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/cwise_op_gpu_sub.cu.o' was not created.\n> ERROR: /home/bernardo/programs/tensorflow/tensorflow/core/BUILD:334:1: not all outputs were created.\n> Target //tensorflow/cc:tutorials_example_trainer failed to build\n", "comments": ["Alright, it turns out that when NVIDIA says only gcc-4.9 and 4.8 are supported for CUDA-7.0 they really mean it, even though a lot of people say it's fine to just use a newer version. Installed gcc-4.9 and everything worked! http://www.codegur.net/32369786/using-cuda-7-0-with-gcc-5-2-0-and-c11\n"]}, {"number": 1103, "title": "Inception-v3 only for specific classes", "body": "Hi,\n\nis it possible to use Inception-v3 to classify images only for dogs?The example at tensorflow/examples/label_image/ classify images into 1000 classes.\n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/504\n"]}, {"number": 1102, "title": "AttributeError: 'module' object has no attribute 'Copy' on fully_connected.py. Built from source on 15th February.", "body": "I built from source on 15th February. I can run cifar10_train.py, with the warning during training as stated here.\n\nhttps://github.com/tensorflow/tensorflow/issues/1076\n\nI can run the convolutional mnist with no problems. However, when I try fully_connected.py, this error shows up.\n\nTraceback (most recent call last):\n  File \"fully_connected_feed.py\", line 228, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"fully_connected_feed.py\", line 224, in main\n    run_training()\n  File \"fully_connected_feed.py\", line 130, in run_training\n    data_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 198, in read_data_sets\n    local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 42, in maybe_download\n    tf.gfile.Copy(temp_file_name, filepath)\nAttributeError: 'module' object has no attribute 'Copy'\n\nI have also tried uninstalling and reinstalling TensorFlow. I don't think it's a problem with the installation since some examples run properly. Anyone else facing this too?\n", "comments": ["This looks like a bug, a missing function that we need to implement.   Thanks for reporting.\n", "Thanks for the fix. I have verified that Copy is back. But it's still missing Open.\n\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 57, in extract_images\n    with tf.gfile.Open(filename) as f, gzip.GzipFile(fileobj=f) as bytestream:\nAttributeError: 'module' object has no attribute 'Open'\n", "Yep, I have another fix for that coming.\n"]}, {"number": 1101, "title": "Add draw_bounding_box and sample_distorted_bounding_box", "body": "", "comments": ["Jenkins, test this please.\n", "merged.\n"]}, {"number": 1100, "title": "Protocol messages are limited to 64MB in python", "body": "The classify_image.py example isn't working because inception v3 is larger than 64MB\n\n[libprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/582\n"]}, {"number": 1099, "title": "remove some signed/unsigned integer comparison warnings", "body": "Jenkins, test this please.\n\nI saw some areas where -1 was used as marker, so I didn't try to change those.  A lot of the dimensions specified the dimensions had to be positive.  I wanted to try running tests locally, but I get the following error message:\n\nbazel test ... \n/_clipped_/\nERROR: /usr/local/lib/bazel/base_workspace/tools/BUILD:7:1: no such package 'tools/android': BUILD file not found on package path and referenced by '//tools:srcs'.\nERROR: Loading failed; build aborted.\n____Elapsed time: 5.022s\nERROR: Couldn't start the build. Unable to run tests.\n\nThis is as much as asking for how to run the test locally as the pull request itself.  Reference #128\n", "comments": ["Can one of the admins verify this patch?\n", "What version of bazel are you running?  We'd like to help you test locally, since only admins can trigger tests.\n", "You might be better off specifically testing \"tensorflow/...\" instead of \"...\"\n", "Great.  Thanks.  Seems to be working so far.  \nI guess I was in the wrong directory for the previous command.\n\nI'll get back to you if I find problems.\nThanks.\n", "Seems like there're a lot of tests I need to work on with this patch.  \n\nAre these the expected results currently?  I'm getting this on the tensorflow/master branch.\nExecuted 176 out of 315 tests: 106 tests pass, 62 fail to build and 147 fail locally.\n\nI will close this pull request soon after, please do not merge.\nThanks.\n", "(I think our Mac build is broken, let us fix it first)\n", "Okay I pushed a change to fix, you can try again now\n", "I simplified this pull request to minimal (similar to the previous pull request #1050) and should be fine to check in.  However, I'm still getting python test failures.  Maybe a python2/3 issue or a protobuf issue?  The last error message is shown below, \nTypeError: **init**() got an unexpected keyword argument 'syntax'\n\nwith the pastebin showing the full error\nhttp://pastebin.com/H59N4k36\n", "That error is usually due to an older protobuf version installed somewhere on your system.\n", "Jenkins, test this please.\n", "@tensorflow-jenkins: test this please\n", "Merged\n"]}, {"number": 1098, "title": "Update release notes for 0.7", "body": "", "comments": ["LGTM\n", "merged.\n"]}, {"number": 1097, "title": "tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on this platform.", "body": "INSTALLATION ERROR:\n$ sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl\nsudo: pip3: command not found\n\nCONDITIONS\nI am running iPython using Python 3.4.3 | Anaconda 2.2.0 (64-bit), running on Ubuntu/Linux 14.04 LTS (64-bit) OS.  \n\nDESCRIPTION: \nThe Tensorflow installation fails using PIP  for Python 3, as suggested here: (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md).\n# Ubuntu/Linux 64-bit\n\n$ sudo apt-get install python-pip python-dev\n# Ubuntu/Linux 64-bit, CPU only:\n\n$ sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl\n\nDETAILS:\ntensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on this platform.\nException information:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 257, in run\n    InstallRequirement.from_line(name, None))\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 168, in from_line\n    raise UnsupportedWheel(\"%s is not a supported wheel on this platform.\" % wheel.filename)\nUnsupportedWheel: tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on this platform.\n", "comments": ["For some reason, pip3 seems to be using python2.7 pip internally (see the\nstacktrace). It appears there's something amiss in your pip installation.\n\nOn Sun, Feb 14, 2016 at 12:28 PM vladalonso notifications@github.com\nwrote:\n\n> INSTALLATION ERROR:\n> $ sudo pip3 install --upgrade\n> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl\n> sudo: pip3: command not found\n> \n> CONDITIONS\n> I am running iPython using Python 3.4.3 | Anaconda 2.2.0 (64-bit), running\n> on Ubuntu/Linux 14.04 LTS (64-bit) OS.\n> \n> DESCRIPTION:\n> The Tensorflow installation fails using PIP for Python 3, as suggested\n> here: (\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md\n> ).\n> Ubuntu/Linux 64-bit\n> \n> $ sudo apt-get install python-pip python-dev\n> Ubuntu/Linux 64-bit, CPU only:\n> \n> $ sudo pip3 install --upgrade\n> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl\n> \n> DETAILS:\n> tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on\n> this platform.\n> Exception information:\n> Traceback (most recent call last):\n> File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in\n> main\n> status = self.run(options, args)\n> File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 257,\n> in run\n> InstallRequirement.from_line(name, None))\n> File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 168, in from_line\n> raise UnsupportedWheel(\"%s is not a supported wheel on this platform.\" %\n> wheel.filename)\n> UnsupportedWheel: tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a\n> supported wheel on this platform.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1097.\n", "sudo apt-get install python3-dev and python3-pip ?\n", "$ pip --version\n$ pip 8.0.2 from ... anaconda3/lib/python3.4/site-packages (python 3.4)\n", "pip3 --version?\n\nOn Sun, Feb 14, 2016 at 12:44 PM vladalonso notifications@github.com\nwrote:\n\n> $ pip --version\n> $ pip 8.0.2 from ... anaconda3/lib/python3.4/site-packages (python 3.4)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1097#issuecomment-183973249\n> .\n", "$ pip3 --version\npip 8.0.2 from /home/vladimir/anaconda3/lib/python3.4/site-packages (python 3.4)\n", "The sudo command probably messes with your environment. Your backtrace\nshows that you are using files from /usr/lib/python2.7/dist-packages/pip.\n\nSince you're using anaconda, you should be able to install without sudo,\ncan you try that?\n\nOr, run `sudo pip3 --version` to verify my hunch.\n\nOn Sun, Feb 14, 2016 at 12:49 PM vladalonso notifications@github.com\nwrote:\n\n> $ pip3 --version\n> pip 8.0.2 from /home/vladimir/anaconda3/lib/python3.4/site-packages\n> (python 3.4)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1097#issuecomment-183973520\n> .\n", "$ sudo pip3 --version\nsudo: pip3: command not found\n", "I have RESOLVED - As follow: \nconda install -c https://conda.anaconda.org/jjhelmus tensorflow\nREFERENCE:\nI found that suggestions on http://stackoverflow.com/questions/33646541/tensorflow-and-anaconda-on-ubuntu. \nTESTED:\nI ran the suggested TEST form the Command Line, and also from the Notebook using Python3 , and it works all fine :)\n\nThank you so much for all the collaboration. - \n", "Glad you were able to make progress!\n", "similar issue solved here by renaming\n\n```\nmv tensorflow-0.7.0-py2-none-linux_x86_64.whl tensorflow-0.7.0-cp27-none-linux_x86_64.whl or \nmv tensorflow-0.7.0-py2-none-linux_x86_64.whl tensorflow-0.7.0-py2-none-any.whl\nsudo pip2 install --upgrade tensorflow-0.7.0-cp27-none-linux_x86_64.whl\nand \nmv tensorflow-0.7.0-py3-none-linux_x86_64.whl tensorflow-0.7.0-cp35-none-linux_x86_64.whl\nsudo pip3 install --upgrade tensorflow-0.7.0-cp35-none-linux_x86_64.whl\n```\n"]}, {"number": 1096, "title": "Changes for r0.7", "body": "1) Inculding https://github.com/tensorflow/tensorflow/pull/1072\n2) Minor tweaks to ci_parameterzied_build.sh (a missing-space bug fix, env var display and timestamps)\n3) Version bump from 0.6.0 to 0.7.0\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "@vrv version.h updated.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@vrv's comments are addressed. This PR is squashed and ready to be merged. \n"]}, {"number": 1095, "title": "Gradient of tf.py_func and how to define gradients for a custom op only with python", "body": "I know about `tf.py_func` which I can use a python code as an op but how gradient is calculated with it?\n\nAlso, is there anyway I can set the gradient only with python? I think one way might be adding a custom op that calculate gradients into an array and use the array as an argument of `apply_gradients`.\n\nFor example:\n\n```\nx = tf.Variable(3.) # this is an custom op\ny = tf.Variable(4.)\nloss = 13 - x * y\n\nopt = tf.train.GradientDescentOptimizer(0.1)\n[(y_grad, y_val)] = opt.compute_gradients(loss, [y])\nclipped_grads_and_vars = [(x * y, x), (y_grad, y_val)] # custom gradient is x * y\n\noptim = opt.apply_gradients(clipped_grads_and_vars)\n```\n\nIs this safe way to apply custom gradients only with python?\n", "comments": ["You can use the Python decorator `@tf.RegisterGradient` to add a gradient to a Python op while only touching Python code. https://www.tensorflow.org/versions/v0.6.0/api_docs/python/framework.html#RegisterGradient\n", "It seems that using `@tf.RegisterGradient` doesn't work, as it is specific to a certain `op.type` (which is always `PyFunc` here), and not to `op.name`.\nThe only way to set `op.type` seems to be to define a custom C++ op.\nDid I miss something, or can someone confirm this?\nWould be nice if I could define custom gradients for my Python ops.\n", "Here is a sketch you can you extend the current code to implement the gradient for py_func and all changes will be restricted to script_ops.py:\n1. \n", "@zffchen78: It seems that your sketch didn't make it into the post :confused:\n\nMaybe you were suggesting to define a `tf.RegisterGradient('PyFunc')` that dispatches to a different  registry of gradients that is based on something other than `op.type` (maybe `op.name`, or maybe the underlying Python function?).\n\nEntries in that registry could be defined with something like `tf.RegisterPyFuncGradient('MyCustomOp')`\n\nThat would indeed be straightforward to implement, but maybe you had something else in mind?\n", "oops. sorry that I were distracted. But yes, you are welcome to extend the code to support gradient of a pyfunc. What I have in mind may be slightly different from your sketch. I'm thinking you can change\ntf.py_func(lambda, ...) to take an extra argument grad_func=lambda. In the py_func.py's registry, keep lambda and grad_lambda keyed by the unique token. In the PyFuncGrad(op), dispatch to grad_lambda based on op.attr['token'].\n", "I don't think `py_func` should have any special functionality here.  You can already do this with a combination of `RegisterGradient` and `Graph.gradient_override_map`.  It's ugly, but if we want to make it less ugly we should clean up gradient overriding for all ops, not just `py_func`.\n", "Ah, I didn't know about `Graph.gradient_override_map`.\nIt could be used to implement the API @zffchen78 suggested like this:\n\n``` python\ndef py_func(func, grad=None):\n  tf.RegisterGradient(\"CustomPyFunc123\")(grad) # Generate a unique name here\n  g = tf.get_default_graph()\n  with g.gradient_override_map({\"PyFunc\": \"CustomPyFunc123\"}):\n    return old_py_func(func)\n```\n\nBut having to generate a unique name is quite ugly.\nMaybe a possible future gradient overriding method won't have that requirement.\nIt would be simple if `gradient_override_map` used a function directly like this:\n\n```\ndef grad_func(op, grad):\n  # ...\nwith g.gradient_override_map({\"PyFunc\": grad_func}):\n  # ...\n```\n", "Agreed, `gradient_override_map` is pretty ugly.  Separate issue from py_func, but so far I don't think we have either an internal or external bug tracking that ugliness. :)\n", "In case some other n00b like me stumbles here wondering how to practically implement the py_func grad, I came up with an explicit example (after some struggling) which I posted [here](https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342).\n\nThe \"unique\" name generation is a bit hacky...\n", "@harpone Thanks for sharing the code! It really helps me. I was searching for such a solution for a long time.\n", "Hi! I wrote a custom binarizer inspired from @harpone 's code. But I am facing problem if nan output cost when using it in two copies of same network simultaneously. Detailed question is here on stackoverflow : http://stackoverflow.com/questions/40735766/tensorflow-custom-operation-used-in-two-networks-simultaneously-produces-nan\r\n\r\nCould anyone please answer my query? ", "Regarding the method used by @harpone, the example given only shows how to do this if the gradient can be defined in pure TensorFlow. How would you go ahead if both the function and its gradient should be computed with e.g. numpy?", "@adler-j you can define op using `tf.py_func` that have additional outputs: derivatives of output with respect to inputs. Next, when you will define gradient of the new op, you can define gradient with respect to inputs in pure TensorFlow (using additional outputs of the new op).\r\n\r\nI posted example [here](https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342#gistcomment-2070556).", "Hello, everybody. I am trying to embed a python function into my tensorflow network code using tf.py_func. But I find that my code doesn't go into my python function. That's strange. Do anyone have suggestion?", "Hello, @CassieMai. Could you provide some code? Or maybe describe in more details?\r\nHow did you find out that code doesn't go into python function?", "Hello, @IFLED, thanks.  The layer definition and python function definition are as following. When my code was executed, it didn't go into myPyFunc at all. Did I miss anything?\r\n\r\n```\r\ndef myLayer(self, input, name):\r\n        with tf.variable_scope(name) as scope:\r\n             d_num = input.get_shape()[-1]\r\n             d = tf.reshape(input[0][:,:,1], [1,-1])\r\n             e = tf.py_func(myPyFunc, [d], [tf.float32])\r\n             e = tf.convert_to_tensor(e)\r\n             return e\r\n\r\ndef myPyFunc(input):\r\n        print \"===Hahaha===\"\r\n        return 0\r\n```", "@CassieMai that is not a [minimal working example](https://stackoverflow.com/help/mcve). Can you please provide the full code?", "```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef myLayer(self, input, name):\r\n        with tf.variable_scope(name) as scope:\r\n             d_num = input.get_shape()[-1]\r\n             d = tf.reshape(input[0][:,:,1], [1,-1])\r\n             e = tf.py_func(myPyFunc, [d], [tf.float32])\r\n             e = tf.convert_to_tensor(e)\r\n             return e\r\n\r\ndef myPyFunc(input):\r\n        print \"===Hahaha===\"\r\n        return np.float32(0)\r\n\r\ninput_tensor = tf.placeholder(tf.float32, (2,2,2,2))\r\ninput_value = np.ones((2,2,2,2))\r\n\r\noutput_tensor = myLayer(\"asdf\", input_tensor, \"asdf\")\r\n\r\nwith tf.Session() as sess:\r\n    print sess.run(output_tensor, { input_tensor : input_value })\r\n```\r\n\r\n@CassieMai, this code produces the following output:\r\n```\r\n===Hahaha===\r\n[ 0.]\r\n```\r\n", "Thank you @IFLED @adler-j . It's ok to run this small code by inputting a custom input_value. Maybe there is something wrong for my input in my code. I am not familiar with tensor operations on tensor format.", "How can we output a full Jacobean matrix in `grad_func`, which has size  |output| x |input|? For examples, in the examples by @harpone and @IFLED, how can we rewrite the gradient function if the forward function np.square is changed to np.product?", "Sure can, in my example above you replace\r\n\r\n```python\r\ndef _MySquareGrad(op, grad):\r\n    x = op.inputs[0]\r\n    return grad * 20 * x\r\n```\r\n\r\nwith\r\n\r\n```python\r\ndef _MySquareGrad(op, grad):\r\n    x = op.inputs[0]\r\n    jacobian = ComputeJacobian(x)  # compute jacobian in x\r\n    return np.dot(jacobian.T, grad)\r\n```\r\n\r\nwith that said, this is in general inferior to an explicit computation.", "> def _MySquareGrad(op, grad):\r\n\r\nI was wondering what exactly this **grad** is doing. I was trying to design a custom mean squared error. My loss computation is\r\n```\r\ndef mse_loss(logits,labels):\r\n    total=len(logits)\r\n    loss=0.0\r\n    #print(logits)\r\n    #print(labels)\r\n    for t in range(total):\r\n        s=0.0\r\n        for j in range(2):\r\n           s=s+(logits[t][j]-labels[t][j])**2\r\n        loss=loss+s\r\n    #loss=loss/float(total)\r\n    return loss\r\n```\r\nfor this function I have defined a gradient function as\r\n```\r\ndef _MeanSquareGrad(op, grad):\r\n    x = op.inputs[0]\r\n    y = op.inputs[1]\r\n    return grad * (x-y),grad*0.01\r\n```\r\nI called it in main tensorflow body as\r\n`loss=py_func(mse_loss,[logits,y],tf.float64,grad=_MeanSquareGrad)`\r\nNow, the code is correctly finding the Loss and optimizer is also minimizing it. But I am not sure about\r\n\r\n1. What is the purpose of using ` grad * (x-y)`. What is the value of grad?\r\n2. I have send a dummy `grad*0.01` value back as I am sending two inputs as in `py_func(mse_loss,[logits,y],tf.float64,grad=_MeanSquareGrad)`. Why it is asking for two returns from grad procedure", "1. if you call `tf.gradients(F, x)` (and F depends on `loss`) then `grad` will be derivative of `F` with respect to `loss`. For example you can take `F = tf.sqrt(loss)`, print `grad` with different values of labels or logits.\r\n2. `mse_loss` is a function that depends on two inputs (`logits`, `labels`), so `grad` procedure should return derivatives with respect to both inputs.", "Is there a document on what the 'grad' argument in the gradient operator do? (the |grad| in |_MeanSquareGrad(op, grad)|)", "What do you mean by document?\r\n\r\nAs far as I remember, [here](https://github.com/tensorflow/tensorflow/issues/1095#issuecomment-378592178) function `_MeanSquareGrad` is used to define gradent of `loss` op and param `grad` is a tensor that golds gradients of \"some global function\" with respect to output of `loss` op. See [my previous comment](https://github.com/tensorflow/tensorflow/issues/1095#issuecomment-384059102).\r\n\r\n", "Thank you, I was just asking if there is a detailed description about it somewhere, as I don't clearly get it from previous comments.\r\nI have defined loss value by a py_func, and that's why I am overriding the gradient to define the actual gradient of it. Now I don't get what exactly |grad| means and how should I use it. I don't clearly get what you mean by \"gradient of some global function with respect to outputs of loss op\"."]}, {"number": 1094, "title": "Add a CNN sample for MNIST", "body": "The purpose of this PR is,\n- Separate net structure (net.py, mnist.py) and train process (fully_connected_feed.py) clearly.\n- Add a CNN sample (mnist_cnn.py) for MNIST\n\nAny advices and comments would be greatly appreciated.\nThanks!\n", "comments": ["Can one of the admins verify this patch?\n", "My high level comment is that, while this is nice, I think the original intent of the tutorials was to keep the examples minimal and self-contained, rather than factored out -- the longer term intent is to convert these into IPython Notebooks that have most of the relevant content in one.\n\nIf you feel strongly this should be factored out, can you explain what the goal of your refactoring is?\n", "> the original intent of the tutorials was to keep the examples minimal and self-contained\n> the longer term intent is to convert these into IPython Notebooks\n\nI strongly agreed.\nThank you for the comment.\n\nWhen I leaned TensorFlow, I felt a big barrier between the beginner's mnist tutorial and the expert's one.\nIt's a reason that I wrote the code which has the same style between those.\nI am looking forward to see such a sophisticated tutorial and code.\n"]}, {"number": 1093, "title": "Gradient computation through while loop", "body": "Computation of gradients through a simple while op doesn't seem to work:\n\n``` python\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\ndef cond(i, var):\n    return tf.less(i, 1)\n\ndef body(i, var):\n    return [tf.add(i, 1), var]\n\ni = tf.constant(0)\nvar = tf.Variable(tf.constant(2.))\n\n_, var = control_flow_ops.While(cond, body, [i, var])\n\nout = var*var\n\noptimizer = tf.train.GradientDescentOptimizer(.9)\noptimizer_op = optimizer.minimize(out)\n\nwith tf.Session() as sess:\n    tf.initialize_all_variables().run()\n    print sess.run([out, optimizer_op])\n    print sess.run([out, optimizer_op])\n    print sess.run([out, optimizer_op])\n```\n\ngives the following result:\n\n```\n[4.0, None]\n[4.0, None]\n[4.0, None]\n```\n\nWhereas commenting out the while op on line 13 gives:\n\n```\n[4.0, None]\n[2.5599997, None]\n[1.6383994, None]\n```\n\nSince the while op here only iterates once, and doesn't really do anything, I would expect that the result should be the same with or without it.\n\nI know that the while operator is not officially documented or supported, but the wording in #593 made me believe that computation of gradients through a single simple loop was likely to work. (Why experiment with nested loops if single loops didn't work)\n", "comments": ["This should be fixed in the next push.\n"]}, {"number": 1092, "title": "Contrib docs", "body": "Add contrib to the set of docs generated, and run gen_docs at head to produce an up to date docset.\n", "comments": ["Jenkins, test this please.\n"]}, {"number": 1091, "title": "Adding summaries changes random number generation", "body": "Here is a simple example:\n\n```\nimport tensorflow as tf\n\nwith tf.Session('') as sess:\n    tf.set_random_seed(42)\n#    tf.scalar_summary('d', 1.)\n    y = tf.get_variable('y', [1, 5])\n    sess.run(tf.initialize_all_variables())\n    print sess.run(y)\n```\n\nCommenting / uncommenting scalar_summary changes results:\n`[[ 1.58872545  0.94011414 -1.35924089  1.19359171  1.48940694]]` to `[[ 0.47769845 -1.17613626 -1.06156325 -0.85632306 -0.62591958]]`.\n\nRelated to https://github.com/tensorflow/skflow/issues/101\n", "comments": ["I believe this is 'working as intended': tf.set_random_seed only sets the graph level randomization seed, not the op level randomization seed.  You have to set both if you want deterministic results.\n\n(I believe tf.set_random_seed documentation explains the details).\n", "To clarify, you need to set both if you want deterministic results when the _graphs_ are different.  If the graphs were the same and you set the graph level seed, you wouldn't need to set the op level seed, but here your graphs are different, so you would need to set both.\n\nWe could probably add this to our documentation, since this example is a useful one to show.\n"]}, {"number": 1090, "title": "keep special GPU kernels inside #if GOOGLE_CUDA... #endif", "body": "We don't need to compile these GPU kernels when compiling Tensorflow for CPU device only.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "A mistake in constant_op.cc. I will fix it.\n", "@tensorflow-jenkins: test this please\n", "merged.\n", "Thanks!\n", "@martinwicke, there are two commits needed to merge.\n", "I squashed them into one.\n"]}]