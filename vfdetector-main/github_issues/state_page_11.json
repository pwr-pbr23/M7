[{"number": 54157, "title": "Ability to clear cache on CacheDataset", "body": "**System information**\r\n- TensorFlow version (you are using): 2.6.0\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIf a dataset is cached, there is currently no public API for explicitly clearing the cache (I think?).\r\n\r\n**Will this change the current api? How?**\r\n\r\nAdd `clear()` or `clear_cache()` method (perhaps on DatasetV2 class?). After calling this function, the cache has been evicted.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople using data augmentations who would like to create a fresh set of augmentations after each epoch (or say, every N epochs). For example, [here is an unanswered stack overflow question](https://stackoverflow.com/q/65037119/1430829) where a number of people have upvoted indicating the desire for this capability.\r\n\r\n**Any Other info.**\r\n\r\nFor file-based caches, I think simply deleting the the cache file would suffice as a workaround for now. I'm not sure if there's a workaround for in-memory caches.\r\n", "comments": ["Hi @notmatthancock, would it work for your use case to re-create the dataset to drop the cache? Something like\r\n\r\n```python\r\ndef create_dataset():\r\n  dataset = tf.data.Dataset(...)\r\n  dataset = dataset.map(<expensive_augmentation>)\r\n  dataset = dataset.shuffle(...)\r\n  dataset = dataset.batch(...)\r\n  return dataset\r\n\r\nfor epoch in range(num_epochs):\r\n  # Drop the cache every 8 epochs.\r\n  if epoch % 8 == 0: dataset = create_dataset()\r\n  for batch in dataset:\r\n    train(batch)\r\n```", "I'm using the keras API at the moment, where you pass in the dataset via [`Model.fit(x=dataset, ...)`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit). I'm not sure this suggestion would exactly work in that context because the dataset is mostly out of your hands after passed to `fit`. Although, I'm achieving an effect similar to what you suggest by adding a Callback that deletes the associated cache files [on epoch end](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback#on_epoch_end). I'm doing the deletion by globbing on the original file name string passed to `Dataset.cache`, which is probably not ideal.", "In that case you could use `flat_map` to create one big dataset that does the re-caching:\r\n\r\n```python\r\ndataset = tf.data.Dataset.range(num_epochs // 8)\r\ndataset = dataset.flat_map(lambda x: create_dataset().repeat(8))\r\nModel.fit(x=dataset, ...)\r\n```"]}, {"number": 54155, "title": "TFLITE conversion Failed - Custom Model", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): Pip\r\n- TensorFlow installation (pip package or built from source): tensorflow==2.8.0-rc0\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\nReference [TensorFlow Lite Model Colab]: https://colab.research.google.com/drive/1crWPg__nYgt5IUFOA_OzgU0qgvgzmsZc?usp=sharing\r\n\r\n\r\n### 3. Failure after conversion\r\nconversion Fails when adding the \"update_control_variate\" function.\r\n\r\n\r\n### 4. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n2022-01-27 11:53:08.600343: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\r\n2022-01-27 11:53:08.600385: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\r\n2022-01-27 11:53:08.600602: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model\r\n2022-01-27 11:53:08.647834: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\r\n2022-01-27 11:53:08.647877: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: saved_model\r\n2022-01-27 11:53:08.787707: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\r\n2022-01-27 11:53:09.230312: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: saved_model\r\n2022-01-27 11:53:09.631676: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 1031073 microseconds.\r\n2022-01-27 11:53:10.288551: E tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate.cc:207] SavedModel V1 import failed: FAILED_PRECONDITION: Graph does not contain node: \r\n\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n/tmp/ipykernel_5933/617434638.py in <module>\r\n     10 converter.experimental_new_converter = True\r\n     11 converter.allow_custom_ops = True\r\n---> 12 tflite_model = converter.convert()\r\n\r\n~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n   1712         Invalid quantization parameters.\r\n   1713     \"\"\"\r\n-> 1714     return super(TFLiteConverterV2, self).convert()\r\n   1715 \r\n   1716 \r\n\r\n~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in wrapper(self, *args, **kwargs)\r\n    801   def wrapper(self, *args, **kwargs):\r\n    802     # pylint: disable=protected-access\r\n--> 803     return self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\n    804     # pylint: enable=protected-access\r\n    805 \r\n\r\n~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in _convert_and_export_metrics(self, convert_func, *args, **kwargs)\r\n    787     self._save_conversion_params_metric()\r\n    788     start_time = time.process_time()\r\n--> 789     result = convert_func(self, *args, **kwargs)\r\n    790     elapsed_time_ms = (time.process_time() - start_time) * 1000\r\n    791     if result:\r\n\r\n~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n   1371     \"\"\"\r\n   1372     if self.experimental_lower_to_saved_model:\r\n-> 1373       saved_model_convert_result = self._convert_as_saved_model()\r\n   1374       if saved_model_convert_result:\r\n   1375         return saved_model_convert_result\r\n\r\n~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in _convert_as_saved_model(self)\r\n   1351       if self.saved_model_dir:\r\n   1352         self._validate_inputs(graph_def, input_tensors)\r\n-> 1353         return self._convert_from_saved_model(graph_def)\r\n   1354     finally:\r\n   1355       shutil.rmtree(temp_dir, True)\r\n\r\n~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/lite.py in _convert_from_saved_model(self, graph_def)\r\n    965     converter_kwargs.update(quant_mode.converter_flags())\r\n    966 \r\n--> 967     result = _convert_saved_model(**converter_kwargs)\r\n    968     return self._optimize_tflite_model(\r\n    969         result, quant_mode, quant_io=self.experimental_new_quantizer)\r\n\r\n~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py in wrapper(*args, **kwargs)\r\n    211         else:\r\n    212           report_error_message(str(converter_error))\r\n--> 213         raise converter_error from None  # Re-throws the exception.\r\n    214       except Exception as error:\r\n    215         report_error_message(str(error))\r\n\r\n~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py in wrapper(*args, **kwargs)\r\n    204     def wrapper(*args, **kwargs):\r\n    205       try:\r\n--> 206         return func(*args, **kwargs)\r\n    207       except ConverterError as converter_error:\r\n    208         if converter_error.errors:\r\n\r\n~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/convert.py in convert_saved_model(**kwargs)\r\n    787   model_flags = build_model_flags(**kwargs)\r\n    788   conversion_flags = build_conversion_flags(**kwargs)\r\n--> 789   data = convert(\r\n    790       model_flags.SerializeToString(),\r\n    791       conversion_flags.SerializeToString(),\r\n\r\n~/anaconda3/envs/tf_2.8/lib/python3.9/site-packages/tensorflow/lite/python/convert.py in convert(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    304       for error_data in _metrics_wrapper.retrieve_collected_errors():\r\n    305         converter_error.append_error(error_data)\r\n--> 306       raise converter_error\r\n    307 \r\n    308   return _run_deprecated_conversion_binary(\r\n\r\nConverterError: Graph does not contain node: ", "comments": ["Hi @sachinprasadhs ! Could you please look at this issue ? Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/0b6470da4eb662ffeb93cf0d52cfbae3/test_mode.ipynb#scrollTo=Nbvrn_9uaCSY) for reference.", "Any updates on this one? Is this a bug or an error from my side? ", "Any updates here? I really would appreciate it if you could provide some answers. ", "Hello @raminmohammadi, this will take a longer to debug, please give us a few days to take a look and get back to you.", "Thanks", "@MeghnaNatraj Any updates on this ticket?", "@raminmohammadi could you update the gist and fix the errors on cell 8, 9 and 10?\r\n\r\n**Cell 8**\r\n\r\n```\r\nsignatures = [\r\n  m.train.get_concrete_function(),\r\n  m.infer.get_concrete_function(),\r\n  m.save.get_concrete_function(),\r\n  m.restore.get_concrete_function(),\r\n  m.get_the_update.get_concrete_function(),\r\n  m.update_control_variate.get_concrete_function()               \r\n]\r\n    \r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions(signatures, m) \r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.\r\n    tf.lite.Optimize.DEFAULT\r\n]\r\nconverter.target_spec.supported_types = [tf.float32]\r\nconverter.experimental_enable_resource_variables = True\r\nconverter.experimental_new_converter = True\r\nconverter.allow_custom_ops = True\r\ntflite_model = converter.convert()\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[<ipython-input-10-7df60c3cf334>](https://localhost:8080/#) in <module>()\r\n     18 converter.experimental_new_converter = True\r\n     19 converter.allow_custom_ops = True\r\n---> 20 tflite_model = converter.convert()\r\n\r\n6 frames\r\n[/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _freeze_concrete_function(self)\r\n   1390 \r\n   1391     if len(self._funcs) > 1:\r\n-> 1392       raise ValueError(\"This converter can only convert a single \"\r\n   1393                        \"ConcreteFunction. Converting multiple functions is \"\r\n   1394                        \"under development.\")\r\n\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n```\r\n\r\n**Cell 9**\r\n```\r\nSAVED_MODEL_DIR = \"saved_model\"\r\n\r\ntf.saved_model.save(\r\n    m,\r\n    SAVED_MODEL_DIR,\r\n    signatures={\r\n        'train':\r\n            m.train.get_concrete_function(),\r\n        'infer':\r\n            m.infer.get_concrete_function(),\r\n        'save':\r\n            m.save.get_concrete_function(),\r\n        'restore':\r\n            m.restore.get_concrete_function(),\r\n        'get_the_update':\r\n            m.get_the_update.get_concrete_function(),\r\n        'update_control_variate':\r\n            m.update_control_variate.get_concrete_function()\r\n    })\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[<ipython-input-9-b5016c469a88>](https://localhost:8080/#) in <module>()\r\n     16             m.get_the_update.get_concrete_function(),\r\n     17         'update_control_variate':\r\n---> 18             m.update_control_variate.get_concrete_function()\r\n     19     })\r\n     20 \r\n\r\n18 frames\r\n[/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/signature_serialization.py](https://localhost:8080/#) in _normalize_outputs(outputs, function_name, signature_key)\r\n    217     if not isinstance(value, (ops.Tensor, composite_tensor.CompositeTensor)):\r\n    218       raise ValueError(\r\n--> 219           f\"Got a non-Tensor value {value!r} for key {key!r} in the output of \"\r\n    220           f\"the function {compat.as_str_any(function_name)} used to generate \"\r\n    221           f\"the SavedModel signature {signature_key!r}. \"\r\n\r\nValueError: in user code:\r\n\r\n\r\n    ValueError: Got a non-Tensor value <tf.Operation 'PartitionedCall' type=PartitionedCall> for key 'output_0' in the output of the function __inference_update_control_variate_8606 used to generate the SavedModel signature 'update_control_variate'. Outputs for functions used as signatures must be a single Tensor, a sequence of Tensors, or a dictionary from string to Tensor.\r\n```\r\n\r\n**Cell 10**\r\n```\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.\r\n    tf.lite.Optimize.DEFAULT\r\n]\r\nconverter.target_spec.supported_types = [tf.float32]\r\nconverter.experimental_enable_resource_variables = True\r\nconverter.experimental_new_converter = True\r\nconverter.allow_custom_ops = True\r\ntflite_model = converter.convert()\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n[<ipython-input-11-ca397ff95c32>](https://localhost:8080/#) in <module>()\r\n      1 # Convert the model\r\n----> 2 converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\r\n      3 converter.target_spec.supported_ops = [\r\n      4     tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n      5     tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.\r\n\r\n4 frames\r\n[/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py](https://localhost:8080/#) in parse_saved_model(export_dir)\r\n    114   else:\r\n    115     raise IOError(\r\n--> 116         f\"SavedModel file does not exist at: {export_dir}{os.path.sep}\"\r\n    117         f\"{{{constants.SAVED_MODEL_FILENAME_PBTXT}|\"\r\n    118         f\"{constants.SAVED_MODEL_FILENAME_PB}}}\")\r\n\r\nOSError: SavedModel file does not exist at: saved_model/{saved_model.pbtxt|saved_model.pb}\r\n```", "@MeghnaNatraj its updated now. Please take a look. Thanks", "@MeghnaNatraj Can you look into this? I assume 3 months waiting period for a help is fair :)   ", "@sachinprasadhs - Can you look into this? "]}, {"number": 54146, "title": "Only hoist after-all instruction if it allows another instruction to be hoisted", "body": "\r\nThe after all instruction doesn't produce any code and is only used at compile\r\ntime, hence it can be put with the other instructions that are not hoisted\r\nindividually. The motivation is to reduce compile time of models by reducing\r\ntotal number of instructions and in the case where this avoids having to clone\r\nand replace the while loop and it's called computations.", "comments": ["@r4nt  Can you please review this PR ? Thank you!"]}, {"number": 54116, "title": "tf.image.adjust_brightness and random_brightness miss input check", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nx = [[[1.0, 2.0, 3.0],\r\n      [4.0, 5.0, 6.0]],\r\n    [[7.0, 8.0, 9.0],\r\n      [10.0, 11.0, 12.0]]]\r\nprint(tf.image.adjust_brightness(x, delta=1.1))\r\nprint(tf.image.random_brightness(x, max_delta=1.1))\r\n\r\n```\r\n\r\n**Expected output**\r\nAccording to the document [tf.image.adjust_brightness](https://www.tensorflow.org/api_docs/python/tf/image/adjust_brightness?hl=en), the delta should be in range `(-1, 1)`. And random_brightness is  quivalent to adjust_brightness() using a delta randomly picked in the interval [-max_delta, max_delta), so valid input check is also missing here.", "comments": ["Hi @jvishnuvardhan ! Could you please look into this issue ? It is replicating in [2.6](https://colab.sandbox.google.com/gist/mohantym/aa7ca251a9388c45411aa67dcb9748ff/github_54116.ipynb#scrollTo=96aKifvB8EyQ), [2.7](https://colab.sandbox.google.com/gist/mohantym/b2e8fdb4207ad6fcc477c0d0a1e1ef03/github_54116.ipynb) and [2.8](https://colab.sandbox.google.com/gist/mohantym/d042a09b947b8c4e6ab437083059bd31/github_54116.ipynb#scrollTo=-GRf3hmP8AYE). ", "Can look into this\r\n", "@ArrowIntoTheSky like the documentation says I believe delta should be in the range (-1,1) when your pixel ranges are in between 0 and 1 however if there are some exceptional irregular images, the delta value can exceed the range given, thus the validation is not implented", "That being said I guess we can add a validation based on pixel ranges and then validate the delta values, I am working on it and will submit a pr"]}, {"number": 54115, "title": "tf.image.adjust_hue miss input check", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimage = [[[1, 2, 3], [4, 5, 6]],\r\n         [[7, 8, 9], [10, 11, 12]],\r\n         [[13, 14, 15], [16, 17, 18]]]\r\nimage = tf.constant(image)\r\ntf.image.adjust_hue(image, -2)\r\ntf.image.adjust_hue(image, 2)\r\n\r\n```\r\n\r\n**Expected output**\r\nAccording to the document [tf.image.adjust_hue](https://www.tensorflow.org/api_docs/python/tf/image/adjust_hue?hl=en), the delta should be in range `[-1, 1]`. Valid input check is missing here.", "comments": []}, {"number": 54105, "title": "Copy-on-read Limits Usable GPU Memory", "body": "When sparsely accessed individual variables occupy a large portion of available memory, the necessary copy during dense reads, e.g. while saving a checkpoint, can substantially increase the total memory usage. In the limit of a single variable (such as an embedding table) occupying all allocated memory, this behavior causes the memory usage to double when the model is saved. Therefore, a single sparsely accessed variable can at most occupy half the available memory to avoid OOM while saving the model.\r\n\r\nPossible solutions could include for instance allowing a conversion from copy-on-read to copy-on-write, or dense access without copy using an exclusive lock in some situations.\r\n[](url)\r\n\r\n\r\n\r\n\r\n**System information**\r\nReproducer: [copy_on_read_oom.zip](https://github.com/tensorflow/tensorflow/files/7946149/copy_on_read_oom.zip)\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.6\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 4.2.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n", "comments": ["Hi @sachinprasadhs ! Could you look at this feature request?", "@rohan100jain can you please take a look at this as well?\r\n"]}, {"number": 54091, "title": "TFLite SignatureRunner support for the C API", "body": "**System information**\r\n- TensorFlow version (you are using): 2.7 / tf-nightly\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe current TFLite interpreter C API allows using TFLite in situations where a stable ABI is needed and the C++ API is not an option. For example, in cases where TFLite needs to be used in bigger projects that cannot use bazel to build and might use different toolchains or incompatible C++ compiler settings.\r\n\r\nHowever, while the C++ API already has support for multiple signatures [through the use of SignatureRunner](https://www.tensorflow.org/lite/guide/signatures#c), the C API has not been updated accordingly. As such, the use of multiple signatures in TFLite models is not possible if ABI stability requirements prevent you from using the C++ API directly.\r\n\r\n**Will this change the current api? How?**\r\nNew APIs that allow using signature runners would need to be added [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api.h), or in a separate header if appropriate.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who needs stable ABIs and wants to run TFLite models with multiple signatures.\r\n\r\n**Any Other info.**\r\nAs mentioned [here](https://www.tensorflow.org/lite/guide/signatures#known_limitations) this feature is not available _yet_, suggesting it's planned. I'm opening this feature request so that it's easier to track its status, as well as any kind of information on when it might be implemented.", "comments": ["@leandro-gracia-gil ,\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!", "> @leandro-gracia-gil , Can you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!\r\n\r\nThe feature is basically bring support for signature runners (what you can do [here](https://www.tensorflow.org/lite/guide/signatures#c) in C++) to the C API. There should already internal be support for this, but it's simply not yet exposed in the C API.\r\n\r\nThe use cases are using TFLite models that have more than a single TF function in situations where ABI stability issues don't allow you using the C++ API.\r\n\r\nFor example, I've tried integrating TFLite with big projects using Unreal Engine in Windows where it is not in my power to do things like enabling exceptions or making some changes in toolchain settings that affects the whole project. If you try to build a TFLite C++ dll separately using bazel and run it from this bigger project you often get ABI-related issues and crashes. Depending on what you do you can also be missing required C++ symbols due to the dll export limit of 64k functions. Trying to build everything on bazel, or to build TFLite using the Unreal Engine build system are not feasible options.\r\n\r\nOn the other hand, using a smaller C API dll provides a stable C ABI that avoids these issues, and makes it easier to get things working by building TFLite separately in bazel. But my TFLite models have more than a single TF function, so I need C APIs to be able to use the signature runners that already exist in C++."]}, {"number": 54086, "title": "Tensorflow restore memory leak", "body": "**System information**\r\n\r\n- OS Platform and Distribution: MacOS Monterey 12.1\r\n- TensorFlow installed: from binary\r\n- TensorFlow version: The issue could be reproduced by v2.6.0\r\n- Python version: 3.8.3\r\n\r\n**Describe the current behavior**\r\nI notice that Tensorflow (TF) leaks memory when I restore TF models. Specifically if I iteratively load model checkpoint from disk with TF restore API, the memory usage keeps growing no matter what I do `tf.reset_default_graph()` or `tf.keras.backend.clear_session()` or both (But it seems `tf.reset_default_graph()` and `tf.keras.backend.clear_session()` slow down the memory leak speed rate).\r\n\r\n** Standalone code to reproduce the issue **\r\nThe issue could be reproduced as below. Run `tf_save_model.py` to create and save a model in the folder `./test_model_repro/` with the model name `foo`. Then run `tf_restore_model.py` to load the model iteratively. You would see after each iteration calling `restore()`, even if `tf.reset_default_graph()` and `tf.keras.backend.clear_session()` are being called, the log shows the memory usage keeps increasing.\r\n\r\ntf_save_model.py\r\n```\r\nfrom typing import Tuple\r\n\r\nimport tensorflow.compat.v1 as tf\r\nfrom tensorflow.keras import models\r\nfrom tensorflow.keras import layers\r\n\r\ntf.disable_v2_behavior()\r\n\r\n\r\ndef create_model() -> Tuple[tf.Tensor, tf.Tensor]:\r\n    \"\"\"\r\n    Create a model with Neural-Net: 1024 x 1024\r\n    \"\"\"\r\n    graph = tf.get_default_graph()\r\n    with graph.as_default():\r\n        # The Neural Net Size: 200 -> 1024 x 1024 -> 6\r\n        model = models.Sequential()\r\n        input_shape, hidden_shape, output_shape = 200, 1024, 6\r\n        model.add(\r\n            layers.Dense(\r\n                hidden_shape,\r\n                activation='tanh',\r\n                input_shape=(input_shape,),\r\n                name=\"layer0\"\r\n            )\r\n        )\r\n        model.add(layers.Dense(hidden_shape, activation='tanh', name=\"layer1\"))\r\n        model.add(layers.Dense(output_shape, activation='relu', name=\"final_layer\"))\r\n        input_tensor = tf.placeholder(dtype=tf.float32, shape=[None, input_shape])\r\n        output_tensor = model(input_tensor)\r\n    return input_tensor, output_tensor\r\n\r\n\r\ndef save_model(sess: tf.Session, path: str) -> None:\r\n    \"\"\"\r\n    Save the TF model according to the path.\r\n    \"\"\"\r\n    saver = tf.train.Saver()\r\n    sess.run(tf.global_variables_initializer())\r\n    save_path = saver.save(sess, path)\r\n    print(f\"Saved the model in {save_path}\")\r\n\r\n\r\ndef main() -> None:\r\n    input_tensor, output_tensor = create_model()\r\n    print(\r\n        f\"Create the model with Input tensor {input_tensor} \"\r\n        f\"and output tensor {output_tensor}.\"\r\n    )\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n    save_model(sess, \"./test_model_repro/foo\")\r\n\r\n\r\nmain()\r\n```\r\n\r\ntf.restore_model.py\r\n```\r\nimport os\r\nimport psutil\r\n\r\nimport tensorflow.compat.v1 as tf\r\n\r\ntf.disable_v2_behavior()\r\n\r\n\r\ndef _get_memory_rss() -> float:\r\n    \"\"\"\r\n    Get the RSS memory value in GB.\r\n    \"\"\"\r\n    return psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3\r\n\r\n\r\ndef restore(path: str) -> tf.train.Saver:\r\n    \"\"\"\r\n    Restores the TF graph from the path and returns the saver.\r\n    \"\"\"\r\n    sess = tf.Session()\r\n    saver = tf.train.import_meta_graph(f\"{path}.meta\")\r\n    saver.restore(sess, path)\r\n\r\n\r\ndef main() -> None:\r\n    init_prev = _get_memory_rss()\r\n    for _ in range(10000):\r\n        restore(\"./test_model_repro/foo\")\r\n        mem_after = _get_memory_rss() - init_prev\r\n        print(\r\n            f\"The memory increased after restoring the TF model: {mem_after}\"\r\n        )\r\n        tf.reset_default_graph()\r\n        tf.keras.backend.clear_session()\r\n\r\n\r\nmain()\r\n```\r\nThe issue could be 100% repro-d by the above Python scripts.\r\n", "comments": ["@sachinprasadhs ,\r\nI was able to reproduce the issue in tf [v2.5](https://colab.research.google.com/gist/tilakrayal/61cfb2bcb6db7f15b18f939c0d45ded5/54086-2-5.ipynb),[v2.7](https://colab.research.google.com/gist/tilakrayal/e3ad29fcde5cb4fe5ddc2f05769db298/54086-2-7.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/aea162370bef4cb9a9968756a9bd9f3f/54086-nightly.ipynb).Please find the gist here.", "Hi did you try the same in eager mode and is there any specific reason you are testing it by using `tf.disable_v2_behavior()`", "Hi @sachinprasadhs We use the v1-compatible one and don't use the eager mode, due to our product spec in the current phase."]}, {"number": 54078, "title": "[TFLite] Restrict inputs and outputs scaling/zero-point to be the same for the EXPAND_DIMS op", "body": "Hi,\r\n\r\nThis PR sets `OperatorProperty::restrict_same_input_output_scale` to true for the EXPAND_DIMS operator and add some checks about it in the `Prepare` method of the operator. The MLIR quantizer properly sets the scaling/zero-point thanks to the `SameOperandsAndResultsScale` trait for `TFL_ExpandDimsOp` but it isn't the case unfortunately for the old TOCO quantizer still used for int16.\r\n\r\nThibaut", "comments": ["Someone in TFLite should review this.", "@daverim  Can you please review this PR ? Thank you!"]}, {"number": 54077, "title": "[TFLite] Check that the SameOperandsAndResultsScale trait of the Maximum/Minimum ops is respected", "body": "Hello,\r\n\r\nThe Maximum/Minimum ops require the scaling and zero-point to be the same for the inputs and outputs. This PR adds an extra check in the Prepare method of the ops to avoid failing silently if it isn't the case due to wrongly set quantization parameters in a TFLite file.\r\n\r\nThibaut", "comments": ["@renjie-liu Can you please review this PR ? Thank you!"]}, {"number": 54069, "title": "Cross compile  libtensorflow-lite.a is ok but many undefined reference when link it", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: ARM V7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:  2.6.2\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI can cross-compile the libtensorflow-lite.a successfully for armv7,  but when I link it with an application, I meet the similar problem like #50149. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe following is how I build the libtensorflow-lite.a\r\n```\r\nARMCC_FLAGS=\"-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations\"\r\nARMCC_PREFIX=/tools/toolchain/gcc-9.1.0-2020.07-x86_64_arm-linux-gnueabihf/bin/arm-linux-gnueabihf-\r\ncmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \\\r\n  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \\\r\n  -DCMAKE_C_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n  -DCMAKE_CXX_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \\\r\n  -DCMAKE_SYSTEM_NAME=Linux \\\r\n  -DCMAKE_SYSTEM_PROCESSOR=armv7 \\\r\n  ../tensorflow/lite/\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I also build a libtensorflow-lite.a with gcc just for linux, I meet the same question.", "> cmake\r\n```\r\ncmake ../tensorflow/lite/\r\nmake -j32\r\n```", "It seems that the libtensorflow-lite.a does not link ruy and xnnpack\r\n![image](https://user-images.githubusercontent.com/22525811/151132498-bc6a92a7-53df-4c04-ad2d-9415c669efc1.png)\r\n", "@Ryuk17 Could you please try on the latest TF v2.7.0 and let us know the outcome?\r\nPlease provide the full traceback in text format  that would be helpful to diagnose the problem.\r\nThanks!", "The problem still exists in TF v2.7.0, the following is my full traceback with gcc version 9.4.0, cmake version 3.21.3:\r\n\r\nFirst download TF v2.7.0 from [here](https://github.com/tensorflow/tensorflow/releases/tag/v2.7.0)\r\n```\r\ncd {TF_DIR}\r\nmkdir build\r\ncd build\r\ncmake ../tensorflow/lite/ \r\n```\r\nhere are my options\r\n![image](https://user-images.githubusercontent.com/22525811/151278151-f065c127-ed5c-4e59-bb6f-6da8e6ebd56f.png)\r\n\r\nThen\r\n```\r\ncmake --build . -j32\r\n```\r\nI can get the  libtensorflow-lite.a\r\n![image](https://user-images.githubusercontent.com/22525811/151278265-e35da915-5071-417d-9b77-c2c1c3055d25.png)\r\n\r\nNext, I use the built libtensorflow-lite.a my own to replace thirdpart/lib/libtensorflow-lite.a in this [repo](https://github.com/avcodecs/DTLNtfliteC)\r\n\r\nFinally\r\n```\r\n./compile_lib.sh\r\n./compile_exm.sh\r\n```\r\nwhen runing ./compile_exm.sh, I met many undefined reference problems like this\r\n![image](https://user-images.githubusercontent.com/22525811/151278730-db3d0e4c-f6ec-47b6-afcd-990b7f7c389e.png)\r\n\r\n\r\n\r\n", "In #43646, G4V mentioned that the built library libtensorflow-lite.a doesn't itself include all the dependencies it needs as build in [build dir]/_deps. Is this the reason and how to solve it?", "Can anyone give some suggestions to solve this problem, plz?"]}, {"number": 54001, "title": "Some operations didn't work in tflite train procedure.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS\r\n- TensorFlow version (use command below): tensorflow 2.7.0\r\n- Python version: python 3.9\r\n- CUDA/cuDNN version: Cuda 11.2/ cudnn 8.2\r\n- GPU model and memory: rtx A6000\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n I want to generate fully train able tflite model. I found mapped ops in ['tfl' Dialect](https://www.tensorflow.org/mlir/tfl_ops?hl=en). And then, I use these functions in my model.  All operation can convert to graph model use converter.convert(), but it dosen't work. Some operation can use in inference, but can't use in train without any error just stop.(ex. Avgpool2d). \r\n\r\n**Describe the expected behavior**\r\n\r\nI have three questions in upper problem.\r\n\r\n First, Is this operation(avgpool) can't use in  tflite?\r\n\r\n Second, If first question is true, can i get trainable operation documentation in tflite?\r\n\r\n Third, When can i use fully trainable tflite model in mobile?\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n** custom source code\r\n```\r\n`import tensorflow as tf\r\nimport numpy as np\r\n\r\nimport h5py\r\nimport json\r\n\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\r\nprint(tf.__version__)\r\nprint(\"here\")\r\n\r\n\r\nclass Deepphys(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Deepphys,self).__init__()\r\n        self.a_1 = tf.keras.layers.ZeroPadding2D(1)\r\n        self.a_2 = tf.keras.layers.Conv2D(filters=32,kernel_size=3,name='a_2')\r\n        # self.a_3 = tf.keras.layers.BatchNormalization()\r\n\r\n        self.a_4 = tf.keras.layers.ZeroPadding2D(1)\r\n        self.a_5 = tf.keras.layers.Conv2D(filters=32, kernel_size=3,name='a_5')\r\n        # self.a_6 = tf.keras.layers.BatchNormalization()\r\n\r\n        self.a_7 = tf.keras.layers.AveragePooling2D(pool_size =2,strides=2)\r\n\r\n        self.a_8 = tf.keras.layers.ZeroPadding2D(1)\r\n        self.a_9 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,name='a_7')\r\n        # self.a_10 = tf.keras.layers.BatchNormalization()\r\n\r\n        self.a_11 = tf.keras.layers.ZeroPadding2D(1)\r\n        self.a_12 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,name='a_12')\r\n        # self.a_13 = tf.keras.layers.BatchNormalization()\r\n\r\n        self.att_conv_1 = tf.keras.layers.Conv2D(filters=1,kernel_size=1,activation='sigmoid',name='att_1')\r\n        self.att_conv_2 = tf.keras.layers.Conv2D(filters=1, kernel_size=1,activation='sigmoid',name='att_2')\r\n\r\n        self.m_1 = tf.keras.layers.ZeroPadding2D(1)\r\n        self.m_2 = tf.keras.layers.Conv2D(filters=32,kernel_size=3,padding='valid',name='m_2')\r\n        # self.m_3 = tf.keras.layers.BatchNormalization()\r\n\r\n        self.m_4 = tf.keras.layers.ZeroPadding2D(1)\r\n        self.m_5 = tf.keras.layers.Conv2D(filters=32,kernel_size=3,name='m_5')\r\n        # self.m_6 = tf.keras.layers.BatchNormalization()\r\n\r\n        # self.m_7 = tf.keras.layers.AveragePooling2D(pool_size=2, strides=2)\r\n        self.m_7 = tf.keras.layers.Conv2D(filters = 32, kernel_size=2, strides=2)\r\n\r\n\r\n        self.m_8 = tf.keras.layers.ZeroPadding2D(1)\r\n        self.m_9 = tf.keras.layers.Conv2D(filters=64,kernel_size=3,strides=1,name='m_9')\r\n        # self.m_10 = tf.keras.layers.BatchNormalization()\r\n\r\n        self.m_11 = tf.keras.layers.ZeroPadding2D(1)\r\n        self.m_12 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,name='m_12')\r\n        # self.m_13 = tf.keras.layers.BatchNormalization()\r\n\r\n        # self.m_14 = tf.keras.layers.AveragePooling2D(pool_size=2,strides=2)\r\n        self.m_14 = tf.keras.layers.Conv2D(filters=64, kernel_size=2, strides=2)\r\n\r\n        self.f_1 = tf.keras.layers.Flatten()\r\n        self.f_2 = tf.keras.layers.Dense(256)\r\n        self.f_3 = tf.keras.layers.Dense(1)\r\n\r\n\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec([None,36,36,6], tf.float32),\r\n    ])\r\n    def call(self,inputs):\r\n        _in = tf.split(inputs,2,axis=3)\r\n        A = self.a_1(_in[1])\r\n        A = self.a_2(A)\r\n        # A = self.a_3(A)\r\n        A = tf.keras.activations.tanh(A)\r\n\r\n        A = self.a_4(A)\r\n        A = self.a_5(A)\r\n        # A = self.a_6(A)\r\n        A = tf.keras.activations.tanh(A)\r\n\r\n        M1 = self.att_conv_1(A)\r\n        # M1 = tf.keras.activations.sigmoid(M1)\r\n        # B,_,H,W = tf.shape(M1)\r\n        norm = tf.abs(M1)\r\n        norm = tf.math.reduce_sum(norm)\r\n        norm = 2 * norm\r\n        norm = tf.reshape(norm,[1,1,1,1])\r\n        M1 = tf.divide(M1,norm)\r\n\r\n        A = self.a_7(A)\r\n        A = self.a_8(A)\r\n        A = self.a_9(A)\r\n        # A = self.a_10(A)\r\n        A = tf.keras.activations.tanh(A)\r\n\r\n        A = self.a_11(A)\r\n        A = self.a_12(A)\r\n        # A = self.a_13(A)\r\n        A = tf.keras.activations.tanh(A)\r\n\r\n        M2 = self.att_conv_2(A)\r\n        # M2 = tf.keras.activations.sigmoid(M2)\r\n        # B, _, H, W = tf.shape(M2)\r\n        norm = tf.abs(M2)\r\n        norm = tf.math.reduce_sum(norm)\r\n        norm = 2 * norm\r\n        norm = tf.reshape(norm, [1, 1, 1, 1])\r\n        M2 = tf.divide(M2, norm)\r\n\r\n        M = self.m_1(_in[0])\r\n        M = self.m_2(M)\r\n        # M = self.m_3(M)\r\n        M = tf.keras.activations.tanh(M)\r\n\r\n        M = self.m_4(M)\r\n        M = self.m_5(M)\r\n        # M = self.m_6(M)\r\n\r\n        ones_1 = tf.ones([1,36,36,36])\r\n        mat_1 = ones_1 @ M1 #tf.matmul(ones_1,M1)\r\n        g1 = mat_1*M\r\n        M = tf.keras.activations.tanh(g1)\r\n\r\n        M = self.m_7(M)\r\n\r\n        M = self.m_8(M)\r\n        M = self.m_9(M)\r\n        # M = self.m_10(M)\r\n        M = tf.keras.activations.tanh(M)\r\n\r\n        M = self.m_11(M)\r\n        M = self.m_12(M)\r\n        # M = self.m_13(M)\r\n\r\n        ones_2 = tf.ones([1,18,18,18])\r\n        mat_2 = ones_2 @ M\r\n        g2 = mat_2 * M\r\n        M = tf.keras.activations.tanh(g2)\r\n\r\n        M = tf.keras.activations.tanh(M)\r\n\r\n        M = self.m_14(M)\r\n\r\n        F = self.f_1(M)\r\n        F = self.f_2(F)\r\n        F = self.f_3(F)\r\n\r\n        return F\r\n\r\nclass Model(tf.Module):\r\n    def __init__(self):\r\n        self.model = Deepphys()\r\n        self.model.compile(\r\n            optimizer='adam',\r\n            loss=tf.keras.losses.mean_squared_error)\r\n\r\n\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec([1, 36,36,6], tf.float32),\r\n        tf.TensorSpec([1, ], tf.float32),\r\n    ])\r\n    def train(self, x, y):\r\n        with tf.GradientTape() as tape:\r\n            predictions = self.model(x)\r\n            loss = self.model.loss(predictions, y)\r\n        gradients = tape.gradient(loss, self.model.trainable_variables)\r\n        self.model.optimizer.apply_gradients(\r\n            zip(gradients, self.model.trainable_variables))\r\n        result = {\"loss\":loss}\r\n        return result\r\n\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec([None,36,36,6], tf.float32),\r\n    ])\r\n    def infer(self, x):\r\n\r\n        output = self.model(x)\r\n        return {\r\n            \"output\" : output\r\n        }\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n    def save(self, checkpoint_path):\r\n        tensor_names = [weight.name for weight in self.model.weights]\r\n        tensors_to_save = [weight.read_value() for weight in self.model.weights]\r\n        tf.raw_ops.Save(\r\n            filename=checkpoint_path, tensor_names=tensor_names,\r\n            data=tensors_to_save, name='save')\r\n        return {\r\n            \"checkpoint_path\": checkpoint_path\r\n        }\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n    def restore(self, checkpoint_path):\r\n        restored_tensors = {}\r\n        for var in self.model.weights:\r\n            restored = tf.raw_ops.Restore(\r\n                file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\r\n                name='restore')\r\n            var.assign(restored)\r\n            restored_tensors[var.name] = restored\r\n        return restored_tensors\r\n\r\nwith open('../../params.json') as f:\r\n    jsonObject = json.load(f)\r\n    params = jsonObject.get(\"params\")\r\n    model_params = jsonObject.get(\"model_params\")\r\n    save_root_path = params[\"save_root_path\"]\r\n    model_name = model_params[\"name\"]\r\n    dataset_name = params[\"dataset_name\"]\r\n    option=\"train\"\r\n\r\nhpy_file = h5py.File(save_root_path + model_name + \"_\" + dataset_name + \"_\" + option + \".hdf5\", \"r\")\r\nvideo_data = []\r\nlabel_data = []\r\nfor key in hpy_file.keys():\r\n     video_data.extend(hpy_file[key]['preprocessed_video'])\r\n     label_data.extend(hpy_file[key]['preprocessed_label'])\r\nhpy_file.close()\r\n\r\n# train_loader = DataLoader(video_data,label_data,1)\r\n\r\n# train_loader = DataLoader(video_data[:(int)(video_data.__len__()*0.8)],label_data[:(int)(label_data.__len__()*0.8)],1)\r\n# valid_loader = DataLoader(video_data[(int)(video_data.__len__()*0.8):],label_data[(int)(label_data.__len__()*0.8):],1)\r\n# test_loader = DataLoader(video_data,label_data,32)\r\n\r\nNUM_EPOCHS = 10\r\nBATCH_SIZE = 1\r\nepochs = np.arange(1, NUM_EPOCHS + 1, 1)\r\nlosses = np.zeros([NUM_EPOCHS])\r\nm = Model()\r\n# m.model.build(input_shape=(1,36,36,6))\r\n# m.model.summary()\r\nvideo_data = np.asarray(video_data)\r\nlabel_data = np.asarray(label_data)\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((video_data[:2],label_data[:2]))\r\ntrain_ds = train_ds.batch(BATCH_SIZE)\r\n\r\n# trian\r\nfor i in range(NUM_EPOCHS):\r\n  for x,y in train_ds:\r\n    # x = tf.reshape(x, [-1, 2, 36, 36, 3])\r\n    x = tf.cast(x,tf.float32)\r\n    # y = tf.cast(y,tf.float32)\r\n    result = m.train(x, y)\r\n  losses[i] = result['loss']\r\n  if (i + 1) % 1 == 0:\r\n      # print(result['loss'])\r\n   print(f\"Finished {i+1} epochs\")\r\n   print(f\"  loss: {losses[i]:.3f}\")\r\n\r\nm.save('/tmp/trained_model')\r\n\r\nSAVED_MODEL_DIR = \"./tmp\"\r\n\r\ntf.saved_model.save(\r\n    m,\r\n    SAVED_MODEL_DIR,\r\n    signatures={\r\n        'train':\r\n            m.train.get_concrete_function(),\r\n        'infer':\r\n            m.infer.get_concrete_function(),\r\n        # 'save':\r\n        #     m.save.get_concrete_function(),\r\n        # 'restore':\r\n        #     m.restore.get_concrete_function(),\r\n    })\r\n\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\r\n# converter = tf.lite.TFLiteConverter.from_concrete_functions()\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n]\r\n\r\nconverter.experimental_enable_resource_variables = True\r\ntflite_model = converter.convert()\r\nf = open('test.tflite','wb')\r\nf.write(tflite_model)\r\nf.close()\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\ninfer = interpreter.get_signature_runner(\"infer\")\r\n\r\nfor x, _ in train_ds:\r\n    x = tf.cast(x, tf.float32)\r\n    logits_original = m.infer(x=x)['output'][0]\r\n    logits_lite = infer(x=x)['output'][0]\r\n\r\n    # print(logits_original, logits_lite)\r\n\r\nprint(\"try\")\r\ntrain = interpreter.get_signature_runner(\"train\")\r\n\r\n# train_loader = DataLoader(video_data[:(int)(video_data.__len__()*0.8)],label_data[:(int)(label_data.__len__()*0.8)],1)\r\nprint(\"tried\")\r\nfor i in range(NUM_EPOCHS):\r\n  for x,y in train_ds:\r\n    x = tf.cast(x,tf.float32)\r\n    y = tf.cast(y,tf.float32)\r\n    result = train(x=x,y=y)\r\n    print(result)\r\n\r\n\r\n\r\n# # EncoderBlock\r\n            #\r\n            # # ConvBlock1\r\n            # tf.keras.layers.ZeroPadding3D(padding=(0,2,2),input_shape=(32,128,128,3)),\r\n            # tf.keras.layers.Conv3D(filters=16,kernel_size=(1,5,5),strides=(1,1,1)),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ReLU(),\r\n            # #MaxPool3d1r\r\n            # # tf.keras.layers.MaxPool3D(pool_size=(1,2,2), strides=(1,2,2)),\r\n            # tf.keras.layers.Conv3D(filters=16, kernel_size=(1, 2, 2), strides=(1, 2, 2)),\r\n            # # #\r\n            # # # ConvBlock2\r\n            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),\r\n            # tf.keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), strides=(1, 1, 1)),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ReLU(),\r\n            # # ConvBlock3\r\n            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),\r\n            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ReLU(),\r\n            #\r\n            # # # MaxPool3d2\r\n            # tf.keras.layers.Conv3D(filters=64, kernel_size=(2, 2, 2), strides=(2, 2, 2)),\r\n            # # tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2)),\r\n            # #\r\n            # # ConvBlock4\r\n            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),\r\n            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ReLU(),\r\n            # # ConvBlock5\r\n            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),\r\n            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ReLU(),\r\n            # #\r\n            # # # MaxPool3d3\r\n            # # tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2)),\r\n            # tf.keras.layers.Conv3D(filters=64, kernel_size=(2, 2, 2), strides=(2, 2, 2)),\r\n            # #\r\n            # # # ConvBlock6\r\n            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),\r\n            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ReLU(),\r\n            # # ConvBlock7\r\n            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),\r\n            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ReLU(),\r\n            # #\r\n            # # # MaxPool3d4\r\n            # # tf.keras.layers.MaxPool3D(pool_size=(1, 2, 2), strides=(1, 2, 2)),\r\n            # tf.keras.layers.Conv3D(filters=64, kernel_size=(1, 2, 2), strides=(1, 2, 2)),\r\n            # # #\r\n            # # # ConvBlock8\r\n            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),\r\n            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ReLU(),\r\n            # # ConvBlock9\r\n            # tf.keras.layers.ZeroPadding3D(padding=(1, 1, 1)),\r\n            # tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 1, 1)),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ReLU(),\r\n            #\r\n            # # # #\r\n            # # # # DecoderBlock\r\n            # # # #\r\n            # # DeConvBlock1\r\n            # tf.keras.layers.Convolution3DTranspose(filters=64,kernel_size=(4,1,1),strides=(2,1,1),padding='same'),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ELU(),\r\n            #\r\n            # # DeConvBlock2\r\n            # tf.keras.layers.Convolution3DTranspose(filters=64, kernel_size=(4, 1, 1), strides=(2, 1, 1),padding='same'),\r\n            # tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.ELU(),\r\n            #\r\n            # #\r\n            # # AdaptivePooling\r\n            # #\r\n            # AdaptivePooling3D(tf.reduce_max,(32,1,1)),\r\n            # # #\r\n            # #Conv3D\r\n            # #\r\n            # # tf.keras.layers.Conv3D(filters=32,kernel_size=(8,8,1)),\r\n            #\r\n            # tf.keras.layers.Conv3D(1,kernel_size=(1,64,64),strides=(1,1,1),padding='same'),\r\n            # tf.keras.layers.Reshape((-1,)),`\r\n\r\n\r\n```", "comments": ["@SpicyYeol \r\nIn order to expedite the trouble-shooting process, please provide a  code snippet to reproduce the issue reported here. Thanks!", "I use the modified example code. I added print(\"train_finish) in train loop. but I can't see that.\r\n\r\n```\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(\"TensorFlow version:\", tf.__version__)\r\n\r\nIMG_SIZE = 28\r\n\r\nclass Model(tf.Module):\r\n\r\n  def __init__(self):\r\n    self.model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation=\"sigmoid\",input_shape=(IMG_SIZE,IMG_SIZE,1)),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.AveragePooling2D(pool_size=2),\r\n        tf.keras.layers.Flatten(name='flatten'),\r\n        tf.keras.layers.Dense(128, activation='relu', name='dense_1'),\r\n        tf.keras.layers.Dense(10, name='dense_2')\r\n    ])\r\n\r\n    self.model.compile(\r\n        optimizer='sgd',\r\n        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))\r\n\r\n  @tf.function(input_signature=[\r\n      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\r\n      tf.TensorSpec([None, 10], tf.float32),\r\n  ])\r\n  def train(self, x, y):\r\n    with tf.GradientTape() as tape:\r\n      prediction = self.model(x)\r\n      loss = self.model.loss(y, prediction)\r\n    gradients = tape.gradient(loss, self.model.trainable_variables)\r\n    self.model.optimizer.apply_gradients(\r\n        zip(gradients, self.model.trainable_variables))\r\n    result = {\"loss\": loss}\r\n    return result\r\n\r\n  @tf.function(input_signature=[\r\n      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\r\n  ])\r\n  def infer(self, x):\r\n    logits = self.model(x)\r\n    probabilities = tf.nn.softmax(logits, axis=-1)\r\n    return {\r\n        \"output\": probabilities,\r\n        \"logits\": logits\r\n    }\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n  def save(self, checkpoint_path):\r\n    tensor_names = [weight.name for weight in self.model.weights]\r\n    tensors_to_save = [weight.read_value() for weight in self.model.weights]\r\n    tf.raw_ops.Save(\r\n        filename=checkpoint_path, tensor_names=tensor_names,\r\n        data=tensors_to_save, name='save')\r\n    return {\r\n        \"checkpoint_path\": checkpoint_path\r\n    }\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n  def restore(self, checkpoint_path):\r\n    restored_tensors = {}\r\n    for var in self.model.weights:\r\n      restored = tf.raw_ops.Restore(\r\n          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\r\n          name='restore')\r\n      var.assign(restored)\r\n      restored_tensors[var.name] = restored\r\n    return restored_tensors\r\n\r\nfashion_mnist = tf.keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n\r\n\r\ntrain_images = (train_images / 255.0).astype(np.float32)\r\ntest_images = (test_images / 255.0).astype(np.float32)\r\n\r\ntrain_labels = tf.keras.utils.to_categorical(train_labels)\r\ntest_labels = tf.keras.utils.to_categorical(test_labels)\r\n\r\nNUM_EPOCHS = 10\r\nBATCH_SIZE = 100\r\nepochs = np.arange(1, NUM_EPOCHS + 1, 1)\r\nlosses = np.zeros([NUM_EPOCHS])\r\nm = Model()\r\n\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\ntrain_ds = train_ds.batch(BATCH_SIZE)\r\n\r\nfor i in range(NUM_EPOCHS):\r\n  for x,y in train_ds:\r\n    result = m.train(x, y)\r\n\r\n  losses[i] = result['loss']\r\n  if (i + 1) % 10 == 0:\r\n    print(f\"Finished {i+1} epochs\")\r\n    print(f\"  loss: {losses[i]:.3f}\")\r\n\r\nm.save('/tmp/model.ckpt')\r\n\r\n\r\nSAVED_MODEL_DIR = \"saved_model\"\r\n\r\ntf.saved_model.save(\r\n    m,\r\n    SAVED_MODEL_DIR,\r\n    signatures={\r\n        'train':\r\n            m.train.get_concrete_function(),\r\n        'infer':\r\n            m.infer.get_concrete_function(),\r\n        'save':\r\n            m.save.get_concrete_function(),\r\n        'restore':\r\n            m.restore.get_concrete_function(),\r\n    })\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n]\r\nconverter.experimental_enable_resource_variables = True\r\ntflite_model = converter.convert()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\r\n\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n]\r\n\r\nconverter.experimental_enable_resource_variables = True\r\ntflite_model = converter.convert()\r\nf = open('test2.tflite','wb')\r\nf.write(tflite_model)\r\nf.close()\r\n\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\ninfer = interpreter.get_signature_runner(\"infer\")\r\n\r\n\r\nlogits_original = m.infer(x=train_images[:1])['logits'][0]\r\nlogits_lite = infer(x=train_images[:1])['logits'][0]\r\n\r\nprint(logits_original,logits_lite)\r\n\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\ntrain_ds = train_ds.batch(BATCH_SIZE)\r\n\r\ntrain = interpreter.get_signature_runner(\"train\")\r\n\r\nfor i in range(NUM_EPOCHS):\r\n  for x,y in train_ds:\r\n    x = tf.cast(x,tf.float32)\r\n    y = tf.cast(y,tf.float32)\r\n    result = train(x=x,y=y)\r\n    print(\"train_finish\")\r\n    print(result)\r\n```\r\n", "@jvishnuvardhan  Was able to reproduce this issue on colab using TF v2.7.0 and tf-nightly(2.9.0-dev20220130) ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/31415cc59d2dc304629d02f5fa46d5dd/54001.ipynb#scrollTo=L__q4B26ZeRO) for reference.\r\nThanks!", "@sushreebarsa Thanks for the answer. I tested my code with tf-nightly(2.9.0) and I confirmed training. \r\nAll 2D model works very well. Thanks.\r\nMy final goal is  train a 3D conv model on mobile using tflite. however, It already doesn't work yet.\r\nAny plans for support to 3d models in the near future?", "@haozha111 Triaging training related questions."]}, {"number": 53939, "title": "backporting setup.py relaxation of requirements to 2.7", "body": "Hi folks wondering if it would be possible to backport https://github.com/tensorflow/tensorflow/commit/41e66148999080da5b2b80a59740a8f2a59f17da to 2.7 since that would make 2.7 way more flexible to install on environments.\r\n\r\n\r\n", "comments": ["@casassg \r\nWhat issues are you facing with this version, that need to backport.", "Mostly being unable to use typing-extensions severely limit my ability to use latest typing pep while also using tensorflow."]}, {"number": 53905, "title": "[MHLO] add DynamicBroadcastInDimOp canonicalize pattern", "body": "* replace constant shape's `dynamic_broadcast_in_dim` with `broadcast_in_dim`", "comments": []}, {"number": 53870, "title": "Generated debug info can not be read by TensorBoard under distributed training environment", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, a distributed training example\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Train on CentOS 7; Read the dump on Macbook M1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Does not apply\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.8.0-rc0-24-g671aa8d 2.8.0-rc0\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source): bazel 4.2.2\r\n- GCC/Compiler version (if compiling from source): GCC 9.3.0\r\n- CUDA/cuDNN version: Does not apply\r\n- GPU model and memory: Does not apply\r\n\r\n**Describe the current behavior**\r\n\r\nThe dump files can not be read by TensorBoard under the distributed training environment\r\n\r\n**Describe the expected behavior**\r\n\r\nIt can be read from TensorBoard.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n- Run the following code and generate the dump files:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport os\r\nimport json\r\nimport sys\r\nimport time\r\nfrom absl import logging\r\n\r\nnum_epochs = 1\r\nbatch_size_per_replica = 64\r\nlearning_rate = 0.001\r\n\r\nworker_idx = int(sys.argv[1])\r\nnum_workers = 2\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"10.3.1.101:20000\", \"10.3.1.102:20000\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': worker_idx}\r\n})\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\r\nos.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'\r\n\r\nlogging.set_verbosity(logging.DEBUG)\r\n\r\n################ HERE ######################\r\ntf.debugging.experimental.enable_dump_debug_info('/home/geng.161/my-tfdbg-dumps', tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\r\n################ HERE ######################\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\nbatch_size = batch_size_per_replica * num_workers\r\n\r\ndef resize(image, label):\r\n    image = tf.image.resize(image, [224, 224]) / 255.0\r\n    return image, label\r\n\r\ndataset = tfds.load(\"cats_vs_dogs\", split=tfds.Split.TRAIN, as_supervised=True)\r\ndataset = dataset.map(resize).shuffle(1024).batch(batch_size)\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.applications.MobileNetV2(weights=None, classes=2)\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\r\n    )\r\n\r\nbegin = time.time()\r\nmodel.fit(dataset, epochs=num_epochs)\r\nend = time.time()\r\nprint(\"Train Time: %s\" % (end - begin))\r\n```\r\n\r\n- Copy the dumps on my laptop, and use TensorFlow to read the dumps.\r\n```bash\r\ntensorboard --logdir ~/Downloads/my-tfdbg-dumps\r\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\r\nTensorBoard 2.8.0 at http://localhost:6006/ (Press CTRL+C to quit)\r\n```\r\n\r\nBut I got nothing on the TensorBoard,\r\n![image](https://user-images.githubusercontent.com/14839342/150647620-6b9923c6-d0c2-49d6-a507-925929a59f28.png)\r\n\r\nI've posted this issue on the TensorBoard repo, but they think this is a bug stem from TensorFlow since the dumps can be read without the distributed strategy. They suggested that I should post an issue on the TensorFlow repo.\r\n\r\nRefer: https://github.com/tensorflow/tensorboard/issues/5518\r\n", "comments": ["@pwrliang Could you please post this issue on [TensorBoard repo](https://github.com/tensorflow/tensorboard/issues) to get the right help there?Thanks!", "> @pwrliang Could you please post this issue on [TensorBoard repo](https://github.com/tensorflow/tensorboard/issues) to get the right help there?Thanks!\r\n\r\n@sushreebarsa Hi, I opened a issue on TensorBoard, they said this issue only happened when under the distributed training settings, so they suggested me open the issue  on TensorFlow repo. Please read my issue throughout, thanks"]}, {"number": 53869, "title": "Installing tensorflow with pip", "body": "**System information**\r\n- TensorFlow version (you are using): 2.7\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen installing tensorflow on windows, after `pip install tensorflow`, it further needs to install Cuda and cudann and also needs to add their paths to windows environmental utility.  I'm not sure why we need to face this hassle whereas, in pytorch, we don't see such things. \r\n\r\n**Will this change the current api? How?** dunno. \r\n\r\n**Who will benefit from this feature?** all of the painful souls who use tensorflow\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @Saduf2019 ! Could you please look at this feature request?", "Hey I want to contribute to this, can you please guide me through\r\n", "@mihaimaruseac @sachinprasadhs any idea? "]}, {"number": 53845, "title": "XLA docs for `Map` are unclear and incomplete", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/xla/operation_semantics#map\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nNo. The section\r\n```\r\nThe mapped function is an arbitrary computation with the restriction that it has N inputs of scalar type T and a single output with type S. The output has the same dimensions as the operands except that the element type T is replaced with S.\r\n```\r\nis not clear. Does it mean the output of the mapped function or the output of `Map`? I think it means the latter but it's not worded like that.\r\n\r\n### Correct links\r\n\r\nYes\r\n\r\n### Parameters defined\r\n\r\nNo. It's not clear what `dimensions` is for, and the argument `static_operands` is missing, though it it possibly alluded to in the definition of `computation` (see mention of `M` indices)\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nAs much as any other function in these docs.\r\n\r\n### Usage example\r\n\r\nNo. An example would be amazing, should the devs feel like adding one.\r\n\r\n### Request visuals, if applicable\r\n\r\nn/a\r\n\r\n### Submit a pull request?\r\n\r\nNo.", "comments": ["@joelberkeley ,\r\nPlease feel free to submit a PR for the requested change or share the link where requested change is to be made", "@tilakrayal the link is included. I can't submit a PR because the problem is a lack of knowledge", "The sentence `The output has the same dimensions as the operands except that the element type T is replaced with S.` refers to output of Map function. \r\nWill it be fine for you if the sentence is changed to `The output of Map has the same dimensions as the operands except that the element type T is replaced with S.`", "sounds good, thanks. There are a few other things missing in the `Map` docs. Would be great if they were fixed too", "Could you please mention the issues you find in the doc or the missing details in the doc. Thanks", "I've mentioned them in the ticket description", "see \"Parameters defined\""]}, {"number": 53844, "title": "Loading optimizer weights on on TPUs", "body": "**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): GCP\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 2.5\r\nPython version: 3\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n\r\nRunning many training sessions on TPUs using GCP (typically v2-32). Instead of using module.fit in keras, we use optimizer.apply_gradients.  Basically, we would like to be able to stop and restart training sessions by loading model weights and optimizer weights. The optimizer weights are really important because we would like to tune.\r\n\r\nIn keras, there are options to save optimizer weights using np.save(PATH_TO_OPT, optimizer.get_weights())\r\n\r\nOn GPUs we can re-load them using:\r\n            def _loads_opt_weights():\r\n               opt_weights = np.load(PATH_TO_OPT, allow_pickle=True)\r\n               grad_vars = model.trainable_weights\r\n               zero_grads = [tf.zeros_like(w) for w in grad_vars]\r\n               optimizer.apply_gradients(zip(zero_grads, grad_vars))\r\n               optimizer.set_weights(opt_weights)\r\n            strategy.run(_loads_opt_weights)\r\n\r\nFor each GPU, but on the TPUs this doesn't work. We get an error of:\r\n\r\nNotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into strategy.run is a tf.function or strategy.run is called inside a tf.function if eager behavior is enabled\r\n\r\nTurning eager mode off using tf.compat.v1.disable_eager_execution(): \r\nResults in InaccessibleTensorError: Operation 'LogicalAnd_30' has been marked as not fetchable. Typically this happens when it is defined in another function or code block. Use return values,explicit Python locals or TensorFlow collections to access it. When calling optimizer.set_weights(..)\r\n\r\nHow do we get this to load on distributed TPUs?\r\n\r\nDo you want to contribute a PR? (yes/no): no\r\nBriefly describe your candidate solution(if contributing): N/A\r\n\r\nStandalone code to reproduce the issue: See above\r\n", "comments": ["Hi @ttdd11 ! As this error \"NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into strategy.run is a tf.function or strategy.run is called inside a tf.function if eager behavior is enabled\" suggests , Can you try again by including the function inside tf.function like this ? Please provide a sample stand alone code to proceed further . Thank you!\r\n\r\n```\r\n@tf.function\r\ndef _loads_opt_weights():\r\n  opt_weights = np.load(PATH_TO_OPT, allow_pickle=True)\r\n  grad_vars = model.trainable_weights\r\n  zero_grads = [tf.zeros_like(w) for w in grad_vars]\r\n  optimizer.apply_gradients(zip(zero_grads, grad_vars))\r\n  optimizer.set_weights(opt_weights)\r\n  strategy.run(_loads_opt_weights)\r\n```", "@mohantym thanks for the quick reply. I should have made the original post a bit more comprehensive. \r\n\r\nHere is standalone code to reproduce this (you have to put in a valid tpu name or set tpu to false if using gpus):\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os.path as osp\r\nimport pickle\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras import Model\r\n\r\n#build model\r\nX, y = np.random.rand(100, 50), np.random.randint(2, size=100)\r\nx = Input((50,))\r\nout = Dense(1, activation='sigmoid')(x)\r\nmodel = Model(x, out)\r\n\r\n#build setup an optimizer and save the state\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\r\nopt_path = 'opt_wights.npy'\r\ngrad_vars = model.trainable_weights\r\nzero_grads = [tf.zeros_like(w) for w in grad_vars]\r\noptimizer.apply_gradients(zip(zero_grads, grad_vars))\r\nnp.save(opt_path, optimizer.get_weights())\r\n\r\n#do the same thing with a load, but in strategy scope\r\n#get the strategy\r\nbTPU = True\r\ntpu_name = 'tpu-name'\r\nif bTPU:\r\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)  # TPU detection\r\n    tf.config.experimental_connect_to_cluster(tpu)\r\n    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n    strategy = tf.distribute.TPUStrategy(tpu)\r\nelse:    \r\n    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\r\n    if len(gpus) > 1:\r\n        strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n    elif len(gpus) == 1:\r\n        strategy = tf.distribute.get_strategy()\r\n    else:\r\n        strategy = tf.distribute.get_strategy()\r\n\r\nwith strategy.scope():\r\n    #build the model and optimizer again\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\r\n    X, y = np.random.rand(100, 50), np.random.randint(2, size=100)\r\n    x = tf.keras.layers.Input((50,))\r\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n    model = tf.keras.Model(x, out)\r\n    @tf.function\t\r\n    def _model_weight_setting():\r\n        opt_weights = np.load(opt_path, allow_pickle=True)\r\n        grad_vars = model.trainable_weights\r\n        zero_grads = [tf.zeros_like(w) for w in grad_vars]\r\n        optimizer.apply_gradients(zip(zero_grads, grad_vars))\r\n        optimizer.set_weights(opt_weights)\r\n    strategy.run(_model_weight_setting)\r\n```\r\n\r\n\r\nWe have tried your solutions mentioned above (setting the @tf.function), and unfortunately this does not work on GPUs or TPUs. Without using the @tf.function, this script works on GPUs but not on TPUs.\r\n\r\nThe error log for both is very similar and is as follows when calling set_weights on the optimizer:\r\n\r\n2022-01-21 16:20:03.947418: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\r\n  return array(a, dtype, copy=False, order=order, subok=True)\r\n2022-01-21 16:20:04.430294: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.120.42.42:8470, 1 -> 10.120.42.43:8470, 2 -> 10.120.42.44:8470, 3 -> 10.120.42.45:8470}\r\n2022-01-21 16:20:04.430470: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:43503}\r\n2022-01-21 16:20:04.452118: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.120.42.42:8470, 1 -> 10.120.42.43:8470, 2 -> 10.120.42.44:8470, 3 -> 10.120.42.45:8470}\r\n2022-01-21 16:20:04.452298: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:43503}\r\n2022-01-21 16:20:04.454347: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:43503\r\nTraceback (most recent call last):\r\n  File \"test_opt.py\", line 56, in <module>\r\n    strategy.run(_model_weight_setting)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 402, in run\r\n    return self.extended.tpu_run(fn, args, kwargs, options)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 1428, in tpu_run\r\n    return func(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 933, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 764, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 3050, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 986, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nRuntimeError: in user code:\r\n\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1448 replicated_fn  *\r\n        result[0] = fn(*replica_args, **replica_kwargs)\r\n    test_opt.py:55 _model_weight_setting  *\r\n        optimizer.set_weights(opt_weights)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/adam.py:161 set_weights  **\r\n        super(Adam, self).set_weights(weights)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1148 set_weights\r\n        param_values = backend.batch_get_value(params)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:3746 batch_get_value\r\n        raise RuntimeError('Cannot get value inside Tensorflow graph function.')\r\n\r\n    RuntimeError: Cannot get value inside Tensorflow graph function.\r\n\r\n", "Hi @ttdd11 ! There was a syntax error in earlier answer. Have attached  updated [gist](https://colab.sandbox.google.com/gist/mohantym/0bab7c440905503bbeabc1bd8c0d1769/github_53844.ipynb) in 2.7 for reference. Thank you!", "@mohantym thanks for the reply. I'm a bit confused though.\r\n\r\nThe change I see is that the strategy.run call was moved into the @tf.function. Was there a syntax error in what I sent? Do you need me to correct something so move this along? \r\n\r\nIs this run call even executed? So if you print optimizer.get_weights() at the end of modified code will it print the loaded weights?\r\n\r\n", "Hi @Saduf2019 ! Could you look at this issue? Attaching gist in [2.6](https://colab.sandbox.google.com/gist/mohantym/0dab4bb65ba2b800f1a5fbc74c2aefc7/github_53844.ipynb#scrollTo=aMnmqhS_PY6-), [2.7](https://colab.sandbox.google.com/gist/mohantym/0bab7c440905503bbeabc1bd8c0d1769/github_53844.ipynb#scrollTo=aMnmqhS_PY6-) and [2.8.rc1](https://colab.sandbox.google.com/gist/mohantym/2bd6c423f1e04598e469a46720b6fd9e/github_53844.ipynb#scrollTo=I1XCsNqyPfvD) for reference. Thanks!", "@Saduf2019 is there anything I can do/tests I can run to help with this? Currently all of our GCP experiments are on hold.", "@mohantym @Saduf2019 any ideas? Is loading optimizer weights not currently supported on distributed TPU training? Is there a short term solution that would allow this to happen with some code changes? ", "@mohantym @Saduf2019 @jvishnuvardhan Is there really nothing that can be done about this? Can I save the model in a different format? Manually load the weights? I think the documentation should reflect that loading optimizer states is not supported on TPUs if there is no solution. ", "@ttdd11 Can you please share a standalone code that we can use to reproduce the issue? I tried your code and it throws different error as shown in [this gist](https://colab.research.google.com/gist/jvishnuvardhan/463f703c8d38554c85176b2ac2d6b9cb/github_53844.ipynb). May be share a *.py file or colab file. Thanks!", "@jvishnuvardhan I believe that timeout is occurring because the tpu of \"tpu-name\" doesn't exist with your system. If you use no parameters in the cluster resolver, ie.  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() it won't time out on that line.", "@jvishnuvardhan Is this enough information to take a look? Would you like anything else? If you take a look at the gists from @mohantym from 22 days ago they have the correct TPUClusterResolver.", "@jvishnuvardhan @Saduf2019 I'm really looking for any method at this point to load the optimizer state on tpus. Is there a different model format that I can use instead? To me it seems like there must be a work around, otherwise no model tuning would be possible on TPUs.", "@jvishnuvardhan @Saduf2019 here is a standalone file if that helps. Is there no workaround to this? \r\n\r\n[standaalone_load_opt_state.txt](https://github.com/tensorflow/tensorflow/files/8147647/standaalone_load_opt_state.txt)\r\n\r\nIn summary, we just want to re-start training where I was. Same model and optimizer state. For us this is important to tune models, but also restart a training session if the GCP times out. I hope this helps understand where we are coming from. Please let me know if I can help test anything.", "@gadagashwini do you need anything from us in order to escalate this?", "device and focus isnt being registered properly due to disruptive intergrating users on the ios operating system", "@fig666 @gadagashwini I'm not sure if I follow the last point. \r\n\r\nMy questions:\r\n1.  Anything I can do to help here?\r\n2. Are there any other solutions to this that would allow us to load optimizer and model weights via a different mechanism so we can tune some models? \r\n3. Is something that is being investigated in general? \r\n\r\nI'm mostly asking because I haven't yet heard from @Saduf2019 who is assigned to this, and it's generally preventing us from effectively tuning models. ", "@Saduf2019 @gadagashwini @jvishnuvardhan I'm unsure what other options we have here. We are coming up on three months and the assignee has not yet responded. It effectively means that we can't run GCP experiments. "]}, {"number": 53838, "title": "Add encoding support for make_csv_dataset and File IO", "body": "For now, you can't read CSV files with an encoding except for utf-8. \r\nThis PR enables `tf.data.experimental.make_csv_dataset`  to accept an encoding parameter. \r\nTo make it available, I also changed `FileIO` to take an arbitrary encoding parameter.", "comments": ["Sorry, I'm not sure why I was selected here, but I don't think I'm the best reviewer for this change since this is not code I'm responsible for. I'd suggest someone on TF data, at least `tf.data` parts.", "Hi @gbaned.\r\nInstead of @nfelt, will someone review this pull request?", "Hi, could I know when this PR will be merged? Otherwise, is there anything that is needed to correct in this PR?"]}, {"number": 53828, "title": "Descriptions of `tf.RaggedTensor`'s methods '__xx__' are unclear", "body": "Documentation bug for: https://www.tensorflow.org/api_docs/python/tf/RaggedTensor?hl=en#__and__\r\n\r\n## Description of issue:\r\n\r\n### Usage examples are unmatched\r\n`tf.RaggedTensor` has a number of class methods, for example, `__abs__`, `__add__`, ... The usage examples for these methods are unmatched. For instance, the example code for `__add__` does not contain any **ragged tensors**, instead the examples are all normal tensors (of type `tf.Tensor`). \r\n\r\nThe example code is:\r\n```\r\na = tf.constant([True])\r\nb = tf.constant([False])\r\ntf.math.logical_and(a, b)\r\n```\r\nThat is not expected for a `tf.RaggedTensor.__add__()` method.", "comments": ["@ArrowIntoTheSky \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose), and provide more details on this?\r\nPlease let us know if you want to contribute to this.\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@sushreebarsa Sorry for the late response. I have updated the description of the issue to include more details. Can you please look into this?", "Added a PR [#55222](https://github.com/tensorflow/tensorflow/pull/55222) for the fix.", "@ArrowIntoTheSky,\r\nA RaggedTensor is a subclass of Tensor. If the method is not redefined, the parent class method is used. Hence the examples given are in dense tensor. Thanks!"]}, {"number": 53816, "title": "`tf.norm` not accurate in Tensorflow 2.7.0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): pypl\r\n- TensorFlow version (use command below): v2.7.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): not required\r\n- GCC/Compiler version (if compiling from source): not required\r\n- CUDA/cuDNN version: 11.0 (from docker image of TF 2.7.0)\r\n- GPU model and memory: 2080ti\r\n\r\n**Describe the current behavior**\r\n\r\nWe are using `tf.norm` to make a vector be unit vector.\r\n\r\nThis is quite simple and we use the following code in TF 2.2.0.\r\n\r\n```python\r\nvec = np.array([4.6463e-03, -4.8086258e+01], dtype=\"float32\")\r\namount = tf.norm(vec, ord=2, axis=-1)\r\n```\r\n\r\nThe amount is `<tf.Tensor: shape=(), dtype=float32, numpy=48.086258>` in TF 2.2.0.\r\nThus the normalizing is correct and we get an unit vector by `vec / amount`.\r\nBut in TF 2.7.0, the amount will be `<tf.Tensor: shape=(), dtype=float32, numpy=48.086254>`, which is incorrect.\r\n\r\nWe also found that this situation only happened when the tensor is computed on GPU.\r\nIn other word, if we set `os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"`, we could get correct value in TF 2.7.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe norm value should be `<tf.Tensor: shape=(), dtype=float32, numpy=48.086258>` in both CPU or GPU.\r\n", "comments": ["@Janus-Shiau ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.", "I think using the code block above can reproduce the problem.\r\nBut I can also provide more if you need.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nif not tf.__version__ == \"2.7.0\" or not tf.test.is_gpu_available():\r\n    return\r\n\r\nvec = np.array([4.6463e-03, -4.8086258e+01], dtype=\"float32\")\r\namount = tf.norm(vec, ord=2, axis=-1)\r\n\r\nprint(amount)\r\n``` \r\nIn my case, this is enough to reproduce the issue.", "@Janus-Shiau ,\r\nI was facing different error while trying to execute the mentioned code.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/0c9110805d5e9ecb36ea12f907004de9/untitled196.ipynb).", "@tilakrayal delete the `if` and the `return` and you see the problem. This code may make it even clearer:\r\n\r\n```python\r\nimport math\r\nimport tensorflow as tf\r\n\r\nx = 48.086258\r\nprint(x)\r\nprint(math.sqrt(x**2))\r\nprint(tf.constant(x).numpy())\r\nprint(tf.norm(x).numpy())\r\n```\r\n\r\nOutput:\r\n```\r\n48.086258\r\n48.086258\r\n48.086258\r\n48.086254 # note this!\r\n```", "Thank you @bersbersbers. Your example is quite clear!\r\nThe `if` statement is merely ensuring the TF version is correct and GPUs are utilized.", "This problem has two parts, offering two workarounds:\r\n\r\n1. Part of the problem is that GPU TensorFlow is computing, by default, using `float32`, while CPU TensorFlow is doing `float64` computations (as do `numpy` and Python's `math` module):\r\n\r\n   ```python\r\n   import math\r\n   import numpy as np\r\n   import tensorflow as tf\r\n   \r\n   x = 99.0\r\n   a = np.array(x)\r\n   a32 = np.array(x, dtype=np.float32)\r\n   a64 = np.array(x, dtype=np.float64)\r\n   t = tf.constant(x)\r\n   t32 = tf.constant(x, dtype=tf.float32)\r\n   t64 = tf.constant(x, dtype=tf.float64)\r\n   \r\n   print(np.sqrt(a32))\r\n   print(tf.sqrt(t).numpy())\r\n   print(tf.sqrt(t32).numpy())\r\n   \r\n   print(math.sqrt(x))\r\n   print(np.sqrt(a))\r\n   print(np.sqrt(a64))\r\n   print(tf.sqrt(t64).numpy())\r\n   ```\r\n   \r\n   Output:\r\n   ```\r\n   9.949874\r\n   9.949874\r\n   9.949874\r\n   9.9498743710662\r\n   9.9498743710662\r\n   9.9498743710662\r\n   9.9498743710662\r\n   ```\r\n   \r\n   You see that `a` behaves like `a64`, while `t` behaves like `t32`. You can avoid the problem using `float64`.\r\n\r\n2. But this is not the whole story. Yes, it's similar for `tf.norm` in that you can avoid the problem with `float64`:\r\n\r\n   ```python\r\n   import numpy as np\r\n   import tensorflow as tf\r\n   \r\n   x = 48.086258\r\n   a = np.array(x)\r\n   a32 = np.array(x, dtype=np.float32)\r\n   a64 = np.array(x, dtype=np.float64)\r\n   t = tf.constant(x)\r\n   t32 = tf.constant(x, dtype=tf.float32)\r\n   t64 = tf.constant(x, dtype=tf.float64)\r\n   \r\n   print(tf.norm(t).numpy())\r\n   print(tf.norm(t32).numpy())\r\n   \r\n   print(np.linalg.norm(a))\r\n   print(np.linalg.norm(a32))\r\n   print(np.linalg.norm(a64))\r\n   print(tf.norm(t64).numpy())\r\n   ```\r\n   \r\n   Output:\r\n   ```\r\n   48.086254\r\n   48.086254\r\n   48.086258\r\n   48.086258\r\n   48.086258\r\n   48.086258\r\n   ```\r\n   \r\n   But you also see that this time, `numpy` computes correctly in `float32`. Interestingly, the same is true in TF using the alternative code commented out here:\r\n   \r\n   https://github.com/tensorflow/tensorflow/blob/c256c071bb26e1e13b4666d1b3e229e110bc914a/tensorflow/python/ops/linalg_ops.py#L759-L763\r\n   \r\n   ```python\r\n   import tensorflow as tf\r\n   from tensorflow.python.ops import math_ops\r\n   \r\n   x = 48.086258\r\n   t32 = tf.constant(x, dtype=tf.float32)\r\n   \r\n   print(math_ops.sqrt(math_ops.reduce_sum(t32 * math_ops.conj(t32))).numpy())\r\n   print(math_ops.reduce_euclidean_norm(t32).numpy())\r\n   ```\r\n   \r\n   Output:\r\n   ```\r\n   48.086254\r\n   48.086258\r\n   ```\r\n   \r\n   So another workaround is to use `math_ops.reduce_euclidean_norm`, and you can stick to your `float32` data type.\r\n\r\nI guess the TF team should switch to `math_ops.reduce_euclidean_norm` according to the `TODO` comment.", "@jvishnuvardhan ,\r\nI was able to reproduce the issue in tf [v2.5](https://colab.research.google.com/gist/tilakrayal/f0607f8b519b7cd48e46fe9e958a2c41/53816_2-5.ipynb),[v2.7](https://colab.research.google.com/gist/tilakrayal/178449572d81bfd8f6a1949897832195/53816-2-7.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/d4edc7553898f01c0876a23bd72d4577/53816-nightly.ipynb).Please find the gist here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53816\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53816\">No</a>\n", "Excuse my ignorance, but it looks to me like https://github.com/tensorflow/tensorflow/commit/18937a7232d8c66559b44afb99da2743f907da5a exactly reverted https://github.com/tensorflow/tensorflow/commit/d5a0174ddd279c1dd8ee434aa05edea4e0afaaf0. Should this issue be re-opened?"]}, {"number": 53810, "title": "Installing TF without Google OAuthLib", "body": "**System information**\r\n- OS Platform and Distribution: Docker deployment with RedHat 8 base image\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.7\r\n- Python version: 3.8.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- CUDA/cuDNN version: TBA (Will get info, but don't think it's relevant to issue)\r\n- GPU model and memory: TBA (Will get info, but don't think it's relevant to issue)\r\n\r\n\r\n**Problem**\r\nHi,\r\n\r\nWhere I work, we use Twistlock, a container security analysis tool, in our current workflow.\r\n\r\nTwistlock reports this error with our current container:\r\n\r\n`There is no support for PKCE implementation in the oauthlib client. Client-side PKCE for OAuth2 RFC 7636 is required for applications to have secure communication with the authorization server. OAuth 2.0 public clients utilizing the Authorization Code Grant are susceptible to the authorization code interception attack.`\r\n\r\nWe don't have the luxury to use another version of TF; we effectively need to use as close to the latest version as we can. That said, we need to be able to install TF but without this lib. Is this possible?\r\n\r\nI should mention it seems this lib is needed to train models on Google Cloud (or so I read), but we don't need that functionality.\r\n\r\nIt would be helpful if this could be an optional feature instead of a required one. Currently, this is preventing us from deploying our container, as we can't deploy if this error yields.\r\n\r\nI also apologize if this is improper use of this ticket (or ticketing system), I was asked to create one in the mean time, so the people managing Twistlock can make an exception if they can refer back to this ticket, but I also am seeking information about this issue we ran into.\r\n\r\nThanks for your time in advance.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Create docker container with Redhat 8 and Python 3.8.8, and startup container\r\n2. create virtualenv\r\n3. pip install tensorflow\r\n4. Run Twistlock on container (I'm greatly simplifying this step because Twistlock is part of service that's not directly in our hands; we submit the container to a service that automatically runs Twistlock 3rd party for us.)\r\n\r\n**Any other info / logs**\r\n\r\nTwistlock:\r\n`There is no support for PKCE implementation in the oauthlib client. Client-side PKCE for OAuth2 RFC 7636 is required for applications to have secure communication with the authorization server. OAuth 2.0 public clients utilizing the Authorization Code Grant are susceptible to the authorization code interception attack.`\r\n", "comments": ["@x0734x \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "I updated it a bit. Let me know if you need more info.", "@x0734x Could you please create another virtual env and try the same?Please refer to the [build from source](https://www.tensorflow.org/install/source?hl=nb), [install TF2](https://www.tensorflow.org/install),[ docker guide](https://www.tensorflow.org/install/docker?hl=sk) and let us know if it helps?Thanks!", "I think there is a misunderstanding here... I'm not having trouble installing Tensorflow. I'm saying the installation of TF requires a library *that has a known security vulnerability* as described by Twistlock - a service we use to scan for vulnerabilities. So I'm asking is there a way to install TF without this Google OAuth lib? (Cause we don't need the library)\r\n\r\nI don't see anywhere in the installation guides where this lib can be made optional if installed from source either."]}, {"number": 53800, "title": "TFLite GPU delegate crash", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): nightly\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): 4.2.2\r\n- GCC/Compiler version (if compiling from source): 8.4\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: GTX 1650, 4GB\r\n\r\nThere is a segfault in the destructor of `tflite::gpu::cl::Buffer::Release()`, the stack trace is as follows:\r\n\r\n```\r\n==5463== Process terminating with default action of signal 11 (SIGSEGV)\r\n==5463==  Access not within mapped region at address 0x8\r\n==5463==    at 0xC0B4C20: ??? (in /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.460.106.00)\r\n==5463==    by 0xBFA452B: ??? (in /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.460.106.00)\r\n==5463==    by 0xBFA1AD7: ??? (in /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.460.106.00)\r\n==5463==    by 0xBFA3BF1: ??? (in /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.460.106.00)\r\n==5463==    by 0x1756C1: tflite::gpu::cl::Buffer::Release() (buffer.cc:82)\r\n==5463==    by 0x1636DD: tflite::gpu::cl::Buffer::~Buffer() (buffer.h:46)\r\n==5463==    by 0x171213: void std::_Destroy<tflite::gpu::cl::Buffer>(tflite::gpu::cl::Buffer*) (stl_construct.h:98)\r\n==5463==    by 0x16F706: void std::_Destroy_aux<false>::__destroy<tflite::gpu::cl::Buffer*>(tflite::gpu::cl::Buffer*, tflite::gpu::cl::Buffer*) (stl_construct.h:108)\r\n==5463==    by 0x16D497: void std::_Destroy<tflite::gpu::cl::Buffer*>(tflite::gpu::cl::Buffer*, tflite::gpu::cl::Buffer*) (stl_construct.h:137)\r\n==5463==    by 0x16A046: void std::_Destroy<tflite::gpu::cl::Buffer*, tflite::gpu::cl::Buffer>(tflite::gpu::cl::Buffer*, tflite::gpu::cl::Buffer*, std::allocator<tflite::gpu::cl::Buffer>&) (stl_construct.h:206)\r\n==5463==    by 0x16684C: std::vector<tflite::gpu::cl::Buffer, std::allocator<tflite::gpu::cl::Buffer> >::~vector() (stl_vector.h:567)\r\n==5463==    by 0x164A8B: tflite::gpu::cl::InferenceContext::~InferenceContext() (inference_context.h:64)\r\n==5463==  If you believe this happened as a result of a stack\r\n==5463==  overflow in your program's main thread (unlikely but\r\n==5463==  possible), you can try to increase the size of the\r\n==5463==  main thread stack using the --main-stacksize= flag.\r\n==5463==  The main thread stack size used in this run was 8388608.\r\n==5463==  \r\n```\r\n\r\nThis problem is only producible with NVidia's OpenCL. However, other implementations of OpenCL (e.g. Qualcomm) is known to let undefined behaviors pass silently. \r\nI believe the problem is related to OpenCL sub buffers. If I hack the tensorflow/lite/delegates/gpu/cl/inference_context.cc file and set `use_offset_assignment` to `false` right before: \r\n```\r\n  if (use_offset_assignment) {\r\n    shared_buffers_.resize(offset_assignment.offsets.size());\r\n    RETURN_IF_ERROR(CreateReadWriteBuffer(offset_assignment.total_size, context,\r\n                                          &shared_buffers_parent_));\r\n    for (int i = 0; i < offset_assignment.offsets.size(); ++i) {\r\n      RETURN_IF_ERROR(CreateReadWriteSubBuffer(\r\n          shared_buffers_parent_, offset_assignment.offsets[i],\r\n          buffer_usage_records[i].tensor_size, context, &shared_buffers_[i]));\r\n    }\r\n  } ...\r\n```\r\nthus preventing `CreateReadWriteSubBuffer` from being ever called, then this problem goes away.\r\nIncidentally, I also noticed that the model I am trying to infer runs faster without using sub buffers (20ms with my hack disabling sub buffers, 30ms if I allow sub buffers, on my GTX 1650). ", "comments": ["@DwayneDuane ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.", "@tilakrayal You can reproduce this problem with:\r\n```\r\n    TfLiteGpuDelegateOptionsV2 opts = TfLiteGpuDelegateOptionsV2Default();\r\n\r\n    TfLiteDelegate* gpuDelegate = TfLiteGpuDelegateV2Create(&opts);\r\n    TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n\r\n    TfLiteInterpreterOptionsAddDelegate(options, gpuDelegate);\r\n\r\n    //This problem should be be reproducible using any convolutional model\r\n    TfLiteModel* model = TfLiteModelCreateFromFile(\"./inception_v4.tflite\");\r\n    TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n\r\n    TfLiteInterpreterAllocateTensors(interpreter);\r\n\r\n    TfLiteInterpreterInvoke(interpreter);\r\n\r\n    TfLiteInterpreterDelete(interpreter);\r\n    TfLiteGpuDelegateV2Delete(gpuDelegate);\r\n    TfLiteInterpreterOptionsDelete(options);\r\n    TfLiteModelDelete(model);\r\n```\r\nand the floating point Inception V4 model from https://www.tensorflow.org/lite/guide/hosted_models\r\nJust a reminder that this problem is only reproducible on NVidia GPU and using TFLite v2.7 or more recent. I am using the nightly branch.", "@DwayneDuane \r\nCould you please try with 2.6 and let us know if this is still an issue.", "@Saduf2019 \r\n\r\n- This issue is not reproducible in 2.6. \r\n- In the current nightly branch, I am almost sure the problem is caused by the `GreedyBySizeAssignment` function in `greedy_by_size_assignment.cc`.  If I change the **memory strategy** in this line inside `AllocateMemoryForBuffers` function (inside `inference_context.cc`):\r\n```\r\n    RETURN_IF_ERROR(\r\n        AssignOffsetsToTensors(buffer_usage_records, MemoryStrategy::GREEDY_BY_SIZE,\r\n                               &offset_assignment, base_align_bytes));\r\n```\r\nto `MemoryStrategy::EQUALITY`, the problem goes away and valgrind turns up clean as well.", "@DwayneDuane \r\nCan you please use 2.6 as its a stable version and nightly has is still worked upon and yest to be tested.", "@Saduf2019\r\nI might be able to use 2.6 for now. But will you be fixing this issue for 2.8? In the meantime, if you wish, I can make a PR to change MemoryStrategy::GREEDY_BY_SIZE to MemoryStrategy::EQUALITY (or some other strategy).", "@DwayneDuane \r\nPlease feel free to create a pr for the same."]}, {"number": 53798, "title": "Why is my CPU Performance of Float64 tf.matmul in TensorFlow2 significantly lower than the NumPy matmul, even in the graph mode?", "body": "\r\n\r\nI'm comparing the single thread performance of the matrix-matrix products in **TensorFlow 2** and **NumPy**. I compare separately for single precision (float32) and double precision (float64). I find that the **NumPy** performance is almost equivalent to the Intel MKL C++ implementation (used as a benchmark for matrix multiplication) for both single and double precision (DGEMM and SGEMM). But in **TensorFlow**, only the single precision (float32) performance is equivalent to the MKL, and the double precision (float64) performance is significantly slower. **Why is Tensorflow slower when used with double precision data?**\r\n\r\n**Sample Scripts:**\r\n\r\nI consider the following instance to reproduce my observation. Consider the matrix multiplication:     \r\n\r\n>  **C = AB** where A and B are of size 3000x3000\r\n\r\nThe TensorFlow2 and NumPy code are given below:\r\n\r\n***Tensorflow2 code***\r\n```python\r\nimport tensorflow as tf\r\nimport os\r\nimport time\r\n\r\n\r\n#Check if MKL is enabled\r\nimport tensorflow.python.framework as tff\r\nprint(\"MKL Enabled : \", tff.test_util.IsMklEnabled())\r\n\r\n\r\n#Set threads\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\ntf.config.threading.set_intra_op_parallelism_threads(1)\r\n\r\n#Problem size\r\nN = 3000\r\nREPS = 20\r\nDTYPE = tf.float64\r\n#DTYPE = tf.float32\r\n\r\n\r\n@tf.function\r\ndef gemm_implicit_noup(A, B):\r\n    #C = A @ B\r\n    start = tf.timestamp()\r\n    with tf.control_dependencies([start]):\r\n        C = tf.matmul(A,B)\r\n    with tf.control_dependencies([C]):\r\n        end = tf.timestamp()\r\n    tf.print(end-start)\r\n    return C\r\n\r\ntf.config.run_functions_eagerly(False)\r\n\r\nA = tf.random.normal([N, N], dtype=DTYPE)\r\nB = tf.random.normal([N, N], dtype=DTYPE)\r\n\r\n\r\n#Building Trace\r\nC = gemm_implicit_noup(A,B)\r\n\r\nfor i in range(REPS):\r\n   C = gemm_implicit_noup(A,B)\r\n```\r\n\r\n***Numpy code***\r\n\r\n```python\r\nimport os\r\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\r\nimport numpy as np\r\nimport time\r\n\r\nN = 3000\r\nREPS = 20\r\nDTYPE = np.float64\r\n#DTYPE = np.float32\r\n\r\ndef gemm_implicit_noup(A, B):\r\n    #C = A @ B\r\n    C = np.matmul(A,B)\r\n    return C\r\n\r\n\r\n\r\nA = np.random.randn(N,N).astype(DTYPE)\r\nB = np.random.randn(N,N).astype(DTYPE)\r\n\r\nfor i in range(REPS):\r\n   start = time.perf_counter()\r\n   C = gemm_implicit_noup(A,B)\r\n   end = time.perf_counter()\r\n   print(end-start)\r\n```\r\n\r\n**System and Installation settings:**\r\n\r\nThe performance was compared on Intel Xeon Skylake 2.1 GHz with CentOS 7 and also on MacBook Pro 2018 with BigSur. The performance was compared on both **Tensorflow 2.7** and **2.8**, which were built with Intel MKL. **Python 3.9.7** and **3.7.4** were checked. I compare the single thread performance so that the results can be reliably reproduced. I observe similar performance numbers in all the settings:\r\n\r\nSingle precision performance is as expected:\r\n\r\n - Intel MKL C++ SGEMM ~ **0.5s**\r\n - NumPy float32 ~ **0.5s**\r\n - TensorFlow float32 ~ **0.5s**\r\n\r\nBut Double precision performance:\r\n\r\n - Intel MKL C++ DGEMM ~ **0.9s**\r\n - NumPy float64 ~ **1s**\r\n - TensorFlow float64 > **2.5s** (Much Slower!!)\r\n\r\n", "comments": ["Hi @Saduf2019 ! Could you please look at this issue. Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/7677b90b32da9402547dd8c83ad74616/github_53798.ipynb#scrollTo=6C2Tx0r5op0E) for reference."]}, {"number": 53784, "title": "Conv 16x8 incorrect reference kernel usage", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):NA\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):NA\r\n- TensorFlow version (use command below):NA\r\n- Python version:NA\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIntroduced by PR: https://github.com/tensorflow/tensorflow/commit/0d8705c82c64dfb39c49e346de1a66182e5eabd1\r\n\r\nCode in 16x8 conv.cc file expects input/kernel/output offsets to be present if 16x8 conv has a 32 bit bias. But the reference kernel that is invoked does not use offsets as that is the default behavior for 16x8 conv.\r\n\r\n**Describe the expected behavior**\r\nIt is one of the two\r\n1. Do not expect offsets for 16x8 conv in case of 32 bit bias. In which case the correction is in conv.cc else\r\n2. Add offset handling in the reference kernel.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):no\r\n- Briefly describe your candidate solution(if contributing):NA\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nNA\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@dayeongl Please have a look..", "Could you elaborate more on this?\r\n\"Code in 16x8 conv.cc file expects input/kernel/output offsets to be present if 16x8 conv has a 32 bit bias. But the reference kernel that is invoked does not use offsets as that is the default behavior for 16x8 conv.\r\n\"\r\n\r\nCould you show any error msg that you are seeing here?", "@dayeongl This is just from a code review. So, there aren't any error messages.\r\n\r\nSo, in conv.cc, \r\nhttps://github.com/tensorflow/tensorflow/blob/1a46edf47699a2a10b0e801a48afdc95f5660395/tensorflow/lite/kernels/conv.cc#L811 this line states that for a 16x8 case and with 32 bit bias there can be offsets and the reference kernel is invoked(reference_integer_ops::ConvPerChannel)\r\n\r\nThe reference implementation for 16x8 (https://github.com/tensorflow/tensorflow/blob/1a46edf47699a2a10b0e801a48afdc95f5660395/tensorflow/lite/kernels/internal/reference/integer_ops/conv.h#L129) does not have a provision for handling offsets. \r\n\r\n \r\n", "Note that `has_non_zero_point` https://github.com/tensorflow/tensorflow/blob/1a46edf47699a2a10b0e801a48afdc95f5660395/tensorflow/lite/kernels/conv.cc#L797\r\nshould never be true as quantized int16 tensors should always have a null zero-point. \r\n\r\nIt's also enforced by the \r\nhttps://github.com/tensorflow/tensorflow/blob/1a46edf47699a2a10b0e801a48afdc95f5660395/tensorflow/lite/kernels/conv.cc#L359\r\ncheck in the Prepare method of the Conv2D operator.\r\n\r\nNote that the same issue applies for FC and TranposeConv2D operators.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@dayeongl Could you confirm the intention with zero points and int16?"]}, {"number": 53781, "title": "[TFLite] An multi-headers model output problem on GPU Delegates.", "body": "Hi TFers,\r\nI just found the problem that I was getting wrong data output from my model having multi-headers. For example, if the graph my model is like 0 -> 1 -> 2 -> 3 and the model inputs is [0] outputs is [2, 3], I will only get correct answer from 3 but 2 is not, because our GraphFloat32::outputs() only believe the value with no consumer, the 3 above, is an output value. However, our tflite::Interpreter still tell us the outputs is [2, 3] and it's right. So, I try to fix with this [PR](https://github.com/tensorflow/tensorflow/pull/53780), please take a check.\r\n\r\nFollowing is my code snippet with [this model](https://github.com/tensorflow/tensorflow/files/7875588/small_test.zip) for test :\r\n```\r\n#define PRINT(format, ...)                              \\\r\n  do {                                                  \\\r\n    TFLITE_LOG(TFLITE_LOG_INFO, format, ##__VA_ARGS__); \\\r\n  } while (false);\r\n\r\nbool SmallTest(Json::Value& config) {\r\n  auto RunInterpreter =\r\n      [](std::unique_ptr<tflite::Interpreter>* interpreter) -> bool {\r\n    assert((*interpreter)->inputs().size() == 1);\r\n    assert((*interpreter)->outputs().size() == 2);\r\n\r\n    float* input_buffer = (*interpreter)->typed_input_tensor<float>(0);\r\n    input_buffer[0] = 1.;\r\n    auto input_index = (*interpreter)->inputs()[0];\r\n    auto size_inputs = (*interpreter)->inputs().size();\r\n    PRINT(\"(SmallTest) Input: [%f] to input tensors: [%d] with size %d.\",\r\n          input_buffer[0], input_index, size_inputs);\r\n\r\n    bool is_ok = (*interpreter)->Invoke() == kTfLiteOk;\r\n    if (!is_ok) {\r\n      return false;\r\n    }\r\n\r\n    float* first_output = (*interpreter)->typed_output_tensor<float>(0);\r\n    float* second_output = (*interpreter)->typed_output_tensor<float>(1);\r\n    PRINT(\"(SmallTest) Output: [%f, %f] from output tensors: [%d, %d]\",\r\n          first_output[0], second_output[0], (*interpreter)->outputs()[0],\r\n          (*interpreter)->outputs()[1]);\r\n\r\n    return true;\r\n  };\r\n\r\n  {\r\n    auto model = tflite::FlatBufferModel::BuildFromFile(\r\n        config[\"model_path\"].asCString());\r\n    if (!model) return false;\r\n\r\n    tflite::ops::builtin::BuiltinOpResolver op_resolver;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    tflite::InterpreterBuilder(*model, op_resolver)(&interpreter);\r\n    interpreter->AllocateTensors();\r\n    PRINT(\"(SmallTest) CPU:\");\r\n    RunInterpreter(&interpreter);\r\n  }\r\n\r\n  {\r\n    auto model = tflite::FlatBufferModel::BuildFromFile(\r\n        config[\"model_path\"].asCString());\r\n    if (!model) return false;\r\n    tflite::ops::builtin::BuiltinOpResolver op_resolver;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    tflite::InterpreterBuilder(*model, op_resolver)(&interpreter);\r\n\r\n    // NEW: Prepare GPU delegate.\r\n    auto options = TfLiteGpuDelegateOptionsV2Default();\r\n    auto* delegate = TfLiteGpuDelegateV2Create(&options);\r\n    if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk)\r\n      assert(false);\r\n\r\n    interpreter->AllocateTensors();\r\n    PRINT(\"(SmallTest) GPU:\");\r\n    RunInterpreter(&interpreter);\r\n\r\n    // NEW: Clean up.\r\n    TfLiteGpuDelegateV2Delete(delegate);\r\n  }\r\n\r\n  return true;\r\n}\r\n#undef PRINT\r\n```\r\n", "comments": ["@multiverse-tf Please give me a hand.", "Hi @sachinprasadhs ! Could you please look at this issue? It is linked with PR #53780 . Thank you!"]}, {"number": 53777, "title": "[TFLite] Optional Debug Tools can only support log in Desktop Computer", "body": "Hi TensorFlowers,\r\nThe `tflite::PrintInterpreterState(interpreter.get())` in [minimal.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc#L62) can only be used on computer system with desktop. Because it use `printf` style to output info in [optional_debug_tools.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/optional_debug_tools.cc#L278), not the `TFLITE_LOG` in [minimal_logging](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/minimal_logging.h) the better way supporting more systems. Besides, why not use something like `std::stringstream` to format info, that seems more general?", "comments": []}, {"number": 53776, "title": "[TFLite] Code Snippet in official GPU Advances Guide Doc Get Crash.", "body": "Hi TensorFlowers,\r\nThe following codes in [official guide](https://tensorflow.google.cn/lite/performance/gpu_advanced#android_cc) seem to be crash until my [PR](https://github.com/tensorflow/tensorflow/pull/53734) applied, please take a check, thx.\r\n```\r\n// Set up interpreter.\r\nauto model = FlatBufferModel::BuildFromFile(model_path);\r\nif (!model) return false;\r\nops::builtin::BuiltinOpResolver op_resolver;\r\nstd::unique_ptr<Interpreter> interpreter;\r\nInterpreterBuilder(*model, op_resolver)(&interpreter);\r\n\r\n// NEW: Prepare GPU delegate.\r\nauto* delegate = TfLiteGpuDelegateV2Create(/*default options=*/nullptr);\r\nif (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;\r\n\r\n// Run inference.\r\nWriteToInputTensor(interpreter->typed_input_tensor<float>(0));\r\nif (interpreter->Invoke() != kTfLiteOk) return false;\r\nReadFromOutputTensor(interpreter->typed_output_tensor<float>(0));\r\n\r\n// NEW: Clean up.\r\nTfLiteGpuDelegateV2Delete(delegate);\r\n```", "comments": ["@multiverse-tf Can you give me a hand?"]}, {"number": 53772, "title": "issue with building manylinux wheel", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n\r\nThe system is the image build from [this dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.rbe.cuda11.4-cudnn8.2-ubuntu18.04-manylinux2010-multipython)\r\n\r\n**Describe the problem**\r\nWhen fixing the wheel built in the above environment into a manylinux wheel, auditwheel complains \r\n```\r\nauditwheel: error: cannot repair \"/tmp/tensorflow-2.8.0rc0-cp39-cp39-linux_x86_64.whl\" to \"manylinux2010_x86_64\" ABI because of the presence of too-recent versioned symbols. You'll need to compile the wheel on an older toolchain.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nBuild commands:\r\n```\r\ncd <tensorflow source>\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=11.4\r\nexport TF_CUDNN_VERSION=8\r\n\r\nexport LD_LIBRARY_PATH=\"/usr/local/cuda-${CUDA}/targets/x86_64-linux/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH:/usr/local/cuda/lib64/stubs:/usr/local/cuda-${CUDA}/lib64:/usr/local/cuda-${CUDA}/lib64\"\r\n\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=3.7,5.2,7.0,7.5,8.0\r\n  \r\nexport TF_BUILD_FLAGS=\"--config=opt --config=v2 --config=cuda \\\r\n    --distinct_host_configuration=false \\\r\n    --action_env=TF_CUDA_VERSION --action_env=TF_CUDNN_VERSION \\\r\n    --action_env=LD_LIBRARY_PATH --action_env=TF_CUDA_COMPUTE_CAPABILITIES\"\r\n\r\nyes \"\" | python3.9 configure.py\r\nbazel build --color=yes --curses=yes $TF_BUILD_FLAGS --verbose_failures \\\r\n //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nbefore 2.6 there used to be a toolchain to build manylinux wheel \r\n```\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.6/third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010-nvcc-cuda11.2/BUILD\r\n```\r\nsince 2.7 this toolchain has been removed, I wonder how one should build manylinux wheels without those toolchains\r\n\r\n", "comments": ["@mihaimaruseac Could you please provide some insight into this?", "The toolchains are now in tensorflow/toolchains repository. TF's `.bazelrc` and `workspaceN.bzl` files should point to the corresponding file", "Hi @mihaimaruseac , I think you are referring to this repo https://github.com/tensorflow/toolchains/tree/master \r\nbut I don't really see nvcc toolchains for building cuda supported manylinux wheel. This file https://github.com/tensorflow/toolchains/blob/master/toolchains/BUILD seems to include a toolchain for building cpu manylinux wheel, but my guess is you need a different toolchain for building gpu wheel "]}, {"number": 53771, "title": "Deterministic selection of deterministic cuDNN convolution algorithms removed in TF 2.5", "body": "Summary: deterministic selection of deterministic cuDNN convolution algorithms (added in TensorFlow version 1.14) has been broken since version 2.5. This issue explains how it was broken, provides a work-around, and discusses next-steps.\r\n\r\nPull request [34951](https://github.com/tensorflow/tensorflow/pull/34951), opened by @duncanriach (merged on 2020-01-27 for TF 2.2), added multi-algorithm deterministic cuDNN convolutions, which included deterministically selecting (via the auto-tuning code), for each path through the convolution op, the first working algorithm in a list of deterministic algorithms.\r\n\r\nThis refactoring [commit](https://github.com/tensorflow/tensorflow/commit/c865c2d8822ecc663f95a01e08b2f465d478f4e9), part of pull request [46965](https://github.com/tensorflow/tensorflow/pull/46965) opened by @kaixih (merged on 2021-03-19 for TF 2.5), accidentally removed the code from `tensorflow/core/kernels/gpu_utils.cc` that caused the first working algorithm to be deterministically selected when op determinism is enabled. Instead, the auto-tuner code runs as normal on the list of deterministic algorithms. This lead to a functional regression in deterministic operation that still exists up to and including TF version 2.8.\r\n\r\nAlthough I have heard @reedwm mention that he thought there might be a regression, this issue was confirmed by [this comment](https://github.com/tensorflow/tensorflow/pull/34951#issuecomment-974714116), from @leiwuzheng, in the original pull request 34951 (which added the functionality) for TF version 2.5. @leiwuzheng, please add some simple reproducer code here.\r\n\r\nThis regression was not prevented, and not discovered sooner, because there were and are no tests for this functionality. At the time I implemented it, it seemed very difficult or impossible to test.\r\n\r\nThankfully, the XLA-equivalent code (to deterministically select the first working algorithm from a list of deterministic algorithms) appears to have been retained in a similar refactoring by @cheshire in [this commit](https://github.com/tensorflow/tensorflow/commit/eed38bdd4d7bfa7d81efb99d8bd7d68e2288adc5) (committed on 2021-05-20).\r\n\r\nThe current work-around, as confirmed by @leiwuzheng, is to set the environment variable `TF_CUDNN_USE_FRONTEND` to `1` (from TF version 2.5 through at least 2.8, the default is `0`). This is an effective work-around because the code that uses the new cuDNN 8 frontend/backend API correctly selects the first algorithm from the list of deterministic algorithms, as the legacy code did prior to the regression.\r\n\r\nWhen `TF_CUDNN_USE_FRONTEND` defaults to `1`, the severity and urgency of this issue will be reduced, but the issue will persist until `TF_CUDNN_USE_FRONTEND` is removed along with the associated legacy code.\r\n\r\nThe intention of this current issue is to drive the reinstatement of the functionality in the legacy code, and to ensure that the functionality is tested in both the legacy code and the new code (with `TF_CUDNN_USE_FRONTEND` set to both `0` and `1`). Testing may require the addition of a mechanism to allow auto-tuning to be reset dynamically (in the running python process). I believe that @reedwm may have further thoughts on how to test.", "comments": ["@reedwm, if this cannot be fixed (and the fix cherry-picked into the r2.8 branch) before the final version 2.8 release, then I think that the work-around should be added to the [documentation](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism) for `tf.config.experimental.enable_op_determinism`. What are your thoughts on this?", "Changing the documentation also requires a cherry-pick, unfortunately.\r\n\r\nI'm still reading the code and the commits you referenced to fully understand what is going on. I mistakenly believed `TF_CUDNN_USE_FRONTEND` defaulted to `1`. Instead, it only defaults to `1` if TF is built with at least cuDNN 7.2. This is true in internal builds, which is what I've been testing with, but the pip package is built with cuDNN 7.1, so `TF_CUDNN_USE_FRONTEND` defaults to `0`.\r\n\r\nOnce I finish reading the code to understand this fully, I'll create and submit a fix and immediately try to cherry-pick it.\r\n\r\nI'll try to think of how to test this after we cherry-pick the fix in, since that is less urgent.", "Nice. Thanks, @reedwm.", "Stream executor will still filter out nondeterministic algorithms at least. So the only (known) source of nondeterminism is the fact which algorithm is chosen is nondeterministic. I will fix and try to cherry-pick.\r\n\r\nI found an example to reproduce. On a Titan V, with CUDA 11.2.2, cudnn 8.1.1 and TensorFlow 2.8rc0, the following is nondeterministic:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.keras.utils.set_random_seed(1)\r\ntf.config.experimental.enable_op_determinism()\r\n\r\ninp = tf.random.normal((64, 8, 8, 8))\r\nf = tf.random.normal((3, 3, 8, 8))\r\nout = tf.nn.conv2d(inp, f, [1, 1, 1, 1], padding='SAME')\r\nprint(tf.reduce_sum(out))\r\n```\r\n\r\nThis convolution doesn't look very unusual, so it's clear this issue can happen in practice. Hopefully this is what was causing #53792 and also hopefully there are no other major determinism issues like this one. I will fix and try very very hard to cherry-pick.", "I plan on fixing this by choosing the first working algorithm in `gpu_utils.cc` when determinism is enabled. Unfortunately, this still has a very slightly possibility of nondeterminism: Because memory consumption is nondeterministic, an algorithm may non deterministically OOM, and so the \"first working algorithm\" is also nondeterministic. It's possible to fix this by selecting the first working or OOMing algorithm, but I worry that logic might be too complicated for a last minute cherrypick.\r\n\r\nI'm pretty sure even before the [commit](https://github.com/tensorflow/tensorflow/commit/c865c2d8822ecc663f95a01e08b2f465d478f4e9) that caused this issue, this slightly possibility of nondeterminism could occur, so I think leaving this bug in TF 2.8 is OK. @duncanriach let me know if you disagree. This bug will be fixed for TF 2.9.\r\n\r\n", "@reedwm, very nice analysis and thinking. I agree with you, the OOM issue has not been addressed before and leaving it in 2.8 is okay.\r\n\r\n@edwardyehuang, my understanding is that @reedwm is intending to undo the regression that was introduced in TF2.5 in TF2.8, via a cherry-pick. He is also suggesting addressing a further (OOM) issue that has been raised in TF2.9. It's extremely unlikely the further issue would lead to nondeterminism for you.", "I created a cherry-pick request here https://github.com/tensorflow/tensorflow/pull/53826. It will likely get merged.", "Yay! I see it was merged. Thanks, @reedwm. BTW, the most recent release was 2.8.0-rc0 (not rc1).", "Note that, in TensorFlow 2.8.0, currently I have 5% chance to produce a non-determinsitc result (but closer), after I run again, it will produce the determinstic result.\r\n\r\nI did not meet this issue in TensorFlow 2.8.0 RC0 (with TF_CUDNN_USE_FRONTEND set)", "@edwardyehuang, are you saying that, with your particular model running on TensorFlow version 2.8.0, you get the same result on only 95% of the runs?\r\n\r\nDoes setting `TF_CUDNN_USE_FRONTEND=1` (when running on TensorFlow version 2.8.0) lead to the same result being produced on 100% of runs?", "> @edwardyehuang, are you saying that, with your particular model running on TensorFlow version 2.8.0, you get the same result on only 95% of the runs?\r\n> \r\n> Does setting `TF_CUDNN_USE_FRONTEND=1` (when running on TensorFlow version 2.8.0) lead to the same result being produced on 100% of runs\r\n\r\n1: TensorFlow 2.8 rc0 + TF_CUDNN_USE_FRONTEND=1. My models always produce same results every runs.\r\n2: TensorFlow 2.8 + TF_CUDNN_USE_FRONTEND=1. Some models, only happend once or twice, produce non-determinsitc results. However, after I ran again, result become the determinsitc one.", "@edwardyehuang, as far as I know, when `TF_CUDNN_USE_FRONTEND=1`, there is no functional difference between version 2.8.0-rc0 and version 2.8.0 with respect to determinism. Therefore, I expect that whatever you noticed is unrelated to changes that were made between version 2.8.0-rc0 and version 2.8.0. Most likely, the version difference is a red herring and you just happened to observe the nondeterminism when using version 2.8.0 and not when using version 2.8.0-rc0.\r\n\r\nIt is possible that this nondeterminism you noticed was caused by the nondeterministic out-of-memory (OOM) scenario that @reedwm [identified](https://github.com/tensorflow/tensorflow/issues/53771#issuecomment-1016028174) above. I believe that that functionality has existed going all the way back to TF 2.2 and is present whether `TF_CUDNN_USE_FRONTEND` is set to `0` (default) or `1`.\r\n\r\nHowever, without isolating the source, as @leiwuzheng did [here](https://github.com/tensorflow/tensorflow/pull/34951#issuecomment-974714116), we cannot know for sure if the nondeterminism you saw was really caused by nondeterministic selection of deterministic cuDNN convolution algorithms (due to running out of memory), or by something else.\r\n\r\nBTW, @leiwuzheng, (if possible) I would appreciate you confirming that with version 2.8.0 your model operates fully deterministically without setting `TF_CUDNN_USE_FRONTEND=1`. The nondeterministic selection of algorithms that you described [here](https://github.com/tensorflow/tensorflow/pull/34951#issuecomment-974714116), which is the primary focus of this current issue, should now be fixed.\r\n\r\nGeneral reminder to myself and others: before this issue can be closed, we're waiting for the OOM scenario to be addressed (regardless of whether `TF_CUDNN_USE_FRONTEND` is set to `0` or `1`), which @reedwm may complete before the version 2.9.0 branch is cut. @edwardyehuang, when those changes are in the master branch, you will be able to utilize them in the `tf-nightly` build.", "@duncanriach, sorry for late respond. Currently, we do not have TF version 2.8 installed yet, will try once we have it ready. Another not so optimal way of fixing this non deterministic behavior is just turn off auto tune by setting TF_CUDNN_USE_AUTOTUNE = 0. ", "No problem, @leiwuzheng. The recommended work-around for TensorFlow 2.5, 2.6, and 2.7 is to set `TF_CUDNN_USE_FRONTEND=1`. Setting `TF_CUDNN_USE_AUTOTUNE=0` may result in an exception with the message \"No algorithm worked!\" in some cases, and is therefore not recommended.", "@duncanriach , thanks for clarification. It's more clear now."]}]