[{"number": 42944, "title": "[ROCm] Fix for ROCm CSB Breakage - 200902 - 2", "body": "The following commit introduces regressions in one unit-test, in the ROCm TF build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/458d0906ebe75072ebd87cf5088772405d0cd03e\r\n\r\n```\r\n//tensorflow/python/kernel_tests:pooling_ops_test_gpu                    FAILED in 3 out of 3 in 41.8s\r\n```\r\n\r\nThe failures occur in the newly added subtests for testing the explicit padding feature. The code in the ROCm path has a couple of bugs in it, which lead to the failures. This commit fixes those bugs.\r\n\r\nNote that changes in PR #42897 are also required for to make the above unit test pass again with the ROCm TF build. The isssue being addressed in that PR is orthogonal to what is being addressed here, and hence the two separate PRs.\r\n\r\n\r\n------------------------------------------------\r\n\r\n\r\n@chsigg @cheshire @nvining-work \r\n", "comments": ["@chsigg gentle ping", "@chsigg gentle ping", "@chsigg gentle ping", "@chsigg gentle ping", "@chsigg gentle ping", "@chsigg gentle ping", "@chsigg gentle ping"]}, {"number": 42943, "title": "New tf.image.resize method: Maximum Value", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe tf.image.resize method does not include a Maximum Value method. \r\n\r\n**Will this change the current api? How?**\r\nNo, I don't think so.\r\n\r\n**Who will benefit with this feature?**\r\ntf.image.resize may be a useful function for Region of Interest (RoI) MaxPooling if it allowed for a method that extracts the maximum value for each pooled areas.\r\n\r\nIn my situation, 3D voxelised data (i.e. XYZ regular point cloud) needs to be unstacked  from 3D into 2D, and then  each 2D stack resized into a new dimension.  Using tf.image.resize with maximum voxel value that falls within each resized element could be useful. \r\n\r\n\r\n\r\n**Any Other info.**\r\n", "comments": ["Could [tf.nn.max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) be the functionality you're looking for?", "@dominikj2 Can you please follow the suggestion mentioned above and let us know whether the issue resolved for you. If not, can you please add more details about your use-case and the feature you are looking. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42942, "title": "[SE] Include absl/memory/memory.h", "body": "Otherwise our build environment fails to build it with:\n```\nexternal/org_tensorflow/tensorflow/stream_executor/host/host_platform.cc: In member function 'virtual stream_executor::port::StatusOr<std::unique_ptr<stream_executor::StreamExecutor> > stream_executor::host::HostPlatform::GetUncachedExecutor(const stream_executor::StreamExecutorConfig&)':\nexternal/org_tensorflow/tensorflow/stream_executor/host/host_platform.cc:73:25: error: 'make_unique' is not a member of 'absl'\n   auto executor = absl::make_unique<StreamExecutor>(\n                         ^~~~~~~~~~~\n```", "comments": []}, {"number": 42941, "title": "TFLite TransposeConvV2 Operator Slow on x86 CPU Ubuntu", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tf-nightly 2.4.0.dev20200902\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.5.0\r\n- GCC/Compiler version (if compiling from source): gcc 7.5.0\r\n- CUDA/cuDNN version: CUDA 10.1\r\n- GPU model and memory: Using CPU (Intel(R) Core(TM) i7-8086K CPU @ 4.00GHz)\r\n\r\n**Describe the current behavior**\r\nThe `TRANSPOSE_CONV` operator takes up >80% of total computation time when using the TFLite benchmarking tool.\r\n```\r\n============================ Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t          TRANSPOSE_CONV\t          235.882\t  487.948\t  482.097\t 26.927%\t 26.927%\t     0.000\t        1\t[device_0/g_ae/dec_0/conv2d_transpose1]:102\r\n\t          TRANSPOSE_CONV\t          719.253\t  163.323\t  162.187\t  9.059%\t 35.986%\t     0.000\t        1\t[device_0/g_ae/dec_1/conv2d_transpose1]:113\r\n\t          TRANSPOSE_CONV\t         1634.919\t  108.162\t  111.988\t  6.255%\t 42.241%\t     0.000\t        1\t[device_0/g_ae/dec_9/conv2d_transpose1]:201\r\n\t          TRANSPOSE_CONV\t         1501.813\t  116.089\t  109.471\t  6.114%\t 48.355%\t     0.000\t        1\t[device_0/g_ae/dec_8/conv2d_transpose1]:190\r\n\t          TRANSPOSE_CONV\t          882.714\t  111.952\t  108.459\t  6.058%\t 54.413%\t     0.000\t        1\t[device_0/g_ae/dec_2/conv2d_transpose1]:124\r\n\t          TRANSPOSE_CONV\t          993.583\t  103.796\t   97.807\t  5.463%\t 59.876%\t     0.000\t        1\t[device_0/g_ae/dec_3/conv2d_transpose1]:135\r\n\t          TRANSPOSE_CONV\t         1287.885\t   92.329\t   95.829\t  5.352%\t 65.229%\t     0.000\t        1\t[device_0/g_ae/dec_6/conv2d_transpose1]:168\r\n\t          TRANSPOSE_CONV\t         1394.631\t  109.527\t   95.786\t  5.350%\t 70.579%\t     0.000\t        1\t[device_0/g_ae/dec_7/conv2d_transpose1]:179\r\n\t          TRANSPOSE_CONV\t         1093.908\t   92.043\t   93.959\t  5.248%\t 75.827%\t     0.000\t        1\t[device_0/g_ae/dec_4/conv2d_transpose1]:146\r\n\t          TRANSPOSE_CONV\t         1193.164\t   88.003\t   89.509\t  4.999%\t 80.826%\t     0.000\t        1\t[device_0/g_ae/dec_5/conv2d_transpose1]:157\r\n\r\nNumber of nodes executed: 216\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t          TRANSPOSE_CONV\t       11\t  1466.204\t    81.898%\t    81.898%\t     0.000\t       11\r\n\t                 CONV_2D\t       11\t   159.086\t     8.886%\t    90.785%\t     0.000\t       11\r\n\t                     ABS\t       21\t   111.310\t     6.217%\t    97.002%\t     0.000\t       21\r\n\t                     ADD\t       32\t    11.551\t     0.645%\t    97.647%\t     0.000\t       32\r\n\t                     MUL\t       42\t    10.792\t     0.603%\t    98.250%\t     0.000\t       42\r\n\t                 RESHAPE\t       44\t     9.645\t     0.539%\t    98.789%\t     0.000\t       44\r\n\t                     SUB\t       21\t     9.366\t     0.523%\t    99.312%\t     0.000\t       21\r\n\t                    RELU\t       21\t     6.514\t     0.364%\t    99.676%\t     0.000\t       21\r\n\t           CONCATENATION\t       11\t     5.129\t     0.286%\t    99.962%\t     0.000\t       11\r\n\t      TfLiteFlexDelegate\t        1\t     0.430\t     0.024%\t    99.986%\t     0.000\t        1\r\n\t                    TANH\t        1\t     0.245\t     0.014%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=1811076 curr=1775110 min=1738229 max=1895058 avg=1.79038e+06 std=32080\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nFaster execution of this operator. I expected my model to run inference faster once I converted to TFLite, but currently it is running more slowly than regular tensorflow on the same hardware.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nbazel-bin/tensorflow/lite/tools/benchmark/benchmark_model_plus_flex \\\r\n  --graph=.../converted_model_float32.tflite --num_threads=4 --enable_op_profiling=true > .../float32_benchmark.txt\r\n```\r\n(The benchmarking tool was built from tf master source commit 86db5756535f70f1b1fab61c6f3f0483141510e8)\r\n\r\n**Other info / logs**\r\n[Full TFLite benchmark output](https://github.com/tensorflow/tensorflow/files/5171974/float32_benchmark.txt)\r\n\r\nI'd appreciate any tips for how I could profile this operator. How can I find out why it is taking so much time in my network? Can I use C++ profiling tools to find the computation time sinks in the [transpose_conv.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L5804)?", "comments": ["Hi, can you try adding \r\n\r\n```--define=tflite_with_ruy=true``` when building the benchmark tool?\r\n\r\nthanks!", "Hey @renjie-liu, I added that ruy flag when building the benchmarking tool (from master 86db5756535f70f1b1fab61c6f3f0483141510e8)\r\n```\r\nbazel build -c opt \\\r\n  --config=monolithic --define=tflite_with_ruy=true \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model_plus_flex\r\n```\r\nAnd then I ran the benchmarking tool on my float32 TFLite model\r\n```\r\nbazel-bin/tensorflow/lite/tools/benchmark/benchmark_model_plus_flex \\\r\n  --graph=.../converted_model_float32.tflite --num_threads=4 --enable_op_profiling=true > ~/Desktop/float32_ruy_benchmark.txt\r\n```\r\nIt seems it is running even more slowly and there is no additional information in the output (comparing to my original benchmark output linked in my original post). Do I need to run the benchmarking tool a different way to see the results of the ruy profiler?\r\n\r\n[float32_ruy_benchmark.txt](https://github.com/tensorflow/tensorflow/files/5190234/float32_ruy_benchmark.txt)\r\n", "I did some profiling of the [TransposeConvV2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L5804) while running the benchmarking.\r\n\r\n[transpose_conv_profiling.txt](https://github.com/tensorflow/tensorflow/files/5191661/transpose_conv_profiling.txt)\r\n\r\nEvidently, the largest time sink in the function is the `cpu_backend_gemm::Gemm` that is called in the `batch_size` for loop.\r\n\r\nI stepped through the [cpu_backend_gemm.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/cpu_backend_gemm.h#L117) code with gdb to check what this `cpu_backend_gemm::Gemm` function was doing. I noticed it defaults to the generic case, (`must_use_ruy` and `try_custom_gemv` are both false) whether I build the benchmark tool with `--define=tflite_with_ruy=true` or not.", "I see, this is really strange, can you add \r\n`--define=ruy_profiler=true` as well when building the benchmark_tool, we should be able to see the detailed profiling,", "That flag seemed to work! It output more info about `cpu_backend_gemm::Gemm`\r\n\r\n[float32_ruy_benchmark.txt](https://github.com/tensorflow/tensorflow/files/5191838/float32_ruy_benchmark.txt)\r\n", "Thanks for sharing! Will take a look!", "I wanted to see if this problem was specific to x86, so I built the benchmark tool on an AWS A1 arm instance. I did two benchmarks of the same model, one with `--define=ruy_profiler=true` and one without.\r\n\r\n```\r\nbazel-bin/tensorflow/lite/tools/benchmark/benchmark_model_plus_flex \\\r\n  --graph=.../converted_model_float32.tflite --num_threads=4 --enable_op_profiling=true --verbose=true > ~/float32_arm_benchmark.txt\r\n```\r\n\r\n[float32_arm_ruy_benchmark.txt](https://github.com/tensorflow/tensorflow/files/5198671/float32_arm_ruy_benchmark.txt)\r\n[float32_arm_benchmark.txt](https://github.com/tensorflow/tensorflow/files/5198670/float32_arm_benchmark.txt)\r\n\r\n\r\n", "ruy is enabled by default for arm devices, so the numbers should be similar", "From comparing these two architectures, the model seems to perform about 80% faster on arm while my clock speed on x86 is  2x faster.\r\n\r\nx86 avg time = 1.1e+07\r\narm avg time = 1.9+06\r\n\r\nx86 max gHz = 5.0\r\n[arm max gHz = 2.3](https://www.extremetech.com/computing/281338-amazon-launches-new-arm-based-custom-graviton-cpu#:~:text=The%20Graviton%20CPU,clock%20speed%20of%202.3GHz.)\r\n\r\n```\r\nx86 ruy timings:\r\nTimings (microseconds): count=15 first=10617766 curr=10610651 min=10610651 max=10833023 avg=1.06631e+07 std=73060\r\n\r\narm ruy timings:\r\nTimings (microseconds): count=50 first=1849736 curr=1847769 min=1844926 max=1857462 avg=1.85012e+06 std=2777\r\n\r\n( 1.1e7 - 1.9e6 ) / (1.1e7) ~~ 80% time reduction\r\n```\r\n\r\nSomething else I noticed is that x86 with ruy was much slower than x86 without ruy. ARM with ruy timings are similar to x86 without ruy:\r\n```\r\nx86 without ruy:\r\nTimings (microseconds): count=50 first=1811076 curr=1775110 min=1738229 max=1895058 avg=1.79038e+06 std=32080\r\nMemory (bytes): count=0\r\n```\r\n\r\nAny idea why the x86 benchmark is slower with ruy? (compared to the x86 without ruy)", "@talumbau is already the right assignee :-)", "how can I disable ruy on arm so I can get a comparable benchmark to the x86 without ruy?", "`bazel build --define=tflite_with_ruy=false`", "@bjacob setting ruy to false on arm throws a compile error, but that is likely out of the scope of this issue.\r\n\r\nI did an additional x86 benchmark compiling with some different flags and managed to eek out a bit more performance, but still `TRANSPOSE_CONV` operator takes up the vast majority of compute time:\r\n```\r\nbazel build -c opt \\\r\n  --config=mkl --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model_plus_flex\r\n```\r\n```\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t          TRANSPOSE_CONV\t          145.739\t  334.845\t  327.682\t 30.711%\t 30.711%\t     0.000\t        1\t[device_0/g_ae/dec_0/conv2d_transpose1]:102\r\n\t          TRANSPOSE_CONV\t          474.519\t   98.657\t   98.284\t  9.211%\t 39.922%\t     0.000\t        1\t[device_0/g_ae/dec_1/conv2d_transpose1]:113\r\n\t          TRANSPOSE_CONV\t          573.890\t   68.139\t   67.579\t  6.334%\t 46.255%\t     0.000\t        1\t[device_0/g_ae/dec_2/conv2d_transpose1]:124\r\n\t          TRANSPOSE_CONV\t          975.095\t   56.976\t   57.059\t  5.348%\t 51.603%\t     0.000\t        1\t[device_0/g_ae/dec_9/conv2d_transpose1]:201\r\n\t          TRANSPOSE_CONV\t          902.487\t   52.394\t   53.175\t  4.984%\t 56.587%\t     0.000\t        1\t[device_0/g_ae/dec_8/conv2d_transpose1]:190\r\n\t          TRANSPOSE_CONV\t          643.654\t   51.886\t   51.722\t  4.847%\t 61.434%\t     0.000\t        1\t[device_0/g_ae/dec_3/conv2d_transpose1]:135\r\n\t          TRANSPOSE_CONV\t          848.040\t   44.327\t   45.076\t  4.225%\t 65.659%\t     0.000\t        1\t[device_0/g_ae/dec_7/conv2d_transpose1]:179\r\n\t          TRANSPOSE_CONV\t          794.084\t   43.691\t   44.680\t  4.187%\t 69.846%\t     0.000\t        1\t[device_0/g_ae/dec_6/conv2d_transpose1]:168\r\n\t          TRANSPOSE_CONV\t          697.544\t   47.731\t   44.555\t  4.176%\t 74.022%\t     0.000\t        1\t[device_0/g_ae/dec_4/conv2d_transpose1]:146\r\n\t          TRANSPOSE_CONV\t          746.798\t   42.475\t   42.731\t  4.005%\t 78.027%\t     0.000\t        1\t[device_0/g_ae/dec_5/conv2d_transpose1]:157\r\n\r\nNumber of nodes executed: 216\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t          TRANSPOSE_CONV\t       11\t   847.637\t    79.448%\t    79.448%\t     0.000\t       11\r\n\t                     ABS\t       21\t   106.461\t     9.978%\t    89.426%\t     0.000\t       21\r\n\t                 CONV_2D\t       11\t    79.186\t     7.422%\t    96.848%\t     0.000\t       11\r\n\t                     ADD\t       32\t     6.867\t     0.644%\t    97.492%\t     0.000\t       32\r\n\t                 RESHAPE\t       44\t     6.518\t     0.611%\t    98.103%\t     0.000\t       44\r\n\t                     MUL\t       42\t     6.445\t     0.604%\t    98.707%\t     0.000\t       42\r\n\t                     SUB\t       21\t     5.671\t     0.532%\t    99.239%\t     0.000\t       21\r\n\t                    RELU\t       21\t     4.125\t     0.387%\t    99.625%\t     0.000\t       21\r\n\t           CONCATENATION\t       11\t     3.426\t     0.321%\t    99.946%\t     0.000\t       11\r\n\t      TfLiteFlexDelegate\t        1\t     0.461\t     0.043%\t    99.990%\t     0.000\t        1\r\n\t                    TANH\t        1\t     0.111\t     0.010%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=1067538 curr=1117788 min=1041534 max=1142071 avg=1.067e+06 std=24935\r\n```\r\n[float32_mavx_msse4.1_msse4.2_benchmark.txt](https://github.com/tensorflow/tensorflow/files/5211910/float32_mavx_msse4.1_msse4.2_benchmark.txt)\r\n", "Hi,\r\n\r\nUnfortunately I don't have a full answer but I can provide some info. First, can you say what the run time is for TF vs. TFL on x86 with this model built/run in a standard way (basically just `-c opt` for the build line)? If TFL is 2-3x slower than TF, it is likely something bad is happening. If TFL is on the order of 10-20% slower than TF for this float model, it might just mean that deeper work is required on the TFL side to optimize transpose conv (so no easy fix available). For certain kinds of TF Ops, or TF Ops with certain options (like certain stride values) the transformation from TF -> TFL can result in a correct but unoptimized graph. So, the main thing to say is that there might be no magic bullet on the performance side.\r\n\r\nRegarding the runs with Ruy, I am able to see something that is going wrong. When you add the `--define=ruy_profiler=true` the resulting output is really helpful to figure out performance issues. Please use it by default as we keep debugging this issue. In this case, the output:\r\n\r\n```\r\n       * 12.38% TrMul (Path=0x1, max_num_threads=4, is_prepacked=(0,0))\r\n          * 12.38% TrMulImpl, general case\r\n            * 11.42% Kernel (Standard Cpp)\r\n```\r\n\r\nshows that you are getting the \"standard Cpp\" path, which is unoptimized naive code only used for debugging purposes or if no optimized code path could be found. On x86, Ruy has optimized kernels for AVX, AVX2, and AVX512. From the CPU you indicated, it appears that you do have AVX2 on your machine, at least according to [this](https://ark.intel.com/content/www/us/en/ark/products/148263/intel-core-i7-8086k-processor-12m-cache-up-to-5-00-ghz.html)\r\n\r\nRuy should autodetect the platform and choose the most performant path (AVX2 in this case), but for some reason it is not doing that. You can manually set the path with the env variable `RUY_PATHS`, so as an experiment you can do:\r\n\r\n```\r\nRUY_PATHS=0x20  <rest of  execution line here>\r\n```\r\n\r\nAlthough if you hit an illegal instruction for your path the code will crash. Please give it a try and let me know the result.", "It is worth pointing out that this model is quite large (peak memory footprint > 800 MB) and 1.8 seconds inference time on ARM when actually getting optimized code. ~90% of the execution time is in GEMM in ARM case, so I think if we are able to get the AVX2 path going on your machine, that's probably the low hanging fruit here. "]}, {"number": 42940, "title": "Converter spews out alphanumeric symbols", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n- Python 3.8.5\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom transformers import DistilBertConfig, TFDistilBertForSequenceClassification\r\n\r\nmodel_name = 'distilbert-base-uncased'\r\nconfig = DistilBertConfig(num_labels=2)\r\nmodel = TFDistilBertForSequenceClassification.from_pretrained(model_name, config=config)\r\nmodel.load_weights('tf_model.h5')\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nwith tf.io.gfile.GFile('distilbert_sst2.tflite', 'wb') as f:\r\n        f.write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\nThere are 3 sets of debug messages/errors. For the third, I'm not sure if they are error or debug messages but a tsunami of alphanumeric symbols is outputted to the terminal.\r\n\r\n1.\r\n```\r\nloc(callsite(\"tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._0/ffn/activation_10/Erf\"(\"/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\":742:0) at callsite(\"/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\":3295:0 at callsite(\"/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/transformers/modeling_tf_distilbert.py\":70:0 at callsite(\"/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\":427:0 at callsite(\"/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\":985:0 at callsite(\"/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/transformers/modeling_tf_distilbert.py\":297:0 at callsite(\"/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\":302:0 at callsite(\"/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\":985:0 at callsite(\"/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/transformers/modeling_tf_distilbert.py\":347:0 at \"/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\":302:0)))))))))): error: 'tf.Erf' op is neither a custom op nor a flex op\r\n```\r\n2.\r\n```\r\nerror: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Erf {device = \"\"}\r\n```\r\n3.\r\n```\r\nFF04BD305693BC0F148BBCC1E52DBC089D96BDE60BE2BC4919B5BCE816F8BC0FB82FBD1A333BBDFFB50BBCB3349...\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nhttps://drive.google.com/drive/folders/1D5cNxuya7uxFvF7DhmSs1pap-8ATbY08?usp=sharing\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Even after 5+ minutes of alphanumeric symbols showing up on the terminal, the `tflite` file is not generated.\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@taylorshin \r\nI ran the code shared, it is incomplete. Please share complete stand alone code if possible or share a  colab gist with the error faced.\r\nWith respect tot he error \"Ops that can be supported by the flex runtime\" can you refer tot his issue and let us know #42184 ", "I updated my code. The `in_file` should have been `tf_model.h5`, which is the weights file included in the Google Drive link. I also added the import statements so the code should be able to run now.", "The erf operator currently are not supported by TFLite builtin operator set. You need to enable Select TF op feature to convert your model. Please refer to the following documents:\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\nhttps://www.tensorflow.org/lite/guide/reduce_binary_size", "@taylorshin \r\nPlease update as per above comment, and feel free to move the issue to closed status if resolved.", "Enabling the Select TF op feature resulted in the model converting without spewing alphanumeric symbols. However, the converted TFLite model has a fixed input size of `3 x 5`. Could that have been set by the TFLite conversion process?\r\n\r\n![Screen Shot 2020-09-04 at 3 00 00 PM](https://user-images.githubusercontent.com/10328411/92506626-925cfc80-f1ba-11ea-81fe-33edb371c9a2.png)\r\n", "TFLite converter uses the same shape information from the original TF model.", "So I used a library called `ktrain` to fine-tune DistilBert on SST2 and got a weights h5 file which is included in the Google Drive link in my original post. All I do to convert that into a TFLite model is load a pretrained `TFDistilBertForSequenceClassification` from HuggingFace and load the weights onto the model. According to the author of the library, `ktrain` is most likely not the reason for the fixed input size because all it does is set the output shape, metrics, and loss functions of existing HuggingFace models. I have a feeling it has to do something with HuggingFace because the graph for the `TFDistilBertModel` shows that the input is of type `int32[?,5]`.", "You can override the input shape with V1 converter API. Please take a look at the input_shapes argument in the following document.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#from_keras_model_file", "@taylorshin It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Please have a look at this[ link](https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#from_keras_model_file) and let us know if it helps ?Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42940\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42940\">No</a>\n"]}, {"number": 42939, "title": "Keras inconsistent behavior when using model.predict for accessing intermediate layers output with spektral GCN", "body": "**Describe the current behavior**\r\nI'm trying to access output of intermediate layers of Graph Convolutional Networks (GCN) and model.predict is throwing InvalidArgument Error for input value where as model.fit is working fine with the same input.\r\n\r\nI have copied my code below and it using 'CORA' citation dataset from OGB provided by spektral library (https://graphneural.network/) that provide algorithms and examples for Graph Convolutional network. My code is based on one of the example from the same library, here (https://graphneural.network/getting-started/)\r\n\r\nI am using Functional API and I have also verified that the same logic works if I apply it on cases other than Graph Convolutional network. \r\n\r\n**Describe the expected behavior**\r\nThe error message is related to mismatch of inner dimensions for multiplication. I tried to use the transponse for input like model_input = [X, At] to fix the issue but still face the same error.\r\n\r\nSince, the model gets trailed and evaluated without problems on the input i.e. [X,A], it is expected that model.predict should also work consistently. \r\n\r\n**Standalone code to reproduce the issue**\r\n`from spektral.datasets import citation\r\nfrom spektral.layers import GraphConv\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Dropout, Dense\r\nimport numpy as np\r\n\r\nA, X, y, train_mask, val_mask, test_mask = citation.load_data('cora')\r\n\r\nAt = A.transpose()\r\n\r\nN = A.shape[0]\r\nF = X.shape[-1]\r\nn_classes = y.shape[-1]\r\n\r\nX_in = Input(shape=(F, ))\r\nA_in = Input((N, ), sparse=True)\r\nX_1 = GraphConv(16, 'relu', name=\"layer1\")([X_in, A_in])\r\nX_1 = Dropout(0.5, name=\"layer2\")(X_1)\r\nX_2 = GraphConv(n_classes, 'softmax', name=\"output\")([X_1, A_in])\r\nmodel = Model(inputs=[X_in, A_in], outputs=X_2)\r\n\r\nA = GraphConv.preprocess(A).astype('f4')\r\nAt = GraphConv.preprocess(At).astype('f4')\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='categorical_crossentropy',\r\n              weighted_metrics=['acc'])\r\nmodel.summary()\r\n\r\n# Prepare data\r\nX = X.toarray()\r\nA = A.astype('f4')\r\nAt = At.astype('f4')\r\nvalidation_data = ([X, A], y, val_mask)\r\n\r\n# Train model\r\nmodel.fit([X, A], \r\n          y,\r\n          sample_weight=train_mask,\r\n          validation_data=validation_data,\r\n          epochs=1,\r\n          batch_size=N,\r\n          shuffle=False\r\n)\r\n\r\n# Access intemediate layers of model\r\nlayer_name = 'layer2'\r\nintermediate_layer_model = Model(inputs=model.input,\r\n                                 outputs=model.get_layer(layer_name).output)\r\n\r\nmodel_input = [X,A]\r\nintermediate_output = intermediate_layer_model.predict(model_input)\r\nprint(\"\\n\\nIntermediate_output=\",intermediate_output,\"\\n\\n\")`\r\n\r\n**Other info / logs** \r\n`Traceback (most recent call last):\r\n  File \"PLGcn_example4_stackflow_debug.py\", line 53, in <module>\r\n    intermediate_output = intermediate_layer_model.predict(model_input)\r\n  File \"/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 130, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1599, in predict\r\n    tmp_batch_outputs = predict_function(iterator)\r\n  File \"/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 846, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Cannot multiply A and B because inner dimension does not match: 2708 vs. 32.  Did you forget a transpose?  Dimensions of A: [32, 2708).  Dimensions of B: [32,16]\r\n         [[node functional_3/layer1/SparseTensorDenseMatMul/SparseTensorDenseMatMul (defined at /home/mansoor4/.local/lib/python3.7/site-packages/spektral/layers/ops/matmul.py:33) ]] [Op:__inference_predict_function_22928]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node functional_3/layer1/SparseTensorDenseMatMul/SparseTensorDenseMatMul:\r\n stack (defined at PLGcn_example4_stackflow_debug.py:53)\r\n functional_3/layer1/MatMul (defined at /home/mansoor4/.local/lib/python3.7/site-packages/spektral/layers/ops/matmul.py:45)\r\n\r\nFunction call stack:\r\npredict_function`\r\n== check pips ===================================================\r\nnumpy                  1.18.5\r\nprotobuf               3.13.0\r\ntensorflow             2.3.0\r\ntensorflow-estimator   2.3.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.3.0\r\ntf.version.GIT_VERSION = v2.3.0-rc2-23-gb36436b087\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\n\r\n== check python ===================================================\r\npython version: 3.7.5\r\npython branch:\r\npython build version: ('default', 'Dec 16 2019 14:09:08')\r\npython compiler version: GCC 6.3.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #1 SMP Tue Mar 17 23:49:17 UTC 2020\r\nos release version: 3.10.0-1062.18.1.el7.x86_64\r\nos platform: Linux-3.10.0-1062.18.1.el7.x86_64-x86_64-with-centos-7.7.1908-Core\r\nlinux distribution: ('CentOS Linux', '7.7.1908', 'Core')\r\nlinux os distribution: ('centos', '7.7.1908', 'Core')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='login1.cluster', release='3.10.0-1062.18.1.el7.x86_64', version='#1 SMP Tue Mar 17 23:49:17 UTC 2020', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 6.3.0\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /vol/apps/gcc/gcc-6.3.0/lib/:/vol/apps/gcc/gcc-6.3.0/lib64:/vol/apps/gcc/gcc-6.3.0/lib/gcc/x86_64-redhat-linux/6.3.0\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.3.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/mansoor4/.local/lib/python3.7/site-packages\r\nRequired-by: spektral\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 5, 'final', 0)\r\n\r\n\r\n", "comments": ["I realized that there was a problem in my logic that caused the issue and it is not Keras issue.  Daniele Grattarola pointed out on stackoverflow that it could be fixed by adding Batchsize=N in the model.predict call. \r\n\r\n`intermediate_output = intermediate_layer_model.predict(model_input, batch_size=N)`\r\n\r\nSorry for the inconvenience. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42939\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42939\">No</a>\n"]}, {"number": 42938, "title": "fix the pylint hang issue while performing sanity checks", "body": "This PR addresses the issue: https://github.com/tensorflow/tensorflow/issues/41600 by updating the following:\r\n- pylint version update for python3 while performing sanity checks resolves the issue where pylint hangs indefinitely during step 2 of the checklist in `tensorflow/tools/ci_build/ci_sanity.sh`.\r\n- Provides a verbose message if pylint is not installed, so that we fail the step immediately.\r\n-  Name the docker container in `tensorflow/tools/ci_build/ci_build.sh` for clarity purposes.", "comments": ["@angerson since this instruction was present in the contributing docs to run the sanity checks using docker, I thought of fixing it. Let me know, thanks.\r\n\r\nRef: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#running-sanity-check\r\n", "Ah, thank you for the pointer. This will definitely help until we get a chance to review those docs."]}, {"number": 42937, "title": "Tflite ops: BatchMatMul, TensorListReserve, TensorListSetItem, TensorListStack", "body": "**System information**\r\n- Linux Ubuntu 18.04\r\n- TensorFlow installed from source \r\n- TensorFlow version 2.1.0\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, COS, DIV, EXPAND_DIMS, FULLY_CONNECTED, MAX_POOL_2D, MEAN, MUL, NEG, PACK, PAD, RESHAPE, SIN, SQRT, SQUEEZE, STRIDED_SLICE, SUB, SUM. Here is a list of operators for which you will need custom implementations: BatchMatMul, TensorListReserve, TensorListSetItem, TensorListStack.\r\n```\r\n\r\nI try to convert generator model from here: https://github.com/russoale/hmr2.0/blob/master/src/main/generator.py\r\n\r\nCan you please give me an estimate when such operations as \r\nBatchMatMul, TensorListReserve, TensorListSetItem, TensorListStack will be transferred to tflite?\r\n\r\nLooking forward to hearing from you!\r\n\r\n", "comments": ["TensorListReserve, TensorListSetItem, TensorListStack ops will be disappeared when you have a fixed size in your model, for example, batch size.\r\n\r\nBatchMatMul can be supported via Select TF ops and we offer a selective build mechanism for Select TF ops for slim binary size deployment.\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\nhttps://www.tensorflow.org/lite/guide/reduce_binary_size", "Thanks a lot for your help Jae!\r\n\r\nI have set up the input shape for my model via the attached code, nevertheless, I still get the error that TensorList operations are used. Can you please explain the proper way to set a fixed size in the model.\r\n\r\n```\r\nimport sys\r\nsys.path.append(\"hmr2.0/src/\")\r\n\r\nimport tensorflow as tf\r\n\r\nfrom main.model import Model\r\nfrom notebooks.vis_util import preprocess_image\r\nfrom main.config import Config\r\n\r\n\r\ndef main():\r\n    model = Model()\r\n    config = Config()\r\n    generator = model.generator   \r\n    \r\n    original_img, input_img, params = preprocess_image('hmr2.0/src/notebooks/images/coco4.png', \\\r\n                                                        config.ENCODER_INPUT_SHAPE[0])\r\n    \r\n    if len(tf.shape(input_img)) is not 4:\r\n        # [224, 224, 3] -> [1, 224, 224, 3]\r\n        input_img = tf.expand_dims(input_img, 0) \r\n    generator._set_inputs(input_img)\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(generator)\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n        tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n    ]\r\n    tflite_model = converter.convert()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "It happens when I try to transfer the following model:\r\n\r\n```\r\nclass Regressor(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Regressor, self).__init__(name='regressor')\r\n        self.config = Config()\r\n\r\n        self.mean_theta = tf.Variable(model_util.load_mean_theta(), name='mean_theta', trainable=True)\r\n\r\n        self.fc_one = layers.Dense(1024, name='fc_0')\r\n        self.dropout_one = layers.Dropout(0.5)\r\n        self.fc_two = layers.Dense(1024, name='fc_1')\r\n        self.dropout_two = layers.Dropout(0.5)\r\n        variance_scaling = tf.initializers.VarianceScaling(.01, mode='fan_avg', distribution='uniform')\r\n        self.fc_out = layers.Dense(85, kernel_initializer=variance_scaling, name='fc_out')\r\n\r\n    def call(self, inputs, **kwargs):\r\n        batch_size = inputs.shape[0] or self.config.BATCH_SIZE\r\n        shape = (batch_size, 2048)\r\n        assert inputs.shape[1:] == shape[1:], 'shape mismatch: should be {} but is {}'.format(shape, inputs.shape)\r\n\r\n        batch_theta = tf.tile(self.mean_theta, [batch_size, 1])\r\n        thetas = tf.TensorArray(tf.float32, self.config.ITERATIONS)\r\n        for i in range(self.config.ITERATIONS):\r\n            # [batch x 2133] <- [batch x 2048] + [batch x 85]\r\n            total_inputs = tf.concat([inputs, batch_theta], axis=1)\r\n            batch_theta = batch_theta + self._fc_blocks(total_inputs, **kwargs)\r\n            thetas = thetas.write(i, batch_theta)\r\n\r\n        return thetas.stack()\r\n\r\n    def _fc_blocks(self, inputs, **kwargs):\r\n        x = self.fc_one(inputs, **kwargs)\r\n        x = tf.nn.relu(x)\r\n        x = self.dropout_one(x, **kwargs)\r\n        x = self.fc_two(x, **kwargs)\r\n        x = tf.nn.relu(x)\r\n        x = self.dropout_two(x, **kwargs)\r\n        x = self.fc_out(x, **kwargs)\r\n        return x\r\n\r\n```", "@whiteRa2bit,\r\nOn running the code with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/12f6dcb30348344a2f386cc84303adc1/42937-2-1.ipynb), I am facing an error stating `Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow.`.\r\n\r\nHowever with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/bc2bbf611b03c4bfbddc09d2bc215d9b/42937.ipynb#scrollTo=KMKC7J3mtevN) and TF-nightly, the error changes to `error: requires element_shape to be 1D tensor during TF Lite transformation pass`. Please find the attached gist. Thanks!", "@amahendrakar Thanks for your reply! \r\nYes, I got the same error when I run my model using tf 2.3, I was rather wondering whether it is possible to fix the current error in tf2.1 the way @abattery mentioned", "As I found it [here](https://github.com/tensorflow/tensorflow/issues/37134) the reason of this error is the same: all shapes have to be fixed, but I have already set up the input shapes.", "@amahendrakar looking forward to hear from you! Thanks in advance!", "@whiteRa2bit have you set a fixed batch size as well? Usually keras models have non-fixed size if you have not set it explicitly.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42936, "title": "Tensorflow GPU Support Installation instructions do not work due to missing cuBLAS", "body": "### System information\r\n\r\n-   **OS**: Linux Ubuntu 16.04\r\n-   **TensorFlow installed from (source or binary)**: PyPI binary\r\n-   **TensorFlow version**: 2.3.0\r\n-   **Python version**: 3.7.7\r\n-   **CUDA/cuDNN version**: CUDA 10.1 _installed as suggested using_ `apt-get --no-install-recommends cuda-10-1`, cuDNN 7.6\r\n-   **GPU model and memory**: GeForce GTX 1080 Ti 10.92GiB\r\n-   **Exact command to reproduce**: `tf.config.experimental.list_physical_devices('GPU')`\r\n\r\n### Describe the problem\r\nInstalling/upgrading CUDA using verbatim instructions for Ubuntu 16.04 from [TF Install GPU Support](https://www.tensorflow.org/install/gpu) results in CUDA _without_ cuBLAS, which does not work for TensorFlow.\r\n\r\nIt appears there is an issue with the `cuda-10-1` package on APT, because installing from the [Nvidia-provided CUDA 10.1 update 2 runfile](https://developer.nvidia.com/cuda-10.1-download-archive-update2) resulted in the expected cuBLAS files. My workaround was to sloppily extract the raw cuda-toolkit lib directory from the aforementioned runfile and copy the libcublas files into `/usr/local/cuda-10.1/targets/x86_64-linux/lib`. I didn't have to mess with any `include` headers or `/usr/lib` symlinks to get Tensorflow to work. But that's clearly not a good solution, and that shouldn't be the only way to get the \"tested\" configuration to work using the install instructions from TensorFlow's own website.\r\n\r\n**To reproduce:**\r\nOn an Ubuntu 16.04 machine, uninstall all CUDA drivers and toolkits. Install CUDA using the exact commands listed under _Install CUDA with apt_ on the above page. Install tensorflow in a new virtualenv using pip: `python3 -m pip install tensorflow`. Any GPU code will produce the following error.\r\n\r\n### Source code / logs\r\n`Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory`\r\n", "comments": ["@cdjameson \r\nCan you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/26182#issuecomment-684993950) on same error and let us know if it helps.\r\nsimilar issues to refer to:\r\n#34759 [link](https://stackoverflow.com/questions/55224016/importerror-libcublas-so-10-0-cannot-open-shared-object-file-no-such-file-or)", "@Saduf2019 Thanks for your quick response! This certainly is the correct solution to the problem, _and should be documented as such_. I hope to see this fixed in the [installation documentation](https://www.tensorflow.org/install/gpu).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42936\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42936\">No</a>\n", "**OS**: Linux Ubuntu 18.04\r\n**TensorFlow installed from (source or binary)**: PyPI binary\r\n**TensorFlow version**: 2.1.0\r\n**Python version**: 3.6.12\r\n**CUDA/cuDNN version**: CUDA 10.1 installed as suggested using apt-get --no-install-recommends cuda-10-1, cuDNN 7.6\r\n**GPU model and memory**: GeForce GTX 980 Ti 6075MiB\r\n\r\nDear Tensorflow developers\r\n\r\nI found @cdjameson 's thread here https://forums.developer.nvidia.com/t/cublas-for-10-1-is-missing/71015/19 because I am encountering the same problem today October 17th. I had to follow the solution he and fellow user phillip3m commented on the forum. I just wanted to post a reminder for you to please update the documentation. Thanks!\r\n\r\n"]}, {"number": 42935, "title": "Fix extraneous depecation message caused by setdiff1d", "body": "This PR tries to address the issue raised in #42909 where extraneous\r\ndeprecation messages showed up when with the following:\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.ones(5)\r\nwith tf.GradientTape() as g:\r\n    g.watch(x)\r\n    y = tf.math.reduce_prod(x)\r\n\r\ngrad = g.gradient(y, x)\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:300: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\r\nInstructions for updating:\r\nThis op will be removed after the deprecation date. Please switch to tf.sets.difference().\r\n```\r\n\r\nThe deprecation is misleading to users.\r\n\r\nThis PR switch to use internal `gen_array_ops.list_diff` so that\r\ndeprecation messages will not be triggered.\r\n\r\nSeveral other places using setdiff1d have also been fixed.\r\n\r\nThis PR fixes #42909.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 42933, "title": "Add github handles for maintainer for micro/kernels/arc_mli to the README", "body": "@tensorflow/micro\r\n\r\nTagging @JaccovG and @dzakhar since I'm requesting info from them.", "comments": ["[This is an example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/kernels/vexriscv) of a micro/kernels readme with maintainers listed on them."]}, {"number": 42932, "title": "More details on the reduce_codesize tag used for arc", "body": "@tensorflow/micro\r\n\r\nThis came up during the review for PR #42020 in [this comment](https://github.com/tensorflow/tensorflow/pull/42020#discussion_r483160984)\r\n\r\nThe TFLM team would like to understand better what this `reduce_codesize` tag is doing in `micro/examples/micro_speech/arc_emsdp/Makefile`\r\n\r\nTopics of discussion:\r\n\r\n * In the current design, TAGS was mostly meant as a way to allow for multiple optimized kernel implementations. While not enforced, the expectation is that each tag has a corresponding directory in micro/kernels/\r\n\r\n * We are planning on making some changes in the interest of being able to register different kernel variants that might be useful for this instead of what appears to be a find and replace.\r\n", "comments": ["Tagging @JaccovG and @dzakhar since this issue is requesting more info.\r\n", "Hi,\r\nAs described, in the makefile, this tag does sources modifications to use a specific version of kernel in the exact \u201cfind and replace\u201d way.  It gives a notable reductione in total size of application by removing not-used versions of optimized kernels, which is important for us. Same mechanism is used in `micro/examples/person_detection_experimental/arc_emsdp/Makefile` \r\n\r\nWe considered several options to deal with it including an application specific version of the library, or moving part of library available for application build. However, since we are planning some related changes to our library in the future, we concluded that this current solution would be acceptable in the interim as something which could be done quickly\r\n\r\nWe are looking forward to migrating to your proposed kernels registration methodology when it will be available. Can you give more information/reference about it so we can assess how it can fits to our needs? \r\n", "#43682 is a first implementation of the changes that allow kernels to support registration of a subset of functionality from the application code via the MicroMutableOpResolver.\r\n\r\nLet me know if this is something that you guys can leverage.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42932\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42932\">No</a>\n"]}, {"number": 42931, "title": "Multiprocessing TypeError: can't pickle _thread.RLock objects, using tf.keras.model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise, cygwin64 2.905\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: V10.1.105\r\n- GPU model and memory: NVIDIA GeForce RTX 2070 Super 4.0 GB\r\n\r\n**Describe the current behavior**\r\nWhen I run the standalone code (see below) that loads a tf.keras.model from a .h5 file, see attached and tries to use the model in a python multiprocessing pool I receive the following error\r\n```Traceback (most recent call last):\r\n  File \"simple.py\", line 18, in <module>\r\n    pool.starmap(load_model,[[m,x],[m,x],[m,x]])\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 276, in starmap\r\n    return self._map_async(func, iterable, starmapstar, chunksize).get()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 657, in get\r\n    raise self._value\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 431, in _handle_tasks\r\n    put(task)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\connection.py\", line 206, in send\r\n    self._send_bytes(_ForkingPickler.dumps(obj))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\nTypeError: can't pickle _thread.RLock objects\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nI expect the code to provide the output of model.predict() using the loaded model using 2 CPUs\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport multiprocessing\r\nimport numpy as np\r\n\r\ndef load_model(model,pt):\r\n    return model.predict(pt)\r\n\r\nif __name__=='__main__':\r\n    x = np.zeros(6)\r\n    m = tf.keras.models.load_model('test.h5')\r\n\r\n    with multiprocessing.Pool(2) as pool:\r\n        pool.starmap(load_model,[[m,x],[m,x],[m,x]])\r\n```\r\n[test.zip](https://github.com/tensorflow/tensorflow/files/5170124/test.zip)\r\n\r\nNote: This happens if only the CPU is used as well by adding\r\n```\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\r\n```\r\n\r\nThere are a number of github posts referencing this error, but I have not found any that came to a helpful conclusion.\r\n", "comments": ["I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200904`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/08ad44fe425558fc8f2ac965bbb0cc3e/untitled309.ipynb). Thanks!", "Hi, has there been any progress on this?", "I am also facing the same issue @scawsome \r\n\r\nI did a workaround for this. The workaround is - Instead of passing the model in pool.starmap, I passed the model path, and it worked.\r\n\r\ntest_def.py\r\n```\r\nimport tensorflow as tf\r\n\r\ndef load_model(model_path,pt):    \r\n    model=tf.keras.models.load_model(model_path)    \r\n    return model.predict(pt)\r\n```\r\n\r\nTensorflow_Multiprocessing.py\r\n```\r\n\r\nfrom test_def import load_model\r\nimport multiprocessing\r\nimport numpy as np\r\n\r\nif __name__=='__main__':  \r\n    x = np.zeros(6)\r\n    m='test.h5'\r\n    with multiprocessing.Pool(2) as pool:\r\n        pool.starmap(load_model,[[m,x],[m,x],[m,x]])\r\n```\r\n\r\nHope this helps!", "Very helpful, thanks!", "Closing the issue as it has been resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42931\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42931\">No</a>\n", "I am facing the same issue. However, I want to fetch the model from a cloud bucket. The fetching takes some time. Therefore, I would prefer to do it only once. Do I understand it correctly that if I want to use this solution, I would need to save the model locally and then load the locally stored model for every process?", "> I am facing the same issue. However, I want to fetch the model from a cloud bucket. The fetching takes some time. Therefore, I would prefer to do it only once. Do I understand it correctly that if I want to use this solution, I would need to save the model locally and then load the locally stored model for every process?\r\n\r\nYes, your understanding is correct.", "Wont it be a performance issue as it involves a lot of disk reads, if we were to load model for each processes rather than loading only once?"]}, {"number": 42930, "title": "TF 1.15+ mutating sys.modules", "body": "**System information**\r\n- No custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.15.2-30-g4386a66 1.15.3\r\n- Python version: 3.6.12\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nCan't iterate though sys.modules.values after importing tensorflow. Raises `RuntimeError` of `dictionary changed size during iteration`\r\n\r\n**Describe the expected behavior**\r\nShould not raise error\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport sys\r\nfor v in sys.modules.values():\r\n    print(v)\r\n```\r\nraises no error\r\n```python\r\nimport sys\r\nimport tensorflow as tf\r\nfor v in sys.modules.values():\r\n    print(v)\r\n```\r\nraises `RuntimeError: dictionary changed size during iteration`\r\n\r\nCalling `list` on `sys.modules.values()` makes the problem go away, but sadly is not a viable solution. Lots of code currently depends on iterating through `sys.modules.values`, e.g. `unittest.TestCase.assertWarns`\r\n\r\nConfirmed this behavior on tf1.15.3, tf2.0. \r\nConfirmed this behavior is not present in tf1.14 or 1.13\r\n\r\n", "comments": ["@mikeyshulman,\r\nAs per [this comment](https://github.com/tensorflow/tensorflow/issues/33183#issuecomment-543931248) from a similar issue and [this](https://stackoverflow.com/a/33679686) StackOverflow answer, I was able to run the code by making a copy of the dictionary. \r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f7d4043416e85b95090b169b71aa03c2/42930.ipynb). Thanks!", "Thanks @amahendrakar! That would definitely fix the issue for the snippet I pasted into the issue. The problem is there lots of existing code from third parties libraries do exactly this behavior. Notably, `unittest.TestCase.assertWarns`.\r\n\r\nIs this known/desired behavior in tensorflow? Is it known why simply importing tensorflow causes this issue?", "Just for the sake of being concrete: Any existing code that uses tensorflow, right now won't be able to adequately test their code. Consider:\r\n\r\n```python\r\nimport unittest\r\n\r\nfrom my_module import MyCoolThing  # this imports tensorflow\r\n\r\nclass TestMyAwesomeModel(unittest.TestCase):\r\n    def test_cool_thing_warns(self):\r\n        my_cool_thing = MyCoolThing()\r\n        with self.assertWarns(UserWarning):\r\n            my_cool_thing.do_somthing_naughty()\r\n```\r\n\r\nI'd expect to raise a `UserWaring`, but this will now raise a `RuntimeException` instead because `assertWarns` accesses `sys.modules`. \r\n\r\nThis seems a bit restrictive. I think it would be tough to modify every third-party piece of code that touches `sys.modules`, and in some cases, we may actually need to access the modules themselves, and not a copy. Also, importing tensorflow broke a completely unrelated test. This behavior can be very difficult to debug. \r\n\r\nThanks in advance for the help! ", "I think this has been fixed in TF2.2 / TF2.3 with the removal of the virtual pip package.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42930\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42930\">No</a>\n"]}, {"number": 42929, "title": "libhexagon_nn_skel.so: error adding symbols", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device: Samsung Galaxy S10\r\n- TensorFlow version: 2.3\r\n- Installed using: virtualenv, pip\r\n- Bazel version: 3.1.0\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to add the Hexagon Delegate to a Native (C++) Android app in Android Studio, but build fails with error:\r\n`libhexagon_nn_skel.so: error adding symbols: File in wrong format`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- Created `libtensorflowlite_hexagon_jni.so` and `libhexagon_interface.so` from Tensorflow Docker container using: `bazel build -c opt --config=android_arm64 [HEXAGON_TARGET_PATH]` (the hexagon repo header included [this link](https://storage.googleapis.com/mirror.tensorflow.org/storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_headers_v1.20.0.0.tgz) to `v1.20.0.0.tgz` so I used `v1.20` interface)\r\n- Created `libhexagon_nn_skel.so`, `libhexagon_nn_skel_v65.so` and `libhexagon_nn_skel_v66.so` from the [v1.20](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.0.run) link\r\n- Put all shared libraries into the proper ABI folder (`arm64-v8a`) in a JNI folder in the Android Studio project\r\n- Configured `CMakeLists.txt` to include all shared libraries like so:\r\n\r\n```\r\nadd_library(tflite-hexagon-lib SHARED IMPORTED)\r\nset_target_properties(tflite-hexagon-lib PROPERTIES IMPORTED_LOCATION\r\n        ${JNI_DIR}/${ANDROID_ABI}/libtensorflowlite_hexagon_jni.so)\r\ninclude_directories(\r\n        ${JNI_LIBS_DIR}/${ANDROID_ABI}/tensorflow/lite/delegates/hexagon)\r\n\r\n# The same for libhexagon_interface.so\r\n\r\n# The same for libhexagon_nn_skel.so\r\n\r\n# The same for libhexagon_nn_skel_v65.so\r\n\r\n# The same for libhexagon_nn_skel_v66.so\r\n\r\n...\r\n\r\ntarget_link_libraries(\r\n        native-lib\r\n        ...\r\n        tflite-hexagon-lib\r\n        tflite-hexagon-interface-lib\r\n        tflite-hexagon-nn-skel-lib\r\n        tflite-hexagon-nn-skel-v65-lib\r\n        tflite-hexagon-nn-skel-v66-lib)\r\n```\r\n\r\nThe build runs OK excluding the `nn_skel*` libraries -\r\nMeaning the Tensorflow-Lite shared library itself, and the `libtensorflowlite_hexagon_jni.so` and `libhexagon_interface.so` build just fine.\r\nWhen adding the `nn_skel*` libraries and trying to build, it fails with `libhexagon_nn_skel.so: error adding symbols: File in wrong format`.\r\n\r\nI've checked that they are the same version as the `hexagon-interface` library (which included `--config=android_arm64` in the Bazel command), and they shouldn't be ABI dependent (current build is for `arm64-v8a` only).\r\n\r\n\r\n**Any other info / logs**\r\n\r\n```\r\n[4/4] Linking CXX shared library /path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so\r\nFAILED: /path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so \r\n: && /path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=aarch64-none-linux-android29 --gcc-toolchain=/path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64 --sysroot=/path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security  -std=c++11 -frtti -fexceptions -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc_real.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--fatal-warnings -Wl,--no-undefined -Qunused-arguments -shared -Wl,-soname,libnative-lib.so -o /path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libtensorflowlite.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libtensorflowlite_hexagon_jni.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libhexagon_interface.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libhexagon_nn_skel.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libhexagon_nn_skel_v65.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libhexagon_nn_skel_v66.so /path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android/29/liblog.so -latomic -lm && :\r\n/path/to/app/src/main/cpp/../jni/arm64-v8a/libhexagon_nn_skel.so: error adding symbols: File in wrong format\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\nninja: build stopped: subcommand failed.\r\n```", "comments": ["@orangesomethingorange \r\nPlease refer to [this issue](https://stackoverflow.com/questions/52760801/cmake-building-for-windows-clang-cl-using-ninja-generator) and let us know if it helps.\r\nIs this issue related to #42519 issue created by you.", "Hi @Saduf2019 ,\r\n\r\nThanks for the link :)\r\n\r\n> @orangesomethingorange\r\n> Please refer to [this issue](https://stackoverflow.com/questions/52760801/cmake-building-for-windows-clang-cl-using-ninja-generator) and let us know if it helps.\r\n\r\nThis, unfortunately, is not relevant - it concerns `cmake` on Windows machines / Visual Studio.\r\nThe setup described in this issue has Linux Ubuntu 16.04 platform, and calls `cmake` via Android Studio.\r\n\r\n> Is this issue related to #42519 issue created by you.\r\n\r\nI'm not sure - both issues did occur on the same setup, and concern issues with external libraries, but I haven't solved the former one, just \"bypassed it\".\r\nFor the Tensorflow-Lite library I wrote the code I needed myself - but I can't do that for a Qualcomm library, it's proprietary code.", "It seems like the shared object does not match the linker.\r\n\r\nJust in case I've tried [v1.14 and v1.17](https://www.tensorflow.org/lite/performance/hexagon_delegate#step_2_add_hexagon_libraries_to_your_android_app_2) but got similar results.\r\n\r\nThese libraries are not meant for the ARM processor, so I expect to get the same results for any ABI, but I tried to link them for the `armeabi-v7a` ABI, and got this error:\r\n\r\n`/path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: fatal error: /path/to/app/src/main/cpp/../jni/armeabi-v7a/libhexagon_nn_skel.so: unsupported ELF machine number 164\r\n`\r\n", "I installed the [Hexagon SDK v3.5.2 - Linux](https://developer.qualcomm.com/downloads/hexagon-sdk-v352-linux?referrer=node/6116) (from [here](https://developer.qualcomm.com/software/hexagon-dsp-sdk/tools)) and cloned the [nnlib repository](https://source.codeaurora.org/quic/hexagon_nn/nnlib) (from [CodeAurora](https://source.codeaurora.org/quic/hexagon_nn/nnlib)) to build the libraries.\r\nBuild the `v66` and `v65` Hexagon NN dynamic libraries, and I get (both cases):\r\n- `libhexagon_nn_skel.a`\r\n- `libhexagon_nn_skel.so`\r\n- `hexagon_nn_ops.h`\r\n- `ops.def`\r\n\r\n(*) Note that the `soname` for shared object in both cases is `libhexagon_nn_skel.so` (as noted in `libhexagon_nn_skel_so_link.txt`)\r\n\r\nUsing the shared object resulted in the same error:\r\n`error adding symbols: File in wrong format`\r\n\r\nBut when I add `libhexagon_nn_skel.a` instead of the shared one, it seems to build and compile OK, but I get an error when I call `ModifyGraphWithDelegate`:\r\n```\r\nW: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:750: Warning: apps_std_fopen_with_env failed with 0xd for libhexagon_nn_skel_v66.so (Permission denied)\r\nE: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror cannot open libhexagon_nn_skel_v66.so)\r\nE: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp\r\nW/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\nI/tflite: Hexagon Delegate is not supported.\r\n```\r\nSo Tensflow-Lite is looking for the `v66` shared one.\r\n\r\nPrior to calling `ModifyGraphWithDelegate` I have the following code:\r\n\r\nIn Java:\r\n`System.loadLibrary(\"tensorflowlite_hexagon_jni\");`\r\n\r\nIn C++:\r\n```\r\nTfLiteHexagonInit();\r\nTfLiteHexagonDelegateOptions hexagon_opts = {0};\r\nTfLiteDelegate * hexagon_delegate = TfLiteHexagonDelegateCreate(& hexagon_opts);\r\n```\r\n\r\nand I notice that `TfLiteHexagonDelegateCreate` always returns `NULL`.\r\n\r\n1. Is this behavior (`TfLiteHexagonDelegateCreate` returns `NULL`) expected? Is that normal?\r\n2. My HW uses the `v66`, and looks for it, but static version doesn't seem to match - can I explicitly call it from the static library?\r\n3. Where does Tensorflow-Lite call / uses the Hexagon ops?\r\n4. `soname` is always `libhexagon_nn_skel.so`, so what does Tensorflow-Lite is looking for when calling `v66`? What changes are expected for Tensorflow-Lite to address the `v66` library (and not `v65` for example)?", "Sorry for the late reply.\r\nlibhexagon_nn_skel are not ARM compiled, you should package them with your AAR. \r\nSee instructions\r\nhttps://www.tensorflow.org/lite/performance/hexagon_delegate\r\n\r\nThanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42929\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42929\">No</a>\n"]}, {"number": 42928, "title": "Added an example to Arguments in function on_epoch_end.", "body": "Described the dictionary that will be returned as metric results at the end of the training of each epoch. The example clearly shows that the accuracy and the loss at the end of each epoch can be accessed by the get method as - \r\n1. `logs.get('loss')`\r\n2. `logs.get('acc')`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42928) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42928) for more info**.\n\n<!-- ok -->"]}, {"number": 42927, "title": "tflite model compiled from h5 outputs just nan", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 2.3.1\r\n\r\nHey everybody,\r\ni am still relatively inexperienced with tensorflow lite and would like to convert a model with custom training function into a tflite model in order to run it on a google coral accelerator if possible. I am training the model on a NVIDIA Tesla V100 and would like to convert the saved model/graph (.pb, h5) into a tflite.\r\nMy model is almost identical to the one in cycleGan Tutorial. \r\nAfter the conversion into the tflite format I get only \"NAN\" as model output.\r\nI have also tried to convert the model from the tutorial into tflite and the same error occurs.\r\nI have also tried different conversion types, Tensorflow versions etc. and nothing worked.\r\nAre there possibly any functions in it which tflite does not support?\r\n\r\nThanks a lot in advance.", "comments": ["@ptrem \r\n\r\nCan you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@ravikyram\r\nThe Notebook tf_cyclehorse.ipynb is used to train the model and to create the .h5 file. \r\nTo convert the .h5 file to a .tflite file I use a script similar to conv_tflite.py. \r\n(to run the file you use the console comand similar to: python conv_tflite.py --model model.h5 --output model.tflite)\r\n tflite_test.ipynb is a simple notebook to test the return of the tflite model. \r\nThereby the image horse.jpg and the model model.tflite is read in. The output of the errors is then reproduced.\r\n\r\nFollow the google drive link:\r\nThere you find all the notebooks, scripts, models and images. Almost everything you need.", "Is there any solution for my problem?\r\n\r\nThe files are also available on github.\r\nhttps://github.com/ptrem/cycleGan_tf_tutorial", "It seemed to be a problem of the tf.keras.layers.BatchNormalization().\r\nIf this issue occurs just set the attribute fused=False and it should work fine."]}, {"number": 42926, "title": "I got error", "body": "![1](https://user-images.githubusercontent.com/7116366/92135241-22e8b500-ee28-11ea-8039-191a6b92f3ce.PNG)\r\n![2](https://user-images.githubusercontent.com/7116366/92135268-2bd98680-ee28-11ea-8c6f-de4bcf107110.PNG)\r\n I used the code and got the problem , so please suggest me the solution, please remember I am naive in tensorflow.", "comments": ["@sandipuncc,\r\nPlease fill in the issue template while submitting the issue. Looks like you are running code meant for TF 1.x on TF 2.x. In this case, you can tweak your code a bit as shown [this gist](https://colab.research.google.com/gist/amahendrakar/a2eed230ada70b1800d01b2f6c77edee/42926.ipynb).\r\n\r\nHowever TF 1.x is not actively support, I'd suggest you to migrate your code to 2.x as shown in [this guide](https://www.tensorflow.org/guide/migrate). Thanks!", "`tf.placeholder` does not exist in TF 2.x", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42925, "title": "Sports betting data using AI to predict results.  I\u2019m using the tensor flow module in python to aid in my project set by my professor to create a program to predict soccer matches, the only problem is that I\u2019m confused about how the data should be presented.  Data types e.g. possession, av shots, av goals etc. Any suggestions on how to layout this data?", "body": "", "comments": ["@jacobhamilton2004 \r\nWe see that you have not filled the issue template, in case you are facing any error in the tensorflow code please share the details.\r\n\r\nFor suggestions on coding, ask this on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there. \r\n", "Better fit for StackOverflow."]}, {"number": 42924, "title": "TensorFlow Lite Micro: Make it possible to specify optimization level when building with the Makefile", "body": "@tensorflow/micro\r\n\r\nIt would be useful to be able to manually set the optimization levels for different BUILD_TYPE:s from the make-command.\r\n\r\nMy idea is that the default optimization flags should be as it is now, none for BUILD_TYPE=debug, -O3 for BUILD_TYPE=release. If the user want to use different optimization levels, that should be configurable.\r\n\r\nMy proposed fix for this is to change tensorflow/lite/micro/tools/make/Makefile as can be seen in [makefile.txt](https://github.com/tensorflow/tensorflow/files/5168744/makefile.txt) , if it looks OK I can submit a pull request.\r\n\r\n", "comments": ["Making the optimization levels configurable from the command line sounds good to me.\r\n\r\nWe are not particular about the optimization level for the debug build so if it works for you I would suggest making it common for all builds to reduce the number of options and make it easier to switch between debug and non-debug builds with the same optimization level.\r\n\r\nSomething like\r\n```\r\nOPTIMIZATION_LEVEL := -O3\r\nCXXFLAGS := -std=c++11 -Wstrict-aliasing -DTF_LITE_STATIC_MEMORY $(CC_WARNINGS) $(OPTIMIZATION_LEVEL)\r\nCCFLAGS := -DTF_LITE_STATIC_MEMORY $(CC_WARNINGS) $(OPTIMIZATION_LEVEL)\r\n```\r\n\r\nA pull request for this would be welcome.", "A common optimization level works for us, I'll submit a pull request right away.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42924\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42924\">No</a>\n"]}, {"number": 42923, "title": "Aborted (core dumped) - tf 2.3, tf lite model converts but crashes on invoke ", "body": "Hey there! \r\nI'm trying to convert StyleGan2 to TF lite. Currently, there seems to be an issue with invoking the synthesis blocks of the generator, so I've included a small model here which is just one synthesis block, to display the issue.\r\n\r\nConverts fine in tf 2.3, with no issues. Then on attempting to invoke, I get the exit code\r\n`Process finished with exit code 134 (interrupted by signal 6: SIGABRT)`\r\nin Pycharm, and\r\n`Aborted (core dumped)`\r\nif running from the terminal. \r\n\r\nJudging from the graph in Netron, seems there are 2 Flex ops (Conv2D on 4-dim tensors in NCHW format) and everything else is built-in ops. \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source - conda\r\n- TensorFlow version (or github SHA if from source): converted with tf 2.3, tried to invoke on tf2.3 and tf-nightly \r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nHere's a colab notebook which downloads the tflite model and attempts to invoke: \r\nhttps://colab.research.google.com/drive/1g586G8gWdCrA2EQRzJ3V6PmtbnxpfQUT?usp=sharing\r\n\r\n**Link to .tflite model**\r\nhttps://drive.google.com/file/d/1yvcvTWZlgBbA6mihNJBfEiGCPVz9FqtD/view?usp=sharing\r\n\r\n**Link to SavedModel**\r\nhttps://drive.google.com/drive/folders/1JGtipSJ37p-JJSD-k4j5ZbbWgow0gHJo?usp=sharing\r\n\r\n** Log from conversion**\r\n[conversion_log.txt](https://github.com/tensorflow/tensorflow/files/5168414/conversion_log.txt)\r\n\r\n```\r\n\r\n# Here is the code used to convert the model:\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(synth_const)\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n        tflite_model = converter.convert()\r\n\r\n        with tf.io.gfile.GFile('synth_const.tflite', 'wb') as f:\r\n            f.write(tflite_model)\r\n\r\n```\r\n\r\nAttempted on both the SavedModel and the tf keras functional model. \r\n\r\n**The output from the converter invocation**\r\nWithout GPU:\r\n```\r\n2020-09-03 13:00:19.623097: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2020-09-03 13:00:20.461562: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-03 13:00:20.490338: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3199980000 Hz\r\n2020-09-03 13:00:20.490824: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c893461c80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-03 13:00:20.490859: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-03 13:00:20.495591: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-09-03 13:00:20.508318: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-09-03 13:00:20.508384: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: y4tsu-pc\r\n2020-09-03 13:00:20.508399: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: y4tsu-pc\r\n2020-09-03 13:00:20.508513: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.66.0\r\n2020-09-03 13:00:20.508567: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.66.0\r\n2020-09-03 13:00:20.508580: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.66.0\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 25 nodes with 2 partitions.\r\n\r\nProcess finished with exit code 134 (interrupted by signal 6: SIGABRT)\r\n```\r\nWith GPU:\r\n```\r\n2020-09-03 13:03:30.866202: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2020-09-03 13:03:31.687743: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-03 13:03:31.710426: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3199980000 Hz\r\n2020-09-03 13:03:31.711119: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564ecd17eb30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-03 13:03:31.711173: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-03 13:03:31.718118: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-09-03 13:03:31.873839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-03 13:03:31.886701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-03 13:03:31.887313: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564ecd21f560 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-03 13:03:31.887326: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-09-03 13:03:31.887331: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2020-09-03 13:03:31.887558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-03 13:03:31.887995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-03 13:03:31.888041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-03 13:03:31.888337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:05:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.392GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-09-03 13:03:31.888357: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-03 13:03:31.889727: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-03 13:03:31.891078: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-03 13:03:31.891344: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-03 13:03:31.892774: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-03 13:03:31.893593: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-03 13:03:31.896654: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-03 13:03:31.896761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-03 13:03:31.897299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-03 13:03:31.897737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-03 13:03:31.898261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-03 13:03:31.898550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1843] Ignoring visible gpu device (device: 1, name: GeForce GTX 1050 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1) with core count: 6. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\r\n2020-09-03 13:03:31.898559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-03 13:03:31.898586: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-03 13:03:32.301209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-03 13:03:32.301234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \r\n2020-09-03 13:03:32.301241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N \r\n2020-09-03 13:03:32.301244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N \r\n2020-09-03 13:03:32.301444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-03 13:03:32.302199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-03 13:03:32.302626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9647 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 25 nodes with 2 partitions.\r\n\r\nProcess finished with exit code 134 (interrupted by signal 6: SIGABRT)\r\n```\r\nAny idea what's going on or a possible fix for this? Should I be changing some of the ops to fit tf lite better in some way? ", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b87159411a0928f24889ceabea17e661/42923-tf-nightly.ipynb). Thanks!", "I can confirm it that this abortion is because current multiplication operator partially supports 5-D dimension inputs. Sorry for that.", "Ah I see. Thanks very much for getting back to me. It seems I'll have to try and re-wire some things to make it compatible. In that case, I just have a couple of questions:\r\n - Are there any ops which are supported in 5D? Specifically, I'm wondering about the reshape and transpose operations. \r\n - Further down in the model, I use a reshape and padding op on 6D tensors. Would this be compatible? ", "Have managed to figure this out using only 4D ops. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42923\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42923\">No</a>\n", "> I can confirm it that this abortion is because current multiplication operator partially supports 5-D dimension inputs. Sorry for that.\r\n\r\n@abattery  Hi Abattery, I wonder when Does Transpose Operator Support >=6D? \r\n", "Currently, there are no plans to support 6D transpose op. Could you file a separate feature request if you need it?"]}, {"number": 42922, "title": "Matrices do not consider sample weight ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): provided below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Docker\r\n- TensorFlow version (use command below): Below\r\n- Python version: 3.7\r\n\r\nTF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.3.0-rc2-23-gb36436b087 2.3.0\r\n\r\n\r\n**Describe the current behavior**\r\nWhen  sample weight is provided metrics do not consider that\r\n**Describe the expected behavior**\r\nWhen  sample weight is provided metrics should consider that\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata_size = 100\r\ninput_size=3\r\nclasses=2\r\n\r\nx_train = np.random.rand(data_size ,input_size)\r\ny_train= np.random.randint(0,classes,data_size )\r\nx_val = np.random.rand(data_size ,input_size)\r\ny_val= np.random.randint(0,classes,data_size )\r\n\r\n\r\ninputs = tf.keras.layers.Input(shape=(input_size))\r\npred=tf.keras.layers.Dense(1, activation='sigmoid')(inputs)\r\n\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=pred)\r\n\r\nloss = tf.keras.losses.binary_crossentropy\r\nmetrics = tf.keras.metrics.BinaryCrossentropy()\r\n\r\nmodel.compile(loss=loss , metrics=[metrics], optimizer='adam')\r\n\r\n\r\nfor layer in model.layers:\r\n    layer.trainable = False\r\n\r\n\r\nsample_weight_train = np.random.uniform(0,1,100)\r\nsample_weight_val = np.random.uniform(0,1,100)\r\nmodel.fit(x=x_train,y=y_train,sample_weight=sample_weight_train, validation_data=(x_val,y_val,sample_weight_val))\r\n29ms/step - loss: 0.3799 - binary_crossentropy: 0.7369 - val_loss: 0.3454 - val_binary_crossentropy: 0.7502\r\n\r\n\r\npred1=model.predict(x_val)\r\nlog_loss(y_val,pred1,sample_weight=sample_weight_val),log_loss(y_val,pred1),log_loss(y_val,pred1,sample_weight=sample_weight_val)*np.sum(sample_weight_val)/len(sample_weight_val)\r\n(0.7352043427636902, 0.7502077868580819, 0.34536829505647026)\r\n\r\n\r\n\r\n\r\n```\r\n\r\nIs it expected behaviour or am I missing something?", "comments": ["I needed to change compile code.\r\nmodel.compile(loss=loss , optimizer='adam',weighted_metrics=[metrics])\r\nClosing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42922\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42922\">No</a>\n"]}, {"number": 42921, "title": "Does the framework support IPv6 networks?", "body": "Is this framework suitable for IPv6 network environment?", "comments": ["@ Can you please follow the template [here](https://github.com/tensorflow/tensorflow/issues/new/choose) to create the issue. Thanks!\r\n\r\nFrom this [issue](https://github.com/tensorflow/tensorboard/issues/1713https://github.com/tensorflow/tensorboard/issues/1713), IPV6 network is supported by tensorflow.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42920, "title": "tf.audio.decode_wav error in chinese wav file", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):tf_nightly-2.3.0.dev20200515-cp36-cp36m-manylinux2010_x86_64.whl\r\n- TensorFlow version (use command below):v1.12.1-31980-g2b2e441205 2.3.0-dev20200515\r\n- Python version:python 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:CUDA 10\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nwhen I use tensorflow to decode wav in an open dataset \"mobovihotwords\"\r\n![image](https://user-images.githubusercontent.com/56855140/92093777-4f8bd500-ee06-11ea-811f-56a57de61175.png)\r\nDoes tensorflow unable to decode chinese wav file? Or another error?\r\nIt's vert strange to me.\r\n**Describe the expected behavior**\r\nAnd the wav file is ok\r\n![image](https://user-images.githubusercontent.com/56855140/92094121-af827b80-ee06-11ea-9033-bb66803233bf.png)\r\n besides I can success to decode the wav file in dataset speech commond in the same way?\r\n![image](https://user-images.githubusercontent.com/56855140/92094243-caed8680-ee06-11ea-816d-280b61a3f56b.png)\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42920\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42920\">No</a>\n"]}, {"number": 42919, "title": "\u8bf7\u95ee\u8be5\u6846\u67b6\u662f\u5426\u652f\u6301ipv6\u7684\u7f51\u7edc\uff1f", "body": "\u8bf7\u95ee\u8be5\u6846\u67b6\u662f\u5426\u9002\u7528ipv6\u7f51\u7edc\u73af\u5883\uff1f\u53ef\u4ee5\u5728ipv6\u73af\u5883\u4e0b\u542f\u52a8\u5417\r\n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/42921", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42919\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42919\">No</a>\n"]}, {"number": 42918, "title": "Non responsive model when building micro speech with cmsis-nn", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source \r\n- Tensorflow version (commit SHA if source): d3cdadd\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):  Mbed OS (STM32F746)\r\n\r\n**Describe the problem**\r\nTrying to build the micro_speech application for STM32F746 (also seen with NXP FRDM K66F) with the cmsis-nn optimized kernels, results in an application which is non-responsive to input. Each iteration of the model results in equal average scores for each category (`= [64, 64, 64, 64]`), such that no predicted commands are ever displayed. The same network response is seen when supplying feature data from `yes_micro_features_data.h` and `no_micro_features_data.h` into the model.\r\n\r\nThis behavior is only seen when compiling for release mode, when compiling with debug mode the application is responsive to input data, and seems to be somewhat able to detect the spoken words yes and no. \r\n\r\nWhen generating the application without the cmsis-nn tag, the application runs fine, both for release and debug mode.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"cmsis-nn disco_f746ng\" generate_micro_speech_mbed_project`\r\n\r\nIn location of generated project:\r\n`mbed config root .`\r\n`mbed deploy` \r\n`mbed compile -m DISCO_F746NG -t GCC_ARM --profile release --flash`\r\n", "comments": ["Looping in @jenselofsson ", "I'm able to reproduce the error, and I've also found the cause for it.\r\n\r\nIt is due to the filter_offset parameter in the CMSIS-NN version of the FullyConnected-kernel being uninitialized, which in turn leads to it being assigned an incorrect value.\r\n\r\nWhen building micro_speech_test with the debug-profile, which uses the -Og flag, it gets assigned 8.\r\nWhen building with the release-profile, which uses the -Os flag, it get assigned 537196096 (on the board I'm running it on at least)\r\nThe correct value is 0.\r\n\r\nThe micro_speech_test still passes with the debug-profile, since the error is small enough so that the correct option (\"silence\", \"unknown\", \"yes\", or \"no\") still have the highest score.\r\n\r\n@Gitariansen This PR fixes the problem with filter_offset being un-initialized, could you try that on your end?\r\n#43110 \r\n", "This change seemed to fix the problem on my end. With the changes applied, the application now runs like expected when built with release mode. Thank you, @jenselofsson, for your help in debugging this issue and providing the fix. ", "I'm facing the same issue, but in my case the difference is that I'm compiling for DISCO_F769NI and I used CMSIS as tag (as suggested in the readme). There's also no difference between debug or release mode. Could there be other steps that I'm missing? . Thanks in advance for your help.", "Today I also tried with the cmsis-nn flag instead of cmsis but the score is always [64 64 64 64]", "@lucalazzaroni Can you make sure that you have applied the changes in #43110?", "@jenselofsson yes, I applied those changes and recompiled, but apparently there's no difference. I also checked with the debugger and those values are correctly assigned. If you had any suggestion I explained my problem more in detail in a new issue (#43201)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42918\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42918\">No</a>\n"]}, {"number": 42917, "title": "tf.string_split() cannot speedup with num_parallel_calls increased in Dataset.map()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   ``Ubuntu 18.04.4 LTS``\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): ``tensorflow 1.14, tensorflow 1.15, tensorflow 2.3``\r\n- Python version:  ``3.6.9``\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\nuse the official docker image: ``tensorflow 1.14.0 cpu``, ``tensorflow 2.3.0 cpu``\r\n\r\ntesting code:\r\n```\r\nimport time, sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndef benchmark(dataset, num_epochs=1):\r\n    for epoch_num in range(num_epochs):\r\n        _iter = tf.compat.v1.data.make_initializable_iterator(dataset)\r\n        _next = _iter.get_next()\r\n        with tf.compat.v1.Session() as sess:\r\n            sess.run(_iter.initializer)\r\n            start_time = time.time()\r\n            for i in range(1000):\r\n                sess.run(_next)\r\n            seconds = time.time() - start_time\r\n    return seconds\r\n\r\ndef mapped_function(s):\r\n    return tf.compat.v1.string_split([s], sep=',').values\r\n\r\nnum_calls = int(sys.argv[1])\r\ncsv_datas = [ ','.join(['1' for i in range(8000)]) ] * 1000\r\nds = tf.data.Dataset.from_tensor_slices(np.array(csv_datas))\r\nds = ds.map(mapped_function, num_parallel_calls=num_calls).prefetch(10)\r\nprint(benchmark(ds))\r\n```\r\n\r\n**Describe the current behavior**\r\nrun this code on a machine with 56 cores, cost \r\n\r\n*  tf1.14:  ``1.748 ``seconds, which means each ``tf.string_split()`` cost around ``1.7`` ms\r\n*  tf2.3:   ``1.428 ``seconds, which means each ``tf.string_split()`` cost around ``1.4`` ms\r\n*  tf2.3(eager mode):  \r\n     * ``0.34`` seconds when num_calls=1\r\n     * ``0.17`` seconds when num_calls=2 \r\n     * ``0.17`` seconds when num_calls > 2 (cannot decreased any more)\r\n\r\nthe cost seconds not descrease, when ``num_calls`` increased. \r\n\r\nI also tried ``tf.config.threading.set_inter_op_parallelism_threads()``, not work\r\n\r\n\r\n**Describe the expected behavior**\r\ncost seconds should descreased when num_calls increased\r\n\r\nbenchmark for tf2 eager mode :\r\n```\r\ndef benchmark2(dataset, num_epochs=1):\r\n    start_time = time.time()\r\n    for epoch in range(num_epochs):\r\n        for ele in dataset:\r\n            pass\r\n    seconds = time.time() - start_time\r\n    return seconds\r\n```\r\n", "comments": ["Was able to reproduce the issue, please find the gist below\r\n\r\n- [TF v1.15](https://colab.research.google.com/gist/amahendrakar/95fe5973b2f7a13ba35aa6daeb54510e/42917-1-15.ipynb) - 1.3446729183197021s,\r\n- [TF v2.3 graph mode](https://colab.research.google.com/gist/amahendrakar/9de9ac235ea8dfa4c9e2606e1aff7b42/42917-2-3-graph.ipynb) - 1.4894318580627441s and \r\n- [TF v2.3 eager mode](https://colab.research.google.com/gist/amahendrakar/cf3ba9f43b49515ad680ba1b2b3c57eb/42917-2-3-eager.ipynb)\r\n  - when n=1 0.5180261135101318\r\n  - when n=2 0.40242767333984375\r\n  - when n=3 0.37236595153808594\r\n  - when n=4 0.3709437847137451 \r\n\r\nThanks!", "@amahendrakar I think that it is hard to declare that you can reproduce this over colab gist cause:\r\n```\r\nimport multiprocessing\r\nmultiprocessing.cpu_count()\r\n```\r\nGives 2 cores.\r\n\r\nAlso you need to recover the `map` overhead on a small function. You could try wtih vectorized mapping: \r\nhttps://www.tensorflow.org/guide/data_performance#vectorizing_mapping\r\n\r\nYou can also try to experiment with `tf.data.experimental.AUTOTUNE` instead of hardcoding many values (e.g. prefetching, num_calls etc..)", "try with ``AUTOTUNE``,  it doesn't improve anything, no change.\r\n\r\ntry with  ``batch before map``, the total time doesn't decrease. The code as below: \r\n```\r\ndef mapped_function(s):\r\n    return tf.compat.v1.string_split(s, sep=',').values\r\nnum_calls = int(sys.argv[1])\r\nif num_calls <= 0:\r\n    num_calls = tf.data.experimental.AUTOTUNE\r\nbatch_size = int(sys.argv[2])\r\nds = tf.data.Dataset.from_tensor_slices(np.array(csv_datas))\r\nds = ds.batch(batch_size).map(mapped_function, num_parallel_calls=num_calls).prefetch(10)\r\nprint(benchmark(ds))\r\n```\r\nbenchmark result as below\uff1a\r\n\r\n* bs=1:  ``total ms: 1657.7699184417725,  each ms: 1.6577699184417725, iterations=1000``\r\n* bs=2:  ``total ms: 1671.933889389038,  each ms: 3.343867778778076, iterations=500``\r\n* bs=4:  ``total ms: 1770.486831665039,  each ms: 7.081947326660156,  iterations=250``\r\n\r\nall above tests, tried on both tf1.14 and tf2.3(disable eager).\r\n\r\non tf2.3(eager), ``batch before map``, will have some improvement,  but AUTOTUNE  don't. \r\n\r\n* bs=1:  ``total ms: 189.85533714294434, each ms: 0.18985533714294434, iterations: 1000``\r\n* bs=2:  ``total ms: 177.9470443725586, each ms: 0.3558940887451172, iterations: 500``\r\n* bs=3:  ``total ms: 167.3729419708252, each ms: 0.6694917678833008, iterations: 250``\r\n* bs=4:   ``total ms: 151.60536766052246, each ms: 1.2128429412841797, iterations: 125``", "Hi @wstian \r\n\r\nI'm running the TF 2.3 code you've provided on a machine with 8 cores. Here's what I see:\r\n\r\nbatch size 3:\r\nnum_calls = 1, cost: 0.20\r\nnum_calls = 2, cost: 0.11\r\nnum_calls = 3, cost: 0.09\r\nSo the cost does go down as you increase the num_calls. Increasing the num_calls beyond 3 doesn't make a difference but I don't think that's surprising that the returns should level off at some point, especially given the small batch size.\r\n\r\nI am also curious as to why you're not seeing an improvement when using `AUTOTUNE`\r\nI ran two simple experiments:\r\n\r\nbatch size 32:\r\nnum_calls = None, cost: 0.22\r\nnum_calls = AUTOTUNE, cost: 0.12\r\n\r\nbatch size 8:\r\nnum_calls = None, cost: 0.20\r\nnum_calls = AUTOTUNE, cost: 0.10\r\n\r\nSo I'm seeing that when you use `AUTOTUNE`, it does decrease the time. \r\n\r\nPerhaps I am misunderstanding the question here. Can you run your benchmark code and compare num_calls = None and num_calls = AUTOTUNE and confirm that you do see a decrease in time?", "> Hi @wstian\r\n> \r\n> I'm running the TF 2.3 code you've provided on a machine with 8 cores. Here's what I see:\r\n> \r\n> batch size 3:\r\n> num_calls = 1, cost: 0.20\r\n> num_calls = 2, cost: 0.11\r\n> num_calls = 3, cost: 0.09\r\n> So the cost does go down as you increase the num_calls. Increasing the num_calls beyond 3 doesn't make a difference but I don't think that's surprising that the returns should level off at some point, especially given the small batch size.\r\n> \r\n> I am also curious as to why you're not seeing an improvement when using `AUTOTUNE`\r\n> I ran two simple experiments:\r\n> \r\n> batch size 32:\r\n> num_calls = None, cost: 0.22\r\n> num_calls = AUTOTUNE, cost: 0.12\r\n> \r\n> batch size 8:\r\n> num_calls = None, cost: 0.20\r\n> num_calls = AUTOTUNE, cost: 0.10\r\n> \r\n> So I'm seeing that when you use `AUTOTUNE`, it does decrease the time.\r\n> \r\n> Perhaps I am misunderstanding the question here. Can you run your benchmark code and compare num_calls = None and num_calls = AUTOTUNE and confirm that you do see a decrease in time?\r\n\r\nnum_calls = ``AUTOTUNE`` can decrease but no more than 50%, which is the question.\r\nwhy the time cannot be decreased with num_calls linearly?\r\n\r\nby the way,  as I understood, ``AUTOTUNE`` is a chosen value, which will be constant when the ``map`` work constant, so it should not be better than a manual config value.\r\n\r\nFrom the source code, ``num_calls`` is the concurrent number of map fn calls, put into the ``queue``, isn't the number of threads. So I suspect there maybe some problem with the working threads.", "In general the speedup scales with the number of threads, but there are lots of factors which can complicate things. To identify the root issue in your case, can you use the [TF Profiler](https://www.tensorflow.org/guide/profiler) to provide a profile of the dataset execution?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42916, "title": "Failed to convert SparseTensor to Tensor", "body": "I'm using tensorflow==2.3.0 and keras==2.4.3 on ubunto 20.04\r\nthe code work okay on  tensorflow==2.1.0 and keras==2.3.1\r\n\r\n```\r\nclass Args():\r\n    def __init__(self,dataset_path=\"../datasets/train\" ,mymodel=\"outputs/my_model.h5\", label=\"outputs/le.pickle\", embeddings=\"outputs/embeddings.pickle\", image_out=\"../datasets/test/img_test.jpg\", image_in=\"../datasets/test/001.jpg\", video_out=\"../datasets/videos_output/test.mp4\", video_in=\"../datasets/videos_input/Ok_Arya_Stark.mp4\", image_size='112,112', model='../models/arcface_r100_v1/model,0', ga_model='', detector='', gpu=0, det=0, flip=0, threshold=1.24):\r\n        \r\n        self.dataset=dataset_path\r\n        self.mymodel=mymodel\r\n        self.le=label\r\n        self.embeddings=embeddings\r\n        self.image_out=image_out\r\n        self.image_in=image_in\r\n        self.video_out=video_out\r\n        self.video_in=video_in\r\n        self.image_size=image_size\r\n        self.model=model\r\n        self.ga_model=ga_model\r\n        self.detector=detector\r\n        self.gpu=gpu\r\n        self.det=det\r\n        self.flip=flip\r\n        self.threshold=threshold\r\n        \r\n    def init_parsearges(self):\r\n        ap = argparse.ArgumentParser()\r\n        \r\n        # Argument of insightface\r\n        ap.add_argument(\"--dataset\", default=self.dataset,\r\n                help=\"Path to training dataset\")\r\n        \r\n        ap.add_argument(\"--mymodel\", default=self.mymodel,\r\n            help=\"Path to recognizer model\")\r\n        ap.add_argument(\"--le\", default=self.le,\r\n            help=\"Path to label encoder\")\r\n        ap.add_argument(\"--embeddings\", default=self.embeddings,\r\n            help='Path to embeddings')\r\n        ap.add_argument(\"--image-out\", default=self.image_out,\r\n            help='Path to output image')\r\n        ap.add_argument(\"--image-in\", default=self.image_in,\r\n            help='Path to output image')\r\n        ap.add_argument(\"--video-out\", default=self.video_out,\r\n            help='Path to output video')\r\n        ap.add_argument(\"--video-in\", default=self.video_in)\r\n\r\n\r\n        ap.add_argument('--image-size', default=self.image_size, help='')\r\n        ap.add_argument('--model', default=self.model, help='path to load model.')\r\n        ap.add_argument('--ga-model', default=self.ga_model, help='path to load model.')\r\n        ap.add_argument('--detector', default=self.detector, type=str, help='face detector name')\r\n        ap.add_argument('--gpu', default=self.gpu, type=int, help='gpu id')\r\n        ap.add_argument('--det', default=self.det, type=int, help='mtcnn option, 1 means using R+O, 0 means detect from begining')\r\n        ap.add_argument('--flip', default=self.flip, type=int, help='whether do lr flip aug')\r\n        ap.add_argument('--threshold', default=self.threshold, type=float, help='ver dist threshold')\r\n\r\n        args = ap.parse_args()\r\n        \r\n        return args\r\n\r\n\r\n\r\nclass SoftMax():\r\n    def __init__(self, input_shape, num_classes):\r\n        self.input_shape = input_shape\r\n        self.num_classes = num_classes\r\n\r\n    def build(self):\r\n        from keras.losses import categorical_crossentropy\r\n        from keras.models import Sequential\r\n        from keras.optimizers import Adam\r\n        from keras.layers import Dense, Dropout\r\n\r\n\r\n        # create model\r\n        model = Sequential()\r\n\r\n        # add model layers\r\n        model.add(Dense(1024, activation='relu', input_shape=self.input_shape))\r\n        model.add(Dropout(0.5))\r\n        model.add(Dense(1024, activation='relu'))\r\n        model.add(Dropout(0.5))\r\n        model.add(Dense(self.num_classes, activation='softmax'))\r\n\r\n        # loss and optimizer\r\n        optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\r\n        model.compile(loss=categorical_crossentropy,\r\n                      optimizer=optimizer,\r\n                      metrics=['accuracy'])\r\n        return model\r\n\r\n\r\ndef make_model(args, classifier=SoftMax):\r\n\r\n    # Load the face embeddings\r\n    data = pickle.loads(open(args.embeddings, \"rb\").read())\r\n\r\n    num_classes = len(np.unique(data[\"names\"])) \r\n    ct = ColumnTransformer([('my\u0654Name', OneHotEncoder(), [0])])\r\n    labels = np.array(data[\"names\"]).reshape(-1, 1)\r\n    labels = ct.fit_transform(labels)\r\n\r\n    embeddings = np.array(data[\"embeddings\"])\r\n\r\n    # Initialize Softmax training model arguments\r\n    BATCH_SIZE = 32\r\n    EPOCHS = 32\r\n    input_shape = embeddings.shape[1]\r\n\r\n    # Build classifier\r\n    init_classifier = classifier(input_shape=(input_shape,), num_classes=num_classes)\r\n    model = init_classifier.build()\r\n\r\n    # Create KFold\r\n    cv = KFold(n_splits = 5, random_state = None, shuffle=True)\r\n    history = {'acc': [], 'val_acc': [], 'loss': [], 'val_loss': []}\r\n    # Train\r\n    for train_idx, valid_idx in cv.split(embeddings):\r\n        X_train, X_val, y_train, y_val = embeddings[train_idx], embeddings[valid_idx], labels[train_idx], labels[valid_idx]\r\n        his = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_data=(X_val, y_val))\r\n\r\n\r\n    # write the face recognition model to output\r\n    model.save(args.mymodel)\r\n    f = open(args.le, \"wb\")\r\n    f.write(pickle.dumps(LabelEncoder()))\r\n    f.close()\r\n\r\n\r\n```\r\nI get the error:\r\n\r\n```\r\n---> 28         his = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_data=(X_val, y_val))\r\n\r\n TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"DeserializeSparse:0\", shape=(None, 2), dtype=int64), values=Tensor(\"DeserializeSparse:1\", shape=(None,), dtype=float32), dense_shape=Tensor(\"stack:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.\r\n```\r\nI have used several issues solutions but none have worked.", "comments": ["@woreom\r\nI ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5e1b47c3d96fa04a2ee976360f3962c7/untitled403.ipynb).\r\n\r\n", "> @woreom\r\n> I ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5e1b47c3d96fa04a2ee976360f3962c7/untitled403.ipynb).\r\n\r\nI edited the notebook, Just changed the order of the class and function, first SoftMax. since we are using it a default argument for the function it must be defined first.  I just copied related parts of my project in the issue. Args class is just the functions argument in a way that I can change them in cmd, completely skippable.", "@woreom \r\nI ran the updated code and do not face any errors now, please see [gist here](https://colab.research.google.com/gist/Saduf2019/f297bc18a47460a5b3717de47a56ad2a/untitled407.ipynb), and verify.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @woreom\r\n> I ran the updated code and do not face any errors now, please see [gist here](https://colab.research.google.com/gist/Saduf2019/f297bc18a47460a5b3717de47a56ad2a/untitled407.ipynb), and verify.\r\n\r\nbut what did you run?  you dont have my models.", "@woreom \r\nWe ran what was shared by you, please share a colab gist with error reported as the code shared by you does not show any errors.", "> @woreom\r\n> We ran what was shared by you, please share a colab gist with error reported as the code shared by you does not show any errors.\r\n\r\nI generated the error [here](https://colab.research.google.com/gist/Saduf2019/f297bc18a47460a5b3717de47a56ad2a/untitled407.ipynb#scrollTo=UZAjtSmJGDuE)", "@woreom \r\nI cannot find any error in the gist shared, nor in the code shared, please find [gist here](https://colab.research.google.com/gist/Saduf2019/975976b444930a04a4b3f5e043ef23e7/untitled.ipynb).\r\nIf you are making changes to my colab, it wont be. You have to have a new gist with the error reported.", "@Saduf2019 Okay it seems I have forgotten to save the notebook, is this [link](https://colab.research.google.com/drive/1W7ZV-DZ5xP0TthFapP9uX8lYrWJmXyaq?usp=sharing) okay?", "@woreom \r\nPlease provide access to the link shared.", "@Saduf2019 \r\nOkay does [this](https://colab.research.google.com/drive/1W7ZV-DZ5xP0TthFapP9uX8lYrWJmXyaq?usp=sharing) work?", "@woreom \r\nI ran the gist shared and face this issues, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/fd46df3582beff1ac41f173a131686cb/untitled421.ipynb)", "@Saduf2019 Okay, How do I fix it?", "@Saduf2019 any updates?", "I have the same issue, even if I used the function below:\r\n\r\n`tf.squeeze(\r\n        tf.sparse.to_dense(\r\n            tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\r\n            default_value),\r\n        axis=1)`", "Did you folks manage on finding the solution for this? I'm also hitting the same problem here", "@irvifa @iamvarol the current solution I have is downgrading tensorflow to 2.1.0", "This solution works for me: https://github.com/tensorflow/tensorflow/issues/25980#issuecomment-714344567", "Anything on this.. i have a memory constrain so need to work with sparse data. Any help would be highly appreciated.", "> Anything on this.. i have a memory constrain so need to work with sparse data. Any help would be highly appreciated.\r\n\r\nI haven't tried it myself but  check @eternalphane [comment](https://github.com/tensorflow/tensorflow/issues/42916#issuecomment-715275839)", "> > Anything on this.. i have a memory constrain so need to work with sparse data. Any help would be highly appreciated.\r\n> \r\n> I haven't tried it myself but check @eternalphane [comment](https://github.com/tensorflow/tensorflow/issues/42916#issuecomment-715275839)\r\n\r\nThanks for checking. I have tried to run with similar data generator, which converts sparse -> dense in runtime. Because of this the training time has been very very slow. Just for one epoch it takes around 3 hours, becoming impossible to train the model.\r\nAnything that you can suggest would be very much appreciated.", "It seems I encountered a similar problem when I tried the [Google Machine Learning Guide on Text Classification](https://developers.google.com/machine-learning/guides/text-classification/).\r\n\r\nAdding todense() solved it for me:\r\n\r\n```\r\nx_train = vectorizer.fit_transform(train_texts).todense()\r\nx_val = vectorizer.transform(val_texts).todense()\r\n```", "Is this still an issue? For others who are still facing this issue can you please post a new issue with small reproducible example? Thanks!", "My code is based on this [Google guide on Text Classification](https://developers.google.com/machine-learning/guides/text-classification) for an ngram-model.\r\n\r\nIt still doesn't work if I do not add `.todense()` as indicated in my post above. (Python 3.8.6, TensorFlow 2.4.1, MacOS Big Sur 11.2.3).\r\n\r\nIt raises this error message:\r\n`TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"DeserializeSparse:0\", shape=(None, 2), dtype=int64), values=Tensor(\"DeserializeSparse:1\", shape=(None,), dtype=float32), dense_shape=Tensor(\"stack:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Is there any way to use sparse tensor or scipy sparse data with keras?", "Im having the same issue and when tried to use the solution suggested [here](https://github.com/tensorflow/tensorflow/issues/25980#issuecomment-714344567), Im getting `TypeError: Input must be a SparseTensor.`.\r\nCan anyone please advise me on what to do?", "Maybe this helps:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/47931\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42916\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42916\">No</a>\n"]}, {"number": 42915, "title": "Failed to convert SparseTensor to Tensor", "body": "", "comments": []}, {"number": 42914, "title": "centos8 gcc8.3 compile meet such question, anyone can help me ?", "body": "\r\nERROR: /home/docker_data/tensorflow_2.3/tensorflow/BUILD:754:1: Linking of rule '//tensorflow:libtensorflow_cc.so.2.3.0' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/docker_data/.bazel_cache/7a9fafc24832c7560e0a6e1a434658e0/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-10.2 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib: \\\r\n    NCCL_INSTALL_PATH=/usr \\\r\n    PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/cuda-10.2/bin:/usr/local/cuda-10.2/targets/x86_64-linux/lib:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages \\\r\n    TENSORRT_INSTALL_PATH=/usr/local/cuda-10.2 \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_PATHS=/usr/local/cuda-10.2 \\\r\n    TF_CUDA_VERSION=10.2 \\\r\n    TF_CUDNN_VERSION=8 \\\r\n    TF_ENABLE_XLA=1 \\\r\n    TF_NCCL_VERSION=2 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_TENSORRT_VERSION=7 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.2.3.0-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text+0x45c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#2}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text+0x5ac): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#3}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text+0x6fc): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#4}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text+0x84c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#5}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text+0x99c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::kernel_factory::OpKernelRegistrar::OpKernelRegistrar(tensorflow::KernelDef const*, absl::lts_2020_02_25::string_view, tensorflow::OpKernel* (*)(tensorflow::OpKernelConstruction*))':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow14kernel_factory17OpKernelRegistrarC2EPKNS_9KernelDefEN4absl14lts_2020_02_2511string_viewEPFPNS_8OpKernelEPNS_20OpKernelConstructionEE[_ZN10tensorflow14kernel_factory17OpKernelRegistrarC5EPKNS_9KernelDefEN4absl14lts_2020_02_2511string_viewEPFPNS_8OpKernelEPNS_20OpKernelConstructionEE]+0x55): undefined reference to `tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::lts_2020_02_25::string_view, std::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `std::_Function_handler<void (), tensorflow::DynamicPartitionOpGPU<std::complex<float> >::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZNSt17_Function_handlerIFvvEZN10tensorflow21DynamicPartitionOpGPUISt7complexIfEE12ComputeAsyncEPNS1_15OpKernelContextESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data[_ZNSt17_Function_handlerIFvvEZN10tensorflow21DynamicPartitionOpGPUISt7complexIfEE12ComputeAsyncEPNS1_15OpKernelContextESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data]+0xa5): undefined reference to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::DynamicPartitionOpGPU<Eigen::half>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow21DynamicPartitionOpGPUIN5Eigen4halfEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow21DynamicPartitionOpGPUIN5Eigen4halfEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x31e): undefined reference to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::DynamicPartitionOpGPU<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow21DynamicPartitionOpGPUIfE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow21DynamicPartitionOpGPUIfE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x30e): undefined reference to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::DynamicPartitionOpGPU<double>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow21DynamicPartitionOpGPUIdE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow21DynamicPartitionOpGPUIdE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x30e): undefined reference to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::DynamicPartitionOpGPU<std::complex<float> >::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)':\r\ntmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow21DynamicPartitionOpGPUISt7complexIfEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow21DynamicPartitionOpGPUISt7complexIfEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x30e): undefined reference to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o):tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow21DynamicPartitionOpGPUISt7complexIdEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow21DynamicPartitionOpGPUISt7complexIdEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x30e): more undefined references to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)' follow\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libgenerate_box_proposals_op_gpu.pic.lo(generate_box_proposals_op.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text+0x31a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\ntmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text+0x3d8): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libgenerate_box_proposals_op_gpu.pic.lo(generate_box_proposals_op.cu.pic.o): In function `_GLOBAL__sub_I_tmpxft_000097d4_00000000_5_generate_box_proposals_op.cu.cudafe1.cpp':\r\ntmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text.startup+0x3e0): undefined reference to `tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::lts_2020_02_25::string_view, std::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libgenerate_box_proposals_op_gpu.pic.lo(generate_box_proposals_op.cu.pic.o): In function `tensorflow::GenerateBoundingBoxProposals::Compute(tensorflow::OpKernelContext*)':\r\ntmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text._ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE]+0x2b0): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\ntmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text._ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE]+0x7a4): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\ntmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text._ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE]+0x88b): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libnon_max_suppression_op_gpu.pic.lo(non_max_suppression_op.cu.pic.o): In function `_GLOBAL__sub_I_tmpxft_0000b1d1_00000000_5_non_max_suppression_op.cu.cudafe1.cpp':\r\ntmpxft_0000b1d1_00000000-5_non_max_suppression_op.cu.cudafe1.cpp:(.text.startup+0x4d5): undefined reference to `tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::lts_2020_02_25::string_view, std::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libsvd_op_gpu.pic.lo(svd_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#3}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0x7a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)'\r\ntmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0xb2): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libsvd_op_gpu.pic.lo(svd_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0x1da): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)'\r\ntmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0x212): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libsvd_op_gpu.pic.lo(svd_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#2}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0x33a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libsvd_op_gpu.pic.lo(svd_op_gpu.cu.pic.o):tmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0x372): more undefined references to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)' follow\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/libkernels_gpu.pic.lo(kernels_gpu.cu.pic.o): In function `tensorflow::functor::CalculateNNZPerBatchMatrixFromIndices<Eigen::GpuDevice>::operator()(tensorflow::OpKernelContext*, Eigen::TensorMap<Eigen::Tensor<long long const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 1, 1, long>, 16, Eigen::MakePointer>)':\r\ntmpxft_0000d31a_00000000-5_kernels_gpu.cu.cudafe1.cpp:(.text+0x6ad): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\ntmpxft_0000d31a_00000000-5_kernels_gpu.cu.cudafe1.cpp:(.text+0xa07): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/liboptional_ops_gpu.pic.lo(optional_ops.cu.pic.o): In function `tensorflow::Status tensorflow::ZerosLikeTensor<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor*)':\r\ntmpxft_0000cdb1_00000000-5_optional_ops.cu.cudafe1.cpp:(.text._ZN10tensorflow15ZerosLikeTensorIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorEPS6_[_ZN10tensorflow15ZerosLikeTensorIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorEPS6_]+0x15c2): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/liboptional_ops_gpu.pic.lo(optional_ops.cu.pic.o): In function `tensorflow::Status tensorflow::BinaryAddTensors<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor*)':\r\ntmpxft_0000cdb1_00000000-5_optional_ops.cu.cudafe1.cpp:(.text._ZN10tensorflow16BinaryAddTensorsIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorES8_PS6_[_ZN10tensorflow16BinaryAddTensorsIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorES8_PS6_]+0xa71): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/liboptional_ops_gpu.pic.lo(optional_ops.cu.pic.o): In function `tensorflow::Status tensorflow::data::OptionalBinaryAdd<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::data::OptionalVariant const&, tensorflow::data::OptionalVariant const&, tensorflow::data::OptionalVariant*)':\r\ntmpxft_0000cdb1_00000000-5_optional_ops.cu.cudafe1.cpp:(.text._ZN10tensorflow4data17OptionalBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS0_15OptionalVariantES9_PS7_[_ZN10tensorflow4data17OptionalBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS0_15OptionalVariantES9_PS7_]+0x13d): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/data/liboptional_ops_gpu.pic.lo(optional_ops.cu.pic.o):tmpxft_0000cdb1_00000000-5_optional_ops.cu.cudafe1.cpp:(.text._ZN10tensorflow4data17OptionalBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS0_15OptionalVariantES9_PS7_[_ZN10tensorflow4data17OptionalBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS0_15OptionalVariantES9_PS7_]+0x1bf): more undefined references to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)' follow\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#3}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x272c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#67}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x281c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#43}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x290c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#103}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x29fc): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#62}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x2aec): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o):tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x2bdc): more undefined references to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)' follow\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#25}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4a12): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#85}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4b3a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4b72): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#37}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4c9a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4cd2): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#73}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4dfa): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4e32): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#97}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4f5a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4f92): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#49}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x50ba): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x50f2): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x521a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5252): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#61}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x537a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x53b2): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#13}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x54da): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5512): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#101}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x564d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x566d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x56c1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#53}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x58fd): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x591d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5971): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#30}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5bad): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5bcd): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5c21): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#65}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5e5d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5e7d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5ed1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#78}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x610d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x612d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6181): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#18}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x63bd): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x63dd): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6431): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#90}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x666d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x668d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x66e1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#54}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x691d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x693d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6991): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#6}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6bcd): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6bed): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6c41): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#102}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6e7d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6e9d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6ef1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#17}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x712d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x714d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x71a1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#41}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x73dd): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x73fd): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7451): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#42}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x768d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x76ad): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7701): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#29}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x793d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x795d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x79b1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#5}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7bed): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7c0d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7c61): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#66}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7e9d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7ebd): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7f11): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#77}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x814d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x816d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x81c1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#89}::_FUN(tensorflow::OpKernelConstruction*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x83fd): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x841d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x8471): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::Status tensorflow::TensorListBinaryAdd<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::TensorList const&, tensorflow::TensorList const&, tensorflow::TensorList*)':\r\ntmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text._ZN10tensorflow19TensorListBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_10TensorListES8_PS6_[_ZN10tensorflow19TensorListBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_10TensorListES8_PS6_]+0x1c2): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libwhere_op_gpu.pic.lo(where_op_gpu_impl_1.cu.pic.o): In function `tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*)':\r\ntmpxft_0000cbd8_00000000-5_where_op_gpu_impl_1.cu.cudafe1.cpp:(.text._ZN10tensorflow6errors8InternalIJPKcS3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8InternalIJPKcS3_EEENS_6StatusEDpT_]+0x68): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libwhere_op_gpu.pic.lo(where_op_gpu_impl_1.cu.pic.o): In function `tensorflow::Status tensorflow::errors::Internal<char const*, unsigned long, char const*, char const*>(char const*, unsigned long, char const*, char const*)':\r\ntmpxft_0000cbd8_00000000-5_where_op_gpu_impl_1.cu.cudafe1.cpp:(.text._ZN10tensorflow6errors8InternalIJPKcmS3_S3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8InternalIJPKcmS3_S3_EEENS_6StatusEDpT_]+0xc5): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libbincount_op_gpu.pic.lo(bincount_op_gpu.cu.pic.o): In function `tensorflow::Status tensorflow::errors::Internal<char const*, char const*, char const*>(char const*, char const*, char const*)':\r\ntmpxft_00008562_00000000-5_bincount_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow6errors8InternalIJPKcS3_S3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8InternalIJPKcS3_S3_EEENS_6StatusEDpT_]+0x94): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libtopk_op_gpu.pic.lo(topk_op_gpu_double.cu.pic.o): In function `tensorflow::functor::TopKFunctor<Eigen::GpuDevice, double>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<double, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)':\r\ntmpxft_0000d38b_00000000-5_topk_op_gpu_double.cu.cudafe1.cpp:(.text._ZN10tensorflow7functor11TopKFunctorIN5Eigen9GpuDeviceEdE7ComputeEPNS_15OpKernelContextEbiRKNS2_9TensorMapINS2_6TensorIKdLi2ELi1ElEELi16ENS2_11MakePointerEEExxNS7_INS8_IdLi2ELi1ElEELi16ESB_EENS7_INS8_IiLi2ELi1ElEELi16ESB_EE[_ZN10tensorflow7functor11TopKFunctorIN5Eigen9GpuDeviceEdE7ComputeEPNS_15OpKernelContextEbiRKNS2_9TensorMapINS2_6TensorIKdLi2ELi1ElEELi16ENS2_11MakePointerEEExxNS7_INS8_IdLi2ELi1ElEELi16ESB_EENS7_INS8_IiLi2ELi1ElEELi16ESB_EE]+0x1c9): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libtopk_op_gpu.pic.lo(topk_op_gpu_double.cu.pic.o):tmpxft_0000d38b_00000000-5_topk_op_gpu_double.cu.cudafe1.cpp:(.text._ZN10tensorflow4impl16LaunchSortKernelIdEENS_6StatusEPNS_15OpKernelContextEPKT_iiiNS_6TTypesIS5_Li2ElE6TensorEN5Eigen9TensorMapINSB_6TensorIiLi2ELi1ElEELi16ENSB_11MakePointerEEE[_ZN10tensorflow4impl16LaunchSortKernelIdEENS_6StatusEPNS_15OpKernelContextEPKT_iiiNS_6TTypesIS5_Li2ElE6TensorEN5Eigen9TensorMapINSB_6TensorIiLi2ELi1ElEELi16ENSB_11MakePointerEEE]+0x590): more undefined references to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)' follow\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 2229.194s, Critical Path: 364.17s\r\nINFO: 11324 processes: 11324 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42914\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42914\">No</a>\n", "@passion765 Hello, I also have the same problem on `\"undefined reference to tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)\"` , is there a solution? thanks"]}]