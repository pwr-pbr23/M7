[{"number": 26474, "title": "Android TFLite benchmark performance issue with deeplab segmentation model (DepthwiseConv2d)", "body": "**System information**\r\nThe benchmark tests were carried out using the following tools and devices:-\r\nBazel version:Build label: 0.23.1\r\nTensorflow Version: 1.13.0\r\nAndroid Device: OnePlus 3\r\n\r\n\r\n**Describe the current behavior**\r\nWe trained a segmentation model using deeplab with mobilenet with TF 1.13.0 to replicate the\r\nsegmentation model by tensorflow i.e 'deeplabv3_257_mv_gpu.tflite'(Using sample code  in repo for pascal voc).\r\nHowever there was significant time difference for average running time for 'DepthwiseConv2d', when we benchmarked the two tflite models with 'tflite android benchmark tool'.\r\nIn the official model avg time was around 28 ms; whereas in our own model it was around 103 ms.\r\nThe only difference between the two float models being the quantization levels i.e Ours was 0-255\r\nand official model has -1 to 0.99.Also, there is a difference in model size(Official:2.7Mb vs. Ours:3.3Mb).\r\n\r\nIt seems the official model is using a newer kernel for depthwiseconv2d and hence runs significantly faster compared to our trained model.Was the official model trained or optimised using TF2.0 tools?What could be done to achieve similar speed (DepthwiseConv2d) using TF 1.13.0 or do we have to migrate and retrain our model using TF 2.0?\r\n\r\n**Describe the expected behavior**\r\nThe depthwise conv2d performance should be same in the official model and the replicated model.\r\n\r\n**Code to reproduce the issue**\r\nBenchmark commands:-\r\n\r\n```\r\n# bazel build -c opt   --config=android_arm64   --cxxopt='--std=c++11' --copt=-DTFLITE_PROFILING_ENABLED tensorflow/lite/tools/benchmark:benchmark_model\r\n\r\n# adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1\r\n```\r\n\r\nBenchmark results:-\r\n\r\n_Official model:-_\r\n\r\n```\r\nNumber of nodes executed: 70\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t                 CONV_2D\t       38\t   137.792\t    76.447%\t    76.447%\t     0.000\t       38\r\n\t       DEPTHWISE_CONV_2D\t       17\t    27.808\t    15.428%\t    91.875%\t     0.000\t       17\r\n\t         RESIZE_BILINEAR\t        3\t    13.577\t     7.533%\t    99.408%\t     0.000\t        3\r\n\t           CONCATENATION\t        1\t     0.528\t     0.293%\t    99.701%\t     0.000\t        1\r\n\t                     ADD\t       10\t     0.410\t     0.227%\t    99.928%\t     0.000\t       10\r\n\t         AVERAGE_POOL_2D\t        1\t     0.129\t     0.072%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=180683 curr=179967 min=177601 max=186179 avg=180284 std=1486\r\nMemory (bytes): count=0\r\n70 nodes observe\r\n```\r\n\r\n\r\n_Replicated model:-_\r\n\r\n```\r\nNumber of nodes executed: 70\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t                 CONV_2D\t       38\t   141.189\t    54.356%\t    54.356%\t     0.000\t       38\r\n\t       DEPTHWISE_CONV_2D\t       17\t   103.263\t    39.755%\t    94.110%\t     0.000\t       17\r\n\t         RESIZE_BILINEAR\t        3\t    14.178\t     5.458%\t    99.568%\t     0.000\t        3\r\n\t           CONCATENATION\t        1\t     0.554\t     0.213%\t    99.782%\t     0.000\t        1\r\n\t                     ADD\t       10\t     0.437\t     0.168%\t    99.950%\t     0.000\t       10\r\n\t         AVERAGE_POOL_2D\t        1\t     0.130\t     0.050%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=258703 curr=259361 min=255581 max=269627 avg=259789 std=3844\r\nMemory (bytes): count=0\r\n70 nodes observed\r\n```\r\n\r\n**Other info / logs**\r\n\r\nHowever using TF-12.0 with Bazel 0.16.0, the 'benchmark_model' built failed for 'official model'; but worked with replicated model.\r\n\r\nBazel build:-\r\n\r\n```\r\n# bazel build -c opt   --config=android_arm64   --cxxopt='--std=c++11' --linkopt='-llog' --copt=-DTFLITE_PROFILING_ENABLED tensorflow/contrib/lite/tools/benchmark:benchmark_model\r\n\r\n# adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1\r\n```\r\n\r\n\r\nTF-Lite benchmark:-\r\n\r\nadb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1\r\nadb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)\r\nSTARTING!\r\nNum runs: [50]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nWarmup runs: [1]\r\nGraph: [/data/local/tmp/deeplabv3_257_mv_gpu.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model /data/local/tmp/deeplabv3_257_mv_gpu.tflite\r\nresolved reporter\r\nDidn't find op for builtin opcode 'DEPTHWISE_CONV_2D' version '2'\r\n\r\nRegistration failed.\r\n\r\nFailed to construct interpreter\r\nAborted\r\n\r\nCan you provide the corresponding optimised pb file before tflite conversion, for the same model ?", "comments": ["@anilsathyan7 : \r\nVersion 2 of [Depthwiseconv2d](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/operator.cc#L177) :\r\nis used when dilation width and height factors are equal to 1.\r\nThis doesn't have to do anything with TF 2.0. Adding @jianlijianli  who can give a definitive answer.", "It looks like the dilation_w_factor and dilation_h_factor in the official model for depthwiseconv2d  are not equal to 1 everywhere.Initially it is 1 for couple of depthwiseconv2d layers, then it becomes 2 and finally it is 4 for last few depthwiseconv2d layers. This seems to be the difference between both models (replicated model has dilation width and height factors equal to one in all the nodes in network).This also seems to make the avg execution time for deothwiseonv2d in official model smaller.How can we configure the dilation width and height factors separately for each nodes  during deeplabv3+ training?Or is it a problem with tflite conversion or tf-version ? \r\n", "We tried benchmarking the tflite obtained from code in official deeplab repository, with depthwise conv2d Version 2. The dilation_w_factor and dilation_h_factor for deeplab is set as 1, by default. The tflite from the output of the graph transform model, seemed to provide no speed difference as we believe graph transformation (Flatten_atrous_conv), has affected the speed improvement by DepthwiseConv2d Version 2. The direct output from the optimize for inference, was giving speedup in the Depthwise Conv2d V2, but when trying to run it on the mobile GPU, it increased the time of execution, since it had operators like SpacetoBatchND and BatchtoSpaceND, which are not optimized for GPU processing. Is there any way, we can remove the spacetobatch and batchtospace nodes from the optimized graph, so as to obtain the speed gain by Depthwise Conv2d Version2 on android GPU... Tried doing the remove_nodes graph transform on the optimized graph, but it did not remove the nodes..\r\n\r\nAttached with here, are the commands, benchmarks and tflite files..\r\n\r\n**Benchmark Results: (Oneplus 3)**\r\n\r\n**1.Without Batch2Space**\r\n\r\nCommand & Result:-\r\n\r\n`adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/withoutbatchtospace.tflite --num_threads=1`\r\nadb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)\r\nSTARTING!\r\nNum runs: [50]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nWarmup runs: [1]\r\nGraph: [/data/local/tmp/withoutbatchtospace.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model /data/local/tmp/withoutbatchtospace.tflite\r\nresolved reporter\r\nInitialized session in 7.374ms\r\nRunning benchmark for 1 iterations \r\ncount=1 curr=248072\r\n\r\nRunning benchmark for 50 iterations \r\ncount=50 first=243147 curr=234508 min=234107 max=245867 avg=237050 std=3526\r\n\r\n\r\nNumber of nodes executed: 70\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t                 CONV_2D\t       38\t   128.807\t    54.361%\t    54.361%\t     0.000\t       38\r\n\t       DEPTHWISE_CONV_2D\t       17\t    90.016\t    37.990%\t    92.351%\t     0.000\t       17\r\n\t         RESIZE_BILINEAR\t        3\t    17.095\t     7.215%\t    99.566%\t     0.000\t        3\r\n\t           CONCATENATION\t        1\t     0.505\t     0.213%\t    99.779%\t     0.000\t        1\r\n\t                     ADD\t       10\t     0.401\t     0.169%\t    99.949%\t     0.000\t       10\r\n\t         AVERAGE_POOL_2D\t        1\t     0.122\t     0.051%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=243080 curr=234449 min=234036 max=245808 avg=236985 std=3526\r\nMemory (bytes): count=0\r\n70 nodes observed\r\n\r\nAverage inference timings in us: Warmup: 248072, Init: 7374, no stats: 237050\r\n\r\n\r\n**2.With Batch2Space**\r\n\r\nCommand & Result:-\r\n\r\n`adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/withbatchtospace.tflite --num_threads=1`\r\nadb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)\r\nSTARTING!\r\nNum runs: [50]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nWarmup runs: [1]\r\nGraph: [/data/local/tmp/withbatchtospace.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model /data/local/tmp/withbatchtospace.tflite\r\nresolved reporter\r\nInitialized session in 6.757ms\r\nRunning benchmark for 1 iterations \r\ncount=1 curr=207830\r\n\r\nRunning benchmark for 50 iterations \r\ncount=50 first=197780 curr=196627 min=195049 max=200776 avg=196418 std=1165\r\n\r\n\r\nNumber of nodes executed: 110\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t                 CONV_2D\t       38\t   129.772\t    66.131%\t    66.131%\t     0.000\t       38\r\n\t       DEPTHWISE_CONV_2D\t       17\t    30.027\t    15.302%\t    81.432%\t     0.000\t       17\r\n\t         RESIZE_BILINEAR\t        3\t    17.773\t     9.057%\t    90.489%\t     0.000\t        3\r\n\t                     MUL\t       10\t     8.193\t     4.175%\t    94.665%\t     0.000\t       10\r\n\t       SPACE_TO_BATCH_ND\t       10\t     3.707\t     1.889%\t    96.554%\t     0.000\t       10\r\n\t                     ADD\t       20\t     3.707\t     1.889%\t    98.443%\t     0.000\t       20\r\n\t       BATCH_TO_SPACE_ND\t       10\t     2.446\t     1.246%\t    99.689%\t     0.000\t       10\r\n\t           CONCATENATION\t        1\t     0.492\t     0.251%\t    99.940%\t     0.000\t        1\r\n\t         AVERAGE_POOL_2D\t        1\t     0.118\t     0.060%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=197577 curr=196500 min=194929 max=200638 avg=196286 std=1161\r\nMemory (bytes): count=0\r\n110 nodes observed\r\n\r\n\r\nAverage inference timings in us: Warmup: 207830, Init: 6757, no stats: 196418\r\n\r\n**Optimize for inference** \r\n`bazel-bin/tensorflow/python/tools/optimize_for_inference --input=frozen.pb --output=stripped.pb --frozen_graph=True --input_names=\"sub_2\" --output_names=\"ResizeBilinear_2\"`\r\n\r\n**Graph Transforms**\r\n`bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=\"stripped.pb\" --out_graph=\"flatten.pb\"  --inputs='sub_2' --outputs='ResizeBilinear_2' --transforms='flatten_atrous_conv'`\r\n\r\n**TFLite Conversion** \r\n\r\n`CUDA_VISIBLE_DEVICES=\"0\" tflite_convert --graph_def_file=\"flatten.pb\" --output_file=\"withoutbatchtospace.tflite\" --output_format=TFLITE --input_format=TENSORFLOW_GRAPHDEF --input_arrays=\"sub_2\" --output_arrays=\"ResizeBilinear_2\" --input_shapes=1,257,257,3\r\n`\r\n\r\n`\r\nCUDA_VISIBLE_DEVICES=\"0\" tflite_convert --graph_def_file=\"stripped.pb\" --output_file=\"withbatchtospace.tflite\" --output_format=TFLITE --input_format=TENSORFLOW_GRAPHDEF --input_arrays=\"sub_2\" --output_arrays=\"ResizeBilinear_2\" --input_shapes=1,257,257,3\r\n`\r\n[tf_depthwise_conv2d_v2.zip](https://github.com/tensorflow/tensorflow/files/2990941/tf_depthwise_conv2d_v2.zip)\r\n", "Hi @ilous12, those frozen graph and tflite files will have depth multiplier 0.5 in this case...", "> @anilsathyan7 :\r\n> Version 2 of [Depthwiseconv2d](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/operator.cc#L177) :\r\n> is used when dilation width and height factors are equal to 1.\r\n> This doesn't have to do anything with TF 2.0. Adding @jianlijianli who can give a definitive answer.\r\n\r\n@jianlijianli @shashishekhar  The code snippet referred, states that version2 of depthwiseconv2d will be used, when either of dilation_width_factor or dilation_height_factor is not equal to 1. This is exactly opposite what was written above. \r\n\r\n@jianlijianli How to set dilation height and width when using Mobilenet-v2 architecture to take advantage of Depthwiseconv2d version 2? ", "@anilsathyan7 thanks your help. I succeeded by following you. \r\n\r\nOne more question.\r\nI don't know why my NHWC is \"1x17x17x96\" and \"deeplabv3_257_mv_gpu.tflite\"'s NHWC is \"1x17x17x96\" on MobilenetV2/expanded_conv_6. You will see an image below.\r\n\r\n<img width=\"1546\" alt=\"\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2019-03-22 \u110b\u1169\u1112\u116e 8 01 33\" src=\"https://user-images.githubusercontent.com/3174693/54818973-e1b91f80-4cdd-11e9-9992-cf4f277e3d9d.png\">\r\n\r\nDo you know how to fix?\r\nthanks.", "The following seems to be the major differences between the official model and our replicated model:-\r\nDeptwiseconv2d dilation factors, its corresponding weight dimensions and the input bias id (i.e no batc2spaceND_bias):-\r\n\r\n![compare](https://user-images.githubusercontent.com/1130185/55138564-c9bc2280-5159-11e9-9d3a-2c21571caf05.png)\r\n\r\nHow can we fix it? Should we use some different training settings or can we fix it while tflite conversion?\r\n@ilous12  are you having similar issues?If so, did you manage to resolve the same?What is dilation_w_factor and dilation_h_factor values for node 25?\r\n", "Similar issue here. Does anyone replicated model without BATCH_TO_SPACE?", "Any updates on this? Getting the same error on arm64_v8 with tensorflow lite 1.13.1 ", "Any luck getting ride of those pesty BATCH_TO_SPACE?", "I have the same issue,  can anyone help us and give a definite answer?thanks.", "@zhuyan288  @jsolves \r\nI think tflite_convert in 1.15 can do the conversion directly. ", "Latest tf-nightly and tf1.15 seems to solve this problem ; but tf-nightly sometimes creates new nodes that may not be supported by gpu-delegate.Other option is to use transform graph tool (with corresponding tf version & bazel) with flatten atrous conv option. This seems to work for spacetobatch/batchtospace issue for depthwise or diated convolutions in general.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26474\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26474\">No</a>\n"]}, {"number": 26473, "title": "The use of graph_editor.graph_replace", "body": "Hi, I'm quite new to tensorflow and I came from this question: [tensorflow-replace-an-operation-with-an-other](https://stackoverflow.com/questions/47081900/tensorflow-replace-an-operation-with-an-other) , which is asked quite a long time ago and I've got similar need.\r\nI have a tacotron model, freezed, with LSTMBlockCell and dynamic_rnn involved, which seems to not have a quantization version in tensorflow/tools/graph_transforms/quantize_nodes. \r\nThe idea is that quant the weights and bias, and replace things in the graph first, maybe add dequant nodes later and modify the lstmblockcell itself. I tried to process the first step in this way:\r\n\r\n```\r\ntensors = [n for n in session.graph.as_graph_def().node]\r\nfor t in tensors:\r\n  if 'lstm_cell' in t.name:\r\n    lstm_cell = graph.get_tensor_by_name(t.name + ':0')\r\n    val = session.run(lstm_cell)\r\n    quant_val = quant_weights(val)\r\n    new_t = tf.convert_to_tensor(quant_val, np.uint8)\r\n    tf.contrib.graph_editor.graph_replace(lstm_cell, {lstm_cell : new_t})\r\n```\r\nAnd it returned:\r\n```\r\nTraceback (most recent call last):\r\n  File \"edit_graph.py\", line 131, in <module>\r\n    load('my_graph.pb')\r\n  File \"edit_graph.py\", line 100, in load\r\n    tf.contrib.graph_editor.graph_replace(lstm_cell, {lstm_cell : new_t}, reuse_dst_scope = True)\r\n  File \"/home/admin/.local/lib/python3.6/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 736, in graph_replace\r\n    raise ValueError(\"Targets and replacements are not connected!\")\r\nValueError: Targets and replacements are not connected!\r\n```\r\nCould you kindly point out the mistake I made? Thanks in advance.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 26472, "title": "Keras model save/load is totally wrong with sessions", "body": "## Bug report: Keras `model.save()` does not work with sessions\r\n\r\nActually, it only works with the default session.\r\n\r\nI know tf.keras is mainly targetting at TF 2.0, but it really doesn't play nice with conventional TF-1.x ways (e.g. Session). I encountered this bug and nailed down why it happens after a day. Arguably, I believe this is a severe bug and hard to realize when one encounters. This is also somewhat related to #26430.\r\n\r\n**In summary (the current behavior)**: if one is using a custom session created (say `MonitoredSession`, as it was a conventional way of opening and sessions in TF 1.x way), Keras model's load/save will be COMPLETELY not working.\r\n\r\n### System information\r\n\r\n- OS Platform and Distribution: Linux, but agnostic to Platform\r\n- TensorFlow installed from (source or binary): binary, pypi\r\n- TensorFlow version (use command below): **1.13.1**\r\n- Python version: 3.6\r\n\r\n## Code to reproduce the issue\r\n\r\nPlease see this notebook:\r\n\r\nhttps://gist.github.com/wookayin/0712261896799e0f655df50e910055df\r\n\r\nOr in a code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.models.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.layer = tf.keras.layers.Dense(5)\r\n        \r\n    def __call__(self, x):\r\n        return self.layer(x)\r\n    \r\nmodel = MyModel()\r\n\r\n# Build the model\r\nmodel(tf.zeros([3, 10]))\r\n\r\n# this is something like a train_op that \"changes\" the content of the variable.\r\nassign_op = model.variables[1].assign( tf.ones_like(model.variables[1]) )\r\n\r\n# Let's create a session\r\nsess = tf.train.SingularMonitoredSession()\r\n#sess.run(tf.global_variables_initializer())\r\n\r\n# we can see the 'bias' variable is initialized to ZERO\r\nassert sess.run(model.variables[1]).mean() == 0.0\r\n\r\n# Now let's make the 'bias' variable to all one...\r\nsess.run(assign_op)\r\n\r\n# sure it is ONE ...\r\nassert sess.run(model.variables[1]).mean() == 1.0\r\n\r\n# Let's save the Keras model parameter. The bias is set to ONE, right ?????\r\n# Since model.save_weights try to create a new op (another bug #26430)\r\n# and the graph has been finalized, we will 'unfinalize' the graph with a bit of hack\r\nsess.graph._finalized = False\r\nmodel.save_weights('/tmp/keras-one.h5')\r\nsess.graph.finalize()\r\n\r\n\r\n# Let's see what is stored in the model file ....\r\nimport h5py\r\nh = h5py.File(\"/tmp/keras-one.h5\")\r\nassert h['dense']['dense']['bias:0'].value.mean() == 1.0     # <------ This will fail\r\n# Actual output is: array([0., 0., 0., 0., 0.], dtype=float32)\r\n```\r\n\r\nThe above code does the following:\r\n\r\n* Build a Keras model\r\n* Initialize the layer (bias will be zero)\r\n* Update the trainable variable (e.g. bias: 0->1)\r\n* Save the model into a H5 File\r\n* Read the model file, but the bias variable is not 1 but 0 (same as the initialization)\r\n\r\n\r\n\r\n### Additional Information\r\n\r\n* `tf.Session()` behaves exactly the same as with `tf.train.MonitoredSession()`. In this case we need to run custom variable initializer ops.\r\n* However, when `tf.InteractiveSession()` is used, it works exactly as expected (the readout from `keras-one.h5` is 1.0)\r\n* It would work well in the eager mode.\r\n\r\n\r\n## A work-around\r\n\r\nIf we change the code as follows, it would work.\r\n\r\n```\r\nwith sess.as_default():\r\n    model.save_weights('/tmp/keras-one.h5')\r\n```\r\n\r\nA quick note: 'SingularMonitoredSession' object has no attribute 'as_default'\r\n\r\n\r\nThis is because [Keras only plays with **the default session**][keras-session] (see #26430 as well) and in this example the manually created session was never registered as default. Kinda makes sense as there is no way for `model.save()` function to be aware of the session... However, this leads to a **very inconsistent and unexpected behavior.** If no session is registered as default, `model.save()` should instead throw an error.\r\n\r\n[keras-session]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L401", "comments": ["Any feedbacks? I think we should make tf.Keras **NOT** use its own default session (`_SESSION`) when integrated to TF core. However, it can also break many things that have been relying on this implicit Keras session.", "This is an most important problem. Please check it .", "@fchollet @ymodak Can we revisit this issue? As TF 1.x is expecting the last release soon, I hope this bug should be fixed. ", "If this is not fixed quite soon, one should at least introduce a warning and some documentation about this behavior, this was quite unexpected for me and wasted a few trainings.", "Has this been solved for keras 2.x? I'm using 2.3.1 and I can still see this problem.\r\nI'm using Jupyter notebooks. In one notebook, I'm saving a model using `model.save()` and later loading the model using `load_model()` and it works perfectly fine. \r\nBut when I try to open the same saved model from a different notebook, I get very random accuracy and predictions. From what I understand, the weights are getting reinitialized randomly. What is the workaround for this? Urgent.  ", "@aashnajena Make sure the session you are using is registered as a \"default\" session. I already have put a code snippet for workaround.", "This still needs to be fixed in 2.x.", "@wookayin,\r\nCan someone provide a reproducible code in `Tensorflow 2.x` so that it can be looked into? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26472\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26472\">No</a>\n"]}, {"number": 26471, "title": "Not pass dtype from Glorot initialization", "body": "`VarianceScaling` is deprecated `dtype` argument and this flow is called only for floating number initialization. So, passing the `dtype` argument can be ignored. This issue will not be there with the `init_ops_v2` implementation. But currently used one is `init_ops` and this PR avoid warning thrown from all the tests run this initialization code.", "comments": ["Can you point to the place where it's ignoring the `dtype` in `VarianceInitialization`? I don't see that. Note that there are multiple floating points half, bfloat, 32, and 64 bits.", "@drpngx sorry for the late reply, \r\nI meant, `dtype` is a deprecated argument of `VarianceScaling`, it is giving warning as well. And also the same have been removed from `init_ops_v2`. Currently its supporting with a default value FLAOT32. So I thought of removing the call with the deprecated argument`dtype` and keep `float32` as default type. Does it make any problem?"]}, {"number": 26470, "title": "Broken link in a notebook", "body": "Notebook - Intro to CNNs (TF 2.0 Alpha tutorial - images): https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/images/intro_to_cnns.ipynb\r\nIn the last cell there is a broken link (404): https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/_index_/advanced.ipynb (\"_As you can see, our simple CNN has achieved a test accuracy of over 99%. Not bad for a few lines of code! For another style of writing a CNN (using the Keras Subclassing API and a GradientTape) head [here](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/_index_/advanced.ipynb)._\")", "comments": ["See PR https://github.com/tensorflow/docs/pull/379\r\n\r\nShould this be something done in Colab? I saw that had created problems for some people."]}, {"number": 26469, "title": "[doc]: dead link in \"Recurrent Neural Networks\"", "body": "\r\n\r\n**System information**\r\n- TensorFlow version: master\r\n- Doc Link: https://www.tensorflow.org/tutorials/sequences/recurrent#language_modeling\r\n\r\n**Describe the documentation issue**\r\n\r\n\r\nThe link <https://catalog.ldc.upenn.edu/ldc99t42> for \" Penn Tree Bank\" is no long accessible.\r\n\r\nThe correct URL might be https://catalog.ldc.upenn.edu/LDC99T42 , that is, upper cases for \"LDC99T42\"\r\n\r\n\r\n", "comments": []}, {"number": 26468, "title": "Tensorflow.loadLibrary() not working on Windows", "body": "Running:\r\n\r\n`TensorFlow.loadLibrary(\"ner-dl/win/_lstm_ops.so\")`\r\n\r\nReturns:\r\n\r\n```\r\njava.lang.UnsatisfiedLinkError: ner-dl\\win\\_lstm_ops.so not found\r\n\tat org.tensorflow.TensorFlow.loadLibrary(TensorFlow.java:47)\r\n```\r\n\r\nWe're trying to load contrib .so files dynamically from java API. These .so files are generated on Windows by installing tensorflow through pip. Using python 3.6.8 to do so and retrieving them from the library source code. These libraries are from tensorflow.contrib, since this is what we're using on our graphs. To me, this sounds like a path resolving issue, since it's the same error than making up an invented path.\r\n\r\nWorks fine when using appropriate .so files on Linux and Mac.\r\n\r\nSame happens if using absolute paths btw:\r\n```\r\njava.lang.UnsatisfiedLinkError: C:\\Users\\saifa\\IdeaProjects\\spark-nlp\\src\\main\\resources\\ner-dl\\win\\_lstm_ops.so not found\r\n\tat org.tensorflow.TensorFlow.loadLibrary(TensorFlow.java:47)\r\n```\r\n\r\nIt seems this also happened to this poor guy here:\r\nhttps://stackoverflow.com/questions/50115117/using-ops-from-tensorflow-contrb-on-windows-via-java-api\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n`TensorFlow.loadLibrary(\"ner-dl/win/_lstm_ops.so\")`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nBinary from pip\r\n- TensorFlow version (use command below):\r\n`b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0`\r\n- Python version:\r\nUsing Java API\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nRunning\r\n`TensorFlow.loadLibrary(\"ner-dl/win/_lstm_ops.so\")`\r\nreturns \r\n```\r\njava.lang.UnsatisfiedLinkError: ner-dl\\win\\_lstm_ops.so not found\r\n\tat org.tensorflow.TensorFlow.loadLibrary(TensorFlow.java:47)\r\n```\r\n\r\n**Describe the expected behavior**\r\nShould be able to load libraries dynamically on Windows. This works fine from Linux and Mac.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n`TensorFlow.loadLibrary(\"ner-dl/win/_lstm_ops.so\")`\r\n", "comments": ["@asimshankar Can you PTAL? Thanks!", "Unfortunately, dynamic library loading in Windows is not supported yet AFAIR.\r\nI might be mis-remembering, but dynamically loading shared libraries doesn't work on Python on Windows either, right?\r\n\r\n@allenlavoie @sjamesr @gunan  might have more details on this and might be able to explain what needs to happen to make this possible. ", "@meteorcloudy as he helped us a lot with shared objects on windows.\r\nI think we finally had custom ops working on windows. Were they using loadLibrary?", "Dynamically loading shared libraries works in Python on Windows, but I never tired using Java API so it might still be not supported.", "Hi, I am still interested on this issue. Was closed by bot. Still happening.", "I noticed that the `loadLibrary` call is loading a `.so` file. Shouldn't it be loading a `.dll` file? Does `ner-dl\\win\\_lstm_ops.so` exist?", "Yes, the file exists. the error is the same if you make up a file name. The same code works on linux and mac. I am not sure if it should be a .dll. If so, I'd like to know where to get the lstm contrib libs for windows.\r\n\r\nHowever, most likely it is a path resolution error, with slashes and backslashes, because I noticed it converts windows slashes to unix ones.", "Hi everyone! I faced with the same problem when I tried to load \"tf.contrib\" using TensorFlow Java API on Windows (for Linux Ubuntu works fine).  \r\nI found answer how to solve this : [link](https://stackoverflow.com/questions/50115117/using-ops-from-tensorflow-contrb-on-windows-via-java-api). But there is another problem -> unable to load model. \r\nSo, how do we solve this problem?", "@asimshankar Can you PTAL? Thanks!\r\n", "@sjamesr : Could you take a look? (I haven't been working on TensorFlow for a while now :))", "Has there been any advance on this?", "Will take a look, sorry for the delay.", "Hi, is this problem resolved ?", "Hi @sjamesr, any updates on the issue?", "@sjamesr, @asimshankar, any updates?", "Hi, this issue is no longer valid since Tensorflow 1.15, given contrib libraries are handled differently.", "@saif-ellafi \r\nPlease confirm if we may moved this to closed status.", "@Saduf2019 I can confirm this issue can be closed. Me and Saif work on the same project and this issue is no longer presents in 1.15.x\n\n\nMany thanks.", "I confirm this can be closed. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26468\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26468\">No</a>\n"]}, {"number": 26467, "title": "[CI] The PR Checkers of TenrsorFlow are broken.", "body": "It is strange. It seems that the PR Checkers are suddenly broken. Anyone why this issue is still not fixed?\r\nAs you can see, Many PRs have been not reviewed by reviewers because the Tensorflow CI facilities are still a running status. BTW, Why this issue is generated? What is a possible reasons? Who is in charge of the CI facilities of the TensorFlow github repository?\r\n\r\n```bash\r\n* Ubuntu CC Expected \u2014 Waiting for status to be reported  Required\r\n* Ubuntu Sanity Expected \u2014 Waiting for status to be reported  Required\r\n* import/copybara Expected \u2014 Waiting for status to be reported  Required\r\n```\r\n\r\n* Screenshot: \r\n\r\n![image](https://user-images.githubusercontent.com/82404/54006477-d5db4280-41a0-11e9-974f-2708aa2f39ec.png)\r\n\r\n![image](https://user-images.githubusercontent.com/82404/54006516-f6a39800-41a0-11e9-98b0-b45a32135c78.png)\r\n", "comments": ["The behavior you see here is expected. It says that those tests are required but are waiting to be initialized. Once we trigger the build tests their status will be updated accordingly.", ">> Ubuntu CC Expected \u2014 Waiting for status to be reported\r\n>It says that those tests are required but are waiting to be initialized. \r\n\r\n@ymodak It seems that the tests are too slow when PRs are submitted.  Are the slow tests generated from the too many pull requests (PRs)?"]}, {"number": 26466, "title": "Java API can not create empty tensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MAC OS 10.14.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NO\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.8\r\n- Python version:None\r\n- Bazel version (if compiling from source):None\r\n- GCC/Compiler version (if compiling from source):None\r\n- CUDA/cuDNN version:None\r\n- GPU model and memory:None\r\n\r\nC++ API is able to create empty tensor\r\n```cpp\r\n        Tensor ts_indices(tensorflow::DT_INT64, tensorflow::TensorShape(\r\n                   { 0, 2 }));\r\n```\r\nHowever, Java API will raise Exception.\r\n```java\r\n        long[][] index = new long[0][2];\r\n        Tensors.create(index);\r\n ```\r\n> java.lang.IllegalArgumentException: cannot create Tensors with a 0 dimension\r\n\tat org.tensorflow.Tensor.fillShape(Tensor.java:703)\r\n\tat org.tensorflow.Tensor.create(Tensor.java:142)\r\n\tat org.tensorflow.Tensor.create(Tensor.java:115)\r\n\tat org.tensorflow.Tensors.create(Tensors.java:336)", "comments": ["@asimshankar Can you PTAL? Thanks!", "hi @asimshankar please have a look", "Hi @egolearner !\r\nWe are checking to see if you still need help in this issue , Have you tried latest stable version TF 2.6  yet?  Please post this issue in [TF forum](https://discuss.tensorflow.org/) for further assistance.  Thanks!", "I'm afraid I do not have the necessary environment to take a try.\r\nAnd I do not need this for the moment.", "Ok @egolearner! Closing this issue for now ,Please create a new one if you need further assistance.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26466\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26466\">No</a>\n", "try this:\r\nlong[] shape = {0,2};\r\nTensor.create(shape, LongBuffer.allocate(0));"]}, {"number": 26465, "title": "Wrong built-in precision metric for estimator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.13.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nDuring my experiment with binary classification, I've noticed a significant discrepancy between _tf.estimator.DNNClassifier_ built-in metric of _precision_ from the one computed by _tf.metrics.precision_.\r\n\r\n**Describe the expected behavior**\r\nIf the calculations were correct. The score of _precision_ should agree with that of _tf.metrics.precision_.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# tf.enable_eager_execution()\r\n\r\n# Metadata describing the text columns\r\nCSV_COLUMNS = \"...\".split(',')\r\n\r\nDEFAULTS = [...]\r\n\r\nKEY_COLUMN = 'KEY'\r\n\r\nLABEL_COLUMN = 'LABEL'\r\n\r\n# Create an input function reading a file using the Dataset API\r\n# Then provide the results to the Estimator API\r\ndef read_dataset(filename, mode, batch_size = 128):\r\n    def _input_fn():\r\n        def decode_csv(value_column):\r\n            columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\r\n            features = dict(zip(CSV_COLUMNS, columns))\r\n            \r\n            label = features.pop(LABEL_COLUMN)\r\n            \r\n            return features, label\r\n\r\n        # Create list of files that match pattern\r\n        file_list = tf.gfile.Glob(filename)\r\n\r\n        # Create dataset from file list\r\n        dataset = (tf.data.TextLineDataset(file_list).skip(1)  # Read text file, skip header\r\n                     .map(decode_csv))  # Transform each elem by applying decode_csv fn\r\n        \r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            num_epochs = None # indefinitely\r\n            dataset = dataset.shuffle(buffer_size=10*batch_size)\r\n            dataset = dataset.repeat(num_epochs).batch(batch_size)\r\n        else:\r\n            num_epochs = 1 # end-of-input after this\r\n            dataset = dataset.repeat(num_epochs).batch(10000)\r\n        \r\n        return dataset\r\n    return _input_fn\r\n\r\n# Define feature columns\r\ndef get_categorical(key, hash_bucket_size):\r\n    return tf.feature_column.categorical_column_with_hash_bucket(key, hash_bucket_size)\r\n\r\ndef get_numeric(key):\r\n    return tf.feature_column.numeric_column(key)\r\n\r\ndef get_cols():\r\n  # Define column types\r\n  return [\\\r\n             ...\r\ntf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity('PAXCOUNT',10)),\r\n             ...\r\n      ]\r\n\r\n# Create serving input function to be able to serve predictions later using provided inputs\r\ndef serving_input_fn():\r\n    feature_placeholders = {\r\n        ...\r\n        'PAXCOUNT': tf.placeholder(tf.int32, [None]),\r\n        ...\r\n        KEY_COLUMN: tf.placeholder_with_default(tf.constant(['nokey']), [None])\r\n    }\r\n    features = {\r\n        key: tf.expand_dims(tensor, -1)\r\n        for key, tensor in feature_placeholders.items()\r\n    }\r\n    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\r\n\r\n# I've defined additional metric here\r\ndef my_precision(labels, predictions):\r\n    return {'my_precision': tf.metrics.precision(labels, \r\n                                                 tf.strings.to_number(predictions['classes']))}\r\n\r\n# I've defined additional metric here\r\ndef my_recall(labels, predictions):\r\n    return {'my_recall': tf.metrics.recall(labels,\r\n                                           tf.strings.to_number(predictions['classes']))}\r\n\r\n# I've defined additional metric here\r\ndef my_f1_score(labels, predictions):\r\n    return {'my_f1_score': tf.contrib.metrics.f1_score(labels, \r\n                                                    tf.strings.to_number(predictions['classes']))}\r\n\r\nestimator = tf.estimator.DNNClassifier(    \r\n                   hidden_units = [256,128,64],\r\n                   feature_columns = get_cols(),\r\n                   model_dir = outdir,\r\n                   n_classes = 2, \r\n                   optimizer = tf.train.AdamOptimizer(learning_rate=0.001),\r\n                   batch_norm = False)\r\n    \r\n# Custom metric is added here\r\nestimator = tf.estimator.add_metrics(estimator, my_f1_score)\r\nestimator = tf.estimator.add_metrics(estimator, my_precision)\r\nestimator = tf.estimator.add_metrics(estimator, my_recall)\r\n\r\nestimator.train(input_fn=read_dataset('train.csv', mode = tf.estimator.ModeKeys.TRAIN), steps = 1000)\r\nestimator.evaluate(input_fn=read_dataset('eval.csv', mode = tf.estimator.ModeKeys.EVAL))\r\n```\r\n\r\n**Other info / logs**\r\n**Output of Evaluation**\r\n```\r\n{'accuracy': 0.73864836,\r\n 'accuracy_baseline': 0.5066931,\r\n 'auc': 0.8154568,\r\n 'auc_precision_recall': 0.8016961,\r\n 'average_loss': 0.5324211,\r\n 'label/mean': 0.5066931,\r\n 'loss': 7318.703,\r\n 'my_f1_score': 0.6467407,\r\n 'my_precision': 0.6260541,\r\n 'my_recall': 0.66884124,\r\n 'precision': 0.78366596,\r\n 'prediction/mean': 0.46659818,\r\n 'recall': 0.66884094,\r\n 'global_step': 100}\r\n```\r\nNotice _precision_ is 0.78 while _my_precision_ is 0.62, thats a big difference!\r\n\r\nAdditionally, I have also added _my_ recall_ and _my_f1 score_, using _tf.metrics.recall_ and _tf.contrib.metrics.f1_score_ respectively.\r\n\r\nTaking F1_Score = 2 * ((Precision * Recall) / (Precision + Recall)) to validate F1Score, it seems to agree with _my_precision_ instead of the built-in _precision_ metric.\r\n\r\nDoes anyone face a similar issue like the above?\r\n", "comments": ["@bingyuanlee Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Hi @jvishnuvardhan, I've modified the thread following the template.\r\nAppreciate your help! ", "Here's another example output using train_and_evaluate.\r\n\r\n`\r\nINFO:tensorflow:Saving dict for global step 10000: accuracy = 0.6664861, accuracy_baseline = 0.501966, auc = 0.72566515, auc_precision_recall = 0.70266324, average_loss = 0.6134248, global_step = 10000, label/mean = 0.498034, loss = 10034.449, my_f1_score = 0.41968825, my_precision = 0.31234384, my_recall = 0.639451, precision = 0.67412585, prediction/mean = 0.48334572, recall = 0.639451\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: model/context_rt/Seat_20190312-040131/model.ckpt-10000\r\n`", "@yhliang2018 -- can you comment on the differences here? My guess is that the default aggregation for add_metric is different than for DNNClassifier or something like that?", "@bingyuanlee the `precision` metric in evaluation is based `labels` and `class_ids`: https://github.com/tensorflow/estimator/blob/afa11287d6d01c6e44e31b54971fc5d1e4712a3d/tensorflow_estimator/python/estimator/canned/head.py#L1043, where `class_ids` is computed from the `logits`: https://github.com/tensorflow/estimator/blob/afa11287d6d01c6e44e31b54971fc5d1e4712a3d/tensorflow_estimator/python/estimator/canned/head.py#L1178; while your `my_precision` is based on `labels` and the `predictions['classes']` (which is from the predictions). That's why they produce different results.", "@yhliang2018 : Appreciate the response. I was intending to evaluate the model of its final predicted labels. However, how do we explain the values between `recall` and `my_recall` then, since both evaluate to identical values? Seems like we are facing a standardization issue here..", "@bingyuanlee Ah, I see the problem. Can you try `tf.contrib.estimator.add_metrics` for your `add_metrics` ? The `tf.estimator.add_metrics` API uses the metrics of TF 2.0, and may produce different results. ", "@yhliang2018 : Here's another sample result once I used `tf.contrib.estimator.add_metrics` instead. Apparently, this is not the cause of the issue. \r\n\r\n`INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.7981806, accuracy_baseline = 0.5024219, auc = 0.8651023, auc_precision_recall = 0.86191803, average_loss = 0.45824683, global_step = 1000, label/mean = 0.5024219, loss = 5030.3315, my_f1_score = 0.7808682, my_precision = 0.7731018, my_recall = 0.7887923, precision = 0.8054877, prediction/mean = 0.50340647, recall = 0.788792`", "Hi @bingyuanlee !We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26465\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26465\">No</a>\n"]}, {"number": 26464, "title": "segment erro:tensorflow/tensorflow/examples/label_image/main.cc", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):r1.13\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):0.23\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.0/7.0\r\n- GPU model and memory:1080ti/11gb\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\ntensorflow/tensorflow/examples/label_image/main.cc\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\ni think the reason is \"TF_RETURN_IF_ERROR(env->NewRandomAccessFile(filename, &file));\"\r\nhttps://github.com/tensorflow/tensorflow/blob/5fad2f50d610fdb765f4f99c7e0531b7a3ddbe94/tensorflow/examples/label_image/main.cc#L100", "comments": ["I make a mistake\r\nhere\r\nhttps://github.com/tensorflow/tensorflow/blob/5fad2f50d610fdb765f4f99c7e0531b7a3ddbe94/tensorflow/examples/label_image/main.cc#L103", "If I commente out this \uff0cit's ok.", "@allendred Is it resolved or still an issue? Could you provide more details on the issue and provide a code to reproduce the bug? Thanks!", "> @allendred Is it resolved or still an issue? Could you provide more details on the issue and provide a code to reproduce the bug? Thanks!\r\n\r\nI changged to another demo(detection),abondon the function (\"read_entirefiles\"),so it works", "@allendred Thanks for the confirmation. I will close the issue. Thanks!"]}, {"number": 26463, "title": "Load data tutorial model fails to converge", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:\r\n- Doc Link:\r\n\r\nsite/en/tutorials/load_data/images.ipynb\r\n\r\n*Describe the documentation issue**\r\nThe model fails to converge most of times. I changed the steps per epoch to run through all the data but it fails to converge. A lot of times just stuck in high loss low accuracy. \r\n\r\nI noted last layer output logits. It this expected?\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["BTW I ran only on colab with GPU. Yes I am aware this is a data loading tutorial but it would be nice to see it converges.", "@edwardCao Could you point out which tutorial you are referring to? Please let us know If there is any mismatch in what was mentioned in the website and what you are seeing. Thanks!", "site/en/tutorials/load_data/images.ipynb", "@edwardCao Thanks for the link. I think intention of the tutorials is to teach how to load image dataset using tf.data. In order keep the tutorial simple as well as take minimum time, it was developed like that. There are other tutorials that talk about how to improve performance of a DL model. Thanks!", "I am closing the issue. Thanks!"]}, {"number": 26462, "title": "glibc", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing this issue due to lack of information. Please provide all the information asked by the template and post an issue again from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!"]}, {"number": 26461, "title": "[TF2 Upgrade] contrib usage and several other TF 1.x APIs uncaught by TF2 upgrade script", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below): TF 2 nightly as of 3/7/19 (comparison version is 1.13.1\r\n- Python version: 3.4.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nIn order to make sure that my crusty old TF 1.x code works on TF 2, I'm using the `tf_upgrade_v2` script [as documented](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md). \r\n\r\nHowever, on the first bit of [code](https://gist.github.com/zmjjmz/2ee2090233217c0d1be2dc975e5c3a47) I tried I was told that it 'Detected 0 issues that require attention', and the report it gave was essentially blank. Additionally, the diff between the old file and the new file showed no changes!\r\n\r\nThis would have been great, except there's a few things that are very obviously incorrect.\r\n\r\n- On lines 42 and 43, there's a use of `tf.contrib.lookup` which should be moved to `tf.lookup`.\r\n- On line 63 there's a call to `tf.sparse_tensor_to_dense` which should be moved to `tf.sparse.to_dense`\r\n- On lines 118-121 there's a tf.Session context and a `tables_initializer` which no longer exist.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would hope that the above 4 issues would be pointed out by the upgrade script as needing my attention, even if it cannot (for some reason, although I imagine the one on line 63 can be done with a string replacement) fix them automatically. \r\n\r\nNote: I still haven't gotten this script (despite some manual upgrading) to work on TF2, but it (pre-manual upgrade) works fine on TF 1.13.1, although it gives me a bunch a bunch of deprecation warnings :)\r\n\r\n**Code to reproduce the issue**\r\n\r\nSee the attached [gist](https://gist.github.com/zmjjmz/2ee2090233217c0d1be2dc975e5c3a47)\r\n", "comments": ["The script currently assumes that import has the form \"import tensorflow as tf\". However, the snippet you linked just uses \"import tensorflow\".\r\nWe should definitely update documentation and may be look into supporting other import names. Thank you for catching that and sorry for the trouble!", "Ok, I can confirm that it works as expected when I do that.", "@zmjjmz,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "I think the only outstanding task was adding a note to documentation. Just checked our documentation here: https://www.tensorflow.org/guide/upgrade has this sentence:\r\n\"The script assumes that tensorflow is imported using `import tensorflow as tf.`\"\r\nSo, I am closing this issue.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26461\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26461\">No</a>\n"]}, {"number": 26460, "title": "GPU Device Selector in TensorFlow 2.0", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Happy to help as much as I can!\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTensorFlow 1.x support specifying GPU devices to use:\r\n```python\r\n# Horovod: pin GPU to be used to process local rank (one GPU per process)\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.visible_device_list = str(hvd.local_rank())\r\n```\r\n\r\nThere's no comparable API in TensorFlow 2.0.  The closest option is to use the `CUDA_VISIBLE_DEVICES` environment variable.  Unfortunately, `CUDA_VISIBLE_DEVICES` prevents processes from doing `cudaMemcpy` from/to devices not owned by the process.  There's a significant performance degradation when NCCL is used with P2P communication disabled.\r\n\r\nThe ask is to add an API to TensorFlow 2.0 to enable device selection.\r\n\r\n**Will this change the current api? How?**\r\nYes, will introduce an API to select GPU devices to use.\r\n\r\n**Who will benefit with this feature?**\r\nUsers of [Horovod](http://horovod.ai).\r\n\r\n**Any Other info.**\r\ncc @azaks2 @alextp @jaingaurav @guptapriya ", "comments": ["Duplicate of #25446", "@jaingaurav is a replacement for setting `visible_device_list` on your radar? I ask because I think Alex said there are some technical difficulties in implementing it. ", "@guptapriya: It is on my radar, but I need to still sync up with @alextp on the potential issues.", "A number of new API were added in `tf.config` namespace to support this use case. Please let me know if there is anything we missed regarding this specific issue.", "Thanks for the update, @jaingaurav!\r\n\r\nI have tried new functionality via `tensorflow/tensorflow:nightly-gpu-py3` and have a couple of questions.\r\n\r\nFirst, the API requires one to do `tf.config.experimental.list_physical_devices('GPU')`, filter that list and provide remnants to `tf.config.experimental.set_visible_devices(physical_devices[1:], 'GPU')`.\r\n\r\nDuring the list operation, TensorFlow creates a GPU context on every GPU, including ones that we're not planning to use.  You can see how this is wasteful if we will run 8 TensorFlow processes on 8-GPU server, each taking up ~120MB of GPU memory, totaling almost 1GB of wasted GPU memory.\r\n\r\nCould you add a way to set visible devices w/o binding GPU contexts?\r\n\r\nSecond, I noticed that our legacy usage of `config.gpu_options.visible_device_list = '0'` and `tf.enable_eager_execution(config=config)` has stopped working.  Is this intentional for 1.14?\r\n\r\n```\r\nroot@fc725ca05627:/# python\r\nPython 3.5.2 (default, Nov 12 2018, 13:43:14)\r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> config = tf.ConfigProto()\r\n>>> config.gpu_options.visible_device_list = '0'\r\n>>> tf.enable_eager_execution(config=config)\r\n>>> tf.constant(1)\r\n2019-04-19 07:12:06.657522: I tensorflow/stream_executor/platform/default/dso_loader.cc:43] Successfully opened dynamic library libcuda.so.1\r\n2019-04-19 07:12:13.742849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-04-19 07:12:14.238725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1589] Found device 0 with properties:\r\nname: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:00:04.0\r\ntotalMemory: 14.73GiB freeMemory: 14.60GiB\r\n2019-04-19 07:12:14.461207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-04-19 07:12:14.463142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1589] Found device 1 with properties:\r\nname: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:00:05.0\r\ntotalMemory: 14.73GiB freeMemory: 14.60GiB\r\n2019-04-19 07:12:14.880827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-04-19 07:12:14.881876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1589] Found device 2 with properties:\r\nname: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:00:06.0\r\ntotalMemory: 14.73GiB freeMemory: 14.60GiB\r\n2019-04-19 07:12:15.076517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-04-19 07:12:15.077554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1589] Found device 3 with properties:\r\nname: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\r\npciBusID: 0000:00:07.0\r\ntotalMemory: 14.73GiB freeMemory: 14.60GiB\r\n2019-04-19 07:12:15.085892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1712] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-04-19 07:12:15.089494: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-04-19 07:12:15.114457: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x68e92e0 executing computations on platform CUDA. Devices:\r\n2019-04-19 07:12:15.114490: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\r\n2019-04-19 07:12:15.114504: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\r\n2019-04-19 07:12:15.114511: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Tesla T4, Compute Capability 7.5\r\n2019-04-19 07:12:15.114517: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Tesla T4, Compute Capability 7.5\r\n2019-04-19 07:12:15.117618: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-04-19 07:12:15.121237: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6a80e90 executing computations on platform Host. Devices:\r\n2019-04-19 07:12:15.121270: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-04-19 07:12:15.121415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1712] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-04-19 07:12:15.121678: I tensorflow/stream_executor/platform/default/dso_loader.cc:43] Successfully opened dynamic library libcudart.so.10.0\r\n2019-04-19 07:12:15.127364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-19 07:12:15.127398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126]      0 1 2 3\r\n2019-04-19 07:12:15.127407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1139] 0:   N Y N N\r\n2019-04-19 07:12:15.127414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1139] 1:   Y N N N\r\n2019-04-19 07:12:15.127420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1139] 2:   N N N Y\r\n2019-04-19 07:12:15.127426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1139] 3:   N N Y N\r\n2019-04-19 07:12:15.128428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1260] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\r\n2019-04-19 07:12:15.128847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1260] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14202 MB memory) -> physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5)\r\n2019-04-19 07:12:15.129209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1260] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 14202 MB memory) -> physical GPU (device: 2, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5)\r\n2019-04-19 07:12:15.129622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1260] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 14202 MB memory) -> physical GPU (device: 3, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5)\r\n<tf.Tensor: id=0, shape=(), dtype=int32, numpy=1>\r\n>>> tf.config.experimental.get_visible_devices()\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\r\n>>>\r\n```", "@alsrgv: Are you sure that you the listing operation is causing the GPU memory to be used? The listing API was supposed to be a lightweight operation that would not involve any memory allocation. Did you experience this with the 1.0 or 2.0 nightly?\r\n\r\nRegarding the bug you mentioned with `tf.enable_eager_execution(config=config)`. This is a bug in the implementation. I'll look into it tomorrow and get a fix into the 1.14 branch.", "@jaingaurav, thanks for the quick response.  I did experience it in `tensorflow/tensorflow:nightly-gpu-py3`, which reports itself as `1.14.1-dev20190417`.  Do you expect 2.0-nightly behavior to be different?  Is there a docker image with 2.0-gpu-nightly?\r\n\r\nThe way I verify GPU memory usage is via `nvidia-smi`.  After running `tf.config.experimental.list_physical_devices('GPU')` in one of the terminals, I see memory usage on all GPUs in another:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      6287      C   python                                       119MiB |\r\n|    1      6287      C   python                                       119MiB |\r\n|    2      6287      C   python                                       119MiB |\r\n|    3      6287      C   python                                       119MiB |\r\n+-----------------------------------------------------------------------------+\r\n```", "Thanks for the details. I have a fix for the regression that I am getting through code review.\r\n\r\nLooking into the GPU memory allocation issue now. I was able to reproduce it locally.", "@alsrgv: The fix for `tf.enable_eager_execution(config=config)` has been merged. Could you let me know if it works for you, and I'll have it cherry-picked into 1.14.\r\n\r\nI am working on the GPU memory allocation issue. However, the fix requires quite a bit of code re-structuring. It seems we end up allocating memory as a function of querying CUDA capabilities.", "@jaingaurav, thanks for the fix!  I see the following error: `ValueError: Invalid visible device index: 0` in CPU environments.  Historically, using `visible_device_list = '0'` on CPU machine was a no-op, but now it's crashing.  Is it possible to avoid the crash in this scenario?\r\n\r\nLooking forward to the memory usage fix.  Memory usage could be caused by the creation of CUDA context.  If that's the case, [driver API](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DEVICE.html#group__CUDA__DEVICE) should allow querying device capabilities w/o CUDA context (and memory usage).", "@alsrgv: Thanks for the previous behavior. I'll ensure that I maintain compatibility. I've got the memory issue almost fixed. One last change needed.\r\n\r\nIn terms of releases, I will ensure the regression is cherry-picked into 1.14. However, for the memory issue, can it wait till 1.15 & 2.0 or would you like it for 1.14 as well? Just depends on what you'd like to support.", "@jaingaurav, looking forward to the fixes!\r\n\r\nIt would be great if memory issue fix can be picked in 1.14 as well.  1.13 did have memory issue with XLA (it was binding memory on all devices as well), and it was causing out of memory issues with cuDNN.  So there are no release w/o memory issues since 1.12.", "@alsrgv: The XLA issues has been fixed with a427c1361a281dff6e6cd55a39bb84116314d8fb correct? If so, it'll be in the 1.14 branch.", "@jaingaurav, yes, that's it - hence the request to pick the fix for this memory issue into 1.14 branch as well.", "@alsrgv: All known issues should be fixed in tonight's nightly. Once you confirm the behavior, I will speak to the release team about trying to get the memory fixes into 1.14. The changes weren't too bad, but they do incur some risk to get into the release.", "@jaingaurav, thanks for the fixes!\r\n\r\nI just tried https://files.pythonhosted.org/packages/e1/c6/6cde177c97e975d3c0aa36a7df87b353e8cfa26660735f6668d314106d81/tf_nightly_gpu-1.14.1.dev20190426-cp37-cp37m-manylinux1_x86_64.whl.\r\n\r\nThe memory leak with `tf.config.experimental.list_physical_devices('GPU')` is fixed :thumbsup:\r\n\r\nI'm still getting an error with `visible_device_list` in absence of GPUs:\r\n```\r\n(env) root@153afad1fa58:/# CUDA_VISIBLE_DEVICES='' python\r\nPython 3.7.3 (default, Mar 26 2019, 00:55:50)\r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> config = tf.ConfigProto()\r\n>>> config.gpu_options.visible_device_list = '0'\r\n>>> tf.enable_eager_execution(config=config)\r\n>>> tf.constant(1)\r\n2019-04-26 06:20:39.548228: I tensorflow/stream_executor/platform/default/dso_loader.cc:43] Successfully opened dynamic library libcuda.so.1\r\n2019-04-26 06:20:44.481123: E tensorflow/stream_executor/cuda/cuda_driver.cc:320] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2019-04-26 06:20:44.481316: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:166] retrieving CUDA diagnostic information for host: 153afad1fa58\r\n2019-04-26 06:20:44.481341: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:173] hostname: 153afad1fa58\r\n2019-04-26 06:20:44.481492: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 410.72.0\r\n2019-04-26 06:20:44.481544: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 410.72.0\r\n2019-04-26 06:20:44.481555: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:308] kernel version seems to match DSO: 410.72.0\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/env/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 180, in constant_v1\r\n    allow_broadcast=False)\r\n  File \"/env/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 254, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/env/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File \"/env/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 405, in ensure_initialized\r\n    config_str = self.config.SerializeToString()\r\n  File \"/env/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 672, in config\r\n    self._initialize_physical_devices()\r\n  File \"/env/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 917, in _initialize_physical_devices\r\n    self._import_config()\r\n  File \"/env/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 971, in _import_config\r\n    raise ValueError(\"Invalid visible device index: %s\" % index)\r\nValueError: Invalid visible device index: 0\r\n>>>\r\n```\r\n\r\nI tried this on CPU build with the same outcome.", "Thanks @alsrgv. From the looks of it that nightly build might not have that latest change yet. We can re-verify in the next build.", "@jaingaurav, I just built the master from source and can confirm it works, thanks!", "@jaingaurav thanks a lot for this improvement! I can verify that it also fixes another old issue at https://github.com/tensorflow/tensorflow/issues/8136#issuecomment-361727732.\r\nThis code:\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.config.experimental.list_physical_devices('GPU'))\r\ncfg = tf.ConfigProto()\r\ncfg.gpu_options.visible_device_list = '1'\r\nsess = tf.Session(config=cfg)   # do not fail\r\n```\r\ndo not fail when running on a 2-gpu machine. But it would fail if using `tf.test.is_gpu_available()` instead of `list_physical_devices`.\r\n\r\nAny chance we can have this great improvement into `tf.test.is_gpu_available()`?", "@ppwwyyxx: This is exactly why the new API was created. Unfortunately any changes to `tf.test.is_gpu_available()` would possibly break backwards compatibility. Hence, I'd prefer if you used the new APIs. Also, unless you are writing test cases, I'd probably avoid using that API.", "@jaingaurav, any news whether these fixes can be picked into r1.14?  We'd really like a release since 1.12.x that has correct GPU memory binding behavior.\r\n\r\ncc @martinwicke ", "Cherry-picking these bug fixes makes sense I think if the release is still\nnot too advanced.\n\nOn Mon, Apr 29, 2019 at 12:11 PM Alex Sergeev <notifications@github.com>\nwrote:\n\n> @jaingaurav <https://github.com/jaingaurav>, any news whether these fixes\n> can be picked into r1.14? We'd really like a release since 1.12.x that has\n> correct GPU memory binding behavior.\n>\n> cc @martinwicke <https://github.com/martinwicke>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26460#issuecomment-487703614>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNOWJIJREFDXPMTX53PS5B43ANCNFSM4G4Q2REA>\n> .\n>\n\n\n-- \n - Alex\n", "@alsrgv: We had a chat about it this morning. Given the status of the 1.14, we're going to aim to cherry-pick the memory fixes. We'd greatly appreciate any testing that you can do to help ensure that we don't incur any regressions and that we have everything you need in 1.14. Thank you for all that you have done so far!", "@jaingaurav, perfect, thanks!  I will test 1.14 RCs as they come out.", "@jaingaurav Hi, I try to use device selector with tf2, but there are still some problems:\r\n<pre>\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: tf.__version__\r\nOut[2]: '2.0.0-dev20190606'\r\n\r\nIn [3]: gpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\n\r\nIn [4]: gpus\r\nOut[4]:\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\r\n PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\r\n\r\nIn [5]: tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\n\r\nIn [6]: tf.config.experimental.set_memory_growth(gpus[0], True)\r\n\r\nIn [7]: tf.constant(1)\r\n...\r\nntext.py in _compute_gpu_options(self)\r\n    851       memory_growths = set(self._memory_growth_map.values())\r\n    852       if len(memory_growths) > 1:\r\n--> 853         raise ValueError(\"Memory growth cannot differ between GPU devices\")\r\n    854       allow_growth = memory_growths.pop()\r\n    855     else:\r\n\r\nValueError: Memory growth cannot differ between GPU devices\r\n</pre>", "@llan-ml: Please see the updated guide at https://www.tensorflow.org/beta/guide/using_gpu#limiting_gpu_memory_growth. Currently we require the memory growth option to be uniform across all GPUs. This may change in the future if someone were to implement the changes.", "@jaingaurav What I mean is that after I have selected a GPU by calling `tf.config.experimental.set_visible_devices(gpus[0], 'GPU')`, it still raises `ValueError: Memory growth cannot differ between GPU devices`.\r\n\r\nFor now, I still have to select a specific GPU by setting `CUDA_VISIBLE_DEVICES` in a multi-gpu machine.", "@llan-ml: Thanks, that is indeed a valid bug. The fix has been pushed now and I'll ensure it makes it into the upcoming 1.14 release.", "@jaingaurav I am seeing a lot of memory issues when using depthwise conv2d native. Seems like it does not respect the session config visible devices list and grabs all the GPUs. You mentioned this above - \"I am working on the GPU memory allocation issue. However, the fix requires quite a bit of code re-structuring. It seems we end up allocating memory as a function of querying CUDA capabilities.\" I am wondering if these fixes made their way to TF 1.14. Can you point me to any PRs with these fixes? Thanks!", "@pidajay: How are you querying the GPUs, could you share a code snippet? Note this issue was primarily focused on querying GPUs with eager execution. If you are using sessions and experiencing issues there might be something else going on.\r\n\r\nHere is a tutorial on how to use the new APIs:\r\nhttps://www.tensorflow.org/beta/guide/using_gpu#limiting_gpu_memory_growth", "@jaingaurav Appreciate the response. I am using an estimator (TPU estimator but running on GPU). Single GPU is fine but problem shows up when distributing across multiple GPUs (I use horovod. But horovod does not seem to be the issue here). I create the visible device list as follows at the beginning of the program\r\n```\r\nsess_config=tf.ConfigProto()\r\nsess_config.gpu_options.allow_growth = True\r\nsess_config.gpu_options.visible_device_list = str(hvd.local_rank())\r\n```\r\nBut I notice that the estimator violates the visible device list above and allocates all the GPUs. And this happens only when using depthwise conv 2D. \r\nI guess this thread is related to eager mode and probably not the right thread. If you can't think of anything on top of your head, I will try to create a new issue with a tiny example reproducing the problem. Thanks!", "Hi @jaingaurav It seems that disabling all GPUs does not work properly. I ran the following code\r\n```\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: tf.__version__\r\nOut[2]: '2.0.0-dev20190725'\r\n\r\nIn [3]: physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n2019-07-28 03:12:44.161668: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-07-28 03:12:44.243318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\r\npciBusID: 0000:3b:00.0\r\n2019-07-28 03:12:44.244091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\r\npciBusID: 0000:af:00.0\r\n2019-07-28 03:12:44.244418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-28 03:12:44.246004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-28 03:12:44.247384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-07-28 03:12:44.247752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-07-28 03:12:44.249599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-07-28 03:12:44.251015: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-07-28 03:12:44.255340: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-28 03:12:44.258154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n\r\nIn [4]: tf.config.experimental.set_visible_devices([], 'GPU')\r\n\r\nIn [5]: visible_devices = tf.config.experimental.get_visible_devices()\r\n\r\nIn [6]: visible_devices\r\nOut[6]: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\r\n\r\nIn [7]: x = tf.Variable(1.0)\r\n2019-07-28 03:13:21.109555: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-07-28 03:13:22.043938: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e7a5774b90 executing computations on platform CUDA. Devices:\r\n2019-07-28 03:13:22.044006: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0\r\n2019-07-28 03:13:22.044029: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla V100-PCIE-16GB, Compute Capability 7.0\r\n2019-07-28 03:13:22.067710: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz\r\n2019-07-28 03:13:22.074734: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e7a994ac40 executing computations on platform Host. Devices:\r\n2019-07-28 03:13:22.074809: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-28 03:13:22.074977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-28 03:13:22.075003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]\r\n\r\nIn [8]: x.device\r\nOut[8]: '/job:localhost/replica:0/task:0/device:CPU:0'\r\n\r\nIn [9]: !nvidia-smi\r\nSun Jul 28 03:13:56 2019\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\r\n| N/A   30C    P0    41W / 250W |    418MiB / 16130MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |\r\n| N/A   35C    P0    42W / 250W |    418MiB / 16130MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     52456      C   ...2.0/envs/tf-nightly-2.0-0725/bin/python   407MiB |\r\n|    1     52456      C   ...2.0/envs/tf-nightly-2.0-0725/bin/python   407MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nAlthough the variable `x` is placed on CPU, the process still occupies some gpu memory. The expected behavior is that running `nvidia-smi` does not show any process info.   ", "What if users want specific 2 GPUs outof 4:\r\n\r\n```\r\ntf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\n```\r\n\r\nHow to enable 2 of them.", "As the [documentation said](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_visible_devices), you can give it a list of devices."]}, {"number": 26459, "title": "TensorFlow 1.13.1 with Docker under Raspbian Stretch", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Docker version 18.09.0, build 4d60db4 on top of Linux stf-provider 4.14.34-hypriotos-v7+ #1 SMP Sun Apr 22 14:57:31 UTC 2018 armv7l GNU/Linux. Raspbian is 9.8 in the Docker container.\r\n- TensorFlow installed from binary: `pip3 install tensorflow`\r\n- TensorFlow version: 1.13.1 \r\n- Python version: 3.5.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: nope\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```bash\r\nroot@ed47bf54107c:/tmp# python3 -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\n```\r\n\r\ngives\r\n\r\n```bash\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\r\n  return f(*args, **kwds)\r\ntf.Tensor(-611.39856, shape=(), dtype=float32)\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nIf you ever want to give it a try, here is the image:\r\n\r\n```bash\r\ndocker pull gounthar/tensorflow-armv7:1.13\r\n```\r\n\r\nI tried another image from the Data Science Docker Stack:\r\n\r\n```bash\r\nfedec07fb96b        elswork/tf-opencv:latest   \"/bin/bash\"              4 minutes ago       Up 4 minutes                                        practical_dirac\r\n```\r\nand I got the same kind of error:\r\n\r\n```bash\r\nroot@fedec07fb96b:/# python3 tensorFlow.py\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\r\n  return f(*args, **kwds)\r\n```\r\n", "comments": ["I have the same problem as mentioned in this issue https://github.com/tensorflow/tensorflow/issues/23840\r\n\r\n....ticket is closed without solution.\r\n", "The warnings shouldn't result in any bad behavior, they're just a nuisance. We have recently updated our RPi pip packages to Py 3.7, so you should no longer see the log warnings on that version. Closing, since it doesn't result in any actual errors."]}, {"number": 26458, "title": "TypeError in test files", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nI encountered these when running tests after building tensorflow using bazel on Windows:\r\n```\r\n======================================================================\r\nERROR: testLossesForwarded (__main__.ListTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"%PREFIX%\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 946, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"%PREFIX%\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 1105, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"\\\\?\\C:\\Users\\nwani\\AppData\\Local\\Temp\\Bazel.runfiles_iqsmzf36\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\training\\checkpointable\\data_structures_test.py\", line 128, in testLossesForwarded\r\n    model = HasList()\r\n  File \"\\\\?\\C:\\Users\\nwani\\AppData\\Local\\Temp\\Bazel.runfiles_iqsmzf36\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\training\\checkpointable\\data_structures_test.py\", line 60, in __init__\r\n    list(sequence=[core.Dense(11)]) + [core.Dense(12)]))\r\nTypeError: list() takes no keyword arguments\r\n\r\n======================================================================\r\nERROR: testTracking (__main__.ListTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"%PREFIX%\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 946, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"%PREFIX%\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 1105, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"\\\\?\\C:\\Users\\nwani\\AppData\\Local\\Temp\\Bazel.runfiles_iqsmzf36\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\training\\checkpointable\\data_structures_test.py\", line 78, in testTracking\r\n    model = HasList()\r\n  File \"\\\\?\\C:\\Users\\nwani\\AppData\\Local\\Temp\\Bazel.runfiles_iqsmzf36\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\training\\checkpointable\\data_structures_test.py\", line 60, in __init__\r\n    list(sequence=[core.Dense(11)]) + [core.Dense(12)]))\r\nTypeError: list() takes no keyword arguments\r\n```", "comments": ["@nehaljwani Could you provide more details on the issue and context? Could you check whether it is persisting with the TF.20? Thanks!", "I am curious, do you folks not test your builds before making a release/releasing wheels? \r\n\r\nThis is what I ran:\r\n```\r\nbazel --batch test -c opt $BUILD_OPTS -k --test_output=errors \\\r\n  --define=no_tensorflow_py_deps=true --test_lang_filters=py \\\r\n  --build_tag_filters=-no_pip,-no_windows,-no_oss --build_tests_only \\\r\n  --test_timeout 9999999 --test_tag_filters=-no_pip,-no_windows,-no_oss \\\r\n  -- //${PY_TEST_DIR}/tensorflow/python/... \\\r\n     //${PY_TEST_DIR}/tensorflow/contrib/...\r\n```", "@nehaljwani Sorry for the typo. I meant TF 2.0. We do lot of testing before releasing. Thanks!", "@nehaljwani Thanks for the report.\r\nWhile we ran the tests, we sometimes skip such issues as this is only a bug in the test because of a recent change in python 3.7. Our larger scale tests showed that this does not affect any of the production code paths in tensorflow, so we decided to fix it after.\r\nIf you like to contribute a fix, we would be happy to accept.", "@nehaljwani I took a look at the master branch. It seems the issue has been fixed. I will close the issue for now but feel free to reopen if the issue persists.", "Thank you very much!"]}, {"number": 26457, "title": "[ROCm] Enable ROCm support for \"softplus_op\"", "body": "This PR enables ROCm support for the \"softplus_op\".\r\n\r\nPR #25676  is a pre-requisite for this PR. Since that is not yet merged the commits for that PR are included in this one as well (the first 6 commits), but should be ignored for review purposes.\r\n\r\nOf the 4 commits exclusive to this PR, the first 3 are for minor fixes required for dynamic loading of ROCm runtime libraries. Only the last commit pertains to the \"softplus_op\".\r\n\r\n@tatianashp : just FYI\r\n@whchung ", "comments": ["@rthadur, hey, as in\r\n\r\n  https://github.com/tensorflow/tensorflow/pull/25796#issuecomment-465241088\r\n  https://github.com/tensorflow/tensorflow/pull/25676#issuecomment-462574555\r\n  https://github.com/tensorflow/tensorflow/pull/26342#issuecomment-469904659\r\n\r\nbecause this does not touch XLA, I am not the right person to review this code.\r\n\r\nIs there a doc somewhere that I can update that will help you find the right person more easily for these changes?", "> @rthadur, hey, as in\r\n> \r\n> [#25796 (comment)](https://github.com/tensorflow/tensorflow/pull/25796#issuecomment-465241088)\r\n> [#25676 (comment)](https://github.com/tensorflow/tensorflow/pull/25676#issuecomment-462574555)\r\n> [#26342 (comment)](https://github.com/tensorflow/tensorflow/pull/26342#issuecomment-469904659)\r\n> \r\n> because this does not touch XLA, I am not the right person to review this code.\r\n> \r\n> Is there a doc somewhere that I can update that will help you find the right person more easily for these changes?\r\n\r\n@jlebar will make a note of it and make sure will assign correct person.", "Hi Deven, PR #25676 has landed (finally, thanks for your patience). Would you mind to sync this one to HEAD? I can start reviewing it early next week. Thank you!", "@chsigg \r\n\r\nrebasing done....much smaller PR now.   Also most of the changes are \"internal\" to ROCm. \r\n\r\nThanks again for getting the ROCm PRs reviewed and merged.\r\n\r\ndeven\r\n", "@chsigg \r\n\r\nThank you for getting this merged.\r\n\r\nThe merge process seems to have picked up the wrong line for one the commits.\r\n\r\nchange in this PR : https://github.com/tensorflow/tensorflow/pull/26457/files?file-filters%5B%5D=.bzl#diff-38c70d53f71218cab252a7f458600d6dR1212\r\n\r\nchange that actually got commited : https://github.com/tensorflow/tensorflow/commit/500d747dadaa6eb3cf11f9190f741a500a751ce4#diff-38c70d53f71218cab252a7f458600d6dR1237\r\n\r\nIt seems like the merge was off by one line.\r\n\r\nI will fix this when I rebase the PR for `softsign` op.\r\n"]}, {"number": 26456, "title": "sparsetensor-input in the form of ids and values as FeatureColumns for tf.estimator use", "body": "I have been struggling with this issue for a while and in TF Dev Summit 2019, also brought it with @mrry who helped me a lot but still it is a fully unresolved issue.\r\n\r\nI have input data in the form of ids and values; that is sparse representation for a giant input feature whose non-zero element indices are stored in ids and corresponding values are stored in values. I would like to use estimators but estimators require feature_column format as high-level APIs to dead with my input and currently I don't see how I can use feature-columns with my data format---please note I already have done all the feature engineering and representations and stored them in my ids, values so no need for bucketizzation, and other feature transformations that feature_columns are used for. Also the inputs are in the form of sparsetensors since I am dealing with a giant feature-vector and sparse representation will save a lot of computation and memory. How can I do it with estimators and go around the ecisting feature_column format?\r\n", "comments": ["Just in case here is a simple demonstration to show that I can use this sparse input for training without using estimators but when it comes to estimators I don't now how to do it using feature columns.\r\n\r\nSay we already have stored tfrecords dataset in a file. The data is stored in the form of dictionary with keys such as labels, ids and values--we generate mini-batches after calling .batch and parse data using .map-- then:\r\n\r\n```\r\niterator= dataset.make_one_shot_iterator()\r\nfeatures=iterator.get_next()\r\nlabels =features[\"label\"]\r\nsparse_ids = features[\"ids\"]\r\nsparse_values = features[\"values\"]\r\n```\r\n\r\nNow I can call the following function to do dot product needed for a simple linear regression:\r\n\r\n```\r\ndef sparse_dot_product(sparse_ids,\r\n                          sparse_values,\r\n                          weights_shape,\r\n                          biases_shape,\r\n                          is_train=True):\r\n    weights = tf.get_variable(\r\n        \"weights\", weights_shape, initializer=tf.initializers.zeros())\r\n    biases = tf.get_variable(\r\n        \"biases\", biases_shape, initializer=tf.initializers.zeros())\r\n    return tf.nn.embedding_lookup_sparse(\r\n        weights, sparse_ids, sparse_values, combiner=\"sum\") + biases\r\n```\r\n\r\nand then can do logistic regression training using tf.nn.sparse_softmax_cross_entropy_with_logits() and tf.reduce_mean(..) functions. Very simple and fast since I am doing lookup! \r\n\r\nNow using estimators is supposed to be easier, but to me it doesn't, due to the current forms of feature_columns. Any help will be appreciated\r\n", "I actually found the solution! custom estimators are the way to implement this using model_fn as follows:\r\n\r\n```\r\ndef my_model_fn(features,labels, mode):\r\n  batch_ids = features['ids']\r\n  batch_values = features['values']\r\n  batch_labels = labels\r\n  logits= your_inference_function(batch_ids, batch_values)\r\n\r\n  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n                                                              logits=logits, labels=batch_labels\r\n                                                              )\r\n  loss = tf.reduce_mean(cross_entropy, name=\"loss\")\r\n  optimizer = get_your_optimizer , e.g. rmsprop\r\n  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n  tf.get_variable_scope().reuse_variables()\r\n\r\n --now Define accuracy op for train data\r\n  train_accuracy_logits = inference(batch_ids, batch_values, True)\r\n  train_softmax = tf.nn.softmax(train_accuracy_logits)\r\n  train_correct_prediction = tf.equal(\r\n                              tf.argmax(train_softmax, 1), batch_labels\r\n                              )\r\n  train_accuracy = tf.reduce_mean(\r\n                              tf.cast(train_correct_prediction, tf.float32)\r\n                              )\r\n  \r\n\r\n  return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)`\r\n```\r\nthen use: \r\n```\r\nclassifier = tf.estimator.Estimator(\r\n        model_fn=my_model_fn)\r\n\r\n\r\nclassifier.train(\r\n         input_fn=lambda:input_fn(),\r\n         steps=steps_to_run)\r\n```", "Thanks for posting solution also. I am closing the issue. Thanks!", "Could we reopen this issue? The solution proposed by @hamidmaei is a legitimate workaround, but it doesn't solve the underlying incompatibility between sparse tensors and feature columns. Using sparse tensor representations of feature counts is a common use case (e.g. bag-of-words models) and it should be possible to plug these sparse tensors directly into a premade Estimator, but it seems like this currently isn't possible with feature columns.", "@tddevlin Can you please open a new issue and mention about this old issue. Can you please provide a simple standalone code to reproduce the issue? Thanks!"]}, {"number": 26455, "title": "where are the conflow flow ops like abort used?", "body": "Previous, TF control flow primitives are Switch, Merge, Enter, NextIteration, and Exit, we can see them in a simple while_loop graph. Currently, in your r1.13 and r2.0 [docs](https://www.tensorflow.org/versions/r2.0/api_docs/cc/group/control-flow-ops), seems the control flow ops changed a lot, how could I see this ops in a graphdef? are they still in while_loop and cond?", "comments": ["Tried to use while_loop and tf 2.0, the graphdef is still the same as previous tf versions, haven't seen abort, RefNextIteration, RefSelect, RefSwitch. Please help share a simple model example which uses these ops in your new docs.\r\n\r\n", "FYI, in general, questions like these are best asked on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow), since we GitHub issues to track bugs and features.\r\n\r\nThe docs you linked to are to the C++ API, which contains many lower-level (and sometimes outdated ops) than the Python API. For example, in TF 2.0, the Python API no longer produces Ref ops. So you won't see all the control flow ops still included in the C++ API when using the Python API.\r\n\r\nThese resources should help you learn more about control flow in the TF Python API:\r\nNew tf.cond doc: https://github.com/tensorflow/community/blob/master/rfcs/20180507-cond-v2.md\r\nNew tf.while doc: https://github.com/tensorflow/community/blob/master/rfcs/20180821-differentiable-functional-while.md\r\nOld-style control flow doc (using switch, merge, etc.): http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf", "@skye  thanks!"]}, {"number": 26454, "title": "[Bug report] wrong container setting in OpsTestBase::AddResourceInput, easily fix", "body": "File: tensorflow/core/kernels/ops_testutil.h\r\nClass: OpsTestBase\r\nFunction: [AddResourceInput](https://github.com/wendy2003888/tensorflow/commit/18b867eabd1bb8f3540ee56a8f47f96d9f3bc20d)\r\n\r\nresource container set to empty when using default container.\r\n\r\nDue to CLA problem,  I can't contribute rn. \r\nPlease review pull reques [#26428](https://github.com/tensorflow/tensorflow/pull/26428)  from ppwwyyxx. \r\n\r\nThank you\r\n\r\n<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary (python3 pip)\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source):  5.4.0\r\n- CUDA/cuDNN version: 10.0 \r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 26453, "title": "Allow py_function to support functions that return RaggedTensor", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`py_function` only supports functions that return `Tensor`s. However the wrapped function is executed in eager mode and therefore ideally should support other return types consistent with eager mode. As of tf-1.13.1, `py_function` attempts to convert the returned objects to `Tensor`s, but a `RaggedTensor` cannot be converted directly to a `Tensor`. Any attempt to return a `RaggedTensor` raises an exception during the attempted conversion. E.g.,\r\n\r\n```python\r\nwith tf.Graph().as_default():\r\n    elements = [[1., 2., 3.], [4., 5.]]\r\n    ragged1 = tf.ragged.constant(elements)\r\n    def py_func():\r\n        return tf.ragged.constant(elements)\r\n    \r\n    ragged2 = tf.py_function(\r\n        py_func, [], tf.dtypes.float32\r\n    )\r\n    with tf.Session() as sess:\r\n        print(sess.run(ragged1))\r\n        print(sess.run(ragged2))\r\n\r\n>>> <tf.RaggedTensorValue [[1.0, 2.0, 3.0], [4.0, 5.0]]>\r\n>>> ...\r\n>>> ValueError: TypeError: object of type 'RaggedTensor' has no len()\r\n```\r\n\r\nI propose that `py_function` detect which output arguments, if any, are `RaggedTensor`s and returns them without attempting to convert to `Tensor`s. If the proposal is rejected, I suggest that the documentation is updated to make clearer (either in the API or guides) that a `RaggedTensor` is not a suitable return type for functions wrapped by `py_function`. \r\n\r\n**Will this change the current api? How?** No.\r\n\r\n**Who will benefit with this feature?** Anyone who uses `RaggedTensor` in conjunction with `py_function`.\r\n\r\n**Any Other info.**\r\nAs a workaround, one can construct a `RaggedTensor` from the output of `py_function`. E.g.,\r\n\r\n```python\r\nwith tf.Graph().as_default():\r\n    elements = [[1., 2., 3.], [4., 5.]]\r\n    ragged1 = tf.ragged.constant(elements)\r\n    def py_func():\r\n        lengths = [len(element) for element in elements]\r\n        return sum(elements, []), lengths\r\n    \r\n    concatenated, lengths = tf.py_function(\r\n        py_func, [], [tf.dtypes.float32, tf.dtypes.int64]\r\n    )\r\n    ragged2 = tf.RaggedTensor.from_row_lengths(concatenated, lengths)\r\n    with tf.Session() as sess:\r\n        print(sess.run(ragged1))\r\n        print(sess.run(ragged2))\r\n\r\n>>> <tf.RaggedTensorValue [[1.0, 2.0, 3.0], [4.0, 5.0]]>\r\n>>> <tf.RaggedTensorValue [[1.0, 2.0, 3.0], [4.0, 5.0]]>\r\n```\r\n", "comments": ["In https://github.com/tensorflow/tensorflow/issues/27679#issuecomment-522578000, I showed how py_function could be extended to handle composite tensor inputs and outputs (and other nested structures, like dicts, tuples, lists, etc).  If you have bandwidth to work on a PR that adds that to TensorFlow (with tests etc.), then it would be very welcome; otherwise, you could just use the `new_py_function` that I defined there, which wraps `tf.py_function`.", "The issue should have been fixed by @edloper. The following example should work.\r\n```Python\r\nimport tensorflow\r\n\r\n# TF1\r\ntf = tensorflow.compat.v1\r\nwith tf.Graph().as_default():\r\n    elements = [[1., 2., 3.], [4., 5.]]\r\n    ragged1 = tf.ragged.constant(elements)\r\n    def py_func():\r\n        return tf.ragged.constant(elements)\r\n    \r\n    ragged2 = tf.py_function(\r\n        py_func, [], Tout=tf.RaggedTensorSpec([2, None], tf.float32)\r\n    )\r\n    with tf.Session() as sess:\r\n        print(sess.run(ragged1))  # <tf.RaggedTensorValue [[1.0, 2.0, 3.0], [4.0, 5.0]]>\r\n        print(sess.run(ragged2))  # <tf.RaggedTensorValue [[1.0, 2.0, 3.0], [4.0, 5.0]]>\r\n\r\n# TF2\r\ntf = tensorflow.compat.v2\r\ndef py_func():\r\n  return tf.ragged.constant([[1., 2., 3.], [4., 5.]])\r\n\r\nprint(tf.py_function(py_func, [], Tout=tf.RaggedTensorSpec([2, None], tf.float32)))  # <tf.RaggedTensor [[1.0, 2.0, 3.0], [4.0, 5.0]]>\r\n```\r\n\r\nNote the main difference of this example with the example in the original issue is the `Tout` argument of `tf.py_function`. When `tf.py_function` returns a `CompositeTensor`, the `Tout` argument should be a subclass of `tf.TypeSpec`, i.e., `tf.RaggedTensorSpec` for a `RaggedTensor`.\r\n\r\nSee https://www.tensorflow.org/api_docs/python/tf/py_function for more information on `tf.py_function` and https://www.tensorflow.org/api_docs/python/tf/TypeSpec on `tf.TypeSpec`.", "> The issue should have been fixed by @edloper. The following example should work.\r\n> \r\n> ```python\r\n> import tensorflow\r\n> \r\n> # TF1\r\n> tf = tensorflow.compat.v1\r\n> with tf.Graph().as_default():\r\n>     elements = [[1., 2., 3.], [4., 5.]]\r\n>     ragged1 = tf.ragged.constant(elements)\r\n>     def py_func():\r\n>         return tf.ragged.constant(elements)\r\n>     \r\n>     ragged2 = tf.py_function(\r\n>         py_func, [], Tout=tf.RaggedTensorSpec([2, None], tf.float32)\r\n>     )\r\n>     with tf.Session() as sess:\r\n>         print(sess.run(ragged1))  # <tf.RaggedTensorValue [[1.0, 2.0, 3.0], [4.0, 5.0]]>\r\n>         print(sess.run(ragged2))  # <tf.RaggedTensorValue [[1.0, 2.0, 3.0], [4.0, 5.0]]>\r\n> \r\n> # TF2\r\n> tf = tensorflow.compat.v2\r\n> def py_func():\r\n>   return tf.ragged.constant([[1., 2., 3.], [4., 5.]])\r\n> \r\n> print(tf.py_function(py_func, [], Tout=tf.RaggedTensorSpec([2, None], tf.float32)))  # <tf.RaggedTensor [[1.0, 2.0, 3.0], [4.0, 5.0]]>\r\n> ```\r\n> \r\n> Note the main difference of this example with the example in the original issue is the `Tout` argument of `tf.py_function`. When `tf.py_function` returns a `CompositeTensor`, the `Tout` argument should be a subclass of `tf.TypeSpec`, i.e., `tf.RaggedTensorSpec` for a `RaggedTensor`.\r\n> \r\n> See https://www.tensorflow.org/api_docs/python/tf/py_function for more information on `tf.py_function` and https://www.tensorflow.org/api_docs/python/tf/TypeSpec on `tf.TypeSpec`.\r\n\r\nThanks for your brief tutorial, but the given code will not work, if I convert it into the lambda function.\r\n![image](https://user-images.githubusercontent.com/52521165/144815600-6c2b84e2-fb2d-4e4d-9a9d-85751a3d59c3.png)\r\n\r\n![image](https://user-images.githubusercontent.com/52521165/144815642-0054a429-fa1a-4f3f-9ad4-2bab8e8fc7cc.png)\r\n\r\n![image](https://user-images.githubusercontent.com/52521165/144815573-c24bda40-1f4d-4f30-b521-5e1f18741804.png)\r\n\r\nlook forward to see the further improvement of tf..\r\n", "@HuangChiEn Are you using TensorFlow 2.7?  Support for using composite tensors (such as `RaggedTensor`) with `py_function` was added with 2.7, so if you're using an earlier version of TensorFlow, then it won't work.  I tried executing your code as written:\r\n\r\n```python\r\ntmp = lambda _: tf.ragged.constant([[1., 2., 3.], [4., 5.]])\r\ntf.py_function(tmp, [], Tout=tf.RaggedTensorSpec([2, None], tf.float32))\r\n```\r\n\r\nAnd it failed with \"`<lambda>() missing 1 required positional argument: '_'`\" (which is expected, since your lambda takes one argument, but you didn't supply any arguments when you called `tf.py_function`).  If I change it the `lambda` to not expect any argument:\r\n\r\n```python\r\ntmp = lambda: tf.ragged.constant([[1., 2., 3.], [4., 5.]])\r\ntf.py_function(tmp, [], Tout=tf.RaggedTensorSpec([2, None], tf.float32))\r\n```\r\n\r\nThen it succeeds for me (in TF 2.7).\r\n"]}, {"number": 26452, "title": "cpu tensorflow win: 7, DLL load failed with error code -1073741795", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): https://www.tensorflow.org/install/pip\r\n TensorFlow version: 1.12\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):-\r\n- GCC/Compiler version (if compiling from source):-\r\n- CUDA/cuDNN version: CUDA 9.2, cuDNN64_7\r\n- GPU model and memory: NVIDIA quadro 2000 graphic card, Intel Xeon.\r\n\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", lin\r\ne 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal\r\n.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal\r\n.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module\r\n>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in\r\n<module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", lin\r\ne 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", lin\r\ne 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal\r\n.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal\r\n.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\nI have problem with CPU model!!!!!!!!!!!!!!!!!!\r\n\r\nI have problem with cpu version. only 1.5 workes for me and faced error for installing CUDA 9.0. therefore I installed CUDA 9.2! what is the source od error? I tried different version of tensorflow and cuda!!!!!!\r\n\r\nthanks.", "comments": ["hello! how can you solve this problem?", "I could not solve it!!!!!!"]}, {"number": 26451, "title": "cpu tensorflow win: 7, DLL load failed with error code -1073741795", "body": "Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template\r\n\r\nSystem information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 7\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\nTensorFlow installed from (source or binary): https://www.tensorflow.org/install/pip\r\nTensorFlow version: 1.12\r\nPython version: 3.6.8\r\nInstalled using virtualenv? pip? conda?: pip\r\nBazel version (if compiling from source):-\r\nGCC/Compiler version (if compiling from source):-\r\nCUDA/cuDNN version: CUDA 9.2, cuDNN64_7\r\nGPU model and memory: NVIDIA quadro 2000 graphic card, Intel Xeon.\r\nDescribe the problem\r\n\r\nTraceback (most recent call last):\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", lin\r\ne 58, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal\r\n.py\", line 28, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal\r\n.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\nreturn _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"\", line 1, in \r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_init_.py\", line 24, in <module\r\n\r\nfrom tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python_init_.py\", line 49, in\r\n\r\nfrom tensorflow.python import pywrap_tensorflow\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", lin\r\ne 74, in \r\nraise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", lin\r\ne 58, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal\r\n.py\", line 28, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal\r\n.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\nreturn _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions. Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nAny other info / logs\r\n\r\nI have problem with CPU model!!!!!!!!!!!!!!!!!!\r\n\r\nI have problem with cpu version. only 1.5 workes for me and faced error for installing CUDA 9.0. therefore I installed CUDA 9.2! what is the source of error? I tried different version of tensorflow and cuda!!!!!!\r\n\r\nthanks.\r\n\r\n", "comments": ["@nouranik Can you please tell the template. Please provide details about what platform you are using (operating system, architecture), your TensorFlow version, did you compile from source or install a binary. Make sure you also include the exact command if possible to produce the output included in your test case.\r\nIt is really difficult to help without this information. Thanks!", "Thanks @Ayush517 \r\n@nouranik Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks! ", "> @nouranik Can you please tell the template. Please provide details about what platform you are using (operating system, architecture), your TensorFlow version, did you compile from source or install a binary. Make sure you also include the exact command if possible to produce the output included in your test case.\r\n> It is really difficult to help without this information. Thanks!\r\n\r\nthanks, done! \r\nit is about two weeks that I am facing with this issue!", "@nouranik Could you check tested build [configuration](https://www.tensorflow.org/install/source_windows#cpu). [Here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12/blob/master/README.md) are the instructions I created that could you help you installing TF1.12. I think there is a similar resolved [issue](https://github.com/tensorflow/tensorflow/issues/26227#issuecomment-471068552) on Win7 that could help you. Please let me know how it progresses. Thanks!", "thnak you for your helps. I already used the instructions. I can not install CUDA 9.0 it has error.\r\nI installed 9.2 but still does not work. why I face with tensorflow CPU error?", "@nouranik Did you uninstall python and tensorflow before installing then by following the instructions I provided. Thank you. It worked for many users. Some time when you have installed different versions of python and tensorflow, there will be some modules that could affect reinstalling of TF. Thanks!", "@could you check this [resource](https://github.com/tensorflow/tensorflow/issues/12748) that might help you. Do you have python 32 bit or 64 bit? For some reason, if you cannot install CUDA9.0, and can only install CUDA9.2, you need to make sure that the paths are correctly referencing cuda and cuDNN folders. Thanks!", "> > @nouranik Can you please tell the template. Please provide details about what platform you are using (operating system, architecture), your TensorFlow version, did you compile from source or install a binary. Make sure you also include the exact command if possible to produce the output included in your test case.\r\n> > It is really difficult to help without this information. Thanks!\r\n> \r\n> thanks, done!\r\n> it is about two weeks that I am facing with this issue!\r\n\r\nHow did you solve it? I got the same problem.THANKS", "Hi @nouranik ,\r\n\r\nI am a new PhD student in bioengineering. I am going to do vessel segmentation. I actually saw your message in the issues of \"https://github.com/ellisdg/3DUnetCNN\". I am also new to python and want to use 3D UNet from the mentioned github page. I was wondering if you could help me to use it. I was not able to find your email; therefore, I am contacting you heare! my email address is maalidoost@ucla.edu.\r\n\r\nThanks for your time and consideration and look forward to hearing from you.\r\n\r\nBest, Mohammad", "> I installed 9.2 but still does not work. why I face with tensorflow CPU error?\r\n\r\nThere could be some dependencies that weren't removed completely. Check Environmental file and remove any CUDA or cuDNN related paths as you want to use TF-cpu. Thanks!", "Closing due to lack of recent activity.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26451\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26451\">No</a>\n"]}, {"number": 26450, "title": "Lite: Pooling Missing Test Cases Added", "body": "35 Missing Test cases added for Pooling Op.", "comments": ["Load balancing review to Jian.", "@jianlijianli : Would you please help review the changes. TIA!", "@haozha111: Welcome for review, TIA!", "> Thanks for adding the tests. Could you please also expand the commit message to reflect that you are:\r\n> \r\n>     * allowing the capability to test different activation, bias, padding and strides\r\n> \r\n>     * adding more test coverage\r\n> \r\n> \r\n> Thanks.\r\n\r\nUpdated as per your comments, Thanks!", "@jianlijianli : I have resolved the conflict, would you please check and approve again, Thanks!", "@jianlijianli : Request to approve again, TIA!", "@jianlijianli : gentle reminder!"]}, {"number": 26449, "title": "[tensorflow.org] Site not working; Potential service worker issue", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:\r\n- Doc Link: https://www.tensorflow.org\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nNone of http://tensorflow.org  is working on Safari (latest macOS) for me; seems related to Service Worker. Works fine on Safari Technology Preview and on @googlechrome.\r\nMight have visited the page during the last 48h (pre-launch).\r\n\r\n![skjermbilde 2019-03-06 kl 23 11 45](https://user-images.githubusercontent.com/939844/53964361-81c45580-40ef-11e9-9812-d42291dfb970.png)\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes, if applicable.", "comments": ["@eivind88 It is working on my Mac without any problem. Could you check it by clearing cache and reload it. Thanks!", "Emptied cache, and did a hard reload and got the site.\r\n\r\nDid another normal reload \u2013\u00a0same error.\r\n\r\nRestarted Safari, and it's the same thing.\r\nCan only seem to get the new site when ignoring cache and reloading using <kbd>alt</kbd>+<kbd>cmd</kbd>+<kbd>R</kbd> (no hero-graphics, though) \u2013 every normal reload gives me the same error. Emptying my cache does nothing.\r\n\r\nI'm thinking there's been an error with whatever service worker I hit first during the last couple of days, which leads to bad cache-handling in the browser.", "These seem related: \r\nhttps://github.com/angular/angular/issues/26500\r\nhttps://github.com/GoogleChrome/workbox/issues/1730", "Thanks. Tested on Safari and this issue seems resolved when the redesign rollout completed.\r\n", "@lamberta @jvishnuvardhan \r\nActually, if this is what I suspect it is (bungled service worker at some point) you wouldn't be able to reproduce this issue if you didn't access the site at the same time as I must have originally done (at the time when the service worker was incorrect).\r\n\r\nI'm still having the exact same issue.", "I can file an internal ticket, but will need to be able to reproduce the issue. Is there anything about your setup that needs to be taken into account? (JavaScript blockers, etc.) Thanks\r\n", "Hi @eivind88, I've filed an internal issue.\r\n\r\nCan you load the site with the developer console open and let us know if there are any errors in the console? Thank you", "Hey, @lamberta \r\n\r\nNothing in the console when loading:\r\n\r\n![Skjermbilde 2019-03-28 kl  12 07 19](https://user-images.githubusercontent.com/939844/55173423-113eb080-5152-11e9-948e-efbf3288eaee.png)\r\n\r\nWhen I empty the cache and reload, there's a bunch. All of them include \"TypeError: Failed writing data to the file system\" or otherwise failing to load resources. This is also similar to the Safari error-screen I see when trying to load the page normally, which is part of why I think it's related to service workers.\r\n\r\n![Skjermbilde 2019-03-28 kl  12 09 32](https://user-images.githubusercontent.com/939844/55173604-5f53b400-5152-11e9-856c-02f02f5b2062.png)\r\n\r\nTextual representation:\r\n\r\n```\r\n[Error] FetchEvent.respondWith received an error: TypeError: Failed writing data to the file system\r\n[Error] Cannot load https://www.tensorflow.org/root_7bf9d7a55c4703d1d1fd5d9b6fca665eea25a03bc986290c9a6352762ed8402a.frame.\r\n[Error] Failed to load resource: FetchEvent.respondWith received an error: TypeError: Failed writing data to the file system (root_7bf9d7a55c4703d1d1fd5d9b6fca665eea25a03bc986290c9a6352762ed8402a.frame, line 0)\r\n[Info] Successfuly preconnected to https://fonts.googleapis.com/\r\n[Info] Successfuly preconnected to https://www.gstatic.com/\r\n[Info] Successfuly preconnected to https://fonts.gstatic.com/\r\n[Error] Failed to load resource: the server responded with a status of 404 () (devsite_app__nb.js, line 0)\r\n[Error] Refused to execute https://www.gstatic.com/devrel-devsite/vc9bb015437ae3e13b823152db938df0b60068cf5df8dd0c787a73605454011ee/tensorflow/js/devsite_app__nb.js as script because \"X-Content-Type: nosniff\" was given and its Content-Type is not a script MIME type.\r\n[Error] Cache API operation failed: Failed writing data to the file system\r\n[Error] Unhandled Promise Rejection: TypeError: Failed writing data to the file system\r\n\t(anonym funksjon)\r\n\trejectPromise\r\n\tpromiseReactionJob\r\n[Error] FetchEvent.respondWith received an error: TypeError: Failed writing data to the file system\r\n[Error] Cannot load https://www.tensorflow.org/root_7bf9d7a55c4703d1d1fd5d9b6fca665eea25a03bc986290c9a6352762ed8402a.frame.\r\n[Error] Failed to load resource: FetchEvent.respondWith received an error: TypeError: Failed writing data to the file system (root_7bf9d7a55c4703d1d1fd5d9b6fca665eea25a03bc986290c9a6352762ed8402a.frame, line 0)\r\n[Error] FetchEvent.respondWith received an error: TypeError: Failed writing data to the file system\r\n```", "Thanks. Tracking in b/129421358", "@eivind88 are you experiencing this problem on all sites with Service Workers?\r\n\r\ne.g. airhorner.com, chromestatus.com\r\n\r\nThere's very little control about where we store things\u2014SW just lets us put data into `Cache`.", "@samthor no problem anywhere else.", "Still tracking this webserver bug internally but closing this since it's not a TensorFlow issue. Thanks"]}, {"number": 26448, "title": "cpu build failed with openmpi", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : ubuntu 18.04 (64-bit FriendlyDesktop image file based on Ubuntu desktop 18.04 64bit)\r\n- Mobile device: friendlyARM's NanoPC-T4\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.13.0\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?: non\r\n- Bazel version (if compiling from source):0.19.2\r\n- GCC/Compiler version (if compiling from source): gcc 7.3\r\n- CUDA/cuDNN version: non\r\n- GPU model and memory: non\r\n\r\n\r\n\r\n**Describe the problem**\r\nmissing links related to OpenMPI(ex:mpi.h, libmpi.so, ...)\r\nwhen building with bazel \r\nerror message: tensorflow/pyhton/4301:0 missing input file thirdparty/mpi:mpi:h\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@peggy0135 Can you please try doing this :\r\nsudo apt install libopenmpi-dev\r\npip install mpi4py", "@peggy0135 Could you specify what options you selected for config?  Could you follow several other user's [issues](https://github.com/tensorflow/tensorflow/issues?q=is%3Aissue+mpi+is%3Aclosed) that had similar bug like you. I see some are relevant to you. Follow this [discussion](https://github.com/tensorflow/tensorflow/issues/25638) and [this](https://github.com/tensorflow/tensorflow/issues/26403). Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26448\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26448\">No</a>\n"]}, {"number": 26446, "title": "tensorflow 1.13.1: module 'tensorflow._api.v1.keras.applications' has no attribute 'resnet'", "body": "Hi every one\r\n\r\nI have used Google Colab and when use keras resnet, it raise this error: module 'tensorflow._api.v1.keras.applications' has no attribute 'resnet'\r\n\r\nmy code \r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nmodel = keras.applications.resnet.ResNet50(weights=None,classes=4, input_shape=(150,150,3))\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.01), \r\n                loss='binary_crossentropy',\r\n                metrics=['accuracy'])\r\n```\r\n\r\nmy tensorflow is 1.13.1, as default of google colab\r\n\r\nOther keras application like Mobinet, VGG16 still work fine\r\n\r\nThank for you help", "comments": ["@keyyuki Can you please add this line and try again :\r\nfrom keras.applications.resnet import ResNet50\r\n\r\nLet us know how it proceeds. Thanks :)", "@Ayush517 \r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom keras.applications.resnet import ResNet50\r\n```\r\nand it said\r\n\r\n```\r\nModuleNotFoundError: No module named 'keras.applications.resnet'\r\n```", "@keyyuki Please try this before import:\r\n!pip install keras --upgrade\r\n!pip install Keras-Applications", "@Ayush517 \r\nI think it keras, not tensorflow.keras\r\nso after install, i run\r\n\r\n```\r\nimport keras\r\nprint(keras.__version__)\r\n```\r\nprint\r\n```\r\n2.2.4\r\n```\r\nand continue\r\n\r\n```\r\nimport keras\r\nfrom keras.applications.resnet import ResNet50\r\n```\r\n\r\nit still error\r\n```\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-5-d327cd2bb017> in <module>()\r\n      1 import keras\r\n----> 2 from keras.applications.resnet import ResNet50\r\n\r\nModuleNotFoundError: No module named 'keras.applications.resnet'\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n\"Open Examples\" button below.\r\n---------------------------------------------------------------------------\r\n```\r\nnote that, I run on google colab, GPU, python 3", "import tensorflow as tf\r\nfrom keras.applications import ResNet50 as ResNet50\r\nfrom keras.optimizers import Adam as Adam\r\nmodel = ResNet50(weights=None,classes=4, input_shape=(150,150,3))\r\nmodel.compile(optimizer=Adam(0.01), loss='binary_crossentropy', metrics=['accuracy'])\r\n\r\n@keyyuki This works for me. I am not sure if it will work for you or if it's the correct way of doing it or just a hack. Can you please see if your issue is resolved by this.", "ok I see\r\nmy code\r\nkeras.applications.resnet.ResNet50\r\n\r\nand your code\r\nkeras.applications.ResNet50\r\n\r\nYou dont have resnet.ResNet50, and it run. But as document at https://www.tensorflow.org/api_docs/python/tf/keras/applications\r\n\r\nthey said read at https://github.com/keras-team/keras-applications and read at https://keras.io/applications/\r\n\r\nin https://keras.io/applications/#resnet\r\n\r\nkeras.applications.resnet.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\r\n\r\nso, document of tensorflow.keras is different with keras \r\n\r\nIt still issue, but for document, not lib\r\n", "Yeah, it's different than the document on their official website. Did the code resolved your issue? I mean, is it working the way you were expecting it to?", "ok, it run now, thank you so much", "Your welcome \ud83d\ude04 \r\nLet us know if face any other issues. You can close this issue now.", "@keyyuki using tf.keras works as you expected. Just for your information, if you upgrade to TF2.0, try to use tf.keras instead of keras as most of the functionality is still work-in-progress in Keras.\r\n\r\nimport tensorflow as tf\r\n#from tensorflow import keras\r\n\r\nmodel = tf.keras.applications.resnet50.ResNet50(weights=None,classes=4, input_shape=(150,150,3))\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.01), \r\n                loss='binary_crossentropy',\r\n                metrics=['accuracy'])\r\n\r\nRegarding document, i don't see any error. Please point out the \"document error\" if you see any. Website in currently upgrading, and most of the modifications will be available in couple of days. If you see any document error after two weeks, please do PR with the modifications. Thanks!"]}, {"number": 26445, "title": "How to install TF_GAN", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win7 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:0.1.0\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):No\r\n- GCC/Compiler version (if compiling from source):No\r\n- CUDA/cuDNN version:No\r\n- GPU model and memory:No\r\n\r\n\r\n\r\n**Describe the problem**\r\nHow to install TF-GAN in tensorflow 0.1.0?\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@whyseu Please follow this [Link](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/gan). Let us know if this helps or if there's something we can do. Thanks \ud83d\ude04 ", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26444, "title": "Tensorflow.js API documentation example issue line 19, incorect code, build error", "body": "In the object callback on **line 19** in the bottom example on this page:\r\n[See-sample-code-for-node.js-usage](https://www.tensorflow.org/js/tutorials/setup#see-sample-code-for-node.js-usage) has a little error.\r\n\r\n> Currently you have\r\n\r\n`onEpochEnd: (epoch, log) => console.log(`Epoch ${epoch}: loss = ${log.loss}`);`\r\n\r\n> You need to remove the ;\r\n\r\n`onEpochEnd: (epoch, log) => console.log(`Epoch ${epoch}: loss = ${log.loss}`)`\r\n\r\n> Compile error is as follows\r\n\r\n```\r\nSyntaxError: Unexpected token ;\r\n    at new Script (vm.js:80:7)\r\n    at createScript (vm.js:274:10)\r\n    at Object.runInThisContext (vm.js:326:10)\r\n    at Module._compile (internal/modules/cjs/loader.js:664:28)\r\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:712:10)\r\n    at Module.load (internal/modules/cjs/loader.js:600:32)\r\n    at tryModuleLoad (internal/modules/cjs/loader.js:539:12)\r\n    at Function.Module._load (internal/modules/cjs/loader.js:531:3)\r\n    at Function.Module.runMain (internal/modules/cjs/loader.js:754:12)\r\n    at startup (internal/bootstrap/node.js:283:19)\r\n```\r\n\r\n>  Node -v\r\n\r\n`v11.11.0`", "comments": ["This is working as expected , please check in latest version. Closing this issue. Thank you"]}]