[{"number": 37679, "title": "Support two CUDNN CTC Loss algorithms", "body": "This PR enables CUDNN CTC Loss to support both deterministic and non-deterministic algos.\r\n\r\nIf determinism is required, we will stick to deterministic algo.\r\nOtherwise, the faster non-deterministic algo will be tried in cudnnGetCtcLossWorkspace(). If it fails, we fall-back to the deterministic algo.\r\n\r\n\r\nfyi @nluehr @sanjoy ", "comments": ["@sanjoy  PTAL.", "Gentle ping @sanjoy . Thx.", "@sanjoy Done. PTAL.", "@sanjoy Done.", "@sanjoy Is this PR gonna be merged into 2.2? Do we need to do anything else?", "> @sanjoy Is this PR gonna be merged into 2.2? Do we need to do anything else?\r\n\r\nI think this is too risky for 2.2 at this point.", "Sure. Then, I think the PR will be merged into master. Do we need to do anything else for now?", "Is there a difference in the version of cuDNN that being used Kokoro? The same test seem to pass in our internal builds.\r\n\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/ctc_loss_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1125, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/ctc_loss_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/ctc_loss_op_test.py\", line 1035, in testCtcLossAlgorithmFallback\r\n    self.assertAllClose(loss, ref_loss, atol=1e-6)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/ctc_loss_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1167, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/ctc_loss_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2519, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/ctc_loss_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2479, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/ctc_loss_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2414, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/testing/nose_tools/utils.py\", line 1396, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/testing/nose_tools/utils.py\", line 779, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-06, atol=1e-06\r\nMismatched value: a is different from b. \r\nnot close where = (array([0]),)\r\nnot close lhs = [9.573787]\r\nnot close rhs = [9.891954]\r\nnot close dif = [0.3181677]\r\nnot close tol = [1.0891955e-05]\r\ndtype = float32, shape = (1,)\r\n(mismatch 100.0%)\r\n x: array([9.573787], dtype=float32)\r\n y: array([9.891954], dtype=float32)", "@pkanwar23 I believe internally we use cuDNN 7.6.2 and externally we use 7.6.4.  I'll ping you offline on how we can try the test with 7.6.4 internally to see if that's the root cause for the difference.", "I see this error in both internal tests and in kokoro. @houtoms can you try rebasing to head and validating that the tests pass?", "@pkanwar23 After rebase, the test fails on my machine as well. Looking into it.", "The issue of failed test is in the graph mode, the inputs are generated twice with `random_ops.random_uniform()`. So, I limit the added test to eager mode to make sure the input is only generated once.   ", "Changes have been merged into master by commit d779e84. So closing the PR."]}, {"number": 37678, "title": "Cannot create feature_column from sparse tensor of feature counts", "body": "A common modeling pattern is to use sparse tensors to represent feature counts. For example, in a bag-of-words model, the input is a document represented by a tensor of token counts \u2014 the indices are token IDs and the values are their corresponding counts in the document.\r\n\r\nTensorflow's premade Estimators consume data via [feature_columns](https://www.tensorflow.org/api_docs/python/tf/feature_column). Currently, however, there is no way to create a feature_column directly from a sparse tensor.\r\n\r\n\r\n**System information**\r\n- TensorFlow version: all\r\n- Are you willing to contribute it (Yes/No): maybe\r\n\r\n**Will this change the current api? How?**\r\nYes, this change will add support to construct a feature_column directly from a sparse tensor.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to use premade Estimators with feature data in sparse tensor form.\r\n", "comments": ["we have this [RFC](https://github.com/tensorflow/community/pull/188) where you can use CategoricalEncoding layer, however you will need to use sparse tensor to represent feature counts not by its indices, but by its values. Not sure if that would suffice your request", "@tddevlin,\r\nSorry for the delayed response. Can you please let us know if [this PR](https://github.com/tensorflow/community/pull/188) followed by [this RFC PR](https://github.com/tensorflow/community/pull/209) has implemented the **`Feature`** that you are looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 37677, "title": "Add missing dependencies download command", "body": "It takes me a lot time to fix this issue:\r\n\r\n```\r\n./tensorflow/lite/schema/schema_generated.h:21:37: fatal error: flatbuffers/flatbuffers.h: No such file or directory\r\n```\r\nAnd finally, I found this: https://www.tensorflow.org/lite/guide/build_rpi\r\nSo there is a missing command to download dependencies before build in the README file.\r\n\r\nBy the way, this pr should close #33983 and close #34423", "comments": []}, {"number": 37676, "title": "tf2.1 tf.nn.ctc_loss slower!", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code : yes \r\n- OS Platform and Distribution: centos 7, not Distribution\r\n- TensorFlow installed from docker: docker pull nvcr.io/nvidia/tensorflow:20.02-tf2-py3 .tensorflow version is 2.1.0\r\n\r\n**Describe the current behavior**\r\n\r\nJust run the forward algorithm and use watch -n 0.2 nvidia-smi to watch the GPU utilization rate always stay at 86% ~ 95%. However, after calculating tf.nn.ctc_loss, the GPU usage jitter is drastically from 0% to 85%. Seeing the decline in gpu utilization is particularly severe, personally guess that this tf.nn.ctc_loss is implemented by cpu, will there be a switch between gpu memory and memory when calculating loss?\r\n\r\n**Describe the expected behavior**\r\n\r\nCalculate the loss and update gradient normally, the GPU usage rate remains at 85%. Currently, the performance of calculating the loss using tf.nn.ctc_loss of tf2.1 does not meet expectations.\r\n", "comments": ["@attendfov Can you please provide us a reproducible test case especially in google colab for us to reproduce this issue. Thanks!", "Here is my test code. The run_forward_noloss function is a simple forward calculation function. Run_forward_withloss is forward calculation +loss calculation. \r\n```\r\n# _*_ coding:utf-8 _*_\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport time\r\nimport tensorflow as tf\r\nlayers = tf.keras.layers\r\ntime_func = lambda: time.clock()*1000\r\n\r\nclass CRNNEncoder(tf.keras.Model):\r\n    def __init__(self, configs, name=None):\r\n        super(CRNNEncoder, self).__init__(name=name)\r\n        self.vocab_size = configs['vocab_size']\r\n        self.conv1 = layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', name='conv1')\r\n        self.pool1 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1')\r\n        self.conv2 = layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', name='conv2')\r\n        self.pool2 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2')\r\n        self.conv3 = layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', name='conv3')\r\n        self.conv4 = layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', name='conv4')\r\n        self.padd4 = layers.ZeroPadding2D(padding=(0, 1))\r\n        self.pool4 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 1), padding='valid', name='pool3')\r\n        self.conv5 = layers.Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='conv5')\r\n        self.bncv5 = layers.BatchNormalization(axis=-1, name='bnconv5')\r\n        self.conv6 = layers.Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='conv6')\r\n        self.bncv6 = layers.BatchNormalization(axis=-1, name='bnconv6')\r\n        self.pddd6 = layers.ZeroPadding2D(padding=(0, 1))\r\n        self.pool6 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 1), padding='valid', name='pool4')\r\n        self.conv7 = layers.Conv2D(512, kernel_size=(3, 3), activation='relu', padding='valid', name='conv7')\r\n        self.final_layer = tf.keras.layers.Dense(self.vocab_size, name='ctc_decoder_linear')\r\n\r\n    def get_feature_step(self, widths):\r\n        return tf.cast((tf.cast(widths, tf.float32)/4.0), dtype=tf.int32)\r\n    @tf.function\r\n    def call(self, inputs, widths, training=True):\r\n        tf.print('call input:', inputs.shape)\r\n        features = self.conv1(inputs)\r\n        features = self.pool1(features)\r\n        features = self.conv2(features)\r\n        features = self.pool2(features)\r\n        features = self.conv3(features)\r\n        features = self.conv4(features)\r\n        features = self.padd4(features)\r\n        features = self.pool4(features)\r\n        features = self.conv5(features)\r\n        features = self.bncv5(features, training=training)\r\n        features = self.conv6(features)\r\n        features = self.bncv6(features, training=training)\r\n        features = self.pddd6(features)\r\n        features = self.pool6(features)\r\n        features = self.conv7(features)\r\n        cnn_features = tf.reduce_max(features, axis=1)\r\n        # rnn_features = self.run_bilstm1(rnn_features)\r\n        rnn_features = cnn_features\r\n        final_logits = self.final_layer(rnn_features)\r\n        widths = self.get_feature_step(widths)\r\n        return cnn_features, rnn_features, widths, final_logits\r\n\r\ndef run_forward_noloss():\r\n    batch = 24 \r\n    imgh = 48\r\n    imgw = 1024 \r\n    imgc = 3\r\n    vocab_size = 1424\r\n    configs = {'vocab_size': vocab_size}\r\n    model = CRNNEncoder(configs)\r\n    images = tf.random.uniform([batch, imgh, imgw, imgc], minval=-1, maxval=1)\r\n    widths = tf.fill([batch], imgw)\r\n    logit_mean = tf.keras.metrics.Mean('logit_mean')\r\n    init_time = time_func()\r\n    for i in range(200):\r\n        cnn_features, rnn_features, widths, final_logits = model(images, widths)\r\n        final_logits = tf.reduce_mean(final_logits)\r\n        logit_mean.update_state(final_logits)\r\n    fini_time = time_func()\r\n    print('time:', fini_time-init_time)\r\n    print(logit_mean.result().numpy)\r\n\r\ndef run_forward_withloss():\r\n    batch = 24\r\n    imgh = 48\r\n    imgw = 1024\r\n    imgc = 3\r\n    txtlen = 64\r\n    vocab_size = 1424\r\n    eos_id = vocab_size - 1\r\n    configs = {'vocab_size': vocab_size}\r\n    model = CRNNEncoder(configs)\r\n    images = tf.random.uniform([batch, imgh, imgw, imgc], minval=-1, maxval=1)\r\n    widths = tf.fill([batch], imgw)\r\n    labels = tf.fill([batch, txtlen], 0)\r\n    labels_len = tf.fill([batch], txtlen)\r\n    ctc_loss_mean = tf.keras.metrics.Mean('ctc_loss_mean')\r\n    init_time = time_func()\r\n    for i in range(200):\r\n        cnn_features, rnn_features, widths, final_logits = model(images, widths)\r\n        ctc_loss = tf.nn.ctc_loss(labels=labels,\r\n                                  logits=final_logits,\r\n                                  label_length=labels_len,\r\n                                  logit_length=widths,\r\n                                  blank_index=eos_id,\r\n                                  logits_time_major=False)\r\n\r\n        ctc_loss = tf.reduce_mean(ctc_loss)\r\n        ctc_loss_mean.update_state(ctc_loss)\r\n    fini_time = time_func()\r\n    print('time:', fini_time - init_time)\r\n    print(ctc_loss_mean.result().numpy)\r\n\r\nif __name__ == '__main__':\r\n    run_forward_noloss()\r\n    run_forward_withloss()\r\n```\r\n\r\n> @attendfov Can you please provide us a reproducible test case especially in google colab for us to reproduce this issue. Thanks!\r\n\r\n"]}, {"number": 37675, "title": "TFRecord dataset reader causes excessive page faults", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): Ubuntu 14.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 7\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ntf.data.TFRecordDataset causes excessive page faults, especially when compression is used. We were able to tame the page faults in other parts of the graph by setting `TF_CPU_ALLOCATOR_USE_BFC=1`, but tf.data.TFRecordDataset doesn't seem to honor the allocator.\r\n\r\n**Describe the expected behavior**\r\nI expect, after setting `TF_CPU_ALLOCATOR_USE_BFC=1`, CPU memory allocations to be handled by the bfc allocator. There should not be large amounts of page faults because the tensorflow runtime manages memory allocation itself.\r\n\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nWe have an input_fn that's essentially:\r\n```\r\ntrain_dataset = tf.data.TFRecordDataset(\r\n    filenames,\r\n    compression_type=\"ZLIB\",\r\n    buffer_size=100 * 1024 * 1024,\r\n    num_parallel_reads=16)\r\ntrain_dataset = train_dataset.map(\r\n    map_func=lambda x: (tf.io.parse_single_example(x, features), {}), \r\n    num_parallel_calls=num_parallel_calls)\r\ntrain_dataset = train_dataset.batch(4)\r\ntrain_dataset = train_dataset.prefetch(1)\r\n```\r\nEach record is about 8MB compressed, and 250MB decompressed.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nFlamegraph: https://drive.google.com/file/d/15LqZBBDov9Sikr9yt6OjMrVcw9IsebRk/view?usp=sharing\r\n\r\nIt's clear from the flamegraph that `tensorflow::io::ZlibInputStream::ReadBytesFromCache` and `tensorflow::io::BufferedInputStream::ReadNBytes` are dominated by page faults. `tensorflow::example::FastParseSingleExample` was also dominated by page faults when `TF_CPU_ALLOCATOR_USE_BFC=1` was not specified (not shown here,) but not when `TF_CPU_ALLOCATOR_USE_BFC=1` is specified (shown here.)\r\n\r\nOne source of page faults seems to be this line: https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/core/lib/io/zlib_inputstream.cc#L175, according to the flamegraph.", "comments": ["@guoshimin,\r\nOn running the above code, I got an error stating `NameError: name 'num_parallel_calls' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/83a90744a8855ce803098df0b8df9b6f/37675.ipynb).\r\n\r\nCould you please share the complete code to reproduce the issue reported here and also the supporting files you are using. Thanks!", "@amahendrakar The code sample was meant to give you a general idea. You can put whatever as num_parallel_calls. It's inconsequential. You would also need input files and a training loop to actually run it.", "@guoshimin,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal working example to reproduce the issue. Thanks!", "Hi @amahendrakar, here is a minimal working example that reproduces the issue.\r\n```from __future__ import absolute_import, division, print_function, unicode_literals\r\nimport os\r\nimport shutil\r\nimport time\r\nimport zlib\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom multiprocessing import Process\r\nassert tf.__version__[0] == '2'\r\n\r\nDATASET_DIR = '/tmp/pagefaults'\r\nTHREADS = 16\r\nZLIB_FILES = [os.path.join(DATASET_DIR, 'zlib_{}.tfrecord'.format(i)) for i in range(THREADS)]\r\nTRIALS = 1024\r\n\r\ndef zero_bytes():\r\n    zeros = np.zeros((250 * 10**6,), dtype=np.int8)\r\n    return zeros.tobytes()\r\n\r\ndef create_zlib_dataset():\r\n    for filepath in ZLIB_FILES:\r\n        writer = tf.io.TFRecordWriter(filepath, options=\"ZLIB\")\r\n        writer.write(zero_bytes())\r\n        writer.close()\r\n    print(\"Created ZLIB dataset\")\r\n\r\ndef read_zlib_dataset():\r\n    total = 0\r\n    start = time.time()\r\n    dataset = tf.data.TFRecordDataset(ZLIB_FILES,\r\n                                      compression_type=\"ZLIB\",\r\n                                      num_parallel_reads=THREADS)\r\n    dataset = dataset.repeat().prefetch(THREADS)\r\n    for sample in dataset:\r\n        if total >= TRIALS:\r\n            break\r\n        total += 1\r\n    elapsed = time.time() - start\r\n    print(\"ZLIB read TFRecord: {:.5f} s per sample\".format(elapsed / TRIALS))\r\n\r\ndef decompress_process(compressed):\r\n    for _ in range(TRIALS // THREADS):\r\n        zlib.decompress(compressed)\r\n\r\ndef decompress_only():\r\n    compressed = zlib.compress(zero_bytes())\r\n    start = time.time()\r\n    processes = []\r\n    for _ in range(THREADS):\r\n        p = Process(target=decompress_process, args=(compressed,))\r\n        p.start()\r\n        processes.append(p)\r\n    for p in processes:\r\n        p.join()\r\n    elapsed = time.time() - start\r\n    print(\"Decompress only: {:.5f} s per sample\".format(elapsed / TRIALS))\r\n\r\nif __name__ == \"__main__\":\r\n    os.environ['TF_CPU_ALLOCATOR_USE_BFC'] = \"1\"\r\n    if not os.path.exists(DATASET_DIR):\r\n        os.mkdir(DATASET_DIR)\r\n    print(\"Starting\")\r\n    create_zlib_dataset()\r\n    read_zlib_dataset()\r\n    decompress_only()\r\n```\r\nIf we read a ZLIB-compressed TFRecord using `read_zlib_dataset` and look at the flamegraph, a substantial amount of the runtime consists of page faults. In contrast, if we only decompress the files in memory using `decompress_only`, it is about twice as fast and almost all of the runtime is in the decompression algorithm itself. The results might be slightly different depending on machine, but if you generate a flamegraph you should see that a lot of time is being wasted on pagefaults.\r\n\r\nLet me know if you need any more information from us, thank you!", "@guoshimin ,\r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 37674, "title": "Handle CUPTI_ERROR_MAX_LIMIT_REACHED and disable TF tracing", "body": "This PR fixes an issue caused by multiple processes attempting to use CUPTI simultaneously. For example, when using NVIDIA Nsight System with TF tracing at the same time (both rely on CUPTI APIs), users will get wrong TF tracing and incomplete Nsight reports.\r\n\r\nAccording to https://docs.nvidia.com/cupti/Cupti/r_limitations.html#r_limitations:\r\n\r\n> The application which calls CUPTI APIs cannot be used with Nvidia tools like nvprof, Nvidia Visual Profiler, Nsight Compute, Nsight Systems, Nvidia Nsight Visual Studio Edition, cuda-gdb and cuda-memcheck.\r\n\r\nSo, we should avoid calling CUPTI APIs if the function cuptiSubscribe() detects CUPTI_ERROR_MAX_LIMIT_REACHED.\r\n\r\nIn this PR, we let TensorFlow issue a warning when that CUPTI_ERROR_MAX_LIMIT_REACHED is detected and skip all the following CUPTI calls. Then, the external profiling tool can have exclusive access to CUPTI APIs.\r\n\r\nFYI. @nluehr ", "comments": ["CC @jbaiocchi (I tried assigning you as the reviewer but for some reason github does not let me)", "Unfortunately supporting external tools concurrently with TensorFlow tracing and profiler is not a use case that we support. Our tools are not designed to work concurrently with NVIDIA tools, hence this is the expected behavior.\r\n\r\nIt would help to explain the problem you are trying to solve, that would involve requiring Nsight Systems?", "@yisitu This PR's purpose is to support the external profiling tools and at the same time not to affect existing TF tracing.\r\n\r\nOne use case: suppose the example.py has TF tracing in it. If we do `nsys profile python example.py`, both Nsys tracing and TF tracing will collect nothing. With this change, TF tracing will be ignored and Nsys gets correct report. And `python example.py` won't be affected.", "I know that in your case, once subscribe exist, it probably lasts for the life time of the process. however  there might be other cases that subscriber can collide with each other,  e.g. on-demand with tensorboard callback, we don't want to disable it permanently, so the next call to Enable() (when all other subscribers have expired) should be successful, \r\nhow do you think?\r\n", "In your use case, could you have disabled TF tracing in the first place\nsince you will be using `nsys profile`?\n\nOn Wed, Mar 18, 2020 at 11:20 AM Jie Sun <notifications@github.com> wrote:\n\n> I know that in your case, once subscribe exist, it probably lasts for the\n> life time of the process. however there might be other cases that\n> subscriber can collide with each other, e.g. on-demand with tensorboard\n> callback, we don't want to disable it permanently, so the next call to\n> Enable() (when all other subscribers have expired) should be successful,\n> how do you think?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37674#issuecomment-600787844>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AONQZYDG5A66HH7KDS7APULRIEGGDANCNFSM4LN6NOMA>\n> .\n>\n", "@yisitu Do you mean by changing the python script?", "Yes, I imagine that the python script was instrumented to enable profiling\nin the first place.\n\nOn Wed, Mar 18, 2020 at 11:39 AM Kaixi Hou <notifications@github.com> wrote:\n\n> @yisitu <https://github.com/yisitu> Do you mean by changing the python\n> script?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37674#issuecomment-600796634>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AONQZYHPOZDH3C6PB6X6RNTRIEIMPANCNFSM4LN6NOMA>\n> .\n>\n", "@trisolaran Yes, can you confirm my understanding of your example is correct? So, before this change, TF can handle the case that issues multiple subscriber sessions: if subscriber exists, ignore it; if not exist, create it. However, after this change, only the first session can create the subscriber but all the other sessions cannot create new subscriber anymore.", "yes. I think that you statement is right.\r\n I wonder if we can fail silently for this time, and able to enable cupti tracer next time. ( not applicable to your case though).", "I am wondering adding more conditions in the Disable() might solve the problem. Basically, we need to make sure we still unsubscribe if we know the existing subscriber is from TF rather than external tools. Something like (isSet() is an imaginary function for now.)\r\n```c++\r\nvoid CuptiTracer::Disable() {\r\n  if (!subscriber_existed || (subscriber_existed && subscriber_.isSet())) {\r\n   //do as normal\r\n  }\r\n}\r\n```\r\n\r\n ", "@houtoms Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@houtoms Any update on this PR, please. Thanks!\r\n", "Hi, I rebased the code to include a new design to disable all TF tracers when the external profiler is detected. Could someone take a look? Thx.", "It should work. I think you are referring changes here: https://github.com/tensorflow/tensorflow/commit/b97cc6fd437e3ab8b788853558733f9420125245, which I feel is similar to the original design that disables the cupti tracer then external profiler is detected. But, as we discussed previously, my understanding is that we'd like to turn off all TF tracers if external profiler is detected. That's why the new commit here is to put the detection code to an earlier phase of the profiler initialization session. Anyway, if you are OK with the original plan, we can close this one.   ", "thanks Kaixi.\r\n"]}, {"number": 37673, "title": "tf.Keras BatchNormalization layer causing impossible loss/accuracy in GAN training", "body": "**System information** \r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Windows 10, version 1809, os build 17763.1098\r\n- Mobile device if the issue happens on mobile device: N/A\r\n- TensorFlow installed from:  binary\r\n- TensorFlow version: 2.0.0\r\n- Python version: Python 3.7.6\r\n- Bazel version: N/A\r\n- GCC/Compiler version: N/A\r\n- CUDA/cuDNN version: 10.1, V10.1.105\r\n- GPU model and memory: GeForce RTX 2080 Ti, 11GB\r\n\r\nI've been getting unusual losses and accuracies when training GANs with batch normalization layers in the discriminator using tf.keras. GANs have an optimal objective function value of log(4), which occurs when the discriminator is completely unable to discern real samples from fakes and hence predicts 0.5 for all samples. When I include BatchNormalization layers in my discriminator, both the generator and the discriminator achieve near perfect scores (high accuracy, low loss), which is impossible in an adversarial setting.\r\n\r\nWithout BatchNorm:\r\n[This figure](https://i.stack.imgur.com/4F7l8.png) shows the losses (y) per epoch (x) when BN is not used. Note that occasional values below the theoretical minimum are due to the training being an iterative process. [This figure](https://i.stack.imgur.com/YGGCt.png) shows the accuracies when BN is not used, which settle at about 50% each. Both of these figures show reasonable values.\r\n\r\nWith BatchNorm:\r\n[This figure](https://i.stack.imgur.com/HRzyc.png) shows the losses (y) per epoch (x) when BN is used. See how the GAN objective, which shouldn't fall below log(4), approaches 0. [This figure](https://i.stack.imgur.com/jS5XN.png) shows the accuracies when BN is used, with both approaching 100%. GANs are adversarial; the generator and discriminator can't both have 100% accuracy.\r\n\r\nI suspect this has something to do with setting `model.trainable = False`.\r\n\r\nStandalone code to reproduce and visualize can be found [here](https://gist.github.com/ConorLazarou/584af3f87a9a14503b9714580c567dab); set `BATCHNORM_MOMENTUM` to `0.9` to enable batchnorm or `None` to disable it\r\n\r\nUpdate: It seems that BatchNorm in the generator also causes this problem, but has been harder to reproduce.", "comments": ["@ConorLazarou, Will it be possible to provide the sample code snippet. Thanks!", "hi @ConorLazarou . I haven't looked at your code yet but there is this paper \nTOWARDS PRINCIPLED METHODS FOR TRAINING\rGENERATIVE ADVERSARIAL NETWORKS\nthat talks about discriminator having a theoretically impossible loss. could you please have a look at that... \nthanks.", "@gadagashwini here it is\r\n```\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, LeakyReLU\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\ndef build_G(batchnorm_momentum=0.9):\r\n    input_layer = Input(shape=(2,))\r\n    for i in range(4):\r\n        X = Dense(64)(input_layer)\r\n        X = Activation('relu')(X)\r\n    output_layer = Dense(2)(X)\r\n    model = Model(input_layer, output_layer)\r\n    return model\r\n\r\n\r\ndef build_D(batchnorm_momentum=0.9):\r\n    input_layer = Input(shape=(2,))\r\n    for i in range(4):\r\n        X = Dense(64)(input_layer)\r\n        if batchnorm_momentum:\r\n            X = BatchNormalization(momentum=batchnorm_momentum)(X)\r\n        X = Activation('relu')(X)\r\n    output_layer = Dense(1, activation='sigmoid')(X)\r\n    model = Model(input_layer, output_layer)\r\n    model.compile(Adam(lr=0.001, beta_1=0.5),\r\n                  loss='binary_crossentropy',\r\n                  metrics=['accuracy'],)\r\n    return model\r\n\r\n\r\ndef build_GAN(G, D):\r\n    D.trainable=False\r\n    input_layer = Input(shape=(2,))\r\n    X = G(input_layer)\r\n    output_layer = D(X)\r\n    model = Model(input_layer, output_layer)\r\n    model.compile(Adam(lr=0.0002, beta_1=0.5),\r\n                  loss='binary_crossentropy',\r\n                  metrics=['accuracy'],)\r\n    return model\r\n\r\n\r\ndef get_noise(num):\r\n    return np.random.random((num, 2)) * 2 - 1\r\n\r\n\r\ndef get_samples(num):\r\n    return np.random.normal(0, 1, (num, 2))\r\n\r\n\r\nBATCHNORM_MOMENTUM = 0.9\r\n# BATCHNORM_MOMENTUM = None\r\nG = build_G()\r\nD = build_D(batchnorm_momentum=BATCHNORM_MOMENTUM)\r\nGAN = build_GAN(G, D)\r\n\r\n\r\nEPOCHS = 100\r\nBATCH_SIZE = 32\r\nBATCHES_PER_EPOCH = 100\r\ng_loss = []\r\ng_accuracy = []\r\nd_real_loss = []\r\nd_real_accuracy = []\r\nd_fake_loss = []\r\nd_fake_accuracy = []\r\nfor epoch in range(EPOCHS):\r\n    g_running_loss = 0\r\n    g_running_accuracy = 0\r\n    d_real_running_loss = 0\r\n    d_real_running_accuracy = 0\r\n    d_fake_running_loss = 0\r\n    d_fake_running_accuracy = 0\r\n    for batch in range(BATCHES_PER_EPOCH):\r\n        real = get_samples(BATCH_SIZE)\r\n        fake = G.predict(get_noise(BATCH_SIZE))\r\n        l, a = D.train_on_batch(real, np.ones((BATCH_SIZE, 1)))\r\n        d_real_running_loss += l\r\n        d_real_running_accuracy += a\r\n        l, a = D.train_on_batch(fake, np.zeros((BATCH_SIZE, 1)))\r\n        d_fake_running_loss += l\r\n        d_fake_running_accuracy += a\r\n\r\n        l, a = GAN.train_on_batch(get_noise(BATCH_SIZE), np.ones((BATCH_SIZE, 1)))\r\n        g_running_loss += l\r\n        g_running_accuracy += a\r\n\r\n        print(f'Epoch {epoch+1} [{batch+1}/{BATCHES_PER_EPOCH}]: '\r\n              f'G={g_running_loss/(batch+1):.4f} [{g_running_accuracy/(batch+1):.2%}]; '\r\n              f'Dr={d_real_running_loss/(batch+1):.4f} [{d_real_running_accuracy/(batch+1):.2%}]; '\r\n              f'Df={d_fake_running_loss/(batch+1):.4f} [{d_fake_running_accuracy/(batch+1):.2%}]; '\r\n              f'Overall={(g_running_loss + (d_real_running_loss + d_fake_running_loss) / 2) / (batch+1):.4f}',\r\n              end='\\r'\r\n              )\r\n    g_loss.append(g_running_loss / BATCHES_PER_EPOCH)\r\n    g_accuracy.append(g_running_accuracy / BATCHES_PER_EPOCH)\r\n    d_real_loss.append(d_real_running_loss / BATCHES_PER_EPOCH)\r\n    d_real_accuracy.append(d_real_running_accuracy / BATCHES_PER_EPOCH)\r\n    d_fake_loss.append(d_fake_running_loss / BATCHES_PER_EPOCH)\r\n    d_fake_accuracy.append(d_fake_running_accuracy / BATCHES_PER_EPOCH)\r\n    print()\r\n\r\ng_loss = np.array(g_loss)\r\nd_real_loss = np.array(d_real_loss)\r\nd_fake_loss = np.array(d_fake_loss)\r\nd_loss = (d_real_loss + d_fake_loss) / 2\r\nplt.plot(np.arange(len(g_loss)) + 1, g_loss, label='G Loss')\r\nplt.plot(np.arange(len(d_loss)) + 1, d_loss, label='D Loss')\r\nplt.plot(np.arange(len(d_loss)) + 1, g_loss + d_loss, label='GAN Objective')\r\nplt.plot([1, len(d_loss)], [np.log(4), np.log(4)], label='GAN Objective Theoretical Minimum')\r\nplt.plot([1, len(d_loss)], [np.log(4)/2, np.log(4)/2], label='Equilibrium')\r\nplt.title(\"With BatchNorm\" if BATCHNORM_MOMENTUM else \"Without BatchNorm\")\r\nplt.legend(loc='lower left')\r\nplt.show()\r\n\r\ng_accuracy = np.array(g_accuracy)\r\nd_real_accuracy = np.array(d_real_accuracy)\r\nd_fake_accuracy = np.array(d_fake_accuracy)\r\nd_accuracy = (d_real_accuracy + d_fake_accuracy) / 2\r\nplt.plot(np.arange(len(g_accuracy)) + 1, g_accuracy, label='G accuracy')\r\nplt.plot(np.arange(len(d_accuracy)) + 1, d_accuracy, label='D accuracy')\r\nplt.plot([1, len(d_accuracy)], [1, 1], label='100%')\r\nplt.title(\"With BatchNorm\" if BATCHNORM_MOMENTUM else \"Without BatchNorm\")\r\nplt.ylim(-0.05, 1.05)\r\nplt.legend(loc='lower left')\r\nplt.show()\r\n```", "Hi @HopefulRational ,\r\nI skimmed the paper but I'm not sure I understand what you're referring to. The problem here is that the GAN objective function, `-log(D(x)) - log(1-D(G(z)))` (where the range of D is [0, 1]) has a global minimum of log(4) (when D returns 0.5 for all inputs), but BatchNormalization results in values much lower than the global minimum.", "@ConorLazarou i was just referring to thing that even in the paper the issue with which the author starts discussion is that discriminator attains a loss far lower than the actual value...  please see the **third page first paragraph**  . so i thought maybe you can try the ideas mentioned in the paper like adding noise (if I remember it well) and tell us what is happening... \nthanks for considering my option... \nand  sorry for the confusion.. ", "Ah yes thanks for pointing that out. Unfortunately, this issue has nothing to do with GAN stability. The issue that Arjovsky and Bottou describe results in the discriminator loss approaching 0 while the generator loss approaches infinity; the issue I'm describing involves the discriminator loss and the generator loss approaching 0, which shouldn't be possible.", "@ConorLazarou oh yeah.... sorry I missed that.. we having an adversarial setting here. \nso silly of me... ok will look at your code as well as tensorflow bn... \nsorry again... \nthanks for patience... ", "Hi @ConorLazarou . I ran the code as it was given by you. I get this error : \r\n```\r\nTraceback (most recent call last):\r\n  File \"GAN_issue.py\", line 93, in <module>\r\n    l, a = GAN.train_on_batch(get_noise(BATCH_SIZE), np.ones((BATCH_SIZE, 1)))\r\n  File \"/home/vsairaam/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1188, in train_on_batch\r\n    outputs = self.train_function(ins)  # pylint: disable=not-callable\r\n  File \"/home/vsairaam/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3076, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/vsairaam/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\r\n    run_metadata_ptr)\r\n  File \"/home/vsairaam/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_2' with dtype float and shape [?,2]\r\n\t [[{{node input_2}}]]\r\n\t [[{{node _arg_model_1_target_0_1}}]]\r\n\r\n```\r\nThe placeholder referred in the error is that of the Discriminator. I checked this by using `name` argument in each `Input` call.  \r\n\r\nTensorflow : 1.13.1\r\ndevice 0 with properties: name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076", "Hi @ConorLazarou . verified your claim. Yes, there is some issue. But it works well on tf 1.15 (current default version on colab).", "Oh interesting, I suppose I'll downgrade. I did a bit more testing, and have confirmed what you found, that this does not occur when using tf 1.15 (on a CPU and on a GPU). \r\n\r\nAnd that this does occur when using tf 2.1 (on a CPU and on a GPU).\r\n\r\nThank you so much for finding this workaround for me!", "@ConorLazarou,\r\nI tried with Tf 2.1.Please take a look at the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/9deefe1cc9a6831964848b3882779b62/untitled471.ipynb) and confirm whether the reported issue is reproduced or not. Thanks! ", "@gadagashwini \r\nYeah that's it reproduced. The objective shouldn't ever go much below the theoretical minimum.", "There were some changes in the way trainable behaves between 1.x and 2.x; please take a look at the BatchNorm docs [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) under \"About setting layer.trainable = False on a BatchNormalization layer\", as this may be causing the discrepancy.", "@karmel interesting. Using `tensorflow.compat.v1.keras.layers.BatchNormalization` instead appears to resolve the problem (although I haven't tested it extensively).", "@ConorLazarou thanks for the info. It might be one workaround when working with Gan, however when using any type of `tf.keras.Model` you will run into issue, apparently you can not mix `v1` and `v2` layers in a model which kinda make sense.\r\nBut there is definitely something wrong with `BatchNormalization` layer.", "Hello\r\n\r\nwhen I train a GAN_model with `tf.keras.layers.BatchNormalization`(training=True), It is OK.\r\n![image](https://user-images.githubusercontent.com/26713992/81525738-a4f03200-9390-11ea-88b2-ab1d8a4a065f.png)\r\n\r\nBut, when I test the model (training=False), It is failed... \r\n![image](https://user-images.githubusercontent.com/26713992/81525782-ca7d3b80-9390-11ea-8b23-264e0c10cb0c.png)\r\n\r\n\r\nIs it related to this?\r\n\r\nI think, In TF 2.1, for Inference mode in batch normalization(training=False), the model do not use `moving_statistics` learned during training. ", "Any news / updates on this issue on any `2.x` version ?", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/c1fbc77d9d8ff72102113ad4eed4eafb/untitled7.ipynb). Thanks!", "Was able to reproduce your issue in Tf 2.7.0, please find the gist [here](https://colab.research.google.com/gist/kumariko/4ada2f3ca53e79db6462296a82634a10/untitled7.ipynb#scrollTo=fPzJ0Pz_8T8A). Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37673\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37673\">No</a>\n"]}, {"number": 37672, "title": "[r2.2:Cherrypick]Update PIP_BIN_PATH to work with all py versions.", "body": "PiperOrigin-RevId: 301407013\nChange-Id: I86e22da7e18e5c5e0f58d89b1cffaf65d422cef1", "comments": []}, {"number": 37671, "title": "Encoding images as TF_STRING Tensor in the C API", "body": "TF Version : 1.15.0\r\nOS : Windows 10 64-bit\r\nCompiler : MSVC 2017\r\n\r\n\r\nI'm attempting to load a TF SavedModel and run inference on it using the [C API](https://www.tensorflow.org/install/lang_c) for TF version 1.15.0.\r\n\r\n**Essential outlay**:\r\nInput Tensor(s) : 'encoded_image_string_tensor:0'\r\nOutput Tensor(s) [_not exhaustive_] : ['detection_boxes:0' , 'detection_scores:0', 'detection_classes:0']\r\n\r\nWhile there's some documentation in the API header that describes how TF_Tensors of type TF_STRING are encoded, I can't seem to find any concrete examples/illustrations which is probably why I keep running into an error when attempting to encode an image.\r\n\r\nI picked up some parts from [this](https://stackoverflow.com/questions/41138822/how-to-create-a-string-type-tensor-in-tensorflow-c-api) unchecked answer on StackOverflow to get started:\r\n```\r\n// char* image (contains a pointer to the image)\r\n// const unsigned int imageSize (contains the size of the image)\r\n\r\nstd::vector<int64_t> inputDims = { static_cast<int64_t>(TF_DataTypeSize(TF_UINT64)) + static_cast<int64_t>(imageSize) };\r\nsize_t encodedSize = TF_StringEncodedSize(imageSize);\r\nsize_t totalSize = TF_DataTypeSize(TF_UINT64) + encodedSize;\r\nchar* encodedInput = new char[totalSize];\r\nfor (size_t i = 0; i < TF_DataTypeSize(TF_UINT64); i++)\r\n  encodedInput[i] = 0;\r\n\r\nTF_StringEncode((const char*)image.data, imageSize, encodedInput + 8, encodedSize, status);\r\nif (TF_GetCode(status) == TF_OK) {\r\n  std::cerr << \"Failed to encode image\\n\"; // The code enters this block and TF_Message(status) returns nothing to the output stream\r\n  std::cerr << TF_Message(status) << std::endl;\r\n  return false;\r\n}\r\n\t\r\nTF_Tensor* input = TF_NewTensor(TF_STRING, inputDims.data(), inputDims.size(), encodedInput, totalSize, NULL, 0);\r\n\r\n```\r\nIs there any guide on how to encode images as TF_STRING type tensors?\r\n\r\nThanks!\r\n\r\n\r\n\r\n\r\nPlease use [Netron](https://lutzroeder.github.io/netron/) to view the model if necessary\r\n[saved_model.zip](https://github.com/tensorflow/tensorflow/files/4344883/saved_model.zip)\r\n\r\n  ", "comments": ["Your code says \"failed to encode\" if the TF status is OK, which is supposed to mean success.", "(facepalm)\r\nSorry about that. I should have been more careful.\r\n\r\nI now receive an error when I run the session with the input tensor formed from the code above.\r\n\r\n`Malformed TF_STRING tensor; element 0 out of range`\r\n\r\nIt appears that the model expects a TF_String tensor of shape (1, ). I still don't understand how the encoding process works. Could you briefly explain it? \r\n\r\nI know I'm asking a lot, but I'd really appreciate the help.", "@gharibian can you help here?", "Is the same behaviour happening in 2.2.0-rc1/nightly/master?", "> Is the same behaviour happening in 2.2.0-rc1/nightly/master?\r\n\r\nI'm not sure if there's libtensorflow support for TensorFlow 2 yet. This [page](https://www.tensorflow.org/install/lang_c) claims so.\r\n\r\nI understand that the C API isn't meant for inference, but, quite unfortunately, that's my restriction right now.", "Oh, you're right. So we can exclude the `tstring` new API from being the root cause of this ", "I'm fairly certain at this point that the problem lies in the way I'm building the input Tensor. Hence, my request for some illustration/example on how to build a TF_String Tensor from a 3-channel RGB image.", "Figured out the issue. Nothing wrong with the encoding, but with the dimensions.\r\n Here's the corrected code:\r\n\r\n\r\n```\r\nsize_t encoded_size = TF_StringEncodedSize(imageCharSize);\r\nsize_t total_size = 8 + encoded_size;\r\nchar* input_encoded = new char[total_size];\r\nfor (size_t i = 0; i < 8; i++)\r\n\tinput_encoded[i] = 0;\r\n\r\nTF_StringEncode((const char*)imageChar, imageCharSize, input_encoded + 8, encoded_size, status);\r\nif (TF_GetCode(status) != TF_OK) {\r\n\tstd::cerr << \"Failed to encode image\\n\";\r\n\tstd::cerr << TF_Message(status) << std::endl;\r\n\treturn false;\r\n}\r\n\r\n\t\r\nstd::vector<std::int64_t> inputDims = { 1 };\r\nTF_Tensor* input = TF_NewTensor(TF_STRING, inputDims.data(), inputDims.size(), input_encoded, total_size, NULL, 0);\r\n```"]}, {"number": 37670, "title": "AutoGraph could not transform function", "body": "I have tried to reproduce https://www.tensorflow.org/guide/function , and got a \"Please report this to the TensorFlow team.\" message:\r\n\r\n```\r\n#!/usr/bin/env python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\nimport os\r\nassert os.getenv(\"AUTOGRAPH_VERBOSITY\") == \"10\"\r\n@tf.function\r\ndef add(a, b):\r\n  return a + b\r\nadd(tf.ones([2, 2]), tf.ones([2, 2]))\r\n```\r\n\r\noutput:\r\n\r\n> 2020-03-17 18:06:18.294500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\n> 2020-03-17 18:06:19.371349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n> 2020-03-17 18:06:19.384766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-03-17 18:06:19.385191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1\r\n> coreClock: 1.7715GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\r\n> 2020-03-17 18:06:19.385217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\n> 2020-03-17 18:06:19.386622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2020-03-17 18:06:19.387892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n> 2020-03-17 18:06:19.388198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n> 2020-03-17 18:06:19.389675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-03-17 18:06:19.390453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-03-17 18:06:19.393493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-03-17 18:06:19.393632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-03-17 18:06:19.394206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-03-17 18:06:19.394571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> 2020-03-17 18:06:19.394880: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\n> 2020-03-17 18:06:19.414984: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3301330000 Hz\r\n> 2020-03-17 18:06:19.415262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55714b447370 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-03-17 18:06:19.415294: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> 2020-03-17 18:06:19.415474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-03-17 18:06:19.416213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1\r\n> coreClock: 1.7715GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\r\n> 2020-03-17 18:06:19.416246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\n> 2020-03-17 18:06:19.416275: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2020-03-17 18:06:19.416299: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n> 2020-03-17 18:06:19.416319: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n> 2020-03-17 18:06:19.416337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-03-17 18:06:19.416369: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-03-17 18:06:19.416392: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-03-17 18:06:19.416469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-03-17 18:06:19.417223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-03-17 18:06:19.417977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> 2020-03-17 18:06:19.418016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\n> 2020-03-17 18:06:19.888411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-03-17 18:06:19.888453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n> 2020-03-17 18:06:19.888461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n> 2020-03-17 18:06:19.888673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-03-17 18:06:19.889087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-03-17 18:06:19.889446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-03-17 18:06:19.889799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5327 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n> 2020-03-17 18:06:19.891138: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55715b807340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-03-17 18:06:19.891155: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1060 6GB, Compute Capability 6.1\r\n> WARNING:tensorflow:AutoGraph could not transform <function add at 0x7f2cdacf68b0> and will run it as-is.\r\n> Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n> Cause: Bad argument number for Name: 3, expecting 4\r\n> \r\n> \r\n\r\n\r\n\r\n**System information** \r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Arch Linux\r\n- TensorFlow installed from: pacman -S python-tensorflow-cuda\r\n- TensorFlow version (use command below): GIT_VERSION=\"unknown\" VERSION=2.1.0\r\n- Python version: 3.8.2\r\n- CUDA/cuDNN version: Output of nvcc --version :\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Wed_Oct_23_19:24:38_PDT_2019\r\nCuda compilation tools, release 10.2, V10.2.89\r\n- GPU model and memory: GeForce GTX 1060 6GB", "comments": ["I just tried TF 2.1.0 on Linux, Python 3.7.3, and everything works.\r\n\r\nIt is surprising that you have TF 2.1 on Python 3.8, given that the official 2.1 packages do not support Python 3.8. Maybe Arch Linux compiles their own versions? But that may be the cause of the problem.", "You're right, TF 2.x does not support Python 3.8 as written here: https://www.tensorflow.org/install\r\nI use TF 2.1 because it's the newest official version and python 3.8 because it is the newest official version and installing an older version of Python including tensorflow and other python packages is quite a hassle under arch linux. You can either close this issue or rename this issue to \"feature request: support Python 3.8\"", "BTW, TF 2.2 does support Python 3.8 and a rc1 is already available, so you can just `pip install tensorflow==2.2.0rc1` with working Python 3.8 support.", "Thank you. I have also flagged the arch package as out of date (here: https://www.archlinux.org/packages/community/x86_64/tensorflow/). I included a link to this thread in the message to the arch tensorflow maintainer, because I couldn't find another source that said that TF 2.2. support Python 3.8.", "The Github release https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0-rc1 contains information about Python 3.8 being available for Windows and Linux (OS X support should be coming soon I believe).", "Ok, thank you.", "@Volker-Weissmann \r\n\r\n Windows/Linux binaries for py3.8 are available in TF 2.2 .Can you try installing `!pip install tensorflow==2.2.0rc1` and see still you are facing the issue?. Thanks!\r\n`", "The autograph error no longer occurs if I use pip install tensorflow==2.2.0rc1 :+1: \r\n\r\n> 2020-03-23 14:02:45.808164: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n> 2020-03-23 14:02:45.830684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but th\r\n> ere must be at least one NUMA node, so returning NUMA node zero                                                                                            \r\n> 2020-03-23 14:02:45.831287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1\r\n> coreClock: 1.7715GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\r\n> 2020-03-23 14:02:45.831384: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: lib\r\n> cudart.so.10.1: cannot open shared object file: No such file or directory                                                                                  \r\n> 2020-03-23 14:02:45.901102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2020-03-23 14:02:45.929450: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n> 2020-03-23 14:02:45.936238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n> 2020-03-23 14:02:45.990984: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-03-23 14:02:45.998184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-03-23 14:02:46.090721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-03-23 14:02:46.090760: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing librarie\r\n> s mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and se\r\n> tup the required libraries for your platform.                                                                                                              \r\n> Skipping registering GPU devices...\r\n> 2020-03-23 14:02:46.091027: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled\r\n>  to use: AVX2 FMA                                                                                                                                          \r\n> 2020-03-23 14:02:46.111318: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3301250000 Hz\r\n> 2020-03-23 14:02:46.111578: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fafe0000b60 initialized for platform Host (this does not guaran\r\n> tee that XLA will be used). Devices:                                                                                                                       \r\n> 2020-03-23 14:02:46.111593: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> 2020-03-23 14:02:46.112797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-03-23 14:02:46.112807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108] ", "@Volker-Weissmann \r\n\r\nPlease, let me know if we can close this issue since it looks to be fixed. Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37670\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37670\">No</a>\n"]}, {"number": 37669, "title": "Scatter nd add has legacy docstrings from scatter nd", "body": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add\r\n\r\n## Description of issue (what needs changing):\r\nThese sentences: \r\n> `indices` is an integer tensor containing indices into a new tensor of shape `shape`. The last dimension of `indices` can be at most the rank of `shape`:\r\n\r\nSeem to be direct copy-paste from the docs of [`scatter_nd`](https://www.tensorflow.org/api_docs/python/tf/scatter_nd). However, there is no `shape` args in `scatter_nd_add`.\r\n\r\n### Clear description\r\n\r\nThe docs should instead refer to `tensor.shape`.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? There is no link (that's another issue though I guess), only to the tf v1\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? Yes \r\n\r\n### Returns defined\r\n\r\nAre return values defined? Yes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? No errors raised apparently\r\n\r\n### Usage example\r\n\r\nIs there a usage example? Yes\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content? No, but there are some in `scatter_nd`, I guess it's enough\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n\r\nNot right now\r\n", "comments": ["@zaccharieramzi , can you explain the issue using example and elaborate it ? The given description is not enough to clearly understand the issue. Thanks!", "Well it's a doc issue so it's not really fit for an example. I just meant that the docs for `tensor_scatter_nd_add` should not mention a `shape` argument that is not part of the arguments of `tensor_scatter_nd_add`.", "Thanks for the quick reply @zaccharieramzi . Are you able to locate the file where `tensor_scatter_nd_add` is implemented? If Yes, then please also provide the link. I am not able to locate the method.", "Like I pointed in the issue there is no link to the source code in the docs (that's another problem I guess). I think it's because it's a C++ op.\r\nHowever, I don't understand why you would need it to understand this particular issue.\r\nAnyway, I think the function is implemented here: https://github.com/tensorflow/tensorflow/blob/5214758e1f9335008a788be554633d974f569cf1/tensorflow/compiler/tf2xla/kernels/scatter_nd_op.cc#L172", "@zaccharieramzi , I have found the code with the doc [here](https://github.com/tensorflow/java/blob/master/tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/core/TensorScatterNdAdd.java).", "Oh you meant the docstring code, right.", "@zaccharieramzi and @mihaimaruseac , I have raised PR [#35](https://github.com/tensorflow/java/pull/35) to resolve this issue. Please review.", "@mihaimaruseac , sorry for wrong PR. review #37832 for this issue."]}, {"number": 37668, "title": "TFLu: replace old cmsis scratch buffer", "body": "", "comments": []}, {"number": 37667, "title": "Inconsistently sized samples with TimeseriesGenerator", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): **Win 10**\r\n- TensorFlow installed from (source or\r\nbinary): **Binary** \r\n- TensorFlow version (use command below): **v2.1.0**\r\n- Python version: **3.6.10**\r\n\r\n**Describe the current behavior**\r\n`tensorflow.keras.preprocessing.sequence.TimeseriesGenerator` generates inconsistently sized samples depending on parameters passed to `length, batch_size`. Ends up throwing input shape errors when passed into a model such as LSTM.\r\n\r\n**Describe the expected behavior**\r\nEach sample should be of size `(batch_size, length, n_features), (batch_size,)`.\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport numpy as np\r\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\r\n\r\ndef GenerateData(TIMESTEPS, BATCHSIZE, N_FEATURES, N_SAMPLES):\r\n    \"\"\"\r\n    Generates batches of one-step forward samples (x_1,...,x_n), (y_n+1)\r\n    Args: TIMESTEPS (int) Number of past observations\r\n          BATCHSIZE (int) Number of samples in each batch\r\n          N_FEATURES (int) Number of features in dataset\r\n          N_SAMPLES (int) Length of dataset\r\n\r\n    Each generated sample should be shape (BATCHSIZE, TIMESTEPS, N_FEATURES), (BATCHSIZE,)\r\n    \"\"\"\r\n    # Dummy multivariate series\r\n    dummy_x = np.random.rand(N_SAMPLES, N_FEATURES)\r\n    dummy_y = np.array([x[0] for x in dummy_x]) # Predict future values of first column\r\n    print(f'Data X Shape: {dummy_x.shape}, Data Y Shape: {dummy_y.shape}')\r\n\r\n    generator = TimeseriesGenerator(data=dummy_x, targets=dummy_y,\r\n                                    length=TIMESTEPS,\r\n                                    batch_size=BATCHSIZE,\r\n                                    sampling_rate=1,\r\n                                    stride=1\r\n                                   )\r\n\r\n    # Check whether each sample is shaped correctly\r\n    for n in range(len(generator)):\r\n        x,y = generator[n]\r\n        if x.shape != (BATCHSIZE, TIMESTEPS, N_FEATURES):\r\n            print(f'Check index {n}.')\r\n```\r\n\r\n**Tests:**\r\n```\r\n[In]\r\nGenerateData(TIMESTEPS=5, BATCHSIZE=2, N_FEATURES=5, N_SAMPLES=1000)\r\n[Out]\r\nData X Shape: (1000, 5), Data Y Shape: (1000,)\r\nCheck index 497.\r\nIndex 497 shapes: (1, 5, 5), (1,)\r\n\r\n[In]\r\nGenerateData(TIMESTEPS=5, BATCHSIZE=2, N_FEATURES=5, N_SAMPLES=1001)\r\n[Out]\r\nData X Shape: (1001, 5), Data Y Shape: (1001,)\r\n\r\n[In]\r\nGenerateData(TIMESTEPS=6, BATCHSIZE=2, N_FEATURES=5, N_SAMPLES=1001)\r\n[Out]\r\nData X Shape: (1001, 5), Data Y Shape: (1001,)\r\nCheck index 497.\r\nIndex 497 shapes: (1, 6, 5), (1,)\r\n\r\n[In]\r\nGenerateData(TIMESTEPS=6, BATCHSIZE=32, N_FEATURES=5, N_SAMPLES=1001)\r\n[Out]\r\nData X Shape: (1001, 5), Data Y Shape: (1001,)\r\nCheck index 31.\r\nIndex 31 shapes: (3, 6, 5), (3,)\r\n```", "comments": ["In the tests above, only the parameters `length=2, batch_size=2, n_samples=1001` generates consistently sized samples.", "Oh, This problem also occurs on my machine.\r\n\r\n#### System information\r\n\r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n\r\n- OS Platform and Distribution:Ubuntu18.04 LTS\r\n- TensorFlow installed from (source or\r\nbinary): pip\r\n- TensorFlow version (use command below): v2.1.0\r\n- Python version: 3.6.10\r\n", "Something that might be helpful information: If `N_SAMPLE % BATCH_SIZE == TIMESTEPS`, samples are consistently shaped. The converse is generally not true.", "@adamw96, I tried to replicate this issue on colab with Tf 2.1 but I am not seeing any output. \r\nPlease find the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/074aaff597159ed7ed42e116fb9da7da/untitled465.ipynb) and provide more information to replicate the issue. Thanks! ", "@gadagashwini Added some tests to the [gist](https://colab.research.google.com/gist/adamw96/0471618572a3848a834f5acf4fa0a424/untitled465.ipynb). I think it'll be helpful if `TimeseriesGenerator` discards the first `length + k` values where `k` is chosen such that the remaining data can be divided into evenly sized samples. Otherwise with inconsistent batch shapes, if fed into a model (say, LSTM) this throws an input shape error.", "I could reproduce the issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/6cb9012e4d81c2f02c28d983dde22444/untitled474.ipynb). Thanks!", "I could reproduce the issue with Tf 2.2-rc3.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/31cbd02f23b9fd9bcedbdf1e0ba01bab/untitled843.ipynb).Thanks!", "@adamw96 , the TimeseriesGenerator has been replaced with an updated utility that works directly with TF datasets: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array\r\n\r\nThat is available in the nightlies now; does that work for your use-case?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@adamw96 \r\n\r\nCan you please try with TF nightly versions and see if the problem still persists.Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37667\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37667\">No</a>\n"]}, {"number": 37666, "title": "enable_mixed_precision_graph_rewrite error", "body": "tensorflow  1.14 and ubuntu 18.04\r\n \r\nI'm trying the mixed precision training using the `tf.train.experimental.enable_mixed_precision_graph_rewrite`. \r\nBy adding the wrapper of `enable_mixed_precision_graph_rewrite` to the `dnn_optimizer` in the [census wide-n-deep model](https://github.com/baidu-research/tensorflow-allreduce/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py#L147), I find the tensorflow internal error of `ValueError: Tensor(\"current_loss_scale/Read/ReadVariableOp:0\", shape=(), dtype=float32) must be from the same graph as Tensor(\"head/weighted_loss/Sum:0\", shape=(), dtype=float32)`\r\n\r\n```\r\n    m = tf.estimator.DNNLinearCombinedClassifier(\r\n        model_dir=model_dir,\r\n        linear_feature_columns=crossed_columns,\r\n        dnn_feature_columns=deep_columns,\r\n        dnn_hidden_units=[100, 50])\r\n```\r\nis changed to\r\n```\r\n    m = tf.estimator.DNNLinearCombinedClassifier(\r\n        model_dir=model_dir,\r\n        linear_feature_columns=crossed_columns,\r\n        dnn_feature_columns=deep_columns,\r\n        dnn_optimizer=tf.train.experimental.enable_mixed_precision_graph_rewrite(\r\n              tf.compat.v1.train.GradientDescentOptimizer(0.05)),\r\n        dnn_hidden_units=[100, 50])\r\n```\r\nMaybe it's a compatible error of estimator and enable_mixed_precision_graph_rewrite, please give some help, thanks.\r\n", "comments": ["@sjtusmartboy \r\nCould you please provide with simple stand alone code for us to replicate and analyse, if possible you could share a colab gist.", "@Saduf2019 \r\n\r\n[wide_deep.zip](https://github.com/tensorflow/tensorflow/files/4346521/wide_deep.zip)\r\n\r\nPlease just unzip the wide_deep.zip and run the command of `bash run.sh` in the environment of python 2.7 with tensorflow 1.14.\r\n\r\nIf the code line of `dnn_optimizer=tf.train.experimental.enable_mixed_precision_graph_rewrite(tf.compat.v1.train.GradientDescentOptimizer(0.05)),\r\n` is deleted, the whole project runs normally. \r\n\r\nIf not, the project has the error of `ValueError: Tensor(\"current_loss_scale/Read/ReadVariableOp:0\", shape=(), dtype=float32) must be from the same graph as Tensor(\"head/weighted_loss/Sum:0\", shape=(), dtype=float32).`\r\n\r\nPlease check the result, thanks", "Another question is that how much gpu memory is required to train a model with N parameters if it is trained with Adagrad when using `enable_mixed_precision_graph_rewrite`?@Saduf2019", "@reedwm hi, is there any update on this issue?", "I encountered the same issue with keras and all `tf.compat.v1.train` optimizers in tf2.0. One walk around in my case is to use the counterpart in `tf.keras.optmizers`.", "@sjtusmartboy, can you give a smaller example to reproduce? Alternatively, it looks like you branched off from the official models, so can you point me to a fork of the official models on GitHub that reproduces the issue? I am hesitant to run a lot of code in a zip file without looking through all the code first. ", "@reedwm I have simplified the code in one file(wide_deep.py)from the original [code](https://github.com/tensorflow/models/tree/d0600d40d02bd927caf97a30895dc482db5d01ab/official/r1/wide_deep) as possible as I can. The model is just wide-deep model with linear column and dnn column. I think it's the problem of using the `enable_mixed_precision_graph_rewrite` for the parameter of `dnn_optimizer` in the `tf.estimator.DNNLinearCombinedClassifier`. So maybe the provided code above is the most simple code that can be submit.  Please recheck, thanks.\r\n\r\n", "@zhuzilin Thank you for your advice. Now I've change the code to \r\n\r\n`\r\n\r\n    m = tf.estimator.DNNLinearCombinedClassifier(\r\n        model_dir=model_dir,\r\n        linear_feature_columns=crossed_columns,\r\n        dnn_feature_columns=deep_columns,\r\n        dnn_optimizer=tf.train.experimental.enable_mixed_precision_graph_rewrite(\r\n                tf.keras.optimizers.SGD(0.05)),\r\n        dnn_hidden_units=[100, 50])\r\n\r\nUnfortunately there seems to have some other bugs, `ValueError: The given object is not an Optimizer instance. Given:<tensorflow.python.keras.mixed_precision.experimental.loss_scale_optimizer.LossScaleOptimizer object at 0x7fcbc1943350>`\r\n\r\n\r\n", "@sjtusmartboy It seems like estimator in 1.14 cannot use v2 optimizer yet. if it's not convenient to upgrade tf, maybe my fix in this pr would help #37965 ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37666\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37666\">No</a>\n"]}, {"number": 42787, "title": "Support Arabic and RTL in Tensorflow documentation", "body": "As suggested by @lamberta, I am creating this issue to gather information about the extent to which Tensorflow website supports Arabic and RTL (Right to left) layout in general. \r\n\r\nIn fact, I started translating two of Tensorflow's tutorials into Arabic with the following two PRs:\r\n* https://github.com/tensorflow/docs-l10n/pull/121\r\n* https://github.com/tensorflow/docs-l10n/pull/122\r\n\r\nKnowing that Google Colab, like jupyter notebooks, supports html tags, I used them to ensure that the RTL text is displayed correctly, while keeping the LTR (left to right) layout for everything else in the tutorial, since the code needs to be LTR. Which results in a correct page layout as you can see in the following two notebooks rendering from the above two PRs:\r\n\r\n* https://colab.research.google.com/github/abjed/docs-l10n/blob/ar-quickstart/site/ar/tutorials/quickstart/beginner.ipynb\r\n* https://colab.research.google.com/github/abjed/docs-l10n/blob/ar-quickstart-advanced/site/ar/tutorials/quickstart/advanced.ipynb\r\n\r\nOpening these notebooks in https://nbviewer.jupyter.org/ , which renders the notebook to html,  shows that the RTL layout defined by wrapping the Arabic text with `<div dir=\"rtl\"></div>` works as intended. As you can see in the following link:\r\n\r\n* https://nbviewer.jupyter.org/github/abjed/docs-l10n/blob/ar-quickstart/site/ar/tutorials/quickstart/beginner.ipynb\r\n\r\nSo, my questions, are: \r\n\r\n1. Does the pipeline that generates the tutorials from the notebooks preserves these HTML tags, and displays them correctly (we might need to do an experiment)?\r\n2. If not is there an alternative to support Arabic (RTL) properly? \r\n\r\nThanks. ", "comments": ["Thanks @mohamed-ali \r\n\r\nJust making a note to test wrapping Markdown text within a `<div>` since I think this doesn't work in some Markdown parsers (for the site?)\r\n", "@lamberta I understand that wrapping the markdown with `<div dir=\"rtl\"></div>` could disable the markdown parser on the wrapped text. However, for a first version, having an understandable/readable content with the correct display is more important than minor styling. Also, as an arabic reader, a text mixing RTL and LTR words, if not properly displayed, loses all meaning. \r\n\r\nOn the other hand, most of the stylings impacted with the `<div dir=\"rtl\"></div>` workaround can be replaced with standard html alternatives very easily. Here are some examples:\r\n\r\nLinks\r\n```\r\n[Tensorflow website](https://www.tensorflow.org/)\r\n```\r\nis equivalent to\r\n```html\r\n<a href=\"https://www.tensorflow.org/\">Tensorflow website</a>\r\n```\r\n\r\nCode\r\n```\r\n`tf.Keras`\r\n```\r\nis equivalent to\r\n```\r\n<code>tf.Keras</code>\r\n```\r\netc. \r\n\r\nSo I suggest we move forward and do an initial experiment with these html workarounds. \r\nWe can always improve iteratively in the future.\r\n\r\n@lamberta  What do you think? ", "> having an understandable/readable content with the correct display is more important than minor styling.\r\n\r\nIt will break elements like lists. Also, among our preprocessing steps, we use backticks around code symbols to auto-link to the corresponding API reference page.\r\n\r\nI support adding right-to-left support, but we need to investigate where in the pipeline this should be added. Requiring Arabic community contributors to know when to use this particular element vs something else is too much---which will lead to more bugs filed when pages don't render as intended.\r\n\r\n> I suggest we move forward and do an initial experiment with these html workarounds. We can always improve iteratively in the future.\r\n\r\nI agree that we will need to iterate. Unfortunately, due to the COVID-19 situation, we are working on this feature request as time allows. See Martin's [message](https://groups.google.com/a/tensorflow.org/d/msg/docs/BdZpILm3__g/9FBJcONfAwAJ). Thanks", "@lamberta  It seems that there is an easier alternative to achieve correct RTL display: The layout can be fixed as a post-processing using the following CSS. \r\n\r\n```css\r\np, ul, ol, h1, h2, h3, h4, h5, h6 {\r\n  direction: rtl;\r\n}\r\n``` \r\n\r\nThis has the added advantage of keeping the current pipeline with all its features, that you described, intact.\r\n\r\nI tried these rules on the live website https://www.tensorflow.org/tutorials/keras/classification and it changes the relevant page sections to RTL properly.\r\n\r\n<img width=\"512\" alt=\"Screenshot 2020-03-18 at 18 21 33\" src=\"https://user-images.githubusercontent.com/2883926/76989272-6026bc80-6946-11ea-9476-e28d02f996cd.png\">\r\n<img width=\"518\" alt=\"Screenshot 2020-03-18 at 18 21 46\" src=\"https://user-images.githubusercontent.com/2883926/76989276-6157e980-6946-11ea-9dac-3cbf2dda5e78.png\">\r\n<img width=\"522\" alt=\"Screenshot 2020-03-18 at 18 21 59\" src=\"https://user-images.githubusercontent.com/2883926/76989284-6452da00-6946-11ea-843e-0894ce5d0b29.png\">\r\n\r\nIf it's possible to add a custom css file per language then the problem should be easy to solve. ", "Ah, that is interesting. Good one\r\nI've filed an internal issue (b/151835181) to communicate with the site infra and Colab teams to see if we can decide on the \"right\" solution for this.\r\n\r\nIf we add the CSS for the site, do any browser display settings kick in on COlab if it detects a locale or anything?", "If we want to, it is possible to add a logic that loads the rtl file when the user is accessing the website from a country that requires RTL. \r\n\r\nI also checked on colab, the same css rules as above work\r\n```css\r\np, ul, ol, h1, h2, h3, h4, h5, h6 {\r\n  direction: rtl;\r\n}\r\n```\r\nHere is the original link https://colab.research.google.com/notebooks/intro.ipynb, and here's how it looks with the css corrections:\r\n\r\n<img width=\"707\" alt=\"Screenshot 2020-03-18 at 18 45 03\" src=\"https://user-images.githubusercontent.com/2883926/76991135-623e4a80-6949-11ea-9531-bd4e436c771b.png\">\r\n<img width=\"705\" alt=\"Screenshot 2020-03-18 at 18 45 19\" src=\"https://user-images.githubusercontent.com/2883926/76991137-636f7780-6949-11ea-93ab-037d42a31776.png\">\r\n\r\nTo help colab load the css for RTL, we can send a hint as url parameter in the GET request , for example:\r\n\r\nhttps://colab.research.google.com/notebooks/intro.ipynb?dir=rtl \r\n\r\nAny other way to detect the language or locale of the text can also be used to decide whether or not to load the RTL css file.\r\n", "Ok, the site infra for tensorflow.org should already support this (though we need to test).\r\nColab does not support RTL but there is a feature request (b/110534608) and I'll bump that.\r\n\r\nSo, for now, let's add a notebook *without* `rtl` divs or CSS to see where we're at. And then work on some of the underlying infra issues before adding workarounds (which may complicate our docs pipeline).\r\n", "Ok, new plan ...\r\n\r\nAt this time, there are no plans for Colab to support the \"toggle rtl layout\" preference that Jupyter currently has. So:\r\n\r\n1. `ar/` notebooks will need to wrap all *text* cells in `<div dir=\"rtl\">`. This seems to render the inner Markdown and HTML content correctly in both Colab and Jupyter.\r\n2. We need to add support to our docs pipeline to strip these divs before building the HTML for the site. The site should currently work as-is.\r\n\r\nWill not be using CSS or URL params.", "That's great. \r\n\r\nFew clarifications about the notebooks linked above which are rendered from the two PRs I put in the issue description:\r\n\r\nLinks don't seem to be rendered correctly:\r\n```\r\n[something](https://somelink.com) \r\n```\r\nTherefore, I wrapped them with\r\n```\r\n<a href=\"https://somelink.com\">something</a>\r\n```\r\n\r\nCode doesn't render \r\n```\r\n`tf.Keras`\r\n```\r\n\r\nTherefore, I used `<code>`\r\n```\r\n<code>tf.Keras</code>\r\n```\r\n\r\nThe `code` isn't  mandatory, it  can be kept as is. However rendering links correctly is useful.\r\n\r\nPotential solution:\r\n\r\n1. Strengthen colab's ability to render these tags even within `<div dir=\"rtl\"></div>`\r\n2. I can push a PR to the pipeline to convert the tags html back to their markdown counterparts before preparing the website. They can remains in the notebooks for correct rendering.\r\n\r\n", "We can use this PR to test the above plan: https://github.com/tensorflow/docs-l10n/pull/121 ", "My tests in Colab show the following renders correctly in a text cell. Are you seeing something different? ...\r\n\r\n```\r\n<div dir=\"rtl\">\r\n\r\n* Foo\r\n* [Bar](http://localhost)\r\n* `tf.keras`\r\n\r\n</div>\r\n```\r\n\r\nAs mentioned, among our preprocessing steps, we use backticks around code symbols to auto-link to the corresponding API reference page. Aside from adding the wrapping `<div dir=\"rtl\">` formatting should remain consistent with the source-of-truth. Since these notebook are rendered in a few places, the priorities are: 1. tensorflow.org, 2. Colab, 3. Jupyter, 4. GitHub preview.\r\n", "@lamberta You are right, what your example renders correctly\r\n\r\n```\r\n<div dir=\"rtl\">\r\n\r\n* Foo\r\n* [Bar](http://localhost)\r\n* `tf.keras`\r\n\r\n</div>\r\n```\r\n\r\nHowever, it doesn't render correctly without the whitespace, as follows:\r\n\r\n```\r\n<div dir=\"rtl\">\r\n* Foo\r\n* [Bar](http://localhost)\r\n* `tf.keras`\r\n</div>\r\n```", "@lamberta  I updated the PR https://github.com/tensorflow/docs-l10n/pull/121 to remove all the use of html except `<div dir=\"rtl\"></div>`. Can we use it to test the output? ", "Yes, looks good. Thanks\r\nI tagged in @baheytharwat to review that notebook. Once merged, we can use that to test. We still need to add functionality on our side, though.\r\n", "This is now live: https://www.tensorflow.org/tutorials/quickstart/beginner?hl=ar", "Great! Thanks, Yash.\r\nAnd congrats to @mohamed-ali , @baheytharwat , and @tayciryahmed . I look forward to seeing the community grow", "It's great to see the results. Thanks @lamberta and @yashk2810 for your assistance in bringing this to life. \r\n\r\nThanks @baheytharwat  and @tayciryahmed for your contributions and reviews. ", "No problem :)\r\n\r\nKeep those PR's coming!"]}, {"number": 37665, "title": "Added Adaptive pooling layers to tf.keras", "body": "Added the following layers in the file tensorflow/python/keras/layers/adaptive_pooling.py:\r\nAdaptiveAveragePooling1D\r\nAdaptiveAveragePooling2D\r\nAdaptiveAveragePooling3D\r\nAdaptiveMaxPooling1D\r\nAdaptiveMaxPooling2D\r\nAdaptiveMaxPooling3D\r\n", "comments": ["Can you move this to tf_addons?", "I've submitted the PR to tensorflow_addons. Thank you.", "Shall I close this PR?", "> Shall I close this PR?\r\n\r\nThat's great. Thanks! Closing it."]}, {"number": 37664, "title": "tf.sparse.SparseTensor and tf.repeat doc", "body": "- Afaik, tf.SparseTensor is currently just an alias for tf.sparse.SparseTensor. But since the later might get removed in the future, suggest replacing usage with the more consistent alias.\r\n\r\n- Change \"repeat\" to \"tf.repeat\" for consistency in this doc and easier to copy and experiment with the code.", "comments": ["@Angus-Luo Can you please address Ubuntu Sanity errors? Thanks!", "@Angus-Luo Still,  Ubuntu Sanity errors appearing. Can you please check? Thanks!", "pylint fails. Please donwload the sanity log and locate the pylint error.", "> pylint fails. Please donwload the sanity log and locate the pylint error.\r\n\r\nThank you, I will try it soon."]}, {"number": 37663, "title": "r1.15 cherry-pick request: Fix linking issue on TFLite r1.15 branch.", "body": "\u2026btensorflow-lite.a\r\n\r\nlite/kernels/rfft2d.cc has reference to rdft2d.\r\nAs a consequence, libtensorflow-lite.a need to include fft2d/fftsg2d.c source\r\nin its build.\r\n\r\nIf fftsg2d.c is not part of libtensorflow-lite.a, a C/C++ application\r\nthat use the libtensorflow-lite.a static library is not able to link\r\nwith the following error:\r\nrfft2d.cc:(.text+0x594): undefined reference to `rdft2d'\r\n\r\nSigned-off-by: Vincent ABRIOU <vincent.abriou@st.com>", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37663) for more info**.\n\n<!-- need_author_consent -->", "@apivovarov this PR will fix r1.15 branch.", "We will only merge this if we are doing another 1.15 patch release.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37663) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 37662, "title": "dimension lost by using tf_trt", "body": "I use:\r\nOS: ubuntu 18.04\r\ncuda: 10.0\r\ncudnn: 7.6.3\r\npython:3.6\r\ntensorflow-gpu:1.15\r\nGPU: nvidia jetson xavier kit\r\nmemory: 32G\r\n\r\nthe source code line is [https://github.com/nwojke/deep_sort/blob/280b8bdb255f223813ff4a8679f3e1321b08cdfc/tools/generate_detections.py#L71](url)\r\nthe tf model is [https://drive.google.com/open?id=1bB66hP9voDXuoBoaCcKYY7a8IYzMMs4P](url)\r\nold code : \r\n```\r\n74    def __init__(self, checkpoint_filename, input_name=\"images\",\r\n75                output_name=\"features\"):\r\n76        self.session = tf.Session()\r\n77        with tf.gfile.GFile(checkpoint_filename, \"rb\") as file_handle:\r\n78             graph_def = tf.GraphDef()\r\n79             graph_def.ParseFromString(file_handle.read())\r\n80         tf.import_graph_def(graph_def, name=\"net\")\r\n81         self.input_var = tf.get_default_graph().get_tensor_by_name(\r\n82             \"net/%s:0\" % input_name)\r\n83         self.output_var = tf.get_default_graph().get_tensor_by_name(\r\n84             \"net/%s:0\" % output_name)\r\n85 \r\n86         assert len(self.output_var.get_shape()) == 2\r\n87         assert len(self.input_var.get_shape()) == 4\r\n88         self.feature_dim = self.output_var.get_shape().as_list()[-1]\r\n89         self.image_shape = self.input_var.get_shape().as_list()[1:]\r\n```\r\nnew code with tf_trt : \r\n```\r\n91    def __init__(self, checkpoint_filename, input_name=\"images\",\r\n92                 output_name=\"features\"):\r\n93        self.session = tf.Session()\r\n94        with tf.gfile.GFile(checkpoint_filename, \"rb\") as file_handle:\r\n95            graph_def = tf.GraphDef()\r\n96            graph_def.ParseFromString(file_handle.read())\r\n97        converter = trt.TrtGraphConverter(input_graph_def=graph_def, nodes_blacklist=[\"images:0\", \"features:0\"])\r\n98        trt_graph = converter.convert()\r\n99        tf.import_graph_def(trt_graph, name=\"net\")\r\n100       self.input_var = tf.get_default_graph().get_tensor_by_name(\r\n101           \"net/%s:0\" % input_name)\r\n102       self.output_var = tf.get_default_graph().get_tensor_by_name(\r\n103           \"net/%s:0\" % output_name)\r\n104\r\n105       assert len(self.output_var.get_shape()) == 2\r\n106       assert len(self.input_var.get_shape()) == 4\r\n107       self.feature_dim = self.output_var.get_shape().as_list()[-1]\r\n108        self.image_shape = self.input_var.get_shape().as_list()[1:]\r\n```\r\nerror is follow : \r\n```\r\nTraceback (most recent call last):\r\n  File \"tools/generate_detections.py\", line 235, in <module>\r\n    main()\r\n  File \"tools/generate_detections.py\", line 229, in main\r\n    encoder = create_box_encoder(args.model, batch_size=32)\r\n  File \"tools/generate_detections.py\", line 120, in create_box_encoder\r\n    image_encoder = ImageEncoder(model_filename, input_name, output_name)\r\n  File \"tools/generate_detections.py\", line 105, in __init__\r\n    assert len(self.output_var.get_shape()) == 2\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\", line 827, in __len__\r\n    raise ValueError(\"Cannot take the length of shape with unknown rank.\")\r\nValueError: Cannot take the length of shape with unknown rank.\r\n```\r\n\r\nthen i print input and output tensor : \r\n```\r\nprint(self.input_var)\r\nprint(self.output_var)\r\n```\r\nthe result is :\r\nold code : \r\n```\r\nTensor(\"net/images:0\",  shape=(?, 128, 64, 3), dtype=uint8)\r\nTensor(\"net/features:0\", shape=(?, 128), dtype=float32)\r\n```\r\nnew code :\r\n```\r\nTensor(\"net/images:0\",  shape=(?, 128, 64, 3), dtype=uint8)\r\nTensor(\"net/features:0\", dtype=float32)\r\n```\r\nfrom above comparison, can found that new code lost tensor \"net/features:0\" dimension. please give me some advise, thank you\r\n", "comments": ["@chennuo0125 \r\n\r\nWill it be possible to share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "thank you for your respond @ravikyram, you can do as follows to reproduce the issue\r\n```\r\ngit clone https://github.com/chennuo0125/deep_sort.git\r\n./generate.sh\r\n```\r\nand you will get the error follows : \r\n```\r\nTensor(\"net/images:0\", shape=(?, 128, 64, 3), dtype=uint8)\r\nTensor(\"net/features:0\", dtype=float32)\r\nTraceback (most recent call last):\r\n  File \"tools/generate_detections.py\", line 242, in <module>\r\n    main()\r\n  File \"tools/generate_detections.py\", line 236, in main\r\n    encoder = create_box_encoder(args.model, batch_size=32)\r\n  File \"tools/generate_detections.py\", line 127, in create_box_encoder\r\n    image_encoder = ImageEncoder(model_filename, input_name, output_name)\r\n  File \"tools/generate_detections.py\", line 112, in __init__\r\n    assert len(self.output_var.get_shape()) == 2\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\", line 827, in __len__\r\n    raise ValueError(\"Cannot take the length of shape with unknown rank.\")\r\nValueError: Cannot take the length of shape with unknown rank.\r\n```", "@chennuo0125 I also encountered the issue. You can check this issue: [#25417](https://github.com/tensorflow/tensorflow/issues/25417)\r\n\r\nAs they say, in most cases, the shape is not useful.\r\n\r\nHowever if you really need it, I did the following work-around:\r\n```\r\n        with tf.Graph().as_default() as graph:\r\n            tf.graph_util.import_graph_def(converter._converted_graph_def, name=\"\")\r\n            # You need to get the list of output_tensors before conversion  \r\n            for tensor in output_tensors:\r\n                graph.get_tensor_by_name(tensor.name).set_shape(tensor.shape)\r\n\r\n            # Overwrite the converter graph def\r\n            converter._converted_graph_def = graph.as_graph_def(add_shapes=True)\r\n\r\n        # Save saved model\r\n        converter.save(output_saved_model_dir)\r\n\r\n```", "thank your advise @jnd77  i follow your advise to saved optimized model , but get error as follows : \r\n```\r\nValueError: Not able to save to a SavedModel since input is a GraphDef\r\n``` \r\nthen i change save method as follows and get success\r\n```\r\nwith tf.gfile.GFile(\"optimized.pb\", \"wb\") as f:\r\n            f.write(trt_graph.SerializeToString())\r\n```\r\nbut above convertion cost too much time by about 8 minutes, its terriable. then i load optimized model directly  as follows : \r\n```\r\n        with tf.gfile.GFile(checkpoint_filename, \"rb\") as file_handle:\r\n            graph_def = tf.GraphDef()\r\n            graph_def.ParseFromString(file_handle.read())\r\n        tf.import_graph_def(graph_def, name=\"net\")\r\n```\r\nbut the loading also cost too much time by about 8 minutes, its too slowly, please give me some advise, thank you very much :)\r\n\r\nabove code link : [https://github.com/chennuo0125/deep_sort](https://github.com/chennuo0125/deep_sort)", "Am not familiar with saving/loading GraphDef, so I can't help with that.\r\nIn any case, you would have the same issue I guess even without the shapes.\r\n\r\nIf you don't plan to save the graph, then you don't have to. :)\r\n", "thank you @jnd77 , yes, when i run another project without shapes, using the tf_trt to optimize the model, which also cost time too much . above indicts that using tf_trt to optimize model need much time ?   \r\ni have methond this issue here : [https://github.com/tensorflow/tensorflow/issues/37661](https://github.com/tensorflow/tensorflow/issues/37661)\r\nexpect for your solution, thank you very much :)", "Normally you would run the TensorRT optimization only once and save the model.\r\nWhen you need to do inference, then you would load that model.\r\nLoading can indeed take a while, but it's an upfront cost. You can then run inference as many times as you want.\r\nI'm not aware of how the loading could be made faster ... If you have memory issues, maybe you can play with the `max_workspace_size_bytes` parameter when building the TrtGraphConverter.\r\n\r\nI just noticed you were using: `nodes_blacklist=[\"images:0\", \"features:0\"]`\r\nMaybe it should be `nodes_blacklist=[\"images\", \"features\"]`, and it would keep the shape.", "thank for your advise @jnd77 , i want to guarantee that using tf_trt optimized model need more memory corresponding to no optimized model ?", "Hmm sorry, I don't know, I never had any memory issues, so never looked at it.\r\n", "@chennuo0125 Please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/19619) where memory consumption has been discussed in detail. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@chennuo0125 \r\n\r\nAny update on this issue please. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37662\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37662\">No</a>\n"]}, {"number": 37661, "title": "use tf_trt to optimize model, but cost too much time and too much memory consumed ", "body": "I use:\r\nOS: ubuntu 18.04\r\ncuda: 10.0\r\ncudnn: 7.6.3\r\npython:3.6\r\ntensorflow-gpu:1.15\r\nGPU: nvidia jetson xavier kit\r\nmemory: 32G\r\n\r\nsource code line is [https://github.com/YunYang1994/tensorflow-yolov3/blob/eb02b0731dfb1c2614e216c619ac7b22db57d994/core/utils.py#L126](url)\r\nand changed as follows : \r\n```\r\n    converter = trt.TrtGraphConverter(input_graph_def=frozen_graph_def,\\\r\n            nodes_blacklist=return_elements,\\\r\n            is_dynamic_op=True,\\\r\n            precision_mode=trt.TrtPrecisionMode.FP16,\\\r\n            max_workspace_size_bytes=1*10**9,\\\r\n            max_batch_size=10\r\n            )\r\n    trt_graph = converter.convert()\r\n    with graph.as_default():\r\n        return_elements = tf.import_graph_def(trt_graph,\r\n                                              return_elements=return_elements)\r\n```\r\nthe result fps is ok, detection fps improve from 5 to 15, but which cost too much time and memory. usually i need to wait for 15 minutes to get the model loaded, besides memory cost too much and just 1G surplus (total 32G), sometimes no enough memory to run the program and program  is killed itself.\r\nthen i saved converted model as follows:\r\n```\r\n    # save optimized graph\r\n    with tf.gfile.GFile(\"/home/xavier/python_ws/models/optimized_yolov3_f16.pb\", \"wb\") as f:\r\n        f.write(trt_graph.SerializeToString())\r\n```\r\nand load the optimized model as follows : \r\n```\r\n    with tf.gfile.FastGFile(pb_file, 'rb') as f:\r\n        frozen_graph_def = tf.GraphDef()\r\n        frozen_graph_def.ParseFromString(f.read())\r\n\r\n    with graph.as_default():\r\n        return_elements = tf.import_graph_def(frozen_graph_def,\r\n                                              return_elements=return_elements)\r\n```\r\nbut the result is as previous, and the problem dont get improved, how can i to solve the problem ?", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 37660, "title": "Table not initialized with TF-TRT", "body": "**System information**\r\nCentos 7\r\nTITAN Xp\r\n\r\nCUDA 10\r\nTensorflow 1.15.0\r\nPython 2.7\r\n\r\n**Describe the current behavior**\r\nI want to convert my model to TRT-INT8, however, when i get into converter.calibrate function, i get the error:\r\nFile \"/usr/lib64/python2.7/site-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py\", line 612, in calibrate\r\n    fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)\r\n  File \"/usr/lib64/python2.7/site-packages/tensorflow_core/python/client/session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib64/python2.7/site-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/lib64/python2.7/site-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"/usr/lib64/python2.7/site-packages/tensorflow_core/python/client/session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n**FailedPreconditionError: Table not initialized.**\r\n         **[[node index_to_string_Lookup (defined at usr/lib64/python2.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]**\r\n\r\nNode index_to_string_Lookup is a output node of my model.\r\n\r\nI googled this problem and get the an suggest to add sess.run(tf.tables_initializer()), so i change the calibrate func by add calibration_sess.run(tf.tables_initializer()):\r\n```\r\nwith session.Session(\r\n  graph=self._calibration_graph,\r\n  config=self._session_config) as calibration_sess:\r\n  calibration_sess.run(tf.tables_initializer())\r\n   for _ in range(num_runs):\r\n     calibration_sess.run(\r\n        fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)\r\n```\r\nbut i failed, how can i slove this problem?\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37660\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37660\">No</a>\n"]}, {"number": 37659, "title": "Stateful Keras RNN Conversion", "body": "**System information**\r\n- Mac OSX\r\n- 2.2.0.dev20200316\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nhttps://colab.research.google.com/drive/1H7EWTn9lQZ18R6DDBWJtJUxxX3XIVd2K\r\n\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nValueError: Input 0 of node sequential_3/gru_3/AssignVariableOp was passed float from sequential_3/gru_3/Read/ReadVariableOp/resource:0 incompatible with expected resource.\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nI am trying to make predictions using an RNN model that maintains state across forward pass of batches of unit size and consecutive time samples. \r\n", "comments": ["we are able to replicate [this](https://colab.sandbox.google.com/gist/Saduf2019/b7497474ab06b670b57fb365e6461b13/untitled93.ipynb) issue", "Any updates on this bug or clues on how to fix it, even if temporarily?", "Could you try using TFLiteConverter.from_saved_model via converting keras model to saved model?", "@abattery I tried that by calling\r\n```\r\nmodel.save('saved_model')\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model')\r\n```\r\n\r\nI get the following errors:\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-10-cb973e0e0655> in <module>()\r\n      6 converter.target_spec.supported_types = [tf.float16]\r\n      7 \r\n----> 8 tflite_fp16_model = converter.convert()\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    239       stdout = _try_convert_to_unicode(stdout)\r\n    240       stderr = _try_convert_to_unicode(stderr)\r\n--> 241       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    242   finally:\r\n    243     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-04-15 07:05:33.401098: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:307] Ignored output_format.\r\n2020-04-15 07:05:33.401146: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:310] Ignored drop_control_dependency.\r\n2020-04-15 07:05:33.428922: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\nloc(\"gru_2/Variable\"): error: is not immutable\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(\"gru_2/Variable\"): is not immutable\r\n<unknown>:0: note: loc(\"gru_2/Variable\"): see current operation: \"tf_saved_model.global_tensor\"() {is_mutable, sym_name = \"gru_2/Variable\", type = tensor<1x20xf32>, value = dense<0.000000e+00> : tensor<1x20xf32>} : () -> ()\r\n```", "As a temporary workaround, I use `tf.compat.v1.lite.TFLiteConverter.from_sess()` after disabling eager execution and using the streaming layers in https://github.com/google-research/google-research/tree/master/kws_streaming. ", "Any update for fix?  Looks like it is still not working in v2.3", "I have the same issue with an LSTM saved model (Keras model -> saved model).\r\n\r\nCan someone shed some light on why this is not implemented in the first place? In my understanding _stateless_ RNNs need the whole input sequence at once to perform meaningful inference. Yet, in embedded environments (where tflite converted models come into play), one usually\r\n\r\n1. does not have enough memory to record/store the entire input sequence and\r\n2. wants to have step-by-step inference results for each input vector instead of the complete sequence.\r\n\r\nSo, it seems to make no sense to use _stateless_ RNNs in an embedded environment&mdash;yet it is the only available option. What is the error in my assumption?\r\n\r\n@Abhipray Do you have a minimal example for your https://github.com/tensorflow/tensorflow/issues/37659#issuecomment-614194122? I would really appreciate it if it's not too much effort.", "I tried to run the code on Colab  with TF 2.5 and faced ConverterError  ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/3a6f737f45f4bb71ba4165365dabaf88/untitled93.ipynb#scrollTo=2g6eBG_Pb2gB) ..Thanks!", "Mutable variable support is available in the nightly when converting from SavedModel only (Other types are coming soon).\r\nBuiltin ops are available now for VarHandleOp, ReadVariable, AssignVariable in the nightly also\r\n\r\nOnly thing needed is to set this flag during conversion:\r\nconverter.experimental_enable_resource_variables = True\r\n\r\nPlease let us know if you're having any issues.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37659\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37659\">No</a>\n"]}, {"number": 37658, "title": "get is_training from bn_op so moving_mean and moving_var tensor is se\u2026", "body": "\u2026t correctly\r\n\r\nIf not set so, moving_mean_tensor and moving_variance_tensor is None Type", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37658) for more info**.\n\n<!-- need_sender_cla -->", "@charlieguo0307 Thank you for your contribution. Please sign CLA.", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37658) for more info**.\n\n<!-- ok -->", "Is this fixing an issue on 1.15 or the issue is on master?\r\n\r\nPlease first test if the issue reproduces on master. If it does, please reopen against master and then we can do a cherry-pick here.\r\n\r\nIf it only manifests in 1.15, then we can keep this PR open until we need to create a patch release on 1.15.", "@mihaimaruseac Thanks for your reply!\r\nThe issue exists in tf.contrib.quantize, which is not a part in Tensorflow 2.x now.\r\nSo we can keep this until a patch release on 1.15?", "Looks good to me. I will review and merge once we are ready to do a patch release on 1.15"]}, {"number": 37657, "title": "Add gitpod config", "body": "this commit adds support for Gitpod.io, a free automated\ndev environment that makes contributing and generally working on GitHub\nprojects much easier. It allows anyone to start a ready-to-code dev\nenvironment for any branch, issue and pull request with a single click.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37657) for more info**.\n\n<!-- need_sender_cla -->", "@namckim1 Thank you for your contribution. Please sign CLA.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 37656, "title": "Get different results in eager and graph mode when I use tf.keras.Reduction.NONE on Loss object.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):2.2.0\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\n\r\n[Colab example](https://colab.research.google.com/drive/1VWVty280phZLVsmB91aW6GqdQoEgDCdP)\r\n\r\n\r\nRelated [addons](https://github.com/tensorflow/addons/issues/1320)", "comments": ["I have tried on colab with TF version 2.2.0-rc0 , 2.2.0-dev20200316 and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/b40935f0f80653729e4ef90294b47f1c/untitled732.ipynb) Thanks!", "@fsx950223 The tensorflow doc on TF website describes the reported loss value will be a scalar, but unreduced vector will be passed to the optimizer.\r\n\r\n> NONE: Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit/evaluate, the unreduced vector loss is passed to the optimizer but ***the reported loss will be a scalar value***.\r\n\r\nBased on this description on [TF website](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction), you are getting a scalar value (`30.75`) which is correct right? Thanks!\r\n", "> @fsx950223 The tensorflow doc on TF website describes the reported loss value will be a scalar, but unreduced vector will be passed to the optimizer.\r\n> \r\n> > NONE: Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit/evaluate, the unreduced vector loss is passed to the optimizer but _**the reported loss will be a scalar value**_.\r\n> \r\n> Based on this description on [TF website](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction), you are getting a scalar value (`30.75`) which is correct right? Thanks!\r\n\r\nYes, it's correct. But graph and eager mode should return same result when I use ```evaluate```.", "I have tried in TF nightly version(`2.5.0-dev20201029`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/bf520e40b8fedb7e93840d2fc1e5501f/untitled96.ipynb?authuser=1).Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/055ff3a8ee0b28db2352a0ddc5d96721/untitled7.ipynb). Thanks!", "Was able to reproduce issue in Tf 2.7, please find the gist [here](https://colab.research.google.com/gist/kumariko/99780d7b0e651df8f45be967ddb68b82/untitled7.ipynb#scrollTo=R-FZoQlKLuxr). Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37656\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37656\">No</a>\n"]}, {"number": 37655, "title": "concat to tf.concat for consistency", "body": "Change concat to tf.concat for consistency in this doc and easier to copy and experiment with the code.", "comments": []}, {"number": 37654, "title": "Subclass of `tf.keras.Model` returns an empty list by `model.losses` ", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  yes, as follows\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass MyModel(keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(MyModel, self).__init__(**kwargs)\r\n    def call(self, inputs):\r\n        self.add_loss(tf.reduce_mean(tf.square(inputs)))\r\n        return inputs\r\n\r\nx = tf.placeholder(tf.float32, [None, 5])\r\nmodel = MyModel()\r\nmodel(x)\r\n\r\nmodel.losses  # returns []\r\nmodel._losses # returns [<tf.Tensor 'my_model/Mean:0' shape=() dtype=float32>]\r\n```\r\n\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): macOS Catalina, 10.15.3\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 1.12.0\r\n- Python version: Python 2.7.17 |Anaconda, Inc.| (default, Oct 21 2019, 14:10:59)\r\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\r\n\r\n**Describe the current behavior**\r\n\r\nThe current returned value of model.losses does not contain elements in model._losses\r\n\r\n**Describe the expected behavior**\r\n\r\nAs we can see in the code of `Network.losses`, it should return \r\n\r\n```\r\nlist(set(relevant_conditional_losses + unconditional_losses + self._losses))\r\n```\r\n\r\nIf it does not contain tensors in model._losses, `model.add_loss` will not take any effects.", "comments": ["Hi. I am new here but want to contribute. \r\nSo, @siyuany if this is what you are talking about : \r\n[https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/engine/network.py](url) line no : 400\r\n(link not working... please copy paste in new tab)\r\nthen we see that in line 439, we are returning `unique_tensors + non_tensors`\r\nSo, will you please be little specific. Please dont mind if i am wrong coz i am totally new and just want to get a hang and contribute. Thanks.", "Hi @siyuany. Ok, are you saying we are not adding `self._losses` to the returned list?", "Hi @siyuany. I see that `self._losses` is initialized as empty list. Will you please tell where is `self._losses` getting updated ?\r\nI mean in which file?", "> Hi. I am new here but want to contribute.\r\n> So, @siyuany if this is what you are talking about :\r\n> [https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/engine/network.py](url) line no : 400\r\n> (link not working... please copy paste in new tab)\r\n> then we see that in line 439, we are returning `unique_tensors + non_tensors`\r\n> So, will you please be little specific. Please dont mind if i am wrong coz i am totally new and just want to get a hang and contribute. Thanks.\r\n\r\nHi, @HopefulRational. I think you're talking about the independent keras. But I refer to the keras in tensorflow, `tf.keras`. So the source codes are not the same. BTW, it works fine if I subclass `keras.Model` in the independent keras.", "> Hi @siyuany. I see that `self._losses` is initialized as empty list. Will you please tell where is `self._losses` getting updated ?\r\n> I mean in which file?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5b900cfe4b3b848f577315a0dde09a729f770e95/tensorflow/python/keras/engine/network.py#L679\r\n\r\nI think maybe here the property should return `list(set(losses + self._losses))`, or else losses added by `model.add_loss` will not be contained in the final list. \r\n\r\nYou can check the following scripts\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.python.keras.utils import tf_utils\r\n\r\n\r\nclass MyModel(keras.Model):\r\n\r\n  def __init__(self, **kwargs):\r\n    super(MyModel, self).__init__(**kwargs)\r\n\r\n  def call(self, inputs):\r\n    self.add_loss(tf.reduce_mean(tf.square(inputs)))\r\n    return inputs\r\n\r\n\r\nx = tf.placeholder(tf.float32, [None, 5])\r\nmodel = MyModel()\r\nmodel(x)\r\n\r\nprint('model.losses: ', model.losses)  # returns []\r\nprint('model._losses: ', model._losses)\r\n# returns [<tf.Tensor 'my_model/Mean:0' shape=() dtype=float32>]\r\n\r\n\r\ndef losses(self):\r\n  # this function is copied from `Network.losses`, but revised the line commented.\r\n  losses = self._unfiltered_losses\r\n  if context.executing_eagerly():\r\n    return losses\r\n  relevant_inputs = []\r\n  for i in range(0, len(self._inbound_nodes)):\r\n    inputs = self.get_input_at(i)\r\n    if isinstance(inputs, list):\r\n      relevant_inputs += inputs\r\n    else:\r\n      relevant_inputs.append(inputs)\r\n  ########## revised here!  ##########\r\n  if not relevant_inputs:  \r\n    # return losses\r\n    return list(set(losses + self._losses))\r\n  ###############################\r\n  reachable = tf_utils.get_reachable_from_inputs(relevant_inputs, losses)\r\n  relevant_conditional_losses = [x for x in losses if x in reachable]\r\n  unconditional_losses = [x for x in losses if x._unconditional_loss]  # pylint: disable=protec\r\n  return list(\r\n    set(relevant_conditional_losses + unconditional_losses + self._losses))\r\n\r\n\r\nprint(losses(model))\r\n```", "Hi @siyuany. Thanks for such a quick reply.\r\nI am sorry. I thought that the two keras are equivalent in implementation. I want to know where are we filling `model._losses`.\r\nI think that in the `add_loss` : \r\n[https://github.com/tensorflow/tensorflow/blob/97103c3b9b447a9889be93fa36324b3ed3daf377/tensorflow/python/keras/engine/base_layer.py#L1172](url)\r\n(again link is not working please copy paste link in new tab)\r\nwe are putting symbolic losses in `model._losses` and thats the only thing we are putting in `model._losses`. Am I right?\r\n\r\nAnd would you please also tell me how to link a line like the way you linked line 679. Thanks.", "Was able to reproduce the issue. Please find the Gist [here](https://colab.research.google.com/gist/amahendrakar/77fc2e8c9039a0f310f847a0e9a5fef0/37654.ipynb). Thanks!", "It seems in tensorflow==1.15.0 the issue has been fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37654\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37654\">No</a>\n", "> Hi @siyuany. Thanks for such a quick reply.\r\n> I am sorry. I thought that the two keras are equivalent in implementation. I want to know where are we filling `model._losses`.\r\n> I think that in the `add_loss` :\r\n> [https://github.com/tensorflow/tensorflow/blob/97103c3b9b447a9889be93fa36324b3ed3daf377/tensorflow/python/keras/engine/base_layer.py#L1172](url)\r\n> (again link is not working please copy paste link in new tab)\r\n> we are putting symbolic losses in `model._losses` and thats the only thing we are putting in `model._losses`. Am I right?\r\n> \r\n> And would you please also tell me how to link a line like the way you linked line 679. Thanks.\r\n\r\nHi @HopefulRational\r\n\r\nYes, it's the same as I understand.\r\n\r\nFor the second question, You can just click the line no., click the three dots in the left, and select 'copy permalink', and just paste the link in an isolated line, no extra characters at all. You can try.\r\n\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/97103c3b9b447a9889be93fa36324b3ed3daf377/tensorflow/python/keras/engine/base_layer.py#L1172", "Hi @siyuany. Thanks for quick reply.\r\nSo, I see that `Network.losses` is not there.\r\nIs it that in `base_layer.add_losses`, we are filling `_losses` and by `base_layer.losses` we are returning losses to solve the issue?\r\n\r\nThanks again. You have been very kind :-) ", "@siyuany hi... could you please reply to my latest comment.. thanks :) ", "> Thanks again. You have been very kind :-)\r\n\r\nThe bug had been fixed in higher version (tf>1.12), so I just closed this issue."]}, {"number": 37653, "title": "Memory Leak in tf.data.Dataset.from_generator", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: Python 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: \r\nCUDA Version: 10.1\r\ncudnn-10.1\r\n- GPU model and memory:\r\nTITAN RTX\r\n24190MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`tf.data.Dataset.from_generator` leaks memory after each call even if followed by `gc.collect()`.\r\n\r\n**Describe the expected behavior**\r\nMemory should be released when no reference exists for the dataset.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport gc\r\nimport os\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\nimport tensorflow as tf\r\nimport tracemalloc\r\nimport linecache\r\n\r\n\r\ndef display_top(snapshot, key_type='lineno', limit=3):\r\n    snapshot = snapshot.filter_traces((\r\n        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\r\n        tracemalloc.Filter(False, \"<unknown>\"),\r\n    ))\r\n    top_stats = snapshot.statistics(key_type)\r\n\r\n    print(\"Top %s lines\" % limit)\r\n    for index, stat in enumerate(top_stats[:limit], 1):\r\n        frame = stat.traceback[0]\r\n        # replace \"/path/to/module/file.py\" with \"module/file.py\"\r\n        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\r\n        print(\"#%s: %s:%s: %.1f KiB\"\r\n              % (index, filename, frame.lineno, stat.size / 1024))\r\n        line = linecache.getline(frame.filename, frame.lineno).strip()\r\n        if line:\r\n            print('    %s' % line)\r\n\r\n    other = top_stats[limit:]\r\n    if other:\r\n        size = sum(stat.size for stat in other)\r\n        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\r\n    total = sum(stat.size for stat in top_stats)\r\n    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\r\n\r\n\r\ndef generator():\r\n    yield tf.zeros(2, 3)\r\n\r\n\r\ntracemalloc.start()\r\nfor i in range(1000):\r\n    dataset = tf.data.Dataset.from_generator(generator, output_types=tf.int32, output_shapes=[None])\r\n    del dataset\r\n    gc.collect()\r\n    snapshot = tracemalloc.take_snapshot()\r\n    display_top(snapshot)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTop 3 lines\r\n#1: python3.6/_weakrefset.py:84: 159.5 KiB\r\n    self.data.add(ref(item, self._remove))\r\n#2: python3.6/_weakrefset.py:37: 38.2 KiB\r\n    self.data = set()\r\n#3: python3.6/_weakrefset.py:48: 32.4 KiB\r\n    self._iterating = set()\r\n461 other: 306.4 KiB\r\nTotal allocated size: 536.4 KiB\r\nTop 3 lines\r\n#1: python3.6/_weakrefset.py:84: 159.5 KiB\r\n    self.data.add(ref(item, self._remove))\r\n#2: python3.6/_weakrefset.py:37: 38.2 KiB\r\n    self.data = set()\r\n#3: python3.6/_weakrefset.py:48: 32.4 KiB\r\n    self._iterating = set()\r\n516 other: 343.1 KiB\r\nTotal allocated size: 573.1 KiB\r\n\r\n...\r\n\r\nTop 3 lines\r\n#1: python3.6/weakref.py:335: 257.8 KiB\r\n    self = ref.__new__(type, ob, callback)\r\n#2: debug/tf_dataset_memory_leak.py:45: 189.7 KiB\r\n    dataset = tf.data.Dataset.from_generator(generator, output_types=tf.int32, output_shapes=[None])\r\n#3: ops/script_ops.py:257: 174.7 KiB\r\n    return \"pyfunc_%d\" % uid\r\n519 other: 2423.3 KiB\r\nTotal allocated size: 3045.5 KiB\r\n```\r\n\r\nIt leaks 3MB in 1000 calls. In [some real projects](https://github.com/hankcs/HanLP/issues/1437), it can leak as much as 5GB and keeps increasing.", "comments": ["Was able to replicate the issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/b7c34b3750157bda6246930c090e59ff/untitled463.ipynb). Thanks!", "@kkimdev could you please take a look? Thank you. ", "memory debug colab link : https://colab.corp.google.com/drive/1TYW_QWcJ6j6FfuepdTu6nfj1TP2Cw5PU#scrollTo=ViLA0wnRmblE\r\n\r\nleak per 100 iteration:\r\n```\r\n============================================================================\r\nType                          Old_ids  Current_ids      New_ids Count_Deltas\r\n============================================================================\r\ncell                            74412        75112         +700         +700\r\ndict                           100309       100909         +604         +600\r\ntuple                           84895        85395         +500         +500\r\nfunction                       113257       113757         +500         +500\r\nlist                            50680        50980         +302         +300\r\nKeyedRef                        26530        26830         +300         +300\r\nmethod                           9305         9405         +100         +100\r\n_GeneratorState                  8845         8945         +100         +100\r\nTensorShape                      8849         8949         +100         +100\r\nDimension                        8844         8944         +100         +100\r\n```\r\n\r\nReference graph from _GeneratorState https://graphviz.corp.google.com/svg?graph_id=2afe255c1644cc79fe98e01ab09c6be8\r\n\r\nSeems like the leaking edge is `_py_funcs_used_in_graph`", "Probably one of the calls `script_ops.numpy_function(...)` in [from_generator(...)](http://google3/third_party/tensorflow/python/data/ops/dataset_ops.py?l=678&rcl=301443563) \r\n\r\n`_py_funcs_used_in_graph` is just to maintain the lifetime of the function, so I think the correct fix is attaching to something else, not the graph.  @mdanatg @akshaym ", "The function needs to be attached to the graph, otherwise the py_func Op would have nothing to call. There should be no reference from the function to the graph though - it should have no knowledge of it. That said, it's easy for it to close over something that points to it.", "btw, FYI: in this case it was the global graph.", "any progress in this issues? It seems like still occurs in tensorflow-gpu==2.2.0. \r\nPlease provide some information to prevent this problem..... My process has been killed by this OOM problems", "4 months passed without any progress. I can do nothing but rewrite my entire project into PyTorch. I really love Keras and hope one day the user experience of TensorFlow will line up with Keras.", "> 4 months passed without any progress. I can do nothing but rewrite my entire project into PyTorch. I really love Keras and hope one day the user experience of TensorFlow will line up with Keras.\n\nI have a pretrained model for my product So I can't change to PyTorch easily unless I have a confidence that converting the weights giving the same performance due to the Precision between Float and Double which I think double data type is being use in keras.\n\nWhat if we use only normal Python Generator? Shouldn't that be solve....? Anyone has try that instead of using tf.data.Dataset.from_generator?", "@luvwinnie @hankcs I deeply apologize for the issues.  Would you mind trying this workaround?\r\n\r\n```\r\n# Before calling `tf.data.Dataset.from_generator`.\r\ntf.compat.v1.get_default_graph()._py_funcs_used_in_graph = []\r\n```", "@kkimdev I have tried by adding the _py_funcs_used_in_graph, however I got this error instead. By the way I used `tf.distribute.MirroredStrategy()` for multi-gpu, I tried `strategy.make_dataset_iterator` or `strategy.experimental_distribute_dataset` gives the same results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 1228, in <module>\r\n    main(args)\r\n  File \"train.py\", line 1001, in main\r\n    ditributred_train(train_iterator)\r\n  File \"/misc/home/usr16/cheesiang_leow/.virtualenvs/tensorflow-gpu-2.3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/misc/home/usr16/cheesiang_leow/.virtualenvs/tensorflow-gpu-2.3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 644, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/misc/home/usr16/cheesiang_leow/.virtualenvs/tensorflow-gpu-2.3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/misc/home/usr16/cheesiang_leow/.virtualenvs/tensorflow-gpu-2.3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/misc/home/usr16/cheesiang_leow/.virtualenvs/tensorflow-gpu-2.3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/misc/home/usr16/cheesiang_leow/.virtualenvs/tensorflow-gpu-2.3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"/misc/home/usr16/cheesiang_leow/.virtualenvs/tensorflow-gpu-2.3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  ValueError: callback pyfunc_0 is not found\r\nTraceback (most recent call last):\r\n\r\n  File \"/misc/home/usr16/cheesiang_leow/.virtualenvs/tensorflow-gpu-2.3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 232, in __call__\r\n    raise ValueError(\"callback %s is not found\" % token)\r\n\r\nValueError: callback pyfunc_0 is not found\r\n\r\n\r\n\t [[{{node PyFunc}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional_1]]\r\n  (1) Cancelled:  Function was cancelled before it was started\r\n0 successful operations.\r\n1 derived errors ignored. [Op:__inference_ditributred_train_28590]\r\n\r\nFunction call stack:\r\nditributred_train -> ditributred_train\r\n```", "Hi @kkimdev , thanks for your workaround. I tried it on 2.1 and 2.2, neither works.\r\n\r\n@luvwinnie The parallelization and cache can't be easily implemented using NumPy. Besides, I have a whole pile of data pipelines built on top of tf.data API. To abandon all these was a hard decision for me. But I guess that's life... I'm happy with PyTorch so far, its data API is not as good as TF but it is stable at least.\r\n\r\nOne user of my project said he ended up with using a docker and he has to restart it once tf exceeds the memory quota. I don't think this is the right thing to do for a decent project.", "@hankcs yes i agree with you, I have an exactly same situation with you which my pile of data pipelines built on top of tf.data API too. one of my user he said that he ended up using a normal python generator instead, but doing so, he can't use multiple GPU which the tensorflow need  a tf.data API.\r\n\r\n@kkimdev I think this is a urgent issues for every heavy user of tf.data. How come this didn't solve in 4months? I think this is a problem of eager execution currently the multi-gpu strategy still depends on old Session thing.", "Another workaround try:\r\n\r\n```python\r\n# Cleanup utility class\r\nclass TfDataset(object):\r\n    def __init__(self):\r\n        self.py_func_set_to_cleanup = set()\r\n\r\n    def from_generator(self, generator, output_types, output_shapes=None, args=None):\r\n        if not hasattr(tf.compat.v1.get_default_graph(), '_py_funcs_used_in_graph'):\r\n            tf.compat.v1.get_default_graph()._py_funcs_used_in_graph = []\r\n        py_func_set_before = set(tf.compat.v1.get_default_graph()._py_funcs_used_in_graph)\r\n        result = tf.data.Dataset.from_generator(generator, output_types, output_shapes, args)\r\n        py_func_set_after = set(tf.compat.v1.get_default_graph()._py_funcs_used_in_graph) - py_func_set_before\r\n        self.py_func_set_to_cleanup |= py_func_set_after\r\n        return result\r\n  \r\n    def cleanup(self):\r\n        new_py_funcs = set(tf.compat.v1.get_default_graph()._py_funcs_used_in_graph) - self.py_func_set_to_cleanup\r\n        tf.compat.v1.get_default_graph()._py_funcs_used_in_graph = list(new_py_funcs)\r\n        self.py_func_set_to_cleanup = set()\r\n\r\n# Usage example\r\ntf_dataset = TfDataset()\r\ndataset = tf_dataset.from_generator(generator, output_types=tf.int32, output_shapes=[None])\r\ndel dataset\r\ntf_dataset.cleanup()  # Call this after done using the generator.\r\n```\r\n\r\nSample Colab run:  https://colab.research.google.com/gist/kkimdev/0047ce8444a14197c60c19bba0349156/copy-of-untitled463.ipynb\r\n", "@kkimdev Thank you for your workaround! just another question about this, you did a `del dataset` and `tf_dataset.cleanup()`,  I have a loop like below, what do you suggest the timing for cleanup the dataset? \r\n\r\n```python\r\nstrategy = tf.distribute.MirroredStrategy()\r\ntrain_iterator = strategy.make_dataset_iterator(train_dataset)\r\ndef train_step(inputs):\r\n    images, labels = inputs\r\n    with tf.GradientTape() as tape:\r\n        y_pred = recognizer(images, training=True)\r\n        loss = compute_loss(\r\n                   y_pred, labels\r\n        )\r\n    losses.update_state(loss)\r\n    gradients = tape.gradient(loss, recognizer.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, recognizer.trainable_variables))\r\n@tf.function\r\ndef distributred_train(dataset):\r\n        return strategy.experimental_run(train_step, dataset)\r\n\r\nfor epoch in range(epochs):\r\n    for steps in range(train_steps_per_epoch):\r\n        ditributred_train(train_iterator)\r\n        # it use the train_iterator to run the distributed training step.\r\n        # How should I suppose to reset the dataset?\r\n```", "@luvwinnie Actually, your snippet could be a different issue.  This issue is for memory leaks when `tf.data.Dataset.from_generator` is called repeatedly.  Could you file another issue? (ideally with a reproducible Colab example like this bug)", "@kkimdev I trying to find out the problem, but it seems like the problem of multi-GPUs settings, I would like to open an issue with a when I'm able to make a reproducible code since the Colab environment can't use 2gpus", "The issue is unrelated to GPU, and is perfectly reproducible with CPU training. I am using flat numpy arrays as training input, so it is not a dataset/generator issue either. Memory leak rate seems to depend on training set size though, I am collecting some stats, will update post in a couple of hours once I get a nice plot of RAM use vs time.", "Found a workaround, del together with gc.collect() working fine for me.:\r\n\r\n       `images_list = np.vstack(images_to_load)\r\n        pred_result = self.model.predict(images_list, batch_size=10)\r\n\r\n        del(pred_result)\r\n        gc.collect()`", "@kkimdev  I am going to check your workaround right now since I am experiencing memory leaks using the `from_generator` method. \r\n\r\nIf I understand from your snipped provided in Colab, the dataset is cleared at every iteration. Would you do this periodically but not necessarily every step? I'm asking this because it seems that if the dataset is cleared at every step there is no gain in terms of caching (say the dataset is cleared every `x` epochs)", "@jjrugui Yes that sounds reasonable, but let's first confirm that the memory leak could be resolved by using the wrapper.", "> @jjrugui Yes that sounds reasonable, but let's first confirm that the memory leak could be resolved by using the wrapper.\r\n\r\n@kkimdev actually, from_tensor_slices() also faces memory leak problem", "I am also seeing this error occur with using tf agents's `tf_uniform_replay_buffer` and calling the `as_dataset` method. After each call (regardless of replay size or variables I am getting): \r\n\r\nafter episode 2\r\n```\r\ndict                               49318      +128\r\nlist                               20866       +60\r\n_Listener                           4494       +42\r\nweakproxy                           4494       +42\r\nRepeatedCompositeFieldContainer      615       +12\r\nAttrValue                           1502       +10\r\nMessageMap                           647        +8\r\nset                                 7087        +7\r\nTensorShape                          679        +7\r\nArgDef                               276        +7\r\n```\r\n\r\nafter episode 3: \r\n\r\n```\r\ndict                               49446      +128\r\nlist                               20926       +60\r\n_Listener                           4536       +42\r\nweakproxy                           4536       +42\r\nRepeatedCompositeFieldContainer      627       +12\r\nAttrValue                           1512       +10\r\nMessageMap                           655        +8\r\nTensorShape                          686        +7\r\nArgDef                               283        +7\r\ntuple                              49100        +6\r\n```\r\n\r\nI have removed all my training code. The only method being called in this loop is: \r\n\r\n```\r\ndataset = replay_buffer.as_dataset(\r\n        ...\r\n      single_deterministic_pass=False\r\n    )\r\n```\r\n\r\nHowever, if I change it to: \r\n\r\n```\r\ndataset = replay_buffer.as_dataset(\r\n        ...\r\n      single_deterministic_pass=True\r\n    )\r\n```\r\n\r\nthe memory leak goes away 100% ... Not sure what the tf agent code is doing with that variable, but it might help someone track down the problem or help someone else using tf agents. ", "@hankcs ,\r\nCan you please look at this [workaround](https://github.com/tensorflow/tensorflow/issues/37653#issuecomment-657176452) in latest tf v2.7 and let us know if the issue still persists.Thanks! ", "> @hankcs , Can you please look at this [workaround](https://github.com/tensorflow/tensorflow/issues/37653#issuecomment-657176452) in latest tf v2.7 and let us know if the issue still persists.Thanks!\r\n\r\nThank you for the workaround and I can confirm it works with the latest v2.7: https://colab.research.google.com/drive/1xNMnqzM0Zrfr3I8XwcS0E9kdhhslxuZd?usp=sharing", "@hankcs ,\r\nPlease feel free to move this issue to closed status  as it answers your question.Thanks!", "Thank you so much!"]}, {"number": 37652, "title": "Predict API does not work with VarLenFeature when batch inputs from different size", "body": "\r\n_I thought this [issue](https://github.com/tensorflow/serving/issues/1574)  was related to tensorflow / serving repo , but they told me it belongs to this repo, and hoped that put it here to discuss, so I created this issue_\r\n\r\n-------------\r\n\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04):  MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 2.7.13\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling fromsource): \r\n- CUDA/cuDNN version: \r\n- GPU model and memory: \r\n\r\n**Describe the current behavior**\r\n\r\nPredict API is not compatible with batch variable length inputs. Everything is ok when the input is just one instance, or multiple instances with the same length of a var length feature.\r\n\r\nps: Classify API works pretty good in this case.\r\n\r\nI tried to generate the exact same model through Estimator, and tested the above situation through the Classify API, everything was perfect. But since TensorFlow 2.0, Predict API is the default and only signature of keras model. I really want to solve this problem, instead of switching back to using Estimator or custom signature. This problem has troubled me for a while, thank you very much for your help :)\r\n\r\n**Describe the expected behavior**\r\n\r\nPredict API should be as compatible with variable length input features as Classify API\r\n\r\n### Source code / logs\r\n\r\nHere is a simple code to reproduce this, which will output a model and serving from it.\r\n\r\n[code]\r\n```python\r\nimport tensorflow as tf\r\n\r\n# generate dummy dataset \r\ndef serialize_example(val, label):\r\n    features = {\r\n      'color': tf.train.Feature(bytes_list=tf.train.BytesList(value=val)),\r\n      'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\r\n    }\r\n    example_proto = tf.train.Example(features=tf.train.Features(feature=features))\r\n    return example_proto.SerializeToString()\r\n\r\ntfrecord_writer = tf.io.TFRecordWriter('./color.tfrecord')\r\nfor val, label in [([b'G', b'R'], 1), ([b'B'], 1), ([b'B', b'G'], 0), ([b'R'], 1)]:\r\n    tfrecord_writer.write(serialize_example(val, label))\r\ntfrecord_writer.close()\r\n\r\n# load the data generate above\r\ndef parse(example_proto):\r\n    feature_description = {\r\n        'color': tf.io.VarLenFeature(tf.string) ,           # ** VarLenFeature **\r\n        'label': tf.io.FixedLenFeature([], tf.int64)        \r\n    }\r\n    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\r\n    labels = parsed_features.pop('label')\r\n    return parsed_features, labels\r\n    \r\ndataset = tf.data.TFRecordDataset('./color.tfrecord').map(parse).repeat(5).batch(2)\r\n\r\n# feature column & inputs.\r\ncolor_cat = tf.feature_column.categorical_column_with_vocabulary_list(\r\n                    key='color', vocabulary_list=[\"R\", \"G\", \"B\"])    \r\n\r\ncolor_emb = tf.feature_column.embedding_column(color_cat, dimension=4, combiner='mean')\r\n\r\ninputs = {\r\n    'color': tf.keras.layers.Input(name='color', shape=(None, ), sparse=True, dtype=tf.string)    \r\n}\r\n\r\n# build model\r\ndeep = tf.keras.layers.DenseFeatures([color_emb, ])(inputs)\r\noutput = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(deep)\r\nmodel = tf.keras.Model(inputs, output)\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\r\n\r\nmodel.summary()\r\n\r\nmodel.fit(dataset, epochs=5)\r\nmodel.save('./dummy_model', save_format='tf')\r\n```\r\n\r\n[output log]\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ncolor (InputLayer)           [(None, None)]            0         \r\n_________________________________________________________________\r\ndense_features (DenseFeature (None, 4)                 12        \r\n_________________________________________________________________\r\noutput (Dense)               (None, 1)                 5         \r\n=================================================================\r\nTotal params: 17\r\nTrainable params: 17\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nEpoch 1/5\r\n10/10 [==============================] - 1s 119ms/step - loss: 0.7157\r\nEpoch 2/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7045\r\nEpoch 3/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.6977\r\nEpoch 4/5\r\n10/10 [==============================] - 0s 5ms/step - loss: 0.6911\r\nEpoch 5/5\r\n10/10 [==============================] - 0s 5ms/step - loss: 0.6847\r\nWARNING:tensorflow:From /Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nINFO:tensorflow:Assets written to: ./dummy_model/assets\r\n```\r\n\r\n[saved_model signature]\r\n```\r\n$ saved_model_cli show --dir ./ --all\r\n--------------------------------------------------------------------------\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is:\r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['color'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, -1)\r\n        name: serving_default_color:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['output'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: StatefulPartitionedCall_8:0\r\n  Method name is: tensorflow/serving/predict\r\n```\r\n### Exact Steps to Reproduce\r\n1. let me say there are two examples with the same shape , these work pretty good\r\n```\r\ncurl -XPOST 0.0.0.0:8501/v1/models/dummy_model:predict -d '\r\n{\r\n  \"inputs\": {\r\n    \"color\": [\r\n      [\"R\"],\r\n      [\"G\"]\r\n    ]\r\n  }\r\n}'\r\n----- response ----\r\n{\r\n    \"outputs\": [\r\n        [\r\n            0.370285\r\n        ],\r\n        [\r\n            0.49326694\r\n        ]\r\n    ]\r\n}\r\n\r\n====================================================\r\ncurl -XPOST 0.0.0.0:8501/v1/models/dummy_model:predict -d '\r\n{\r\n  \"inputs\": {\r\n    \"color\": [\r\n      [\"R\", \"G\"],\r\n      [\"G\", \"B\"]\r\n    ]\r\n  }\r\n}'\r\n------- response ---------\r\n{\r\n    \"outputs\": [\r\n        [\r\n            0.430707693\r\n        ],\r\n        [\r\n            0.558493555\r\n        ]\r\n    ]\r\n}\r\n```\r\n2. batch examples with different shape , does not work as expected\r\n```\r\ncurl -XPOST 0.0.0.0:8501/v1/models/dummy_model:predict -d '\r\n{\r\n  \"inputs\": {\r\n    \"color\": [\r\n      [\"R\"],\r\n      [\"G\", \"B\"]\r\n    ]\r\n  }\r\n}'\r\n-------- response -------\r\n\r\n{ \"error\": \"Encountered list at unexpected size: 2 at level: 1 expected size: 1\" }\r\n\r\n```\r\n3.  \"instances\" key also not works\r\n\r\n```\r\ncurl -XPOST 0.0.0.0:8501/v1/models/dummy_model:predict -d '\r\n{\r\n  \"instances\": [\r\n    {\"color\": [\"R\"]},\r\n    {\"color\": [\"G\", \"B\"]}\r\n  ]\r\n}'\r\n\r\n--------- response -----------\r\n{ \"error\": \"Failed to process element: 1 key: color of \\'instances\\' list. Error: Invalid argument: Expecting tensor size: 1 but got: 2\" }\r\n\r\n```", "comments": ["@gowthamkpr ", "Hi, guys. @gowthamkpr, do you have any update on this? I'm also interested in seeing this issue resolved. Facing the same behaviour. Let me know if you need more details to fix it. Thanks in advance!", "Is the problem solved, can you share the result?", "i'm looking forward to the answer.", "Was able to replicate the issue with TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ea60a328461f46184c8580250a3c6234/untitled259.ipynb) ..Thanks!", "@FelixHo  Is the problem solved\uff1fsame issue ,this bug  is still there  in version of TF2.6", "in estimator it can work, but in keras it doesn't.\r\nhttps://colab.research.google.com/gist/sushreebarsa/828257b5977439dea95ee3d48bcab242/untitled105.ipynb#scrollTo=45EerngFwltz", "Was able to replicate the issue with TF v2.7, please find the gist [here](https://colab.research.google.com/gist/kumariko/2aec36644ad00433f93d649557748d61/untitled259.ipynb#scrollTo=PBEgk3qwH9rc) ..Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37652\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37652\">No</a>\n"]}, {"number": 42786, "title": "Add CI for notebook formatting with nbfmt", "body": "Previewing notebook diffs in GitHub can be difficult. We wrote [nbfmt.py](https://github.com/tensorflow/docs/blob/master/tools/nbfmt.py) to normalize the formatting for notebooks\u2014alpha sort JSON keys, set indentation, etc.\r\n\r\nAfter some successful manual tests (https://github.com/tensorflow/docs-l10n/pull/114, https://github.com/tensorflow/docs/pull/1498), the next step is to add it to the CI for new pull requests.\r\n", "comments": ["This is now working for new pull requests (and updates to existing ones)"]}]