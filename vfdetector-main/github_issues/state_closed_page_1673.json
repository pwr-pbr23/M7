[{"number": 2714, "title": "Support for histogram_summary in either rnn() or dynamic_rnn()", "body": "Hello,\nI am trying to add tf.historgram_summary() in my LSTM cells in order to visualize  the output. But currently\nneither rnn() or dynamic_rnn() supports writing out summaries. \nFor rnn(), The log indicates the summary tensor are invalid, which seems to due to the dynamic unrolling of the rnn loop.\n\n```\nTraceback (most recent call last):\n  File \"test_dynamic_rnn.py\", line 55, in <module>\n    summarys = sess.run(thing, feed_dict=feeds)\n  File \"/home/ezheng/tf_head/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 333, in run\n    run_metadata_ptr)\n  File \"/home/ezheng/tf_head/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 573, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/ezheng/tf_head/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 648, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/ezheng/tf_head/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 668, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: The tensor returned for dynamic_scope/RNN/cond_2/BasicLSTMCell/HistogramSummary_1:0 was not valid.\n```\n\nI am currently constructing rnn() like this\n\n``` python\n  outputs, state = tf.nn.rnn(\n      cell, inputs, dtype=tf.float32, sequence_length=sequence_length)\n```\n\nFor dynamic_rnn(), it just hangs when I am trying to obtain summaries with the session.run()\nAnd this is how I am constructing dynamic_rnn()\n\n``` python\n  outputs, state = tf.nn.dynamic_rnn(\n      cell, inputs=concat_inputs,\n      time_major=True, dtype=tf.float32)\n```\n\nWondering if there is plan to support histogram summary with either of these two RNNs. Thanks.\n", "comments": ["@ebrevdo: Do summaries work inside control flow? \n", "I need to see the code where you add the summaries.\n", "in the `__call__` function of LSTMCell\n\n``` python\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n    with vs.variable_scope(scope or self.scope or type(self).__name__):  # \"BasicLSTMCell\"\n      # Parameters of gates are concatenated into one multiply for efficiency.\n      c, h = array_ops.split(1, 2, state)\n      concat = xnor_linear_tf([inputs, h], 4 * self._num_units, True,\n              biasBinarized=False\n              )\n\n      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n      i, j, f, o = array_ops.split(1, 4, concat)\n\n      sig_activation = sigmoid\n      new_c = tanh(c * sig_activation(f + self._forget_bias) + sig_activation(i) * (j))\n      new_h = (new_c) * sig_activation(o)\n      if self._is_training:\n          timestep = get_timestep()\n          tf.histogram_summary(\"i/%03d\" % (timestep), i)\n          tf.histogram_summary(\"j/%03d\" % (timestep), j)\n          tf.histogram_summary(\"f/%03d\" % (timestep), f)\n          tf.histogram_summary(\"o/%03d\" % (timestep), o)\n          tf.histogram_summary(\"c/%03d\" % (timestep), c)\n          tf.histogram_summary(\"h/%03d\" % (timestep), h)\n          tf.histogram_summary(\"x/%03d\" % (timestep), inputs)\n\n      return new_h, array_ops.concat(1, [new_c, new_h])\n\n```\n", "can you paste more of the error trace?  need more context.\n", "Unfortunately there is not much output. I have added a minimal script to reproduce the hanging on dynamic_rnn and the exception on rnn().\n\n``` python\nimport numpy as np\nimport collections\nimport tensorflow as tf\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops.math_ops import sigmoid\nfrom tensorflow.python.ops.math_ops import tanh\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\n\n_LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\nclass LSTMStateTuple(_LSTMStateTuple):\n  \"\"\"Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n  Stores two elements: `(c, h)`, in that order.\n  Only used when `state_is_tuple=True`.\n  \"\"\"\n  __slots__ = ()\n\nclass BasicLSTMCell(tf.nn.rnn_cell.RNNCell):\n  \"\"\"Basic LSTM recurrent network cell.\n  The implementation is based on: http://arxiv.org/abs/1409.2329.\n  We add forget_bias (default: 1) to the biases of the forget gate in order to\n  reduce the scale of forgetting in the beginning of the training.\n  It does not allow cell clipping, a projection layer, and does not\n  use peep-hole connections: it is the basic baseline.\n  For advanced models, please use the full LSTMCell that follows.\n  \"\"\"\n\n  def __init__(self, num_units, forget_bias=1.0, input_size=None, is_training=True,\n               state_is_tuple=False, activation=tanh, add_summary=True):\n    \"\"\"Initialize the basic LSTM cell.\n    Args:\n      num_units: int, The number of units in the LSTM cell.\n      forget_bias: float, The bias added to forget gates (see above).\n      input_size: Deprecated and unused.\n      state_is_tuple: If True, accepted and returned states are 2-tuples of\n        the `c_state` and `m_state`.  By default (False), they are concatenated\n        along the column axis.  This default behavior will soon be deprecated.\n      activation: Activation function of the inner states.\n    \"\"\"\n    if not state_is_tuple:\n      tf.logging.warn(\n          \"%s: Using a concatenated state is slower and will soon be \"\n          \"deprecated.  Use state_is_tuple=True.\" % self)\n    if input_size is not None:\n      tf.logging.warn(\"%s: The input_size parameter is deprecated.\" % self)\n    self._input_size = num_units if input_size is None else input_size\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._state_is_tuple = state_is_tuple\n    self._activation = activation\n    self._is_training = is_training\n    self._add_summary = add_summary\n\n  @property\n  def input_size(self):\n    return self._input_size\n  @property\n  def state_size(self):\n    return (LSTMStateTuple(self._num_units, self._num_units)\n            if self._state_is_tuple else 2 * self._num_units)\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n    with vs.variable_scope(scope or type(self).__name__) as sc:  # \"BasicLSTMCell\"\n      print(\"called with timestep\" + sc.name + \" reuse \" + str(sc.reuse))\n      # Parameters of gates are concatenated into one multiply for efficiency.\n      if self._state_is_tuple:\n        c, h = state\n      else:\n        c, h = array_ops.split(1, 2, state)\n      concat = tf.nn.rnn_cell._linear([inputs, h], 4 * self._num_units, True)\n\n      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n      i, j, f, o = array_ops.split(1, 4, concat)\n\n      new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + sigmoid(i) *\n               self._activation(j))\n      new_h = self._activation(new_c) * sigmoid(o)\n      if self._is_training:\n          #timestep = get_timestep()\n          timestep = 1\n          tf.histogram_summary(\"i/%03d\" % timestep, i)\n          tf.histogram_summary(\"j/%03d\" % timestep, j)\n          tf.histogram_summary(\"f/%03d\" % timestep, f)\n          tf.histogram_summary(\"o/%03d\" % timestep, o)\n          tf.histogram_summary(\"c/%03d\" % timestep, c)\n          tf.histogram_summary(\"h/%03d\" % timestep, h)\n\n      if self._state_is_tuple:\n        new_state = LSTMStateTuple(new_c, new_h)\n      else:\n        new_state = array_ops.concat(1, [new_c, new_h])\n      return new_h, new_state\n\ntime_steps = 8\nnum_units = 3\ninput_size = 5\nbatch_size = 2\n\ninput_values = np.random.randn(time_steps, batch_size, input_size)\n\nsequence_length = tf.placeholder(tf.int32, shape=(batch_size))\nsummary_writer = tf.train.SummaryWriter(\n    'graph_test/')\n\nconcat_inputs = tf.placeholder(\n  tf.float32, shape=(time_steps, batch_size, input_size))\ninputs = [tf.placeholder(tf.float32, shape=(batch_size, input_size)) for _ in range(time_steps)]\ncell = BasicLSTMCell(num_units, forget_bias=False, is_training=True, add_summary=True)\n\nif False:\n    # dynamic_rnn\n    # this hangs\n    outputs, state = tf.nn.dynamic_rnn(\n      cell, inputs=concat_inputs, \n      time_major=True, dtype=tf.float32, sequence_length=sequence_length)\n    feeds = {}\n    feeds[concat_inputs] = input_values\n    feeds[sequence_length] = np.array([2,5])    \nelse:\n    # rnn\n    # InvalidArgumentError\n    outputs, state = tf.nn.rnn(\n        cell, inputs, dtype=tf.float32, sequence_length=sequence_length)\n    feeds = {}\n    for var in inputs:\n        feeds[var] = np.random.randn(batch_size, input_size)\n    feeds[sequence_length] = np.array([2,5])    \n\n\nmerged_summary = tf.merge_all_summaries()\nto_fetch = outputs\nif isinstance(to_fetch, list):\n    to_fetch.append(merged_summary)\nelse:\n    to_fetch = [to_fetch, merged_summary]\n\nwith tf.Session(\"\") as sess:\n  # Initialize\n  tf.initialize_all_variables().run()\n  fetches = sess.run(to_fetch, feed_dict=feeds)\n  summary = fetches[-1]\n  summary_writer.add_summary(summary, 1)\n\n```\n", "Do you need more information @ebrevdo ?\n", "What happens if you remove the \"sequence_length\" parameter to rnn?  to dynamic_rnn?\n", "if I removed the sequence_length, rnn() works, dynamic_rnn doesn't work. maybe just give empty values to histogram when the part of graph wasn't executed can fix this problem.\n", "It does indicate that histogram creation inside contexts, including cond\nand while, is unsupported.  We can add it to rnn() but dynamic_rnn will be\ndifficult without help from Yuan.\n\nOn Aug 19, 2016 11:29 AM, \"eDz\" notifications@github.com wrote:\n\n> if I removed the sequence_length, rnn() works, dynamic_rnn doesn't work.\n> maybe just give empty values to histogram when the part of graph wasn't\n> executed can fix this problem.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2714#issuecomment-241097716,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim8rUlv1DfErhU8WvqDRVLbj-0rCKks5qhfYSgaJpZM4IwQyK\n> .\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 2713, "title": "how to assign values in Tensor according to the indices?", "body": "I want to assign values in a tensor according to the indices.\n\nFor example, According to the pooling values and the corresponding indices output of `tf.nn.max_pool_with_argmax`, I want to put these pooling values back into the original unpooling Tensor given the indices.\n\nAccording to the trick in [Adjust Single Value within Tensor \u2014 TensorFlow](http://stackoverflow.com/questions/34685947/adjust-single-value-within-tensor-tensorflow/34686952#34686952)\nI can recreate sparse tensor with the indices.\nBut here is the problem: `tf.SparseTensor`  should input the unflattened indices i.e. the ndims coordinates list. \nIf I use this method, how to unravel the flattened indices obtained by `tf.nn.max_pool_with_argmax` to the normal indices?\nIf I do not use this method, is there any way in Tensorflow to achieve this work?\nThank you very much.\n", "comments": ["@vrv @mrry @girving \nThank you very much.\n", "I also post my question in stackoverflow:\nhttp://stackoverflow.com/questions/37679697/in-tensorflow-how-to-assign-values-in-tensor-according-to-the-indices\n", "Thank you for posting the question on StackOverflow!  See also #2075.\n"]}, {"number": 2712, "title": "Tensorflow with Pyinstaller", "body": "It seems that Tensorflow does not work with Pyinstaller. I was trying to build a Tensorflow script into a executable file using Pyinstaller on Ubuntu. However the following error is reported:\n\ntensorflow.python.framework.errors.NotFoundError: tensorflow/contrib/layers/python/ops/_bucketization_op.so: cannot open shared object file: No such file or directory\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN:  None\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n### Steps to reproduce\n1. Pyinstaller tensorflow_script.py\n2. ./tensorflow_script\n", "comments": ["@martinwicke: Do we support Pyinstaller?  I've never heard of it. \n\n@thematrixduo: Where did you get `tensorflow_script.py`? \n", "I don't know what pyinstaller is, and consequently I'm certain that we don't support it. \n", "(It is likely that whatever dependency resolution pyinstaller uses to determine which extensions to include in its package does not pick up the correct files. That would be an issue with pyinstaller. You can look at the build_pip_package target to find out what needs to be included for all of TensorFlow.)\n", "@girving tensorflow_script.py is just an example tensorflow script.\n", "@martinwicke Then what is your recommended way to distribute a standalone python application with TF ?", "If you're talking about a python package, create a pip package, and depend on the tensorflow package.\r\n\r\nIf you're talking about an application, this would depend on the platform you're targeting, and there's no answer that would fit all. "]}, {"number": 2711, "title": "Add cumsum and cumprod ops", "body": "This implements `tf.scan_sum` (cumulative sum) and `tf.scan_prod` (cumulative product) ops.\nUnfortunately, this won't compile with the current version of eigen, as I have to make some small changes to `TensorScanOp.h`\nBut this should be useful to discuss how the API should look and whether we want to have inclusive/exclusive ops, and so on.\n\nI'll also add tests and gradients to the two ops.\n\nThis relates to #813.\n", "comments": ["Can one of the admins verify this patch?\n", "Here is the corresponding PR in Eigen, which needs to propagate into TensorFlow first before this PR can compile: https://bitbucket.org/eigen/eigen/pull-requests/195/add-small-fixes-to-tensorscanop\n", "Seeing as `tf.reverse(tf.scan_X(tf.reverse(x)))` is useful in quite a few situations, would it make sense to add a `reverse=True/False` kwarg to the scan ops?\nThis could either be done in the Python wrappers, or in C++.\n\nAlso, I haven't yet figured out how to write the gradient for `scan_prod` without using the same division trick that's also been used in #2641.\n", "I've made a few changes:\n- Renamed the ops to `cumsum` and `cumprod`\n- Added GPU support\n- Added a `reverse` kwarg to efficiently compute backwards scans\n- Added gradients (the `cumprod` gradient still uses the `/ x` hack)\n- Added tests\n\nMy Eigen PR has also been merged, so we will be able to test the PR as soon as Eigen is updated to the newest version.\n", "Sorry for the delay.  Looking over this now.\n", "Jenkins, test this please.\n", "Don't think it'll compile, as the current Eigen version doesn't contain the two fixes that were merged into Eigen two days ago.\n", "@ibab Alas, re the eigen change.  Is there something I should do to help that process along?  If not, can you send me a mention here when we're ready to go?\n", "@benoitsteiner Does something need to happen to get those Eigen changes in?\n", "We could simply modify the bazel workspace and assorted files to pull the newest Eigen version, but that could interfere with @benoitsteiner's work (might produce a merge/rebase conflict for the affected files).\n", "Or not. Go for it @ibab \n", "I've added exclusive scan functionality to Eigen here: https://bitbucket.org/eigen/eigen/pull-requests/197/implement-exclusive-scan-option-for-tensor/diff.\nI also have a version of `ScanOp` that has an optional `exclusive=True/False` argument, which I can push in a new PR once we finish this one.\n", "My changes to Eigen have passed in the nightly tests.\n@benoitsteiner: Would it be okay to update the Eigen version now?\nIs that something I should do as part of this PR?\n", "@girving: The Eigen version in TensorFlow has been updated, so we can have a shot at testing this.\nI've rebased my changes on top of `master` and squashed them into a single commit.\nI've also changed the copyright holder from `Google` to `The TensorFlow Authors`.\n\nThe GPU test is segfaulting for me, but I've debugged this a bit and it seems to be a problem that affects many of the tests on my machine.\nEverything works if I test it manually and force the device with `tf.device('/gpu:0')`, and the CPU tests are working, so I suspect the tests will simply pass if we run them.\n", "Jenkins, test this please.\n", "Seems like it's also failing on travis.\nI can reproduce the segfault on my machine when setting the `opt_level` to `-1` with the following code:\n\n``` python\nfrom tensorflow.core.protobuf import config_pb2\nconfig = config_pb2.ConfigProto()\nconfig.graph_options.optimizer_options.opt_level = -1\ns = tf.Session(config=config)\nwith tf.device('/gpu:0'):\n  x = np.arange(1, 6).astype(np.float32)\n  s.run(tf.cumsum(x, 0, False))\n```\n\nStrangely, it works without problems when running with the default opt level.\nThe segfault occurs when accessing the memory of the input tensor.\n", "Unpleasant!  Probably a job for a debug+optimized compile and valgrind; I don't have any particular smart ideas for what it might be.\n", "Can you try with the latest version of Eigen (version [3f47637d1e7c1](https://bitbucket.org/eigen/eigen/commits/3f47637d1e7c199b10c66bf1f8d3bf046e479d26)) ? I just fixed a potential issue in the implementation of the evaluator of the scan operation that may result in dangling references once the evaluator is copied.\n", "Thanks for fixing the evaluator!\nI've compiled with the changes in `3f47637d1e7c1`, but unfortunately the tests are still segfaulting.\nI'll try to hunt down the problem using valgrind.\n", "@ibab  The problem is that evalSubExprIfNeeded is executed on CPU, while both your input data and the temporary buffer reside on GPU memory. You're going to have to launch a CUDA kernel in which you call actually call accumulateTo for the scan operations to work on GPU.\n", "Thanks!\nI also noticed this when I tried to reproduce the problem with `cuda-gdb`, but didn't actually end up in a kernel.\nI had assumed that the kernel launch would be automatically handled by the framework (e.g. in `TensorExecutor`)\nI can add the launch in a new PR to eigen.\n", "@ibab. TensorExecutor will call the coeff and packet methods from a CUDA kernel. However it will call evalSubExprIfNeeded and cleanup from the host. This makes it possible to do preprocessing on the host if needed. Looking forward to your pull request.\n", "@ibab Any update on this ? from the thread seems like there is some work to be done in the eigen library before this PR can be successfully merged. Is that correct ? \n", "@Mistobaan I've made the necessary changes in Eigen here: https://bitbucket.org/eigen/eigen/commits/bbffe8ed5f26dfc7166e9fd97ce15bc2655af769\nCurrently I'm just waiting for an update of the Eigen version that's used in tensorflow.\n", "@girving @benoitsteiner Maybe I should update the Eigen version as part of this PR, or in a separate PR? \n", "@ibab I'd recommend that you create a separate PR to update Eigen. From time to time we hit a compatibility issue, and we need to update TensorFlow as well. It's best to have a standalone PR that contains both the Eigen update and the corresponding TF changes.\n", "I've updated the Eigen version, so the tests should pass now (they're passing locally for me).\n", "@tensorflow-jenkins test this please\n", "Cool, @girving this looks ready to review.\n", "Looks good.  I read through it again, and nothing seems to have changed since the last review.\n\n@ibab Thank you for the contribution!\n", "@girving Thanks for reviewing!\n"]}, {"number": 2710, "title": "Branch 124251558", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2709, "title": "Unable to infer shape of placeholder", "body": "```\nmyPlaceholder = tf.placeholder(dtype=tf.float32, shape=[4, 5])\ntf.shape(myPlaceholder).eval()\n```\n\n```\nInvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [4,5]\n```\n\nOne shouldn't have to feed a value for the placeholder because the shape is already determined.\n", "comments": ["@mrry has an optimization that decouples this dependency, but maybe it has to be turned on manually?\n", "Why not just use the static shape, `myPlaceholder.get_shape()`?\n\nThere is a prototype optimization (@cwhipkey is working on it now) that uses inferred shapes in constant folding, but I'm not sure that it will make your use case work. I'm inclined to say that you must _always_ feed a placeholder if you want to fetch a tensor whose value is a function of that placeholder, because making the semantics depend on optimization level seems confusing....\n", "@RafaelCosman: I think the conclusion is that we don't want to support the independence you want to rely on, but we also don't want to guarantee that the exception will be thrown.  Thus, what you want to do might happen to start working at some point in the future. \n"]}, {"number": 2708, "title": "TensorBoard shows no histograms or events", "body": "Running TensorBoard r0.9 results in graph visualizations as expected but all events and histograms that successfully displayed in r0.8 [are not](http://stackoverflow.com/q/37684739/656912).\n\nHas r0.9 introduced a change to the command line that should be used to launch TensorBoard, or to the code needed to generate events and histograms for TensorBoard to display?\n\nNote that neither new summaries and histograms written with recent runs using r0.9 TensorFlow, nor existing ones written (and displayed) in the past, are displayed. Graphs generated with both releases display as expected.\n### Environment info\n\nOperating System: OS X 10.11.5\nTensorFlow: .9.0rc0\n### Steps to reproduce\n\n`tensorboard --logdir ./tflog --purge_orphaned_data`\n### What have you tried?\n\nReverting to r0.8, which works as it originally did, displaying all events and histograms present in the `tflog`directory provided as the `logdir`. Reinstalling r0.9, which reproduces the error. A second attempt at reinstalling r0.8 resulted in a TensorBoard that displays jus the menu (no content) and uses a different style (a serif face).\n", "comments": ["PTAL, @danmane \n", "Here's the \"events\" page source:\n\n```\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>TensorBoard</title>\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"/lib/css/global.css\">\n\n    <script src=\"external/lodash/lodash.min.js\"></script>\n    <script src=\"external/d3/d3.min.js\"></script>\n    <script src=\"/external/plottable/plottable.min.js\"></script>\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"/external/plottable/plottable.css\">\n\n    <script src=\"external/graphlib/dist/graphlib.core.min.js\"></script>\n    <script src=\"external/dagre/dist/dagre.core.min.js\"></script>\n\n    <link rel=\"import\" href=\"external/polymer/polymer.html\">\n    <script src=\"external/webcomponentsjs/webcomponents-lite.min.js\"></script>\n\n    <link rel=\"import\" href=\"external/iron-ajax/iron-ajax.html\">\n    <link rel=\"import\" href=\"external/iron-collapse/iron-collapse.html\">\n    <link rel=\"import\" href=\"external/iron-list/iron-list.html\">\n    <link rel=\"import\" href=\"external/paper-button/paper-button.html\">\n    <link rel=\"import\" href=\"external/paper-checkbox/paper-checkbox.html\">\n    <link rel=\"import\" href=\"external/paper-dialog/paper-dialog.html\">\n    <link rel=\"import\" href=\"external/paper-dropdown-menu/paper-dropdown-menu.html\">\n    <link rel=\"import\" href=\"external/paper-header-panel/paper-header-panel.html\">\n    <link rel=\"import\" href=\"external/paper-icon-button/paper-icon-button.html\">\n    <link rel=\"import\" href=\"external/paper-input/paper-input.html\">\n    <link rel=\"import\" href=\"external/paper-item/paper-item.html\">\n    <link rel=\"import\" href=\"external/paper-menu/paper-menu.html\">\n    <link rel=\"import\" href=\"external/paper-progress/paper-progress.html\">\n    <link rel=\"import\" href=\"external/paper-radio-button/paper-radio-button.html\">\n    <link rel=\"import\" href=\"external/paper-radio-group/paper-radio-group.html\">\n    <link rel=\"import\" href=\"external/paper-slider/paper-slider.html\">\n    <link rel=\"import\" href=\"external/paper-styles/paper-styles.html\">\n    <link rel=\"import\" href=\"external/paper-toggle-button/paper-toggle-button.html\">\n    <link rel=\"import\" href=\"external/paper-toolbar/paper-toolbar.html\">\n    <link rel=\"import\" href=\"external/paper-tabs/paper-tabs.html\">\n\n    <link rel=\"import\" href=\"dist/tf-tensorboard.html\">\n\n  </head>\n  <body>\n    <tf-tensorboard></tf-tensorboard>\n  </body>\n</html>\n```\n", "I also get an\n\n> SyntaxError: Unexpected token ')'\n\nin the final line of \n\n```\ndata:text/javascript;charset=utf-8,%0A\u2026%0A:81\n```\n\nbelow:\n\n```\n      this.updateStyles();\n      // The updateStyles call fails silently if the browser doesn't have focus,\n      // e.g. if TensorBoard was opened into a new tab that isn't visible.\n      // As a workaround... we know requestAnimationFrame won't fire until the\n      // page has focus, so updateStyles again on requestAnimationFrame.\n      window.requestAnimationFrame(() => this.updateStyles());\n    },\n```\n", "You're using Safari, right? This is a duplicate of https://github.com/tensorflow/tensorflow/issues/2607. It's fixed in source code but not in the distributed tf-tensorboard bundle; an update for the bundle is coming today. \n", "@danmane: Yes Safari. Thanks!\n", "I have this problem with Chromium 51.0.2704.79 Ubuntu 14.04 64bit. Same with firefox.\nNo visuals except for the main graph are showing.\n", "@alexmonroe87 Can you run TensorBoard with the --inspect flag and post what is generated?\n"]}, {"number": 2707, "title": "Update roadmap.md", "body": "Add mesos integration to ecosystem work, placer work to core. \n", "comments": []}, {"number": 2706, "title": "While loop gradient descent error", "body": "### Environment info\n\nOperating System: Centos 7\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nCPU version\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n0.8.0\n### Steps to reproduce\n\nExample 1 for Tensorflow 0.8\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import rnn\n\nclass model(object):\n    def __init__(self):\n        error_position = 1\n        vocab_size = 20\n        embedding_size = 5\n        # Word2vec\n        W = tf.Variable(tf.random_normal([vocab_size, embedding_size]),\n            trainable=False, name=\"W\")\n        self.story = []\n        story_embedded = []\n        # Embedding\n        for i in range(4):\n            self.story.append(tf.placeholder(tf.int32, shape=[None]))\n            story_embedded.append(tf.nn.embedding_lookup(W, self.story[i]))\n\n        self.answer = tf.constant(3, tf.int64)\n        answer_weights = tf.Variable(tf.truncated_normal([embedding_size, vocab_size], -0.1, 0.1), name=\"answer_weights\")\n        # w2v to sentence2vec for story and question\n        scell = tf.nn.rnn_cell.GRUCell(embedding_size)\n        story_state_array = []\n\n        for i in range(0,4):\n            with tf.variable_scope(\"tt\", reuse=True if i > 0 else None):\n                _, story_state = rnn.rnn(scell, tf.unpack(tf.reshape(story_embedded[i],[4,1,5]), 4), dtype=tf.float32)\n                story_state_array.append(story_state)\n        storys = tf.concat(0,story_state_array)\n\n\n        mem_weights = tf.get_variable(\"mem_weights\", [embedding_size, embedding_size], initializer=tf.random_normal_initializer())\n        l1_weights = tf.get_variable(\"l1_weights\", [embedding_size, 1], initializer=tf.random_normal_initializer())\n\n        episodic_gate_unpacked = []\n        def body(mem_state_previous, hops):\n            mem_state_current = tf.nn.relu(tf.matmul(mem_state_previous, mem_weights))\n            hops = tf.add(hops,1)\n            return  mem_state_current, hops\n        def condition(mem_state_previous, hops):\n            z = tf.mul(storys, mem_state_previous)\n            e_reshaped = tf.reshape(tf.matmul(z , l1_weights) , [1,-1], name=\"e_reshaped\")\n            e_gate = tf.nn.softmax(e_reshaped)\n            e_unpacked = tf.unpack( tf.reshape(e_gate, [4,1]))\n            argmax_e = tf.to_int32(tf.argmax(e_gate, 1)) #should be 1\n            return tf.logical_and(tf.less(argmax_e[0], error_position),tf.less(hops,5))\n\n        init_state = tf.constant([[1.0,1.0,1.0,1.0,1.0,]])\n        initial_hops = tf.constant(0)\n        a_state, _ = tf.while_loop(condition,body,[init_state, initial_hops], back_prop=True)\n\n        # answer\n        predicted_answer = tf.matmul(a_state, answer_weights)\n        answer = tf.reshape(tf.one_hot(self.answer, vocab_size, 1.0, 0.0), [1,vocab_size])\n        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predicted_answer, answer), name='loss')\n\n        # gradient\n        params = tf.trainable_variables()\n        self.gradient_norms = []\n        self.updates = []\n        optimizer = tf.train.AdamOptimizer(0.05)\n        gradients = tf.gradients(self.loss, params)\n        self.updates = optimizer.apply_gradients(\n            zip(gradients, params))\n        self.saver = tf.train.Saver(tf.all_variables())\n\n    def step(self,sess):\n        feed ={}\n        feed[self.story[0].name] = [1,2,3,4]\n        feed[self.story[1].name] = [4,5,6,5]\n        feed[self.story[2].name] = [7,8,9,8]\n        feed[self.story[3].name] = [0,2,3,5]\n\n        print sess.run([self.loss,self.updates],feed)\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        test_model = model()\n        sess.run(tf.initialize_all_variables())\n        test_model.step(sess)\n\n```\n\nExample 2 for Tensorflow 0.9\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import rnn\n\nclass model(object):\n    def __init__(self):\n        error_position = 2\n        vocab_size = 20\n        embedding_size = 5\n        # Word2vec\n        W = tf.Variable(tf.random_normal([vocab_size, embedding_size]),\n                trainable=False, name=\"W\")\n        self.story = []\n        story_embedded = []\n        # Embedding\n        for i in range(4):\n            self.story.append(tf.placeholder(tf.int32, shape=[None,None]))\n            story_embedded.append(tf.nn.embedding_lookup(W, self.story[i]))\n\n        self.question = tf.placeholder(tf.int32, shape=[None,None], name=\"Question\")\n        question_embedded = tf.nn.embedding_lookup(W, self.question)\n\n        self.answer = tf.constant(3, tf.int64)\n        answer_weights = tf.Variable(tf.truncated_normal([embedding_size, vocab_size], -0.1, 0.1), name=\"answer_weights\")   \n        # w2v to sentence2vec for story and question\n        scell = tf.nn.rnn_cell.GRUCell(embedding_size)\n        story_state_array = []\n\n        q_cell = tf.nn.rnn_cell.GRUCell(embedding_size)\n        _, question_state = tf.nn.rnn(q_cell, tf.unpack(question_embedded, 4), dtype=tf.float32)\n\n        for i in range(0,4):\n            with tf.variable_scope(\"tt\", reuse=True if i > 0 else None):\n                _, story_state = tf.nn.rnn(scell, tf.unpack(story_embedded[i], 4), dtype=tf.float32)\n                story_state_array.append(story_state)   \n\n        stories = tf.concat(0,story_state_array)\n\n\n        mem_weights = tf.get_variable(\"mem_weights\", [embedding_size * 2, embedding_size], initializer=tf.random_normal_initializer())\n        l1_weights = tf.get_variable(\"l1_weights\", [embedding_size, 1], initializer=tf.random_normal_initializer())\n\n        e_unpacked = [] \n        def body(mem_state_previous, hops):\n            # context = MGRU(facts_, e_unpacked)\n            mem_state_current = tf.nn.relu(tf.matmul(tf.concat(1,[mem_state_previous, question_state]), mem_weights))\n\n            hops = tf.add(hops,1)\n            return  mem_state_current, hops\n        def condition(mem_state_previous, hops):    \n            z = tf.mul(stories, mem_state_previous)\n            e_reshaped = tf.reshape(tf.matmul(z , l1_weights) , [1,-1], name=\"e_reshaped\")\n            e_gate = tf.nn.softmax(e_reshaped)\n            e_unpacked = tf.unpack( tf.reshape(e_gate, [4,1]))  \n            argmax_e = tf.to_int32(tf.argmax(e_gate, 1)) #should be 1\n            return tf.logical_and(tf.less(argmax_e[0], error_position),tf.less(hops,5))\n\n\n        initial_hops = tf.constant(0)\n        a_state, _ = tf.while_loop(condition,body,[question_state, initial_hops], back_prop=True)   \n\n        # answer\n        predicted_answer = tf.matmul(a_state, answer_weights)\n        answer = tf.reshape(tf.one_hot(self.answer, vocab_size, 1.0, 0.0), [1,vocab_size])\n        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predicted_answer, answer), name='loss')\n\n        # gradient\n        params = tf.trainable_variables()   \n        self.gradient_norms = []\n        self.updates = []\n        optimizer = tf.train.AdamOptimizer(0.05)\n        gradients = tf.gradients(self.loss, params) \n        self.updates = optimizer.apply_gradients(\n            zip(gradients, params))\n        self.saver = tf.train.Saver(tf.all_variables())\n\n    def step(self,sess):    \n        feed ={}\n        feed[self.story[0].name] = [[1],[2],[3],[4]]\n        feed[self.story[1].name] = [[4],[5],[6],[5]]\n        feed[self.story[2].name] = [[7],[8],[9],[8]]\n        feed[self.story[3].name] = [[0],[2],[3],[5]]\n        feed[self.question.name] = [[1],[4],[5],[5]]\n\n        print sess.run([self.loss,self.updates],feed)\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        test_model = model()\n        sess.run(tf.initialize_all_variables())\n        test_model.step(sess)\n\n```\n### What have you tried?\n\nIn my `tf.while_loop()`, it calculates attention in each loop. If the attention is beyond certain threshold, it would terminate the `while_loop`. But it does not work as I expected.\n\nIn this two situations, it works\n1. If I turn `back_prop=False` in `while_loop`, it works.\n2. If `argmax_ep_gate[0]` always `< error_position`, it works.\n\n**Hence, the error might appear after several trails** \n\nAlso, we found that in `z = tf.mul(storys, mem_state_previous)`, if `storys` does not generated after `tf.concat(0, story_state_array)` such as a single sentence2vec from `_, story_state = rnn.dynamic_rnn(scell, story_embedded[i], dtype=tf.float32)`, it works.\n### Logs or other output that would be helpful\n\nBelow log is generated **before** shrinking the example for both Tensorflow 0.8 and 0.9\n\n``` python\nTraceback (most recent call last):\n  File \"github_issue.py\", line 111, in <module>\n    test_model.step(sess)\n  File \"github_issue.py\", line 106, in step\n    print sess.run([self.loss, self.gradient_norms],feed)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: Inputs to operation gradients/AddN of type AddN must have the same size and shape.  Input 0: [1,5] != input 1: []\n     [[Node: gradients/AddN = AddN[N=2, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/while/Enter_grad/Exit, gradients/while/Mul/Enter_1_grad/b_acc_3)]]\nCaused by op u'gradients/AddN', defined at:\n  File \"github_issue.py\", line 109, in <module>\n    test_model = model()\n  File \"github_issue.py\", line 89, in __init__\n    gradients = tf.gradients(self.loss, params)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 431, in gradients\n    out_grads = _AggregatedGrads(grads, op, loop_state, aggregation_method)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 676, in _AggregatedGrads\n    out_grads[i] = math_ops.add_n(out_grad)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 58, in add_n\n    return _op_def_lib.apply_op(\"AddN\", inputs=inputs, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\n```\n\nBelow log is generated **after** shrinking the example, fixed after 0.9\n\n``` python\nTraceback (most recent call last):\n  File \"github_issue.py\", line 82, in <module>\n    test_model.step(sess)\n  File \"github_issue.py\", line 77, in step\n    print sess.run([self.loss,self.updates],feed)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: var and grad do not have the same shape[5,5] []\n     [[Node: Adam/update_mem_weights/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@mem_weights\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](mem_weights, mem_weights/Adam, mem_weights/Adam_1, beta1_power/read, beta2_power/read, Adam/learning_rate, Adam/beta1, Adam/beta2, Adam/epsilon, gradients/while/MatMul_1/Enter_grad/b_acc_3)]]\nCaused by op u'Adam/update_mem_weights/ApplyAdam', defined at:\n  File \"github_issue.py\", line 80, in <module>\n    test_model = model()\n  File \"github_issue.py\", line 67, in __init__\n    zip(gradients, params))\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 299, in apply_gradients\n    update_ops.append(self._apply_dense(grad, var))\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/adam.py\", line 129, in _apply_dense\n    self._epsilon_t, grad, use_locking=self._use_locking).op\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/gen_training_ops.py\", line 120, in apply_adam\n    use_locking=use_locking, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\n```\n", "comments": ["Thanks for reporting this.  Would it be possible to create a simpler example to reproduce the problem?\n", "Thanks for such quick reply. Just shrink our example, but couldn't do much and I add #!!! at the end some lines that might be important.\n", "There are at least two loops involved.  Would it be possible to reduce it to contain only one?\n", "Sorry, I don't quite understand. I'm using only one while loop. Regarding to two loops, do you mean by \n\n``` python\nfor i in range(4):\n   self.story.append(tf.placeholder(tf.int32, shape=[None,None]))\n  story_embedded.append(tf.nn.embedding_lookup(W, self.story[i]))\n```\n\nand\n\n``` python\n for i in range(0,4):\n   with tf.variable_scope(\"tt\", reuse=True if i > 0 else None):\n     _, story_state = rnn.dynamic_rnn(scell, story_embedded[i], dtype=tf.float32)    #!!!\n    story_state_array.append(story_state)   \n```\n\n?\n", "rnn.dynamic_rnn() is implemented using a while_loop, and you also have an explicit while_loop in your code. It would be simpler to debug if the problem could be reproduced with only one of them.\n", "Thank you. Replaced rnn.dynamic_rnn() with rnn.rnn() in example and same issue still.\n", "I have tried TensorFlow 0.9 today. It seems that the first example works, so I make another example, which is able to reproduce first error log.  Also, I am still wondering why example 1 works. Thanks.\n", "FYI: I have a fix in the work. If my understanding is correct, the problem is that we don't support broadcasting for AddN, which is used to accumulate partial gradients.\n", "Thanks. Do you have any suggestion on how to modify my code so as to place me on to the right track?\n", "any updates on this issue? \n", "A possible workaround is to make:\n\ndef condition(mem_state_previous, hops):  \n            z = tf.mul(stories, mem_state_previous)\n            e_reshaped = tf.reshape(tf.matmul(z , l1_weights) , [1,-1], name=\"e_reshaped\")\n            e_gate = tf.nn.softmax(e_reshaped)\n            e_unpacked = tf.unpack( tf.reshape(e_gate, [4,1]))  \n            argmax_e = tf.to_int32(tf.argmax(e_gate, 1)) #should be 1\n            return tf.logical_and(tf.less(argmax_e[0], error_position),tf.less(hops,5))\n\nto be true at least once.  \n", "Thanks, problem solved. I think my code breaks the graph, if `condition()` is `False` in the first run. \n", "I am using tensorflow-1.1.0rc2, when I use the code for:\r\n`scell = tf.nn.rnn_cell.GRUCell(embedding_size)`\r\n\r\nI get the following error:\r\n`AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'`\r\n\r\nAny idea what to use instead for 1.1.0rc2 version? Looks like it has been removed but I couldn't find any good documentation for the solution.", "What is this error is about? I can't find much information about it.\r\n\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Inputs to operation gradients/AddN of type AddN must have the same size and shape.  Input 0: [2,3,320,320] != input 1: [2,1,320,320]"]}, {"number": 2705, "title": "Update Cmake build files to reflect recent dependency change", "body": "Add external library dependencies including\n- jsoncpp\n- boringssl\n- farmhash\n- highwayhash\n\nAlso create targets to generate the `proto_text` binary, and add a custom rule for generating proto text parser code using the previous output tool.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins, test this please.\n", "Thanks for doing this! I've been trying to resuscitate the CMake files, and this saves me a lot of effort :).\n", "@jendap @caisq: Does either of you know why the \"Linux CPU Tests CMAKE\" test might be failing with the following error?\n\n```\n[  0%] Running C++ protocol buffer compiler on tensorflow/core/framework/log_memory.proto\nmake[2]: execvp: /usr/local/bin/protoc: Permission denied\nmake[2]: *** [tensorflow/core/framework/log_memory.pb.cc] Error 127\nmake[1]: *** [CMakeFiles/tf_protos_cc.dir/all] Error 2\nmake: *** [all] Error 2\n```\n\nIt looks like the `install_proto3.sh` script is running. Is there something else we need to do to chmod `protoc`?\n", "@lilac: I've forked this PR (#2839) and added the necessary fixes to make it build the tutorial example.\n", "I'm going to merge this (even though it is incomplete), and follow up with #2839, which makes the build work again.\n"]}, {"number": 2704, "title": "Normalize cuDNN spelling", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 2703, "title": "\"ERROR: Cannot find './util/python/python_lib'\"", "body": "Hello,\n\nI have the following error when I try to compile the pip package from the source code.\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\nls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Feb  9 18:48 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Feb  9 18:48 /usr/local/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 root root 61453024 Feb  8 23:12 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 root root 62025862 Feb  8 23:12 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from sources, provide the commit hash: ea9e00a630f91a459dd5858cb22e8cd1a666ba4e\n### Steps to reproduce\n1. git clone https://github.com/tensorflow/tensorflow\n2. cd tensorflow\n3. ./configure\n4. bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n5. bazel build -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\n### Logs or other output that would be helpful\n\n```\nERROR: /home/plu/git/tensorflow/util/python/BUILD:14:1: Executing genrule //util/python:python_check failed: bash failed: error executing command \n  (cd /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/packages/bin:.:/homes/plu/docker/:/usr/local/cuda-7.5/bin/:/homes/plu/bin/:/usr/local/openmpi-1.10.1/bin/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; OUTPUTDIR=\"bazel-out/host/genfiles/util/python/\"; util/python/python_config.sh --check && touch $OUTPUTDIR/python_checked'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: bash failed: error executing command \n  (cd /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/packages/bin:.:/homes/plu/docker/:/usr/local/cuda-7.5/bin/:/homes/plu/bin/:/usr/local/openmpi-1.10.1/bin/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; OUTPUTDIR=\"bazel-out/host/genfiles/util/python/\"; util/python/python_config.sh --check && touch $OUTPUTDIR/python_checked'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n\n\nERROR: Cannot find './util/python/python_lib'.  Did you run configure?\n```\n\nBut, I do have this folder locally in the Tensorflow repository. Did I do something wrong? Or it is a bug coming from Tensorflow?\n\nThanks in advance for any help!\n", "comments": ["Did you run configure?\n", "Hello,\n\nYes I did run configure as it is written in the \"Steps to reproduce\" section of my previous post :-)\n", "@jplu: I saw that, but one can never be too careful when triaging hundreds of issues. :)\n\nCan you add a `pwd` line to `python_config.sh` to see what directory it's in?  I'm not sure how Bazel genrules work, so it's possible that the BUILD file should declare some dependencies.\n", "@girving: No worries I understand :)\n\nI put a `pwd` command right after the licence header and run once again\n\n`bazel build -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\n\nBut nothing is displayed:\n\n```\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\nWARNING: Output base '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7' is on NFS. This may lead to surprising failures and undetermined behavior.\n......\nINFO: Found 1 target...\nERROR: /home/plu/git/tensorflow/util/python/BUILD:14:1: Executing genrule //util/python:python_check failed: bash failed: error executing command \n  (cd /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/packages/bin:.:/homes/plu/docker/:/usr/local/cuda-7.5/bin/:/homes/plu/bin/:/usr/local/openmpi-1.10.1/bin/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; OUTPUTDIR=\"bazel-out/host/genfiles/util/python/\"; util/python/python_config.sh --check && touch $OUTPUTDIR/python_checked'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: bash failed: error executing command \n  (cd /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/packages/bin:.:/homes/plu/docker/:/usr/local/cuda-7.5/bin/:/homes/plu/bin/:/usr/local/openmpi-1.10.1/bin/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; OUTPUTDIR=\"bazel-out/host/genfiles/util/python/\"; util/python/python_config.sh --check && touch $OUTPUTDIR/python_checked'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow\n\n\nERROR: Cannot find './util/python/python_lib'.  Did you run configure?\n\n\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 15.143s, Critical Path: 2.45s\n```\n", "@jplu You might need a flag to bazel to get it to not suppress stdout.  Not sure which that is, though; `bazel help build` doesn't make it obvious.  The high tech option is `pwd > ~/blah`.\n", "The redirect command did the thing, I had to think about this solution before...\n\nAnyway, here the path:\n\n`/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow`\n", "@damienmg We have a `python_check` genrule that verifies that a symbolic link to a directory exists.  I think the check is failing since we don't depend on the symbolic link.  Can one put a symbolic link to a directory in a normal filegroup, or is something special required?\n\nhttps://github.com/tensorflow/tensorflow/blob/master/util/python/BUILD#L14\n\nOn the other hand, @martinwicke, I suppose if we add `python_lib` as a dependency then `python_check` will fail with a less informative error message.  Thoughts?  I imagine this error message has a significant role in reducing filed issues, so it'd be bad to make it worse. \n", "FWIW: the path was in the output :) `/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow`\n\nThe test that check a symlink is printing a more comprehensive message about it not being a symlink, or did I read the script wrong?\n", "(and nothing special required with symlinks, but remember that when running, most files will actually be symlinks while directories will be recreated).\n", "Can we just use all_files as a data depedency for python_check? That will not fail (since it's just a glob), and if nothing is there, we'll still see it's not there and get our nice message.\n", "@martinwicke Seems reasonable.  Want to assign it to someone?\n", "This used to work out of the box, no?  Sounds like a bug was recently introduced.\n", "Olivia, can you change the BUILD file in util/python to have a filegroup containing all files in the directory, like this:\n\nfilegroup(\n    name = \"configure_files\",\n    data = glob([\n        \"*\",\n    ])\n)\n\n(syntax may be screwy), and add that as a data dependency to the python_check target?\n\nAnd then make sure it works... I hadn't seen this before.\n", "@jplu can you check whether #2958 fixes your problem?\n", "Sorry for the delay I was not able to use this computer until now.\n\nUnfortunately the problem still remains :-(\n\n```\nWARNING: Output base '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7' is on NFS. This may lead to surprising failures and undetermined behavior.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/protobuf/WORKSPACE:1: Workspace name in /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/re2/WORKSPACE:1: Workspace name in /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/highwayhash/WORKSPACE:1: Workspace name in /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /home/plu/git/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nINFO: Found 1 target...\nERROR: /home/plu/git/tensorflow/util/python/BUILD:14:1: Executing genrule //util/python:python_check failed: bash failed: error executing command \n  (cd /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/packages/bin:.:/homes/plu/docker/:/usr/local/cuda-7.5/bin/:/homes/plu/bin/:/usr/local/openmpi-1.10.1/bin/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; OUTPUTDIR=\"bazel-out/local_linux-opt/genfiles/util/python/\"; util/python/python_config.sh --check && touch $OUTPUTDIR/python_checked'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n\n\nERROR: Cannot find './util/python/python_lib'.  Did you run configure?\n\n\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 2.326s, Critical Path: 0.59s\n```\n\nI have done a fresh new clone corresponding to the commit https://github.com/tensorflow/tensorflow/commit/84225a2b612fe748c9c923f0c1cb8471911c3b77\n", "@caisq Could you take a look at this please? \n", "I had this same issue in the closed tickets referenced above, @jmchen-g thanks for re-opening this one. \n", "Hello,\n\nI have tested with the commit fefe52c that contains the fix proposed by @plutoshe. Unfortunately the issue still remains :(\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\nWARNING: Output base '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7' is on NFS. This may lead to surprising failures and undetermined behavior.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/protobuf/WORKSPACE:1: Workspace name in /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/re2/WORKSPACE:1: Workspace name in /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/highwayhash/WORKSPACE:1: Workspace name in /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /home/plu/git/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nINFO: Found 1 target...\nERROR: /home/plu/git/tensorflow/util/python/BUILD:14:1: Executing genrule //util/python:python_check failed: bash failed: error executing command \n  (cd /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/packages/bin:.:/homes/plu/docker/:/usr/local/cuda-7.5/bin/:/homes/plu/bin/:/usr/local/openmpi-1.10.1/bin/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; OUTPUTDIR=\"bazel-out/local_linux-opt/genfiles/util/python/\"; util/python/python_config.sh --check && touch $OUTPUTDIR/python_checked'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n\n\nERROR: Cannot find './util/python/python_lib'.  Did you run configure?\n\n\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 1.909s, Critical Path: 0.20s\n```\n\nDon't hesitate to ask me if you need other information to help you to debug.\n", "what's your bazel version?\n", "I'm using Bazel 0.2.3\n", "I'm using bazel 0.2.2 with the same error\n", "Which python did you specify when running `configure`? Also, can you run `/bin/bash`?\n", "Python specified is /usr/local/bin/python (version Python 2.7.11)\n\nRan /bin/bash before the install instructions with no change in result.\n", "The specified Python is:\n\n```\n/usr/bin/python --version\nPython 2.7.6\n```\n\nI also can run `/bin/bash`:\n\n```\n/bin/bash --version\nGNU bash, version 4.3.11(1)-release (x86_64-pc-linux-gnu)\nCopyright (C) 2013 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n\nThis is free software; you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n```\n", "Ok. I'm just stabbing at the dark -- I cannot reproduce this and I'm wondering what the difference between our installations might be.\n\nCan you try running using docker just to see if that works? \n", "It works via Docker:\n\n```\ndocker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\nUnable to find image 'gcr.io/tensorflow/tensorflow:latest' locally\nlatest: Pulling from tensorflow/tensorflow\n759d6771041e: Pull complete \n8836b825667b: Pull complete \nc2f5e51744e6: Pull complete \na3ed95caeb02: Pull complete \nb1a230d2f7d7: Pull complete \nf4bd848af23b: Pull complete \n9ff2d28f3bea: Pull complete \nde3ac9a732b6: Pull complete \nba1ff7fd8338: Pull complete \n8b638dd1001b: Pull complete \n4c3f9ea22d7e: Pull complete \nDigest: sha256:8ae5229583adf18c1e50ac7fbf4c25301aef186eb3cc7c23d85999328a72ac4b\nStatus: Downloaded newer image for gcr.io/tensorflow/tensorflow:latest\n[I 06:05:16.943 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\n/usr/local/lib/python2.7/dist-packages/widgetsnbextension/__init__.py:30: UserWarning: To use the jupyter-js-widgets nbextension, you'll need to update\n    the Jupyter notebook to version 4.2 or later.\n  the Jupyter notebook to version 4.2 or later.\"\"\")\n[W 06:05:16.965 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.\n[W 06:05:16.966 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using authentication. This is highly insecure and not recommended.\n[I 06:05:16.969 NotebookApp] Serving notebooks from local directory: /notebooks\n[I 06:05:16.969 NotebookApp] 0 active kernels \n[I 06:05:16.969 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:8888/\n[I 06:05:16.969 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n```\n", "When I run:\n\n```\nutil/python/python_config.sh --check\n```\n\nI get the same error.\n", "Once the run of `./configure` is finished the environement variable `$PYTHON_BIN_PATH` is not set and makes crash the command:\n\n```\nutil/python/python_config.sh --setup\n```\n", "Ok, I found the issue, the script `./configure` creates two symlinks in the folder `util/python/`:\n- `python_include -> /usr/include/python2.7`\n- `python_lib -> /usr/lib/python2.7/site-packages`\n\nThe problem is that the path `/usr/lib/python2.7/site-packages` doesn't exists for me. I don't have the `site-packages` folder. Mine is in `/usr/local/python/lib/python2.7/site-packages/`\n\nEverything is working like a charm if I set the symlink `python_lib` to the proper path.\n\nA possible fix could be to ask another question in the configure to set where the packages for Python are installed. WDYT?\n", "Interesting. We actually ask distutils for this path (see [utils/python/python_config.sh](https://github.com/tensorflow/tensorflow/blob/master/util/python/python_config.sh)), so it appears distutils is misconfigured. We could switch to an [alternative method](http://stackoverflow.com/questions/122327/how-do-i-find-the-location-of-my-python-site-packages-directory), but I'm not at all sure it's better in general.\n", "The main problem is that people might have multiple location for their Python's packages. So why not checking the `$PYTHONPATH` variable? As this variable as to be updated to include all the path that are not \"Python standardized\" and that we want to use in general.\n", "Let's see if I understood:\n\nLet's say `distutils` lies to us and gives us a non-existent path (as in your case). What should we do next? We could ask the `site` module and see if it gives us an existing path (would it in your case?). If that fails also, we can ask the user. We should make sure the user gives us a path which is in `$PYTHONPATH`, otherwise it's very unlikely to work.\n", "Yes you have understood. If a Python lib is not in the standard location or in `$PYTHONPATH` then it is not possible to use it as Python will not know where are the files to import.\n\nFor example for my case, my `$PYTHONPATH` looks like this:\n\n```\necho $PYTHONPATH\n/home/plu/git/deepdetect/clients/python/:/usr/local/python/lib/python2.7/site-packages/:/home/plu/git/caffe/python\n```\n\nIf I run the interactive python and do `import caffe` I can load all the files in my current environment. In the case I remove `/home/plu/git/caffe/python` from my `$PYTHONPATH` and redo an `import caffe` I will get an `ImportError: No module named caffe`\n\nSo I think, by just checking this environment variable we should be safe, but we never know. Nevertheless, checking the `site`  is not enough, as for me:\n\n```\n>>> import site; site.getsitepackages()\n['/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages']\n```\n\nI get only those two, and not the others contained in my `$PYTHONPATH` which mean that if someone prefer to compile every package itself and put them in a specific location, we will miss them. So for me the steps to do are the following:\n1. use `site` to get a first list\n2. merge the result with the content of `$PYTHONPATH`\n3. be sure that all the paths exists otherwise remove it from the list\n4. ask the user if all the path are well known, otherwise ask them to add the wanted other paths\n", "@itsmeolivia can you look into modifying the python setup to include the workflow outlined by @jplu?\n", "Perfect! Everything is compiling like a charm with this fix\n\nThanks a lot!\n"]}, {"number": 2702, "title": "API doc typo for tf.nn.avg_pool3d and tf.nn.max_pool3d", "body": "In the tensorflow API doc: https://www.tensorflow.org/versions/master/api_docs/python/nn.html#max_pool3d, it states:\n\nksize: A list of ints that has length >= 5. 1-D tensor of length 5. The size of the window for each dimension of the input tensor. Must have ksize[0] = ksize[1] = 1.\n\nksize[0] = ksize[1] = 1 must be ksize[0] = ksize[4] = 1 for both average and max pooling 3d layers as detailed here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/pooling_ops_3d_test.py#L48\n", "comments": ["Duplicate of #2573.\n"]}, {"number": 2701, "title": "Fixes for Raspberry Pi compilation", "body": "", "comments": ["Jenkins, test this please.\n"]}, {"number": 2700, "title": "cast to integer as required re issue #2420", "body": "issue #2420 workaround fix\n\nFixes #2420 \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it! \nAs corp game-changing labs\n", "Thanks, it may take a bit for the CLA to propagate.  will look at this again tomrorow\n", "Make sure to close #2420 once this is merged, since the commit message doesn't say \"Fixes #2420\".\n", "I updated the commit message.  Corp CLA still hasn't propagated, will check again soon.\n", "signed individual CLA as well because still not seeing the Corp CLA.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "I guess this doesn't have a test for it, but I'll test and merge anyway.\n\n@ilblackdragon @martinwicke need moar test coverage\n\n@tensorflow-jenkins test this please\n"]}, {"number": 2699, "title": "tf.train.Example?", "body": "In the file\n\nhttps://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/how_tos/reading_data/convert_to_records.py#L64\n\nWhat does  tf.train.Example()  do?  I couldn't find it anywhere in the documentation.\n", "comments": ["It's a protocol buffer that is commonly used as the data format for training and evaluation examples in TensorFlow. The format is defined here:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto\n\nThe `tf.train.Example()` call instantiates a new protocol buffer, and fills in some of its fields.\n"]}, {"number": 2698, "title": "Fix gcc error: cannot convert 'auto*' to 'float*'", "body": "gcc 4.8.1 barfs on this reasonable-looking type inference for unclear\nreasons. In this case `const T*` is arguably clearer than `auto*`\nanyway.\n\nFixes https://github.com/tensorflow/tensorflow/issues/867 .\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@girving FYI.\n", "Thanks!  Jenkins, test this please.\n", "Looks like you'll need to sign the CLA first.\n", "@lesniewski I believe Dropbox has signed the corporate CLA so this should be fine, but I'm not sure if we should expect @googlebot to detect this.\n\n@willnorris for corporate CLAs, do we expect googlebot to say this has been signed? or do we just assume that if the user has @dropbox.com in their email that it is fine?\n", "Correct, Dropbox has signed the corporate CLA, but I wanted to get confirmation from our open source team before proceeding.\n\nMy corporate account was just now added to the google group for the google corporate CLA, but I'm not sure if/how that state would propagate to my github account. (It is a member of the github dropbox team, so in principle the infomation is there.)\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Hah, and googlebot pipes up 11 seconds after I post.\n", "@lesniewski Your comment is actually what triggered googlebot. \ud83d\ude04  It rescans a PR any time a new commit is pushed or whenever the initial submitter leaves a comment\n", "Jenkins, test this please.\n", "Sweet, thanks for fixing this Chris!\n", "No problem, hopefully my next patch will be more substantial :)\n"]}, {"number": 2697, "title": "Use -DGPR_BACKWARDS_COMPATIBILITY_MODE in gRPC.", "body": "This avoids using `secure_getenv()`, which prevents building on\noperating systems with a GLIBC < 2.17.\n", "comments": ["@tensorflow-jenkins test this please.\n", "@girving: This should fix #1307.\n", "LGTM, though I'd add \"Fixes #...\" to the commit message.\n", "@girving: Thanks, I'll do that in the merge commit message (after tests pass).\n"]}, {"number": 2696, "title": "Fix UniformUnitScalingInitializer -> uniform_unit_scaling_initializer", "body": "Fixes #1328.\n", "comments": []}, {"number": 2695, "title": "RNN's state_is_tuple doesn't work with initial_state", "body": "Assuming I want to batch series of inputs and propagate the cell state from one session run towards another for an epoch:\n\n```\nfor batch in epoch:\n  state = initial_state.eval()\n  feed_dict = {initial_state: state}\n  state = sess.run([final_state], feed_dict)\n```\n\nSince using state_is_tuple in the cells makes the state be a tuple on return:\n- using .eval() doesn't work for an initial state\n- subsequent states are returned as tuples and cannot be fed back into the session as tuples\n", "comments": ["When you use `state_is_tuple`, it literally means that the type of state is tuple.\nHowever, the first parameter should be a list of tensors and the second should be a dictionary whose key is of tensor type.\nAs far as I know, you have to separate the state tuple.\nSo your code should be like in the below form: (I haven't tested)\n\n```\nfor batch in epoch:\n    state_c = initial_state[0].eval()\n    state_m = initial_state[1].eval()\n    feed_dict = {initial_state[0]: state_c, initial_state[1]: state_m}\n    state_c, state_m = sess.run([final_state[0], final_state[1]], feed_dict)\n```\n\nNote that it is applicable only for simple RNNs without stacked layers; for a multi-layer case, check out my modification of the RNN PTB example: https://github.com/jihunchoi/tensorflow/blob/ptb_use_state_tuple/tensorflow/models/rnn/ptb/ptb_word_lm.py.\n", "Thank you for the example, it did work and it is indeed faster :)\nIt does seem to be quite a struggle to get it exactly right, perhaps being able to pass tuples into the feed dict would help.\n", "This may be something we're working on.  Would you like to open a new bug to track it, explicitly describing the semantics of how you'd like to be able to pass tuples into feed_dict?  Closing this bug for now.  Thanks for answering @jihunchoi!\n", "Sorry to comment on this closed issue, but I came across the exact same problem when trying to get rid of the 'state_is_tuple' warning. The usage of the tuple-less state was very nice and manageable (at least for the basic stuff I was doing), passing in and out a single tensor. With the tuple state, if I have a variable number of cells (e.g. trying different hyperparameters), the code becomes a bit more ugly. I wrote something like this below, which returns a dict which I can use for feeding into the initial state. But then the final state also becomes a problem, and I'm not sure what the best way to manage this dynamically is. is this really the best way?\n\n```\ndef init_state(self):\n    states_dict = {}\n    for layer in self.initial_states_:\n        for state in layer:\n            states_dict[state] = state.eval()\n    return states_dict\n```\n", "just a follow up on this. I think I have it fully working now (tested a small model). For a Graves-style sequence generation it took quite a bit of wrangling. \n\nThis was the original code (relevant bits only) without tuples\n\n```\nstate = model.initial_state.eval()\nloop:\n    feed = {model.inputs:x, model.initial_state: state}\n    loss, state, _ = sess.run([model.outputs, model.state], feed)\n```\n\nand this is what it took (relevant bits only) to get it working with tuples, it would be great to wrap this up somehow, and make it easier:\n\n```\ndef get_states_list(states, state_is_tuple):\n    \"\"\"\n    given a 'states' variable from a tensorflow model,\n    return a flattened list of states\n    \"\"\"\n    if state_is_tuple:\n        states_list = [] # flattened list of all tensors in states\n        for layer in states:\n            for state in layer:\n                states_list.append(state)\n    else:\n        states_list = [states]\n\n    return states_list\n\n\ndef get_states_dict(states, state_is_tuple):\n    \"\"\"\n    given a 'states' variable from a tensorflow model,\n    return a dict of { tensor : evaluated value }\n    \"\"\"\n    if state_is_tuple:\n        states_dict = {} # dict of { tensor : value } \n        for layer in states:\n            for state in layer:\n                states_dict[state] = state.eval()\n\n    else:\n        states_dict = {states : states.eval()}\n\n    return states_dict\n\ninit_states_list = utils.get_states_list(model.initial_state, model.state_is_tuple)\ninit_states_dict = utils.get_states_dict(model.initial_state, model.state_is_tuple)\nfinal_states_list = utils.get_states_list(model.final_state, model.state_is_tuple)\n\nstates_dict = init_states_dict\n\nloop:\n    feed = {model.inputs:x}\n    feed.update(states_dict)\n    fetch = [model.outputs] + final_states_list\n    ret = sess.run(fetch, feed)\n    outputs = ret[0]\n    states = ret[1:]\n    states_dict = dict(zip(init_states_list, states))\n```\n", "Hi , there are a `tutorial_ptb_lstm_state_is_tuple` in TensorLayer repo, hope it help. \nhttps://github.com/zsdonghao/tensorlayer\n"]}, {"number": 2694, "title": "Updated iOS settings", "body": "", "comments": ["Thanks, once this passes I'll merge.\n", "Or since I'm OOO the rest of the day, make sure to squash and merge, not just merge.\n"]}, {"number": 2693, "title": "Branch 124169174", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Looks like a flake but I'm going to try again. @tensorflow-jenkins test this please\n", "@vrv what was the flaky test? We should address it.\n", "@vrv never mind. it is //tensorflow/core:common_runtime_gpu_gpu_allocator_retry_test \n"]}, {"number": 2692, "title": "Remove sudo in virtualenv during installation", "body": "In r0.8 and r0.7, installation inside virtualenv did not need sudo.  Sudo appears in r0.9 document.  It seems a copy-and-paste typo.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "The docs have gone back and forth so many times, I have no idea what the right answer is anymore ;).  Merging anyway.\n"]}, {"number": 2691, "title": "Gated Feedback LSTM implementation", "body": "This PR aims to add a GFLSTM implementation in contrib.\n\nSome of the reasoning for adding a new MultiCell is explained. The new cells cannot be used without a MultiCell, since rnn doesn't know how to pass the state properly.\n\nI am not sure whether splitting all the h's before is better or not, but I think it should use less memory overall on a high number of layers.\n\nThe state_is_tuple implementation has not been tested but follows what's being done in the current MultiRNN and LSTM cell so they should be fine.\n\nI'm also open to suggestions for the TODO item, I'm not sure how to handle that.\n", "comments": ["Can one of the admins verify this patch?\n", "The PR looks ok to me, but it will need tests. Please add tests to rnn_cell_test and make sure to test both with and without state_is_tuple.\n", "Is there a guide on setting up an environment where I can test? The contributing page doesn't seem to say anything.\n\nAlso any suggestions for the TODO item and the slicing part?\n", "I think you can just run rnn_cell_test to test locally, and then github/jenkins will test all other test for you, right? As for TODO: do you mean that MultiGFRNNCell needs to work with other cells than GFXXX cells? Maybe not, we have MultiRNNCell for that in the end. Or did I misunderstand you?\n", "So for the TODO bit: I'm using this piece of code to test whether the cell that's passed in is a GFLSTMCell `if not isinstance(cell, GFLSTMCell)`. However, that GFLSTMCell can be wrapper by a dropout wrapper for example and then I'd need to test `cell._cell` and so on. I was wondering if there's an easier way to do this. I don't know beforehand how many wrappers are around the cell. I guess I could just do `if cell._cell and isinstance(cell._cell)` and recurse on that but it feels yucky.\n\nI refactored the MultiCell a bit to allow it to potentially work with other GF cells. It now passes the whole previous state along with an index to identify the input previous state for the particular cell.\n", "Would it be possible to extend MultiRNNCell by an optional argument rather than making a separate MultiGFXXX Cell? Maybe that'll be cleaner?\n", "Initial comment:\n\nDon't import _linear.  Use contrib.layers.linear with the appropriate initializer instead.\n\nLulasz: I prefer not making MultiRNNCell more complicated, so this is a good place for this code for now.\n", "Oh, and get rid of the state_is_tuple. That's for backwards compatibility. Assume it's true.\n", "Ping. Any progress on this?\n", "Not really. I was busy finishing my thesis, which I just presented today :)\n\nHopefully, I can finish it by the end of this or the next week, with the main issue being the tests themselves. I started using `state_is_tuple` in my project so that bit works. Also, still unsure of how to deal with accepting only the proper cell types.\n", "(another friendly ping!)\n", "Since it's been about a month since the last ping with no activity, I'm going to close this pull request. @bogdanteleaga please feel free to re-open it when you have the time!\n"]}, {"number": 2690, "title": "Shape inference for Reverse doesn't support unknown shape", "body": "This crashes\n\n```\nimport tensorflow as tf\ndims = tf.placeholder(tf.bool)\ntensor = tf.placeholder(tf.int32)\nresult = tf.reverse(tensor, dims)\nprint(result.get_shape())\n\n```\n\nStack trace\n\n```\n\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/env_extra_test.runfiles/org_tensorflow/tensorflow/contrib/immediate/python/immediate/env_extra_test.py\", line 39, in testReverse\n    result = tf.reverse(tensor, dims)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/env_extra_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_array_ops.py\", line 1405, in reverse\n    name=name)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/env_extra_test.runfiles/org_tensorflow/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/env_extra_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 2251, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/env_extra_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 1696, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/env_extra_test.runfiles/org_tensorflow/tensorflow/python/ops/array_ops.py\", line 631, in _ReverseShape\n    input_shape = op.inputs[0].get_shape().with_rank(dims_shape[0])\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/env_extra_test.runfiles/org_tensorflow/tensorflow/python/framework/tensor_shape.py\", line 639, in with_rank\n    return self.merge_with(unknown_shape(ndims=rank))\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/env_extra_test.runfiles/org_tensorflow/tensorflow/python/framework/tensor_shape.py\", line 816, in unknown_shape\n    return TensorShape([Dimension(None)] * ndims)\nTypeError: __index__ returned non-(int,long) (type NoneType)\n> /Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/env_extra_test.runfiles/org_tensorflow/tensorflow/python/framework/tensor_shape.py(816)unknown_shape()\n-> return TensorShape([Dimension(None)] * ndims)\n\n```\n", "comments": ["Fix incoming shortly....\n", " @mrry @girving  @ry   hi merry, sorry to bother you. It seems that the problem still occurs in the latest tensorflow 1.2.1. I run the code provided by @yaroslavvb  and get the errors.\r\n![1](https://user-images.githubusercontent.com/15981416/27848565-9db83386-6178-11e7-8453-976170eb2b27.png)\r\n\r\nI use ubuntu 14.04 , tensorflow-gpu 1.2.1, titianx-black", "@vanpersie32 That error is unrelated: the calling convention of `tf.reverse` changed with 1.0 to match numpy.  See https://www.tensorflow.org/api_docs/python/tf/reverse."]}, {"number": 2689, "title": "Issue while importing tensorflow", "body": "Facing issue while importing tensorflow. \nOS: Windows 8\nPython version 2.7\nInstallation method anaconda. \n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Anaconda2\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"C:\\Anaconda2\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python__init__.py\", line 46, in <module>\n    _default_dlopen_flags = sys.getdlopenflags()\nAttributeError: 'module' object has no attribute 'getdlopenflags'\n", "comments": ["Unfortunately, TensorFlow is not yet supported on Windows.\n", "Hi, may I know if your issue has been solved? since tensorflow is now supported on Windows"]}, {"number": 2688, "title": "My Changing on Tensorflow is not work", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nOS X EI Capitan version 10.11.5\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.Change the file cifar10_input.py with adding some print in it.\n2.rebuild my tensorflow as the https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#installing-from-sources WHICH Section \"Setting up TensorFlow for Development\" instruction.\n3.And when I back to cifar10 to run the cifar10_train.py, I found nothing have changed about my revised.\n### What have you tried?\n\n1.I try the instruction from other section \"Create the pip package and install\" \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Unfortunately this is too little information.  It sounds like you added a print statement or similar and it didn't show up when run.  This is probably a simple shell error, not an issue with TensorFlow itself.\n", "Not only for print I had ever tried to remove some code, could cause program crash  i.e. def ...., from cifar10_input.py. And When I run cifar10_train.py, it can work well.\n", "Unfortunately I still don't understand your issue, but I suspect it is closer to a StackOverflow question than a problem with TensorFlow.  You could try posting your code changes, or you could try posting them on StackOverflow with the `tensorflow` tag.\n", "Actually, I think I just want to know how to correctly install my tensorflow with some code changed by myself. And I guess the reason why my change is unavailable, is that my tensorflow library is old, the before code without my change. So I think what I really need, just a correct method to develop and install tensorflow with python\n", "Okay, I'll close for now.  Let us know if you have further issues, but we'll probably need more details for those too if so.\n", "-.-! But I have no idea about how to develop and install my own code....cloud you give me some method with detail?\n", "@SunAriesCN: Questions about how to write new code on top of TensorFlow belong better on StackOverflow. \n"]}, {"number": 2687, "title": "Cannot build tensorflow from source (Python 3 library not found) ", "body": "Dear TensorFlow developers,\n\nI am trying to install TensorFlow and could already solve a number of problems but I got stuck with the following error message:\n\n```\n[sfux@e2190 tensorflow]$ bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package\nWarning: ignoring _JAVA_OPTIONS in environment.\nWARNING: Output base '/cluster/home/sfux/.cache/bazel/_bazel_sfux/d3b614f4169d4d1d2d1a6ba84ae877cb' is on NFS. This may lead to surprising failures and undetermined behavior.\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nWARNING: /cluster/home/sfux/.cache/bazel/_bazel_sfux/d3b614f4169d4d1d2d1a6ba84ae877cb/external/protobuf/WORKSPACE:1: Workspace name in /cluster/home/sfux/.cache/bazel/_bazel_sfux/d3b614f4169d4d1d2d1a6ba84ae877cb/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /scratch/20837473.tmpdir/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nWARNING: /cluster/home/sfux/.cache/bazel/_bazel_sfux/d3b614f4169d4d1d2d1a6ba84ae877cb/external/highwayhash/WORKSPACE:1: Workspace name in /cluster/home/sfux/.cache/bazel/_bazel_sfux/d3b614f4169d4d1d2d1a6ba84ae877cb/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /cluster/home/sfux/.cache/bazel/_bazel_sfux/d3b614f4169d4d1d2d1a6ba84ae877cb/external/re2/WORKSPACE:1: Workspace name in /cluster/home/sfux/.cache/bazel/_bazel_sfux/d3b614f4169d4d1d2d1a6ba84ae877cb/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nINFO: Found 1 target...\nERROR: /scratch/20837473.tmpdir/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command \n  (cd /cluster/home/sfux/.cache/bazel/_bazel_sfux/d3b614f4169d4d1d2d1a6ba84ae877cb/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/cluster/apps/python/3.3.3/x86_64/bin:/cluster/apps/openblas/0.2.8_seq/x86_64/gcc_4.8.2/bin:/cluster/apps/swig/3.0.5/x86_64/bin:/cluster/apps/bazel/output:/cluster/apps/java/1.8.0_91/x86_64/bin:/cluster/apps/lsf/9.1/linux2.6-glibc2.3-x86_64/etc:/cluster/apps/lsf/9.1/linux2.6-glibc2.3-x86_64/bin:/cluster/apps/modules/bin:/cluster/apps/gcc/4.8.2/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ibutils/bin:/cluster/home/sfux/bin:/cluster/apps/ansys/v150/fluent/license/lnamd64:/cluster/apps/adm:/cluster/home/sfux/shellscript:/cluster/home/sfux/prog/bash \\\n    TMPDIR=/scratch/20837473.tmpdir \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; rm -rf /tmp/half_plus_two; /cluster/apps/python/3.3.3/x86_64/bin/python bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two; cp -r /tmp/half_plus_two/* bazel-out/local-py3-opt/genfiles/tensorflow/contrib/session_bundle/example/half_plus_two'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\n/cluster/apps/python/3.3.3/x86_64/bin/python: error while loading shared libraries: libpython3.3m.so.1.0: cannot open shared object file: No such file or directory\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 2.621s, Critical Path: 0.09s\n[sfux@e2190 tensorflow]$ \n```\n### Environment info\n\nOperating System:\nCentOS 6.7 (Final), Kernel: 2.6.32-504.1.3\n\nInstalled version of CUDA and cuDNN: \nI am trying an installation from source (Commit: ea9e00a630f91a459dd5858cb22e8cd1a666ba4e)\n\nPython 3.3 is installed in a non-standard location (/cluster/apps/python/3.3.3/x86_64) and it was installed from source. The bin directory of the Python installation is in $PATH, the directory, where libpython3.3m.so.1.0 is located is in $LD_LIBRARY_PATH and the library has the correct name:\n\n[sfux@e2190 tensorflow]$ ls /cluster/apps/python/3.3.3/x86_64/lib64/libpython3.3m.so.1.0\n/cluster/apps/python/3.3.3/x86_64/lib64/libpython3.3m.so.1.0\n[sfux@e2190 tensorflow]$ \n\n[sfux@e2190 tensorflow]$ python3-config --ldflags\n-lpthread -ldl -lutil -lm -lpython3.3m -Xlinker -export-dynamic\n\nAny ideas ?\n", "comments": ["@samfux84: Did you point `./configure` to the right place?  Having `Python` in `$PATH` may be problematic for bazel, but I believe this should be solved by passing the absolute path to `configure`. \n", "Hi Geoffrey Irving,\nthank you for your reply. I was not sure if I had set ./configure to the correct place. Therefore I repeated the installation, set ./configure and I still get the same error, even though configure already takes the correct path as default:\n\n```\n[sfux@e1001 tensorflow]$ ./configure\nPlease specify the location of python. [Default is /cluster/apps/python/3.3.3/x86_64/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] N\nNo GPU support will be enabled for TensorFlow\nConfiguration finished\n[sfux@e1001 tensorflow]$ \n```\n\nBest regards\n\nSam\n", "@martinwicke: Do you know if this is on our end or Bazel's? \n", "It may be a recent thing. Can you cherry-pick edf3f943068445ab58ca7d14a3d3103cafc618d0, or build from r0.9 and see if that helps? This will disable the exporter, which uses a genrule using python.\n\nYou can also try adding `--host_force_python=py3` to your bazel commands (and not disable the exporter).\n", "--host_force_python=py3 is already set in my bazel.rc\n\nPlease see below my bazel.rc:\n\n```\n\n# Autogenerated by configure: DO NOT EDIT\nbuild:cuda --crosstool_top=//third_party/gpus/crosstool\nbuild:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n\nbuild --force_python=py3\nbuild --python3_path=/cluster/apps/python/3.3.3/x86_64/bin/python\nbuild --define=use_fast_cpp_protos=true\nbuild --define=allow_oversize_protos=true\n\nbuild --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python\ntest --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python\ntest --force_python=py3\ntest --host_force_python=py3\nrun --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python\n\nbuild --spawn_strategy=standalone\ntest --spawn_strategy=standalone\nrun --spawn_strategy=standalone\n\nbuild --genrule_strategy=standalone\ntest --genrule_strategy=standalone\nrun --genrule_strategy=standalone\n```\n\nThank you for your suggestion. I will try to checkout the newest commit from the TensorFlow repository and try to build it again.\n", "It's only set for test, not for build. You need the line\n\n```\nbuild --host_force_python=py3\n```\n\nOn Tue, Jun 7, 2016 at 12:44 AM samfux84 notifications@github.com wrote:\n\n> --host_force_python=py3 is already set in my bazel.rc\n> \n> Please see below my bazel.rc:\n> \n> # Autogenerated by configure: DO NOT EDIT\n> \n> build:cuda --crosstool_top=//third_party/gpus/crosstool\n> build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n> \n> build --force_python=py3\n> build --python3_path=/cluster/apps/python/3.3.3/x86_64/bin/python\n> build --define=use_fast_cpp_protos=true\n> build --define=allow_oversize_protos=true\n> \n> build --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python\n> test --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python\n> test --force_python=py3\n> test --host_force_python=py3\n> run --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python\n> \n> build --spawn_strategy=standalone\n> test --spawn_strategy=standalone\n> run --spawn_strategy=standalone\n> \n> build --genrule_strategy=standalone\n> test --genrule_strategy=standalone\n> run --genrule_strategy=standalone\n> \n> Thank you for your suggestion. I will try to checkout the newest commit\n> from the TensorFlow repository and try to build it again.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2687#issuecomment-224204683,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_YW6ae9aRpQmQqDzWY2btVH_xqllks5qJSFPgaJpZM4Iu-gD\n> .\n", "Hi, I tried it with \"build --host_force_python=py3\" but I still got the same error:\n\n```\nERROR: /scratch/20862915.tmpdir/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command \n  (cd /cluster/home/sfux/.cache/bazel/_bazel_sfux/e9a2c00193dcd23b9b6bcd416bfa6b80/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/cluster/apps/python/3.3.3/x86_64/bin:/cluster/apps/swig/3.0.5/x86_64/bin:/cluster/apps/bazel/output:/cluster/apps/java/1.8.0_91/x86_64/bin:/cluster/apps/lsf/9.1/linux2.6-glibc2.3-x86_64/etc:/cluster/apps/lsf/9.1/linux2.6-glibc2.3-x86_64/bin:/cluster/apps/modules/bin:/cluster/apps/gcc/4.8.2/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ibutils/bin:/cluster/home/sfux/bin:/cluster/apps/ansys/v150/fluent/license/lnamd64:/cluster/apps/adm:/cluster/home/sfux/shellscript:/cluster/home/sfux/prog/bash \\\n    TMPDIR=/scratch/20862915.tmpdir \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; rm -rf /tmp/half_plus_two; /cluster/apps/python/3.3.3/x86_64/bin/python bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two; cp -r /tmp/half_plus_two/* bazel-out/local-py3-opt/genfiles/tensorflow/contrib/session_bundle/example/half_plus_two'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\n/cluster/apps/python/3.3.3/x86_64/bin/python: error while loading shared libraries: libpython3.3m.so.1.0: cannot open shared object file: No such file or directory\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 395.284s, Critical Path: 186.98s\n[sfux@e1001 tensorflow]$ cat tools/bazel.rc\n# Autogenerated by configure: DO NOT EDIT\nbuild:cuda --crosstool_top=//third_party/gpus/crosstool\nbuild:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n\nbuild --force_python=py3\nbuild --python3_path=/cluster/apps/python/3.3.3/x86_64/bin/python\nbuild --define=use_fast_cpp_protos=true\nbuild --define=allow_oversize_protos=true\n\nbuild --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python\nbuild --host_force_python=py3\ntest --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python\ntest --force_python=py3\ntest --host_force_python=py3\nrun --define PYTHON_BIN_PATH=/cluster/apps/python/3.3.3/x86_64/bin/python\n\nbuild --spawn_strategy=standalone\ntest --spawn_strategy=standalone\nrun --spawn_strategy=standalone\n\nbuild --genrule_strategy=standalone\ntest --genrule_strategy=standalone\nrun --genrule_strategy=standalone\n[sfux@e1001 tensorflow]$ \n```\n", "Ok, now I'm all out of ideas. Can you try building r0.9 to see if that works for you? It won't have some exporter functionality, but you can replace that with a separately built tensorflow/serving.\n", "I tried building r0.9 and for some reason it worked, or at least it did not terminate with an error message (I have no idea why it worked this time, and I am not sure if the build is reproducible).\n\n```\nbazel-out/host/genfiles/tensorflow/core/protobuf/config.pb_text.cc: In function 'bool tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::ConfigProto*)':\nbazel-out/host/genfiles/tensorflow/core/protobuf/config.pb_text.cc:435:34: warning: 'map_value' may be used uninitialized in this function [-Wmaybe-uninitialized]\n       (*map)[map_key] = map_value;\n                                  ^\nbazel-out/host/genfiles/tensorflow/core/protobuf/config.pb_text.cc:426:9: note: 'map_value' was declared here\n   int32 map_value;\n         ^\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\nINFO: Elapsed time: 119.956s, Critical Path: 95.89s\n[sfux@e3001 tensorflow]$ \n```\n\nIn the instructions provided on the tensorflow webpage, it is stated that the files to create the pip wheel are stored in /tmp/tensorflow_pkg:\n\n```\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\n# To build with GPU support:\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\n**$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg**\n\n# The name of the .whl file will depend on your platform.\n$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.9.0rc0-py2-none-any.whl\n```\n\nBut in my case, there is no directory tensorflow_pkg in /tmp. Therefore I don't know how I should build the pip wheel.\n\nHave you ever considered to switch from bazel to GNU autoconf or CMAKE to build tensorflow ? In CMAKE I can specify an absolute path for all dependencies, which avoids issues as for instance bazel not finding Python. In my case tensorflow decided to again install Eigen 3.2.1 even though I already have a central installation of Eigen 3.2.1 and plenty of environment variables are set:\n\n```\n[sfux@euler06 x86_64]$ module show eigen/3.2.1                                                                                   \n-------------------------------------------------------------------                                                              \n/cluster/apps/modules/modulefiles/eigen/3.2.1:                                                                                   \n\nmodule-whatis    Eigen 3.2.1                                                                                                     \nsetenv           MODULE_eigen_ROOT_DIR /cluster/apps/eigen/3.2.1/x86_64/gcc_4.8.2/serial                                         \nprepend-path     CPATH /cluster/apps/eigen/3.2.1/x86_64/gcc_4.8.2/serial/include                                                 \nprepend-path     C_INCLUDE_PATH /cluster/apps/eigen/3.2.1/x86_64/gcc_4.8.2/serial/include                                        \nprepend-path     CPLUS_INCLUDE_PATH /cluster/apps/eigen/3.2.1/x86_64/gcc_4.8.2/serial/include                                    \nprepend-path     INCLUDE /cluster/apps/eigen/3.2.1/x86_64/gcc_4.8.2/serial/include                                               \nprepend-path     FPATH /cluster/apps/eigen/3.2.1/x86_64/gcc_4.8.2/serial/include                                                 \n-------------------------------------------------------------------\n\n[sfux@euler06 x86_64]$ \n\n```\n\nIn CMAKE I would have to set one variable in order to find an existing Eigen installation.\n\nThank you very much for your help.\n", "Have you run `bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`? That's what creates `/tmp/tensorflow_pkg`.\n\nAs for other build systems: there are both contrib/cmake and contrib/makefile directories, but they're far from complete (only building the C++ parts or partial python), and we do not have the resources to maintain two build systems. If someone were to volunteer to maintain them, they'd have our enthusiastic support.\n", "Thank you very much for your feedback.\n\nI could now successfully reproduce the first building steps. I did a change in the CROSSBUILD file of bazel, such that it uses GCC 4.8.2 (I thought that bazel would take the compiler that is specified in the variable CC, CXX etc, but for some reason it ignored these variables and used GCC 4.4.7 instead). Further more I have changed the bazel.rc file according to your suggestions (adding build --host_force_python=py3). This seems to work so far and is reproducible.\n\nBut in the next step (building the pip wheel) it fails again:\n\n```\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\nINFO: Elapsed time: 321.473s, Critical Path: 103.73s\n[sfux@e2050 tensorflow]$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nFri Jun 10 07:42:15 CEST 2016 : === Using tmpdir: /scratch/20942509.tmpdir/tmp.tlG0Xc7IIM\nrsync: change_dir \"/scratch/20942509.tmpdir/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/external\" failed: No such file or directory (2)\nrsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1039) [sender=3.0.6]\n[sfux@e2050 tensorflow]$\n```\n\nFor some reason the directory \"external\" is missing:\n\n```\n[sfux@e2050 tensorflow]$ ls /scratch/20942509.tmpdir/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/\nd3                             iron_flex_layout            neon_animation         paper_radio_button\ndagre                          iron_form_element_behavior  org_tensorflow         paper_radio_group\neigen_archive                  iron_icon                   paper_behaviors        paper_ripple\nes6_promise                    iron_icons                  paper_button           paper_slider\nfont_roboto                    iron_iconset_svg            paper_checkbox         paper_styles\ngraphlib                       iron_input                  paper_dialog           paper_tabs\n__init__.py                    iron_list                   paper_dialog_behavior  paper_toggle_button\niron_a11y_announcer            iron_menu_behavior          paper_dropdown_menu    paper_toolbar\niron_a11y_keys_behavior        iron_meta                   paper_header_panel     plottable\niron_ajax                      iron_overlay_behavior       paper_icon_button      polymer\niron_autogrow_textarea         iron_range_behavior         paper_input            promise_polyfill\niron_behaviors                 iron_resizable_behavior     paper_item             protobuf\niron_checked_element_behavior  iron_selector               paper_material         six_archive\niron_collapse                  iron_validatable_behavior   paper_menu             web_animations_js\niron_dropdown                  lodash                      paper_menu_button      webcomponentsjs\niron_fit_behavior              MANIFEST                    paper_progress\n[sfux@e2050 tensorflow]$ ls /scratch/20942509.tmpdir/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/\n_solib_k8  tensorflow  third_party  util\n[sfux@e2050 tensorflow]$ \n```\n\nBest regards\n\nSam\n", "Probably related to https://github.com/tensorflow/tensorflow/issues/2751 at this point -- let's de-dupe with that thread now.\n"]}, {"number": 2686, "title": "Fix formatting issue in docs caused by lack of backtick", "body": "Missing backtick was causing unhappiness.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@googlebot  I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@Undo1: Thank you for the fix!  Backticks are depressingly hard to see. :) \n"]}, {"number": 2685, "title": "ImportError when I use  \"from tensorflow.models.rnn import rnn, rnn_cell\"", "body": "I install tensorflow on a new cluster and try to run program on it. \nBut I get this error:\n`ImportError(\"This module is deprecated.  Use tf.nn.rnn_* instead.\")`, \n\nfrom this line: \n`from tensorflow.models.rnn import rnn, rnn_cell`. \n\nIt seems the new version of TF has changed the way to import rnn, but I don't know how to fix it. (version of TF is 0.9)\n", "comments": ["Try `tf.nn.rnn_cell` and similar.\n", "I'm having the same issue. I'm confused, where should `tf.nn.rnn_cell` be placed?\n\n**EDIT**\n\nnew files are in `tensorflow/python/ops/`\n", "No, there are two ways to do it. just like throughout the code you see\ntf.nn.something\n\nthat would work, and you would get rid of the import tensorflow.models.rnn.\n\nand what that means is that it is placed in tensorflow.python.ops\n\nso I just replaced tensorflow.models.rnn with tensorflow.python.ops\n\nOn Thu, Jun 16, 2016 at 9:28 AM, ems7 notifications@github.com wrote:\n\n> I am having the same issue, I tried replacing from tensorflow.models.rnn\n> import rnn_cell with\n> tensorflow.nn.rnn_cell but this gives the error tensorflow.nn.rnn\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2685#issuecomment-226484290,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ADb6MQaGRiOtSaxBIEBNpI5e0qM4mBoZks5qMU-OgaJpZM4Iu8Yg\n> .\n", "using tensorflow 0.9:\n\nfrom tensorflow.python.ops.rnn_cell import BasicLSTMCell\n\nworks for me.\n", "So actually the message is suggesting that it doesn't need a seperate import, but its just namespaced onto the main tf module now.\n\n```\nimport tensorflow as tf\n\ncell = tf.nn.rnn_cell.BasicLSTMCell(500)\n```\n", "Possibly some repackaging needs to be done because currently importing using namespaces returns `ImportError`:\n\n``` python\n>>> import tensorflow as tf\n>>> tf.__version__\n'0.9.0'\n>>> from tensorflow.nn import rnn, rnn_cell\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named 'tensorflow.nn'\n\n>>> tf.nn.rnn\n<function rnn at 0x7f79cf35ea60>\n>>> tf.nn.rnn_cell\n<module 'tensorflow.python.ops.rnn_cell' from '/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell.py'>\n```\n\nIs there an existing issue to allow namespaces in tensorflow? \n", "  I have this problem ,anyone can help?\r\n\r\n\r\nFile \"/home/cynthia/Downloads/tensorflow-r0.7/tensorflow/models/image/cifar10/cifar10.py\", line 47, in <module>\r\n    from tensorflow.models.image.cifar10 import cifar10_input\r\nImportError: No module named 'tensorflow.models'\r\n", "If you are having this error\r\n```\r\nImportError: No module named 'tensorflow.models'\r\n```\r\nThe tensorflow models are not included in tensorflow. You will have to download the tensorflow models zip from GitHub and paste them to the tensorflow folder in python packages.", "from tensorflow.python.keras._impl.keras.utils import data_utils\r\njust write it ", "so I just replaced tensorflow.models.rnn with tensorflow.python.ops \r\nit did work"]}]